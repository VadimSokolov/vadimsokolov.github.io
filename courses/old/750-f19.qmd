


## OR 750/610. Deep Learning
**Department of Systems Engineering and Operations Research**\
**George Mason University**\
**Fall 2019**\
\\




[Material](https://www.dropbox.com/sh/baa7xf79uu9ol7a/AAC_We5RC3kqMGoABB51YGxra?dl##0 Course)
\\

**Instructor**: [(vsokolov(at)gmu.edu](http://vsokolov.org Vadim Sokolov)) \
**Office hours**: By appointment\
**TA**: Wanru Li (wli15(at)masonlive.gmu.edu)\
**Office Hours**: Mon 4-6 pm at ENGR 2216

#### Presentation Materials

All of the posts based on the presentations are posted here [dlclass2019.github.io](https://dlclass2019.github.io)

###### PhD
Please upload your materials to [here](https://www.dropbox.com/request/UPTOWsxmzLpByhda34he).

To post your blog, send me your GiHub handle and use this [instructions](https://help.github.com/en/github/working-with-github-pages/adding-content-to-your-github-pages-site-using-jekyll) to add your post

- 10\/30: Uncertainty and Super resolution by Xavier Guitiaux. 40-60 min. Blog by Trajectory team. [2](http://arxiv.org/abs/1911.01490 paper 1), [](http://arxiv.org/abs/1911.01486 paper)
- 11\/6: Unbalanced data and Generalization by Zhengyang Fan, Di Zhang, Zhenlong Jiang. Full lecture. Blog by NLP team\\

[Slides](https://www.dropbox.com/s/rn03ncyn7zgwdb5/or750_zhengyang%20fan%20and%20zhenlong%20jiang%20and%20di%20zhang.pdf?dl##1)\\

Notes from Zhengyang Fan on prep materials:\

The presentation for next class will mainly focus on interpolation phenomenon, and I plan to start this topic from kernel regression, thus some background in kernel space would be helpful for students. I have prepared a brief introduction for kernel space ([helpful](../../pdf/KernelSpace.pdf file)), which was also included in our presentation slides as backup slides. For students who are willing to have a deeper understanding of kernel, the following [](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf lecture note is), which includes some visualizations and simple examples. \

For Rademacher complexity and uniform laws o larger numbers, following papers and lecture notes can be useful for students:
-- [paper](http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf Rademacher complexity)
-- [note](https://www.cs.cmu.edu/~ninamf/ML11/lect1117.pdf  Rademacher complexity lecture)
-- [wiki](https://en.wikipedia.org/wiki/Rademacher_complexity Rademacher complexity)
-- [note](https://www.stat.berkeley.edu/~mjwain/stat210b/Chap4_Uniform_Feb4_2015.pdf Uniform Laws of Large Numbers lecture)
\
My presentation on theory of deep learning is mainly based on the following [series](https://simons.berkeley.edu/workshops/schedule/10624 video lectures). Students who are interested in theory may benefit from these video lectures.


- 11\/13 NLP by Rahul Pandey, Angeela,  Junxiang(Will), Jomana. Full lecture. Blog by the unbalanced/generalization team
- 11\/20 Adverarial Atatchs by Farnaz Behnia and Azadeh Gharibreza Yazdi. Half lecture. Blog by Uncertainty team
- 11\/20 Trajecotry Generation by Chris Grubb. 40-60 min.  Blog by adversarial team

###### MS
Please upload your 1-page proposals to  [here](https://www.dropbox.com/request/sb6aoseTcf03D0XHQnms). Deadline is 11/6. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures. 

- 11\/20
- 12\/4

#### HW
- [tutorial](https://www.dropbox.com/s/6r3ff01q64584zu/1.pdf?dl##1 HW1), Due Sep 18. Optional: [](http://cs231n.github.io/python-numpy-tutorial/ numpy)
- [HW2](https://www.dropbox.com/s/im1r52z56j5bhdm/2.pdf?dl##1), Due Sep 29
- [HW3](https://www.dropbox.com/s/2bmw6hgbwp82squ/3.pdf?dl##1), Due Oct 9
- [HW4](https://www.dropbox.com/s/64d50djasajv7ke/hw4.ipynb?dl##1), Due Oct 30
- [HW5](https://www.dropbox.com/s/wx59moxlzkt9bpc/5.pdf?dl##5)

#### Announcements
- Presentation/Project proposals due Oct 16
- 9\/3\/2019: Room changed to EXPL L111
- 3\/15\/2019: No Class on October 23 (INFORMS Meeting)
- 3\/15\/2019: No Class on November 27 (Thanksgiving recess)
- 3\/15\/2019: First class is on Aug 28 at 7:20pm
- 3\/15\/2019: Last class is on Dec 4


#### Schedule
- DL Overview + Python (numpy and PyTorch) (Week 1)\
Notes Ch 1; [Polson18](https://arxiv.org/abs/1808.08618)
- Probability (Week 2)\
Notes Ch 2-3; [Domingos](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)\
Optional DLB Ch 3
- Optimization: SGD, Backprop (Week 3)\
Notes Ch 4, [Baydin17](https://arxiv.org/pdf/1502.05767.pdf), 
- Architectures  (Week 4)\
Notes ch 5; [Oord16](https://arxiv.org/pdf/1606.05328.pdf)
- More optimization (Week 5)\
[Glorot10](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- Bayes DL (6-7)\
[p3](https://arxiv.org/pdf/1603.00391.pdf p1), [](https://arxiv.org/pdf/1805.00784.pdf p2), [](https://arxiv.org/pdf/1906.02691.pdf)
- DL Theory (Week 8)
- Research paper presentations (Week 9-12)
- Project Presentations (Week 13)

\\
This is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems.  The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic. 
\\
The lectures and homework for 750 and 610 are the same. The difference is in the final project. If you are registered for 750, you will read and present research papers and if you are registered for 610 you will do a Kaggle-type project. Both research paper presentations and projects are to be done in a group of size up to 5 for research papers and 3 for projects. 




#### Research paper presentations by 750 students
During weeks 9-12, this class will be run in a seminar mode. A team of students will prepare a topic and will lead the discussion and another team will write a blog-post about the class  and will post it on Medium.  Students responsible for posting the blog summary will be different from the ones charged with leading the topic discussion, but should work closely with the leaders on the posted write-up.

###### Leading Team. The team responsible for leading a class should:
- Two weeks before the scheduled class, meet briefly with me to discuss plan for the class. You should decide on a team leader for this class, who will be the one responsible for making sure everyone on the team knows what they are doing and coordinating the team's efforts.
- The Monday the week of the class, at least a few representatives from the team should come to my office to discuss the plan for the class. You should come prepared to this meeting with suggested papers and ideas about how to present them.
- On Tuesday before class, send me the preparation materials for the class. This can include links to papers to read, but could also include exercises to do or software to install and experiment with, etc. I will post it on the course page.
- Day of class: lead an interesting, engaging, and illuminating class! This is a 2.5 hour class, so it can’t just be a series of unconnected, dull presentations. You need to think of things to do in class to make it more worthwhile and engaging.
- After class: help the Blogging team by providing them with your materials, answering their questions, and reviewing their write-up.

###### Blogging Team. The team responsible for blogging a class should:
- The week before the scheduled class, develop a team plan for how to manage the blogging. 
- One team member should be designated the team leader for the blogging. The blogging leader is responsible for making sure the team is well coordinated and everyone knows what they are doing and follows through on this.
- During class, participate actively in the class, and take detailed notes (this can be distributed among the team).
- By the Wed following class, have a the blog post ready and posted on Medium. Get comments from  the rest of the class (including the leading team and coordinators).
- By the next Friday (one week after the class), have a final version of the blog post ready.

###### Suggested Research Paper Topics
- Adversarial attacks
- DL in reinforcement learning
- Interpretable DL
- Science applications of DL (physics, molecular biology,...)
- Engineering applications of DL (logistics, energy, smart grids, congestion management,...)
- Natural language processing

#### Data analysis projects by 610 students
You will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 9 of the class you should have a team formed and data set + analysis problem identified. You need to email me a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. During week 13, you will have a time slot to present your findings. You are also encouraged (although it is not required) to post results of your analysis on Medium, if you think it is worth sharing. 

#### Course staff
**Lectures**: [Hall](https://goo.gl/maps/d8yJBSXQXnzpmoz3A Exploratory) L111. 7:20-10pm on Wed\
**Grades**: 40\% homework, 60\% class presentations\


#### List of topics
 - Convex Optimization
	 -- Stochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration)
	 -- Second order methods
	 -- ADMM
	 -- Regularization (l1, l2 and dropout)
	 -- Batch normalization
 - Theory of deep learning 
	 -- Universal approximators
	 -- Curse of dimensionality
	 -- Kernel spaces
	 -- Topology and geometry
 - Computational aspects (accelerated linear algebra, reduced precision calculations, parallelism)
 - Architectures (CNN, LSTM, MLP, VAE)
 - Bayesian DL
 - Deep reinforcement learning
 - Hyperparameter selection and parameter initialization
 - Generative models (GANs)


#### Books
- Deep Learning (DLB)
([page)](http://www.deeplearningbook.org book)
- Deep Learning with Python (DLPB)
([page)](https://www.manning.com/books/deep-learning-with-python book)
- Learning Deep Architectures for AI ([monograph)](files/bengiofull2009.pdf)

#### Per Topic Resources

###### Section 1: Architectures
- Tuning CNN architecture ([blog)](https://bowenbaker.github.io/metaqnn/)
- Sequence to Sequence Learning with Neural Networks ([paper)](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- Skip RNN ([paper)](http://www.gitxiv.com/posts/6ozns2NF7odKrHZ3r/skip-rnn blog and)
- Learning the Enigma with Recurrent Neural Networks ([blog)](https://greydanus.github.io/2017/01/07/enigma-rnn/)
- LSTM [blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- Generative Adversarial Networks ([presentation)](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks)
- GANs at OpenAI ([blog)](https://blog.openai.com/generative-models/?utm_content##buffer50108,utm_medium##social,utm_source##twitter.com,utm_campaign##buffer)
- Adaptive Neural Trees ([paper)](https://arxiv.org/pdf/1807.06699.pdf)
- [Cortex](https://arxiv.org/pdf/1604.03640.pdf Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual)
- [Recognition](https://arxiv.org/pdf/1512.03385v1.pdf Deep Residual Learning for Image)
- [Networks](https://arxiv.org/pdf/1507.06228v2.pdf Training Very Deep)
- [Modeling](https://arxiv.org/pdf/1803.01271.pdf An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence) 
- [solution](https://arxiv.org/pdf/1807.03247.pdf An intriguing failing of convolutional neural networks and the CoordConv)
- [Need](https://arxiv.org/abs/1706.03762 Attention Is All You)
- [Networks](https://arxiv.org/abs/1701.00160 NIPS 2016 Tutorial: Generative Adversarial)
- [Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf Intuitively Understanding Variational)
- [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
- [PixelCNN](http://proceedings.mlr.press/v48/oord16)
- [https://chrisorm.github.io/NGP.html](Neural Processes)


###### Section 2: Optimization
- [Book](https://web.stanford.edu/~boyd/cvxbook/ BV)
- [(1970](http://www.convexoptimization.com/TOOLS/ConvexAnalysisRockafellar.pdf Convex Analysis by Rockafellar))
- [Lecture](https://youtu.be/xpwx0LIz-bs Nesterov's)
- [(1983)](./files/nesterov.pdf Nesterov)
- [(1964)](./files/polyak64.pdf Polyak)
- [Learning](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf The Tradeoffs of Large Scale)
- [(2004)](https://www.springer.com/us/book/9781402075537 Nesterov)
- HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent ([paper)](http://i.stanford.edu/hazy/papers/hogwild-nips.pdf)
- SGD ([link)](https://xianblog.wordpress.com/2017/05/10/the-invasion-of-the-stochastic-gradients/)
- [Sampling](https://arxiv.org/pdf/1704.05201.pdf Stein Variational Adaptive Importance)
- [Dynamics](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf Bayesian Learning via Stochastic Gradient Langevin)
- [Optimization](https://arxiv.org/pdf/1512.07962.pdf Bridging the Gap between Stochastic Gradient MCMC and Stochastic)
- [code)](https://arxiv.org/abs/1806.09055 DARTS: Differentiable Architecture Search) ([](https://t.co/36J7HUbYkD)
- [code)](https://arxiv.org/abs/1611.01578 Neural Architecture Search with Reinforcement Learning) ([](https://github.com/carpedm20/ENAS-pytorch)
- [Search](https://arxiv.org/abs/1802.01548 Regularized Evolution for Image Classifier Architecture)
- [learning](http://proceedings.mlr.press/v28/sutskever13.pdf On the importance of initialization and momentum in deep)
- [Minima](http://www.bioinf.jku.at/publications/older/3304.pdf Flat)
- [Works](https://distill.pub/2017/momentum/ Why Momentum Really)
- [Minima](https://arxiv.org/pdf/1609.04836.pdf On Large-Batch Training for Deep Learning: Generalization Gap and Sharp)
- [Nets](https://arxiv.org/pdf/1703.04933.pdf Sharp Minima Can Generalize For Deep)
- [DNNs](https://arxiv.org/pdf/1802.10026.pdf Loss Surfaces, Mode Connectivity, and Fast Ensembling of)
- [Learning](https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf The Marginal Value of Adaptive Gradient Methods in Machine)
- [Acceleration](https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/ Revisiting Nesterov’s)

###### Section 3: Theory
- Polyak, Boris, and Pavel Shcherbakov. "Why does Monte Carlo fail to work properly in high-dimensional optimization problems?." Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. ([paper)](https://arxiv.org/pdf/1603.00311.pdf)
- Leni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. "Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation." In Signal Image Technology and Internet Based Systems, 2008. SITIS'08. IEEE International Conference on, pp. 344-351. IEEE, 2008. ([paper)](./files/kolmogorov.pdf)
- Klartag, Bo'az. "A central limit theorem for convex sets." Inventiones mathematicae 168, no. 1 (2007): 91-131. ([slides)](https://arxiv.org/pdf/math/0605014.pdf paper), [](http://web.math.unifi.it/users/salani/cortona2007/talks/klartag.pdf)
- Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. "Revisiting unreasonable effectiveness of data in deep learning era." In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017.
 ([blog)](https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)
- Bengio, Yoshua, Aaron Courville, and Pascal Vincent. "Representation learning: A review and new perspectives." IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. ([paper)](http://www.iro.umontreal.ca/~lisa/pointeurs/TPAMISI-2012-04-0260-1.pdf)
- Braun, Jürgen. "An application of Kolmogorov's superposition theorem to function reconstruction in higher dimensions." (2009). ([dissertation)](./files/braun_disser.pdf)
- Kolmogorov. "On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables" ([paper)](./files/Kolmogorov1961.pdf)
- Arnold. "On functions of three variables" ([papers)](./file/Arnold2009Collection.pdf collection of)
- Bianchini, Monica, and Franco Scarselli. "On the complexity of shallow and deep neural network classifiers." In ESANN. 2014.([paper)](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2014-44.pdf)
- Girosi, Federico, and Tomaso Poggio. "Representation properties of networks: Kolmogorov's theorem is irrelevant." Neural Computation 1, no. 4 (1989): 465-469. ([paper)](./files/Girosi89.pdf)
- Kůrková, Věra. "Kolmogorov's theorem and multilayer neural networks." Neural networks 5, no. 3 (1992): 501-506. ([paper)](./files/Kurkova92.pdf)
- Poggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. "Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review." International Journal of Automation and Computing 14, no. 5 (2017): 503-519. ([paper)](https://arxiv.org/pdf/1611.00740.pdf)
- Telgarsky, Matus. "Representation benefits of deep feedforward networks." arXiv preprint arXiv:1509.08101 (2015). ([paper)](https://arxiv.org/abs/1509.08101)
- Montufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. "On the number of linear regions of deep neural networks." In Advances in neural information processing systems, pp. 2924-2932. 2014.  ([paper)](https://arxiv.org/abs/1402.1869)
- Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. "Understanding deep learning requires rethinking generalization." arXiv preprint arXiv:1611.03530 (2016). ([paper)](https://arxiv.org/abs/1611.03530)
- Lin, Henry W., Max Tegmark, and David Rolnick. "Why does deep and cheap learning work so well?." Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. ([paper)](https://arxiv.org/abs/1608.08225)
- Stéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks ([video)](https://youtu.be/0wRItoujFTA)
- [Learning](https://dspace.mit.edu/bitstream/handle/1721.1/107787/CBMM-Memo-066.pdf?sequence##4 Theory of Deep Learning II: Landscape of the Empirical Risk in Deep)
- [addition](./filed/Kolmogorov1957.pdf  On the representation of continuous functions of many variables by superposition of continuous functions of one variable and)
- [Networks](./files/matus.pdf Representation Power of Feed forward Neural)
- [Networks](https://arxiv.org/pdf/1512.03965.pdf The Power of Depth for Feedforward Neural) 


###### Section 4: Reinforcement Learning
[Optimization](https://arxiv.org/abs/1802.10592 Model-Ensemble Trust-Region Policy)
[Models](https://arxiv.org/abs/1805.12114 Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics)
- [Truth](http://www.argmin.net/2018/02/20/reinforce/ The Policy of)
- [Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work)

###### Section 5: Bayesian DL
- VAE with a VampPrior ([paper)](https://arxiv.org/pdf/1705.07120.pdf)
- Bayesian DL ([blog)](http://www.datasciencecentral.com/profiles/blogs/a-curated-list-of-resources-dedicated-to-bayesian-deep-learning?utm_content##buffer3671b,utm_medium##social,utm_source##twitter.com,utm_campaign##buffer)
- Recognition Networks for Approximate Inference in BN20 Networks ([paper)](https://arxiv.org/pdf/1301.2295.pdf)
- Non-linear regression models for Approximate Bayesian Computation ([paper)](https://arxiv.org/abs/0809.4178)
- DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression ([paper)](https://arxiv.org/abs/1602.04805)
- Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation ([paper)](https://arxiv.org/abs/1605.06376)
- Auto-Encoding Variational Bayes ([paper)](https://arxiv.org/abs/1312.6114)
- Composing graphical models with neural networks for structured representations and fast inference ([paper)](https://arxiv.org/abs/1603.06277)
- [Inference](https://arxiv.org/pdf/1802.02538.pdf Yes, but Did It Work?: Evaluating Variational)

###### Section 6: Practical Tricks
- [Averaging](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)
- - Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks ([paper)](https://arxiv.org/pdf/1603.01431.pdf)
- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ([paper)](https://arxiv.org/pdf/1502.03167.pdf)
- Auto-Encoding Variational Bayes ([paper)](https://arxiv.org/pdf/1312.6114.pdf)
- Twin Networks: Using the Future as a Regularizer ([paper)](https://arxiv.org/pdf/1708.06742.pdf)
-  Don't Decay the Learning Rate, Increase the Batch Size ([paper)](https://arxiv.org/abs/1711.00489)
- DL Tuning ([blog)](http://theorangeduck.com/page/neural-network-not-working)
- [Survey](https://arxiv.org/pdf/1703.09039.pdf Efficient Processing of Deep Neural Networks: A Tutorial and)


#### Other  Resources
###### Additional Reading List
- 50 Years of Data Science by Donoho ([paper)](./files/50YearsofDataScience.pdf)
- [Overview](https://arxiv.org/pdf/1404.7828.pdf Deep Learning in Neural Networks: An)
- [Networks](https://arxiv.org/pdf/1806.11146.pdf Adversarial Reprogramming of Neural)
- [learning](http://unsupervised.cs.princeton.edu/deeplearningtutorial.html Toward theoretical understanding of deep)

###### Blogs
- Papers with code [link](https://paperswithcode.com)
- Security ([blog)](http://www.cleverhans.io)
- Unsupervised learning ([blog)](https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9)
- Cybersecurity ([collection)](https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042 paper)
- [Visualization](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html Opt)

###### Videos
- Deep Energy ([blog)](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)
- DL Summer school 2015 ([videos)](http://videolectures.net/deeplearning2015_montreal/)
- DL Representations ([blog)](http://netdissect.csail.mit.edu)
- PyData 2017 ([videos)](https://www.analyticsvidhya.com/blog/2017/05/pydata-amsterdam-2017-machine-learning-deep-learning-data-science/)

###### Other courses with good web presence
- Stanford's CS231n ([page)](http://cs231n.github.io course)
- Stanford's STATS385 ([page)](https://stats385.github.io course)
- [fast.ai](http://www.fast.ai)
- [learning](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ Nando de Freitas’ course on machine)
- UC Berkeley Stat241B ([lectures)](https://bcourses.berkeley.edu/courses/1409209/pages/lectures)
- UCUC CSE598 ([page)](http://mjt.web.engr.illinois.edu/courses/mlt-f17/ course)
- [DRL](https://github.com/udacity/deep-reinforcement-learning Udacity)



###### Tools
- [TensorFlow](https://www.tensorflow.org)
- [Keras](https://keras.io)
- [(Google](http://playground.tensorflow.org TF Playground))
- [Sony](https://dl.sony.com DL) 
- [profiler](https://jiffyclub.github.io/snakeviz/ SnakeViz) (python)
- [PyTorch](http://pytorch.org)
- [Examples](https://github.com/jeffalstott/pystan_time_series TS Stan)
- [Glow](https://blog.openai.com/glow/ OpenAI)

###### Misc Links
- [projects](https://github.com/ritchieng/the-incredible-pytorch Pytorch resources) (a curated list of tutorials, papers,)
- [medium?](https://www.christies.com/Features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx Is artificial intelligence set to become art’s next)


#### Previous instances
- [2017](./2017/index.html Fall)
- [2018](./2018/index.html Fall)

#https://www.coursera.org/learn/intro-to-deep-learning
#http://www.machinelearning.ru/wiki/index.php?title##Глубинное_обучение_%28курс_лекций%29/2017
#https://github.com/aosokin/dl_cshse_ami/tree/master/2019-spring
#https://bayesgroup.ru/teaching/
#saddepal@gmu.edu;savala@gmu.edu;fbehnia@gmu.edu;schava@gmu.edu;hgangava@gmu.edu;xgitiaux@gmu.edu;vgunnala@gmu.edu;nhuang2@gmu.edu;wkirsche@gmu.edu;jkrishn@gmu.edu;slakaman@gmu.edu;elynch9@gmu.edu;mmittapa@gmu.edu;gn@gmu.edu;krajend2@gmu.edu;srayapro@gmu.edu;asaid8@gmu.edu;oshaat@gmu.edu;ashambhu@gmu.edu;asothor2@gmu.edu;tstickl@gmu.edu;rzimme10@gmu.edu;aachary@gmu.edu;jbashata@gmu.edu;zchai2@gmu.edu;ychen37@gmu.edu;zfan3@gmu.edu;agharibr@gmu.edu;bghimire@gmu.edu;cgrubb@gmu.edu;zjiang@gmu.edu;nnewman7@gmu.edu;rpandey4@gmu.edu;tsmith58@gmu.edu;jwang40@gmu.edu;fyu2@gmu.edu;dzhang22@gmu.edu

