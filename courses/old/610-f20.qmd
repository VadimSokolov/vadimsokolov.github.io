


## OR 610. Deep Learning
**Department of Systems Engineering and Operations Research**\
**George Mason University**\
**Fall 2020**\
\\


# Courses
# https://datacamp.com/groups/shared_links/486def6889202c4d41d141bdc1427622d88eaf788c2ec719b020cc75fa32821a
# http://www.machinelearning.ru/wiki/index.php?title##Глубинное_обучение_%28курс_лекций%29/2019
# https://github.com/aosokin/dl_cshse_ami/tree/master/2019-spring
# http://www.machinelearning.ru/wiki/index.php?title##Нейробайесовские_методы_машинного_обучения_%28курс_лекций%29_/_2020
# https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/week/1
# https://github.com/yandexdataschool/Practical_DL
# https://github.com/yandexdataschool/Practical_RL

#password: deepmsclass
[Course Material](https://www.dropbox.com/sh/gddm5kwboe64tyz/AACObyZjs3xUkbq7wUrE9Ujea?dl##0)
\\

#https://piazza.com/demo_login?nid##ke7yfo8fkwx12i&auth##6905b70
[Piazza](https://piazza.com/gmu/fall2020/or610)
\\

**Instructor**: [(vsokolov(at)gmu.edu](http://vsokolov.org Vadim Sokolov)) \
**TA**: Ali Seyedmazloom (aseyedma (at) masonlive.gmu.edu)\
**Office hours**: By appointment\

#### Datacamp
If you are rusty on Python, I suggest you refresh your skills using [Datacamp](https://datacamp.com/groups/shared_links/486def6889202c4d41d141bdc1427622d88eaf788c2ec719b020cc75fa32821a). Datacamp gave students in this class a free access to all of the courses. If you follow the link above you can get your free access using masonlive email. I also listed some of the Python courses I suggest there.

#### Schedule (Homework are due on Mondays at noon)
###### Week 1 (Aug 24): Introduction
- [What is a neural network?](https://coursera.org/share/5c500f68d0a17831417a69284f8e3a4a)
- [Supervised Learning with Neural Networks](https://coursera.org/share/9c132474e78df7c4c019d6c91d224a8d)
- [Why is Deep Learning taking off?](https://coursera.org/share/0a61ee951fa8f1edee71a8bdf764c145)
- [Linear Model](https://coursera.org/share/05dc735b893bd7268a41210d116da060)
- [Linear classification](https://coursera.org/share/549f09ddb524697de6ffbb0865fcbef4)

###### Week 2 (Aug 31): Gradient Descent
- [Function](https://coursera.org/share/710ed082ffe55f749fcf67dac0b2c00e Logistic Regression Cost)
- [Descent](https://coursera.org/share/c15d6d08249e04465f62697025bd2dc9 Gradient)
- [Derivatives](https://coursera.org/share/d12377d5f947cbc7c4b7195fa9c8d2b9)
- [Examples](https://coursera.org/share/dc19bfe6c6214c02ca6dc00511b1b928 More Derivative)
- [graph](https://coursera.org/share/51fd0223f440c4b921a6325009721911 Computation)
- [Graph](https://coursera.org/share/9f167a2ac022e262795db23a31da4773 Derivatives with a Computation)
- [Descent](https://coursera.org/share/1912c891ccb60c95843b056e7ce16ed0 Logistic Regression Gradient)
- [Examples ](https://coursera.org/share/5cd4e8e94de9809a9e9804f324b9c14b Gradient Descent on m)

###### Week 3 (Sep 7): Neural Networks
- HW1 Due
- [Overview](https://coursera.org/share/410ca1012714f99104d0310f087e2aba Neural Networks)
- [Representation](https://coursera.org/share/b49295af20ee8bc35363df4115e4a730 Neural Network)
- [Output](https://coursera.org/share/eba336cd7c3ac2e5bdebdf374acf5ada Computing a Neural Network's)
- [examples](https://coursera.org/share/a1d06e73b2832db2b55aa6f30d512b7d Vectorizing across multiple)
- [Implementation](https://coursera.org/share/f58a66a796711d80bc704b94739796aa Explanation for Vectorized)
- [functions](https://coursera.org/share/ef6aa088c2c54dad657c5d8357c4067e Activation)
- [functions?](https://coursera.org/share/6aa2a5ab825821d906fc3bf1281acdb0 Why do you need non-linear activation)
- [functions](https://coursera.org/share/aa496bf6bcafde8e66e3c6df7744e9e4 Derivatives of activation)
- [Networks ](https://coursera.org/share/c4b2b5c02ea549819d9432043912da5b Gradient descent for Neural)
- [Backpropagation](https://coursera.org/share/a02f6514f0aafdd5385b11d30e0ed93e) 
- [Initialization](https://coursera.org/share/11f721bc9b15dbfcce72f5e995a93fd1 Random) 
- [implementation](https://coursera.org/share/ad07c49b88c58301536544f9ec2d607e Efficient MLP)
- [derivatives](https://coursera.org/share/4739f97c8f5dff9e62ea979bb847fae5 Other matrix)
###### Week 4  (Sep 14): PyTorch
- HW2 Due (NN)
###### Week 5 (Sep 21): Optimization and regularization 
- [(a)](https://youtu.be/wEoyxE0GP2M Training Neural Networks)
- [(b)](https://youtu.be/_JB0AO7QxSA Training Neural Networks)
- HW3 due (PyTorch)
###### Week 6, 7 (Sep 28, Oct 5): CNN
- HW4 Due (Optimisation)
- Lecture 9 of CS231n ([notes)](https://youtu.be/DAOcjicFr1Y video), [](https://cs231n.github.io/convolutional-networks/)
###### Week 8,9 (Oct 12, Oct 19): Recurrent Nets 
- Project proposals due
###### Week 10,11, 12 (Oct 26, Nov 2, Nov 9): Bayesian Neural Networks (Hinton Notes)
- [approach](https://www.cs.toronto.edu/~hinton/coursera/lecture9/lec9d.mp4 9d - Introduction to the bayesian)
- [decay](https://www.cs.toronto.edu/~hinton/coursera/lecture9/lec9e.mp4 9e - The bayesian interpretation of weight)
- [costs](https://www.cs.toronto.edu/~hinton/coursera/lecture9/lec9f.mp4 9f - MacKays quick and dirty method of fixing weight)
- [models](https://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10a.mp4 10a - Why it helps to combine)
- [experts](https://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10b.mp4 10b - Mixtures of)
- [learning](https://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10c.mp4 10c - The idea of full bayesian)
- [practical](https://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10d.mp4 10d - Making full bayesian learning)
- [nets](https://www.cs.toronto.edu/~hinton/coursera/lecture10/lec10e.mp4 10e - Dropout an efficient way to combine neural)
###### Week 13, 14 (Nov 16, Nov 30): Deep Reinforcement Learning
- Lectures 1-5 by David Silver ([slides)](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver videos and)
- HW6 due (Bayes)
###### Week 15 (Dec 7): Final Projects
- Final projects are due. 

#### Data analysis projects
You will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 8 of the class you should have a team formed and data set + analysis problem identified. You need to submit a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures. 


You will post results of your analysis on the class blog post. The final project will be graded on presentation, writing and analysis.

All of the posts based on the presentations are posted here [vsokolov.org\/or610fall2020](http://vsokolov.org/or610fall2020/)

#### Group Work
Both projects and homework can be done in a groups of size of up to 3 people. You can change groups in between. If you do a homework in a gorup, it means that all of the members of the group do it individually and can consult with each other. You can also do 1 submission per group if you prefer. You can use "group" section of the piazza page to find teammates if you need any. If you need help finding a group, please email me. 

#### Grading
Each hw is 10 points, project is 30.

# #### HW
# - [tutorial](https://www.dropbox.com/s/6r3ff01q64584zu/1.pdf?dl##1 HW1), Due Sep 18. Optional: [](http://# cs231n.github.io/python-numpy-tutorial/ numpy)
# - [HW2](https://www.dropbox.com/s/im1r52z56j5bhdm/2.pdf?dl##1), Due Sep 29
# - [HW3](https://www.dropbox.com/s/2bmw6hgbwp82squ/3.pdf?dl##1), Due Oct 9
# - [HW4](https://www.dropbox.com/s/64d50djasajv7ke/hw4.ipynb?dl##1), Due Oct 30
# - [HW5](https://www.dropbox.com/s/wx59moxlzkt9bpc/5.pdf?dl##5)


\\
This is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems.  The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic. 
\\




#### Books
- Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.
- Ripley, Brian D. Pattern recognition and neural networks. Cambridge university press, 2007.
- Bishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.

#### Per Topic Resources

######  Architectures
- Tuning CNN architecture ([blog)](https://bowenbaker.github.io/metaqnn/)
- Sequence to Sequence Learning with Neural Networks ([paper)](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- Skip RNN ([paper)](http://www.gitxiv.com/posts/6ozns2NF7odKrHZ3r/skip-rnn blog and)
- Learning the Enigma with Recurrent Neural Networks ([blog)](https://greydanus.github.io/2017/01/07/enigma-rnn/)
- LSTM [blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- Generative Adversarial Networks ([presentation)](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks)
- GANs at OpenAI ([blog)](https://blog.openai.com/generative-models/?utm_content##buffer50108,utm_medium##social,utm_source##twitter.com,utm_campaign##buffer)
- Adaptive Neural Trees ([paper)](https://arxiv.org/pdf/1807.06699.pdf)
- [Cortex](https://arxiv.org/pdf/1604.03640.pdf Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual)
- [Recognition](https://arxiv.org/pdf/1512.03385v1.pdf Deep Residual Learning for Image)
- [Networks](https://arxiv.org/pdf/1507.06228v2.pdf Training Very Deep)
- [Modeling](https://arxiv.org/pdf/1803.01271.pdf An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence) 
- [solution](https://arxiv.org/pdf/1807.03247.pdf An intriguing failing of convolutional neural networks and the CoordConv)
- [Need](https://arxiv.org/abs/1706.03762 Attention Is All You)
- [Networks](https://arxiv.org/abs/1701.00160 NIPS 2016 Tutorial: Generative Adversarial)
- [Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf Intuitively Understanding Variational)
- [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
- [PixelCNN](http://proceedings.mlr.press/v48/oord16)
- [https://chrisorm.github.io/NGP.html](Neural Processes)


###### Optimization
- [Book](https://web.stanford.edu/~boyd/cvxbook/ BV)
- [(1970](http://www.convexoptimization.com/TOOLS/ConvexAnalysisRockafellar.pdf Convex Analysis by Rockafellar))
- [Lecture](https://youtu.be/xpwx0LIz-bs Nesterov's)
- [(1983)](./files/nesterov.pdf Nesterov)
- [(1964)](./files/polyak64.pdf Polyak)
- [Learning](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf The Tradeoffs of Large Scale)
- [(2004)](https://www.springer.com/us/book/9781402075537 Nesterov)
- HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent ([paper)](http://i.stanford.edu/hazy/papers/hogwild-nips.pdf)
- SGD ([link)](https://xianblog.wordpress.com/2017/05/10/the-invasion-of-the-stochastic-gradients/)
- [Sampling](https://arxiv.org/pdf/1704.05201.pdf Stein Variational Adaptive Importance)
- [Dynamics](https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf Bayesian Learning via Stochastic Gradient Langevin)
- [Optimization](https://arxiv.org/pdf/1512.07962.pdf Bridging the Gap between Stochastic Gradient MCMC and Stochastic)
- [code)](https://arxiv.org/abs/1806.09055 DARTS: Differentiable Architecture Search) ([](https://t.co/36J7HUbYkD)
- [code)](https://arxiv.org/abs/1611.01578 Neural Architecture Search with Reinforcement Learning) ([](https://github.com/carpedm20/ENAS-pytorch)
- [Search](https://arxiv.org/abs/1802.01548 Regularized Evolution for Image Classifier Architecture)
- [learning](http://proceedings.mlr.press/v28/sutskever13.pdf On the importance of initialization and momentum in deep)
- [Minima](http://www.bioinf.jku.at/publications/older/3304.pdf Flat)
- [Works](https://distill.pub/2017/momentum/ Why Momentum Really)
- [Minima](https://arxiv.org/pdf/1609.04836.pdf On Large-Batch Training for Deep Learning: Generalization Gap and Sharp)
- [Nets](https://arxiv.org/pdf/1703.04933.pdf Sharp Minima Can Generalize For Deep)
- [DNNs](https://arxiv.org/pdf/1802.10026.pdf Loss Surfaces, Mode Connectivity, and Fast Ensembling of)
- [Learning](https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf The Marginal Value of Adaptive Gradient Methods in Machine)
- [Acceleration](https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration/ Revisiting Nesterov’s)

###### Theory
- Polyak, Boris, and Pavel Shcherbakov. "Why does Monte Carlo fail to work properly in high-dimensional optimization problems?." Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. ([paper)](https://arxiv.org/pdf/1603.00311.pdf)
- Leni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. "Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation." In Signal Image Technology and Internet Based Systems, 2008. SITIS'08. IEEE International Conference on, pp. 344-351. IEEE, 2008. ([paper)](./files/kolmogorov.pdf)
- Klartag, Bo'az. "A central limit theorem for convex sets." Inventiones mathematicae 168, no. 1 (2007): 91-131. ([slides)](https://arxiv.org/pdf/math/0605014.pdf paper), [](http://web.math.unifi.it/users/salani/cortona2007/talks/klartag.pdf)
- Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. "Revisiting unreasonable effectiveness of data in deep learning era." In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017.
 ([blog)](https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)
- Bengio, Yoshua, Aaron Courville, and Pascal Vincent. "Representation learning: A review and new perspectives." IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. ([paper)](http://www.iro.umontreal.ca/~lisa/pointeurs/TPAMISI-2012-04-0260-1.pdf)
- Braun, Jürgen. "An application of Kolmogorov's superposition theorem to function reconstruction in higher dimensions." (2009). ([dissertation)](./files/braun_disser.pdf)
- Kolmogorov. "On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables" ([paper)](./files/Kolmogorov1961.pdf)
- Arnold. "On functions of three variables" ([papers)](./file/Arnold2009Collection.pdf collection of)
- Bianchini, Monica, and Franco Scarselli. "On the complexity of shallow and deep neural network classifiers." In ESANN. 2014.([paper)](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2014-44.pdf)
- Girosi, Federico, and Tomaso Poggio. "Representation properties of networks: Kolmogorov's theorem is irrelevant." Neural Computation 1, no. 4 (1989): 465-469. ([paper)](./files/Girosi89.pdf)
- Kůrková, Věra. "Kolmogorov's theorem and multilayer neural networks." Neural networks 5, no. 3 (1992): 501-506. ([paper)](./files/Kurkova92.pdf)
- Poggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. "Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review." International Journal of Automation and Computing 14, no. 5 (2017): 503-519. ([paper)](https://arxiv.org/pdf/1611.00740.pdf)
- Telgarsky, Matus. "Representation benefits of deep feedforward networks." arXiv preprint arXiv:1509.08101 (2015). ([paper)](https://arxiv.org/abs/1509.08101)
- Montufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. "On the number of linear regions of deep neural networks." In Advances in neural information processing systems, pp. 2924-2932. 2014.  ([paper)](https://arxiv.org/abs/1402.1869)
- Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. "Understanding deep learning requires rethinking generalization." arXiv preprint arXiv:1611.03530 (2016). ([paper)](https://arxiv.org/abs/1611.03530)
- Lin, Henry W., Max Tegmark, and David Rolnick. "Why does deep and cheap learning work so well?." Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. ([paper)](https://arxiv.org/abs/1608.08225)
- Stéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks ([video)](https://youtu.be/0wRItoujFTA)
- [Learning](https://dspace.mit.edu/bitstream/handle/1721.1/107787/CBMM-Memo-066.pdf?sequence##4 Theory of Deep Learning II: Landscape of the Empirical Risk in Deep)
- [addition](./filed/Kolmogorov1957.pdf  On the representation of continuous functions of many variables by superposition of continuous functions of one variable and)
- [Networks](./files/matus.pdf Representation Power of Feed forward Neural)
- [Networks](https://arxiv.org/pdf/1512.03965.pdf The Power of Depth for Feedforward Neural) 


######  Reinforcement Learning
[Optimization](https://arxiv.org/abs/1802.10592 Model-Ensemble Trust-Region Policy)
[Models](https://arxiv.org/abs/1805.12114 Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics)
- [Truth](http://www.argmin.net/2018/02/20/reinforce/ The Policy of)
- [Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work)

###### Section 5: Bayesian DL
- VAE with a VampPrior ([paper)](https://arxiv.org/pdf/1705.07120.pdf)
- Bayesian DL ([blog)](http://www.datasciencecentral.com/profiles/blogs/a-curated-list-of-resources-dedicated-to-bayesian-deep-learning?utm_content##buffer3671b,utm_medium##social,utm_source##twitter.com,utm_campaign##buffer)
- Recognition Networks for Approximate Inference in BN20 Networks ([paper)](https://arxiv.org/pdf/1301.2295.pdf)
- Non-linear regression models for Approximate Bayesian Computation ([paper)](https://arxiv.org/abs/0809.4178)
- DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression ([paper)](https://arxiv.org/abs/1602.04805)
- Fast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation ([paper)](https://arxiv.org/abs/1605.06376)
- Auto-Encoding Variational Bayes ([paper)](https://arxiv.org/abs/1312.6114)
- Composing graphical models with neural networks for structured representations and fast inference ([paper)](https://arxiv.org/abs/1603.06277)
- [Inference](https://arxiv.org/pdf/1802.02538.pdf Yes, but Did It Work?: Evaluating Variational)

######  Practical Tricks
- [Averaging](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)
- - Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks ([paper)](https://arxiv.org/pdf/1603.01431.pdf)
- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ([paper)](https://arxiv.org/pdf/1502.03167.pdf)
- Auto-Encoding Variational Bayes ([paper)](https://arxiv.org/pdf/1312.6114.pdf)
- Twin Networks: Using the Future as a Regularizer ([paper)](https://arxiv.org/pdf/1708.06742.pdf)
-  Don't Decay the Learning Rate, Increase the Batch Size ([paper)](https://arxiv.org/abs/1711.00489)
- DL Tuning ([blog)](http://theorangeduck.com/page/neural-network-not-working)
- [Survey](https://arxiv.org/pdf/1703.09039.pdf Efficient Processing of Deep Neural Networks: A Tutorial and)


#### Other  Resources
###### Additional Reading List
- 50 Years of Data Science by Donoho ([paper)](./files/50YearsofDataScience.pdf)
- [Overview](https://arxiv.org/pdf/1404.7828.pdf Deep Learning in Neural Networks: An)
- [Networks](https://arxiv.org/pdf/1806.11146.pdf Adversarial Reprogramming of Neural)
- [learning](http://unsupervised.cs.princeton.edu/deeplearningtutorial.html Toward theoretical understanding of deep)

###### Blogs
- Papers with code [link](https://paperswithcode.com)
- Security ([blog)](http://www.cleverhans.io)
- Unsupervised learning ([blog)](https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9)
- Cybersecurity ([collection)](https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042 paper)
- [Visualization](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html Opt)

###### Videos
- Deep Energy ([blog)](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)
- DL Summer school 2015 ([videos)](http://videolectures.net/deeplearning2015_montreal/)
- DL Representations ([blog)](http://netdissect.csail.mit.edu)
- PyData 2017 ([videos)](https://www.analyticsvidhya.com/blog/2017/05/pydata-amsterdam-2017-machine-learning-deep-learning-data-science/)

###### Other courses with good web presence
- Stanford's CS231n ([page)](http://cs231n.github.io course)
- Stanford's STATS385 ([page)](https://stats385.github.io course)
- [fast.ai](http://www.fast.ai)
- [learning](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/ Nando de Freitas’ course on machine)
- UC Berkeley Stat241B ([lectures)](https://bcourses.berkeley.edu/courses/1409209/pages/lectures)
- UCUC CSE598 ([page)](http://mjt.web.engr.illinois.edu/courses/mlt-f17/ course)
- [DRL](https://github.com/udacity/deep-reinforcement-learning Udacity)



###### Tools
- [TensorFlow](https://www.tensorflow.org)
- [Keras](https://keras.io)
- [(Google](http://playground.tensorflow.org TF Playground))
- [Sony](https://dl.sony.com DL) 
- [profiler](https://jiffyclub.github.io/snakeviz/ SnakeViz) (python)
- [PyTorch](http://pytorch.org)
- [Examples](https://github.com/jeffalstott/pystan_time_series TS Stan)
- [Glow](https://blog.openai.com/glow/ OpenAI)

###### Misc Links
- [projects](https://github.com/ritchieng/the-incredible-pytorch Pytorch resources) (a curated list of tutorials, papers,)
- [medium?](https://www.christies.com/Features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx Is artificial intelligence set to become art’s next)


#### Previous instances
- [2017](./2017/index.html Fall)
- [2018](./2018/index.html Fall)

#https://www.coursera.org/learn/intro-to-deep-learning
#http://www.machinelearning.ru/wiki/index.php?title##Глубинное_обучение_%28курс_лекций%29/2017
#https://github.com/aosokin/dl_cshse_ami/tree/master/2019-spring
#https://bayesgroup.ru/teaching/
#saddepal@gmu.edu;savala@gmu.edu;fbehnia@gmu.edu;schava@gmu.edu;hgangava@gmu.edu;xgitiaux@gmu.edu;vgunnala@gmu.edu;nhuang2@gmu.edu;wkirsche@gmu.edu;jkrishn@gmu.edu;slakaman@gmu.edu;elynch9@gmu.edu;mmittapa@gmu.edu;gn@gmu.edu;krajend2@gmu.edu;srayapro@gmu.edu;asaid8@gmu.edu;oshaat@gmu.edu;ashambhu@gmu.edu;asothor2@gmu.edu;tstickl@gmu.edu;rzimme10@gmu.edu;aachary@gmu.edu;jbashata@gmu.edu;zchai2@gmu.edu;ychen37@gmu.edu;zfan3@gmu.edu;agharibr@gmu.edu;bghimire@gmu.edu;cgrubb@gmu.edu;zjiang@gmu.edu;nnewman7@gmu.edu;rpandey4@gmu.edu;tsmith58@gmu.edu;jwang40@gmu.edu;fyu2@gmu.edu;dzhang22@gmu.edu
