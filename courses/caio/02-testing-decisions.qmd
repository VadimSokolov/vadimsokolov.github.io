# Statistics

## Binomial Distribution

**Bernoulli Trials:**
A sequence of repeated experiments are Bernoulli trials if:

1. The result of each trial is either a success or failure.
2. The probability $p$ of a success is the same for all trials.
3. The trials are *independent*.

If $X$ is the number of successes it is a **Binomial Random Variable**.

## Binomial Distribution Formula

:::: {.columns}

::: {.column width="49%"}
We calculate probabilities using:
$$P(X=x) = {n \choose x} p^x(1-p)^{n-x}$$
**${n \choose x}$ counts** the number of ways of getting $x$
successes in $n$ trials.

The formula for $n \choose x$ is
$${n \choose x} = \frac{n!}{x!(n-x)!}$$
where $n!=n\times(n-1)\times(n-2)\times...\times 2 \times 1$.
:::

::: {.column width="49%"}
Binomial Mass Function

![Binomial Distribution](fig/binomial-mass.png){fig-align="center"}

In **R:** `dbinom(z, n, p)` and `rbinom(1000, n, p)`
:::

::::

## Binomial Distribution Parameters

The Mean and Variance of the Binomial are:

| Binomial Distribution | Parameters |
|---|---|
| Expected value | $\mu = E(X) = n p$ |
| Variance | $\sigma^2 = Var(X) = n p(1 - p)$ |

## Binomial: Example

Assuming the Joe DiMaggio's batting average is $0.325$ per at-bat and his hits are independent. What is the probability of getting more than $2$ hits in $4$ at-bats.

\begin{align*} 
P(\text{hits} > 2) &=& P(\text{hits} = 3) + P(\text{hits} = 4) \\
&=& \binom{4}{3}p^3(1-p) + \binom{4}{4}p^4 \\
&=& 10.4\%
\end{align*}

## Bernoulli Process

Let $X$ represent either *success* ($X=1$)
or *failure* ($X=0$)

- A **Bernoulli process** is a binary outcome with 
  $$P(X=1) =p \quad \text{and} \quad P(X=0) = 1-p$$
- We assume that trials are **independent**.
  The probability of two successes in a row is
  \begin{align*}
  P(X_1 = 1, X_2 = 1) &=& P(X_1 = 1) P(X_2 = 1) \\ 
     &=& p \cdot p \\
     &=& p^2
  \end{align*}

**Binomial distribution** counts the **successes** of a *Bernoulli process*.

## EPL Odds

![EPL Odds](figbs/epl-odds.png){fig-align="center"}

## EPL 2017 Data

We have a historical set of data on scores

| home team | results | | visit team |
|---|---|---|---|
| Chelsea | $2$ | $1$ | West Ham | 
| Chelsea | $5$ | $1$ | Sunderland | 
| Watford | $1$ | $2$ | Chelsea | 
| Chelsea | $3$ | $0$ | Burnley | 
| | $\dots$ | | |

Tomorrow Manchester United (MU) is playing Hall U. I want to place a bet. 

How can I predict the outcome of this game?

## Poisson Distribution

The **Poisson distribution** counts the occurrence of events
Given the rate $\lambda$ we calculate probabilities as follows

$$P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!} \quad \text{where } x=0,1,2,3, \ldots$$

The Poisson Mean and Variance are: 

| Poisson Distribution | Parameters |
|---|---|
| Expected value | $\mu = E(X) = \lambda$ |
| Variance | $\sigma^2 = Var(X) = \lambda$ |

$\lambda$ is the rate of occurrence of an event.

## EPL MU

I can build a model assuming goals follow Poisson distribution. 

I calculate $\lambda$ by taking an average.

:::: {.columns}

::: {.column width="50%"}
![MU against](fig/mu-against.png)
(a) MU against
:::

::: {.column width="50%"}
![MU for](fig/mu-for.png)
(b) MU for
:::

::::

*Our Poisson model fits the empirical data!!*

## English Premier League: EPL

Calculate **Odds for the possible scores in a match?**

$$0-0, \; 1-0, \; 0-1, \; 1-1, \; 2-0, \ldots$$

Let 

$X=$ Goals scored by Hall U 

$Y=$ Goals scored by MU

What's the odds of a **MU winning?** $P(X < Y)$
Odds of a **draw?** $P(X = Y)$

```r
x = rpois(100,0.6)
y = rpois(100,1.4)
sum(x<y)/100   # Team 2 wins
sum(x==y)/100   # Draw
```

## EPL: Attack and Defence Strength

Each team gets an "attack" strength and "defence" weakness rating
Adjust home and away average goal estimates

![EPL Table](figbs/epl-table.jpg){width="80%" fig-align="center"}

## EPL: Hull vs ManU

**ManU**
Average away goals $=1.47$. 
Prediction: $1.47 \times 1.46 \times 1.37 = 2.95$

Average $\times$ Attack strength $\times$ Defense weakness

**Hull**
Average home goals $=1.47$.
Prediction: $1.47 \times 0.85 \times 0.52 = 0.65$.

Simulation

| Team | Expected Goals | 0 | 1 | 2 | 3 | 4 | 5 |
|---|---|---|---|---|---|---|---|
| Man U | 2.95 | 7 | 22 | 26 | 12 | 11 | 13 |
| Hull City | 0.65 | 49 | 41 | 10 | 0 | 0 | 0 |

## EPL Predictions

A model is only **as good as its predictions**

- In our simulation Man U wins 88 games out of 100, we should bet when odds ratio is below 88 to 100. 
- Most likely outcome is 0-3 (12 games out of 100)
- The actual outcome was 0-1 (they played on August 27, 2016)
- In out simulation 0-1 was the fourth most probable outcome (9 games out of 100)

## Continuous Random Variables

Suppose we are trying to predict tomorrow's return on the S&P500...

There's a number of questions that come to mind

- What is the random variable of interest?
- How can we describe our uncertainty about
  tomorrow's outcome?
- Instead of listing all possible values we'll work
  with intervals instead.
  The probability of an interval is defined by the area under the
  probability density function.

They are **continuous** (as opposed to discrete) random variables

## Normal Distribution

$Z$ is a **standard normal** random variable

- The standard Normal has mean $0$ and has a variance $1$
  $$Z \sim N(0,1)$$
- We have the probability statements
  \begin{align*}
  P(-1 <Z< 1) &=0.68\\
  P(-1.96 <Z< 1.96) &=0.95
  \end{align*}

`qnorm` and `pnorm`
We can simulate $1000$ draws using `rnorm(1000,0,1)`

## Normal Distribution - Mean Parameter

By changing the mean parameter $\mu$, we change the center of the bell curve

![Normal Distribution - Mean](figbs/normal-mean.png){fig-align="center"}

## Normal Distribution - Variance Parameter

By changing the variance parameter $\sigma^2$, we change the "fatness" of the bell curve

![Normal Distribution - Variance](figbs/normal-variance.png){fig-align="center"}

## Normal Distribution - Real Data

Chicago Wind Speed (2007-2014) data on a log scale seem to be well described by the Normal distribution

![Normal Distribution - Wind Data](figbs/normal-wind.png){width="90%" fig-align="center"}

## `pnorm` and `qnorm`

We can find probabilities and quintiles in **R**. Here are the important values

```r
>pnorm(2.58)
[1] 0.9950
>pnorm(1.96)
[1] 0.9750
>pnorm(1.64)
[1] 0.9499
```

`qnorm` is the inverse of `pnorm`.
Simulation `rnorm`. 
`N=1000, x=rnorm(N,0,1), p=sum(x<1.96)/N`

## Normal Distribution with General Mean and Variance

Here are two useful facts: If $X \sim \text{N}(\mu, \sigma^2)$, then

\begin{align*}
P(\mu - 2.58 \sigma < X < \mu + 2.58 \sigma) =& 0.99 \\
P(\mu - 1.96 \sigma < X < \mu + 1.96 \sigma) =& 0.95
\end{align*}

- The chance that $X$ will be within $2.58 \sigma$ of its mean is $99$%, and the chance that it will be within $2\sigma$ of its mean is about $95$%.

## The Normal Distribution - Standardization

Our probability model is written $X \sim N(\mu,\sigma^2)$
$\mu$ is the mean, $\sigma^2$ is the variance

- **Standardization** if $X \sim N(\mu,\sigma^2)$ then
  $$Z =\frac{X-\mu}{\sigma} \sim N(0,1)$$
- $\mu:$ the center of the distribution
  $\sigma:$ how spread out the data are

$95$% probability $X$ is inside $\mu \pm 1.96 \sigma$.

## Pictorially

![Normal Distribution Tails](figbs/normal-tail.png){width="90%" fig-align="center"}

Examples of upper and lower tail areas. The lower tail area of $0.1$ is at $z = -1.28$. The upper tail area of $0.05$ is at $z=1.64$

## Example: The Crash

**How extreme was the 1987 crash of $-21.76$%?**

1. Prior to the October, 1987 crash
   SP500 monthly returns were $1.2$% with a risk/volatility of $4.3$%
   $$X \sim N(0.012, 0.043^2)$$
   **Standardize:**
   $$Z =\frac{X-\mu}{\sigma} = \frac{X - 0.012}{0.043} \sim N(0,1)$$

2. Calculate the observed $Z$: 
   $$Z = \frac{-0.2176 - 0.012}{0.043} = -5.27$$
   That's a **$5$-sigma event!**

## Example: The Crash - Model Validation

We assumed returns follow normal distribution. Using an inaccurate model can lead to inaccurate results.

![S&P 500 Returns vs Normal](figbs/normal-sp500.png){fig-align="center"}

## Sensitivity and Specificity

**Two errors:** 
An infected person may test negative, a well person tests positive. 

**Sensitivity** (or power) $=$ true positive rate (or recall)
% sick people who are correctly identified $P(T \mid D)$. 

In a perfect world, we'd like $P(\bar{T} \mid D) \approx 0$

**Specificity** $=$ true negative rate 
% of negatives correctly identified as such $P(\bar{T} \mid \bar{D})$.

- False negative $=$ 1 - sensitivity, $\beta$ where $\beta$ is the type I error
- False positive rate $=$ 1 $-$ specificity, $\alpha =$ type II error 

We want the probability: $P(D \mid T)$

## Confusion Matrix

We can use accuracy rate:
$$
\text{accuracy} = \frac{\text{Number of Correct answers}}{n}
$$

or its dual, error rate
$$
\text{error rate} = 1 - \text{accuracy}
$$

You remember, we have two types of errors. We can use confusion matrix to quantify those

|   | Predicted: YES | Predicted: NO |
|---|---|---|
| **Actual: YES** | TPR | FNR |
| **Actual: NO** | FPR | TNR |

True positive rate (TPR) is the sensitivity and false positive rate (FPR) is the specificity of our predictive model