[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Full CV"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vadim Sokolov",
    "section": "Education",
    "text": "Education\n\nPh.D., 2008, Computational Mathematics, Northern Illinois University\nDiploma (summa cum laude), 2004, Applied Mathematics, Rostov State University\nGraduate Studies in Statistics, 2013-2014, University of Chicago"
  },
  {
    "objectID": "cv.html#positions",
    "href": "cv.html#positions",
    "title": "Vadim Sokolov",
    "section": "Positions",
    "text": "Positions\n\nAssociate Professor, Systems Engineering and Operations Research, George Mason University, 2023-present\nAsistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nVisiting Assistant Professor, Statistics,The University of Chicago Booth School of Business, 2019-2023\nAssistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nLecturer, Master of Science in Analytics, University of Chicago, 2015-2016\nFellow, Computation Institute, University of Chicago, 2014-2016\nPrincipal Computational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2013-2016\nComputational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2008-2013"
  },
  {
    "objectID": "courses/568.html",
    "href": "courses/568.html",
    "title": "SYST/OR 568. Applied Predictive Analytics - Mason Analytics MS",
    "section": "",
    "text": "George Mason University\nFall 2023\n\nCourse Material\nVideo Lectures\nTA: Raina Joy Saha (rsaha3 (at) gmu.edu)\nInstructor: Vadim Sokolov\n\nLocation: Krug Hall 7\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\nList of Topics\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesian Inference\nTime series forecasting (notes)\n\n\n\nSchedule\n\nAug 21: First Class\nSept 4: Labor Day : University Closed\nSep 11: HW 1 Due\nSep 25: HW 2 Due\nOct 9: Fall Break (Classes Do Not Meet)\nOct 10: We have a class\nOct 16: HW 3 Due\nOct 23: In-class Midterm\nOct 30: Final Project Proposal\nNov 6: Hw 4 Due\nNov 20: Hw 5 Due\nNov 27: Last class, project presentations\nDec 3: Final Projects Due\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nOptional Textbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/old/610-f20.html",
    "href": "courses/old/610-f20.html",
    "title": "Courses",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2020\n\\"
  },
  {
    "objectID": "courses/old/610-f20.html#or-610.-deep-learning",
    "href": "courses/old/610-f20.html#or-610.-deep-learning",
    "title": "Courses",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2020\n\\"
  },
  {
    "objectID": "courses/old/750-f17.html",
    "href": "courses/old/750-f17.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2017\n\n#For Syllabus click here\n\nThe class will consist of 7-8 lectures given by the instructor on several advanced topics in data analysis. The rest of the semester (another 6-7 lectures) students will present on the topic of their choice.\nFor Lecture notes and homework click here\n\n\n\n\nBayesian Optimization for ML and Engineering. By Laura Schultz (11/1)\nAdversarial Auto-encoders. By Bill Jeffries on 11/8\nRNN for EDU: (11/8. By Qian Hu)\nRNN/LSTM. By Darron Fuller.\nHierarchical Bayes and STAN. By Tuan Lee (11/15)\nConvolutional Networks: Convolution/pooling layers, network design, theory. By Kathleen Perez-Lopez on 11/15. Materials: slides, , \nDL for Threat Detection. By James Lee. 11/29\nVAE: VampPrior. By Hossein Fotouhi on 11/29.\nLanguage Modeling: Documents, , . By Anya Mityushina on 11/29\nReinforcement learning. By Jeff Schneider\n\n\n\n\n\n10/24: No lecture this Wednesday (10/25)\n10/24: HW3 Posted\n10/03/2017: Instructions on setting up Google Cloud Machines posted here\n9/24/2017: HW2 posted\n9/24/2017: Solutions for HW1 posted\n9/7/2017: HW1 posted\n5/3/2017: First class is on Aug 30 at 4:30pm\n\n\n\n\n\nProbabilistic models for Machine Learning – Conjugate distributions, exponential family – Model choice – Hierarchical linear and generalize linear models (regression and classification) – Models for missing data (EM-algorithm) – LDA, Normal mixtures, Bayes PCA – Bayes computations (MCMC, Variational Bayes) – Graphical Models – Probabilistic modeling with Stan\nDeep learning – Optimization – Architectures (CNN, LSTM, MP, VAE) – Bayesian DL – Generative models (GANs) – Modeling with TensorFlow\nFiltering – Kalman Filter – Extended KF and ensemble KF – Particle filter – Modeling with DLM package in R\nProbability and Statistics methods for Decision Making – Real-time hypothesis testing – Brownian motion – Bayesian methods for optimal stopping time detection\nBayesian Optimization – Tuning machine learning algorithms – Engineering model calibration – Modeling with spearmint\n\n\n\n\n#*Lecture Notes: Will be made available one-day in advance on Bb\nInstructor: Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel**: 703-993-4533\n\n\n\n\n\nThe Probability and Statistics Cookbook (page)\nPattern Recognition and Machine Learning (page)\nLectures on “Probability in High Dimension” (pdf)\nBook on “High-Dimensional Probability” (pdf)\nBlog post on SGD (link)\nMachine Learning: A Bayesian and Optimization Perspective(safari)\nA Bayesian Course with Examples in R and Stan (page)\nDoing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (amazon)\n\n\n\n\nVadim Sokolov: By appointment (at Engineering 2242)\n\n\n\n\nLocation: Building 1109\nTimes: 4:30-7:10pm on Wednesday\n\n\n\nGrade composition: No in-class examination. Grade is based entirely on participation in class and homework assignments.\n\n\n\n##PAPERS## - Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper) - Why does Monte Carlo Fail to Work Properly in High-Dimensional Optimization Problems? (paper) - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper) - Auto-Encoding Variational Bayes (paper) - A Central Limit Theorem for Convex Sets (slides),  - Learning Deep Architectures for AI (monograph) - Representation Learning: A Review and New Perspectives (paper) - Sequence to Sequence Learning with Neural Networks (paper) - Twin Networks: Using the Future as a Regularizer (paper) - Skip RNN (paper) - VAE with a VampPrior (paper)\n##BLOGS## - Bayesian DL (blog) - Generative Adversarial Networks (presentation) - GANs at OpenAI (blog) - Revisiting the Unreasonable Effectiveness of Data (blog) - Learning the Enigma with Recurrent Neural Networks (blog) - Tuning CNN architecture (blog) - Security (blog) - Unsupervised learning (blog) - DL Tuning (blog) - Cybersecurity (collection) - Visualization - Momentum - LSTM blog ##VIDEOS## - Deep Energy (blog) - DL Summer school 2015 (videos) - DL Representations (blog) - PyData 2017 (videos)\n\n\n\n\nMultivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTM-based Predictive Data Model (paper)\nSignal-based Bayesian Seismic Monitoring (paper)\nIntroducing practical and robust anomaly detection in a time series (blog)\nAnomaly Detection with Twitter in R (blog)\n\n\n\n\n\nCurse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems (paper)(\nCan local particle filters beat the curse of dimensionality? (paper)\n\n\n\n\n\nThe Markov Chain Monte Carlo Revolution (paper)\nGraphical Models, Exponential Families, and Variational Inference (monograph)\nVariational Inference: A Review for Statisticians (paper)\nNeural Block Sampling (paper)\nStochastic Variational Inference (paper)\nA stochastic approximation method (paper)\nA note on the evidence and Bayesian Occam’s razor (paper)\nHow to become a Bayesian in eight easy steps: An annotated reading list (paper)\nSparse Bayesian Learning and the Relevance Vector Machine ()\nEM Algorithm (blog)\nBayes DL (blog)\nAB Testing (blog)\n\n\n\n\n\nLearning Transferable Architectures for Scalable Image Recognition (paper)\nHyperband (demo)\nCalibration at Google (blog)\n\n\n\n\n\nStan\nTensorFlow\n(R)\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nprojects (a curated list of tutorials, papers,)\n(Facebook)\n(Twitter)\nExamples\n\n\n\n\n\nBayesian Chronological Modeling Workshop site #### Misc Links\nlearning"
  },
  {
    "objectID": "courses/old/750-f17.html#or-750.-advanced-data-analytics",
    "href": "courses/old/750-f17.html#or-750.-advanced-data-analytics",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2017\n\n#For Syllabus click here\n\nThe class will consist of 7-8 lectures given by the instructor on several advanced topics in data analysis. The rest of the semester (another 6-7 lectures) students will present on the topic of their choice.\nFor Lecture notes and homework click here\n\n\n\n\nBayesian Optimization for ML and Engineering. By Laura Schultz (11/1)\nAdversarial Auto-encoders. By Bill Jeffries on 11/8\nRNN for EDU: (11/8. By Qian Hu)\nRNN/LSTM. By Darron Fuller.\nHierarchical Bayes and STAN. By Tuan Lee (11/15)\nConvolutional Networks: Convolution/pooling layers, network design, theory. By Kathleen Perez-Lopez on 11/15. Materials: slides, , \nDL for Threat Detection. By James Lee. 11/29\nVAE: VampPrior. By Hossein Fotouhi on 11/29.\nLanguage Modeling: Documents, , . By Anya Mityushina on 11/29\nReinforcement learning. By Jeff Schneider\n\n\n\n\n\n10/24: No lecture this Wednesday (10/25)\n10/24: HW3 Posted\n10/03/2017: Instructions on setting up Google Cloud Machines posted here\n9/24/2017: HW2 posted\n9/24/2017: Solutions for HW1 posted\n9/7/2017: HW1 posted\n5/3/2017: First class is on Aug 30 at 4:30pm\n\n\n\n\n\nProbabilistic models for Machine Learning – Conjugate distributions, exponential family – Model choice – Hierarchical linear and generalize linear models (regression and classification) – Models for missing data (EM-algorithm) – LDA, Normal mixtures, Bayes PCA – Bayes computations (MCMC, Variational Bayes) – Graphical Models – Probabilistic modeling with Stan\nDeep learning – Optimization – Architectures (CNN, LSTM, MP, VAE) – Bayesian DL – Generative models (GANs) – Modeling with TensorFlow\nFiltering – Kalman Filter – Extended KF and ensemble KF – Particle filter – Modeling with DLM package in R\nProbability and Statistics methods for Decision Making – Real-time hypothesis testing – Brownian motion – Bayesian methods for optimal stopping time detection\nBayesian Optimization – Tuning machine learning algorithms – Engineering model calibration – Modeling with spearmint\n\n\n\n\n#*Lecture Notes: Will be made available one-day in advance on Bb\nInstructor: Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel**: 703-993-4533\n\n\n\n\n\nThe Probability and Statistics Cookbook (page)\nPattern Recognition and Machine Learning (page)\nLectures on “Probability in High Dimension” (pdf)\nBook on “High-Dimensional Probability” (pdf)\nBlog post on SGD (link)\nMachine Learning: A Bayesian and Optimization Perspective(safari)\nA Bayesian Course with Examples in R and Stan (page)\nDoing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan (amazon)\n\n\n\n\nVadim Sokolov: By appointment (at Engineering 2242)\n\n\n\n\nLocation: Building 1109\nTimes: 4:30-7:10pm on Wednesday\n\n\n\nGrade composition: No in-class examination. Grade is based entirely on participation in class and homework assignments.\n\n\n\n##PAPERS## - Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper) - Why does Monte Carlo Fail to Work Properly in High-Dimensional Optimization Problems? (paper) - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper) - Auto-Encoding Variational Bayes (paper) - A Central Limit Theorem for Convex Sets (slides),  - Learning Deep Architectures for AI (monograph) - Representation Learning: A Review and New Perspectives (paper) - Sequence to Sequence Learning with Neural Networks (paper) - Twin Networks: Using the Future as a Regularizer (paper) - Skip RNN (paper) - VAE with a VampPrior (paper)\n##BLOGS## - Bayesian DL (blog) - Generative Adversarial Networks (presentation) - GANs at OpenAI (blog) - Revisiting the Unreasonable Effectiveness of Data (blog) - Learning the Enigma with Recurrent Neural Networks (blog) - Tuning CNN architecture (blog) - Security (blog) - Unsupervised learning (blog) - DL Tuning (blog) - Cybersecurity (collection) - Visualization - Momentum - LSTM blog ##VIDEOS## - Deep Energy (blog) - DL Summer school 2015 (videos) - DL Representations (blog) - PyData 2017 (videos)\n\n\n\n\nMultivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTM-based Predictive Data Model (paper)\nSignal-based Bayesian Seismic Monitoring (paper)\nIntroducing practical and robust anomaly detection in a time series (blog)\nAnomaly Detection with Twitter in R (blog)\n\n\n\n\n\nCurse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems (paper)(\nCan local particle filters beat the curse of dimensionality? (paper)\n\n\n\n\n\nThe Markov Chain Monte Carlo Revolution (paper)\nGraphical Models, Exponential Families, and Variational Inference (monograph)\nVariational Inference: A Review for Statisticians (paper)\nNeural Block Sampling (paper)\nStochastic Variational Inference (paper)\nA stochastic approximation method (paper)\nA note on the evidence and Bayesian Occam’s razor (paper)\nHow to become a Bayesian in eight easy steps: An annotated reading list (paper)\nSparse Bayesian Learning and the Relevance Vector Machine ()\nEM Algorithm (blog)\nBayes DL (blog)\nAB Testing (blog)\n\n\n\n\n\nLearning Transferable Architectures for Scalable Image Recognition (paper)\nHyperband (demo)\nCalibration at Google (blog)\n\n\n\n\n\nStan\nTensorFlow\n(R)\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nprojects (a curated list of tutorials, papers,)\n(Facebook)\n(Twitter)\nExamples\n\n\n\n\n\nBayesian Chronological Modeling Workshop site #### Misc Links\nlearning"
  },
  {
    "objectID": "courses/old/568-f16.html",
    "href": "courses/old/568-f16.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2016\n\nIntroduces predictive analytics with applications in engineering, business, finance, health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\n\n1/11/2016 Regularizarion notes posted\n1/11/2016 Syllabus updated: Optimization @ week 10, SVM @ week 12\n1/11/2016 Convex optimization notes posted\n24/10/2016 Tree based methods notes posted\n12/10/2016 Classification notes posted\n14/9/2016 Model selection notes posted\n14/9/2016 Model selection notes posted\n9/9/2016 Notebooks for ISLR Book labs posted at the bottom of this page\n9/9/2016 Take home midterm is tentatively on week 8, October 20\n9/9/2016 Stats, EDA and Regrssion notes posted.\n9/9/2016 HW3 and HW4 posted. HW3 due 9/23.\n9/9/2016: Some sources for datasets for final projects posted at the bottom, courtesy of Jamie Wheeler\n9/1/2016: When you prefeer to have office hours? Respond by noon 9/2: poll\n9/1/2016: Kuhn-Johnson book posted to Bb\n8/31/2016: HW1 and HW2 posted\n8/19/\\2016: First set of lecture notes posted\n8/17/\\2016: Room was changed to Planet 124\n7/27/2016: Check back regularly for announcements.\n\n\n\n\n*Syllabus\nLecture Notes: Will be made available one-week in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Bahman Pedrood (bpedrood (at) gmu.edu)\nPiazza**: Signup) (\n\n\n\n\nBootstrap\n\n\n\n\nBahman Pedrood: Tuesday 5-7pm (at Engineering 2241)\nVadim Sokolov: Thursday 5-7pm (at Engineering 2242)\n\n#PDF file can be downloaded from here\n\n\n\nLocation: Hall 124\nTimes: 7:20-10pm on Thursday #### Grades Grade composition: No in-class examination. Grade based entirely on participation in class, homework assigments, take-home midterm and final project.\n\n\n\n\nKuhn and Johnson, Modeling, Springer, 2013.\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\n\n\n\n\n\nDebby Kermer (data services): info\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\n\nIntroduction\nRegression\nClasification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll"
  },
  {
    "objectID": "courses/old/568-f16.html#systor-568.-applied-predictive-analytics",
    "href": "courses/old/568-f16.html#systor-568.-applied-predictive-analytics",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2016\n\nIntroduces predictive analytics with applications in engineering, business, finance, health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\n\n1/11/2016 Regularizarion notes posted\n1/11/2016 Syllabus updated: Optimization @ week 10, SVM @ week 12\n1/11/2016 Convex optimization notes posted\n24/10/2016 Tree based methods notes posted\n12/10/2016 Classification notes posted\n14/9/2016 Model selection notes posted\n14/9/2016 Model selection notes posted\n9/9/2016 Notebooks for ISLR Book labs posted at the bottom of this page\n9/9/2016 Take home midterm is tentatively on week 8, October 20\n9/9/2016 Stats, EDA and Regrssion notes posted.\n9/9/2016 HW3 and HW4 posted. HW3 due 9/23.\n9/9/2016: Some sources for datasets for final projects posted at the bottom, courtesy of Jamie Wheeler\n9/1/2016: When you prefeer to have office hours? Respond by noon 9/2: poll\n9/1/2016: Kuhn-Johnson book posted to Bb\n8/31/2016: HW1 and HW2 posted\n8/19/\\2016: First set of lecture notes posted\n8/17/\\2016: Room was changed to Planet 124\n7/27/2016: Check back regularly for announcements.\n\n\n\n\n*Syllabus\nLecture Notes: Will be made available one-week in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Bahman Pedrood (bpedrood (at) gmu.edu)\nPiazza**: Signup) (\n\n\n\n\nBootstrap\n\n\n\n\nBahman Pedrood: Tuesday 5-7pm (at Engineering 2241)\nVadim Sokolov: Thursday 5-7pm (at Engineering 2242)\n\n#PDF file can be downloaded from here\n\n\n\nLocation: Hall 124\nTimes: 7:20-10pm on Thursday #### Grades Grade composition: No in-class examination. Grade based entirely on participation in class, homework assigments, take-home midterm and final project.\n\n\n\n\nKuhn and Johnson, Modeling, Springer, 2013.\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\n\n\n\n\n\nDebby Kermer (data services): info\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\n\nIntroduction\nRegression\nClasification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll"
  },
  {
    "objectID": "courses/old/750-f18.html",
    "href": "courses/old/750-f18.html",
    "title": "jemdoc: nodefaultcss, addcss{../../../jemdoc.css}",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2018\n\n\\ This is a PhD course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 12 lectures given by the instructor on several advanced topics in deep learning. At another 3 lectures students will present on the topic of their choice.\n\n\n\n10/18/2018: HW2 posted, Due Oct 23\n10/04/2018: No class next week (Oct 8) due to Fall break\n9/20/2018: Week 3-5 notes Posted\n9/15/2018: Week 2 notes Posted\n9/15/2018: HW1 Posted\n8/31/2018: Room has been changed to Engineering 2241 (Fusion Lab)\n8/31/2018: First week’s lecture notes are posted\n8/31/2018: No Class on September 3\n4/20/2018: First class is on Aug 27 at 4:30pm\n\n\n\n\nMatrix - Dec 3: – slides) by McKennon-Kelly Ryan ( – learning by Dave Prentiss – Residuals by Jing Liu – Recognition by Jing Liu – Approach by Wenjie Li – (Lem by Liming) Zhang\n\nNov 26:\n– Segmentation by Steven Guan – Networks by Steven Guan – slides) by Brett Patterson ( – slides) by Brett Patterson (\nNov 19: – (Lem by Liming) Zhang –  by Steven Guan – Screenshot by Nima Latifi – graph by Wenjie Li – Prediction by Wenjie Li – Classification by Weiwen Zhou – System by Atilla Ay – problems? by McKennon-Kelly Ryan – slides) by McKennon-Kelly Ryan (\nNov 12: – slides) by Ashton Harvey ( –  by Brett Patterson – Forecasting by Brett Patterson – Networks by Jing Liu – (Lem by Liming) Zhang – slides) by Dave Prentiss ( – Networks by Yudi Chen\n\n\n\n\nCourse Materials: folder\nInstructor: (vsokolov(at)gmu.edu)\nOffice: Building, Room 2242\nTel: 703 993-4533\nOffice hours: By appointment\nLectures: Lab 2241 (Fusion). 4:30-7:10pm on Mondays\nGrades: 40% homework (individual), 60% final research project (group)\n\n\n\n\n\nConvex Optimization – Stochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration) – Second order methods – ADMM – Regularization (l1, l2 and dropout) – Batch normalization\nTheory of deep learning – Universal approximators – Curse of dimensionality – Kernel spaces – Topology and geometry\nComputational aspects (accelerated linear algebra, reduced precision calculations, parallelism)\nArchitectures (CNN, LSTM, MLP, VAE)\nBayesian DL\nDeep reinforcement learning\nHyperparameter selection and parameter initialization\nGenerative models (GANs)\n\n\n\n\n\nDeep Learning (page)\nDeep Learning with Python (page)\nLearning Deep Architectures for AI (monograph)\n\n\n\n\n\n\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\n\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\n\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\n\nOptimization Models - Truth - Yet\n\n\n\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\n\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\n\n\n\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\n\n\nPapers with code link\nSecurity (blog)\nUnsupervised learning (blog)\nCybersecurity (collection)\nVisualization\n\n\n\n\n\nDeep Energy (blog)\nDL Summer school 2015 (videos)\nDL Representations (blog)\nPyData 2017 (videos)\n\n\n\n\n\nStanford’s CS231n (page)\nStanford’s STATS385 (page)\nfast.ai\nlearning\nUC Berkeley Stat241B (lectures)\nUCUC CSE598 (page)\nDRL\n\n\n\n\n\nTensorFlow\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nExamples\nGlow\n\n\n\n\n\nprojects (a curated list of tutorials, papers,)\nmedium?\n\n\n\n\n\n\n2017"
  },
  {
    "objectID": "courses/old/750-f18.html#or-750.-deep-learning",
    "href": "courses/old/750-f18.html#or-750.-deep-learning",
    "title": "jemdoc: nodefaultcss, addcss{../../../jemdoc.css}",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2018\n\n\\ This is a PhD course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 12 lectures given by the instructor on several advanced topics in deep learning. At another 3 lectures students will present on the topic of their choice.\n\n\n\n10/18/2018: HW2 posted, Due Oct 23\n10/04/2018: No class next week (Oct 8) due to Fall break\n9/20/2018: Week 3-5 notes Posted\n9/15/2018: Week 2 notes Posted\n9/15/2018: HW1 Posted\n8/31/2018: Room has been changed to Engineering 2241 (Fusion Lab)\n8/31/2018: First week’s lecture notes are posted\n8/31/2018: No Class on September 3\n4/20/2018: First class is on Aug 27 at 4:30pm\n\n\n\n\nMatrix - Dec 3: – slides) by McKennon-Kelly Ryan ( – learning by Dave Prentiss – Residuals by Jing Liu – Recognition by Jing Liu – Approach by Wenjie Li – (Lem by Liming) Zhang\n\nNov 26:\n– Segmentation by Steven Guan – Networks by Steven Guan – slides) by Brett Patterson ( – slides) by Brett Patterson (\nNov 19: – (Lem by Liming) Zhang –  by Steven Guan – Screenshot by Nima Latifi – graph by Wenjie Li – Prediction by Wenjie Li – Classification by Weiwen Zhou – System by Atilla Ay – problems? by McKennon-Kelly Ryan – slides) by McKennon-Kelly Ryan (\nNov 12: – slides) by Ashton Harvey ( –  by Brett Patterson – Forecasting by Brett Patterson – Networks by Jing Liu – (Lem by Liming) Zhang – slides) by Dave Prentiss ( – Networks by Yudi Chen\n\n\n\n\nCourse Materials: folder\nInstructor: (vsokolov(at)gmu.edu)\nOffice: Building, Room 2242\nTel: 703 993-4533\nOffice hours: By appointment\nLectures: Lab 2241 (Fusion). 4:30-7:10pm on Mondays\nGrades: 40% homework (individual), 60% final research project (group)\n\n\n\n\n\nConvex Optimization – Stochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration) – Second order methods – ADMM – Regularization (l1, l2 and dropout) – Batch normalization\nTheory of deep learning – Universal approximators – Curse of dimensionality – Kernel spaces – Topology and geometry\nComputational aspects (accelerated linear algebra, reduced precision calculations, parallelism)\nArchitectures (CNN, LSTM, MLP, VAE)\nBayesian DL\nDeep reinforcement learning\nHyperparameter selection and parameter initialization\nGenerative models (GANs)\n\n\n\n\n\nDeep Learning (page)\nDeep Learning with Python (page)\nLearning Deep Architectures for AI (monograph)\n\n\n\n\n\n\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\n\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\n\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\n\nOptimization Models - Truth - Yet\n\n\n\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\n\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\n\n\n\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\n\n\nPapers with code link\nSecurity (blog)\nUnsupervised learning (blog)\nCybersecurity (collection)\nVisualization\n\n\n\n\n\nDeep Energy (blog)\nDL Summer school 2015 (videos)\nDL Representations (blog)\nPyData 2017 (videos)\n\n\n\n\n\nStanford’s CS231n (page)\nStanford’s STATS385 (page)\nfast.ai\nlearning\nUC Berkeley Stat241B (lectures)\nUCUC CSE598 (page)\nDRL\n\n\n\n\n\nTensorFlow\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nExamples\nGlow\n\n\n\n\n\nprojects (a curated list of tutorials, papers,)\nmedium?\n\n\n\n\n\n\n2017"
  },
  {
    "objectID": "courses/old/568-f22.html",
    "href": "courses/old/568-f22.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "George Mason University\nFall 2022\\\nMaterial\\ Lectures\\\nTA: Sahar Jolini (sjolini(at)gmu.edu)\nInstructor: (vsokolov(at)gmu.edu)\n\\\nLocation: Planetary Hall 206\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\\\n\n\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized. \\\n\n\n\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12, Tennis)\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesain Inference\nTime series forecasting (FPP)\n\n\n\n\n\nAug 29: No class\nSept 5: Labor Day : University Closed\nSep 12: HW 1 Due\nSep 26: HW 2 Due\nOct 10: HW 3 Due\nOct 10: Fall Break (Classes Do Not Meet)\nOct 17: In-class Midterm\nOct 24: Final Project Proposal\nOct 31: Hw 4 Due\nNov 14: Hw 5 Due\nNov 28: Last class, project presentations\nDec 5: Final Projects Due\n\n\n\n\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions'' and 2-3hands-on’’ problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\n\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\n\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\\\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\n\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\n\n\n2016\n2017\n2020\n2020\n2021"
  },
  {
    "objectID": "courses/old/568-f22.html#systor-568.-applied-predictive-analytics---mason-analytics-ms",
    "href": "courses/old/568-f22.html#systor-568.-applied-predictive-analytics---mason-analytics-ms",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "George Mason University\nFall 2022\\\nMaterial\\ Lectures\\\nTA: Sahar Jolini (sjolini(at)gmu.edu)\nInstructor: (vsokolov(at)gmu.edu)\n\\\nLocation: Planetary Hall 206\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\\\n\n\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized. \\\n\n\n\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12, Tennis)\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesain Inference\nTime series forecasting (FPP)\n\n\n\n\n\nAug 29: No class\nSept 5: Labor Day : University Closed\nSep 12: HW 1 Due\nSep 26: HW 2 Due\nOct 10: HW 3 Due\nOct 10: Fall Break (Classes Do Not Meet)\nOct 17: In-class Midterm\nOct 24: Final Project Proposal\nOct 31: Hw 4 Due\nNov 14: Hw 5 Due\nNov 28: Last class, project presentations\nDec 5: Final Projects Due\n\n\n\n\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions'' and 2-3hands-on’’ problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\n\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\n\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\\\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\n\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\n\n\n2016\n2017\n2020\n2020\n2021"
  },
  {
    "objectID": "courses/664.html",
    "href": "courses/664.html",
    "title": "SYST/OR 664 CSI 674. Bayesian Inference and Decision Theory",
    "section": "",
    "text": "Spring 2024\n\nTA: TBD\nInstructor: Vadim Sokolov\n\nLocation: Hall 120\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nBayesian decision theory provides a unified and intuitively appealing approach to drawing inferences from observations and making rational, informed decisions. Bayesians view statistical inference as a problem in belief dynamics, of using evidence about a phenomenon to revise and update knowledge about it. Bayesian statistics is a scientifically justifiable way to integrate informed expert judgment with empirical data. For a Bayesian, statistical inference cannot be treated entirely independently of the context of the decisions that will be made on the basis of the inferences. In recent years, Bayesian methods have become increasingly common in a variety of disciplines that rely heavily on data. This course introduces students to Bayesian theory and methodology, including modern computational methods for Bayesian inference. Students will learn the commonalities and differences between the Bayesian and frequentist approaches to statistical inference, how to approach a statistics problem from the Bayesian perspective, and how to combine data with informed expert judgment in a sound way to derive useful and policy-relevant conclusions. Students will learn the necessary theory to develop a firm understanding of when and how to apply Bayesian and frequentist methods, and will also learn practical procedures for developing statistical models for phenomena, drawing inferences, and evaluating evidential support for hypotheses. The course covers fundamentals of the Bayesian theory of inference, including probability as a representation for degrees of belief, the likelihood principle, the use of Bayes Rule to revise beliefs based on evidence, conjugate prior distributions for common statistical models, Markov Chain Monte Carlo methods for approximating the posterior distribution, Bayesian hierarchical models, and other key topics. Graphical models are introduced for representing complex probability and decision problems by specifying them in modular components. Assignments make use of modern computational techniques and focus on applying the methods to practical problems.\n\n\nList of Topics\n\nUnit 1: A Brief Tour of Bayesian Inference and Decision Theory\nUnit 2: Random Variables, Parametric Models, and Inference from Observation\nUnit 3: Bayesian Inference with Conjugate Pairs: Single Parameter Models\nUnit 4: Introduction to Monte Carlo Approximation\nUnit 5: The Normal Model\nUnit 6: Markov Chain Monte Carlo\n\nUnit 7: Hierarchical Bayesian Models\nUnit 8: Bayesian Regression\nUnit 9: Multinomial Distribution and Latent Groups\nUnit 10: Hypothesis Tests, Bayes Factors, and Bayesian Model Averaging\n\n\n\nSchedule\n\nJan 22: First Class\nMarch 4: No class (spring break)\nApr 29: Last class, project presentations\n\n\n\nAssignments\nHomework is due 11:59PM on the assigned due date. If you have extenuating circumstances, please contact me in advance, and I will consider giving you additional time to complete the assignment for partial credit. Assignments will be posted here and on Blackboard. Please submit your assignments through Blackboard.\nExams are take home and will be posted on Blackboard.\n\n\nPrerequisites\nThe formal listed prerequisite is OR 542 or STAT 544 or STAT 554 or equivalent.\n\nThe real prerequisites are:\n\nExperience with elementary data analysis such as scatterplots, histograms, hypothesis tests, confidence intervals, and simple linear regression.\nA calculus-based probability course - elementary probability theory, discrete and continuous probability distributions, probability mass and probability density functions, cumulative distribution functions, common parametric models such as the normal, binomial and Poisson distributions.\nExperience with a high-level programming language. We will use R, a programming language for data analysis, and STAN, a language for specifying and performing inference with Bayesian models.\nComfort with mathematical notation. We will not do proofs, but you will be expected to be comfortable following and doing mathematical derivations.\n\n\n\nComputing\nWe will use R, a powerful (free) statistical graphics and computing language, and STAN, an open-source, cross-platform engine for Bayesian data analysis that can be accessed from within R. Many of the exercises will require programming. RStudio is an integrated development environment for R. Some students prefer Python to R. You may use your preferred software as long as your solution is clear and I can understand what you did, but the solutions and examples will all be in R.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, midterm and final project.\n\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on Bb. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nOptional Textbooks\nI teach from my notes, not from a textbook. The material I cover follows the Hoff text fairly closely, but the Hoff text is not as readable as I would like. Most students like to have a text as a supplement to my notes. The first two listed texts have electronic versions available from the library.\n\nPrimary text (recommended): This book, published in 2009, provides about the right level of coverage and is a reasonably up-to-date treatment. An electronic edition of this book is available to George Mason University students and faculty from the university library. Computer code is available for most of the examples in the book.\n\nHoff, Peter D., A First Course in Bayesian Statistical Methods. Springer, 2009.\nSecondary text (recommended): This recently published book was written primarily for social scientists. It is accessible, well-written, and gives a comprehensive treatment beginning from the very basics through sophisticated hierarchical Bayesian models. An electronic edition of this book is available to George Mason University students and faculty from the university library. Computer code is available at the github site for most of the examples in the book.\n\nKruschke, John, Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press, 2014.\nReference text (recommended): This comprehensive text has become the standard reference in Bayesian statistical methods. The hyperlink below contains reviews, exercises, data sets and software.\n\nGelman, A., Carlin, J., Stern, H., Dunson, D. B., Vehtari, A. and Rubin, D., Bayesian Data Analysis (3rd edition). CRC Press, 2013.\nSupplemental text (recommended): This recently published book provides comprehensive coverage of computational Bayesian statistics with a focus on conducting Bayesian analyses of real data sets. The range of topics covered is much more extensive than the Hoff text, and will serve as a useful supplement for readers interested in Bayesian treatment of topics not covered in this course, such as generalized linear models, capture-recapture experiments, time series and image analysis. R code and a solution manual are available.\n\nMarin, Jean-Michel and Robert, Christian, Bayesian Essentials with R (2nd edition). Springer, 2014.\nAlternate text: The text by Peter Lee is accessible and may be helpful as an alternative treatment. Again, the hyperlink contains additional information, including exercises, solutions, errata and software.\n\nLee, Peter, Bayesian Statistics: An Introduction (4th edition), Wiley, 2012.\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\n\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\n\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\n\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/41000/41000.html",
    "href": "courses/41000/41000.html",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvadim.sokolov@chicagobooth.edu\nTA: Ayman Moawad\naymoawad@uchicago.edu\nSyllabus\nDatasets \nAll examples\nCourse Textbook: OpenIntro Statistics pdf, tablet"
  },
  {
    "objectID": "courses/41000/41000.html#class-notes",
    "href": "courses/41000/41000.html#class-notes",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Class Notes",
    "text": "Class Notes\n\nSection 1: Introduction and Probability: notes, code\nSection 2: Statistics and Data: notes, code, epl, abtesting\nSection 3: Regression: notes, code, housing, mammals\nSection 4: Classification: notes, tree notes newfood, OJ, PGA, Tennis, logistic\nSection 5: AI and Deep Learning: notes, Examples: MNIST; NN for Circle Data, IMDB, Trump Tweets, trump-tweets.R"
  },
  {
    "objectID": "courses/41000/41000.html#hw",
    "href": "courses/41000/41000.html#hw",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "HW",
    "text": "HW\n\nHW1 (code)\nHW2 (code)\nHW3 (code)\nHW4 (code\nFinal Project (code)"
  },
  {
    "objectID": "courses/41000/41000.html#midterm",
    "href": "courses/41000/41000.html#midterm",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Midterm",
    "text": "Midterm\n\nQuestions\nQuestions\nMidterm 1\nMidterm 2\nMidterm 3\nMidterm 4\nMidterm 5\nMidterm 6\nMidterm 7\nMidterm 8\nMidterm 9\nMidterm 10\nMidterm 11"
  },
  {
    "objectID": "courses/41000/41000.html#r",
    "href": "courses/41000/41000.html#r",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "R",
    "text": "R\nI made a quick introduciton to R, video is here and here are the slides.\n\nTo get RStudio on you computer, you need to first download and install RStudio then you need to download and install R.\nFurther, I recommend doing those two courses on DataCamp.\n\nIntroduction to R\nIntroduction to Data\n\n\nIn addition, Booth offers access to an online R tutorial on LinkedinLearning. To access the training, go to: here and login using your CNetID and password. You may find it helpful to work through the following R courses offered on LinkedInLearning:\n\nCode Clinic: R with Mark Niemann-Ross\nR Statistics Essential Training with Barton Poulson\nUp and Running with R with Barton Poulson"
  },
  {
    "objectID": "courses/41000/41000.html#demos",
    "href": "courses/41000/41000.html#demos",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Demos",
    "text": "Demos\n\nPortfolio Variance\nBinomial\nNormal Distribution"
  },
  {
    "objectID": "courses/41000/41000.html#other-materials-and-links",
    "href": "courses/41000/41000.html#other-materials-and-links",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Other Materials and Links",
    "text": "Other Materials and Links\n\nAI in Tennis\nKraft-Heinz blog store\nKraft-Heinz WSJ\nFrito-Lay potato peeling\nTyson\nAnheuser-Busch InBev payments\nWalmart smart\nThe State of Data Science and MachineLearning\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb Random Forest\nFacebook regrsssion\nYoutube Deep learning\nUber: time series\nExploratory Data Analysis\nOverfitting\n2016 Election"
  },
  {
    "objectID": "courses/41000/41000.html#deep-learning",
    "href": "courses/41000/41000.html#deep-learning",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learnig\nShort introduction into deep leanring\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Google Scholar\nPolaris Project\nGeneology"
  },
  {
    "objectID": "research.html#papers",
    "href": "research.html#papers",
    "title": "Vadim Sokolov",
    "section": "papers",
    "text": "papers\n\n\nBehnia, F., Karbowski, D., and Sokolov, V. (2023), “Deep generative models for vehicle speed trajectories,” Applied Stochastic Models in Business and Industry, 39, 701–719.\n\n\nBendre, S., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the probability of magnus carlsen reaching 2900,” Applied Stochastic Models in Business and Industry, 39, 372–381.\n\n\nPolson, N. G., and Sokolov, V. (2023b), “Generative AI for bayesian computation,” arXiv preprint arXiv:2305.14972.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023a), “Deep partial least squares for instrumental variable regression,” Applied Stochastic Models in Business and Industry.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023b), “Generative causal inference,” arXiv preprint arXiv:2306.16096.\n\n\nGupta, A., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the value of chess squares,” Entropy, MDPI, 25, 1374.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2023), “Quantum bayesian computation,” Applied Stochastic Models in Business and Industry.\n\n\nPolson, N., and Sokolov, V. (2023a), “Deep learning: A tutorial,” arXiv preprint arXiv:2310.06251.\n\n\nBaker, E., Barbillon, P., Fadikar, A., Gramacy, R. B., Herbei, R., Higdon, D., Huang, J., Johnson, L. R., Ma, P., Mondal, A., and others (2022), “Analyzing stochastic computer models: A review with opportunities,” Statistical Science, Institute of Mathematical Statistics, 37, 64–89.\n\n\nZha, Y., Parker, S. T., Foster, J. J., and Sokolov, V. (2022), “Housing market forecasting using home showing events,” arXiv preprint arXiv:2201.04003.\n\n\nSchultz, L., Auld, J., and Sokolov, V. (2022), “Bayesian calibration for activity based models,” arXiv preprint arXiv:2203.04414.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022a), “Deep partial least squares for iv regression,” arXiv preprint arXiv:2207.02612.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2022), “Quantum bayes AI,” arXiv preprint arXiv:2208.08068.\n\n\nSchultz, L., and Sokolov, V. (2022), “Deep learning gaussian processes for computer models with heteroskedastic and high-dimensional outputs,” arXiv preprint arXiv:2209.02163.\n\n\nWang, Y., Polson, N., and Sokolov, V. O. (2022), “Data augmentation for bayesian deep learning,” Bayesian Analysis, International Society for Bayesian Analysis, 1, 1–29.\n\n\nLey, H., Auld, J., Verbas, Ö., Weimer, R., Driscoll, S., Mohammadian, K., Golshani, N., Rahim, E., Shabanpour, R., Li, Z., and others (2022), Coordinated transit response planning and operations support tools for mitigating impacts of all-hazard emergency events, United States. Department of Transportation. Federal Transit Administration.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022b), “Feature selection for personalized policy analysis,” arXiv preprint arXiv:2301.00251.\n\n\nFotouhi, H., Mori, N., Miller-Hooks, E., Sokolov, V., and Sahasrabudhe, S. (2021), “Assessing the effects of limited curbside pickup capacity in meal delivery operations for increased safety during a pandemic,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2675, 436–452.\n\n\nZavareh, M., Maggioni, V., and Sokolov, V. (2021), “Investigating water quality data using principal component analysis and granger causality,” Water, MDPI, 13, 343.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2021), “Deep learning partial least squares,” arXiv preprint arXiv:2106.14085.\n\n\nSokolova, A. O., Marshall, C. H., Lozano, R., Gulati, R., Ledet, E. M., De Sarkar, N., Grivas, P., Higano, C. S., Montgomery, B., Nelson, P. S., and others (2021), “Efficacy of systemic therapies in men with metastatic castration resistant prostate cancer harboring germline ATM versus BRCA2 mutations,” The Prostate, 81, 1382–1389.\n\n\nBhadra, A., Datta, J., Polson, N., Sokolov, V., and Xu, J. (2021), “Merging two cultures: Deep and statistical learning,” arXiv preprint arXiv:2110.11561.\n\n\nHsu, Y. L., Jeng, C. C., Murali, P. S., Torkjazi, M., West, J., Zuber, M., and Sokolov, V. (2021), “Bayesian learning: A selective overview,” arXiv preprint arXiv:2112.12722.\n\n\nPolson, N., and Sokolov, V. (2020), “Deep learning: Computational aspects,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 12, e1500.\n\n\nHuang, X., Li, B., Peng, H., Auld, J. A., and Sokolov, V. O. (2020), “Eco-mobility-on-demand fleet control with ride-sharing,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 23, 3158–3168.\n\n\nSokolov, V. (2020), “Discussion of ‘multivariate generalized hyperbolic laws for modeling financial log-returns—empirical and theoretical considerations’,” Applied Stochastic Models in Business and Industry, John Wiley & Sons, 36, 777–779.\n\n\nDixon, M. F., Polson, N. G., and Sokolov, V. O. (2019), “Deep learning for spatio-temporal modeling: Dynamic traffic flows and high frequency trading,” Applied Stochastic Models in Business and Industry, 35, 788–807.\n\n\nWarren, J., Lipkowitz, J., and Sokolov, V. (2019), “Clusters of driving behavior from observational smartphone data,” IEEE Intelligent Transportation Systems Magazine, IEEE, 11, 171–180.\n\n\nPolson, N. G., and Sokolov, V. (2019), “Bayesian regularization: From tikhonov to horseshoe,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 11, e1463.\n\n\nWang, Y., Polson, N. G., and Sokolov, V. O. (2019), “Scalable data augmentation for deep learning,” arXiv preprint arXiv:1903.09668.\n\n\nSokolov, V., and Polson, M. (2019), “Strategic bayesian asset allocation,” arXiv preprint arXiv:1905.08414.\n\n\nLi, D., Liu, J., Park, N., Lee, D., Ramachandran, G., Seyedmazloom, A., Lee, K., Feng, C., Sokolov, V., and Ganesan, R. (2019), “Solving large-scale 0-1 knapsack problems and its application to point cloud resampling,” arXiv preprint arXiv:1906.05929.\n\n\nChen, H., Jajodia, S., Liu, J., Park, N., Sokolov, V., and Subrahmanian, V. (2019), “FakeTables: Using GANs to generate functional dependency preserving tables with bounded real data.” in IJCAI, pp. 2074–2080.\n\n\nNicholas G. Polson, V. O. S. (2019), “Deep learning,” Wiley StatsRef: Statistics Reference Online.\n\n\nSchultz, L., and Sokolov, V. (2018a), “Bayesian optimization for transportation simulators,” Procedia computer science, Elsevier, 130, 973–978.\n\n\nSchultz, L., and Sokolov, V. (2018b), “Deep reinforcement learning for dynamic urban transportation problems,” arXiv preprint arXiv:1806.05310.\n\n\nSokolov, V., Imran, M., Etherington, D. W., Karbowski, D., and Rousseau, A. (2018), “Effects of predictive real-time traffic signal information,” in 2018 21st international conference on intelligent transportation systems (ITSC), IEEE, pp. 1834–1839.\n\n\nSchultz, L., and Sokolov, V. (2018c), “Practical bayesian optimization for transportation simulators,” arXiv preprint arXiv:1810.03688.\n\n\nPolson, N., and Sokolov, V. (2017a), “Bayesian particle tracking of traffic flows,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 19, 345–356.\n\n\nSokolov, V., Larson, J., Munson, T., Auld, J., and Karbowski, D. (2017), “Maximization of platoon formation through centralized routing and departure time coordination,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2667, 10–16.\n\n\nSokolov, V. (2017), “Discussion of ‘deep learning for finance: Deep portfolios’,” Applied Stochastic Models in Business and Industry, 33, 16–18.\n\n\nAuld, J., Sokolov, V., and Stephens, T. S. (2017), “Analysis of the effects of connected–automated vehicle technologies on travel demand,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2625, 1–8.\n\n\nPolson, N. G., and Sokolov, V. (2017b), “Deep learning: A bayesian perspective.”\n\n\nVerbas, Ö., Sokolov, V., Auld, J., and Ley, H. (2017), “Time-dependent capacitated transit routing with real-time demand and supply data.”\n\n\nAuld, J., Hope, M., Ley, H., Sokolov, V., Xu, B., and Zhang, K. (2016b), “POLARIS: Agent-based modeling framework development and implementation for integrated travel demand and network and operations simulations,” Transportation Research Part C: Emerging Technologies, Pergamon, 64, 101–116.\n\n\nAuld, J., Karbowski, D., Sokolov, V., and Kim, N. (2016a), “A disaggregate model system for assessing the energy impact of transportation at the regional level,” in TRB 2016 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Jongryeol, J. (2016b), “Fuel saving potential of optimal route-based control for plug-in hybrid electric vehicle,” IFAC-PapersOnLine, Elsevier, 49, 128–133.\n\n\nJacquier, E., Polson, N., and Sokolov, V. (2016), “Sequential bayesian learning for merton’s jump model with stochastic volatility,” arXiv preprint arXiv:1610.09750.\n\n\nLarson, J., Munson, T., and Sokolov, V. (2016), “Coordinated platoon routing in a metropolitan network,” in 2016 proceedings of the seventh SIAM workshop on combinatorial scientific computing, Society for Industrial; Applied Mathematics, pp. 73–82.\n\n\nKarbowski, D., Kim, N., Auld, J., and Sokolov, V. (2016a), “Assessing the energy impact of traffic management and vehicle hybridisation,” International Journal of Complexity in Applied Science and Technology, Inderscience Publishers (IEL), 1, 107–124.\n\n\nPolson, N., and Sokolov, V. (2015), “Bayesian analysis of traffic flow on interstate i-55: The LWR model.”\n\n\nSokolov, V., Karbowski, D., Kim, N., and Auld, J. (2015), “Assessing the energy impact of traffic management and vehicle hybridization,” in 25th ITS annual meeting.\n\n\nLuo, Q., Auld, J., and Sokolov, V. (2015), “Addressing some issues of map-matching for large-scale, high-frequency GPS data sets,” in TRB 2015 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Rousseau, A. (2015), Vehicle energy management optimization through digital maps and connectivity, Argonne National Lab.(ANL), Argonne, IL (United States).\n\n\nRichey, A. S., Richey, J. E., Tan, A., Liu, M., Adam, J. C., and Sokolov, V. (2015), “Assessing the use of remote sensing and a crop growth model to improve modeled streamflow in central asia,” in AGU fall meeting abstracts, pp. H44F–08.\n\n\nSokolov, V., Karbowski, D., and Kim, N. (2014a), “Assessing the energy impact of traffic management and ITS technologies,” in 21st ITS world congress.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and others (2014), “GREET model: The greenhouse gases, regulated emissions, and energy use in transportation model,” Chicago, USA: Argonne National Laboratory.\n\n\nSokolov, V. O., Zhou, X., and Langlois, P.-A. (2014b), “A framework for arterial traffic flow modeling-POLARIS.”\n\n\nAuld, J., Hope, M., Ley, H., Xu, B., Zhang, K., and Sokolov, V. (2013), “Modelling framework for regional integrated simulation of transportation network and activity-based demand (polaris),” in International symposium for next generation infrastructure.\n\n\nAuld, J., Sokolov, V., Fontes, A., and Bautista, R. (2012), “Internet-based stated response survey for no-notice emergency evacuations,” Transportation Letters, Taylor & Francis, 4, 41–53.\n\n\nSokolov, V., Auld, J., and Hope, M. (2012), “A flexible framework for developing integrated models of transportation systems using an agent-based approach,” Procedia Computer Science, Elsevier, 10, 854–859.\n\n\nDatta, B. N., and Sokolov, V. (2011), “A solution of the affine quadratic inverse eigenvalue problem,” Linear Algebra and its Applications, North-Holland, 434, 1745–1760.\n\n\nPark, Y. S., Manli, E., Hope, M., Sokolov, V., and Ley, H. (2010), Fuzzy rule-based approach for evacuation trip demand modeling.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and GREET, M. (2010), “The greenhouse gases, regulated emissions, and energy use in transportation model,” Center for Transportation Research Argonne National Laboratory, Argonne, IL.\n\n\nDatta, B. N., Deng, S., Sokolov, V., and Sarkissian, D. (2009), “An optimization technique for damped model updating with measured data satisfying quadratic orthogonality constraint,” Mechanical Systems and Signal Processing, Academic Press, 23, 1759–1772.\n\n\nDatta, B. N., and Sokolov, V. (2009), “Quadratic inverse eigenvalue problems, active vibration control and model updating,” Applied and Computational Mathematics, 8, 170–191.\n\n\nSokolov, V. O. (2008), “Quadratic inverse eigenvalue problems: Theory, methods, and applications,” PhD thesis, Northern Illinois University.\n\n\nAre, S., Dostert, P., Ettinger, B., Liu, J., Sokolov, V., Wei, A., and Wiegand, K. (2006), “Reservoir model optimization under uncertainty.”\n\n\nKrukier, L., Pichugina, O., Sokolov, V., and Vulkov, L. (2006), “Numerical investigation of krylov subspace methods for solving non-symmetric systems of linear equations with dominant skew-symmetric part,” International Journal of Numerical Analysis and Modeling, University of Alberta, 3, 115–124."
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Vadim Sokolov",
    "section": "software",
    "text": "software\n\nPOLARIS: Designer. Developer. Transportation systems simulations framework (C++)\nGREET: Designer. Lead Developer. An implementation of The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (GREET) Model. (C#, .NET, SQLite). more then 800 unique users within first year of release, 2013\nMATCOM: Contributor. Distributed on CD with Numerical Linear Algebra and Applications, Second Edition book By Biswa Nath Datta, SIAM. (MATLAB)\nTRANSIMS: Contributor. An agent-based forecast software for modeling regional transport systems. (C++); 22,295 total downloads since 2006\nAdvanced Numerical Methods II: Sole Developer. Package for solving large scale control problems. (Mathematica); an experimental library that was not published"
  },
  {
    "objectID": "research.html#research-coverage",
    "href": "research.html#research-coverage",
    "title": "Vadim Sokolov",
    "section": "research coverage",
    "text": "research coverage\n\nDemystifying the future of connected and autonomous vehicles (Newswise\nArgonne wins grant to help transit agencies cope with emergencies (Chicago Tribune)\nArgonne will research how transportation systems should respond to natural hazards (WBEZ)\nWhat Happens When Developers, Scientists and Super-Computers Connect on Urban Design (Next City)\nUM wins $2.7M grant to study driverless cars (The Detroit News)\nUM teams with Argonne, Idaho national labs to study potential energy savings of connected vehicles (Michigan News)\nArgonne to study emergency response of Chicago transit (Chicago Sun Times)\nDesigning future cities (phys.org)\nUsing a Real Life SimCity to Design a Massive Development (Curbed Chicago)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Vadim Sokolov",
    "section": "Contact",
    "text": "Contact\nNguyen Engineering Building MS 4A6\nOffice: 2242\nFairfax, VA, 22030\nPhone: 703 993 4533\nvsokolov@gmu.edu"
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Vadim Sokolov",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise\n\nData science: Bayesian statistics, deep learning, convex optimization, reinforcement learning\nComplex Systems: Agent-based models, Bayesian optimization\nApplications: Urban systems modeling, asset allocation, blockchain"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Vadim Sokolov",
    "section": "Bio",
    "text": "Bio\nVadim Sokolov is an associate professor in the Systems Engineering and Operations Research Department at George Mason University. He works on building robust solutions for large scale complex system analysis, at the interface of simulation-based modeling and statistics. This involves, developing new methodologies that rely on deep learning, Bayesian analysis of time series data, design of computational experiments and development of open-source software that implements those methodologies. Inspired by an interest in urban systems he co-developed mobility simulator called Polaris that is currently used for large scale transportation networks analysis by both local and federal governments. Prior to joining GMU he was a principal computational scientist at Argonne National Laboratory, a fellow at the Computation Institute at the University of Chicago and lecturer at the Master of Science in Analytics program at the University of Chicago.\nHe has published in such leading statistics, mathematics and engineering journals, as the Annals of Applied Statistics, Transportation Research Part C, Linear Algebra and Its Applications as well as in Mechanical Systems and Signal Processing. He holds a PhD in computational mathematics from Northern Illinois University, and pursued graduate studies in statistics at the University of Chicago, while working at Argonne."
  },
  {
    "objectID": "courses/41000/syllabus.html",
    "href": "courses/41000/syllabus.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "href": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/610.html",
    "href": "courses/610.html",
    "title": "SYST/OR 610. Deep Leanring",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2022\n\n\n\n\n\nInstructor: Vadim Sokolov\nLocation and time: Aquia, room 347; 7:20-10pm Mondays\nOffice hours: By appointment\n\n\nDatacamp\nIf you are rusty on Python, I suggest you refresh your skills using Datacamp. Datacamp gave students in this class a free access to all of the courses. If you follow the link above you can get your free access using masonlive email. I also listed some of the Python courses I suggest #there.\n\n\nList of topics and tentative schedule\n\nBasics (Weeks 1-2)\n\nLinear Algebra: intro\nProbability: OpenIntro Ch 3\nGeneralized Linear Models: OpenIntro Ch 8,9\nPyTorch: PyTorch Basics\nFeed Forward Architectures: WHAT IS TORCH.NN REALLY?; Ripley Ch 5; Bishop Ch 3,4\n\nConvex Optimization (Weeks 3-4)\n\nBackpropagation and matrix derivatives\nStochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration): Bishop Ch 7, Goodfellow Ch 8\nSecond order methods: Bishop Ch 7\nADMM\nRegularization (l1, l2 and dropout): dropout paper, Godfellow Ch 7\nBatch normalization: paper\n\nConv Nets and Image Processing (Week 5): Goodfellow Ch 9\nRecurrent Nets and Sequential Data (Week 6): Good Fellow Ch 10, seq2seq, Pytorch eq2seq tutorial\nTheory of deep learning (Week 7): see theory section for the reading list\n\nUniversal approximators\nCurse of dimensionality\nKernel spaces\nTopology and geometry\n\nProbabilistic DL (Weeks 8-9) Langevin, MCMC, , VB\n\nConjugate distributions, exponential family Bishop: Ch 2\nModel choice\nHierarchical linear and generalize linear models (regression and classification): Bishop Ch 10\nModels for missing data (EM-algorithm)\nBayes computations (MCMC, Variational Bayes)\n\nAdditional Topics (Weeks 10-13)\n\nModel Visualization Tensorboard\nGenerative Models (normalizing flows, GANs, recurrent nets): NF Paper Tutorial; NF; \nAttention and Transformers attention paper\nDeep Reinforcement Learning DRL Tutorial\nBayesian Optimisation: Hyperparameter selection and parameter initialization Hyperopt\n\n\n\n\nData analysis projects\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 8 of the class you should have a team formed and data set + analysis problem identified. You need to submit a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\nYou will post results of your analysis on the class blog post. The final project will be graded on presentation, writing and analysis.\n\n\n\nGroup Work\nBoth projects and homework can be done in a groups of size of up to 3 people. You can change groups in between. If you do a homework in a gorup, it means that all of the members of the group do it individually and can consult with each other. You can also do 1 submission per group if you prefer. You can use “group” section of the piazza page to find teammates if you need any. If you need help finding a group, please email me.\n\n\nGrading\nEach hw is 10 points, project is 30.\n\nThis is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic.\n\n\nBooks\n\nPolson, Sokolov notes\nDive into Deep Learning link\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.\nRipley, Brian D. Pattern recognition and neural networks. Cambridge university press, 2007.\nBishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.\n\n\n\nPer Topic Resources\n\nArchitectures\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\nOptimization\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\nTheory\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\nReinforcement Learning\nOptimization Models - Truth - Yet\n\n\nBayesian DL\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\nPractical Tricks\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\nOther Resources\n\nAdditional Reading List\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\nBlogs\n\nPapers with code\nSecurity\nUnsupervised learning\nCybersecurity\nOpt Visualization\nAI and Memory Wall\n\n\n\nVideos\n\nDeep Energy\nDL Summer school 2015\nDL Representations\nPyData 2017\n\n\n\nOther courses with good web presence\n\nStanford’s CS231n\nStanford’s STATS385\nfast.ai\nNando de Freitas’ course on machine\nUC Berkeley Stat241B\nMIT\nUdacity DRL\n\n\n\nTools\n\nTensorFlow\nKeras\nTF Playground\nSony\nSnakeViz python profiler\nPyTorch\nTS Stan Examples\nOpenAI Glow\n\n\n\nMisc Links\n\nPytorch resources (a curated list of tutorials, papers)\nIs artificial intelligence set to become art’s next medium?"
  },
  {
    "objectID": "courses/old/568-f20.html",
    "href": "courses/old/568-f20.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvsokolov@gmu.edu\nSkype: vsokolov83\\ TA: SeyedOmid HashemiAmiri\nohashem@masonlive.gmu.edu\n\\\nMaterials\\\nStarting this week (March 23) this class is moving online. We will use the following online resources: . DataCamp. You need to sign up for it, using the link. You will need to use your @masonlive.gmu.edu email to get free access. . Stanford’s Learning class. . Book . here for discussing HW/project/midterm. You can sign up \n{{\n\n}}\nFor each week I provide direct links to DataCamp courses and Statistical Learning videos.\n\nWeek of March 23\n. Datacamp: R . Resampling Methods (ISLR Chapter 5). Videos: a, b, c, d, e, HandsOn ######## Week of March 30 . (c due. Please submit 3 files to the BB. (a) First page with your signature acknowledging the honor code (b) PDF file with you solution (do not copy-paste large code chunks, just essential parts that you added on top of my scripts)) zip file with R scripts. ######## Week of April 6 . Linear Model Selection and Regularization (ISLR Chapter 6). Videos: a, b, c, e, f, g, h, i, j, k, HandsOn . Project proposals Due. Proposal is to be half-a-page and to contain: (a) description of the data to be analysed. (b) What is the goal of analysis, e.g. predict y from x or understand relations between y and x. (c) Why this is important? (d) What methods you think you will use? You can do it in bullet-points. ######## Week of April 13 . HW3 Due . Tree-based Methods (ISLR Chapter 8). Videos: a, a2, b, c, d, HandsOn . Datacamp: R ######## Week of April 20 . HW4 Due (ISLR: Ch 8, Ex. 4, 7, 8, 11) . Unsupervised Learning (ISLR Chapter 10). Videos: Unsupervised Learning (Chapter 10) a, b, c, d, e, HandsOn . Datacamp: R ######## Week of April 26 . Project intermediate reports are Due . Life session discussing the projects ######## Week of April 27 . HW5 Due (ISLR: Ch 10, Ex. 3, 10) . Optimization (I will post my videos on YouTube and will post links here when done) . Datacamp: website. Plus you can use rmarkdown materials on the  ######## Week of May 4 . Final Project Presentations Due. Delivered as html file generated from R Markdown. Your HTML pages will be published on the course page and will be publicly available.\n{{\n\n}}\n\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized. \\ Syllabus\\\n\n\n\n\nFridays 4:30 pm - 7:10pm at Sandbridge Hall 107 (Jan 21, 2020 - May 13, 2020)\n\n\n\n\nGrade composition: Grade based entirely on participation in class, homework assignments, in-class midterm and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nLearning\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\n\n\nDebby Kermer (data services): info\nShopping\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\nHere are the courses that cover different aspects of data science - Statistical modeling (STAT250, SYST664, OR719, STAT 554) - Data management (AIT614) - Optimization (OR604)\n\n\n\n\nIntroduction\nRegression\nClassification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll\n\n\n\n\n\n2016\n2017"
  },
  {
    "objectID": "courses/old/568-f20.html#systor-568.-applied-predictive-analytics---mason-analytics-ms",
    "href": "courses/old/568-f20.html#systor-568.-applied-predictive-analytics---mason-analytics-ms",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvsokolov@gmu.edu\nSkype: vsokolov83\\ TA: SeyedOmid HashemiAmiri\nohashem@masonlive.gmu.edu\n\\\nMaterials\\\nStarting this week (March 23) this class is moving online. We will use the following online resources: . DataCamp. You need to sign up for it, using the link. You will need to use your @masonlive.gmu.edu email to get free access. . Stanford’s Learning class. . Book . here for discussing HW/project/midterm. You can sign up \n{{\n\n}}\nFor each week I provide direct links to DataCamp courses and Statistical Learning videos.\n\nWeek of March 23\n. Datacamp: R . Resampling Methods (ISLR Chapter 5). Videos: a, b, c, d, e, HandsOn ######## Week of March 30 . (c due. Please submit 3 files to the BB. (a) First page with your signature acknowledging the honor code (b) PDF file with you solution (do not copy-paste large code chunks, just essential parts that you added on top of my scripts)) zip file with R scripts. ######## Week of April 6 . Linear Model Selection and Regularization (ISLR Chapter 6). Videos: a, b, c, e, f, g, h, i, j, k, HandsOn . Project proposals Due. Proposal is to be half-a-page and to contain: (a) description of the data to be analysed. (b) What is the goal of analysis, e.g. predict y from x or understand relations between y and x. (c) Why this is important? (d) What methods you think you will use? You can do it in bullet-points. ######## Week of April 13 . HW3 Due . Tree-based Methods (ISLR Chapter 8). Videos: a, a2, b, c, d, HandsOn . Datacamp: R ######## Week of April 20 . HW4 Due (ISLR: Ch 8, Ex. 4, 7, 8, 11) . Unsupervised Learning (ISLR Chapter 10). Videos: Unsupervised Learning (Chapter 10) a, b, c, d, e, HandsOn . Datacamp: R ######## Week of April 26 . Project intermediate reports are Due . Life session discussing the projects ######## Week of April 27 . HW5 Due (ISLR: Ch 10, Ex. 3, 10) . Optimization (I will post my videos on YouTube and will post links here when done) . Datacamp: website. Plus you can use rmarkdown materials on the  ######## Week of May 4 . Final Project Presentations Due. Delivered as html file generated from R Markdown. Your HTML pages will be published on the course page and will be publicly available.\n{{\n\n}}\n\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized. \\ Syllabus\\\n\n\n\n\nFridays 4:30 pm - 7:10pm at Sandbridge Hall 107 (Jan 21, 2020 - May 13, 2020)\n\n\n\n\nGrade composition: Grade based entirely on participation in class, homework assignments, in-class midterm and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nLearning\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\n\n\nDebby Kermer (data services): info\nShopping\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\nHere are the courses that cover different aspects of data science - Statistical modeling (STAT250, SYST664, OR719, STAT 554) - Data management (AIT614) - Optimization (OR604)\n\n\n\n\nIntroduction\nRegression\nClassification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll\n\n\n\n\n\n2016\n2017"
  },
  {
    "objectID": "courses/old/750-f19.html",
    "href": "courses/old/750-f19.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2019\n\\\nMaterial \\\nInstructor: (vsokolov(at)gmu.edu)\nOffice hours: By appointment\nTA: Wanru Li (wli15(at)masonlive.gmu.edu)\nOffice Hours: Mon 4-6 pm at ENGR 2216\n\n\nAll of the posts based on the presentations are posted here dlclass2019.github.io\n\n\nPlease upload your materials to here.\nTo post your blog, send me your GiHub handle and use this instructions to add your post\n\n10/30: Uncertainty and Super resolution by Xavier Guitiaux. 40-60 min. Blog by Trajectory team. 2, \n11/6: Unbalanced data and Generalization by Zhengyang Fan, Di Zhang, Zhenlong Jiang. Full lecture. Blog by NLP team\\\n\nSlides\\\nNotes from Zhengyang Fan on prep materials:\n\nThe presentation for next class will mainly focus on interpolation phenomenon, and I plan to start this topic from kernel regression, thus some background in kernel space would be helpful for students. I have prepared a brief introduction for kernel space (helpful), which was also included in our presentation slides as backup slides. For students who are willing to have a deeper understanding of kernel, the following , which includes some visualizations and simple examples.\n\nFor Rademacher complexity and uniform laws o larger numbers, following papers and lecture notes can be useful for students: – paper – note – wiki – note\nMy presentation on theory of deep learning is mainly based on the following series. Students who are interested in theory may benefit from these video lectures.\n\n11/13 NLP by Rahul Pandey, Angeela, Junxiang(Will), Jomana. Full lecture. Blog by the unbalanced/generalization team\n11/20 Adverarial Atatchs by Farnaz Behnia and Azadeh Gharibreza Yazdi. Half lecture. Blog by Uncertainty team\n11/20 Trajecotry Generation by Chris Grubb. 40-60 min. Blog by adversarial team\n\n\n\n\nPlease upload your 1-page proposals to here. Deadline is 11/6. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\n\n11/20\n12/4\n\n\n\n\n\n\ntutorial, Due Sep 18. Optional: \nHW2, Due Sep 29\nHW3, Due Oct 9\nHW4, Due Oct 30\nHW5\n\n\n\n\n\nPresentation/Project proposals due Oct 16\n9/3/2019: Room changed to EXPL L111\n3/15/2019: No Class on October 23 (INFORMS Meeting)\n3/15/2019: No Class on November 27 (Thanksgiving recess)\n3/15/2019: First class is on Aug 28 at 7:20pm\n3/15/2019: Last class is on Dec 4\n\n\n\n\n\nDL Overview + Python (numpy and PyTorch) (Week 1)\nNotes Ch 1; Polson18\nProbability (Week 2)\nNotes Ch 2-3; Domingos\nOptional DLB Ch 3\nOptimization: SGD, Backprop (Week 3)\nNotes Ch 4, Baydin17,\nArchitectures (Week 4)\nNotes ch 5; Oord16\nMore optimization (Week 5)\nGlorot10\nBayes DL (6-7)\np3, , \nDL Theory (Week 8)\nResearch paper presentations (Week 9-12)\nProject Presentations (Week 13)\n\n\\ This is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic. \\ The lectures and homework for 750 and 610 are the same. The difference is in the final project. If you are registered for 750, you will read and present research papers and if you are registered for 610 you will do a Kaggle-type project. Both research paper presentations and projects are to be done in a group of size up to 5 for research papers and 3 for projects.\n\n\n\nDuring weeks 9-12, this class will be run in a seminar mode. A team of students will prepare a topic and will lead the discussion and another team will write a blog-post about the class and will post it on Medium. Students responsible for posting the blog summary will be different from the ones charged with leading the topic discussion, but should work closely with the leaders on the posted write-up.\n\n\n\nTwo weeks before the scheduled class, meet briefly with me to discuss plan for the class. You should decide on a team leader for this class, who will be the one responsible for making sure everyone on the team knows what they are doing and coordinating the team’s efforts.\nThe Monday the week of the class, at least a few representatives from the team should come to my office to discuss the plan for the class. You should come prepared to this meeting with suggested papers and ideas about how to present them.\nOn Tuesday before class, send me the preparation materials for the class. This can include links to papers to read, but could also include exercises to do or software to install and experiment with, etc. I will post it on the course page.\nDay of class: lead an interesting, engaging, and illuminating class! This is a 2.5 hour class, so it can’t just be a series of unconnected, dull presentations. You need to think of things to do in class to make it more worthwhile and engaging.\nAfter class: help the Blogging team by providing them with your materials, answering their questions, and reviewing their write-up.\n\n\n\n\n\nThe week before the scheduled class, develop a team plan for how to manage the blogging.\nOne team member should be designated the team leader for the blogging. The blogging leader is responsible for making sure the team is well coordinated and everyone knows what they are doing and follows through on this.\nDuring class, participate actively in the class, and take detailed notes (this can be distributed among the team).\nBy the Wed following class, have a the blog post ready and posted on Medium. Get comments from the rest of the class (including the leading team and coordinators).\nBy the next Friday (one week after the class), have a final version of the blog post ready.\n\n\n\n\n\nAdversarial attacks\nDL in reinforcement learning\nInterpretable DL\nScience applications of DL (physics, molecular biology,…)\nEngineering applications of DL (logistics, energy, smart grids, congestion management,…)\nNatural language processing\n\n\n\n\n\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 9 of the class you should have a team formed and data set + analysis problem identified. You need to email me a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. During week 13, you will have a time slot to present your findings. You are also encouraged (although it is not required) to post results of your analysis on Medium, if you think it is worth sharing.\n\n\n\nLectures: Hall L111. 7:20-10pm on Wed\nGrades: 40% homework, 60% class presentations\n\n\n\n\n\nConvex Optimization – Stochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration) – Second order methods – ADMM – Regularization (l1, l2 and dropout) – Batch normalization\nTheory of deep learning – Universal approximators – Curse of dimensionality – Kernel spaces – Topology and geometry\nComputational aspects (accelerated linear algebra, reduced precision calculations, parallelism)\nArchitectures (CNN, LSTM, MLP, VAE)\nBayesian DL\nDeep reinforcement learning\nHyperparameter selection and parameter initialization\nGenerative models (GANs)\n\n\n\n\n\nDeep Learning (DLB) (page)\nDeep Learning with Python (DLPB) (page)\nLearning Deep Architectures for AI (monograph)\n\n\n\n\n\n\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\n\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\n\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\n\nOptimization Models - Truth - Yet\n\n\n\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\n\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\n\n\n\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\n\n\nPapers with code link\nSecurity (blog)\nUnsupervised learning (blog)\nCybersecurity (collection)\nVisualization\n\n\n\n\n\nDeep Energy (blog)\nDL Summer school 2015 (videos)\nDL Representations (blog)\nPyData 2017 (videos)\n\n\n\n\n\nStanford’s CS231n (page)\nStanford’s STATS385 (page)\nfast.ai\nlearning\nUC Berkeley Stat241B (lectures)\nUCUC CSE598 (page)\nDRL\n\n\n\n\n\nTensorFlow\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nExamples\nGlow\n\n\n\n\n\nprojects (a curated list of tutorials, papers,)\nmedium?\n\n\n\n\n\n\n2017\n2018\n\n#https://www.coursera.org/learn/intro-to-deep-learning #http://www.machinelearning.ru/wiki/index.php?title##Глубинное_обучение_%28курс_лекций%29/2017 #https://github.com/aosokin/dl_cshse_ami/tree/master/2019-spring #https://bayesgroup.ru/teaching/ #saddepal@gmu.edu;savala@gmu.edu;fbehnia@gmu.edu;schava@gmu.edu;hgangava@gmu.edu;xgitiaux@gmu.edu;vgunnala@gmu.edu;nhuang2@gmu.edu;wkirsche@gmu.edu;jkrishn@gmu.edu;slakaman@gmu.edu;elynch9@gmu.edu;mmittapa@gmu.edu;gn@gmu.edu;krajend2@gmu.edu;srayapro@gmu.edu;asaid8@gmu.edu;oshaat@gmu.edu;ashambhu@gmu.edu;asothor2@gmu.edu;tstickl@gmu.edu;rzimme10@gmu.edu;aachary@gmu.edu;jbashata@gmu.edu;zchai2@gmu.edu;ychen37@gmu.edu;zfan3@gmu.edu;agharibr@gmu.edu;bghimire@gmu.edu;cgrubb@gmu.edu;zjiang@gmu.edu;nnewman7@gmu.edu;rpandey4@gmu.edu;tsmith58@gmu.edu;jwang40@gmu.edu;fyu2@gmu.edu;dzhang22@gmu.edu"
  },
  {
    "objectID": "courses/old/750-f19.html#or-750610.-deep-learning",
    "href": "courses/old/750-f19.html#or-750610.-deep-learning",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nFall 2019\n\\\nMaterial \\\nInstructor: (vsokolov(at)gmu.edu)\nOffice hours: By appointment\nTA: Wanru Li (wli15(at)masonlive.gmu.edu)\nOffice Hours: Mon 4-6 pm at ENGR 2216\n\n\nAll of the posts based on the presentations are posted here dlclass2019.github.io\n\n\nPlease upload your materials to here.\nTo post your blog, send me your GiHub handle and use this instructions to add your post\n\n10/30: Uncertainty and Super resolution by Xavier Guitiaux. 40-60 min. Blog by Trajectory team. 2, \n11/6: Unbalanced data and Generalization by Zhengyang Fan, Di Zhang, Zhenlong Jiang. Full lecture. Blog by NLP team\\\n\nSlides\\\nNotes from Zhengyang Fan on prep materials:\n\nThe presentation for next class will mainly focus on interpolation phenomenon, and I plan to start this topic from kernel regression, thus some background in kernel space would be helpful for students. I have prepared a brief introduction for kernel space (helpful), which was also included in our presentation slides as backup slides. For students who are willing to have a deeper understanding of kernel, the following , which includes some visualizations and simple examples.\n\nFor Rademacher complexity and uniform laws o larger numbers, following papers and lecture notes can be useful for students: – paper – note – wiki – note\nMy presentation on theory of deep learning is mainly based on the following series. Students who are interested in theory may benefit from these video lectures.\n\n11/13 NLP by Rahul Pandey, Angeela, Junxiang(Will), Jomana. Full lecture. Blog by the unbalanced/generalization team\n11/20 Adverarial Atatchs by Farnaz Behnia and Azadeh Gharibreza Yazdi. Half lecture. Blog by Uncertainty team\n11/20 Trajecotry Generation by Chris Grubb. 40-60 min. Blog by adversarial team\n\n\n\n\nPlease upload your 1-page proposals to here. Deadline is 11/6. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\n\n11/20\n12/4\n\n\n\n\n\n\ntutorial, Due Sep 18. Optional: \nHW2, Due Sep 29\nHW3, Due Oct 9\nHW4, Due Oct 30\nHW5\n\n\n\n\n\nPresentation/Project proposals due Oct 16\n9/3/2019: Room changed to EXPL L111\n3/15/2019: No Class on October 23 (INFORMS Meeting)\n3/15/2019: No Class on November 27 (Thanksgiving recess)\n3/15/2019: First class is on Aug 28 at 7:20pm\n3/15/2019: Last class is on Dec 4\n\n\n\n\n\nDL Overview + Python (numpy and PyTorch) (Week 1)\nNotes Ch 1; Polson18\nProbability (Week 2)\nNotes Ch 2-3; Domingos\nOptional DLB Ch 3\nOptimization: SGD, Backprop (Week 3)\nNotes Ch 4, Baydin17,\nArchitectures (Week 4)\nNotes ch 5; Oord16\nMore optimization (Week 5)\nGlorot10\nBayes DL (6-7)\np3, , \nDL Theory (Week 8)\nResearch paper presentations (Week 9-12)\nProject Presentations (Week 13)\n\n\\ This is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic. \\ The lectures and homework for 750 and 610 are the same. The difference is in the final project. If you are registered for 750, you will read and present research papers and if you are registered for 610 you will do a Kaggle-type project. Both research paper presentations and projects are to be done in a group of size up to 5 for research papers and 3 for projects.\n\n\n\nDuring weeks 9-12, this class will be run in a seminar mode. A team of students will prepare a topic and will lead the discussion and another team will write a blog-post about the class and will post it on Medium. Students responsible for posting the blog summary will be different from the ones charged with leading the topic discussion, but should work closely with the leaders on the posted write-up.\n\n\n\nTwo weeks before the scheduled class, meet briefly with me to discuss plan for the class. You should decide on a team leader for this class, who will be the one responsible for making sure everyone on the team knows what they are doing and coordinating the team’s efforts.\nThe Monday the week of the class, at least a few representatives from the team should come to my office to discuss the plan for the class. You should come prepared to this meeting with suggested papers and ideas about how to present them.\nOn Tuesday before class, send me the preparation materials for the class. This can include links to papers to read, but could also include exercises to do or software to install and experiment with, etc. I will post it on the course page.\nDay of class: lead an interesting, engaging, and illuminating class! This is a 2.5 hour class, so it can’t just be a series of unconnected, dull presentations. You need to think of things to do in class to make it more worthwhile and engaging.\nAfter class: help the Blogging team by providing them with your materials, answering their questions, and reviewing their write-up.\n\n\n\n\n\nThe week before the scheduled class, develop a team plan for how to manage the blogging.\nOne team member should be designated the team leader for the blogging. The blogging leader is responsible for making sure the team is well coordinated and everyone knows what they are doing and follows through on this.\nDuring class, participate actively in the class, and take detailed notes (this can be distributed among the team).\nBy the Wed following class, have a the blog post ready and posted on Medium. Get comments from the rest of the class (including the leading team and coordinators).\nBy the next Friday (one week after the class), have a final version of the blog post ready.\n\n\n\n\n\nAdversarial attacks\nDL in reinforcement learning\nInterpretable DL\nScience applications of DL (physics, molecular biology,…)\nEngineering applications of DL (logistics, energy, smart grids, congestion management,…)\nNatural language processing\n\n\n\n\n\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 9 of the class you should have a team formed and data set + analysis problem identified. You need to email me a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. During week 13, you will have a time slot to present your findings. You are also encouraged (although it is not required) to post results of your analysis on Medium, if you think it is worth sharing.\n\n\n\nLectures: Hall L111. 7:20-10pm on Wed\nGrades: 40% homework, 60% class presentations\n\n\n\n\n\nConvex Optimization – Stochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration) – Second order methods – ADMM – Regularization (l1, l2 and dropout) – Batch normalization\nTheory of deep learning – Universal approximators – Curse of dimensionality – Kernel spaces – Topology and geometry\nComputational aspects (accelerated linear algebra, reduced precision calculations, parallelism)\nArchitectures (CNN, LSTM, MLP, VAE)\nBayesian DL\nDeep reinforcement learning\nHyperparameter selection and parameter initialization\nGenerative models (GANs)\n\n\n\n\n\nDeep Learning (DLB) (page)\nDeep Learning with Python (DLPB) (page)\nLearning Deep Architectures for AI (monograph)\n\n\n\n\n\n\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\n\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\n\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\n\nOptimization Models - Truth - Yet\n\n\n\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\n\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\n\n\n\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\n\n\nPapers with code link\nSecurity (blog)\nUnsupervised learning (blog)\nCybersecurity (collection)\nVisualization\n\n\n\n\n\nDeep Energy (blog)\nDL Summer school 2015 (videos)\nDL Representations (blog)\nPyData 2017 (videos)\n\n\n\n\n\nStanford’s CS231n (page)\nStanford’s STATS385 (page)\nfast.ai\nlearning\nUC Berkeley Stat241B (lectures)\nUCUC CSE598 (page)\nDRL\n\n\n\n\n\nTensorFlow\nKeras\n(Google)\nSony\nprofiler (python)\nPyTorch\nExamples\nGlow\n\n\n\n\n\nprojects (a curated list of tutorials, papers,)\nmedium?\n\n\n\n\n\n\n2017\n2018\n\n#https://www.coursera.org/learn/intro-to-deep-learning #http://www.machinelearning.ru/wiki/index.php?title##Глубинное_обучение_%28курс_лекций%29/2017 #https://github.com/aosokin/dl_cshse_ami/tree/master/2019-spring #https://bayesgroup.ru/teaching/ #saddepal@gmu.edu;savala@gmu.edu;fbehnia@gmu.edu;schava@gmu.edu;hgangava@gmu.edu;xgitiaux@gmu.edu;vgunnala@gmu.edu;nhuang2@gmu.edu;wkirsche@gmu.edu;jkrishn@gmu.edu;slakaman@gmu.edu;elynch9@gmu.edu;mmittapa@gmu.edu;gn@gmu.edu;krajend2@gmu.edu;srayapro@gmu.edu;asaid8@gmu.edu;oshaat@gmu.edu;ashambhu@gmu.edu;asothor2@gmu.edu;tstickl@gmu.edu;rzimme10@gmu.edu;aachary@gmu.edu;jbashata@gmu.edu;zchai2@gmu.edu;ychen37@gmu.edu;zfan3@gmu.edu;agharibr@gmu.edu;bghimire@gmu.edu;cgrubb@gmu.edu;zjiang@gmu.edu;nnewman7@gmu.edu;rpandey4@gmu.edu;tsmith58@gmu.edu;jwang40@gmu.edu;fyu2@gmu.edu;dzhang22@gmu.edu"
  },
  {
    "objectID": "courses/old/568-f17.html",
    "href": "courses/old/568-f17.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2017\n\nFor Syllabus click here\n\nIntroduces predictive analytics with applications in engineering, business, finance, health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\n\n5/4/2014: We will have lecture on March 9, midterms due March 30.\n2/15/2017: Final projects are due on Monday of the finals week, May 15 at 9am.\n2/15/2017: Final projects presentations scheduled on May 4\n2/15/2017: Take home midterm is during the week of March 6th (before the Spring break) and due March 23\n2/13/2017: HW2 is due this Friday, Feb 17\n12/30/2016: Check back regularly for announcements\n\n\n\n\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: [Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Bahman Pedrood (bpedrood (at) gmu.edu)\n\n\n\n\n\nBootstrap\n\n\n\n\nBahman Pedrood: Wednesday 5-7pm (at Engineering 2241)\nVadim Sokolov: Thursday 5-7pm (at Engineering 2242)\n\n#PDF file can be downloaded from here\n\n\n\nLocation: Hall 105\nTimes: 7:20-10pm on Thursday #### Grades Grade composition: No in-class examination. Grade based entirely on participation in class, homework assigments, take-home midterm and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\n\n\nDebby Kermer (data services): info\nShopping\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\nHere are the courses that cover different aspects of data science - Statistical modeling (STAT250, SYST664, OR719, STAT 554) - Data management (AIT614) - Optimization (OR604)\n\n\n\n\nIntroduction\nRegression\nClasification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll\n\n\n\n\n\n2016"
  },
  {
    "objectID": "courses/old/568-f17.html#systor-568.-applied-predictive-analytics",
    "href": "courses/old/568-f17.html#systor-568.-applied-predictive-analytics",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "#https://patriotweb.gmu.edu/pls/prod/bwckctlg.p_disp_listcrse?term_in##201670&subj_in##SYST&crse_in##568&schd_in##LEC #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75285 #https://patriotweb.gmu.edu/pls/prod/bwckschd.p_disp_detail_sched?term_in##201670&crn_in##75284 Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2017\n\nFor Syllabus click here\n\nIntroduces predictive analytics with applications in engineering, business, finance, health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\n\n5/4/2014: We will have lecture on March 9, midterms due March 30.\n2/15/2017: Final projects are due on Monday of the finals week, May 15 at 9am.\n2/15/2017: Final projects presentations scheduled on May 4\n2/15/2017: Take home midterm is during the week of March 6th (before the Spring break) and due March 23\n2/13/2017: HW2 is due this Friday, Feb 17\n12/30/2016: Check back regularly for announcements\n\n\n\n\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: [Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Bahman Pedrood (bpedrood (at) gmu.edu)\n\n\n\n\n\nBootstrap\n\n\n\n\nBahman Pedrood: Wednesday 5-7pm (at Engineering 2241)\nVadim Sokolov: Thursday 5-7pm (at Engineering 2242)\n\n#PDF file can be downloaded from here\n\n\n\nLocation: Hall 105\nTimes: 7:20-10pm on Thursday #### Grades Grade composition: No in-class examination. Grade based entirely on participation in class, homework assigments, take-home midterm and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\n\n\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\n\n\nDebby Kermer (data services): info\nShopping\nWalmart\nYelp\nAirbnb\nZillow\nUber\nFiveThirtyErigh\neach (Lots of datasets along with descriptions of)\nmining (Lot of links to datasets relevant for data)\ndatasets\ndata (The site has links to plenty of regional, state, and local economic)\ndata (Searchable listing 363 Internet sites of Social Science)\nstatistics\ngovernment (A repository for information collected by the federal)\ndatasets\nStatistics\nNations\nAnalytics (Urban)\nTrips\nVirginia\n\n\n\n\nHere are the courses that cover different aspects of data science - Statistical modeling (STAT250, SYST664, OR719, STAT 554) - Data management (AIT614) - Optimization (OR604)\n\n\n\n\nIntroduction\nRegression\nClasification\nCV\nSelection\nModels\nTrees\nSVM\nPCA\nAll\n\n\n\n\n\n2016"
  },
  {
    "objectID": "courses/old/468-s18.html",
    "href": "courses/old/468-s18.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2018\n\nFor Syllabus click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\n\n\n04/17/2018: Final project is posted. Due May 9 at midnight.\n04/04/2018: HW4 is posted. Due April 11.\n02/28/2018: Midterm review is posted, see hw folder on Dropbox. Will go over it toady.\n02/25/2018: HW3 is posted, due March 7 at the beginning of the class.\n02/22/2018: Midterm is on March 7.\n02/12/2018: HW 2: Problems 8,9,16-19 from probability notes. Due Feb 21 at beginning of the class. Upload to bb.\n01/30/2018: HW 1: Problems 1,2,5,7,14 from notes. Due Feb 7 at beginning of the class.\n01/30/2018: Notes for week 1 and 2 are posted.\n01/11/2018: Payed summer internship in analytics @ GMU: details. Apply by Feb 15!\n12/17/2017: Check back regularly for announcements\n\n\n\n\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Jungho Park (jpark98@gmu.edu)\n\n\n\n\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nJungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\n\nLocation: Hall 127\nTimes: 4:30-7:10pm on Wednesday\n\n\n\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\nAdditional reading: List\n\n\n\n\n\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "courses/old/468-s18.html#syst-468.-applied-predictive-analytics",
    "href": "courses/old/468-s18.html#syst-468.-applied-predictive-analytics",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2018\n\nFor Syllabus click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\n\n\n04/17/2018: Final project is posted. Due May 9 at midnight.\n04/04/2018: HW4 is posted. Due April 11.\n02/28/2018: Midterm review is posted, see hw folder on Dropbox. Will go over it toady.\n02/25/2018: HW3 is posted, due March 7 at the beginning of the class.\n02/22/2018: Midterm is on March 7.\n02/12/2018: HW 2: Problems 8,9,16-19 from probability notes. Due Feb 21 at beginning of the class. Upload to bb.\n01/30/2018: HW 1: Problems 1,2,5,7,14 from notes. Due Feb 7 at beginning of the class.\n01/30/2018: Notes for week 1 and 2 are posted.\n01/11/2018: Payed summer internship in analytics @ GMU: details. Apply by Feb 15!\n12/17/2017: Check back regularly for announcements\n\n\n\n\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: Jungho Park (jpark98@gmu.edu)\n\n\n\n\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nJungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\n\nLocation: Hall 127\nTimes: 4:30-7:10pm on Wednesday\n\n\n\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\n\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\nAdditional reading: List\n\n\n\n\n\nInterpretability\nSurvey\nNote\nSearch\n[Times)](http://www.nytimes.com/interactive/2013/12/20/sunday-review/dialect-quiz-map.html How Y’all (NY)\nHall\nEPL\nCleaning\nForest (Random)\nregrsssion (Decision trees and logistic)\nlearning (deep)\nseries (time)\nAnalysis\nOverfitting\n2016\n\n\n\n\n\nlearning\nlearning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "courses/750.html",
    "href": "courses/750.html",
    "title": "OR 750. Bayesain Learning",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2021\n\nVideos\n\nCourse Material\n\nCourse Number: OR 750\nLocation: Zoom Thu 4:30-7:10pm\nInstructor: Vadim Sokolov\nOffice hours: by Appointment\nPrerequisites: Undergraduate Calculus, probability theory, statistics, computer programming skills (ideally R).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nThis is a graduate course on Bayesain learning. Although basics will be revisited, the pace will be swift so we can get to advanced topics as quickly as possible. This course details classical Bayesain techniques as well as modern approaches from both statistics and machine learning. We will consider some canonical examples of Bayesain analysis but will concentrate on modern Bayesain techniqes, computation and implementation, as well as modern applications. The course material will emphesize deriving and implementing methods over proving theoretical results.\n\nDuring weeks 11-15, this class will be run in a seminar mode. A student or the instructor will lead the discussion.\n\n\nTentative Schedule/List of Topics\n\nOverview: Conditional Probability, Bayes Rule, Bayesain inference, utility theory, distributions and tranformations\nHierarchical models: Bayesian regression, shrinkage (lasso, horseshoe)\nGaussian Process with applications in Bayesian Optimization – Tuning machine learning algorithms – Engineering model calibration\nAlgorithms – Markov Chain Monte Carlo (MCMC) – Expectation Maximization (EM) – Variational Bayes\nMarkov and Hidden Markov Models – Kalman Filter – Particle filter – Dynamic Linear Model – Structural Time Series Models\n\n\n\nGrading\nRubric: 30% HW, 10% Discussion, 60 % Final Project. No in-class examination.\nCutoffs: A: 90, B: 80, C: 70, F: &lt; 70\n\n\nOptional Texts\n\nPattern Recognition and Machine Learning by Bishop (page)\nMachine Learning: a Probabilistic Perspective by Murphy (page)\nAn Introduction to Bayesian Thinking by Clyde et al. (book)\nIntroduction to Statistical Thought by Lavine (book)\nThe Probability and Statistics Cookbook by Vallentin (page)\n\n\n\nPapers\n\nExchange Paradox by Christensen and Utts (pdf)\nInference for Nonconjugate Bayesian Models Using the Gibbs Sampler by Carlin and Polson (pdf)\nData Augmentation for Support Vector Machines by Polson and Scott (pdf)\nThe horseshoe estimator for sparse signals by Carvalho, Polson, Scott (pdf)\nDeep learning: A Bayesian perspective (pdf)\nBayesian regularization: From Tikhonov to horseshoe (pdf)\nSpatial Interaction and the Statistical Analysis of Lattice Systems by Besag (jstor)\nParticle learning and smoothing by Carlos M Carvalho, Michael S Johannes, Hedibert F Lopes, Nicholas G Polson (pdf)\nBayesian model assessment in factor analysis by Lopes and West (pdf)\nTracking epidemics with Google flu trends data and a state-space SEIR model by Dukic, Lopes and Polson (pdf)\nA statistical paradox by Lindley (pdf)\nThe philosophy of statistics by Lindley (pdf)\nThe Relevance Vector Machine by Tipping (pdf)\nBART: Bayesian additive regression trees by Hugh A. Chipman, Edward I. George, Robert E. McCulloch (arxiv)\nBayesian methods for hidden Markov models: Recursive computing in the 21st century by Scott (pdf)\nA modern Bayesian look at the multi‐armed bandit by Scott (pdf)\nBayesian analysis of computer code outputs: A tutorial by O’Hagan (paper)\nBayesian analysis of stochastic volatility models by Jacquier, Polson and Rossi (paper)\nOckham’s razor and Bayesian analysis by Jefferys and Berger (pdf)\nBayesian Learning for Neural Networks by Neal (pdf)\nMCMC Using Hamiltonian Dynamics by Neal (pdf)\nSlice sampling be Neal (pdf)\nBayesian interpolation by MacKay (pdf)\nBayesian online changepoint detectio by Adams and MacKay ()\nThe No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. by Matthew and Gelman (pdf)\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\nPrevious instances\n\n2019\n2017\n2018"
  },
  {
    "objectID": "courses/468.html",
    "href": "courses/468.html",
    "title": "SYST/OR 468. Applied Predictive Analytics",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2019\n\nFor course materials click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\nAnnouncements\n\n04/08/2019: HW3 posted, due date April 17.\n03/09/2019: HW2 posted, due date April 3.\n02/24/2019: Midterm is on March 20 (open book)\n02/24/2019: No class on March 13 (Spring Break)\n02/24/2019: H1 Due date is March 6\n10/10/2018: Check back regularly for announcements\n10/10/2018: First Class is on January 23\n\n\n\nCourse staff\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Eng Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: TBA\n\n\n\nOffie hours\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nTA: Jungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\nLectures\nLocation: Planetary Hall 206\nTimes: 4:30-7:10pm on Wednesday\n\n\nGrades\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\nTextbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nAdditional reading: List\n\n\n\nLinks\n\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb RF\nFacebook regrsssion\nYoutube DL\nUber Time Series\nEDA\nOverfitting\n2016 Election\n\n\n\nDeep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learning\nShort introduction into deep learning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\nPrevious instances\n\nSpring 2018"
  }
]