[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Google Scholar\nPolaris Project\nGeneology"
  },
  {
    "objectID": "research.html#highlights",
    "href": "research.html#highlights",
    "title": "Vadim Sokolov",
    "section": "highlights",
    "text": "highlights\n\nKramnik vs Nakamura or Bayes vs p-value, by Shiva Maharaj, Nick Polson and Vadim Sokolov. November 27, 2023. (pdf)"
  },
  {
    "objectID": "research.html#papers",
    "href": "research.html#papers",
    "title": "Vadim Sokolov",
    "section": "papers",
    "text": "papers\n\n\nBehnia, F., Karbowski, D., and Sokolov, V. (2023), “Deep generative models for vehicle speed trajectories,” Applied Stochastic Models in Business and Industry, 39, 701–719.\n\n\nBendre, S., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the probability of magnus carlsen reaching 2900,” Applied Stochastic Models in Business and Industry, 39, 372–381.\n\n\nPolson, N. G., and Sokolov, V. (2023b), “Generative AI for bayesian computation,” arXiv preprint arXiv:2305.14972.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023a), “Deep partial least squares for instrumental variable regression,” Applied Stochastic Models in Business and Industry.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023b), “Generative causal inference,” arXiv preprint arXiv:2306.16096.\n\n\nGupta, A., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the value of chess squares,” Entropy, MDPI, 25, 1374.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2023), “Quantum bayesian computation,” Applied Stochastic Models in Business and Industry.\n\n\nPolson, N., and Sokolov, V. (2023a), “Deep learning: A tutorial,” arXiv preprint arXiv:2310.06251.\n\n\nBaker, E., Barbillon, P., Fadikar, A., Gramacy, R. B., Herbei, R., Higdon, D., Huang, J., Johnson, L. R., Ma, P., Mondal, A., and others (2022), “Analyzing stochastic computer models: A review with opportunities,” Statistical Science, Institute of Mathematical Statistics, 37, 64–89.\n\n\nZha, Y., Parker, S. T., Foster, J. J., and Sokolov, V. (2022), “Housing market forecasting using home showing events,” arXiv preprint arXiv:2201.04003.\n\n\nSchultz, L., Auld, J., and Sokolov, V. (2022), “Bayesian calibration for activity based models,” arXiv preprint arXiv:2203.04414.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022a), “Deep partial least squares for iv regression,” arXiv preprint arXiv:2207.02612.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2022), “Quantum bayes AI,” arXiv preprint arXiv:2208.08068.\n\n\nSchultz, L., and Sokolov, V. (2022), “Deep learning gaussian processes for computer models with heteroskedastic and high-dimensional outputs,” arXiv preprint arXiv:2209.02163.\n\n\nWang, Y., Polson, N., and Sokolov, V. O. (2022), “Data augmentation for bayesian deep learning,” Bayesian Analysis, International Society for Bayesian Analysis, 1, 1–29.\n\n\nLey, H., Auld, J., Verbas, Ö., Weimer, R., Driscoll, S., Mohammadian, K., Golshani, N., Rahim, E., Shabanpour, R., Li, Z., and others (2022), Coordinated transit response planning and operations support tools for mitigating impacts of all-hazard emergency events, United States. Department of Transportation. Federal Transit Administration.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022b), “Feature selection for personalized policy analysis,” arXiv preprint arXiv:2301.00251.\n\n\nFotouhi, H., Mori, N., Miller-Hooks, E., Sokolov, V., and Sahasrabudhe, S. (2021), “Assessing the effects of limited curbside pickup capacity in meal delivery operations for increased safety during a pandemic,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2675, 436–452.\n\n\nZavareh, M., Maggioni, V., and Sokolov, V. (2021), “Investigating water quality data using principal component analysis and granger causality,” Water, MDPI, 13, 343.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2021), “Deep learning partial least squares,” arXiv preprint arXiv:2106.14085.\n\n\nSokolova, A. O., Marshall, C. H., Lozano, R., Gulati, R., Ledet, E. M., De Sarkar, N., Grivas, P., Higano, C. S., Montgomery, B., Nelson, P. S., and others (2021), “Efficacy of systemic therapies in men with metastatic castration resistant prostate cancer harboring germline ATM versus BRCA2 mutations,” The Prostate, 81, 1382–1389.\n\n\nBhadra, A., Datta, J., Polson, N., Sokolov, V., and Xu, J. (2021), “Merging two cultures: Deep and statistical learning,” arXiv preprint arXiv:2110.11561.\n\n\nHsu, Y. L., Jeng, C. C., Murali, P. S., Torkjazi, M., West, J., Zuber, M., and Sokolov, V. (2021), “Bayesian learning: A selective overview,” arXiv preprint arXiv:2112.12722.\n\n\nPolson, N., and Sokolov, V. (2020), “Deep learning: Computational aspects,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 12, e1500.\n\n\nHuang, X., Li, B., Peng, H., Auld, J. A., and Sokolov, V. O. (2020), “Eco-mobility-on-demand fleet control with ride-sharing,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 23, 3158–3168.\n\n\nSokolov, V. (2020), “Discussion of ‘multivariate generalized hyperbolic laws for modeling financial log-returns—empirical and theoretical considerations’,” Applied Stochastic Models in Business and Industry, John Wiley & Sons, 36, 777–779.\n\n\nDixon, M. F., Polson, N. G., and Sokolov, V. O. (2019), “Deep learning for spatio-temporal modeling: Dynamic traffic flows and high frequency trading,” Applied Stochastic Models in Business and Industry, 35, 788–807.\n\n\nWarren, J., Lipkowitz, J., and Sokolov, V. (2019), “Clusters of driving behavior from observational smartphone data,” IEEE Intelligent Transportation Systems Magazine, IEEE, 11, 171–180.\n\n\nPolson, N. G., and Sokolov, V. (2019), “Bayesian regularization: From tikhonov to horseshoe,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 11, e1463.\n\n\nWang, Y., Polson, N. G., and Sokolov, V. O. (2019), “Scalable data augmentation for deep learning,” arXiv preprint arXiv:1903.09668.\n\n\nSokolov, V., and Polson, M. (2019), “Strategic bayesian asset allocation,” arXiv preprint arXiv:1905.08414.\n\n\nLi, D., Liu, J., Park, N., Lee, D., Ramachandran, G., Seyedmazloom, A., Lee, K., Feng, C., Sokolov, V., and Ganesan, R. (2019), “Solving large-scale 0-1 knapsack problems and its application to point cloud resampling,” arXiv preprint arXiv:1906.05929.\n\n\nChen, H., Jajodia, S., Liu, J., Park, N., Sokolov, V., and Subrahmanian, V. (2019), “FakeTables: Using GANs to generate functional dependency preserving tables with bounded real data.” in IJCAI, pp. 2074–2080.\n\n\nNicholas G. Polson, V. O. S. (2019), “Deep learning,” Wiley StatsRef: Statistics Reference Online.\n\n\nSchultz, L., and Sokolov, V. (2018a), “Bayesian optimization for transportation simulators,” Procedia computer science, Elsevier, 130, 973–978.\n\n\nSchultz, L., and Sokolov, V. (2018b), “Deep reinforcement learning for dynamic urban transportation problems,” arXiv preprint arXiv:1806.05310.\n\n\nSokolov, V., Imran, M., Etherington, D. W., Karbowski, D., and Rousseau, A. (2018), “Effects of predictive real-time traffic signal information,” in 2018 21st international conference on intelligent transportation systems (ITSC), IEEE, pp. 1834–1839.\n\n\nSchultz, L., and Sokolov, V. (2018c), “Practical bayesian optimization for transportation simulators,” arXiv preprint arXiv:1810.03688.\n\n\nPolson, N., and Sokolov, V. (2017a), “Bayesian particle tracking of traffic flows,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 19, 345–356.\n\n\nSokolov, V., Larson, J., Munson, T., Auld, J., and Karbowski, D. (2017), “Maximization of platoon formation through centralized routing and departure time coordination,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2667, 10–16.\n\n\nSokolov, V. (2017), “Discussion of ‘deep learning for finance: Deep portfolios’,” Applied Stochastic Models in Business and Industry, 33, 16–18.\n\n\nAuld, J., Sokolov, V., and Stephens, T. S. (2017), “Analysis of the effects of connected–automated vehicle technologies on travel demand,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2625, 1–8.\n\n\nPolson, N. G., and Sokolov, V. (2017b), “Deep learning: A bayesian perspective.”\n\n\nVerbas, Ö., Sokolov, V., Auld, J., and Ley, H. (2017), “Time-dependent capacitated transit routing with real-time demand and supply data.”\n\n\nAuld, J., Hope, M., Ley, H., Sokolov, V., Xu, B., and Zhang, K. (2016b), “POLARIS: Agent-based modeling framework development and implementation for integrated travel demand and network and operations simulations,” Transportation Research Part C: Emerging Technologies, Pergamon, 64, 101–116.\n\n\nAuld, J., Karbowski, D., Sokolov, V., and Kim, N. (2016a), “A disaggregate model system for assessing the energy impact of transportation at the regional level,” in TRB 2016 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Jongryeol, J. (2016b), “Fuel saving potential of optimal route-based control for plug-in hybrid electric vehicle,” IFAC-PapersOnLine, Elsevier, 49, 128–133.\n\n\nJacquier, E., Polson, N., and Sokolov, V. (2016), “Sequential bayesian learning for merton’s jump model with stochastic volatility,” arXiv preprint arXiv:1610.09750.\n\n\nLarson, J., Munson, T., and Sokolov, V. (2016), “Coordinated platoon routing in a metropolitan network,” in 2016 proceedings of the seventh SIAM workshop on combinatorial scientific computing, Society for Industrial; Applied Mathematics, pp. 73–82.\n\n\nKarbowski, D., Kim, N., Auld, J., and Sokolov, V. (2016a), “Assessing the energy impact of traffic management and vehicle hybridisation,” International Journal of Complexity in Applied Science and Technology, Inderscience Publishers (IEL), 1, 107–124.\n\n\nPolson, N., and Sokolov, V. (2015), “Bayesian analysis of traffic flow on interstate i-55: The LWR model.”\n\n\nSokolov, V., Karbowski, D., Kim, N., and Auld, J. (2015), “Assessing the energy impact of traffic management and vehicle hybridization,” in 25th ITS annual meeting.\n\n\nLuo, Q., Auld, J., and Sokolov, V. (2015), “Addressing some issues of map-matching for large-scale, high-frequency GPS data sets,” in TRB 2015 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Rousseau, A. (2015), Vehicle energy management optimization through digital maps and connectivity, Argonne National Lab.(ANL), Argonne, IL (United States).\n\n\nRichey, A. S., Richey, J. E., Tan, A., Liu, M., Adam, J. C., and Sokolov, V. (2015), “Assessing the use of remote sensing and a crop growth model to improve modeled streamflow in central asia,” in AGU fall meeting abstracts, pp. H44F–08.\n\n\nSokolov, V., Karbowski, D., and Kim, N. (2014a), “Assessing the energy impact of traffic management and ITS technologies,” in 21st ITS world congress.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and others (2014), “GREET model: The greenhouse gases, regulated emissions, and energy use in transportation model,” Chicago, USA: Argonne National Laboratory.\n\n\nSokolov, V. O., Zhou, X., and Langlois, P.-A. (2014b), “A framework for arterial traffic flow modeling-POLARIS.”\n\n\nAuld, J., Hope, M., Ley, H., Xu, B., Zhang, K., and Sokolov, V. (2013), “Modelling framework for regional integrated simulation of transportation network and activity-based demand (polaris),” in International symposium for next generation infrastructure.\n\n\nAuld, J., Sokolov, V., Fontes, A., and Bautista, R. (2012), “Internet-based stated response survey for no-notice emergency evacuations,” Transportation Letters, Taylor & Francis, 4, 41–53.\n\n\nSokolov, V., Auld, J., and Hope, M. (2012), “A flexible framework for developing integrated models of transportation systems using an agent-based approach,” Procedia Computer Science, Elsevier, 10, 854–859.\n\n\nDatta, B. N., and Sokolov, V. (2011), “A solution of the affine quadratic inverse eigenvalue problem,” Linear Algebra and its Applications, North-Holland, 434, 1745–1760.\n\n\nPark, Y. S., Manli, E., Hope, M., Sokolov, V., and Ley, H. (2010), Fuzzy rule-based approach for evacuation trip demand modeling.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and GREET, M. (2010), “The greenhouse gases, regulated emissions, and energy use in transportation model,” Center for Transportation Research Argonne National Laboratory, Argonne, IL.\n\n\nDatta, B. N., Deng, S., Sokolov, V., and Sarkissian, D. (2009), “An optimization technique for damped model updating with measured data satisfying quadratic orthogonality constraint,” Mechanical Systems and Signal Processing, Academic Press, 23, 1759–1772.\n\n\nDatta, B. N., and Sokolov, V. (2009), “Quadratic inverse eigenvalue problems, active vibration control and model updating,” Applied and Computational Mathematics, 8, 170–191.\n\n\nSokolov, V. O. (2008), “Quadratic inverse eigenvalue problems: Theory, methods, and applications,” PhD thesis, Northern Illinois University.\n\n\nAre, S., Dostert, P., Ettinger, B., Liu, J., Sokolov, V., Wei, A., and Wiegand, K. (2006), “Reservoir model optimization under uncertainty.”\n\n\nKrukier, L., Pichugina, O., Sokolov, V., and Vulkov, L. (2006), “Numerical investigation of krylov subspace methods for solving non-symmetric systems of linear equations with dominant skew-symmetric part,” International Journal of Numerical Analysis and Modeling, University of Alberta, 3, 115–124."
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Vadim Sokolov",
    "section": "software",
    "text": "software\n\nPOLARIS: Designer. Developer. Transportation systems simulations framework (C++)\nGREET: Designer. Lead Developer. An implementation of The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (GREET) Model. (C#, .NET, SQLite). more then 800 unique users within first year of release, 2013\nMATCOM: Contributor. Distributed on CD with Numerical Linear Algebra and Applications, Second Edition book By Biswa Nath Datta, SIAM. (MATLAB)\nTRANSIMS: Contributor. An agent-based forecast software for modeling regional transport systems. (C++); 22,295 total downloads since 2006\nAdvanced Numerical Methods II: Sole Developer. Package for solving large scale control problems. (Mathematica); an experimental library that was not published"
  },
  {
    "objectID": "research.html#research-coverage",
    "href": "research.html#research-coverage",
    "title": "Vadim Sokolov",
    "section": "research coverage",
    "text": "research coverage\n\nDemystifying the future of connected and autonomous vehicles (Newswise\nArgonne wins grant to help transit agencies cope with emergencies (Chicago Tribune)\nArgonne will research how transportation systems should respond to natural hazards (WBEZ)\nWhat Happens When Developers, Scientists and Super-Computers Connect on Urban Design (Next City)\nUM wins $2.7M grant to study driverless cars (The Detroit News)\nUM teams with Argonne, Idaho national labs to study potential energy savings of connected vehicles (Michigan News)\nArgonne to study emergency response of Chicago transit (Chicago Sun Times)\nDesigning future cities (phys.org)\nUsing a Real Life SimCity to Design a Massive Development (Curbed Chicago)"
  },
  {
    "objectID": "index.html#current-teaching",
    "href": "index.html#current-teaching",
    "title": "Vadim Sokolov",
    "section": "Current Teaching",
    "text": "Current Teaching\n\nAI 600: Foundations of AI (page)"
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Vadim Sokolov",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise\n\nData science: Bayesian statistics, deep learning, reinforcement learning\nComplex Systems: Agent-based models, Bayesian optimization\nApplications: Urban systems modeling, digital twins"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Vadim Sokolov",
    "section": "Bio",
    "text": "Bio\nVadim Sokolov is an associate professor in the Systems Engineering and Operations Research Department at George Mason University. He works on building robust solutions for large scale complex system analysis, at the interface of simulation-based modeling and statistics. This involves, developing new methodologies that rely on deep learning, Bayesian analysis of time series data, design of computational experiments and development of open-source software that implements those methodologies. Inspired by an interest in urban systems he co-developed mobility simulator called Polaris that is currently used for large scale transportation networks analysis by both local and federal governments. Prior to joining GMU he was a principal computational scientist at Argonne National Laboratory, a fellow at the Computation Institute at the University of Chicago and lecturer at the Master of Science in Analytics program at the University of Chicago.\nHe has published in such leading statistics, mathematics and engineering journals, as the Annals of Applied Statistics, Transportation Research Part C, Linear Algebra and Its Applications as well as in Mechanical Systems and Signal Processing. He holds a PhD in computational mathematics from Northern Illinois University, and pursued graduate studies in statistics at the University of Chicago, while working at Argonne."
  },
  {
    "objectID": "courses/750.html",
    "href": "courses/750.html",
    "title": "OR 750. Bayesain Learning",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2021\n\nVideos\n\nCourse Material\n\nCourse Number: OR 750\nLocation: Zoom Thu 4:30-7:10pm\nInstructor: Vadim Sokolov\nOffice hours: by Appointment\nPrerequisites: Undergraduate Calculus, probability theory, statistics, computer programming skills (ideally R).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nThis is a graduate course on Bayesain learning. Although basics will be revisited, the pace will be swift so we can get to advanced topics as quickly as possible. This course details classical Bayesain techniques as well as modern approaches from both statistics and machine learning. We will consider some canonical examples of Bayesain analysis but will concentrate on modern Bayesain techniqes, computation and implementation, as well as modern applications. The course material will emphesize deriving and implementing methods over proving theoretical results.\n\nDuring weeks 11-15, this class will be run in a seminar mode. A student or the instructor will lead the discussion.\n\n\nTentative Schedule/List of Topics\n\nOverview: Conditional Probability, Bayes Rule, Bayesain inference, utility theory, distributions and tranformations\nHierarchical models: Bayesian regression, shrinkage (lasso, horseshoe)\nGaussian Process with applications in Bayesian Optimization – Tuning machine learning algorithms – Engineering model calibration\nAlgorithms – Markov Chain Monte Carlo (MCMC) – Expectation Maximization (EM) – Variational Bayes\nMarkov and Hidden Markov Models – Kalman Filter – Particle filter – Dynamic Linear Model – Structural Time Series Models\n\n\n\nGrading\nRubric: 30% HW, 10% Discussion, 60 % Final Project. No in-class examination.\nCutoffs: A: 90, B: 80, C: 70, F: &lt; 70\n\n\nOptional Texts\n\nPattern Recognition and Machine Learning by Bishop (page)\nMachine Learning: a Probabilistic Perspective by Murphy (page)\nAn Introduction to Bayesian Thinking by Clyde et al. (book)\nIntroduction to Statistical Thought by Lavine (book)\nThe Probability and Statistics Cookbook by Vallentin (page)\n\n\n\nPapers\n\nExchange Paradox by Christensen and Utts (pdf)\nInference for Nonconjugate Bayesian Models Using the Gibbs Sampler by Carlin and Polson (pdf)\nData Augmentation for Support Vector Machines by Polson and Scott (pdf)\nThe horseshoe estimator for sparse signals by Carvalho, Polson, Scott (pdf)\nDeep learning: A Bayesian perspective (pdf)\nBayesian regularization: From Tikhonov to horseshoe (pdf)\nSpatial Interaction and the Statistical Analysis of Lattice Systems by Besag (jstor)\nParticle learning and smoothing by Carlos M Carvalho, Michael S Johannes, Hedibert F Lopes, Nicholas G Polson (pdf)\nBayesian model assessment in factor analysis by Lopes and West (pdf)\nTracking epidemics with Google flu trends data and a state-space SEIR model by Dukic, Lopes and Polson (pdf)\nA statistical paradox by Lindley (pdf)\nThe philosophy of statistics by Lindley (pdf)\nThe Relevance Vector Machine by Tipping (pdf)\nBART: Bayesian additive regression trees by Hugh A. Chipman, Edward I. George, Robert E. McCulloch (arxiv)\nBayesian methods for hidden Markov models: Recursive computing in the 21st century by Scott (pdf)\nA modern Bayesian look at the multi‐armed bandit by Scott (pdf)\nBayesian analysis of computer code outputs: A tutorial by O’Hagan (paper)\nBayesian analysis of stochastic volatility models by Jacquier, Polson and Rossi (paper)\nOckham’s razor and Bayesian analysis by Jefferys and Berger (pdf)\nBayesian Learning for Neural Networks by Neal (pdf)\nMCMC Using Hamiltonian Dynamics by Neal (pdf)\nSlice sampling be Neal (pdf)\nBayesian interpolation by MacKay (pdf)\nBayesian online changepoint detectio by Adams and MacKay ()\nThe No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. by Matthew and Gelman (pdf)\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\nPrevious instances\n\nFall 2019\nFall 2017\nFall 2018"
  },
  {
    "objectID": "courses/664.html",
    "href": "courses/664.html",
    "title": "SYST/OR 664 CSI 674. Bayesian AI",
    "section": "",
    "text": "Spring 2025\n\nInstructor: Vadim Sokolov\n\nLocation: Engineering Building 2241\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nZoom Link: https://gmu.zoom.us/j/98895158802?pwd=3fGVsYOe332Rw7B2AaZkb2LmzRbXU2.1\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nSlides\nLecture Notes\n\nHWs: HW1, HW2, HW3,HW4,HW5\n\n\n\nContent and goals\nMany artificial intelligence problems involve modeling uncertainty. Bayesian probabilistic models represent uncertainty and dependencies between random variables using probability distributions. You will learn the set of rules of probability and computational algorithms to manipulate these distributions. Bayesian approach enhances the effectiveness of conventional AI techniques. This course summarizes various Bayesian-based models and the standard algorithms used with them, supplemented by instances of their practical use. We will discuss applications in science, engineering, economics, medicine, sport, and law. Students will learn the commonalities and differences between the Bayesian and frequentist approaches to statistical inference, how to approach a statistics problem from the Bayesian perspective, and how to combine data with informed expert judgment soundly to derive useful and policy-relevant conclusions. Assignments focus on applying the methods to practical problems.\n\n\nList of Topics\n\nUnit 1: Introduction: History of AI, Bayes approach to ML and AI. Probability and Bayes Rule.\nUnit 2: Utility and Decision Theory\nUnit 3: Bayesian Inference with Conjugate Pairs: Single Parameter Models\nUnit 4: Bayesian Hypothesis Tests\nUnit 5: Stochastic Processes\nUnit 6: Markov Chain Monte Carlo\n\nUnit 7: Bayesian Regression: Linear and Bayesian Trees\nUnit 8: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\nUnit 9: Bayesian Double Descent and Model Selection: Modern Approach to Bias-Variance Tradeoff\nUnit 10: Bayesian Neural Networks and Deep Learning\n\n\n\nSchedule\n\nJan 27: First Class\nFeb 17: We will not meet, I will post a video lecture\nMarch 3: HW3 due\nMarch 10: HW4 due\nMarch 10: No class (spring break)\nMarch 17: Midterm exam\nMarch 31: HW5 due\nApril 7: Project proposal due\nApril 14: HW6 due\nApril 28: Project Draft due\nMay 5: Last class, project presentations\nMay 12: Final project due\n\n\n\nAssignments\nHomework is due 11:59PM on the assigned due date. If you have extenuating circumstances, please contact me in advance, and I will consider giving you additional time to complete the assignment for partial credit. Assignments will be posted here and on Blackboard. Please submit your assignments through Blackboard.\nExams are take home and will be posted on Blackboard.\n\n\nPrerequisites\nThe formal listed prerequisite is OR 542 or STAT 544 or STAT 554 or equivalent.\n\nThe real prerequisites are:\n\nExperience with elementary data analysis such as scatterplots, histograms, hypothesis tests, confidence intervals, and simple linear regression.\nA calculus-based probability course - elementary probability theory, discrete and continuous probability distributions, probability mass and probability density functions, cumulative distribution functions, common parametric models such as the normal, binomial and Poisson distributions.\nExperience with a high-level programming language. We will use R, a programming language for data analysis, and STAN, a language for specifying and performing inference with Bayesian models.\nComfort with mathematical notation. We will not do proofs, but you will be expected to be comfortable following and doing mathematical derivations.\n\n\n\nComputing\nWe will use R, a powerful (free) statistical graphics and computing language, and STAN, an open-source, cross-platform engine for Bayesian data analysis that can be accessed from within R. Many of the exercises will require programming. RStudio is an integrated development environment for R. Some students prefer Python to R. You may use your preferred software as long as your solution is clear and I can understand what you did, but the solutions and examples will all be in R.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, midterm and final project.\n\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on Bb. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nSelected references\n\nBishop, Christopher M., Pattern Recognition and Machine Learning. Springer, Information Science and Statistics series, 2006.\nMacKay David J. C., Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.\nHoff, Peter D., A First Course in Bayesian Statistical Methods. Springer, 2009.\nComputer code is available for most of the examples in the book.\nGelman, A., Carlin, J., Stern, H., Dunson, D. B., Vehtari, A. and Rubin, D., Bayesian Data Analysis (3rd edition). CRC Press, 2013.\nReference tex. This comprehensive text has become the standard reference in Bayesian statistical methods. The hyperlink below contains reviews, exercises, data sets and software.\nMarin, Jean-Michel and Robert, Christian, Bayesian Essentials with R (2nd edition). Springer, 2014.\nSupplemental text (recommended): This recently published book provides comprehensive coverage of computational Bayesian statistics with a focus on conducting Bayesian analyses of real data sets. The range of topics covered is much more extensive than the Hoff text, and will serve as a useful supplement for readers interested in Bayesian treatment of topics not covered in this course, such as generalized linear models, capture-recapture experiments, time series and image analysis. R code and a solution manual are available.\n\nLee, Peter, Bayesian Statistics: An Introduction (4th edition), Wiley, 2012. Alternate text: The text by Peter Lee is accessible and may be helpful as an alternative treatment. Again, the hyperlink contains additional information, including exercises, solutions, errata and software.\n\n\n\nAdditional Resources\n\nWakefield, J. C., et al. “The evaluation of fibre transfer evidence in forensic science: a case study in statistical modelling.” Journal of the Royal Statistical Society Series C: Applied Statistics 40.3 (1991): 461-476. pdf\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\n\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\n\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\n\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/568.html",
    "href": "courses/568.html",
    "title": "SYST/OR 568. Applied Predictive Analytics - Mason Analytics MS",
    "section": "",
    "text": "George Mason University\nFall 2023\n\nCourse Material\nVideo Lectures\nTA: Raina Joy Saha (rsaha3 (at) gmu.edu)\nInstructor: Vadim Sokolov\n\nLocation: Krug Hall 7\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\nList of Topics\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesian Inference\nTime series forecasting (html notes, pdf notes)\n\n\n\nSchedule\n\nAug 21: First Class\nSept 4: Labor Day : University Closed\nSep 11: HW 1 Due\nSep 25: HW 2 Due\nOct 9: Fall Break (Classes Do Not Meet)\nOct 10: We have a class\nOct 16: HW 3 Due\nOct 23: In-class Midterm\nOct 30: Final Project Proposal\nNov 6: Hw 4 Due\nNov 20: Hw 5 Due\nNov 27: Last class, project presentations\nDec 3: Final Projects Due\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nOptional Textbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/41000/syllabus.html",
    "href": "courses/41000/syllabus.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "href": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/41000.html",
    "href": "courses/41000/41000.html",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvadim.sokolov@chicagobooth.edu\nTA: Ayman Moawad\naymoawad@uchicago.edu\nSyllabus\nDatasets \nAll examples\nCourse Textbook: OpenIntro Statistics pdf, tablet"
  },
  {
    "objectID": "courses/41000/41000.html#class-notes",
    "href": "courses/41000/41000.html#class-notes",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Class Notes",
    "text": "Class Notes\n\nSection 1: Introduction and Probability: notes, code\nSection 2: Statistics and Data: notes, code, epl, abtesting\nSection 3: Regression: notes, code, housing, mammals\nSection 4: Classification: notes, tree notes newfood, OJ, PGA, Tennis, logistic\nSection 5: AI and Deep Learning: notes, Examples: MNIST; NN for Circle Data, IMDB, Trump Tweets, trump-tweets.R"
  },
  {
    "objectID": "courses/41000/41000.html#hw",
    "href": "courses/41000/41000.html#hw",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "HW",
    "text": "HW\n\nHW1 (code)\nHW2 (code)\nHW3 (code)\nHW4 (code\nFinal Project (code)"
  },
  {
    "objectID": "courses/41000/41000.html#midterm",
    "href": "courses/41000/41000.html#midterm",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Midterm",
    "text": "Midterm\n\nQuestions\nQuestions\nMidterm 1\nMidterm 2\nMidterm 3\nMidterm 4\nMidterm 5\nMidterm 6\nMidterm 7\nMidterm 8\nMidterm 9\nMidterm 10\nMidterm 11"
  },
  {
    "objectID": "courses/41000/41000.html#r",
    "href": "courses/41000/41000.html#r",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "R",
    "text": "R\nI made a quick introduciton to R, video is here and here are the slides.\n\nTo get RStudio on you computer, you need to first download and install RStudio then you need to download and install R.\nFurther, I recommend doing those two courses on DataCamp.\n\nIntroduction to R\nIntroduction to Data\n\n\nIn addition, Booth offers access to an online R tutorial on LinkedinLearning. To access the training, go to: here and login using your CNetID and password. You may find it helpful to work through the following R courses offered on LinkedInLearning:\n\nCode Clinic: R with Mark Niemann-Ross\nR Statistics Essential Training with Barton Poulson\nUp and Running with R with Barton Poulson"
  },
  {
    "objectID": "courses/41000/41000.html#demos",
    "href": "courses/41000/41000.html#demos",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Demos",
    "text": "Demos\n\nPortfolio Variance\nBinomial\nNormal Distribution"
  },
  {
    "objectID": "courses/41000/41000.html#other-materials-and-links",
    "href": "courses/41000/41000.html#other-materials-and-links",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Other Materials and Links",
    "text": "Other Materials and Links\n\nAI in Tennis\nKraft-Heinz blog store\nKraft-Heinz WSJ\nFrito-Lay potato peeling\nTyson\nAnheuser-Busch InBev payments\nWalmart smart\nThe State of Data Science and MachineLearning\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb Random Forest\nFacebook regrsssion\nYoutube Deep learning\nUber: time series\nExploratory Data Analysis\nOverfitting\n2016 Election"
  },
  {
    "objectID": "courses/41000/41000.html#deep-learning",
    "href": "courses/41000/41000.html#deep-learning",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learnig\nShort introduction into deep leanring\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "courses/468.html",
    "href": "courses/468.html",
    "title": "SYST/OR 468. Applied Predictive Analytics",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2019\n\nFor course materials click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\nAnnouncements\n\n04/08/2019: HW3 posted, due date April 17.\n03/09/2019: HW2 posted, due date April 3.\n02/24/2019: Midterm is on March 20 (open book)\n02/24/2019: No class on March 13 (Spring Break)\n02/24/2019: H1 Due date is March 6\n10/10/2018: Check back regularly for announcements\n10/10/2018: First Class is on January 23\n\n\n\nCourse staff\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Eng Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: TBA\n\n\n\nOffie hours\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nTA: Jungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\nLectures\nLocation: Planetary Hall 206\nTimes: 4:30-7:10pm on Wednesday\n\n\nGrades\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\nTextbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nAdditional reading: List\n\n\n\nLinks\n\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb RF\nFacebook regrsssion\nYoutube DL\nUber Time Series\nEDA\nOverfitting\n2016 Election\n\n\n\nDeep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learning\nShort introduction into deep learning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\nPrevious instances\n\nSpring 2018"
  },
  {
    "objectID": "courses/610.html",
    "href": "courses/610.html",
    "title": "SYST/OR 610. Deep Leanring",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2022\n\n\n\n\n\nInstructor: Vadim Sokolov\nLocation and time: Aquia, room 347; 7:20-10pm Mondays\nOffice hours: By appointment\n\n\nDatacamp\nIf you are rusty on Python, I suggest you refresh your skills using Datacamp. Datacamp gave students in this class a free access to all of the courses. If you follow the link above you can get your free access using masonlive email. I also listed some of the Python courses I suggest #there.\n\n\nList of topics and tentative schedule\n\nBasics (Weeks 1-2)\n\nLinear Algebra: intro\nProbability: OpenIntro Ch 3\nGeneralized Linear Models: OpenIntro Ch 8,9\nPyTorch: PyTorch Basics\nFeed Forward Architectures: WHAT IS TORCH.NN REALLY?; Ripley Ch 5; Bishop Ch 3,4\n\nConvex Optimization (Weeks 3-4)\n\nBackpropagation and matrix derivatives\nStochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration): Bishop Ch 7, Goodfellow Ch 8\nSecond order methods: Bishop Ch 7\nADMM\nRegularization (l1, l2 and dropout): dropout paper, Godfellow Ch 7\nBatch normalization: paper\n\nConv Nets and Image Processing (Week 5): Goodfellow Ch 9\nRecurrent Nets and Sequential Data (Week 6): Good Fellow Ch 10, seq2seq, Pytorch eq2seq tutorial\nTheory of deep learning (Week 7): see theory section for the reading list\n\nUniversal approximators\nCurse of dimensionality\nKernel spaces\nTopology and geometry\n\nProbabilistic DL (Weeks 8-9) Langevin, MCMC, , VB\n\nConjugate distributions, exponential family Bishop: Ch 2\nModel choice\nHierarchical linear and generalize linear models (regression and classification): Bishop Ch 10\nModels for missing data (EM-algorithm)\nBayes computations (MCMC, Variational Bayes)\n\nAdditional Topics (Weeks 10-13)\n\nModel Visualization Tensorboard\nGenerative Models (normalizing flows, GANs, recurrent nets): NF Paper Tutorial; NF; \nAttention and Transformers attention paper\nDeep Reinforcement Learning DRL Tutorial\nBayesian Optimisation: Hyperparameter selection and parameter initialization Hyperopt\n\n\n\n\nData analysis projects\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 8 of the class you should have a team formed and data set + analysis problem identified. You need to submit a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\nYou will post results of your analysis on the class blog post. The final project will be graded on presentation, writing and analysis.\n\n\n\nGroup Work\nBoth projects and homework can be done in a groups of size of up to 3 people. You can change groups in between. If you do a homework in a gorup, it means that all of the members of the group do it individually and can consult with each other. You can also do 1 submission per group if you prefer. You can use “group” section of the piazza page to find teammates if you need any. If you need help finding a group, please email me.\n\n\nGrading\nEach hw is 10 points, project is 30.\n\nThis is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic.\n\n\nBooks\n\nPolson, Sokolov notes\nDive into Deep Learning link\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.\nRipley, Brian D. Pattern recognition and neural networks. Cambridge university press, 2007.\nBishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.\n\n\n\nPer Topic Resources\n\nArchitectures\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\nOptimization\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\nTheory\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\nReinforcement Learning\nOptimization Models - Truth - Yet\n\n\nBayesian DL\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\nPractical Tricks\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\nOther Resources\n\nAdditional Reading List\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\nBlogs\n\nPapers with code\nSecurity\nUnsupervised learning\nCybersecurity\nOpt Visualization\nAI and Memory Wall\n\n\n\nVideos\n\nDeep Energy\nDL Summer school 2015\nDL Representations\nPyData 2017\n\n\n\nOther courses with good web presence\n\nStanford’s CS231n\nStanford’s STATS385\nfast.ai\nNando de Freitas’ course on machine\nUC Berkeley Stat241B\nMIT\nUdacity DRL\n\n\n\nTools\n\nTensorFlow\nKeras\nTF Playground\nSony\nSnakeViz python profiler\nPyTorch\nTS Stan Examples\nOpenAI Glow\n\n\n\nMisc Links\n\nPytorch resources (a curated list of tutorials, papers)\nIs artificial intelligence set to become art’s next medium?"
  },
  {
    "objectID": "courses/664/final.html",
    "href": "courses/664/final.html",
    "title": "Final Project Guidelines for SYST/OR 664 CSI 674. Bayesian Inference and Decision Theory",
    "section": "",
    "text": "The final project can be done in a group (up to 3) or as is an individual project. You have a lot of freedom in choosing a topic for your final project. The only criterion is that it deeply involves applying Bayesian data analysis to a real-world problem. You choose a dataset, an interesting question about it, and address it with Bayesian modeling.\nThere are four milestones:\n\nProject proposal (due April 7)\nProject Presentation Draft (due April 26)\nPresentation (April 29).\nFinal Project Report (due May 5)\n\nThe project proposal is to have 3 paragraphs on\n\nQuestion you are trying to answer and why this question is interesting or important. How your analysis changes decision making process? For example if you are forecasting demand for a product, how would the company change its production process if they had a better forecast?\nData you are using. If you are using a dataset, where did it come from?\nWhat model are you going to use for the data analysis?\n\nThe proposal should be up to 1 pages long.\nThe report is up to 5 pages long, excluding figures or references. If you wish to add more materia, put it into appending. However, the main 5-page body should be self-contained and complete. The report should be written in a clear and concise manner, and should be well-organized. The report should include the following sections:\n\nIntroduction: Clear description of the problem. Describe the problem you are trying to solve, why it is important or useful, and summarize any important pieces of prior work that you are building upon.\nDataset: Clear description/visualization of the data. Describe the dataset or datasets you are working with. Show examples from the datasets. If you collected or constructed your own dataset, explain the process you used to collect the images and labels, and why you made the choices you did in the data collection process.\nModels/Methods: Clear and thorough description of statistical analysis. Describe the method you are using; this may also contain parts of the implementation of your model, loss function, or other components along with sanity checks to ensure that those components are correctly implemented.\nExperiments: Clear and thorough interpretation of results. Describe the experiments you did, and key results and figures that you obtained. This may interleave explanations of the experiments you run and figures you generate as a result of those experiments.\n\nYou should turn in a .pdf file containing your final report, together wih the notebooks/markdowns containing all the code and the generated results (tables, figures etc) that are included in the report. You must run all cells in your notebook to receive credit; we will not rerun your notebook.\nBoth project and presentation will be evaluated on technical depth, and writing/presentation quality (50/50). Some points to keep in mind\n\nVisualizations should be used judiciously to report findings. Do not overload reader with tables/figures.\nFigures should be appropriately labeled: x-axis, y-axis, legend, and title.\nFigures and tables should be numbered and referenced properly in the write-up.\nOutput of posterior predictive checks should be properly interpreted not merely stated\nAt the end, you need to connect your results to the initial question of investigation.\nNo more than 10 lines of text per slide."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Full CV"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vadim Sokolov",
    "section": "Education",
    "text": "Education\n\nPh.D., 2008, Computational Mathematics, Northern Illinois University\nDiploma (summa cum laude), 2004, Applied Mathematics, Rostov State University\nGraduate Studies in Statistics, 2013-2014, University of Chicago"
  },
  {
    "objectID": "cv.html#positions",
    "href": "cv.html#positions",
    "title": "Vadim Sokolov",
    "section": "Positions",
    "text": "Positions\n\nAssociate Professor, Systems Engineering and Operations Research, George Mason University, 2023-present\nAsistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nVisiting Assistant Professor, Statistics,The University of Chicago Booth School of Business, 2019-2023\nAssistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nLecturer, Master of Science in Analytics, University of Chicago, 2015-2016\nFellow, Computation Institute, University of Chicago, 2014-2016\nPrincipal Computational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2013-2016\nComputational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2008-2013"
  },
  {
    "objectID": "msai/msai.html",
    "href": "msai/msai.html",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "",
    "text": "Application Information\nOfficial Program Catalog\nContact Information: cecmsai@gmu.edu (Vadim Sokolov, program director)\nSchedule an in-perosn meeting: Calendly"
  },
  {
    "objectID": "msai/msai.html#program-overview",
    "href": "msai/msai.html#program-overview",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Program Overview",
    "text": "Program Overview\nIn today’s rapidly evolving technological landscape, artificial intelligence has emerged as the most transformative force across industries, government, and society. Recent research reveals a critical challenge: while 89% of organizations are piloting or investing in AI developments, only 6% of employees feel very comfortable using AI in their roles. This widening gap between organizational adoption and workforce readiness represents one of the most significant barriers to innovation and economic growth.\nThe AI skills shortage has accelerated at an unprecedented rate, becoming the largest and fastest-growing tech skills gap in over 15 years. Almost twice as many technology leaders (51%) now report suffering an AI skills shortage compared to 28% just 18 months ago-an 82% jump that far outpaces other technical skills deficits. Despite this urgent need, only four in ten organizations are upskilling their current staff.\nThe disparity in AI skills access is particularly concerning. While 75% of companies have adopted AI technology, just one-third of employees received AI training in the past year. George Mason University’s Master of Science in Artificial Intelligence program directly addresses these challenges by preparing professionals with comprehensive knowledge and practical skills across the entire AI operations pipeline. Launching in Fall 2025, this innovative 30-credit program equips students to:\n\nIdentify real-world problems that benefit from AI solutions\nImplement secure and scalable AI systems across diverse computing platforms\nDevelop, train, and tune AI models to optimize performance\nAddress ethical considerations and evaluate AI systems against risk frameworks\nTranslate technical details into actionable insights for diverse audiences\n\nOur curriculum balances technical mastery with ethical responsibility, preparing graduates who can not only build AI technologies but also govern their responsible implementation across industries, government agencies, and public services."
  },
  {
    "objectID": "msai/msai.html#learning-journey",
    "href": "msai/msai.html#learning-journey",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Learning Journey",
    "text": "Learning Journey\nThe MS in Artificial Intelligence program consists of 30 credit hours, with 18 credits of core coursework and 12 credits from specialized topic areas. Students will gain both theoretical foundations and hands-on experience through a carefully designed sequence of courses.\n\nCore Courses\nAII 600: Foundations and Practice of Machine Learning for Artificial Intelligence (3 credits) Introduces the foundations of machine learning encountered in AI with emphasis on practical aspects. Students learn to analyze complex datasets, identify problems benefiting from AI solutions, and implement solutions using appropriate libraries and computing platforms while evaluating them against AI risk frameworks.\nAII 601: Planning and Decision Making for Intelligent Agents (3 credits) Explores planning domain representations, optimal search-based strategies, and methods for effective planning under uncertainty. Students master Markov Decision Processes for decision making, reinforcement learning methods, temporal difference learning, and policy gradient algorithms-essential components of modern robotics and game-playing AI systems.\nAII 602: Foundations and Practice of Deep Learning for Artificial Intelligence (3 credits) Provides theoretical motivation and practical implementation of neural networks and deep learning. Students master designing, training, fine-tuning, and monitoring deep networks, including explainability. The course covers supervised, self-supervised, and unsupervised learning approaches with applications in computer vision, natural language processing, deep reinforcement learning, and generative AI.\nAII 603: Engineering Artificial Intelligence Systems and Pipelines (3 credits) Delivers hands-on experience building, deploying, and evaluating large-scale AI technologies across industry sectors. Through team-based projects, students develop scalable API solutions for embedded, edge, and cloud computing platforms, preparing them for collaborative industry environments.\nGBUS 662: Management of IT (3 credits) Covers strategic, economic, and managerial aspects of managing an organization’s IT assets. Students learn to assess business value of IT within organizational structure and strategy, with discussions on management of IT infrastructure.\nME 576: AI: Ethics, Policy, and Society (3 credits) Examines pressing ethical and policy issues in AI, including transparency, privacy, surveillance, misinformation, fairness, algorithmic bias, justice, equity, trust, and labor practices. Students explore these topics through cutting-edge use cases and current events, developing frameworks for responsible AI governance.\n\n\nTopic Areas\nStudents must select at least one course from each of the following four topic areas, with advisor approval:\n1. Artificial Intelligence Policy, Ethics, and Society Sample courses:\n\nAIT 679: Law and Ethics of Big Data\nBIOD 760: National Security Technology and Policy\nGCP 604: New Technologies in the Global Economy\nME 575: AI Design and Deployment Risks\n\n2. Advanced Artificial Intelligence Sample courses:\n\nAIT 616: Interactive Machine Learning and Artificial Intelligence\nAIT 726: Natural Language Processing with Deep Learning\nOR 664: Bayesian Artificial Intelligence\nOR 774: Reinforcement Learning\n\n3. Scalable and Secure Artificial Intelligence Infrastructures Sample courses:\n\nAIT 670: Cloud Computing Security\nAIT 687: IoT and Edge Systems\nECE 554: Machine Learning for Embedded Systems\nCS 695/SWE 699: AI Safety and Assurance\n\n4. Use-inspired Artificial Intelligence Sample courses:\n\nAIT 636: Interpretable Machine Learning\nCYSE 689: Artificial Intelligence Methods for Cybersecurity\nSTAT 646: Probabilistic Machine Learning\nHAP 774: Artificial Intelligence in Health"
  },
  {
    "objectID": "msai/msai.html#program-highlights",
    "href": "msai/msai.html#program-highlights",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Program Highlights",
    "text": "Program Highlights\n\nExpert Faculty\nLed by the College of Engineering and Computing at George Mason University, this program brings together world-class faculty with extensive experience in AI research and application. Our instructors include professors from Computer Science, Statistics, Operations Research, Cyber Security, Public Policy who have over many years of combined teaching and research experience in AI and related fields. Many have published extensively in peer-reviewed journals specific to their disciplinary areas and AI, and serve as leaders in national organizations focused on artificial intelligence.\nThe program draws on interdisciplinary expertise from multiple GMU colleges, including the School of Computing, the College of Humanities and Social Sciences, the Schar School of Policy and Government, and the Donald G. Costello College of Business. This collaborative approach ensures students benefit from diverse perspectives on AI’s technical foundations, business applications, and societal impacts.\nFaculty members like Dr. Jana Kosecka specialize in computer vision and machine learning, while Dr. Zhengdao Wang focuses on intersection of AI and cybersecurity. The program also features experts in generative AI, differential privacy, and statistical machine learning like Dr. Anand Vidyashankar and Dr. Daniel Barbara, and researchers in Bayesian statistics and deep learning like Dr. Vadim Sokolov.\n\n\nIndustry Connections\nGeorge Mason University’s strategic location in Northern Virginia-a hub for technology innovation and government agencies-provides unparalleled opportunities for industry engagement. The College of Engineering and Computing maintains robust industry partnerships that support student success through:\n\nResearch collaborations with faculty on cutting-edge AI problems\nAccess to industry mentors and guest speakers\nOpportunities for internships and job placement\nCollaborative problem-solving through Engineering Artificial Intelligence course projects\n\nRecently, George Mason University was awarded a $1 million grant to establish the nation’s first Center for AI Innovation for Economic Competitiveness (CAIIEC), focused on helping small and medium enterprises adopt AI technologies. The university has also launched an AI-in-Gov Faculty Fellows Program to advance responsible, impactful use of AI in the public sector.\nThese initiatives, along with GMU’s Industry Partner Program, create a vibrant ecosystem where students can connect with potential employers and gain real-world experience implementing AI solutions. This network provides graduates not only with technical skills but also with professional relationships that accelerate their careers in the rapidly evolving field of artificial intelligence.\nBy joining GMU’s Master of Science in Artificial Intelligence program, you’ll become part of a community dedicated to addressing the AI skills gap while developing the next generation of ethical, innovative AI leaders."
  },
  {
    "objectID": "msai/msai.html#faq",
    "href": "msai/msai.html#faq",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "FAQ",
    "text": "FAQ\n\nIn this an in-person program? For the 2025-2026 academic year, the program will be offered in-person. The program may be offered online in the future. Although some of the courses may be offered online, the program is not designed to be fully online.\nIs there is a part-time option? The program is designed to be completed in 1.5-2 years, if you take 3-4 courses per semester (full-time). Students can take courses at their own pace, but the program is not designed to be fully self-paced. Note, that some of the courses are offered only once a year\nI work full-time, can I do this program? Yes, the program is designed to accommodate working professionals. The courses are offered in the evenings (4:30-7:10pm and 7:20-10pm), allowing you to balance your work and study commitments.\nWhat is the cost of the program? The tuition and fees for the program are subject to change. For the most up-to-date information, please visit the George Mason University tuition and fees page.\nWhat is the application process? The application process for the program is managed by the George Mason University Office of Admissions. For detailed information on how to apply, please visit the George Mason University admissions page or the MS in AI admissions page.\nMy background in math, stats and cs is weak, will I be able to succeed in this program? The program is designed to accommodate students with diverse backgrounds. However, some foundation in mathematics, statistics, and computer science will be rfequired for your success in the program. Prior to starting the prgram, each student will go through the assesment process to determine their readiness for the program. Think of this as an informal math-placement test. The assessment will include a review of your knowledge in mathematics, statistics, and computer science, including programming languages (Python, R, etc.), data structures, algorithms, and linear algebra. Based on the results, we will recommend to take additional non-credit and free short courses or complete self-study materials to strengthen your foundation in these areas. Further, every student will be taking the foundations in AI course (AII 600) in the first semester, which will cover the foundations of AI and machine learning. This course is designed to help students build a strong foundation in AI and machine learning concepts, regardless of their prior experience."
  },
  {
    "objectID": "msai/msai.html#usefule-links",
    "href": "msai/msai.html#usefule-links",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Usefule Links",
    "text": "Usefule Links\n\nBehind the Curtain: A white-collar bloodbath\n‘Are We All Doomed?’ The CEO of Fiverr Says AI Is Definitely Taking Your Job. Here’s What to Do About It.\nGenerative AI and the future of work in America\nAI Skills Gap: The Biggest Tech Skills Shortage in 15 Years\nGoldman Sachs says generative A.I. could impact 300 million jobs — here’s which ones"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Nguyen Engineering Building MS 4A6\nOffice: 2242\nFairfax, VA, 22030\nPhone: 703 993 4533\nvsokolov@gmu.edu"
  },
  {
    "objectID": "courses/600.html",
    "href": "courses/600.html",
    "title": "AII 600. Foundations and Practice of Machine Learning for Artificial Intelligence. MS in AI",
    "section": "",
    "text": "George Mason University\nFall 2025\n\nCourse Material\n\nTextbook\n\nCanvas HW Submission\n\nInstructor: Vadim Sokolov\n\nLocation: Engineering Building, Room 2608\nTime: Wednesdays 7:20 - 10:00 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to Canvas\n\n\nContent and goals\nIntroduces the foundations of machine learning encountered in AI with emphasis on practical aspects. Students learn to analyze complex datasets, identify problems benefiting from AI solutions, and implement solutions using appropriate libraries and computing platforms while evaluating them against AI risk frameworks.\n\n\nList of Topics\n\nProbability and Bayes\nModeling using known distributions\nFrequentist and Bayesian Inference\nRegression, Classification\nMixture of expert models\nTree-based methods\nDecision Making\nAB Testing\nTemporal data and forecasting\nDeep Learning\nOptimisation and Regularization\nModel Selection (Bias-Variance, Double Descent)\nAI Risk Frameworks\n\n\n\nSchedule\n\nAug 27: First Class\nSep 10: HW 1 Due\nSep 17: HW 2 Due\nSep 24: HW 3 Due\nOct 1: HW 4 Due\nOct 8: In-class Midterm\nNov 5: Final project proposal due\nNov 7: HW 5 Due\nNov 21: HW 6 Due\nDecember 3: Final Project Presentations (last class)\n\n\n\nCase Studies\n\nSelf-Driving Cars and the Uber Fatal Crash (2018) Focus: Safety, liability, trust in AI, limitations of perception models Key Questions:\n\nWhat caused the failure in perception?\nWho is responsible—the AI developer, the safety driver, or Uber?\nShould fully autonomous driving be paused until X?\n\nCOMPAS Algorithm in Criminal Justice Focus: Algorithmic bias, fairness, transparency Key Questions:\n\nWhy did COMPAS exhibit racial bias?\nCan AI be truly unbiased?\nShould such models be used in sentencing?\n\nAI in Healthcare: IBM Watson for Oncology Focus: Promise vs reality, hype cycle, trust in clinical AI Key Questions:\n\nWhy did Watson fail to meet expectations?\nWhat lessons can we learn for future medical AI systems?\n\nDeepfakes and Synthetic Media Focus: Misinformation, regulation, detection vs creation arms race Key Questions:\n\nWho should be held accountable for deepfake misuse?\nAre technical detection solutions sufficient?\n\nChatGPT and Large Language Models in Education Focus: Plagiarism, learning integrity, productivity vs dependence Key Questions:\n\nShould universities ban or embrace AI tools in coursework?\nHow do we ensure critical thinking is preserved?\n\nAI in Hiring: Amazon’s Recruitment Tool Bias Focus: Gender bias, explainability, data-driven discrimination Key Questions:\n\nWhy did the hiring algorithm become biased?\nHow should companies audit AI systems for fairness?\n\nAI in Warfare: Lethal Autonomous Weapons (LAWS) Focus: Ethics, accountability, international law Key Questions:\n\nShould AI be allowed to make life-and-death decisions?\nWhat governance models could prevent misuse?\n\nAI for Social Good: Predictive Models for Disease Outbreaks (e.g., BlueDot and COVID-19) Focus: Success stories, limitations, data privacy Key Questions:\n\nWhy was BlueDot successful in early COVID detection?\nHow can similar models be scaled responsibly?\n\n\nFormat:\n\nQuick Context (5 min)\nSmall Group Brainstorm (10 min)\nRound Table Reporting and debate (15 min)\nInstructor Wrap-Up + Reflection (15 min)\n\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 6 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using Python or R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn Python and R.\nA great way to start learning is do a data camp or coursera course.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nFinal Project\n\nIn-class presentation (10 min)\nFinal report (up to 10 pages)\n\nApply statistical learning methods to a real world problem (prediction, hypothesis testing). You can use traditional tabular data to train and test the model. If you want to use LLM or CNN, you can use less structured data.\nA good project will have the following components:\n\nFormulate the hypothesis (what is the question you are trying to answer?)\nBuild a model (what is the model you are using?)\nEvaluate the model (how do you know that the model is doing the right thing?)\nModel selection and tweaking (use your evaluation criteria to select a model and tweak it if needed)\nDiscuss the results: what did you learn regarding the hypothesis? Are there any limitations? Are there any risk involved in using the model?\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "index.html#book-on-bayes-ai",
    "href": "index.html#book-on-bayes-ai",
    "title": "Vadim Sokolov",
    "section": "Book on Bayes AI",
    "text": "Book on Bayes AI\nDiscover the intersection of Bayesian statistics, artificial intelligence, and deep learning in our new book, Bayes, AI and Deep Learning: Foundations of Data Science. Co-authored by Nick Polson and Vadim Sokolov, this book offers an accessible yet rigorous journey through the core ideas shaping modern data science. Drawing on years of teaching experience with both business and engineering audiences, we blend intuitive explanations, real-world case studies, and hands-on exercises to bridge theory and practice. Whether you’re a manager seeking to leverage AI for strategic advantage or an engineer building intelligent systems, you’ll find practical insights into topics ranging from probability and Bayesian inference to neural networks and large language models. Join us as we explore how these transformative tools are revolutionizing industries—from personalized medicine to urban systems—and learn how to harness their power for your own projects. Read more and explore the full table of contents."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html",
    "href": "courses/caio/topic-overview-caio.html",
    "title": "Machine – Learning Essentials",
    "section": "",
    "text": "Online: 2 Weeks (14 Days)\nEmail: vsokolov@gmu.edu\nPhone: 703 993 4533\nCourse Textbook: Bayes, AI and Deep Learning by Nick Polson and Vadim Sokolov. The book is to be published by Chapman & Hall/CRC in 2026. Available for free online."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-purpose",
    "href": "courses/caio/topic-overview-caio.html#topic-purpose",
    "title": "Machine – Learning Essentials",
    "section": "Topic Purpose",
    "text": "Topic Purpose\nThe purpose of this topic is to introduce participants to the foundational concepts of artificial intelligence and data-driven decision making. Participants will develop a working understanding of probability, statistical modeling, and modern AI techniques—equipping them to lead AI initiatives, evaluate AI investments, and communicate effectively with technical teams."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-overview",
    "href": "courses/caio/topic-overview-caio.html#topic-overview",
    "title": "Machine – Learning Essentials",
    "section": "Topic Overview",
    "text": "Topic Overview\nThis module takes executives on a journey from the fundamentals of probability and uncertainty through statistical modeling to the cutting edge of modern AI. Rather than focusing on mathematical derivations, we emphasize intuition, real-world applications, and business implications. Through compelling case studies—from wrongful convictions caused by probability errors to the Netflix Prize’s lessons about model complexity—participants will learn to think probabilistically about business decisions. The module culminates in a hands-on project where participants build an AI agent using Cursor IDE, directly experiencing how data, models, and AI agents work together to solve business problems."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-objectives",
    "href": "courses/caio/topic-overview-caio.html#topic-objectives",
    "title": "Machine – Learning Essentials",
    "section": "Topic Objectives",
    "text": "Topic Objectives\nUpon completion of this topic, you should understand and be able to:\n\nApply probabilistic thinking to business decisions under uncertainty\nRecognize common probability fallacies (prosecutor’s fallacy, base rate neglect) and their business implications\nUnderstand the trade-offs between model accuracy, complexity, and business value\nInterpret regression models and explain their predictions to stakeholders\nEvaluate when AI/ML solutions are appropriate versus traditional statistical approaches\nBuild a simple AI agent that combines data analysis with natural language interaction\nLead informed conversations with data science and AI teams"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#course-approach",
    "href": "courses/caio/topic-overview-caio.html#course-approach",
    "title": "Machine – Learning Essentials",
    "section": "Course Approach",
    "text": "Course Approach\nThis topic combines asynchronous learning (recorded lectures, readings, discussion boards) with synchronous sessions (live Zoom calls) and hands-on practice. The approach emphasizes:\n\nCase-based learning: Each concept is grounded in real-world examples—from legal cases to sports analytics to retail pricing\nBusiness-first perspective: Technical concepts are always connected to business decisions and outcomes\nProgressive building: Each module builds on the previous, culminating in an integrated final project\nPeer learning: Discussion boards encourage sharing experiences and learning from diverse industry perspectives\nApplied practice: The final project provides hands-on experience building an AI-powered analytics tool"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#time-commitment",
    "href": "courses/caio/topic-overview-caio.html#time-commitment",
    "title": "Machine – Learning Essentials",
    "section": "Time Commitment",
    "text": "Time Commitment\nThis topic will require approximately 10 hours of work to complete:\n\n\n\nActivity\nHours\n\n\n\n\nRecorded Lectures (6 lectures × 45 min)\n4.5\n\n\nLive Zoom Sessions (3 sessions × 1 hr)\n3.0\n\n\nReading\n1.5\n\n\nDiscussion Boards\n1.0\n\n\nFinal Project\n2.0\n\n\nTotal\n12.0"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#schedule",
    "href": "courses/caio/topic-overview-caio.html#schedule",
    "title": "Machine – Learning Essentials",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nDay\nActivities\n\n\n\n\nDay 1-2\nModule 1 lectures available; begin readings on probability and Bayes rule\n\n\nDay 3\nDiscussion Board 1 opens\n\n\nDay 4\nZoom Session 1: Kick-off + Cursor IDE Hands-on (1 hr)\n\n\nDay 5-6\nModule 2 lectures available; readings on statistics and regression\n\n\nDay 7\nDiscussion Board 2 opens\n\n\nDay 8\nZoom Session 2: Mid-point Check-in (1 hr)\n\n\nDay 9-10\nModule 3 lectures available; readings on NLP and AI agents\n\n\nDay 11\nDiscussion Board 3 opens\n\n\nDay 12-13\nFinal Project work time\n\n\nDay 14\nZoom Session 3: Wrap-up + Final Project Presentations (1 hr)"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#discussion-boards",
    "href": "courses/caio/topic-overview-caio.html#discussion-boards",
    "title": "Machine – Learning Essentials",
    "section": "Discussion Boards",
    "text": "Discussion Boards\n\nDiscussion Board 1: Decision-Making Under Uncertainty\nOpens Day 3 | Due Day 7\n“Consider a strategic decision your organization recently faced (or is currently facing) involving uncertainty. Describe the decision and identify:\n\nWhat were the key uncertain factors?\nHow was probability or likelihood assessed (formally or informally)?\nReflecting on the Ellsberg paradox and Kelly criterion, how might a more systematic probabilistic approach have changed the decision-making process?\n\nRespond to at least two peers’ posts with constructive suggestions.”\n\n\nDiscussion Board 2: Predictive Models in Business\nOpens Day 7 | Due Day 11\n“The Netflix Prize awarded $1 million for a 10% improvement in recommendation accuracy, yet Netflix never fully implemented the winning algorithm—it was too complex and expensive to deploy, and by then, streaming had changed the business model entirely.\nReflecting on this case and the regression concepts from this module:\n\nIdentify a business process in your organization where a predictive model could be applied. What decisions would it inform?\nWhat would happen if the model’s predictions were inaccurate 20% of the time? 40% of the time? How would this affect business outcomes and trust in the system?\nDiscuss the trade-off: Is a highly accurate but complex/expensive model always better than a simpler, ‘good enough’ model? What factors would you consider when making this decision?\n\nRespond to at least two peers’ posts, particularly focusing on whether you agree with their assessment of the accuracy-complexity trade-off.”\n\n\nDiscussion Board 3: AI Agents in the Enterprise\nOpens Day 11 | Due Day 14\n“AI agents are increasingly being deployed in business contexts. Describe a workflow or process in your organization that could potentially be automated or augmented by an AI agent. Address:\n\nWhat tasks would the agent perform?\nWhat data or tools would it need access to?\nWhat guardrails or human oversight would be necessary?\nWhat risks or concerns would need to be addressed before deployment?\n\nRespond to at least two peers’ posts.”"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#live-zoom-sessions",
    "href": "courses/caio/topic-overview-caio.html#live-zoom-sessions",
    "title": "Machine – Learning Essentials",
    "section": "Live Zoom Sessions",
    "text": "Live Zoom Sessions\n\nZoom Session 1: Kick-off + Cursor Hands-on\nDay 4 | 1 hour\n\nWelcome and module overview (15 min)\nHands-on: Setting up Cursor IDE and using coding agents (30 min)\nQ&A on probability concepts from Module 1 (15 min)\n\nPreparation: Install Cursor IDE before the session (instructions provided in cursor-setup-guide.md)\n\n\nZoom Session 2: Mid-point Check-in\nDay 8 | 1 hour\n\nReview of statistical modeling concepts (20 min)\nLive demo: Building a simple regression model with Cursor (25 min)\nDiscussion of final project requirements (15 min)\n\nPreparation: Complete Module 2 lectures and readings\n\n\nZoom Session 3: Wrap-up + Final Project Presentations\nDay 14 | 1 hour\n\nBrief Modern AI recap (10 min)\nFinal project presentations/demonstrations (35 min)\nCourse wrap-up and next steps for AI leadership (15 min)\n\nPreparation: Complete final project; prepare 2-3 minute demonstration"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#assignment-final-project",
    "href": "courses/caio/topic-overview-caio.html#assignment-final-project",
    "title": "Machine – Learning Essentials",
    "section": "Assignment: Final Project",
    "text": "Assignment: Final Project\n\nOrange Juice Pricing Analytics Agent\nBusiness Problem: You are a pricing analyst at a retail chain. Management wants to optimize orange juice pricing and promotional strategies. Build an AI agent that can answer business questions about pricing decisions using historical sales data and a predictive model.\nDataset: Dominick’s Orange Juice Dataset\n\nWeekly sales data for orange juice brands (Tropicana, Minute Maid, Dominick’s)\nVariables: sales volume, price, advertising features, brand\n~28,000 observations across multiple stores\n\nModel: Linear Regression with Interactions\n\nPredict sales volume based on price, advertising, and brand\nCapture how price sensitivity varies by brand\n\nYour Agent Must Answer These Business Questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nDeliverables:\n\nPython code files in Cursor IDE (using provided template)\nWorking agent that answers the 5 business questions above\n1-page summary: What did you learn about OJ pricing? What surprised you?\n2-3 minute demo during Zoom Session 3\n\nEvaluation Criteria:\n\nFunctionality: Agent loads data, builds model, and responds to queries\nBusiness Relevance: Clear connection between model outputs and business decisions\nDocumentation: Clear explanation of approach and results\n\nSee Final Project Guide for detailed step-by-step instructions."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#reading-materials",
    "href": "courses/caio/topic-overview-caio.html#reading-materials",
    "title": "Machine – Learning Essentials",
    "section": "Reading Materials",
    "text": "Reading Materials\n\nModule 1: Probability as a Language of Uncertainty\nRequired Reading (from course textbook):\nChapter 1: Probability and Uncertainty\n\nOpening sections through “Kolmogorov Axioms”\nSection: “Conditional, Marginal and Joint Distributions”\nExample: Salary-Happiness\n\nChapter 2: Bayes Rule\n\nSection: “Intuition and Simple Examples”\nExample: Sally Clark Case\nExample: Nakamura’s Alleged Cheating\n\nChapter 4: Utility, Risk and Decisions\n\nSection: “Expected Utility”\nExamples: Saint Petersburg Paradox, Kelly Criterion, Ellsberg Paradox\nSection: “Decision Trees” (including Medical Testing and Mudslide examples)\n\nSupplemental Reading (online):\n\nDid a US Chess Champion Cheat? - Chicago Booth Review (Bayesian analysis, prosecutor’s fallacy)\nA Refresher on Statistical Significance - Harvard Business Review\nDecision Making in Uncertain Times - McKinsey\n\n\n\nModule 2: Statistics and Modeling\nRequired Reading (from course textbook):\nChapter 1: Probability and Uncertainty\n\nSections: “Normal Distribution,” “Poisson Distribution,” “Binomial Distribution”\nExamples: Heights of Adults, Customer Arrivals, NFL Patriots Coin Toss\n\nChapter 3: Bayesian Learning\n\nSection: “Poisson Model for Count Data”\n\nChapter 12: Linear Regression\n\nSection: “Linear Regression” (opening)\nExamples: Google vs S&P 500, Orange Juice\n\nChapter 13: Logistic Regression and GLMs\n\nSections: “Model Fitting,” “Confusion Matrix,” “ROC Curve”\nExample: NBA point spread\n\nSupplemental Reading (online):\n\nThe Surprising Power of Online Experiments - Harvard Business Review (A/B testing)\nMachine Learning Explained - MIT Sloan\nWhy Even a Million Dollars Couldn’t Buy a Better Algorithm - Wired (Netflix Prize case study)\n\n\n\nModule 3: Modern AI\nRequired Reading (from course textbook):\nChapter 24: Natural Language Processing\n\nSections: “Converting Words to Numbers (Embeddings),” “Word2Vec and Distributional Semantics”\nExample: Word2Vec for War and Peace\nSections: “Attention Mechanisms,” “Transformer Architecture” (overview)\n\nChapter 26: AI Agents\n\nFull chapter overview (agent architecture, tool use, planning, safety)\n\nSupplemental Reading (online):\n\nMaking the Most of AI and Machine Learning in Organizations - Stanford SMJ Paper\nTraditional Statistics vs Machine Learning - ToolsGroup\nThe State of AI in 2024 - McKinsey\nGenerative AI’s Act Two - Sequoia Capital"
  },
  {
    "objectID": "courses/caio/final-project-guide.html",
    "href": "courses/caio/final-project-guide.html",
    "title": "Final Project Guide",
    "section": "",
    "text": "In this project, you will build an AI agent that helps a retail pricing analyst make decisions about orange juice pricing and promotions. The agent will:\n\nLoad and explore sales data\nBuild a regression model to predict sales\nAnswer business questions using natural language\n\nTime Required: ~2 hours\nPrerequisites:\n\nCursor IDE installed (see Cursor Setup Guide)\nBasic familiarity with Cursor from Zoom Session 1\nDownload the project template to get started quickly"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#overview",
    "href": "courses/caio/final-project-guide.html#overview",
    "title": "Final Project Guide",
    "section": "",
    "text": "In this project, you will build an AI agent that helps a retail pricing analyst make decisions about orange juice pricing and promotions. The agent will:\n\nLoad and explore sales data\nBuild a regression model to predict sales\nAnswer business questions using natural language\n\nTime Required: ~2 hours\nPrerequisites:\n\nCursor IDE installed (see Cursor Setup Guide)\nBasic familiarity with Cursor from Zoom Session 1\nDownload the project template to get started quickly"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-1-project-setup",
    "href": "courses/caio/final-project-guide.html#part-1-project-setup",
    "title": "Final Project Guide",
    "section": "2 Part 1: Project Setup",
    "text": "2 Part 1: Project Setup\n\n2.1 Step 1.1: Create Your Project Folder\n\nOpen Cursor IDE\nClick File → Open Folder\nCreate a new folder called oj-pricing-agent on your computer\nSelect that folder to open it in Cursor\n\n\n\n2.2 Step 1.2: Create the Main Python File\n\nIn the Cursor sidebar, right-click and select New File\nName it oj_agent.py\nYou’ll see an empty file open in the editor\n\n\n\n2.3 Step 1.3: Copy the Dataset\nDownload the oj_data.csv file and copy it into your oj-pricing-agent folder."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-2-data-loading-and-exploration",
    "href": "courses/caio/final-project-guide.html#part-2-data-loading-and-exploration",
    "title": "Final Project Guide",
    "section": "3 Part 2: Data Loading and Exploration",
    "text": "3 Part 2: Data Loading and Exploration\n\n3.1 Step 2.1: Load the Required Libraries\nIn your oj_agent.py file, start by adding these lines at the top:\n# Required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\nWhat this does: These libraries help us work with data (pandas), do math (numpy), and build models (sklearn).\n\n\n3.2 Step 2.2: Load the Data\nAdd the following code to load the orange juice sales data:\n# Load the orange juice dataset\nprint(\"Loading data...\")\ndf = pd.read_csv('oj_data.csv')\n\n# Display basic information\nprint(f\"Dataset has {len(df)} rows and {len(df.columns)} columns\")\nprint(f\"\\nColumns: {list(df.columns)}\")\nprint(f\"\\nBrands in dataset: {df['brand'].unique()}\")\nprint(f\"\\nPrice range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\nprint(f\"\\nSample of data:\")\nprint(df.head())\n\n\n3.3 Step 2.3: Run Your Code (First Test)\n\nSave the file (Ctrl+S or Cmd+S)\nOpen the terminal in Cursor: View → Terminal\nRun the script:\n\npython oj_agent.py\nYou should see output showing:\n\nThe dataset has ~28,000 rows\nThree brands: Tropicana, Minute Maid, Dominick’s\nPrice ranges from about $1 to $4\n\nTroubleshooting: If you get an error about missing packages, run:\npip install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-3-building-the-regression-model",
    "href": "courses/caio/final-project-guide.html#part-3-building-the-regression-model",
    "title": "Final Project Guide",
    "section": "4 Part 3: Building the Regression Model",
    "text": "4 Part 3: Building the Regression Model\n\n4.1 Step 3.1: Understanding the Model\nWe’re building a model that predicts log of sales volume based on:\n\nPrice: Higher price → lower sales (negative relationship)\nFeatured (feat): If the product is in the weekly ad circular (1 = yes, 0 = no)\nBrand: Different brands have different base sales levels\nPrice × Brand interaction: Price sensitivity varies by brand\n\n\n\n4.2 Step 3.2: Prepare the Data for Modeling\nAdd this code to prepare features for the model:\n# ============================================\n# PART 3: BUILD THE REGRESSION MODEL\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Building the pricing model...\")\nprint(\"=\"*50)\n\n# Create dummy variables for brand (one-hot encoding)\n# This converts 'brand' text into numbers the model can use\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand', drop_first=False)\n\n# Create the feature matrix\n# We include: price, feat, brand dummies, and price*brand interactions\nX = pd.DataFrame({\n    'price': df['price'],\n    'feat': df['feat'],\n    'brand_minute.maid': brand_dummies['brand_minute.maid'],\n    'brand_tropicana': brand_dummies['brand_tropicana'],\n    # Interaction terms: price effect varies by brand\n    'price_x_minute.maid': df['price'] * brand_dummies['brand_minute.maid'],\n    'price_x_tropicana': df['price'] * brand_dummies['brand_tropicana']\n})\n\n# Target variable: log of sales (logmove)\ny = df['logmove']\n\nprint(f\"Features: {list(X.columns)}\")\nprint(f\"Target: logmove (log of sales volume)\")\n\n\n4.3 Step 3.3: Fit the Model\nAdd code to train the regression model:\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Display the coefficients\nprint(\"\\nModel Coefficients:\")\nprint(\"-\" * 40)\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"  {feature}: {coef:.4f}\")\nprint(f\"  intercept: {model.intercept_:.4f}\")\n\n# Calculate R-squared (how well the model fits)\nr_squared = model.score(X, y)\nprint(f\"\\nModel R-squared: {r_squared:.3f}\")\nprint(\"(This means the model explains {:.1f}% of sales variation)\".format(r_squared * 100))\n\n\n4.4 Step 3.4: Run and Verify the Model\nSave and run the script again. You should see coefficients like:\n\nprice: negative (higher price = lower sales)\nfeat: positive (being featured increases sales)\nbrand coefficients: capture baseline differences between brands\ninteraction terms: show how price sensitivity differs by brand"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-4-creating-helper-functions-for-the-agent",
    "href": "courses/caio/final-project-guide.html#part-4-creating-helper-functions-for-the-agent",
    "title": "Final Project Guide",
    "section": "5 Part 4: Creating Helper Functions for the Agent",
    "text": "5 Part 4: Creating Helper Functions for the Agent\n\n5.1 Step 4.1: Add Prediction Functions\nAdd these functions that the agent will use to answer questions:\n# ============================================\n# PART 4: HELPER FUNCTIONS FOR THE AGENT\n# ============================================\n\ndef predict_sales(brand, price, featured=0):\n    \"\"\"\n    Predict sales volume for a given brand, price, and feature status.\n    \n    Args:\n        brand: 'tropicana', 'minute.maid', or 'dominicks'\n        price: price in dollars (e.g., 2.50)\n        featured: 1 if in ad circular, 0 if not\n    \n    Returns:\n        Predicted sales volume (not log-transformed)\n    \"\"\"\n    # Create feature vector\n    features = {\n        'price': price,\n        'feat': featured,\n        'brand_minute.maid': 1 if brand.lower() == 'minute.maid' else 0,\n        'brand_tropicana': 1 if brand.lower() == 'tropicana' else 0,\n        'price_x_minute.maid': price if brand.lower() == 'minute.maid' else 0,\n        'price_x_tropicana': price if brand.lower() == 'tropicana' else 0\n    }\n    \n    # Convert to dataframe for prediction\n    X_pred = pd.DataFrame([features])\n    \n    # Predict log sales, then convert back\n    log_sales = model.predict(X_pred)[0]\n    sales = np.exp(log_sales)\n    \n    return sales\n\n\ndef get_price_elasticity(brand):\n    \"\"\"\n    Calculate the price elasticity for a given brand.\n    \n    Price elasticity tells us: if price increases by 1%, \n    how much does quantity demanded change (in %)?\n    \n    A more negative number means more price-sensitive.\n    \"\"\"\n    # Base price coefficient\n    base_coef = model.coef_[0]  # price coefficient\n    \n    # Add brand-specific interaction if applicable\n    if brand.lower() == 'minute.maid':\n        interaction_coef = model.coef_[4]  # price_x_minute.maid\n    elif brand.lower() == 'tropicana':\n        interaction_coef = model.coef_[5]  # price_x_tropicana\n    else:  # dominicks (base case)\n        interaction_coef = 0\n    \n    total_elasticity = base_coef + interaction_coef\n    return total_elasticity\n\n\ndef get_advertising_lift(brand):\n    \"\"\"\n    Calculate the sales lift from being featured in advertising.\n    Returns the percentage increase in sales.\n    \"\"\"\n    # The 'feat' coefficient tells us the log-sales increase\n    feat_coef = model.coef_[1]  # feat coefficient\n    \n    # Convert from log to percentage change\n    percentage_lift = (np.exp(feat_coef) - 1) * 100\n    return percentage_lift\n\n\ndef find_optimal_price(brand, min_price=1.0, max_price=4.0, featured=0):\n    \"\"\"\n    Find the price that maximizes revenue for a brand.\n    Revenue = Price × Quantity\n    \"\"\"\n    best_price = min_price\n    best_revenue = 0\n    \n    # Search through price range\n    for price in np.arange(min_price, max_price, 0.05):\n        sales = predict_sales(brand, price, featured)\n        revenue = price * sales\n        \n        if revenue &gt; best_revenue:\n            best_revenue = revenue\n            best_price = price\n    \n    return best_price, best_revenue\n\n\ndef compare_elasticities():\n    \"\"\"\n    Compare price elasticity across all three brands.\n    \"\"\"\n    brands = ['dominicks', 'minute.maid', 'tropicana']\n    results = {}\n    \n    for brand in brands:\n        elasticity = get_price_elasticity(brand)\n        results[brand] = elasticity\n    \n    return results\n\n\n5.2 Step 4.2: Test the Helper Functions\nAdd test code to verify the functions work:\n# ============================================\n# TEST THE HELPER FUNCTIONS\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing helper functions...\")\nprint(\"=\"*50)\n\n# Test prediction\ntest_sales = predict_sales('tropicana', 2.50, featured=0)\nprint(f\"\\nPredicted sales for Tropicana at $2.50 (no ad): {test_sales:.0f} units\")\n\n# Test elasticity\nelasticities = compare_elasticities()\nprint(\"\\nPrice Elasticities by Brand:\")\nfor brand, elast in elasticities.items():\n    print(f\"  {brand}: {elast:.3f}\")\n\n# Test advertising lift\nlift = get_advertising_lift('minute.maid')\nprint(f\"\\nAdvertising lift: {lift:.1f}% increase in sales\")\n\n# Test optimal price\nopt_price, opt_rev = find_optimal_price('dominicks')\nprint(f\"\\nOptimal price for Dominick's: ${opt_price:.2f} (revenue: ${opt_rev:.2f})\")\nRun the script again to verify all functions work correctly."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-5-creating-the-ai-agent",
    "href": "courses/caio/final-project-guide.html#part-5-creating-the-ai-agent",
    "title": "Final Project Guide",
    "section": "6 Part 5: Creating the AI Agent",
    "text": "6 Part 5: Creating the AI Agent\n\n6.1 Step 5.1: Add the Agent Logic\nNow we’ll create the agent that interprets natural language questions and calls the appropriate functions. Add this code:\n# ============================================\n# PART 5: THE AI AGENT\n# ============================================\n\ndef answer_question(question):\n    \"\"\"\n    Simple agent that answers business questions about OJ pricing.\n    \n    This is a rule-based agent that matches keywords in the question\n    to determine which analysis to perform.\n    \"\"\"\n    question_lower = question.lower()\n    \n    # Question 1: Predict sales for specific scenario\n    if 'predict' in question_lower or 'sales volume' in question_lower:\n        # Extract brand and price from question if possible\n        if 'tropicana' in question_lower:\n            brand = 'tropicana'\n        elif 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        else:\n            brand = 'dominicks'\n        \n        # Look for price (default to $2.50 if not found)\n        import re\n        price_match = re.search(r'\\$?(\\d+\\.?\\d*)', question_lower)\n        price = float(price_match.group(1)) if price_match else 2.50\n        \n        # Check for advertising\n        featured = 1 if 'advertis' in question_lower or 'feature' in question_lower else 0\n        if 'no advertis' in question_lower or 'without advertis' in question_lower:\n            featured = 0\n        \n        sales = predict_sales(brand, price, featured)\n        \n        response = f\"\"\"\n**Predicted Sales Analysis**\n\nBrand: {brand.title().replace('.', ' ')}\nPrice: ${price:.2f}\nFeatured in Ad: {'Yes' if featured else 'No'}\n\n**Predicted Sales Volume: {sales:,.0f} units**\n\nThis prediction is based on our regression model that accounts for:\n- Base demand for this brand\n- Price sensitivity (elasticity)  \n- Advertising effects\n\"\"\"\n        return response\n    \n    # Question 2: Which brand is most price-sensitive?\n    elif 'price-sensitive' in question_lower or 'price sensitive' in question_lower or 'most sensitive' in question_lower:\n        elasticities = compare_elasticities()\n        \n        # Find most price-sensitive (most negative elasticity)\n        most_sensitive = min(elasticities, key=elasticities.get)\n        \n        response = f\"\"\"\n**Price Sensitivity Analysis**\n\nPrice Elasticity by Brand:\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            sensitivity = \"HIGH\" if elast &lt; -3 else \"MEDIUM\" if elast &lt; -2 else \"LOW\"\n            response += f\"- {brand.title().replace('.', ' ')}: {elast:.3f} ({sensitivity} sensitivity)\\n\"\n        \n        response += f\"\"\"\n**Most Price-Sensitive: {most_sensitive.title().replace('.', ' ')}**\n\nInterpretation: A 1% price increase leads to a {abs(elasticities[most_sensitive]):.1f}% decrease in sales for {most_sensitive.title().replace('.', ' ')}.\n\nBusiness Implication: Be careful with price increases on {most_sensitive.title().replace('.', ' ')} - customers are very responsive to price changes.\n\"\"\"\n        return response\n    \n    # Question 3: Should we feature a brand in advertising?\n    elif 'feature' in question_lower or 'ad circular' in question_lower or 'advertising' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        lift = get_advertising_lift(brand)\n        \n        # Calculate example impact\n        base_sales = predict_sales(brand, 2.50, featured=0)\n        featured_sales = predict_sales(brand, 2.50, featured=1)\n        \n        response = f\"\"\"\n**Advertising Impact Analysis for {brand.title().replace('.', ' ')}**\n\nExpected Sales Lift from Featuring: **{lift:.1f}%**\n\nExample at $2.50:\n- Without advertising: {base_sales:,.0f} units\n- With advertising: {featured_sales:,.0f} units  \n- Additional sales: {featured_sales - base_sales:,.0f} units\n\n**Recommendation:** {'Yes, feature this product!' if lift &gt; 20 else 'Consider the advertising cost vs. the sales lift.'}\n\nThe advertising effect is consistent across price points. Factor in your advertising costs to determine if the sales lift justifies the expense.\n\"\"\"\n        return response\n    \n    # Question 4: Optimal price for a brand\n    elif 'optimal price' in question_lower or 'maximize revenue' in question_lower or 'best price' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        opt_price, opt_revenue = find_optimal_price(brand)\n        opt_sales = predict_sales(brand, opt_price, featured=0)\n        \n        # Compare with current average price\n        avg_price = df[df['brand'] == brand]['price'].mean()\n        avg_revenue = avg_price * predict_sales(brand, avg_price, featured=0)\n        \n        response = f\"\"\"\n**Revenue Optimization for {brand.title().replace('.', ' ')}**\n\n**Optimal Price: ${opt_price:.2f}**\n\nAt optimal price:\n- Predicted sales: {opt_sales:,.0f} units\n- Revenue per store-week: ${opt_revenue:,.2f}\n\nComparison with current average (${avg_price:.2f}):\n- Current revenue: ${avg_revenue:,.2f}\n- Potential improvement: ${opt_revenue - avg_revenue:,.2f} ({((opt_revenue/avg_revenue)-1)*100:.1f}%)\n\nNote: This optimization assumes no competitor response and stable market conditions.\n\"\"\"\n        return response\n    \n    # Question 5: Compare elasticities across brands\n    elif 'compare' in question_lower or 'elasticity' in question_lower or 'across' in question_lower:\n        elasticities = compare_elasticities()\n        \n        response = \"\"\"\n**Price Elasticity Comparison Across Brands**\n\n| Brand | Elasticity | Interpretation |\n|-------|------------|----------------|\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            interp = f\"1% price ↑ → {abs(elast):.1f}% sales ↓\"\n            response += f\"| {brand.title().replace('.', ' ')} | {elast:.3f} | {interp} |\\n\"\n        \n        response += \"\"\"\n**Key Insights:**\n\n1. **Dominick's** (store brand) is least price-sensitive - customers buying store brands may prioritize value and be less responsive to small price changes.\n\n2. **Tropicana** shows moderate price sensitivity - as a premium brand, some customers are loyal but others will switch if prices rise.\n\n3. **Minute Maid** is most price-sensitive - positioned between store and premium brands, these customers actively compare prices.\n\n**Strategic Implications:**\n- Use competitive pricing on Minute Maid to capture price-sensitive shoppers\n- Tropicana can sustain moderate price premiums\n- Dominick's margins can be optimized with less risk of volume loss\n\"\"\"\n        return response\n    \n    else:\n        return \"\"\"\nI can help you with these types of questions:\n\n1. **Sales Prediction:** \"What is the predicted sales volume if we price Tropicana at $2.50?\"\n2. **Price Sensitivity:** \"Which brand is most price-sensitive?\"\n3. **Advertising Impact:** \"Should we feature Minute Maid in the ad circular?\"\n4. **Price Optimization:** \"What price should we set for Dominick's to maximize revenue?\"\n5. **Elasticity Comparison:** \"Compare the price elasticity across brands\"\n\nPlease try one of these questions!\n\"\"\"\n\n\n6.2 Step 5.2: Add the Interactive Interface\nFinally, add code to let users interact with the agent:\n# ============================================\n# PART 6: INTERACTIVE AGENT INTERFACE\n# ============================================\n\ndef run_agent():\n    \"\"\"\n    Run the interactive agent interface.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"🍊 ORANGE JUICE PRICING ANALYTICS AGENT 🍊\")\n    print(\"=\"*60)\n    print(\"\\nHello! I'm your pricing analytics assistant.\")\n    print(\"I can help you analyze orange juice pricing and promotions.\")\n    print(\"\\nTry asking me questions like:\")\n    print(\"  - What is the predicted sales if we price Tropicana at $2.50?\")\n    print(\"  - Which brand is most price-sensitive?\")\n    print(\"  - Should we feature Minute Maid in the ad circular?\")\n    print(\"  - What price maximizes revenue for Dominick's?\")\n    print(\"  - Compare price elasticity across brands\")\n    print(\"\\nType 'quit' to exit.\\n\")\n    \n    while True:\n        question = input(\"Your question: \").strip()\n        \n        if question.lower() in ['quit', 'exit', 'q']:\n            print(\"\\nThank you for using the OJ Pricing Agent. Goodbye!\")\n            break\n        \n        if not question:\n            continue\n        \n        print(\"\\n\" + \"-\"*50)\n        response = answer_question(question)\n        print(response)\n        print(\"-\"*50 + \"\\n\")\n\n\n# ============================================\n# MAIN: RUN THE AGENT\n# ============================================\n\nif __name__ == \"__main__\":\n    # Run the interactive agent\n    run_agent()"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-6-testing-your-agent",
    "href": "courses/caio/final-project-guide.html#part-6-testing-your-agent",
    "title": "Final Project Guide",
    "section": "7 Part 6: Testing Your Agent",
    "text": "7 Part 6: Testing Your Agent\n\n7.1 Step 6.1: Run the Complete Script\nSave the file and run:\npython oj_agent.py\n\n\n7.2 Step 6.2: Test All Five Required Questions\nTest your agent with these exact questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nRecord the answers for your summary document."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-7-writing-your-summary",
    "href": "courses/caio/final-project-guide.html#part-7-writing-your-summary",
    "title": "Final Project Guide",
    "section": "8 Part 7: Writing Your Summary",
    "text": "8 Part 7: Writing Your Summary\nCreate a 1-page document (Word or PDF) that includes:\n\n8.1 Section 1: Key Findings (half page)\n\nWhich brand is most/least price-sensitive and why this matters\nThe impact of advertising on sales\nThe optimal pricing recommendations\n\n\n\n8.2 Section 2: Surprises and Insights (quarter page)\n\nWhat surprised you about the results?\nHow do these findings compare to your intuition?\n\n\n\n8.3 Section 3: Business Implications (quarter page)\n\nHow would you recommend a retailer use these insights?\nWhat additional data would make this analysis more useful?"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-8-preparing-your-demo",
    "href": "courses/caio/final-project-guide.html#part-8-preparing-your-demo",
    "title": "Final Project Guide",
    "section": "9 Part 8: Preparing Your Demo",
    "text": "9 Part 8: Preparing Your Demo\nFor Zoom Session 3, prepare a 2-3 minute demonstration:\n\nSetup (30 sec): Briefly explain what the agent does\nDemo (1.5 min): Show 2-3 questions and responses\nInsight (1 min): Share your most interesting finding\n\nTips:\n\nHave your script running before you share screen\nPre-type a question so you’re not typing live\nFocus on business insights, not technical details"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#troubleshooting",
    "href": "courses/caio/final-project-guide.html#troubleshooting",
    "title": "Final Project Guide",
    "section": "10 Troubleshooting",
    "text": "10 Troubleshooting\n\n10.1 “ModuleNotFoundError: No module named ‘pandas’”\nRun: pip install pandas numpy scikit-learn\n\n\n10.2 “FileNotFoundError: oj_data.csv”\nMake sure the data file is in the same folder as your Python script.\n\n\n10.3 “Model coefficients look wrong”\nCheck that your data loaded correctly - you should have ~28,000 rows.\n\n\n10.4 “Agent doesn’t understand my question”\nTry rephrasing using keywords like “predict”, “price-sensitive”, “feature”, “optimal”, or “compare”."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#complete-code-reference",
    "href": "courses/caio/final-project-guide.html#complete-code-reference",
    "title": "Final Project Guide",
    "section": "11 Complete Code Reference",
    "text": "11 Complete Code Reference\nThe complete oj_agent.py file should be approximately 350-400 lines. If you get stuck, ask Cursor’s AI assistant for help by selecting your code and pressing Cmd+K (Mac) or Ctrl+K (Windows), then describing your issue. I also prepared a complete code reference for you to refer."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#next-steps-optional-enhancements",
    "href": "courses/caio/final-project-guide.html#next-steps-optional-enhancements",
    "title": "Final Project Guide",
    "section": "12 Next Steps (Optional Enhancements)",
    "text": "12 Next Steps (Optional Enhancements)\nIf you finish early and want to explore further:\n\nAdd more questions: What other business questions could the agent answer?\nImprove the NLP: Use fuzzy matching to better understand varied phrasings\nAdd visualizations: Create charts showing price vs. sales by brand\nConnect to an LLM: Use the OpenAI API to make the agent truly conversational\n\nGood luck with your project! 🍊"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-1-probability-as-a-language-of-uncertainty",
    "href": "courses/caio/topic-overview-caio.html#module-1-probability-as-a-language-of-uncertainty",
    "title": "Machine – Learning Essentials",
    "section": "Module 1: Probability as a Language of Uncertainty",
    "text": "Module 1: Probability as a Language of Uncertainty\nDays 1-4\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nOpening sections through “Kolmogorov Axioms”\nSection: “Conditional, Marginal and Joint Distributions”\nExample: Salary-Happiness\n\nChapter 2: Bayes Rule\n\nSection: “Intuition and Simple Examples”\nExample: Sally Clark Case\nExample: Nakamura’s Alleged Cheating\n\nChapter 4: Utility, Risk and Decisions\n\nSection: “Expected Utility”\nExamples: Saint Petersburg Paradox, Kelly Criterion, Ellsberg Paradox\nSection: “Decision Trees” (including Medical Testing and Mudslide examples)\n\n\n\nSupplemental Reading (online)\n\nDid a US Chess Champion Cheat? - Chicago Booth Review (Bayesian analysis, prosecutor’s fallacy)\nA Refresher on Statistical Significance - Harvard Business Review\nDecision Making in Uncertain Times - McKinsey\n\n\n\nDiscussion Board: Decision-Making Under Uncertainty\nOpens Day 3 | Due Day 7\n“Consider a strategic decision your organization recently faced (or is currently facing) involving uncertainty. Describe the decision and identify:\n\nWhat were the key uncertain factors?\nHow was probability or likelihood assessed (formally or informally)?\nReflecting on the Ellsberg paradox and Kelly criterion, how might a more systematic probabilistic approach have changed the decision-making process?\n\nRespond to at least two peers’ posts with constructive suggestions.”\n\n\nZoom Session 1: Kick-off + Cursor Hands-on\nDay 4 | 1 hour\n\nWelcome and module overview (15 min)\nHands-on: Setting up Cursor IDE and using coding agents (30 min)\nQ&A on probability concepts from Module 1 (15 min)\n\nPreparation: Install Cursor IDE before the session (setup instructions)"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-2-statistics-and-modeling",
    "href": "courses/caio/topic-overview-caio.html#module-2-statistics-and-modeling",
    "title": "Machine – Learning Essentials",
    "section": "Module 2: Statistics and Modeling",
    "text": "Module 2: Statistics and Modeling\nDays 5-8\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nSections: “Normal Distribution,” “Poisson Distribution,” “Binomial Distribution”\nExamples: Heights of Adults, Customer Arrivals, NFL Patriots Coin Toss\n\nChapter 3: Bayesian Learning\n\nSection: “Poisson Model for Count Data”\n\nChapter 12: Linear Regression\n\nSection: “Linear Regression” (opening)\nExamples: Google vs S&P 500, Orange Juice\n\nChapter 13: Logistic Regression and GLMs\n\nSections: “Model Fitting,” “Confusion Matrix,” “ROC Curve”\nExample: NBA point spread\n\n\n\nSupplemental Reading (online)\n\nThe Surprising Power of Online Experiments - Harvard Business Review (A/B testing)\nMachine Learning Explained - MIT Sloan\nWhy Even a Million Dollars Couldn’t Buy a Better Algorithm - Wired (Netflix Prize case study)\n\n\n\nDiscussion Board: Predictive Models in Business\nOpens Day 7 | Due Day 11\n“The Netflix Prize awarded $1 million for a 10% improvement in recommendation accuracy, yet Netflix never fully implemented the winning algorithm—it was too complex and expensive to deploy, and by then, streaming had changed the business model entirely.\nReflecting on this case and the regression concepts from this module:\n\nIdentify a business process in your organization where a predictive model could be applied. What decisions would it inform?\nWhat would happen if the model’s predictions were inaccurate 20% of the time? 40% of the time? How would this affect business outcomes and trust in the system?\nDiscuss the trade-off: Is a highly accurate but complex/expensive model always better than a simpler, ‘good enough’ model? What factors would you consider when making this decision?\n\nRespond to at least two peers’ posts, particularly focusing on whether you agree with their assessment of the accuracy-complexity trade-off.”\n\n\nZoom Session 2: Mid-point Check-in\nDay 8 | 1 hour\n\nReview of statistical modeling concepts (20 min)\nLive demo: Building a simple regression model with Cursor (25 min)\nDiscussion of final project requirements (15 min)\n\nPreparation: Complete Module 2 lectures and readings"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-3-modern-ai",
    "href": "courses/caio/topic-overview-caio.html#module-3-modern-ai",
    "title": "Machine – Learning Essentials",
    "section": "Module 3: Modern AI",
    "text": "Module 3: Modern AI\nDays 9-14\n\nRequired Reading (from course textbook)\nChapter 24: Natural Language Processing\n\nSections: “Converting Words to Numbers (Embeddings),” “Word2Vec and Distributional Semantics”\nExample: Word2Vec for War and Peace\nSections: “Attention Mechanisms,” “Transformer Architecture” (overview)\n\nChapter 26: AI Agents\n\nFull chapter overview (agent architecture, tool use, planning, safety)\n\n\n\nSupplemental Reading (online)\n\nMaking the Most of AI and Machine Learning in Organizations - Stanford SMJ Paper\nTraditional Statistics vs Machine Learning - ToolsGroup\nThe State of AI in 2024 - McKinsey\nGenerative AI’s Act Two - Sequoia Capital\n\n\n\nDiscussion Board: AI Agents in the Enterprise\nOpens Day 11 | Due Day 14\n“AI agents are increasingly being deployed in business contexts. Describe a workflow or process in your organization that could potentially be automated or augmented by an AI agent. Address:\n\nWhat tasks would the agent perform?\nWhat data or tools would it need access to?\nWhat guardrails or human oversight would be necessary?\nWhat risks or concerns would need to be addressed before deployment?\n\nRespond to at least two peers’ posts.”\n\n\nZoom Session 3: Wrap-up + Final Project Presentations\nDay 14 | 1 hour\n\nBrief Modern AI recap (10 min)\nFinal project presentations/demonstrations (35 min)\nCourse wrap-up and next steps for AI leadership (15 min)\n\nPreparation: Complete final project; prepare 2-3 minute demonstration"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html",
    "href": "courses/caio/cursor-setup-guide.html",
    "title": "Cursor IDE Setup Guide",
    "section": "",
    "text": "Cursor is an AI-powered code editor built on VS Code. It allows you to:\n\nWrite code with AI assistance\nAsk questions about your code\nGenerate code from natural language descriptions\nDebug and fix errors with AI help\n\nFor this course, we’ll use Cursor to build an AI agent without needing deep programming expertise."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#what-is-cursor",
    "href": "courses/caio/cursor-setup-guide.html#what-is-cursor",
    "title": "Cursor IDE Setup Guide",
    "section": "",
    "text": "Cursor is an AI-powered code editor built on VS Code. It allows you to:\n\nWrite code with AI assistance\nAsk questions about your code\nGenerate code from natural language descriptions\nDebug and fix errors with AI help\n\nFor this course, we’ll use Cursor to build an AI agent without needing deep programming expertise."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-1-download-and-install-cursor",
    "href": "courses/caio/cursor-setup-guide.html#part-1-download-and-install-cursor",
    "title": "Cursor IDE Setup Guide",
    "section": "2 Part 1: Download and Install Cursor",
    "text": "2 Part 1: Download and Install Cursor\n\n2.1 Step 1: Download Cursor\n\nGo to cursor.sh\nClick the Download button\nThe website will automatically detect your operating system (Mac, Windows, or Linux)\n\n\n\n2.2 Step 2: Install on Mac\n\nOpen the downloaded .dmg file\nDrag the Cursor icon to the Applications folder\nOpen Cursor from Applications\nIf prompted about security, go to System Preferences → Security & Privacy and click “Open Anyway”\n\n\n\n2.3 Step 3: Install on Windows\n\nRun the downloaded .exe installer\nFollow the installation wizard\nLaunch Cursor from the Start Menu"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-2-initial-setup",
    "href": "courses/caio/cursor-setup-guide.html#part-2-initial-setup",
    "title": "Cursor IDE Setup Guide",
    "section": "3 Part 2: Initial Setup",
    "text": "3 Part 2: Initial Setup\n\n3.1 Step 1: Sign In (Optional but Recommended)\n\nWhen Cursor opens, you’ll see a welcome screen\nClick Sign In to create a free account\nYou can sign in with:\n\nGoogle account\nGitHub account\nEmail\n\n\nBenefits of signing in:\n\nFree AI credits for code assistance\nSettings sync across devices\n\n\n\n3.2 Step 2: Choose Your Theme\n\nCursor will ask about your preferred color theme\nChoose Dark or Light based on your preference\nYou can change this later in Settings\n\n\n\n3.3 Step 3: Import VS Code Settings (Optional)\nIf you’ve used VS Code before:\n\nCursor will offer to import your settings\nClick Import to bring over extensions and preferences\nOr click Skip to start fresh"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-3-install-python",
    "href": "courses/caio/cursor-setup-guide.html#part-3-install-python",
    "title": "Cursor IDE Setup Guide",
    "section": "4 Part 3: Install Python",
    "text": "4 Part 3: Install Python\nCursor needs Python installed on your computer to run our project.\n\n4.1 Check if Python is Already Installed\n\nIn Cursor, open the terminal: View → Terminal (or press Ctrl+`)\nType this command and press Enter:\n\npython --version\n\nIf you see Python 3.x.x, you’re good! Skip to Part 4.\nIf you get an error, follow the installation steps below.\n\n\n\n4.2 Install Python on Mac\nOption A: Using Homebrew (Recommended)\n\nOpen Terminal (outside of Cursor)\nInstall Homebrew if you don’t have it:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Python:\n\nbrew install python\nOption B: Direct Download\n\nGo to python.org/downloads\nDownload the latest Python 3.x version\nRun the installer\nImportant: Check “Add Python to PATH” during installation\n\n\n\n4.3 Install Python on Windows\n\nGo to python.org/downloads\nClick “Download Python 3.x.x”\nRun the installer\nIMPORTANT: Check the box that says “Add Python to PATH”\nClick “Install Now”\n\n\n\n4.4 Verify Installation\nAfter installation, close and reopen Cursor, then:\n\nOpen terminal: View → Terminal\nRun:\n\npython --version\n\nYou should see Python 3.x.x"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-4-install-required-packages",
    "href": "courses/caio/cursor-setup-guide.html#part-4-install-required-packages",
    "title": "Cursor IDE Setup Guide",
    "section": "5 Part 4: Install Required Packages",
    "text": "5 Part 4: Install Required Packages\nOur project needs a few Python libraries. Install them in Cursor’s terminal:\npip install pandas numpy scikit-learn\nYou should see output indicating successful installation.\nIf you get a “pip not found” error on Mac:\npip3 install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-5-using-cursors-ai-features",
    "href": "courses/caio/cursor-setup-guide.html#part-5-using-cursors-ai-features",
    "title": "Cursor IDE Setup Guide",
    "section": "6 Part 5: Using Cursor’s AI Features",
    "text": "6 Part 5: Using Cursor’s AI Features\n\n6.1 Feature 1: AI Chat (Cmd+L / Ctrl+L)\nUse this to ask questions or get help:\n\nPress Cmd+L (Mac) or Ctrl+L (Windows)\nA chat panel opens on the right\nAsk questions like:\n\n“How do I load a CSV file in Python?”\n“Explain what this code does”\n“Why am I getting this error?”\n\n\n\n\n6.2 Feature 2: Inline Edit (Cmd+K / Ctrl+K)\nUse this to write or modify code:\n\nSelect some code (or place cursor where you want new code)\nPress Cmd+K (Mac) or Ctrl+K (Windows)\nDescribe what you want in plain English:\n\n“Add a function that calculates the average price”\n“Fix this error”\n“Add comments explaining this code”\n\nReview the suggested changes\nPress Enter to accept or Escape to cancel\n\n\n\n6.3 Feature 3: Code Completion (Tab)\nAs you type, Cursor suggests completions:\n\nStart typing code\nYou’ll see gray “ghost text” suggestions\nPress Tab to accept the suggestion\nPress Escape to dismiss\n\n\n\n6.4 Feature 4: Agent Mode (Cmd+I / Ctrl+I)\nFor larger tasks, use Agent mode:\n\nPress Cmd+I (Mac) or Ctrl+I (Windows)\nDescribe a complex task:\n\n“Create a Python script that loads data and builds a regression model”\n\nThe agent will generate multiple files and complete code"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-6-creating-your-first-project",
    "href": "courses/caio/cursor-setup-guide.html#part-6-creating-your-first-project",
    "title": "Cursor IDE Setup Guide",
    "section": "7 Part 6: Creating Your First Project",
    "text": "7 Part 6: Creating Your First Project\n\n7.1 Step 1: Create a Project Folder\n\nIn Cursor, go to File → Open Folder\nNavigate to where you want your project (e.g., Documents)\nClick New Folder and name it oj-pricing-agent\nSelect this folder and click Open\n\n\n\n7.2 Step 2: Create a Python File\n\nIn the Explorer sidebar (left panel), right-click\nSelect New File\nName it test.py\nAdd this code:\n\nprint(\"Hello from Cursor!\")\n\n\n7.3 Step 3: Run Your Code\n\nOpen the terminal: View → Terminal\nRun your script:\n\npython test.py\n\nYou should see: Hello from Cursor!\n\nCongratulations! You’re ready to build your AI agent."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-7-keyboard-shortcuts-reference",
    "href": "courses/caio/cursor-setup-guide.html#part-7-keyboard-shortcuts-reference",
    "title": "Cursor IDE Setup Guide",
    "section": "8 Part 7: Keyboard Shortcuts Reference",
    "text": "8 Part 7: Keyboard Shortcuts Reference\n\n\n\nAction\nMac\nWindows\n\n\n\n\nAI Chat\nCmd+L\nCtrl+L\n\n\nInline Edit\nCmd+K\nCtrl+K\n\n\nAgent Mode\nCmd+I\nCtrl+I\n\n\nOpen Terminal\nCtrl+| Ctrl+\n\n\n\nSave File\nCmd+S\nCtrl+S\n\n\nOpen File\nCmd+O\nCtrl+O\n\n\nNew File\nCmd+N\nCtrl+N\n\n\nFind\nCmd+F\nCtrl+F"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#troubleshooting",
    "href": "courses/caio/cursor-setup-guide.html#troubleshooting",
    "title": "Cursor IDE Setup Guide",
    "section": "9 Troubleshooting",
    "text": "9 Troubleshooting\n\n9.1 “Python not found” in terminal\nMac:\n\nTry python3 instead of python\nOr run: brew install python\n\nWindows:\n\nReinstall Python and make sure to check “Add Python to PATH”\nRestart Cursor after installation\n\n\n\n9.2 “pip not found”\nMac:\n\nUse pip3 instead of pip\n\nWindows:\n\nTry python -m pip install package_name\n\n\n\n9.3 Cursor won’t start\n\nMake sure you have enough disk space (at least 1GB free)\nTry restarting your computer\nReinstall Cursor from cursor.sh\n\n\n\n9.4 AI features not working\n\nMake sure you’re signed in (check bottom-left corner)\nCheck your internet connection\nTry signing out and back in\n\n\n\n9.5 Code runs but shows errors\n\nCopy the error message\nPress Cmd+L (or Ctrl+L) to open AI Chat\nPaste the error and ask “How do I fix this?”"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#getting-help-during-the-course",
    "href": "courses/caio/cursor-setup-guide.html#getting-help-during-the-course",
    "title": "Cursor IDE Setup Guide",
    "section": "10 Getting Help During the Course",
    "text": "10 Getting Help During the Course\n\nZoom Sessions: Ask questions during live sessions\nCursor AI: Use Cmd+L to ask the AI for help\nDiscussion Board: Post questions for peer assistance\nOffice Hours: [Insert instructor office hours if applicable]"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#next-steps",
    "href": "courses/caio/cursor-setup-guide.html#next-steps",
    "title": "Cursor IDE Setup Guide",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nAfter completing this setup:\n\n✅ Cursor is installed and running\n✅ Python is installed\n✅ Required packages are installed\n✅ You can create and run Python files\n\nYou’re ready for Zoom Session 1 where we’ll practice using Cursor’s AI features together!"
  }
]