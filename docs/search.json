[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Google Scholar\nPolaris Project\nGeneology"
  },
  {
    "objectID": "research.html#highlights",
    "href": "research.html#highlights",
    "title": "Vadim Sokolov",
    "section": "highlights",
    "text": "highlights\n\nKramnik vs Nakamura or Bayes vs p-value, by Shiva Maharaj, Nick Polson and Vadim Sokolov. November 27, 2023. (pdf)"
  },
  {
    "objectID": "research.html#papers",
    "href": "research.html#papers",
    "title": "Vadim Sokolov",
    "section": "papers",
    "text": "papers\n\n\nBehnia, F., Karbowski, D., and Sokolov, V. (2023), “Deep generative models for vehicle speed trajectories,” Applied Stochastic Models in Business and Industry, 39, 701–719.\n\n\nBendre, S., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the probability of magnus carlsen reaching 2900,” Applied Stochastic Models in Business and Industry, 39, 372–381.\n\n\nPolson, N. G., and Sokolov, V. (2023b), “Generative AI for bayesian computation,” arXiv preprint arXiv:2305.14972.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023a), “Deep partial least squares for instrumental variable regression,” Applied Stochastic Models in Business and Industry.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023b), “Generative causal inference,” arXiv preprint arXiv:2306.16096.\n\n\nGupta, A., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the value of chess squares,” Entropy, MDPI, 25, 1374.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2023), “Quantum bayesian computation,” Applied Stochastic Models in Business and Industry.\n\n\nPolson, N., and Sokolov, V. (2023a), “Deep learning: A tutorial,” arXiv preprint arXiv:2310.06251.\n\n\nBaker, E., Barbillon, P., Fadikar, A., Gramacy, R. B., Herbei, R., Higdon, D., Huang, J., Johnson, L. R., Ma, P., Mondal, A., and others (2022), “Analyzing stochastic computer models: A review with opportunities,” Statistical Science, Institute of Mathematical Statistics, 37, 64–89.\n\n\nZha, Y., Parker, S. T., Foster, J. J., and Sokolov, V. (2022), “Housing market forecasting using home showing events,” arXiv preprint arXiv:2201.04003.\n\n\nSchultz, L., Auld, J., and Sokolov, V. (2022), “Bayesian calibration for activity based models,” arXiv preprint arXiv:2203.04414.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022a), “Deep partial least squares for iv regression,” arXiv preprint arXiv:2207.02612.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2022), “Quantum bayes AI,” arXiv preprint arXiv:2208.08068.\n\n\nSchultz, L., and Sokolov, V. (2022), “Deep learning gaussian processes for computer models with heteroskedastic and high-dimensional outputs,” arXiv preprint arXiv:2209.02163.\n\n\nWang, Y., Polson, N., and Sokolov, V. O. (2022), “Data augmentation for bayesian deep learning,” Bayesian Analysis, International Society for Bayesian Analysis, 1, 1–29.\n\n\nLey, H., Auld, J., Verbas, Ö., Weimer, R., Driscoll, S., Mohammadian, K., Golshani, N., Rahim, E., Shabanpour, R., Li, Z., and others (2022), Coordinated transit response planning and operations support tools for mitigating impacts of all-hazard emergency events, United States. Department of Transportation. Federal Transit Administration.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022b), “Feature selection for personalized policy analysis,” arXiv preprint arXiv:2301.00251.\n\n\nFotouhi, H., Mori, N., Miller-Hooks, E., Sokolov, V., and Sahasrabudhe, S. (2021), “Assessing the effects of limited curbside pickup capacity in meal delivery operations for increased safety during a pandemic,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2675, 436–452.\n\n\nZavareh, M., Maggioni, V., and Sokolov, V. (2021), “Investigating water quality data using principal component analysis and granger causality,” Water, MDPI, 13, 343.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2021), “Deep learning partial least squares,” arXiv preprint arXiv:2106.14085.\n\n\nSokolova, A. O., Marshall, C. H., Lozano, R., Gulati, R., Ledet, E. M., De Sarkar, N., Grivas, P., Higano, C. S., Montgomery, B., Nelson, P. S., and others (2021), “Efficacy of systemic therapies in men with metastatic castration resistant prostate cancer harboring germline ATM versus BRCA2 mutations,” The Prostate, 81, 1382–1389.\n\n\nBhadra, A., Datta, J., Polson, N., Sokolov, V., and Xu, J. (2021), “Merging two cultures: Deep and statistical learning,” arXiv preprint arXiv:2110.11561.\n\n\nHsu, Y. L., Jeng, C. C., Murali, P. S., Torkjazi, M., West, J., Zuber, M., and Sokolov, V. (2021), “Bayesian learning: A selective overview,” arXiv preprint arXiv:2112.12722.\n\n\nPolson, N., and Sokolov, V. (2020), “Deep learning: Computational aspects,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 12, e1500.\n\n\nHuang, X., Li, B., Peng, H., Auld, J. A., and Sokolov, V. O. (2020), “Eco-mobility-on-demand fleet control with ride-sharing,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 23, 3158–3168.\n\n\nSokolov, V. (2020), “Discussion of ‘multivariate generalized hyperbolic laws for modeling financial log-returns—empirical and theoretical considerations’,” Applied Stochastic Models in Business and Industry, John Wiley & Sons, 36, 777–779.\n\n\nDixon, M. F., Polson, N. G., and Sokolov, V. O. (2019), “Deep learning for spatio-temporal modeling: Dynamic traffic flows and high frequency trading,” Applied Stochastic Models in Business and Industry, 35, 788–807.\n\n\nWarren, J., Lipkowitz, J., and Sokolov, V. (2019), “Clusters of driving behavior from observational smartphone data,” IEEE Intelligent Transportation Systems Magazine, IEEE, 11, 171–180.\n\n\nPolson, N. G., and Sokolov, V. (2019), “Bayesian regularization: From tikhonov to horseshoe,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 11, e1463.\n\n\nWang, Y., Polson, N. G., and Sokolov, V. O. (2019), “Scalable data augmentation for deep learning,” arXiv preprint arXiv:1903.09668.\n\n\nSokolov, V., and Polson, M. (2019), “Strategic bayesian asset allocation,” arXiv preprint arXiv:1905.08414.\n\n\nLi, D., Liu, J., Park, N., Lee, D., Ramachandran, G., Seyedmazloom, A., Lee, K., Feng, C., Sokolov, V., and Ganesan, R. (2019), “Solving large-scale 0-1 knapsack problems and its application to point cloud resampling,” arXiv preprint arXiv:1906.05929.\n\n\nChen, H., Jajodia, S., Liu, J., Park, N., Sokolov, V., and Subrahmanian, V. (2019), “FakeTables: Using GANs to generate functional dependency preserving tables with bounded real data.” in IJCAI, pp. 2074–2080.\n\n\nNicholas G. Polson, V. O. S. (2019), “Deep learning,” Wiley StatsRef: Statistics Reference Online.\n\n\nSchultz, L., and Sokolov, V. (2018a), “Bayesian optimization for transportation simulators,” Procedia computer science, Elsevier, 130, 973–978.\n\n\nSchultz, L., and Sokolov, V. (2018b), “Deep reinforcement learning for dynamic urban transportation problems,” arXiv preprint arXiv:1806.05310.\n\n\nSokolov, V., Imran, M., Etherington, D. W., Karbowski, D., and Rousseau, A. (2018), “Effects of predictive real-time traffic signal information,” in 2018 21st international conference on intelligent transportation systems (ITSC), IEEE, pp. 1834–1839.\n\n\nSchultz, L., and Sokolov, V. (2018c), “Practical bayesian optimization for transportation simulators,” arXiv preprint arXiv:1810.03688.\n\n\nPolson, N., and Sokolov, V. (2017a), “Bayesian particle tracking of traffic flows,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 19, 345–356.\n\n\nSokolov, V., Larson, J., Munson, T., Auld, J., and Karbowski, D. (2017), “Maximization of platoon formation through centralized routing and departure time coordination,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2667, 10–16.\n\n\nSokolov, V. (2017), “Discussion of ‘deep learning for finance: Deep portfolios’,” Applied Stochastic Models in Business and Industry, 33, 16–18.\n\n\nAuld, J., Sokolov, V., and Stephens, T. S. (2017), “Analysis of the effects of connected–automated vehicle technologies on travel demand,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2625, 1–8.\n\n\nPolson, N. G., and Sokolov, V. (2017b), “Deep learning: A bayesian perspective.”\n\n\nVerbas, Ö., Sokolov, V., Auld, J., and Ley, H. (2017), “Time-dependent capacitated transit routing with real-time demand and supply data.”\n\n\nAuld, J., Hope, M., Ley, H., Sokolov, V., Xu, B., and Zhang, K. (2016b), “POLARIS: Agent-based modeling framework development and implementation for integrated travel demand and network and operations simulations,” Transportation Research Part C: Emerging Technologies, Pergamon, 64, 101–116.\n\n\nAuld, J., Karbowski, D., Sokolov, V., and Kim, N. (2016a), “A disaggregate model system for assessing the energy impact of transportation at the regional level,” in TRB 2016 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Jongryeol, J. (2016b), “Fuel saving potential of optimal route-based control for plug-in hybrid electric vehicle,” IFAC-PapersOnLine, Elsevier, 49, 128–133.\n\n\nJacquier, E., Polson, N., and Sokolov, V. (2016), “Sequential bayesian learning for merton’s jump model with stochastic volatility,” arXiv preprint arXiv:1610.09750.\n\n\nLarson, J., Munson, T., and Sokolov, V. (2016), “Coordinated platoon routing in a metropolitan network,” in 2016 proceedings of the seventh SIAM workshop on combinatorial scientific computing, Society for Industrial; Applied Mathematics, pp. 73–82.\n\n\nKarbowski, D., Kim, N., Auld, J., and Sokolov, V. (2016a), “Assessing the energy impact of traffic management and vehicle hybridisation,” International Journal of Complexity in Applied Science and Technology, Inderscience Publishers (IEL), 1, 107–124.\n\n\nPolson, N., and Sokolov, V. (2015), “Bayesian analysis of traffic flow on interstate i-55: The LWR model.”\n\n\nSokolov, V., Karbowski, D., Kim, N., and Auld, J. (2015), “Assessing the energy impact of traffic management and vehicle hybridization,” in 25th ITS annual meeting.\n\n\nLuo, Q., Auld, J., and Sokolov, V. (2015), “Addressing some issues of map-matching for large-scale, high-frequency GPS data sets,” in TRB 2015 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Rousseau, A. (2015), Vehicle energy management optimization through digital maps and connectivity, Argonne National Lab.(ANL), Argonne, IL (United States).\n\n\nRichey, A. S., Richey, J. E., Tan, A., Liu, M., Adam, J. C., and Sokolov, V. (2015), “Assessing the use of remote sensing and a crop growth model to improve modeled streamflow in central asia,” in AGU fall meeting abstracts, pp. H44F–08.\n\n\nSokolov, V., Karbowski, D., and Kim, N. (2014a), “Assessing the energy impact of traffic management and ITS technologies,” in 21st ITS world congress.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and others (2014), “GREET model: The greenhouse gases, regulated emissions, and energy use in transportation model,” Chicago, USA: Argonne National Laboratory.\n\n\nSokolov, V. O., Zhou, X., and Langlois, P.-A. (2014b), “A framework for arterial traffic flow modeling-POLARIS.”\n\n\nAuld, J., Hope, M., Ley, H., Xu, B., Zhang, K., and Sokolov, V. (2013), “Modelling framework for regional integrated simulation of transportation network and activity-based demand (polaris),” in International symposium for next generation infrastructure.\n\n\nAuld, J., Sokolov, V., Fontes, A., and Bautista, R. (2012), “Internet-based stated response survey for no-notice emergency evacuations,” Transportation Letters, Taylor & Francis, 4, 41–53.\n\n\nSokolov, V., Auld, J., and Hope, M. (2012), “A flexible framework for developing integrated models of transportation systems using an agent-based approach,” Procedia Computer Science, Elsevier, 10, 854–859.\n\n\nDatta, B. N., and Sokolov, V. (2011), “A solution of the affine quadratic inverse eigenvalue problem,” Linear Algebra and its Applications, North-Holland, 434, 1745–1760.\n\n\nPark, Y. S., Manli, E., Hope, M., Sokolov, V., and Ley, H. (2010), Fuzzy rule-based approach for evacuation trip demand modeling.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and GREET, M. (2010), “The greenhouse gases, regulated emissions, and energy use in transportation model,” Center for Transportation Research Argonne National Laboratory, Argonne, IL.\n\n\nDatta, B. N., Deng, S., Sokolov, V., and Sarkissian, D. (2009), “An optimization technique for damped model updating with measured data satisfying quadratic orthogonality constraint,” Mechanical Systems and Signal Processing, Academic Press, 23, 1759–1772.\n\n\nDatta, B. N., and Sokolov, V. (2009), “Quadratic inverse eigenvalue problems, active vibration control and model updating,” Applied and Computational Mathematics, 8, 170–191.\n\n\nSokolov, V. O. (2008), “Quadratic inverse eigenvalue problems: Theory, methods, and applications,” PhD thesis, Northern Illinois University.\n\n\nAre, S., Dostert, P., Ettinger, B., Liu, J., Sokolov, V., Wei, A., and Wiegand, K. (2006), “Reservoir model optimization under uncertainty.”\n\n\nKrukier, L., Pichugina, O., Sokolov, V., and Vulkov, L. (2006), “Numerical investigation of krylov subspace methods for solving non-symmetric systems of linear equations with dominant skew-symmetric part,” International Journal of Numerical Analysis and Modeling, University of Alberta, 3, 115–124."
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Vadim Sokolov",
    "section": "software",
    "text": "software\n\nPOLARIS: Designer. Developer. Transportation systems simulations framework (C++)\nGREET: Designer. Lead Developer. An implementation of The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (GREET) Model. (C#, .NET, SQLite). more then 800 unique users within first year of release, 2013\nMATCOM: Contributor. Distributed on CD with Numerical Linear Algebra and Applications, Second Edition book By Biswa Nath Datta, SIAM. (MATLAB)\nTRANSIMS: Contributor. An agent-based forecast software for modeling regional transport systems. (C++); 22,295 total downloads since 2006\nAdvanced Numerical Methods II: Sole Developer. Package for solving large scale control problems. (Mathematica); an experimental library that was not published"
  },
  {
    "objectID": "research.html#research-coverage",
    "href": "research.html#research-coverage",
    "title": "Vadim Sokolov",
    "section": "research coverage",
    "text": "research coverage\n\nDemystifying the future of connected and autonomous vehicles (Newswise\nArgonne wins grant to help transit agencies cope with emergencies (Chicago Tribune)\nArgonne will research how transportation systems should respond to natural hazards (WBEZ)\nWhat Happens When Developers, Scientists and Super-Computers Connect on Urban Design (Next City)\nUM wins $2.7M grant to study driverless cars (The Detroit News)\nUM teams with Argonne, Idaho national labs to study potential energy savings of connected vehicles (Michigan News)\nArgonne to study emergency response of Chicago transit (Chicago Sun Times)\nDesigning future cities (phys.org)\nUsing a Real Life SimCity to Design a Massive Development (Curbed Chicago)"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Full CV"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vadim Sokolov",
    "section": "Education",
    "text": "Education\n\nPh.D., 2008, Computational Mathematics, Northern Illinois University\nDiploma (summa cum laude), 2004, Applied Mathematics, Rostov State University\nGraduate Studies in Statistics, 2013-2014, University of Chicago"
  },
  {
    "objectID": "cv.html#positions",
    "href": "cv.html#positions",
    "title": "Vadim Sokolov",
    "section": "Positions",
    "text": "Positions\n\nAssociate Professor, Systems Engineering and Operations Research, George Mason University, 2023-present\nAsistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nVisiting Assistant Professor, Statistics,The University of Chicago Booth School of Business, 2019-2023\nAssistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nLecturer, Master of Science in Analytics, University of Chicago, 2015-2016\nFellow, Computation Institute, University of Chicago, 2014-2016\nPrincipal Computational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2013-2016\nComputational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2008-2013"
  },
  {
    "objectID": "courses/664/final.html",
    "href": "courses/664/final.html",
    "title": "Final Project Guidelines for SYST/OR 664 CSI 674. Bayesian Inference and Decision Theory",
    "section": "",
    "text": "The final project can be done in a group (up to 3) or as is an individual project. You have a lot of freedom in choosing a topic for your final project. The only criterion is that it deeply involves applying Bayesian data analysis to a real-world problem. You choose a dataset, an interesting question about it, and address it with Bayesian modeling.\nThere are four milestones:\n\nProject proposal (due April 7)\nProject Presentation Draft (due April 26)\nPresentation (April 29).\nFinal Project Report (due May 5)\n\nThe project proposal is to have 3 paragraphs on\n\nQuestion you are trying to answer and why this question is interesting or important. How your analysis changes decision making process? For example if you are forecasting demand for a product, how would the company change its production process if they had a better forecast?\nData you are using. If you are using a dataset, where did it come from?\nWhat model are you going to use for the data analysis?\n\nThe proposal should be up to 1 pages long.\nThe report is up to 5 pages long, excluding figures or references. If you wish to add more materia, put it into appending. However, the main 5-page body should be self-contained and complete. The report should be written in a clear and concise manner, and should be well-organized. The report should include the following sections:\n\nIntroduction: Clear description of the problem. Describe the problem you are trying to solve, why it is important or useful, and summarize any important pieces of prior work that you are building upon.\nDataset: Clear description/visualization of the data. Describe the dataset or datasets you are working with. Show examples from the datasets. If you collected or constructed your own dataset, explain the process you used to collect the images and labels, and why you made the choices you did in the data collection process.\nModels/Methods: Clear and thorough description of statistical analysis. Describe the method you are using; this may also contain parts of the implementation of your model, loss function, or other components along with sanity checks to ensure that those components are correctly implemented.\nExperiments: Clear and thorough interpretation of results. Describe the experiments you did, and key results and figures that you obtained. This may interleave explanations of the experiments you run and figures you generate as a result of those experiments.\n\nYou should turn in a .pdf file containing your final report, together wih the notebooks/markdowns containing all the code and the generated results (tables, figures etc) that are included in the report. You must run all cells in your notebook to receive credit; we will not rerun your notebook.\nBoth project and presentation will be evaluated on technical depth, and writing/presentation quality (50/50). Some points to keep in mind\n\nVisualizations should be used judiciously to report findings. Do not overload reader with tables/figures.\nFigures should be appropriately labeled: x-axis, y-axis, legend, and title.\nFigures and tables should be numbered and referenced properly in the write-up.\nOutput of posterior predictive checks should be properly interpreted not merely stated\nAt the end, you need to connect your results to the initial question of investigation.\nNo more than 10 lines of text per slide."
  },
  {
    "objectID": "courses/610.html",
    "href": "courses/610.html",
    "title": "SYST/OR 610. Deep Leanring",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2022\n\n\n\n\n\nInstructor: Vadim Sokolov\nLocation and time: Aquia, room 347; 7:20-10pm Mondays\nOffice hours: By appointment\n\n\nDatacamp\nIf you are rusty on Python, I suggest you refresh your skills using Datacamp. Datacamp gave students in this class a free access to all of the courses. If you follow the link above you can get your free access using masonlive email. I also listed some of the Python courses I suggest #there.\n\n\nList of topics and tentative schedule\n\nBasics (Weeks 1-2)\n\nLinear Algebra: intro\nProbability: OpenIntro Ch 3\nGeneralized Linear Models: OpenIntro Ch 8,9\nPyTorch: PyTorch Basics\nFeed Forward Architectures: WHAT IS TORCH.NN REALLY?; Ripley Ch 5; Bishop Ch 3,4\n\nConvex Optimization (Weeks 3-4)\n\nBackpropagation and matrix derivatives\nStochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration): Bishop Ch 7, Goodfellow Ch 8\nSecond order methods: Bishop Ch 7\nADMM\nRegularization (l1, l2 and dropout): dropout paper, Godfellow Ch 7\nBatch normalization: paper\n\nConv Nets and Image Processing (Week 5): Goodfellow Ch 9\nRecurrent Nets and Sequential Data (Week 6): Good Fellow Ch 10, seq2seq, Pytorch eq2seq tutorial\nTheory of deep learning (Week 7): see theory section for the reading list\n\nUniversal approximators\nCurse of dimensionality\nKernel spaces\nTopology and geometry\n\nProbabilistic DL (Weeks 8-9) Langevin, MCMC, , VB\n\nConjugate distributions, exponential family Bishop: Ch 2\nModel choice\nHierarchical linear and generalize linear models (regression and classification): Bishop Ch 10\nModels for missing data (EM-algorithm)\nBayes computations (MCMC, Variational Bayes)\n\nAdditional Topics (Weeks 10-13)\n\nModel Visualization Tensorboard\nGenerative Models (normalizing flows, GANs, recurrent nets): NF Paper Tutorial; NF; \nAttention and Transformers attention paper\nDeep Reinforcement Learning DRL Tutorial\nBayesian Optimisation: Hyperparameter selection and parameter initialization Hyperopt\n\n\n\n\nData analysis projects\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 8 of the class you should have a team formed and data set + analysis problem identified. You need to submit a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\nYou will post results of your analysis on the class blog post. The final project will be graded on presentation, writing and analysis.\n\n\n\nGroup Work\nBoth projects and homework can be done in a groups of size of up to 3 people. You can change groups in between. If you do a homework in a gorup, it means that all of the members of the group do it individually and can consult with each other. You can also do 1 submission per group if you prefer. You can use “group” section of the piazza page to find teammates if you need any. If you need help finding a group, please email me.\n\n\nGrading\nEach hw is 10 points, project is 30.\n\nThis is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic.\n\n\nBooks\n\nPolson, Sokolov notes\nDive into Deep Learning link\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.\nRipley, Brian D. Pattern recognition and neural networks. Cambridge university press, 2007.\nBishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.\n\n\n\nPer Topic Resources\n\nArchitectures\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\nOptimization\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\nTheory\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\nReinforcement Learning\nOptimization Models - Truth - Yet\n\n\nBayesian DL\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\nPractical Tricks\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\nOther Resources\n\nAdditional Reading List\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\nBlogs\n\nPapers with code\nSecurity\nUnsupervised learning\nCybersecurity\nOpt Visualization\nAI and Memory Wall\n\n\n\nVideos\n\nDeep Energy\nDL Summer school 2015\nDL Representations\nPyData 2017\n\n\n\nOther courses with good web presence\n\nStanford’s CS231n\nStanford’s STATS385\nfast.ai\nNando de Freitas’ course on machine\nUC Berkeley Stat241B\nMIT\nUdacity DRL\n\n\n\nTools\n\nTensorFlow\nKeras\nTF Playground\nSony\nSnakeViz python profiler\nPyTorch\nTS Stan Examples\nOpenAI Glow\n\n\n\nMisc Links\n\nPytorch resources (a curated list of tutorials, papers)\nIs artificial intelligence set to become art’s next medium?"
  },
  {
    "objectID": "courses/468.html",
    "href": "courses/468.html",
    "title": "SYST/OR 468. Applied Predictive Analytics",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2019\n\nFor course materials click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\nAnnouncements\n\n04/08/2019: HW3 posted, due date April 17.\n03/09/2019: HW2 posted, due date April 3.\n02/24/2019: Midterm is on March 20 (open book)\n02/24/2019: No class on March 13 (Spring Break)\n02/24/2019: H1 Due date is March 6\n10/10/2018: Check back regularly for announcements\n10/10/2018: First Class is on January 23\n\n\n\nCourse staff\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Eng Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: TBA\n\n\n\nOffie hours\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nTA: Jungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\nLectures\nLocation: Planetary Hall 206\nTimes: 4:30-7:10pm on Wednesday\n\n\nGrades\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\nTextbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nAdditional reading: List\n\n\n\nLinks\n\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb RF\nFacebook regrsssion\nYoutube DL\nUber Time Series\nEDA\nOverfitting\n2016 Election\n\n\n\nDeep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learning\nShort introduction into deep learning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\nPrevious instances\n\nSpring 2018"
  },
  {
    "objectID": "courses/41000/41000.html",
    "href": "courses/41000/41000.html",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvadim.sokolov@chicagobooth.edu\nTA: Ayman Moawad\naymoawad@uchicago.edu\nSyllabus\nDatasets \nAll examples\nCourse Textbook: OpenIntro Statistics pdf, tablet"
  },
  {
    "objectID": "courses/41000/41000.html#class-notes",
    "href": "courses/41000/41000.html#class-notes",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Class Notes",
    "text": "Class Notes\n\nSection 1: Introduction and Probability: notes, code\nSection 2: Statistics and Data: notes, code, epl, abtesting\nSection 3: Regression: notes, code, housing, mammals\nSection 4: Classification: notes, tree notes newfood, OJ, PGA, Tennis, logistic\nSection 5: AI and Deep Learning: notes, Examples: MNIST; NN for Circle Data, IMDB, Trump Tweets, trump-tweets.R"
  },
  {
    "objectID": "courses/41000/41000.html#hw",
    "href": "courses/41000/41000.html#hw",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "HW",
    "text": "HW\n\nHW1 (code)\nHW2 (code)\nHW3 (code)\nHW4 (code\nFinal Project (code)"
  },
  {
    "objectID": "courses/41000/41000.html#midterm",
    "href": "courses/41000/41000.html#midterm",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Midterm",
    "text": "Midterm\n\nQuestions\nQuestions\nMidterm 1\nMidterm 2\nMidterm 3\nMidterm 4\nMidterm 5\nMidterm 6\nMidterm 7\nMidterm 8\nMidterm 9\nMidterm 10\nMidterm 11"
  },
  {
    "objectID": "courses/41000/41000.html#r",
    "href": "courses/41000/41000.html#r",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "R",
    "text": "R\nI made a quick introduciton to R, video is here and here are the slides.\n\nTo get RStudio on you computer, you need to first download and install RStudio then you need to download and install R.\nFurther, I recommend doing those two courses on DataCamp.\n\nIntroduction to R\nIntroduction to Data\n\n\nIn addition, Booth offers access to an online R tutorial on LinkedinLearning. To access the training, go to: here and login using your CNetID and password. You may find it helpful to work through the following R courses offered on LinkedInLearning:\n\nCode Clinic: R with Mark Niemann-Ross\nR Statistics Essential Training with Barton Poulson\nUp and Running with R with Barton Poulson"
  },
  {
    "objectID": "courses/41000/41000.html#demos",
    "href": "courses/41000/41000.html#demos",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Demos",
    "text": "Demos\n\nPortfolio Variance\nBinomial\nNormal Distribution"
  },
  {
    "objectID": "courses/41000/41000.html#other-materials-and-links",
    "href": "courses/41000/41000.html#other-materials-and-links",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Other Materials and Links",
    "text": "Other Materials and Links\n\nAI in Tennis\nKraft-Heinz blog store\nKraft-Heinz WSJ\nFrito-Lay potato peeling\nTyson\nAnheuser-Busch InBev payments\nWalmart smart\nThe State of Data Science and MachineLearning\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb Random Forest\nFacebook regrsssion\nYoutube Deep learning\nUber: time series\nExploratory Data Analysis\nOverfitting\n2016 Election"
  },
  {
    "objectID": "courses/41000/41000.html#deep-learning",
    "href": "courses/41000/41000.html#deep-learning",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learnig\nShort introduction into deep leanring\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "courses/41000/syllabus.html",
    "href": "courses/41000/syllabus.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "href": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/568.html",
    "href": "courses/568.html",
    "title": "SYST/OR 568. Applied Predictive Analytics - Mason Analytics MS",
    "section": "",
    "text": "George Mason University\nFall 2023\n\nCourse Material\nVideo Lectures\nTA: Raina Joy Saha (rsaha3 (at) gmu.edu)\nInstructor: Vadim Sokolov\n\nLocation: Krug Hall 7\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\nList of Topics\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesian Inference\nTime series forecasting (html notes, pdf notes)\n\n\n\nSchedule\n\nAug 21: First Class\nSept 4: Labor Day : University Closed\nSep 11: HW 1 Due\nSep 25: HW 2 Due\nOct 9: Fall Break (Classes Do Not Meet)\nOct 10: We have a class\nOct 16: HW 3 Due\nOct 23: In-class Midterm\nOct 30: Final Project Proposal\nNov 6: Hw 4 Due\nNov 20: Hw 5 Due\nNov 27: Last class, project presentations\nDec 3: Final Projects Due\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nOptional Textbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/664.html",
    "href": "courses/664.html",
    "title": "SYST/OR 664 CSI 674. Bayesian AI",
    "section": "",
    "text": "Spring 2025\n\nInstructor: Vadim Sokolov\n\nLocation: Engineering Building 2241\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nZoom Link: https://gmu.zoom.us/j/98895158802?pwd=3fGVsYOe332Rw7B2AaZkb2LmzRbXU2.1\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nHomeworks\nLecture Notes\n\n\n\nContent and goals\nMany artificial intelligence problems involve modeling uncertainty. Bayesian probabilistic models represent uncertainty and dependencies between random variables using probability distributions. You will learn the set of rules of probability and computational algorithms to manipulate these distributions. Bayesian approach enhances the effectiveness of conventional AI techniques. This course summarizes various Bayesian-based models and the standard algorithms used with them, supplemented by instances of their practical use. We will discuss applications in science, engineering, economics, medicine, sport, and law. Students will learn the commonalities and differences between the Bayesian and frequentist approaches to statistical inference, how to approach a statistics problem from the Bayesian perspective, and how to combine data with informed expert judgment soundly to derive useful and policy-relevant conclusions. Assignments focus on applying the methods to practical problems.\n\n\nList of Topics\n\nUnit 1: Introduction: History of AI, Bayes approach to ML and AI. Probability and Bayes Rule.\nUnit 2: Utility and Decision Theory\nUnit 3: Bayesian Inference with Conjugate Pairs: Single Parameter Models\nUnit 4: Hierarchical Bayesian Models\nUnit 5: Bayesian Hypothesis Tests\nUnit 6: Stochastic Processes\nUnit 7: Markov Chain Monte Carlo\n\nUnit 8: Bayesian Regression: Linear and Bayesian Trees\nUnit 9: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\nUnit 10: Bayesian Double Descent and Model Selection: Modern Appriach to Bias-Variance Tradeoff\nUnit 11: Bayesian Neural Networks and Deep Learning\n\n\n\nSchedule\n\nJan 27: First Class\nFeb 17: We will not meet, I will post a video lecture\nMarch 10: No class (spring break)\nMarch 17: Midterm exam\nMay 5: Last class, project presentations\n\n\n\nHWs\n\nHW1: 1,2,3,5,13,17,26,45,50,62\nHW2: 15, 25, 27,70,71,72,120\nHw3: 86,76,77,75,78,121\nHW4: 122, 123, 124\n\n\n\nAssignments\nHomework is due 11:59PM on the assigned due date. If you have extenuating circumstances, please contact me in advance, and I will consider giving you additional time to complete the assignment for partial credit. Assignments will be posted here and on Blackboard. Please submit your assignments through Blackboard.\nExams are take home and will be posted on Blackboard.\n\n\nPrerequisites\nThe formal listed prerequisite is OR 542 or STAT 544 or STAT 554 or equivalent.\n\nThe real prerequisites are:\n\nExperience with elementary data analysis such as scatterplots, histograms, hypothesis tests, confidence intervals, and simple linear regression.\nA calculus-based probability course - elementary probability theory, discrete and continuous probability distributions, probability mass and probability density functions, cumulative distribution functions, common parametric models such as the normal, binomial and Poisson distributions.\nExperience with a high-level programming language. We will use R, a programming language for data analysis, and STAN, a language for specifying and performing inference with Bayesian models.\nComfort with mathematical notation. We will not do proofs, but you will be expected to be comfortable following and doing mathematical derivations.\n\n\n\nComputing\nWe will use R, a powerful (free) statistical graphics and computing language, and STAN, an open-source, cross-platform engine for Bayesian data analysis that can be accessed from within R. Many of the exercises will require programming. RStudio is an integrated development environment for R. Some students prefer Python to R. You may use your preferred software as long as your solution is clear and I can understand what you did, but the solutions and examples will all be in R.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, midterm and final project.\n\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on Bb. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nSelected references\n\nBishop, Christopher M., Pattern Recognition and Machine Learning. Springer, Information Science and Statistics series, 2006.\nMacKay David J. C., Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.\nHoff, Peter D., A First Course in Bayesian Statistical Methods. Springer, 2009.\nComputer code is available for most of the examples in the book.\nGelman, A., Carlin, J., Stern, H., Dunson, D. B., Vehtari, A. and Rubin, D., Bayesian Data Analysis (3rd edition). CRC Press, 2013.\nReference tex. This comprehensive text has become the standard reference in Bayesian statistical methods. The hyperlink below contains reviews, exercises, data sets and software.\nMarin, Jean-Michel and Robert, Christian, Bayesian Essentials with R (2nd edition). Springer, 2014.\nSupplemental text (recommended): This recently published book provides comprehensive coverage of computational Bayesian statistics with a focus on conducting Bayesian analyses of real data sets. The range of topics covered is much more extensive than the Hoff text, and will serve as a useful supplement for readers interested in Bayesian treatment of topics not covered in this course, such as generalized linear models, capture-recapture experiments, time series and image analysis. R code and a solution manual are available.\n\nLee, Peter, Bayesian Statistics: An Introduction (4th edition), Wiley, 2012. Alternate text: The text by Peter Lee is accessible and may be helpful as an alternative treatment. Again, the hyperlink contains additional information, including exercises, solutions, errata and software.\n\n\n\nAdditional Resources\n\nWakefield, J. C., et al. “The evaluation of fibre transfer evidence in forensic science: a case study in statistical modelling.” Journal of the Royal Statistical Society Series C: Applied Statistics 40.3 (1991): 461-476. pdf\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\n\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\n\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\n\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/750.html",
    "href": "courses/750.html",
    "title": "OR 750. Bayesain Learning",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2021\n\nVideos\n\nCourse Material\n\nCourse Number: OR 750\nLocation: Zoom Thu 4:30-7:10pm\nInstructor: Vadim Sokolov\nOffice hours: by Appointment\nPrerequisites: Undergraduate Calculus, probability theory, statistics, computer programming skills (ideally R).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nThis is a graduate course on Bayesain learning. Although basics will be revisited, the pace will be swift so we can get to advanced topics as quickly as possible. This course details classical Bayesain techniques as well as modern approaches from both statistics and machine learning. We will consider some canonical examples of Bayesain analysis but will concentrate on modern Bayesain techniqes, computation and implementation, as well as modern applications. The course material will emphesize deriving and implementing methods over proving theoretical results.\n\nDuring weeks 11-15, this class will be run in a seminar mode. A student or the instructor will lead the discussion.\n\n\nTentative Schedule/List of Topics\n\nOverview: Conditional Probability, Bayes Rule, Bayesain inference, utility theory, distributions and tranformations\nHierarchical models: Bayesian regression, shrinkage (lasso, horseshoe)\nGaussian Process with applications in Bayesian Optimization – Tuning machine learning algorithms – Engineering model calibration\nAlgorithms – Markov Chain Monte Carlo (MCMC) – Expectation Maximization (EM) – Variational Bayes\nMarkov and Hidden Markov Models – Kalman Filter – Particle filter – Dynamic Linear Model – Structural Time Series Models\n\n\n\nGrading\nRubric: 30% HW, 10% Discussion, 60 % Final Project. No in-class examination.\nCutoffs: A: 90, B: 80, C: 70, F: &lt; 70\n\n\nOptional Texts\n\nPattern Recognition and Machine Learning by Bishop (page)\nMachine Learning: a Probabilistic Perspective by Murphy (page)\nAn Introduction to Bayesian Thinking by Clyde et al. (book)\nIntroduction to Statistical Thought by Lavine (book)\nThe Probability and Statistics Cookbook by Vallentin (page)\n\n\n\nPapers\n\nExchange Paradox by Christensen and Utts (pdf)\nInference for Nonconjugate Bayesian Models Using the Gibbs Sampler by Carlin and Polson (pdf)\nData Augmentation for Support Vector Machines by Polson and Scott (pdf)\nThe horseshoe estimator for sparse signals by Carvalho, Polson, Scott (pdf)\nDeep learning: A Bayesian perspective (pdf)\nBayesian regularization: From Tikhonov to horseshoe (pdf)\nSpatial Interaction and the Statistical Analysis of Lattice Systems by Besag (jstor)\nParticle learning and smoothing by Carlos M Carvalho, Michael S Johannes, Hedibert F Lopes, Nicholas G Polson (pdf)\nBayesian model assessment in factor analysis by Lopes and West (pdf)\nTracking epidemics with Google flu trends data and a state-space SEIR model by Dukic, Lopes and Polson (pdf)\nA statistical paradox by Lindley (pdf)\nThe philosophy of statistics by Lindley (pdf)\nThe Relevance Vector Machine by Tipping (pdf)\nBART: Bayesian additive regression trees by Hugh A. Chipman, Edward I. George, Robert E. McCulloch (arxiv)\nBayesian methods for hidden Markov models: Recursive computing in the 21st century by Scott (pdf)\nA modern Bayesian look at the multi‐armed bandit by Scott (pdf)\nBayesian analysis of computer code outputs: A tutorial by O’Hagan (paper)\nBayesian analysis of stochastic volatility models by Jacquier, Polson and Rossi (paper)\nOckham’s razor and Bayesian analysis by Jefferys and Berger (pdf)\nBayesian Learning for Neural Networks by Neal (pdf)\nMCMC Using Hamiltonian Dynamics by Neal (pdf)\nSlice sampling be Neal (pdf)\nBayesian interpolation by MacKay (pdf)\nBayesian online changepoint detectio by Adams and MacKay ()\nThe No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. by Matthew and Gelman (pdf)\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\nPrevious instances\n\nFall 2019\nFall 2017\nFall 2018"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Vadim Sokolov",
    "section": "Contact",
    "text": "Contact\nNguyen Engineering Building MS 4A6\nOffice: 2242\nFairfax, VA, 22030\nPhone: 703 993 4533\nvsokolov@gmu.edu"
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Vadim Sokolov",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise\n\nData science: Bayesian statistics, deep learning, reinforcement learning\nComplex Systems: Agent-based models, Bayesian optimization\nApplications: Urban systems modeling, digital twins"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Vadim Sokolov",
    "section": "Bio",
    "text": "Bio\nVadim Sokolov is an associate professor in the Systems Engineering and Operations Research Department at George Mason University. He works on building robust solutions for large scale complex system analysis, at the interface of simulation-based modeling and statistics. This involves, developing new methodologies that rely on deep learning, Bayesian analysis of time series data, design of computational experiments and development of open-source software that implements those methodologies. Inspired by an interest in urban systems he co-developed mobility simulator called Polaris that is currently used for large scale transportation networks analysis by both local and federal governments. Prior to joining GMU he was a principal computational scientist at Argonne National Laboratory, a fellow at the Computation Institute at the University of Chicago and lecturer at the Master of Science in Analytics program at the University of Chicago.\nHe has published in such leading statistics, mathematics and engineering journals, as the Annals of Applied Statistics, Transportation Research Part C, Linear Algebra and Its Applications as well as in Mechanical Systems and Signal Processing. He holds a PhD in computational mathematics from Northern Illinois University, and pursued graduate studies in statistics at the University of Chicago, while working at Argonne."
  }
]