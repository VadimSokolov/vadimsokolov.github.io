[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Google Scholar\nPolaris Project\nGeneology"
  },
  {
    "objectID": "research.html#highlights",
    "href": "research.html#highlights",
    "title": "Vadim Sokolov",
    "section": "highlights",
    "text": "highlights\n\nKramnik vs Nakamura or Bayes vs p-value, by Shiva Maharaj, Nick Polson and Vadim Sokolov. November 27, 2023. (pdf)"
  },
  {
    "objectID": "research.html#papers",
    "href": "research.html#papers",
    "title": "Vadim Sokolov",
    "section": "## papers",
    "text": "## papers"
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Vadim Sokolov",
    "section": "software",
    "text": "software\n\nPOLARIS: Designer. Developer. Transportation systems simulations framework (C++)\nGREET: Designer. Lead Developer. An implementation of The Greenhouse Gases, Regulated Emissions, and Energy Use in Transportation (GREET) Model. (C#, .NET, SQLite). more then 800 unique users within first year of release, 2013\nMATCOM: Contributor. Distributed on CD with Numerical Linear Algebra and Applications, Second Edition book By Biswa Nath Datta, SIAM. (MATLAB)\nTRANSIMS: Contributor. An agent-based forecast software for modeling regional transport systems. (C++); 22,295 total downloads since 2006\nAdvanced Numerical Methods II: Sole Developer. Package for solving large scale control problems. (Mathematica); an experimental library that was not published"
  },
  {
    "objectID": "research.html#research-coverage",
    "href": "research.html#research-coverage",
    "title": "Vadim Sokolov",
    "section": "research coverage",
    "text": "research coverage\n\nDemystifying the future of connected and autonomous vehicles (Newswise\nArgonne wins grant to help transit agencies cope with emergencies (Chicago Tribune)\nArgonne will research how transportation systems should respond to natural hazards (WBEZ)\nWhat Happens When Developers, Scientists and Super-Computers Connect on Urban Design (Next City)\nUM wins $2.7M grant to study driverless cars (The Detroit News)\nUM teams with Argonne, Idaho national labs to study potential energy savings of connected vehicles (Michigan News)\nArgonne to study emergency response of Chicago transit (Chicago Sun Times)\nDesigning future cities (phys.org)\nUsing a Real Life SimCity to Design a Massive Development (Curbed Chicago)"
  },
  {
    "objectID": "index.html#current-teaching",
    "href": "index.html#current-teaching",
    "title": "Vadim Sokolov",
    "section": "Current Teaching",
    "text": "Current Teaching\n\nAI 600: Foundations of AI (page)"
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Vadim Sokolov",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise\n\nData science: Bayesian statistics, deep learning, reinforcement learning\nComplex Systems: Agent-based models, Bayesian optimization\nApplications: Urban systems modeling, digital twins"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Vadim Sokolov",
    "section": "Bio",
    "text": "Bio\nVadim Sokolov is an associate professor in the Systems Engineering and Operations Research Department at George Mason University. He works on building robust solutions for large scale complex system analysis, at the interface of simulation-based modeling and statistics. This involves, developing new methodologies that rely on deep learning, Bayesian analysis of time series data, design of computational experiments and development of open-source software that implements those methodologies. Inspired by an interest in urban systems he co-developed mobility simulator called Polaris that is currently used for large scale transportation networks analysis by both local and federal governments. Prior to joining GMU he was a principal computational scientist at Argonne National Laboratory, a fellow at the Computation Institute at the University of Chicago and lecturer at the Master of Science in Analytics program at the University of Chicago.\nHe has published in such leading statistics, mathematics and engineering journals, as the Annals of Applied Statistics, Transportation Research Part C, Linear Algebra and Its Applications as well as in Mechanical Systems and Signal Processing. He holds a PhD in computational mathematics from Northern Illinois University, and pursued graduate studies in statistics at the University of Chicago, while working at Argonne."
  },
  {
    "objectID": "courses/750.html",
    "href": "courses/750.html",
    "title": "OR 750. Bayesain Learning",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2021\n\nVideos\n\nCourse Material\n\nCourse Number: OR 750\nLocation: Zoom Thu 4:30-7:10pm\nInstructor: Vadim Sokolov\nOffice hours: by Appointment\nPrerequisites: Undergraduate Calculus, probability theory, statistics, computer programming skills (ideally R).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nThis is a graduate course on Bayesain learning. Although basics will be revisited, the pace will be swift so we can get to advanced topics as quickly as possible. This course details classical Bayesain techniques as well as modern approaches from both statistics and machine learning. We will consider some canonical examples of Bayesain analysis but will concentrate on modern Bayesain techniqes, computation and implementation, as well as modern applications. The course material will emphesize deriving and implementing methods over proving theoretical results.\n\nDuring weeks 11-15, this class will be run in a seminar mode. A student or the instructor will lead the discussion.\n\n\nTentative Schedule/List of Topics\n\nOverview: Conditional Probability, Bayes Rule, Bayesain inference, utility theory, distributions and tranformations\nHierarchical models: Bayesian regression, shrinkage (lasso, horseshoe)\nGaussian Process with applications in Bayesian Optimization – Tuning machine learning algorithms – Engineering model calibration\nAlgorithms – Markov Chain Monte Carlo (MCMC) – Expectation Maximization (EM) – Variational Bayes\nMarkov and Hidden Markov Models – Kalman Filter – Particle filter – Dynamic Linear Model – Structural Time Series Models\n\n\n\nGrading\nRubric: 30% HW, 10% Discussion, 60 % Final Project. No in-class examination.\nCutoffs: A: 90, B: 80, C: 70, F: &lt; 70\n\n\nOptional Texts\n\nPattern Recognition and Machine Learning by Bishop (page)\nMachine Learning: a Probabilistic Perspective by Murphy (page)\nAn Introduction to Bayesian Thinking by Clyde et al. (book)\nIntroduction to Statistical Thought by Lavine (book)\nThe Probability and Statistics Cookbook by Vallentin (page)\n\n\n\nPapers\n\nExchange Paradox by Christensen and Utts (pdf)\nInference for Nonconjugate Bayesian Models Using the Gibbs Sampler by Carlin and Polson (pdf)\nData Augmentation for Support Vector Machines by Polson and Scott (pdf)\nThe horseshoe estimator for sparse signals by Carvalho, Polson, Scott (pdf)\nDeep learning: A Bayesian perspective (pdf)\nBayesian regularization: From Tikhonov to horseshoe (pdf)\nSpatial Interaction and the Statistical Analysis of Lattice Systems by Besag (jstor)\nParticle learning and smoothing by Carlos M Carvalho, Michael S Johannes, Hedibert F Lopes, Nicholas G Polson (pdf)\nBayesian model assessment in factor analysis by Lopes and West (pdf)\nTracking epidemics with Google flu trends data and a state-space SEIR model by Dukic, Lopes and Polson (pdf)\nA statistical paradox by Lindley (pdf)\nThe philosophy of statistics by Lindley (pdf)\nThe Relevance Vector Machine by Tipping (pdf)\nBART: Bayesian additive regression trees by Hugh A. Chipman, Edward I. George, Robert E. McCulloch (arxiv)\nBayesian methods for hidden Markov models: Recursive computing in the 21st century by Scott (pdf)\nA modern Bayesian look at the multi‐armed bandit by Scott (pdf)\nBayesian analysis of computer code outputs: A tutorial by O’Hagan (paper)\nBayesian analysis of stochastic volatility models by Jacquier, Polson and Rossi (paper)\nOckham’s razor and Bayesian analysis by Jefferys and Berger (pdf)\nBayesian Learning for Neural Networks by Neal (pdf)\nMCMC Using Hamiltonian Dynamics by Neal (pdf)\nSlice sampling be Neal (pdf)\nBayesian interpolation by MacKay (pdf)\nBayesian online changepoint detectio by Adams and MacKay ()\nThe No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. by Matthew and Gelman (pdf)\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\\\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\\\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\\\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services.\n\n\nPrevious instances\n\nFall 2019\nFall 2017\nFall 2018"
  },
  {
    "objectID": "courses/664.html",
    "href": "courses/664.html",
    "title": "SYST/OR 664 CSI 674. Bayesian AI",
    "section": "",
    "text": "Spring 2025\n\nInstructor: Vadim Sokolov\n\nLocation: Engineering Building 2241\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nZoom Link: https://gmu.zoom.us/j/98895158802?pwd=3fGVsYOe332Rw7B2AaZkb2LmzRbXU2.1\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nSlides\nLecture Notes\n\nHWs: HW1, HW2, HW3,HW4,HW5\n\n\n\nContent and goals\nMany artificial intelligence problems involve modeling uncertainty. Bayesian probabilistic models represent uncertainty and dependencies between random variables using probability distributions. You will learn the set of rules of probability and computational algorithms to manipulate these distributions. Bayesian approach enhances the effectiveness of conventional AI techniques. This course summarizes various Bayesian-based models and the standard algorithms used with them, supplemented by instances of their practical use. We will discuss applications in science, engineering, economics, medicine, sport, and law. Students will learn the commonalities and differences between the Bayesian and frequentist approaches to statistical inference, how to approach a statistics problem from the Bayesian perspective, and how to combine data with informed expert judgment soundly to derive useful and policy-relevant conclusions. Assignments focus on applying the methods to practical problems.\n\n\nList of Topics\n\nUnit 1: Introduction: History of AI, Bayes approach to ML and AI. Probability and Bayes Rule.\nUnit 2: Utility and Decision Theory\nUnit 3: Bayesian Inference with Conjugate Pairs: Single Parameter Models\nUnit 4: Bayesian Hypothesis Tests\nUnit 5: Stochastic Processes\nUnit 6: Markov Chain Monte Carlo\n\nUnit 7: Bayesian Regression: Linear and Bayesian Trees\nUnit 8: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\nUnit 9: Bayesian Double Descent and Model Selection: Modern Approach to Bias-Variance Tradeoff\nUnit 10: Bayesian Neural Networks and Deep Learning\n\n\n\nSchedule\n\nJan 27: First Class\nFeb 17: We will not meet, I will post a video lecture\nMarch 3: HW3 due\nMarch 10: HW4 due\nMarch 10: No class (spring break)\nMarch 17: Midterm exam\nMarch 31: HW5 due\nApril 7: Project proposal due\nApril 14: HW6 due\nApril 28: Project Draft due\nMay 5: Last class, project presentations\nMay 12: Final project due\n\n\n\nAssignments\nHomework is due 11:59PM on the assigned due date. If you have extenuating circumstances, please contact me in advance, and I will consider giving you additional time to complete the assignment for partial credit. Assignments will be posted here and on Blackboard. Please submit your assignments through Blackboard.\nExams are take home and will be posted on Blackboard.\n\n\nPrerequisites\nThe formal listed prerequisite is OR 542 or STAT 544 or STAT 554 or equivalent.\n\nThe real prerequisites are:\n\nExperience with elementary data analysis such as scatterplots, histograms, hypothesis tests, confidence intervals, and simple linear regression.\nA calculus-based probability course - elementary probability theory, discrete and continuous probability distributions, probability mass and probability density functions, cumulative distribution functions, common parametric models such as the normal, binomial and Poisson distributions.\nExperience with a high-level programming language. We will use R, a programming language for data analysis, and STAN, a language for specifying and performing inference with Bayesian models.\nComfort with mathematical notation. We will not do proofs, but you will be expected to be comfortable following and doing mathematical derivations.\n\n\n\nComputing\nWe will use R, a powerful (free) statistical graphics and computing language, and STAN, an open-source, cross-platform engine for Bayesian data analysis that can be accessed from within R. Many of the exercises will require programming. RStudio is an integrated development environment for R. Some students prefer Python to R. You may use your preferred software as long as your solution is clear and I can understand what you did, but the solutions and examples will all be in R.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, midterm and final project.\n\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on Bb. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nSelected references\n\nBishop, Christopher M., Pattern Recognition and Machine Learning. Springer, Information Science and Statistics series, 2006.\nMacKay David J. C., Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.\nHoff, Peter D., A First Course in Bayesian Statistical Methods. Springer, 2009.\nComputer code is available for most of the examples in the book.\nGelman, A., Carlin, J., Stern, H., Dunson, D. B., Vehtari, A. and Rubin, D., Bayesian Data Analysis (3rd edition). CRC Press, 2013.\nReference tex. This comprehensive text has become the standard reference in Bayesian statistical methods. The hyperlink below contains reviews, exercises, data sets and software.\nMarin, Jean-Michel and Robert, Christian, Bayesian Essentials with R (2nd edition). Springer, 2014.\nSupplemental text (recommended): This recently published book provides comprehensive coverage of computational Bayesian statistics with a focus on conducting Bayesian analyses of real data sets. The range of topics covered is much more extensive than the Hoff text, and will serve as a useful supplement for readers interested in Bayesian treatment of topics not covered in this course, such as generalized linear models, capture-recapture experiments, time series and image analysis. R code and a solution manual are available.\n\nLee, Peter, Bayesian Statistics: An Introduction (4th edition), Wiley, 2012. Alternate text: The text by Peter Lee is accessible and may be helpful as an alternative treatment. Again, the hyperlink contains additional information, including exercises, solutions, errata and software.\n\n\n\nAdditional Resources\n\nWakefield, J. C., et al. “The evaluation of fibre transfer evidence in forensic science: a case study in statistical modelling.” Journal of the Royal Statistical Society Series C: Applied Statistics 40.3 (1991): 461-476. pdf\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\n\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\n\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\n\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/568.html",
    "href": "courses/568.html",
    "title": "SYST/OR 568. Applied Predictive Analytics - Mason Analytics MS",
    "section": "",
    "text": "George Mason University\nFall 2023\n\nCourse Material\nVideo Lectures\nTA: Raina Joy Saha (rsaha3 (at) gmu.edu)\nInstructor: Vadim Sokolov\n\nLocation: Krug Hall 7\nTime: Mondays 4:30 pm - 7:10 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to BlackBoard\n\n\nContent and goals\nIntroduces predictive analytics with applications in engineering, business, finance,health care, and social economic areas. Topics include time series and cross-sectional data processing, data visualization, correlation, linear and multiple regressions, classification and clustering, time series decomposition, factor models and causal models, predictive modeling performance analysis, and case study. Provides a foundation of basic theory and methodology with applied examples to analyze large engineering, social, and econometric data for predictive decision making. Hands-on experiments with R will be emphasized.\n\n\nList of Topics\n\nPredicting with probability (3 Ch 2,)\nData and Statistics (5 Ch 4,)\nLinear regression (3 Ch)\nClassification (ISLR Ch 4, APM Ch 12\nLasso and Model Selection (ISLR Ch 6)\nTree-based methods (ISLR Ch 8)\nEstimation\nBayesian Inference\nTime series forecasting (html notes, pdf notes)\n\n\n\nSchedule\n\nAug 21: First Class\nSept 4: Labor Day : University Closed\nSep 11: HW 1 Due\nSep 25: HW 2 Due\nOct 9: Fall Break (Classes Do Not Meet)\nOct 10: We have a class\nOct 16: HW 3 Due\nOct 23: In-class Midterm\nOct 30: Final Project Proposal\nNov 6: Hw 4 Due\nNov 20: Hw 5 Due\nNov 27: Last class, project presentations\nDec 3: Final Projects Due\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 5 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Instructor will have 1-2 data sets/analysis problems, in case students have hard time identifying it on their own. Work on the final project can begin as soon as class starts. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn R. Python is good choice as well. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\nA great way to start learning is to buy a book and start working through tutorials. A good guide is Adler’s Nutshell. They have many tutorials to help you get up to speed. You can browse other options by searching ‘R statistics’ on Amazon. If you are new to R (and even if not) you should complete a tutorial to familiarize yourself with the language. A great option is the TryR code school.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nOptional Textbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nKuhn and Johnson, Modeling, Springer, 2013.\nHyndman and Athanasopoulos, Practice, OTexts, 2013.\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "courses/41000/syllabus.html",
    "href": "courses/41000/syllabus.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "href": "courses/41000/syllabus.html#business-statistics-41000-syllabus",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Course Site:vsokolov.org/courses/41000\nInstructor: Vadim Sokolov\nPhone: (815) 793 1428\nEmail: vadim.sokolov@chicagobooth.edu\n\n\nThis course focuses on the application of data analytics in business decisions. You will learn how to visualize data sets, use tools of statistics to gain insights and to predict. You will learn how to make decisions when future is uncertain. It covers both basic underlying concepts and practical computational skills. We will apply those skills to analyze a variety of complex real-world problems. The techniques covered include (i) graphical data visualization; (ii) probability and A/B testing; (iii) decisions under uncertainty; (iv) predictive models: linear, logistic and multiple regression; (v) deep learning\n\n\n\nThe course website provides a self-contained set of notes for the course and has datasets, R code, and midterm examples.\nI recommend OpenIntro stats. It is free and available online!\n\n\n\nMidterm 35% + take-home final project 35% + Homework 30%. \\\nThere are four homework assignments (every other week). Students are encouraged to form groups (of at most three) for homework. You can either submit as a group or individually. Homework assignments should be submitted online to Canvas and should have a clear and professional presentation. You can submit homework late with no penalty before they get graded (you are simply taking a chance that your HW won’t be graded if submitted past due). Homeworks will be graded on a check plus, check, check minus basis.\\\nThe final take-home project can be done individually or in a group. The project will be graded 50% on writing and presentation and 50% on statistical analysis.\\\nRe-grade requests should be written, detailing the reason for a re-grade. The whole exam will be subject to regrade. Regrade requests should be on a timely basis and are accepted up to a week after the work has been returned.\n\n\n\nWe will use R in the class. I recommend investing the time to learn R. The course website provides many resources to help you achieve this goal. R is the dominant software package for real world Predictive Analytics and is used throughout other courses. This open-source software is available for free download at www.r-project.org and you can find documentation there.\\\nWe will demonstrate data analysis in class. The website contains code filed for the code that generated the lecture notes. You may use either software for your project.\n\n\n\nThere are no prerequisites for the course. For a first class assignment reading the chapters 1-4 of the textbook will give you a good idea of the level of the class.\n\n\n\nSee course website.\n\\\\ Students must adhere to our Booth Honor Code standards “I pledge my honor that I have not violated the Honor Code during this examination or assignment”."
  },
  {
    "objectID": "courses/41000/41000.html",
    "href": "courses/41000/41000.html",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "",
    "text": "Instructor: Vadim Sokolov\nvadim.sokolov@chicagobooth.edu\nTA: Ayman Moawad\naymoawad@uchicago.edu\nSyllabus\nDatasets \nAll examples\nCourse Textbook: OpenIntro Statistics pdf, tablet"
  },
  {
    "objectID": "courses/41000/41000.html#class-notes",
    "href": "courses/41000/41000.html#class-notes",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Class Notes",
    "text": "Class Notes\n\nSection 1: Introduction and Probability: notes, code\nSection 2: Statistics and Data: notes, code, epl, abtesting\nSection 3: Regression: notes, code, housing, mammals\nSection 4: Classification: notes, tree notes newfood, OJ, PGA, Tennis, logistic\nSection 5: AI and Deep Learning: notes, Examples: MNIST; NN for Circle Data, IMDB, Trump Tweets, trump-tweets.R"
  },
  {
    "objectID": "courses/41000/41000.html#hw",
    "href": "courses/41000/41000.html#hw",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "HW",
    "text": "HW\n\nHW1 (code)\nHW2 (code)\nHW3 (code)\nHW4 (code\nFinal Project (code)"
  },
  {
    "objectID": "courses/41000/41000.html#midterm",
    "href": "courses/41000/41000.html#midterm",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Midterm",
    "text": "Midterm\n\nQuestions\nQuestions\nMidterm 1\nMidterm 2\nMidterm 3\nMidterm 4\nMidterm 5\nMidterm 6\nMidterm 7\nMidterm 8\nMidterm 9\nMidterm 10\nMidterm 11"
  },
  {
    "objectID": "courses/41000/41000.html#r",
    "href": "courses/41000/41000.html#r",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "R",
    "text": "R\nI made a quick introduciton to R, video is here and here are the slides.\n\nTo get RStudio on you computer, you need to first download and install RStudio then you need to download and install R.\nFurther, I recommend doing those two courses on DataCamp.\n\nIntroduction to R\nIntroduction to Data\n\n\nIn addition, Booth offers access to an online R tutorial on LinkedinLearning. To access the training, go to: here and login using your CNetID and password. You may find it helpful to work through the following R courses offered on LinkedInLearning:\n\nCode Clinic: R with Mark Niemann-Ross\nR Statistics Essential Training with Barton Poulson\nUp and Running with R with Barton Poulson"
  },
  {
    "objectID": "courses/41000/41000.html#demos",
    "href": "courses/41000/41000.html#demos",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Demos",
    "text": "Demos\n\nPortfolio Variance\nBinomial\nNormal Distribution"
  },
  {
    "objectID": "courses/41000/41000.html#other-materials-and-links",
    "href": "courses/41000/41000.html#other-materials-and-links",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Other Materials and Links",
    "text": "Other Materials and Links\n\nAI in Tennis\nKraft-Heinz blog store\nKraft-Heinz WSJ\nFrito-Lay potato peeling\nTyson\nAnheuser-Busch InBev payments\nWalmart smart\nThe State of Data Science and MachineLearning\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb Random Forest\nFacebook regrsssion\nYoutube Deep learning\nUber: time series\nExploratory Data Analysis\nOverfitting\n2016 Election"
  },
  {
    "objectID": "courses/41000/41000.html#deep-learning",
    "href": "courses/41000/41000.html#deep-learning",
    "title": "Business Statistics 41000 - Booth MBA",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learnig\nShort introduction into deep leanring\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop"
  },
  {
    "objectID": "courses/468.html",
    "href": "courses/468.html",
    "title": "SYST/OR 468. Applied Predictive Analytics",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2019\n\nFor course materials click here\n\nSYST 468 is an undergraduate course focused on applying statistical and machine learning methodologies to develop predictive models. We will learn both classical methods for regression and classification, such as linear regression and logistic regression as well as new methods such as deep learning. We will consider applications in engineering, finance and artificial intelligence. There will be an emphasis on assumptions and interpretation. Although basics of probability and statistics will be revisited, it is targeted towards students who have completed (and remember the concepts from) a course in introductory statistics. We will make extensive use of computational tools, such as the R language for statistical computing, both for illustration in class and in homework problems.\n\nAnnouncements\n\n04/08/2019: HW3 posted, due date April 17.\n03/09/2019: HW2 posted, due date April 3.\n02/24/2019: Midterm is on March 20 (open book)\n02/24/2019: No class on March 13 (Spring Break)\n02/24/2019: H1 Due date is March 6\n10/10/2018: Check back regularly for announcements\n10/10/2018: First Class is on January 23\n\n\n\nCourse staff\nLecture Notes: Will be made available one-day in advance on Bb\nInstructor: Vadim Sokolov\nOffice: Eng Building, Room 2242\nvsokolov(at)gmu.edu\nTel: 703-993-4533\nTA: TBA\n\n\n\nOffie hours\nVadim Sokolov: Wed 2:30-4:30pm (at Engineering 2242)\nTA: Jungho Park: Mon 1-3pm (at Egnineering 2216)\n\n\n\nLectures\nLocation: Planetary Hall 206\nTimes: 4:30-7:10pm on Wednesday\n\n\nGrades\nGrade composition: Grade based on participation in class, in-class midterm, homework assignments, and final project.\n\n\nTextbooks\n\nDiez, Barr and Cetinkaya-Rundel Statistics, OpenIntro, 2015\nJames, Witten, Hastie and Tibshirani, R, Springer, 2009.\nAdditional reading: List\n\n\n\nLinks\n\nModel Interpretability\nData Sceince Survey\nJ. M. Keynes’s Investment Performance: A Note\nBayes Search\nHow Y’all (NY) Times\nMonte Hall\nEPL\nData Cleaning\nAirbnb RF\nFacebook regrsssion\nYoutube DL\nUber Time Series\nEDA\nOverfitting\n2016 Election\n\n\n\nDeep Learning\n\nCourse by Jeremy Howard on coding aspects of deep learning\nShort introduction into deep learning\nArchitectures\nKeras\nConvNetJS\nVisualization\nMomentum\nBackprop\n\n\n\nPrevious instances\n\nSpring 2018"
  },
  {
    "objectID": "courses/610.html",
    "href": "courses/610.html",
    "title": "SYST/OR 610. Deep Leanring",
    "section": "",
    "text": "Department of Systems Engineering and Operations Research\nGeorge Mason University\nSpring 2022\n\n\n\n\n\nInstructor: Vadim Sokolov\nLocation and time: Aquia, room 347; 7:20-10pm Mondays\nOffice hours: By appointment\n\n\nDatacamp\nIf you are rusty on Python, I suggest you refresh your skills using Datacamp. Datacamp gave students in this class a free access to all of the courses. If you follow the link above you can get your free access using masonlive email. I also listed some of the Python courses I suggest #there.\n\n\nList of topics and tentative schedule\n\nBasics (Weeks 1-2)\n\nLinear Algebra: intro\nProbability: OpenIntro Ch 3\nGeneralized Linear Models: OpenIntro Ch 8,9\nPyTorch: PyTorch Basics\nFeed Forward Architectures: WHAT IS TORCH.NN REALLY?; Ripley Ch 5; Bishop Ch 3,4\n\nConvex Optimization (Weeks 3-4)\n\nBackpropagation and matrix derivatives\nStochastic gradient descent and its variants (ADAM, RMSpropr, Nesterov acceleration): Bishop Ch 7, Goodfellow Ch 8\nSecond order methods: Bishop Ch 7\nADMM\nRegularization (l1, l2 and dropout): dropout paper, Godfellow Ch 7\nBatch normalization: paper\n\nConv Nets and Image Processing (Week 5): Goodfellow Ch 9\nRecurrent Nets and Sequential Data (Week 6): Good Fellow Ch 10, seq2seq, Pytorch eq2seq tutorial\nTheory of deep learning (Week 7): see theory section for the reading list\n\nUniversal approximators\nCurse of dimensionality\nKernel spaces\nTopology and geometry\n\nProbabilistic DL (Weeks 8-9) Langevin, MCMC, , VB\n\nConjugate distributions, exponential family Bishop: Ch 2\nModel choice\nHierarchical linear and generalize linear models (regression and classification): Bishop Ch 10\nModels for missing data (EM-algorithm)\nBayes computations (MCMC, Variational Bayes)\n\nAdditional Topics (Weeks 10-13)\n\nModel Visualization Tensorboard\nGenerative Models (normalizing flows, GANs, recurrent nets): NF Paper Tutorial; NF; \nAttention and Transformers attention paper\nDeep Reinforcement Learning DRL Tutorial\nBayesian Optimisation: Hyperparameter selection and parameter initialization Hyperopt\n\n\n\n\nData analysis projects\nYou will work in a team of up to 3 people on a Kaggle-like project and will apply deep learning to solve a prediction or data generation problem. By week 8 of the class you should have a team formed and data set + analysis problem identified. You need to submit a 0.5-1 page description of the data and problem you are trying to solve for my feedback and approval. Proposal has to have names and emails of the team members. Description of data set, problem to be solved and proposed architectures.\nYou will post results of your analysis on the class blog post. The final project will be graded on presentation, writing and analysis.\n\n\n\nGroup Work\nBoth projects and homework can be done in a groups of size of up to 3 people. You can change groups in between. If you do a homework in a gorup, it means that all of the members of the group do it individually and can consult with each other. You can also do 1 submission per group if you prefer. You can use “group” section of the piazza page to find teammates if you need any. If you need help finding a group, please email me.\n\n\nGrading\nEach hw is 10 points, project is 30.\n\nThis is a graduate level course focused on developing deep learning predictive models. We will learn both practical and theoretical aspects of deep learning. We will consider applications in engineering, finance and artificial intelligence. It is targeted towards the students who have completed an introductory courses in statistics and optimization. We will make extensive use of computational tools, such as the Python language, both for illustration in class and in homework problems. The class will consist of 9 lectures given by the instructor on several advanced topics in deep learning. At another 5 lectures students will present on a given topic.\n\n\nBooks\n\nPolson, Sokolov notes\nDive into Deep Learning link\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.\nRipley, Brian D. Pattern recognition and neural networks. Cambridge university press, 2007.\nBishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.\n\n\n\nPer Topic Resources\n\nArchitectures\n\nTuning CNN architecture (blog)\nSequence to Sequence Learning with Neural Networks (paper)\nSkip RNN (paper)\nLearning the Enigma with Recurrent Neural Networks (blog)\nLSTM blog\nGenerative Adversarial Networks (presentation)\nGANs at OpenAI (blog)\nAdaptive Neural Trees (paper)\nCortex\nRecognition\nNetworks\nModeling\nsolution\nNeed\nNetworks\nAutoencoders\nWaveNet\nPixelCNN\nhttps://chrisorm.github.io/NGP.html\n\n\n\nOptimization\n\nBook\n(1970)\nLecture\n(1983)\n(1964)\nLearning\n(2004)\nHOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (paper)\nSGD (link)\nSampling\nDynamics\nOptimization\ncode) (\ncode) (\nSearch\nlearning\nMinima\nWorks\nMinima\nNets\nDNNs\nLearning\nAcceleration\n\n\n\nTheory\n\nPolyak, Boris, and Pavel Shcherbakov. “Why does Monte Carlo fail to work properly in high-dimensional optimization problems?.” Journal of Optimization Theory and Applications 173, no. 2 (2017): 612-627. (paper)\nLeni, Pierre-Emmanuel, Yohan D. Fougerolle, and Frédéric Truchetet. “Kolmogorov superposition theorem and its application to multivariate function decompositions and image representation.” In Signal Image Technology and Internet Based Systems, 2008. SITIS’08. IEEE International Conference on, pp. 344-351. IEEE, 2008. (paper)\nKlartag, Bo’az. “A central limit theorem for convex sets.” Inventiones mathematicae 168, no. 1 (2007): 91-131. (slides), \nSun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting unreasonable effectiveness of data in deep learning era.” In Computer Vision (ICCV), 2017 IEEE International Conference on, pp. 843-852. IEEE, 2017. (blog)\nBengio, Yoshua, Aaron Courville, and Pascal Vincent. “Representation learning: A review and new perspectives.” IEEE transactions on pattern analysis and machine intelligence 35, no. 8 (2013): 1798-1828. (paper)\nBraun, Jürgen. “An application of Kolmogorov’s superposition theorem to function reconstruction in higher dimensions.” (2009). (dissertation)\nKolmogorov. “On the Representation of Continuous Functions of Several Variables as Superpositions of Continuous Functions of a Smaller Number of Variables” (paper)\nArnold. “On functions of three variables” (papers)\nBianchini, Monica, and Franco Scarselli. “On the complexity of shallow and deep neural network classifiers.” In ESANN. 2014.(paper)\nGirosi, Federico, and Tomaso Poggio. “Representation properties of networks: Kolmogorov’s theorem is irrelevant.” Neural Computation 1, no. 4 (1989): 465-469. (paper)\nKůrková, Věra. “Kolmogorov’s theorem and multilayer neural networks.” Neural networks 5, no. 3 (1992): 501-506. (paper)\nPoggio, Tomaso, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. “Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.” International Journal of Automation and Computing 14, no. 5 (2017): 503-519. (paper)\nTelgarsky, Matus. “Representation benefits of deep feedforward networks.” arXiv preprint arXiv:1509.08101 (2015). (paper)\nMontufar, Guido F., Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. “On the number of linear regions of deep neural networks.” In Advances in neural information processing systems, pp. 2924-2932. 2014. (paper)\nZhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. “Understanding deep learning requires rethinking generalization.” arXiv preprint arXiv:1611.03530 (2016). (paper)\nLin, Henry W., Max Tegmark, and David Rolnick. “Why does deep and cheap learning work so well?.” Journal of Statistical Physics 168, no. 6 (2017): 1223-1247. (paper)\nStéphane Mallat 1: Mathematical Mysteries of Deep Neural Networks (video)\nLearning\naddition\nNetworks\nNetworks\n\n\n\nReinforcement Learning\nOptimization Models - Truth - Yet\n\n\nBayesian DL\n\nVAE with a VampPrior (paper)\nBayesian DL (blog)\nRecognition Networks for Approximate Inference in BN20 Networks (paper)\nNon-linear regression models for Approximate Bayesian Computation (paper)\nDR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression (paper)\nFast ε-free Inference of Simulation Models with Bayesian Conditional Density Estimation (paper)\nAuto-Encoding Variational Bayes (paper)\nComposing graphical models with neural networks for structured representations and fast inference (paper)\nInference\n\n\n\nPractical Tricks\n\nAveraging\n\nNormalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks (paper)\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\nAuto-Encoding Variational Bayes (paper)\nTwin Networks: Using the Future as a Regularizer (paper)\nDon’t Decay the Learning Rate, Increase the Batch Size (paper)\nDL Tuning (blog)\nSurvey\n\n\n\n\nOther Resources\n\nAdditional Reading List\n\n50 Years of Data Science by Donoho (paper)\nOverview\nNetworks\nlearning\n\n\n\nBlogs\n\nPapers with code\nSecurity\nUnsupervised learning\nCybersecurity\nOpt Visualization\nAI and Memory Wall\n\n\n\nVideos\n\nDeep Energy\nDL Summer school 2015\nDL Representations\nPyData 2017\n\n\n\nOther courses with good web presence\n\nStanford’s CS231n\nStanford’s STATS385\nfast.ai\nNando de Freitas’ course on machine\nUC Berkeley Stat241B\nMIT\nUdacity DRL\n\n\n\nTools\n\nTensorFlow\nKeras\nTF Playground\nSony\nSnakeViz python profiler\nPyTorch\nTS Stan Examples\nOpenAI Glow\n\n\n\nMisc Links\n\nPytorch resources (a curated list of tutorials, papers)\nIs artificial intelligence set to become art’s next medium?"
  },
  {
    "objectID": "courses/664/final.html",
    "href": "courses/664/final.html",
    "title": "Final Project Guidelines for SYST/OR 664 CSI 674. Bayesian Inference and Decision Theory",
    "section": "",
    "text": "The final project can be done in a group (up to 3) or as is an individual project. You have a lot of freedom in choosing a topic for your final project. The only criterion is that it deeply involves applying Bayesian data analysis to a real-world problem. You choose a dataset, an interesting question about it, and address it with Bayesian modeling.\nThere are four milestones:\n\nProject proposal (due April 7)\nProject Presentation Draft (due April 26)\nPresentation (April 29).\nFinal Project Report (due May 5)\n\nThe project proposal is to have 3 paragraphs on\n\nQuestion you are trying to answer and why this question is interesting or important. How your analysis changes decision making process? For example if you are forecasting demand for a product, how would the company change its production process if they had a better forecast?\nData you are using. If you are using a dataset, where did it come from?\nWhat model are you going to use for the data analysis?\n\nThe proposal should be up to 1 pages long.\nThe report is up to 5 pages long, excluding figures or references. If you wish to add more materia, put it into appending. However, the main 5-page body should be self-contained and complete. The report should be written in a clear and concise manner, and should be well-organized. The report should include the following sections:\n\nIntroduction: Clear description of the problem. Describe the problem you are trying to solve, why it is important or useful, and summarize any important pieces of prior work that you are building upon.\nDataset: Clear description/visualization of the data. Describe the dataset or datasets you are working with. Show examples from the datasets. If you collected or constructed your own dataset, explain the process you used to collect the images and labels, and why you made the choices you did in the data collection process.\nModels/Methods: Clear and thorough description of statistical analysis. Describe the method you are using; this may also contain parts of the implementation of your model, loss function, or other components along with sanity checks to ensure that those components are correctly implemented.\nExperiments: Clear and thorough interpretation of results. Describe the experiments you did, and key results and figures that you obtained. This may interleave explanations of the experiments you run and figures you generate as a result of those experiments.\n\nYou should turn in a .pdf file containing your final report, together wih the notebooks/markdowns containing all the code and the generated results (tables, figures etc) that are included in the report. You must run all cells in your notebook to receive credit; we will not rerun your notebook.\nBoth project and presentation will be evaluated on technical depth, and writing/presentation quality (50/50). Some points to keep in mind\n\nVisualizations should be used judiciously to report findings. Do not overload reader with tables/figures.\nFigures should be appropriately labeled: x-axis, y-axis, legend, and title.\nFigures and tables should be numbered and referenced properly in the write-up.\nOutput of posterior predictive checks should be properly interpreted not merely stated\nAt the end, you need to connect your results to the initial question of investigation.\nNo more than 10 lines of text per slide."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Full CV"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Vadim Sokolov",
    "section": "Education",
    "text": "Education\n\nPh.D., 2008, Computational Mathematics, Northern Illinois University\nDiploma (summa cum laude), 2004, Applied Mathematics, Rostov State University\nGraduate Studies in Statistics, 2013-2014, University of Chicago"
  },
  {
    "objectID": "cv.html#positions",
    "href": "cv.html#positions",
    "title": "Vadim Sokolov",
    "section": "Positions",
    "text": "Positions\n\nAssociate Professor, Systems Engineering and Operations Research, George Mason University, 2023-present\nAsistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nVisiting Assistant Professor, Statistics,The University of Chicago Booth School of Business, 2019-2023\nAssistant Professor, Systems Engineering and Operations Research, George Mason University, 2016-2023\nLecturer, Master of Science in Analytics, University of Chicago, 2015-2016\nFellow, Computation Institute, University of Chicago, 2014-2016\nPrincipal Computational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2013-2016\nComputational Scientist, Transportation Research and Analysis Computing Center, Argonne National Laboratory, 2008-2013"
  },
  {
    "objectID": "msai/msai.html",
    "href": "msai/msai.html",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "",
    "text": "Application Information\nOfficial Program Catalog\nContact Information: cecmsai@gmu.edu (Vadim Sokolov, program director)\nSchedule an in-perosn meeting: Calendly"
  },
  {
    "objectID": "msai/msai.html#program-overview",
    "href": "msai/msai.html#program-overview",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Program Overview",
    "text": "Program Overview\nIn today’s rapidly evolving technological landscape, artificial intelligence has emerged as the most transformative force across industries, government, and society. Recent research reveals a critical challenge: while 89% of organizations are piloting or investing in AI developments, only 6% of employees feel very comfortable using AI in their roles. This widening gap between organizational adoption and workforce readiness represents one of the most significant barriers to innovation and economic growth.\nThe AI skills shortage has accelerated at an unprecedented rate, becoming the largest and fastest-growing tech skills gap in over 15 years. Almost twice as many technology leaders (51%) now report suffering an AI skills shortage compared to 28% just 18 months ago-an 82% jump that far outpaces other technical skills deficits. Despite this urgent need, only four in ten organizations are upskilling their current staff.\nThe disparity in AI skills access is particularly concerning. While 75% of companies have adopted AI technology, just one-third of employees received AI training in the past year. George Mason University’s Master of Science in Artificial Intelligence program directly addresses these challenges by preparing professionals with comprehensive knowledge and practical skills across the entire AI operations pipeline. Launching in Fall 2025, this innovative 30-credit program equips students to:\n\nIdentify real-world problems that benefit from AI solutions\nImplement secure and scalable AI systems across diverse computing platforms\nDevelop, train, and tune AI models to optimize performance\nAddress ethical considerations and evaluate AI systems against risk frameworks\nTranslate technical details into actionable insights for diverse audiences\n\nOur curriculum balances technical mastery with ethical responsibility, preparing graduates who can not only build AI technologies but also govern their responsible implementation across industries, government agencies, and public services."
  },
  {
    "objectID": "msai/msai.html#learning-journey",
    "href": "msai/msai.html#learning-journey",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Learning Journey",
    "text": "Learning Journey\nThe MS in Artificial Intelligence program consists of 30 credit hours, with 18 credits of core coursework and 12 credits from specialized topic areas. Students will gain both theoretical foundations and hands-on experience through a carefully designed sequence of courses.\n\nCore Courses\nAII 600: Foundations and Practice of Machine Learning for Artificial Intelligence (3 credits) Introduces the foundations of machine learning encountered in AI with emphasis on practical aspects. Students learn to analyze complex datasets, identify problems benefiting from AI solutions, and implement solutions using appropriate libraries and computing platforms while evaluating them against AI risk frameworks.\nAII 601: Planning and Decision Making for Intelligent Agents (3 credits) Explores planning domain representations, optimal search-based strategies, and methods for effective planning under uncertainty. Students master Markov Decision Processes for decision making, reinforcement learning methods, temporal difference learning, and policy gradient algorithms-essential components of modern robotics and game-playing AI systems.\nAII 602: Foundations and Practice of Deep Learning for Artificial Intelligence (3 credits) Provides theoretical motivation and practical implementation of neural networks and deep learning. Students master designing, training, fine-tuning, and monitoring deep networks, including explainability. The course covers supervised, self-supervised, and unsupervised learning approaches with applications in computer vision, natural language processing, deep reinforcement learning, and generative AI.\nAII 603: Engineering Artificial Intelligence Systems and Pipelines (3 credits) Delivers hands-on experience building, deploying, and evaluating large-scale AI technologies across industry sectors. Through team-based projects, students develop scalable API solutions for embedded, edge, and cloud computing platforms, preparing them for collaborative industry environments.\nGBUS 662: Management of IT (3 credits) Covers strategic, economic, and managerial aspects of managing an organization’s IT assets. Students learn to assess business value of IT within organizational structure and strategy, with discussions on management of IT infrastructure.\nME 576: AI: Ethics, Policy, and Society (3 credits) Examines pressing ethical and policy issues in AI, including transparency, privacy, surveillance, misinformation, fairness, algorithmic bias, justice, equity, trust, and labor practices. Students explore these topics through cutting-edge use cases and current events, developing frameworks for responsible AI governance.\n\n\nTopic Areas\nStudents must select at least one course from each of the following four topic areas, with advisor approval:\n1. Artificial Intelligence Policy, Ethics, and Society Sample courses:\n\nAIT 679: Law and Ethics of Big Data\nBIOD 760: National Security Technology and Policy\nGCP 604: New Technologies in the Global Economy\nME 575: AI Design and Deployment Risks\n\n2. Advanced Artificial Intelligence Sample courses:\n\nAIT 616: Interactive Machine Learning and Artificial Intelligence\nAIT 726: Natural Language Processing with Deep Learning\nOR 664: Bayesian Artificial Intelligence\nOR 774: Reinforcement Learning\n\n3. Scalable and Secure Artificial Intelligence Infrastructures Sample courses:\n\nAIT 670: Cloud Computing Security\nAIT 687: IoT and Edge Systems\nECE 554: Machine Learning for Embedded Systems\nCS 695/SWE 699: AI Safety and Assurance\n\n4. Use-inspired Artificial Intelligence Sample courses:\n\nAIT 636: Interpretable Machine Learning\nCYSE 689: Artificial Intelligence Methods for Cybersecurity\nSTAT 646: Probabilistic Machine Learning\nHAP 774: Artificial Intelligence in Health"
  },
  {
    "objectID": "msai/msai.html#program-highlights",
    "href": "msai/msai.html#program-highlights",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Program Highlights",
    "text": "Program Highlights\n\nExpert Faculty\nLed by the College of Engineering and Computing at George Mason University, this program brings together world-class faculty with extensive experience in AI research and application. Our instructors include professors from Computer Science, Statistics, Operations Research, Cyber Security, Public Policy who have over many years of combined teaching and research experience in AI and related fields. Many have published extensively in peer-reviewed journals specific to their disciplinary areas and AI, and serve as leaders in national organizations focused on artificial intelligence.\nThe program draws on interdisciplinary expertise from multiple GMU colleges, including the School of Computing, the College of Humanities and Social Sciences, the Schar School of Policy and Government, and the Donald G. Costello College of Business. This collaborative approach ensures students benefit from diverse perspectives on AI’s technical foundations, business applications, and societal impacts.\nFaculty members like Dr. Jana Kosecka specialize in computer vision and machine learning, while Dr. Zhengdao Wang focuses on intersection of AI and cybersecurity. The program also features experts in generative AI, differential privacy, and statistical machine learning like Dr. Anand Vidyashankar and Dr. Daniel Barbara, and researchers in Bayesian statistics and deep learning like Dr. Vadim Sokolov.\n\n\nIndustry Connections\nGeorge Mason University’s strategic location in Northern Virginia-a hub for technology innovation and government agencies-provides unparalleled opportunities for industry engagement. The College of Engineering and Computing maintains robust industry partnerships that support student success through:\n\nResearch collaborations with faculty on cutting-edge AI problems\nAccess to industry mentors and guest speakers\nOpportunities for internships and job placement\nCollaborative problem-solving through Engineering Artificial Intelligence course projects\n\nRecently, George Mason University was awarded a $1 million grant to establish the nation’s first Center for AI Innovation for Economic Competitiveness (CAIIEC), focused on helping small and medium enterprises adopt AI technologies. The university has also launched an AI-in-Gov Faculty Fellows Program to advance responsible, impactful use of AI in the public sector.\nThese initiatives, along with GMU’s Industry Partner Program, create a vibrant ecosystem where students can connect with potential employers and gain real-world experience implementing AI solutions. This network provides graduates not only with technical skills but also with professional relationships that accelerate their careers in the rapidly evolving field of artificial intelligence.\nBy joining GMU’s Master of Science in Artificial Intelligence program, you’ll become part of a community dedicated to addressing the AI skills gap while developing the next generation of ethical, innovative AI leaders."
  },
  {
    "objectID": "msai/msai.html#faq",
    "href": "msai/msai.html#faq",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "FAQ",
    "text": "FAQ\n\nIn this an in-person program? For the 2025-2026 academic year, the program will be offered in-person. The program may be offered online in the future. Although some of the courses may be offered online, the program is not designed to be fully online.\nIs there is a part-time option? The program is designed to be completed in 1.5-2 years, if you take 3-4 courses per semester (full-time). Students can take courses at their own pace, but the program is not designed to be fully self-paced. Note, that some of the courses are offered only once a year\nI work full-time, can I do this program? Yes, the program is designed to accommodate working professionals. The courses are offered in the evenings (4:30-7:10pm and 7:20-10pm), allowing you to balance your work and study commitments.\nWhat is the cost of the program? The tuition and fees for the program are subject to change. For the most up-to-date information, please visit the George Mason University tuition and fees page.\nWhat is the application process? The application process for the program is managed by the George Mason University Office of Admissions. For detailed information on how to apply, please visit the George Mason University admissions page or the MS in AI admissions page.\nMy background in math, stats and cs is weak, will I be able to succeed in this program? The program is designed to accommodate students with diverse backgrounds. However, some foundation in mathematics, statistics, and computer science will be rfequired for your success in the program. Prior to starting the prgram, each student will go through the assesment process to determine their readiness for the program. Think of this as an informal math-placement test. The assessment will include a review of your knowledge in mathematics, statistics, and computer science, including programming languages (Python, R, etc.), data structures, algorithms, and linear algebra. Based on the results, we will recommend to take additional non-credit and free short courses or complete self-study materials to strengthen your foundation in these areas. Further, every student will be taking the foundations in AI course (AII 600) in the first semester, which will cover the foundations of AI and machine learning. This course is designed to help students build a strong foundation in AI and machine learning concepts, regardless of their prior experience."
  },
  {
    "objectID": "msai/msai.html#usefule-links",
    "href": "msai/msai.html#usefule-links",
    "title": "Master of Science in Artificial Intelligence at Mason",
    "section": "Usefule Links",
    "text": "Usefule Links\n\nBehind the Curtain: A white-collar bloodbath\n‘Are We All Doomed?’ The CEO of Fiverr Says AI Is Definitely Taking Your Job. Here’s What to Do About It.\nGenerative AI and the future of work in America\nAI Skills Gap: The Biggest Tech Skills Shortage in 15 Years\nGoldman Sachs says generative A.I. could impact 300 million jobs — here’s which ones"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Nguyen Engineering Building MS 4A6\nOffice: 2242\nFairfax, VA, 22030\nPhone: 703 993 4533\nvsokolov@gmu.edu"
  },
  {
    "objectID": "courses/600.html",
    "href": "courses/600.html",
    "title": "AII 600. Foundations and Practice of Machine Learning for Artificial Intelligence. MS in AI",
    "section": "",
    "text": "George Mason University\nFall 2025\n\nCourse Material\n\nTextbook\n\nCanvas HW Submission\n\nInstructor: Vadim Sokolov\n\nLocation: Engineering Building, Room 2608\nTime: Wednesdays 7:20 - 10:00 pm\nOffice hours: By appointment\nPrerequisites: Graduate standing (Undergraduate engineering math: Calculus, probability theory, statistics, and some basic computer programming skills.).\nHW Logistics: You will submit your HW and projects to Canvas\n\n\nContent and goals\nIntroduces the foundations of machine learning encountered in AI with emphasis on practical aspects. Students learn to analyze complex datasets, identify problems benefiting from AI solutions, and implement solutions using appropriate libraries and computing platforms while evaluating them against AI risk frameworks.\n\n\nList of Topics\n\nProbability and Bayes\nModeling using known distributions\nFrequentist and Bayesian Inference\nRegression, Classification\nMixture of expert models\nTree-based methods\nDecision Making\nAB Testing\nTemporal data and forecasting\nDeep Learning\nOptimisation and Regularization\nModel Selection (Bias-Variance, Double Descent)\nAI Risk Frameworks\n\n\n\nSchedule\n\nAug 27: First Class\nSep 10: HW 1 Due\nSep 17: HW 2 Due\nSep 24: HW 3 Due\nOct 1: HW 4 Due\nOct 8: In-class Midterm\nNov 5: Final project proposal due\nNov 7: HW 5 Due\nNov 21: HW 6 Due\nDecember 3: Final Project Presentations (last class)\n\n\n\nCase Studies\n\nSelf-Driving Cars and the Uber Fatal Crash (2018) Focus: Safety, liability, trust in AI, limitations of perception models Key Questions:\n\nWhat caused the failure in perception?\nWho is responsible—the AI developer, the safety driver, or Uber?\nShould fully autonomous driving be paused until X?\n\nCOMPAS Algorithm in Criminal Justice Focus: Algorithmic bias, fairness, transparency Key Questions:\n\nWhy did COMPAS exhibit racial bias?\nCan AI be truly unbiased?\nShould such models be used in sentencing?\n\nAI in Healthcare: IBM Watson for Oncology Focus: Promise vs reality, hype cycle, trust in clinical AI Key Questions:\n\nWhy did Watson fail to meet expectations?\nWhat lessons can we learn for future medical AI systems?\n\nDeepfakes and Synthetic Media Focus: Misinformation, regulation, detection vs creation arms race Key Questions:\n\nWho should be held accountable for deepfake misuse?\nAre technical detection solutions sufficient?\n\nChatGPT and Large Language Models in Education Focus: Plagiarism, learning integrity, productivity vs dependence Key Questions:\n\nShould universities ban or embrace AI tools in coursework?\nHow do we ensure critical thinking is preserved?\n\nAI in Hiring: Amazon’s Recruitment Tool Bias Focus: Gender bias, explainability, data-driven discrimination Key Questions:\n\nWhy did the hiring algorithm become biased?\nHow should companies audit AI systems for fairness?\n\nAI in Warfare: Lethal Autonomous Weapons (LAWS) Focus: Ethics, accountability, international law Key Questions:\n\nShould AI be allowed to make life-and-death decisions?\nWhat governance models could prevent misuse?\n\nAI for Social Good: Predictive Models for Disease Outbreaks (e.g., BlueDot and COVID-19) Focus: Success stories, limitations, data privacy Key Questions:\n\nWhy was BlueDot successful in early COVID detection?\nHow can similar models be scaled responsibly?\n\n\nFormat:\n\nQuick Context (5 min)\nSmall Group Brainstorm (10 min)\nRound Table Reporting and debate (15 min)\nInstructor Wrap-Up + Reflection (15 min)\n\n\n\n\nAssignments\nStudents will have a in-class midterm exam and final project. There are 6 homework assignments; students are encouraged to work in small groups. Each homework has 2-3 theoretical questions and 2-3 hands-on problems. Theoretical questions will be based on the material covered in class. Hands-on problems will require using Python or R and routines provided by instructor to perform data analysis tasks. For the final project a student or a group of students can choose their own data set and a hypothesis to verify. Each group will submit the final report.\n\n\nComputing\nYou can choose which software you use. I recommend investing the time to learn Python and R.\nA great way to start learning is do a data camp or coursera course.\n\n\nGrading\nGrade based entirely on participation in class, homework assignments, in-class midterm and final project.\nMidterm 35% + Final project + 35% + Homework 30%. Scores of each component are normalized to be out of 100. Grades will be posted on (D. Cut-offs: 97 (A+), 93 (A), 90 (A-), 87 (B+), 82 (B), 79 (B-), 77 (C+), 73 (C), 70 (C-), 67 (D+), 60)\n\n\nFinal Project\n\nIn-class presentation (10 min)\nFinal report (up to 10 pages)\n\nApply statistical learning methods to a real world problem (prediction, hypothesis testing). You can use traditional tabular data to train and test the model. If you want to use LLM or CNN, you can use less structured data.\nA good project will have the following components:\n\nFormulate the hypothesis (what is the question you are trying to answer?)\nBuild a model (what is the model you are using?)\nEvaluate the model (how do you know that the model is doing the right thing?)\nModel selection and tweaking (use your evaluation criteria to select a model and tweak it if needed)\nDiscuss the results: what did you learn regarding the hypothesis? Are there any limitations? Are there any risk involved in using the model?\n\n\n\nMason Honor Code\nTo promote a stronger sense of mutual responsibility, respect, trust, and fairness among all members of the George Mason University community and with the desire for greater academic and personal achievement, we, the student members of the university community, have set forth this honor code: Student members of the George Mason University community pledge not to cheat, plagiarize, steal, or lie in matters related to academic work. Students are responsible for their own work, and students and faculty must take on the responsibility of dealing with violations. The tenet must be a foundation of our university culture.\nAll work performed in this course will be subject to Mason’s Honor Code. Students are expected to do their own work in the course. For the group project, students are expected to collaborate with their assigned group members. In papers and project reports, students are expected to write in their own words,\n\n\nIndividuals with Disabilities\nThe university is committed to providing equal access to employment and educational opportunities for people with disabilities.\nMason recognizes that individuals with disabilities may need reasonable accommodations to have equally effective opportunities to participate in or benefit from the university educational programs, services, and activities, and have equal employment opportunities. The university will adhere to all applicable federal and state laws, regulations, and guidelines with respect to providing reasonable accommodations as necessary to afford equal employment opportunity and equal access to programs for qualified people with disabilities.\nApplicants for admission and students requesting reasonable accommodations for a disability should call the Office of Disability Services at 703-993-2474. Employees and applicants for employment should call the Office of Equity and Diversity Services at 703-993-8730. Questions regarding reasonable accommodations and discrimination on the basis of disability should be directed to the Americans with Disabilities Act (ADA) coordinator in the Office of Equity and Diversity Services."
  },
  {
    "objectID": "index.html#book-on-bayes-ai",
    "href": "index.html#book-on-bayes-ai",
    "title": "Vadim Sokolov",
    "section": "Book on Bayes AI",
    "text": "Book on Bayes AI\nDiscover the intersection of Bayesian statistics, artificial intelligence, and deep learning in our new book, Bayes, AI and Deep Learning: Foundations of Data Science. Co-authored by Nick Polson and Vadim Sokolov, this book offers an accessible yet rigorous journey through the core ideas shaping modern data science. Drawing on years of teaching experience with both business and engineering audiences, we blend intuitive explanations, real-world case studies, and hands-on exercises to bridge theory and practice. Whether you’re a manager seeking to leverage AI for strategic advantage or an engineer building intelligent systems, you’ll find practical insights into topics ranging from probability and Bayesian inference to neural networks and large language models. Join us as we explore how these transformative tools are revolutionizing industries—from personalized medicine to urban systems—and learn how to harness their power for your own projects. Read more and explore the full table of contents."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html",
    "href": "courses/caio/topic-overview-caio.html",
    "title": "Machine Learning Essentials",
    "section": "",
    "text": "Online: 2 Weeks (14 Days)\nEmail: vsokolov@gmu.edu\nPhone: 703 993 4533\nCourse Textbook: Bayes, AI and Deep Learning by Nick Polson and Vadim Sokolov. The book is to be published by Chapman & Hall/CRC in 2026. Available for free online."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-purpose",
    "href": "courses/caio/topic-overview-caio.html#topic-purpose",
    "title": "Machine Learning Essentials",
    "section": "Topic Purpose",
    "text": "Topic Purpose\nThe purpose of this topic is to introduce participants to the foundational concepts of artificial intelligence and data-driven decision making. Participants will develop a working understanding of probability, statistical modeling, and modern AI techniques—equipping them to lead AI initiatives, evaluate AI investments, and communicate effectively with technical teams."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-overview",
    "href": "courses/caio/topic-overview-caio.html#topic-overview",
    "title": "Machine Learning Essentials",
    "section": "Topic Overview",
    "text": "Topic Overview\nThis module takes executives on a journey from the fundamentals of probability and uncertainty through statistical modeling to the cutting edge of modern AI. Rather than focusing on mathematical derivations, we emphasize intuition, real-world applications, and business implications. Through compelling case studies—from wrongful convictions caused by probability errors to the Netflix Prize’s lessons about model complexity—participants will learn to think probabilistically about business decisions. The module culminates in a hands-on project where participants build an AI agent using Cursor IDE, directly experiencing how data, models, and AI agents work together to solve business problems."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#topic-objectives",
    "href": "courses/caio/topic-overview-caio.html#topic-objectives",
    "title": "Machine Learning Essentials",
    "section": "Topic Objectives",
    "text": "Topic Objectives\nUpon completion of this topic, you should understand and be able to:\n\nApply probabilistic thinking to business decisions under uncertainty\nRecognize common probability fallacies (prosecutor’s fallacy, base rate neglect) and their business implications\nUnderstand the trade-offs between model accuracy, complexity, and business value\nInterpret regression models and explain their predictions to stakeholders\nEvaluate when AI/ML solutions are appropriate versus traditional statistical approaches\nBuild a simple AI agent that combines data analysis with natural language interaction\nLead informed conversations with data science and AI teams"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#course-approach",
    "href": "courses/caio/topic-overview-caio.html#course-approach",
    "title": "Machine Learning Essentials",
    "section": "Course Approach",
    "text": "Course Approach\nThis topic combines asynchronous learning (recorded lectures, readings, discussion boards) with synchronous sessions (live Zoom calls) and hands-on practice. The approach emphasizes:\n\nCase-based learning: Each concept is grounded in real-world examples—from legal cases to sports analytics to retail pricing\nBusiness-first perspective: Technical concepts are always connected to business decisions and outcomes\nProgressive building: Each module builds on the previous, culminating in an integrated final project\nPeer learning: Discussion boards encourage sharing experiences and learning from diverse industry perspectives\nApplied practice: The final project provides hands-on experience building an AI-powered analytics tool"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#time-commitment",
    "href": "courses/caio/topic-overview-caio.html#time-commitment",
    "title": "Machine Learning Essentials",
    "section": "Time Commitment",
    "text": "Time Commitment\nThis topic will require approximately 10 hours of work to complete:\n\n\n\nActivity\nHours\n\n\n\n\nRecorded Lectures (3 lectures × 30 min)\n1.5\n\n\nLive Zoom Sessions (3 sessions × 1 hr)\n3.0\n\n\nReading\n2\n\n\nDiscussion Boards\n1.0\n\n\nFinal Project\n2.0\n\n\nTotal\n10.0"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#schedule",
    "href": "courses/caio/topic-overview-caio.html#schedule",
    "title": "Machine Learning Essentials",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nDay\nActivities\n\n\n\n\nDay 1-2\nModule 1 lectures available; begin readings on probability and Bayes rule\n\n\nDay 3\nDiscussion Board 1 opens\n\n\nDay 4\nZoom Session 1: Kick-off + Cursor IDE Hands-on (1 hr)\n\n\nDay 5-6\nModule 2 lectures available; readings on statistics and regression\n\n\nDay 7\nDiscussion Board 2 opens\n\n\nDay 8\nZoom Session 2: Mid-point Check-in (1 hr)\n\n\nDay 9-10\nModule 3 lectures available; readings on NLP and AI agents\n\n\nDay 11\nDiscussion Board 3 opens\n\n\nDay 12-13\nFinal Project work time\n\n\nDay 14\nZoom Session 3: Wrap-up + Final Project Presentations (1 hr)"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#discussion-boards",
    "href": "courses/caio/topic-overview-caio.html#discussion-boards",
    "title": "Machine – Learning Essentials",
    "section": "Discussion Boards",
    "text": "Discussion Boards\n\nDiscussion Board 1: Decision-Making Under Uncertainty\nOpens Day 3 | Due Day 7\n“Consider a strategic decision your organization recently faced (or is currently facing) involving uncertainty. Describe the decision and identify:\n\nWhat were the key uncertain factors?\nHow was probability or likelihood assessed (formally or informally)?\nReflecting on the Ellsberg paradox and Kelly criterion, how might a more systematic probabilistic approach have changed the decision-making process?\n\nRespond to at least two peers’ posts with constructive suggestions.”\n\n\nDiscussion Board 2: Predictive Models in Business\nOpens Day 7 | Due Day 11\n“The Netflix Prize awarded $1 million for a 10% improvement in recommendation accuracy, yet Netflix never fully implemented the winning algorithm—it was too complex and expensive to deploy, and by then, streaming had changed the business model entirely.\nReflecting on this case and the regression concepts from this module:\n\nIdentify a business process in your organization where a predictive model could be applied. What decisions would it inform?\nWhat would happen if the model’s predictions were inaccurate 20% of the time? 40% of the time? How would this affect business outcomes and trust in the system?\nDiscuss the trade-off: Is a highly accurate but complex/expensive model always better than a simpler, ‘good enough’ model? What factors would you consider when making this decision?\n\nRespond to at least two peers’ posts, particularly focusing on whether you agree with their assessment of the accuracy-complexity trade-off.”\n\n\nDiscussion Board 3: AI Agents in the Enterprise\nOpens Day 11 | Due Day 14\n“AI agents are increasingly being deployed in business contexts. Describe a workflow or process in your organization that could potentially be automated or augmented by an AI agent. Address:\n\nWhat tasks would the agent perform?\nWhat data or tools would it need access to?\nWhat guardrails or human oversight would be necessary?\nWhat risks or concerns would need to be addressed before deployment?\n\nRespond to at least two peers’ posts.”"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#live-zoom-sessions",
    "href": "courses/caio/topic-overview-caio.html#live-zoom-sessions",
    "title": "Machine – Learning Essentials",
    "section": "Live Zoom Sessions",
    "text": "Live Zoom Sessions\n\nZoom Session 1: Kick-off + Cursor Hands-on\nDay 4 | 1 hour\n\nWelcome and module overview (15 min)\nHands-on: Setting up Cursor IDE and using coding agents (30 min)\nQ&A on probability concepts from Module 1 (15 min)\n\nPreparation: Install Cursor IDE before the session (instructions provided in cursor-setup-guide.md)\n\n\nZoom Session 2: Mid-point Check-in\nDay 8 | 1 hour\n\nReview of statistical modeling concepts (20 min)\nLive demo: Building a simple regression model with Cursor (25 min)\nDiscussion of final project requirements (15 min)\n\nPreparation: Complete Module 2 lectures and readings\n\n\nZoom Session 3: Wrap-up + Final Project Presentations\nDay 14 | 1 hour\n\nBrief Modern AI recap (10 min)\nFinal project presentations/demonstrations (35 min)\nCourse wrap-up and next steps for AI leadership (15 min)\n\nPreparation: Complete final project; prepare 2-3 minute demonstration"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#assignment-final-project",
    "href": "courses/caio/topic-overview-caio.html#assignment-final-project",
    "title": "Machine Learning Essentials",
    "section": "Assignment: Final Project",
    "text": "Assignment: Final Project\n\nOrange Juice Pricing Analytics Agent\nThis part (as ever other part of the module) is optional.\nBusiness Problem: You are a pricing analyst at a retail chain. Management wants to optimize orange juice pricing and promotional strategies. Build an AI agent that can answer business questions about pricing decisions using historical sales data and a predictive model.\nDataset: Dominick’s Orange Juice Dataset\n\nWeekly sales data for orange juice brands (Tropicana, Minute Maid, Dominick’s)\nVariables: sales volume, price, advertising features, brand\n~28,000 observations across multiple stores\n\nModel: Linear Regression with Interactions\n\nPredict sales volume based on price, advertising, and brand\nCapture how price sensitivity varies by brand\n\nYour Agent Must Answer These Business Questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nDeliverables:\n\nPython code files in Cursor IDE (using provided template)\nWorking agent that answers the 5 business questions above\n1-page summary: What did you learn about OJ pricing? What surprised you?\n2-3 minute demo during Zoom Session 3\n\nEvaluation Criteria:\n\nFunctionality: Agent loads data, builds model, and responds to queries\nBusiness Relevance: Clear connection between model outputs and business decisions\nDocumentation: Clear explanation of approach and results\n\nSee Final Project Guide for detailed step-by-step instructions."
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#reading-materials",
    "href": "courses/caio/topic-overview-caio.html#reading-materials",
    "title": "Machine – Learning Essentials",
    "section": "Reading Materials",
    "text": "Reading Materials\n\nModule 1: Probability as a Language of Uncertainty\nRequired Reading (from course textbook):\nChapter 1: Probability and Uncertainty\n\nOpening sections through “Kolmogorov Axioms”\nSection: “Conditional, Marginal and Joint Distributions”\nExample: Salary-Happiness\n\nChapter 2: Bayes Rule\n\nSection: “Intuition and Simple Examples”\nExample: Sally Clark Case\nExample: Nakamura’s Alleged Cheating\n\nChapter 4: Utility, Risk and Decisions\n\nSection: “Expected Utility”\nExamples: Saint Petersburg Paradox, Kelly Criterion, Ellsberg Paradox\nSection: “Decision Trees” (including Medical Testing and Mudslide examples)\n\nSupplemental Reading (online):\n\nDid a US Chess Champion Cheat? - Chicago Booth Review (Bayesian analysis, prosecutor’s fallacy)\nA Refresher on Statistical Significance - Harvard Business Review\nDecision Making in Uncertain Times - McKinsey\n\n\n\nModule 2: Statistics and Modeling\nRequired Reading (from course textbook):\nChapter 1: Probability and Uncertainty\n\nSections: “Normal Distribution,” “Poisson Distribution,” “Binomial Distribution”\nExamples: Heights of Adults, Customer Arrivals, NFL Patriots Coin Toss\n\nChapter 3: Bayesian Learning\n\nSection: “Poisson Model for Count Data”\n\nChapter 12: Linear Regression\n\nSection: “Linear Regression” (opening)\nExamples: Google vs S&P 500, Orange Juice\n\nChapter 13: Logistic Regression and GLMs\n\nSections: “Model Fitting,” “Confusion Matrix,” “ROC Curve”\nExample: NBA point spread\n\nSupplemental Reading (online):\n\nThe Surprising Power of Online Experiments - Harvard Business Review (A/B testing)\nMachine Learning Explained - MIT Sloan\nWhy Even a Million Dollars Couldn’t Buy a Better Algorithm - Wired (Netflix Prize case study)\n\n\n\nModule 3: Modern AI\nRequired Reading (from course textbook):\nChapter 24: Natural Language Processing\n\nSections: “Converting Words to Numbers (Embeddings),” “Word2Vec and Distributional Semantics”\nExample: Word2Vec for War and Peace\nSections: “Attention Mechanisms,” “Transformer Architecture” (overview)\n\nChapter 26: AI Agents\n\nFull chapter overview (agent architecture, tool use, planning, safety)\n\nSupplemental Reading (online):\n\nMaking the Most of AI and Machine Learning in Organizations - Stanford SMJ Paper\nTraditional Statistics vs Machine Learning - ToolsGroup\nThe State of AI in 2024 - McKinsey\nGenerative AI’s Act Two - Sequoia Capital"
  },
  {
    "objectID": "courses/caio/final-project-guide.html",
    "href": "courses/caio/final-project-guide.html",
    "title": "Final Project Guide",
    "section": "",
    "text": "In this project, you will build an AI agent that helps a retail pricing analyst make decisions about orange juice pricing and promotions. The agent will:\n\nLoad and explore sales data\nBuild a regression model to predict sales\nAnswer business questions using natural language\n\nTime Required: ~2 hours\nPrerequisites:\n\nCursor IDE installed (see Cursor Setup Guide)\nBasic familiarity with Cursor from Zoom Session 1\nDownload the project template to get started quickly"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#overview",
    "href": "courses/caio/final-project-guide.html#overview",
    "title": "Final Project Guide",
    "section": "",
    "text": "In this project, you will build an AI agent that helps a retail pricing analyst make decisions about orange juice pricing and promotions. The agent will:\n\nLoad and explore sales data\nBuild a regression model to predict sales\nAnswer business questions using natural language\n\nTime Required: ~2 hours\nPrerequisites:\n\nCursor IDE installed (see Cursor Setup Guide)\nBasic familiarity with Cursor from Zoom Session 1\nDownload the project template to get started quickly"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-1-project-setup",
    "href": "courses/caio/final-project-guide.html#part-1-project-setup",
    "title": "Final Project Guide",
    "section": "2 Part 1: Project Setup",
    "text": "2 Part 1: Project Setup\n\n2.1 Step 1.1: Create Your Project Folder\n\nOpen Cursor IDE\nClick File → Open Folder\nCreate a new folder called oj-pricing-agent on your computer\nSelect that folder to open it in Cursor\n\n\n\n2.2 Step 1.2: Create the Main Python File\n\nIn the Cursor sidebar, right-click and select New File\nName it oj_agent.py\nYou’ll see an empty file open in the editor\n\n\n\n2.3 Step 1.3: Copy the Dataset\nDownload the oj_data.csv file and copy it into your oj-pricing-agent folder."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-2-data-loading-and-exploration",
    "href": "courses/caio/final-project-guide.html#part-2-data-loading-and-exploration",
    "title": "Final Project Guide",
    "section": "3 Part 2: Data Loading and Exploration",
    "text": "3 Part 2: Data Loading and Exploration\n\n3.1 Step 2.1: Load the Required Libraries\nIn your oj_agent.py file, start by adding these lines at the top:\n# Required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\nWhat this does: These libraries help us work with data (pandas), do math (numpy), and build models (sklearn).\n\n\n3.2 Step 2.2: Load the Data\nAdd the following code to load the orange juice sales data:\n# Load the orange juice dataset\nprint(\"Loading data...\")\ndf = pd.read_csv('oj_data.csv')\n\n# Display basic information\nprint(f\"Dataset has {len(df)} rows and {len(df.columns)} columns\")\nprint(f\"\\nColumns: {list(df.columns)}\")\nprint(f\"\\nBrands in dataset: {df['brand'].unique()}\")\nprint(f\"\\nPrice range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\nprint(f\"\\nSample of data:\")\nprint(df.head())\n\n\n3.3 Step 2.3: Run Your Code (First Test)\n\nSave the file (Ctrl+S or Cmd+S)\nOpen the terminal in Cursor: View → Terminal\nRun the script:\n\npython oj_agent.py\nYou should see output showing:\n\nThe dataset has ~28,000 rows\nThree brands: Tropicana, Minute Maid, Dominick’s\nPrice ranges from about $1 to $4\n\nTroubleshooting: If you get an error about missing packages, run:\npip install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-3-building-the-regression-model",
    "href": "courses/caio/final-project-guide.html#part-3-building-the-regression-model",
    "title": "Final Project Guide",
    "section": "4 Part 3: Building the Regression Model",
    "text": "4 Part 3: Building the Regression Model\n\n4.1 Step 3.1: Understanding the Model\nWe’re building a model that predicts log of sales volume based on:\n\nPrice: Higher price → lower sales (negative relationship)\nFeatured (feat): If the product is in the weekly ad circular (1 = yes, 0 = no)\nBrand: Different brands have different base sales levels\nPrice × Brand interaction: Price sensitivity varies by brand\n\n\n\n4.2 Step 3.2: Prepare the Data for Modeling\nAdd this code to prepare features for the model:\n# ============================================\n# PART 3: BUILD THE REGRESSION MODEL\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Building the pricing model...\")\nprint(\"=\"*50)\n\n# Create dummy variables for brand (one-hot encoding)\n# This converts 'brand' text into numbers the model can use\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand', drop_first=False)\n\n# Create the feature matrix\n# We include: price, feat, brand dummies, and price*brand interactions\nX = pd.DataFrame({\n    'price': df['price'],\n    'feat': df['feat'],\n    'brand_minute.maid': brand_dummies['brand_minute.maid'],\n    'brand_tropicana': brand_dummies['brand_tropicana'],\n    # Interaction terms: price effect varies by brand\n    'price_x_minute.maid': df['price'] * brand_dummies['brand_minute.maid'],\n    'price_x_tropicana': df['price'] * brand_dummies['brand_tropicana']\n})\n\n# Target variable: log of sales (logmove)\ny = df['logmove']\n\nprint(f\"Features: {list(X.columns)}\")\nprint(f\"Target: logmove (log of sales volume)\")\n\n\n4.3 Step 3.3: Fit the Model\nAdd code to train the regression model:\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Display the coefficients\nprint(\"\\nModel Coefficients:\")\nprint(\"-\" * 40)\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"  {feature}: {coef:.4f}\")\nprint(f\"  intercept: {model.intercept_:.4f}\")\n\n# Calculate R-squared (how well the model fits)\nr_squared = model.score(X, y)\nprint(f\"\\nModel R-squared: {r_squared:.3f}\")\nprint(\"(This means the model explains {:.1f}% of sales variation)\".format(r_squared * 100))\n\n\n4.4 Step 3.4: Run and Verify the Model\nSave and run the script again. You should see coefficients like:\n\nprice: negative (higher price = lower sales)\nfeat: positive (being featured increases sales)\nbrand coefficients: capture baseline differences between brands\ninteraction terms: show how price sensitivity differs by brand"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-4-creating-helper-functions-for-the-agent",
    "href": "courses/caio/final-project-guide.html#part-4-creating-helper-functions-for-the-agent",
    "title": "Final Project Guide",
    "section": "5 Part 4: Creating Helper Functions for the Agent",
    "text": "5 Part 4: Creating Helper Functions for the Agent\n\n5.1 Step 4.1: Add Prediction Functions\nAdd these functions that the agent will use to answer questions:\n# ============================================\n# PART 4: HELPER FUNCTIONS FOR THE AGENT\n# ============================================\n\ndef predict_sales(brand, price, featured=0):\n    \"\"\"\n    Predict sales volume for a given brand, price, and feature status.\n    \n    Args:\n        brand: 'tropicana', 'minute.maid', or 'dominicks'\n        price: price in dollars (e.g., 2.50)\n        featured: 1 if in ad circular, 0 if not\n    \n    Returns:\n        Predicted sales volume (not log-transformed)\n    \"\"\"\n    # Create feature vector\n    features = {\n        'price': price,\n        'feat': featured,\n        'brand_minute.maid': 1 if brand.lower() == 'minute.maid' else 0,\n        'brand_tropicana': 1 if brand.lower() == 'tropicana' else 0,\n        'price_x_minute.maid': price if brand.lower() == 'minute.maid' else 0,\n        'price_x_tropicana': price if brand.lower() == 'tropicana' else 0\n    }\n    \n    # Convert to dataframe for prediction\n    X_pred = pd.DataFrame([features])\n    \n    # Predict log sales, then convert back\n    log_sales = model.predict(X_pred)[0]\n    sales = np.exp(log_sales)\n    \n    return sales\n\n\ndef get_price_elasticity(brand):\n    \"\"\"\n    Calculate the price elasticity for a given brand.\n    \n    Price elasticity tells us: if price increases by 1%, \n    how much does quantity demanded change (in %)?\n    \n    A more negative number means more price-sensitive.\n    \"\"\"\n    # Base price coefficient\n    base_coef = model.coef_[0]  # price coefficient\n    \n    # Add brand-specific interaction if applicable\n    if brand.lower() == 'minute.maid':\n        interaction_coef = model.coef_[4]  # price_x_minute.maid\n    elif brand.lower() == 'tropicana':\n        interaction_coef = model.coef_[5]  # price_x_tropicana\n    else:  # dominicks (base case)\n        interaction_coef = 0\n    \n    total_elasticity = base_coef + interaction_coef\n    return total_elasticity\n\n\ndef get_advertising_lift(brand):\n    \"\"\"\n    Calculate the sales lift from being featured in advertising.\n    Returns the percentage increase in sales.\n    \"\"\"\n    # The 'feat' coefficient tells us the log-sales increase\n    feat_coef = model.coef_[1]  # feat coefficient\n    \n    # Convert from log to percentage change\n    percentage_lift = (np.exp(feat_coef) - 1) * 100\n    return percentage_lift\n\n\ndef find_optimal_price(brand, min_price=1.0, max_price=4.0, featured=0):\n    \"\"\"\n    Find the price that maximizes revenue for a brand.\n    Revenue = Price × Quantity\n    \"\"\"\n    best_price = min_price\n    best_revenue = 0\n    \n    # Search through price range\n    for price in np.arange(min_price, max_price, 0.05):\n        sales = predict_sales(brand, price, featured)\n        revenue = price * sales\n        \n        if revenue &gt; best_revenue:\n            best_revenue = revenue\n            best_price = price\n    \n    return best_price, best_revenue\n\n\ndef compare_elasticities():\n    \"\"\"\n    Compare price elasticity across all three brands.\n    \"\"\"\n    brands = ['dominicks', 'minute.maid', 'tropicana']\n    results = {}\n    \n    for brand in brands:\n        elasticity = get_price_elasticity(brand)\n        results[brand] = elasticity\n    \n    return results\n\n\n5.2 Step 4.2: Test the Helper Functions\nAdd test code to verify the functions work:\n# ============================================\n# TEST THE HELPER FUNCTIONS\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing helper functions...\")\nprint(\"=\"*50)\n\n# Test prediction\ntest_sales = predict_sales('tropicana', 2.50, featured=0)\nprint(f\"\\nPredicted sales for Tropicana at $2.50 (no ad): {test_sales:.0f} units\")\n\n# Test elasticity\nelasticities = compare_elasticities()\nprint(\"\\nPrice Elasticities by Brand:\")\nfor brand, elast in elasticities.items():\n    print(f\"  {brand}: {elast:.3f}\")\n\n# Test advertising lift\nlift = get_advertising_lift('minute.maid')\nprint(f\"\\nAdvertising lift: {lift:.1f}% increase in sales\")\n\n# Test optimal price\nopt_price, opt_rev = find_optimal_price('dominicks')\nprint(f\"\\nOptimal price for Dominick's: ${opt_price:.2f} (revenue: ${opt_rev:.2f})\")\nRun the script again to verify all functions work correctly."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-5-creating-the-ai-agent",
    "href": "courses/caio/final-project-guide.html#part-5-creating-the-ai-agent",
    "title": "Final Project Guide",
    "section": "6 Part 5: Creating the AI Agent",
    "text": "6 Part 5: Creating the AI Agent\n\n6.1 Step 5.1: Add the Agent Logic\nNow we’ll create the agent that interprets natural language questions and calls the appropriate functions. Add this code:\n# ============================================\n# PART 5: THE AI AGENT\n# ============================================\n\ndef answer_question(question):\n    \"\"\"\n    Simple agent that answers business questions about OJ pricing.\n    \n    This is a rule-based agent that matches keywords in the question\n    to determine which analysis to perform.\n    \"\"\"\n    question_lower = question.lower()\n    \n    # Question 1: Predict sales for specific scenario\n    if 'predict' in question_lower or 'sales volume' in question_lower:\n        # Extract brand and price from question if possible\n        if 'tropicana' in question_lower:\n            brand = 'tropicana'\n        elif 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        else:\n            brand = 'dominicks'\n        \n        # Look for price (default to $2.50 if not found)\n        import re\n        price_match = re.search(r'\\$?(\\d+\\.?\\d*)', question_lower)\n        price = float(price_match.group(1)) if price_match else 2.50\n        \n        # Check for advertising\n        featured = 1 if 'advertis' in question_lower or 'feature' in question_lower else 0\n        if 'no advertis' in question_lower or 'without advertis' in question_lower:\n            featured = 0\n        \n        sales = predict_sales(brand, price, featured)\n        \n        response = f\"\"\"\n**Predicted Sales Analysis**\n\nBrand: {brand.title().replace('.', ' ')}\nPrice: ${price:.2f}\nFeatured in Ad: {'Yes' if featured else 'No'}\n\n**Predicted Sales Volume: {sales:,.0f} units**\n\nThis prediction is based on our regression model that accounts for:\n- Base demand for this brand\n- Price sensitivity (elasticity)  \n- Advertising effects\n\"\"\"\n        return response\n    \n    # Question 2: Which brand is most price-sensitive?\n    elif 'price-sensitive' in question_lower or 'price sensitive' in question_lower or 'most sensitive' in question_lower:\n        elasticities = compare_elasticities()\n        \n        # Find most price-sensitive (most negative elasticity)\n        most_sensitive = min(elasticities, key=elasticities.get)\n        \n        response = f\"\"\"\n**Price Sensitivity Analysis**\n\nPrice Elasticity by Brand:\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            sensitivity = \"HIGH\" if elast &lt; -3 else \"MEDIUM\" if elast &lt; -2 else \"LOW\"\n            response += f\"- {brand.title().replace('.', ' ')}: {elast:.3f} ({sensitivity} sensitivity)\\n\"\n        \n        response += f\"\"\"\n**Most Price-Sensitive: {most_sensitive.title().replace('.', ' ')}**\n\nInterpretation: A 1% price increase leads to a {abs(elasticities[most_sensitive]):.1f}% decrease in sales for {most_sensitive.title().replace('.', ' ')}.\n\nBusiness Implication: Be careful with price increases on {most_sensitive.title().replace('.', ' ')} - customers are very responsive to price changes.\n\"\"\"\n        return response\n    \n    # Question 3: Should we feature a brand in advertising?\n    elif 'feature' in question_lower or 'ad circular' in question_lower or 'advertising' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        lift = get_advertising_lift(brand)\n        \n        # Calculate example impact\n        base_sales = predict_sales(brand, 2.50, featured=0)\n        featured_sales = predict_sales(brand, 2.50, featured=1)\n        \n        response = f\"\"\"\n**Advertising Impact Analysis for {brand.title().replace('.', ' ')}**\n\nExpected Sales Lift from Featuring: **{lift:.1f}%**\n\nExample at $2.50:\n- Without advertising: {base_sales:,.0f} units\n- With advertising: {featured_sales:,.0f} units  \n- Additional sales: {featured_sales - base_sales:,.0f} units\n\n**Recommendation:** {'Yes, feature this product!' if lift &gt; 20 else 'Consider the advertising cost vs. the sales lift.'}\n\nThe advertising effect is consistent across price points. Factor in your advertising costs to determine if the sales lift justifies the expense.\n\"\"\"\n        return response\n    \n    # Question 4: Optimal price for a brand\n    elif 'optimal price' in question_lower or 'maximize revenue' in question_lower or 'best price' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        opt_price, opt_revenue = find_optimal_price(brand)\n        opt_sales = predict_sales(brand, opt_price, featured=0)\n        \n        # Compare with current average price\n        avg_price = df[df['brand'] == brand]['price'].mean()\n        avg_revenue = avg_price * predict_sales(brand, avg_price, featured=0)\n        \n        response = f\"\"\"\n**Revenue Optimization for {brand.title().replace('.', ' ')}**\n\n**Optimal Price: ${opt_price:.2f}**\n\nAt optimal price:\n- Predicted sales: {opt_sales:,.0f} units\n- Revenue per store-week: ${opt_revenue:,.2f}\n\nComparison with current average (${avg_price:.2f}):\n- Current revenue: ${avg_revenue:,.2f}\n- Potential improvement: ${opt_revenue - avg_revenue:,.2f} ({((opt_revenue/avg_revenue)-1)*100:.1f}%)\n\nNote: This optimization assumes no competitor response and stable market conditions.\n\"\"\"\n        return response\n    \n    # Question 5: Compare elasticities across brands\n    elif 'compare' in question_lower or 'elasticity' in question_lower or 'across' in question_lower:\n        elasticities = compare_elasticities()\n        \n        response = \"\"\"\n**Price Elasticity Comparison Across Brands**\n\n| Brand | Elasticity | Interpretation |\n|-------|------------|----------------|\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            interp = f\"1% price ↑ → {abs(elast):.1f}% sales ↓\"\n            response += f\"| {brand.title().replace('.', ' ')} | {elast:.3f} | {interp} |\\n\"\n        \n        response += \"\"\"\n**Key Insights:**\n\n1. **Dominick's** (store brand) is least price-sensitive - customers buying store brands may prioritize value and be less responsive to small price changes.\n\n2. **Tropicana** shows moderate price sensitivity - as a premium brand, some customers are loyal but others will switch if prices rise.\n\n3. **Minute Maid** is most price-sensitive - positioned between store and premium brands, these customers actively compare prices.\n\n**Strategic Implications:**\n- Use competitive pricing on Minute Maid to capture price-sensitive shoppers\n- Tropicana can sustain moderate price premiums\n- Dominick's margins can be optimized with less risk of volume loss\n\"\"\"\n        return response\n    \n    else:\n        return \"\"\"\nI can help you with these types of questions:\n\n1. **Sales Prediction:** \"What is the predicted sales volume if we price Tropicana at $2.50?\"\n2. **Price Sensitivity:** \"Which brand is most price-sensitive?\"\n3. **Advertising Impact:** \"Should we feature Minute Maid in the ad circular?\"\n4. **Price Optimization:** \"What price should we set for Dominick's to maximize revenue?\"\n5. **Elasticity Comparison:** \"Compare the price elasticity across brands\"\n\nPlease try one of these questions!\n\"\"\"\n\n\n6.2 Step 5.2: Add the Interactive Interface\nFinally, add code to let users interact with the agent:\n# ============================================\n# PART 6: INTERACTIVE AGENT INTERFACE\n# ============================================\n\ndef run_agent():\n    \"\"\"\n    Run the interactive agent interface.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"🍊 ORANGE JUICE PRICING ANALYTICS AGENT 🍊\")\n    print(\"=\"*60)\n    print(\"\\nHello! I'm your pricing analytics assistant.\")\n    print(\"I can help you analyze orange juice pricing and promotions.\")\n    print(\"\\nTry asking me questions like:\")\n    print(\"  - What is the predicted sales if we price Tropicana at $2.50?\")\n    print(\"  - Which brand is most price-sensitive?\")\n    print(\"  - Should we feature Minute Maid in the ad circular?\")\n    print(\"  - What price maximizes revenue for Dominick's?\")\n    print(\"  - Compare price elasticity across brands\")\n    print(\"\\nType 'quit' to exit.\\n\")\n    \n    while True:\n        question = input(\"Your question: \").strip()\n        \n        if question.lower() in ['quit', 'exit', 'q']:\n            print(\"\\nThank you for using the OJ Pricing Agent. Goodbye!\")\n            break\n        \n        if not question:\n            continue\n        \n        print(\"\\n\" + \"-\"*50)\n        response = answer_question(question)\n        print(response)\n        print(\"-\"*50 + \"\\n\")\n\n\n# ============================================\n# MAIN: RUN THE AGENT\n# ============================================\n\nif __name__ == \"__main__\":\n    # Run the interactive agent\n    run_agent()"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-6-testing-your-agent",
    "href": "courses/caio/final-project-guide.html#part-6-testing-your-agent",
    "title": "Final Project Guide",
    "section": "7 Part 6: Testing Your Agent",
    "text": "7 Part 6: Testing Your Agent\n\n7.1 Step 6.1: Run the Complete Script\nSave the file and run:\npython oj_agent.py\n\n\n7.2 Step 6.2: Test All Five Required Questions\nTest your agent with these exact questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nRecord the answers for your summary document."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-7-writing-your-summary",
    "href": "courses/caio/final-project-guide.html#part-7-writing-your-summary",
    "title": "Final Project Guide",
    "section": "8 Part 7: Writing Your Summary",
    "text": "8 Part 7: Writing Your Summary\nCreate a 1-page document (Word or PDF) that includes:\n\n8.1 Section 1: Key Findings (half page)\n\nWhich brand is most/least price-sensitive and why this matters\nThe impact of advertising on sales\nThe optimal pricing recommendations\n\n\n\n8.2 Section 2: Surprises and Insights (quarter page)\n\nWhat surprised you about the results?\nHow do these findings compare to your intuition?\n\n\n\n8.3 Section 3: Business Implications (quarter page)\n\nHow would you recommend a retailer use these insights?\nWhat additional data would make this analysis more useful?"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#part-8-preparing-your-demo",
    "href": "courses/caio/final-project-guide.html#part-8-preparing-your-demo",
    "title": "Final Project Guide",
    "section": "9 Part 8: Preparing Your Demo",
    "text": "9 Part 8: Preparing Your Demo\nFor Zoom Session 3, prepare a 2-3 minute demonstration:\n\nSetup (30 sec): Briefly explain what the agent does\nDemo (1.5 min): Show 2-3 questions and responses\nInsight (1 min): Share your most interesting finding\n\nTips:\n\nHave your script running before you share screen\nPre-type a question so you’re not typing live\nFocus on business insights, not technical details"
  },
  {
    "objectID": "courses/caio/final-project-guide.html#troubleshooting",
    "href": "courses/caio/final-project-guide.html#troubleshooting",
    "title": "Final Project Guide",
    "section": "10 Troubleshooting",
    "text": "10 Troubleshooting\n\n10.1 “ModuleNotFoundError: No module named ‘pandas’”\nRun: pip install pandas numpy scikit-learn\n\n\n10.2 “FileNotFoundError: oj_data.csv”\nMake sure the data file is in the same folder as your Python script.\n\n\n10.3 “Model coefficients look wrong”\nCheck that your data loaded correctly - you should have ~28,000 rows.\n\n\n10.4 “Agent doesn’t understand my question”\nTry rephrasing using keywords like “predict”, “price-sensitive”, “feature”, “optimal”, or “compare”."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#complete-code-reference",
    "href": "courses/caio/final-project-guide.html#complete-code-reference",
    "title": "Final Project Guide",
    "section": "11 Complete Code Reference",
    "text": "11 Complete Code Reference\nThe complete oj_agent.py file should be approximately 350-400 lines. If you get stuck, ask Cursor’s AI assistant for help by selecting your code and pressing Cmd+K (Mac) or Ctrl+K (Windows), then describing your issue. I also prepared a complete code reference for you to refer."
  },
  {
    "objectID": "courses/caio/final-project-guide.html#next-steps-optional-enhancements",
    "href": "courses/caio/final-project-guide.html#next-steps-optional-enhancements",
    "title": "Final Project Guide",
    "section": "12 Next Steps (Optional Enhancements)",
    "text": "12 Next Steps (Optional Enhancements)\nIf you finish early and want to explore further:\n\nAdd more questions: What other business questions could the agent answer?\nImprove the NLP: Use fuzzy matching to better understand varied phrasings\nAdd visualizations: Create charts showing price vs. sales by brand\nConnect to an LLM: Use the OpenAI API to make the agent truly conversational\n\nGood luck with your project! 🍊"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-1-probability-as-a-language-of-uncertainty",
    "href": "courses/caio/topic-overview-caio.html#module-1-probability-as-a-language-of-uncertainty",
    "title": "Machine Learning Essentials",
    "section": "Module 1: Probability as a Language of Uncertainty",
    "text": "Module 1: Probability as a Language of Uncertainty\nDays 1-4\n\nRecorded Lectures\n\nPart 1: Probability\nPart 2: Bayes Rule\n\n\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nOpening sections through “Kolmogorov Axioms”\nSection: “Conditional, Marginal and Joint Distributions”\nExample: Salary-Happiness\n\nChapter 2: Bayes Rule\n\nSection: “Intuition and Simple Examples”\nExample: Sally Clark Case\nExample: Nakamura’s Alleged Cheating\n\nChapter 4: Utility, Risk and Decisions\n\nSection: “Expected Utility”\nExamples: Saint Petersburg Paradox, Kelly Criterion, Ellsberg Paradox, Secretary Problem\nSection: “Decision Trees” (including Medical Testing and Mudslide examples)\n\n\n\nSupplemental Reading (online)\n\nDid a US Chess Champion Cheat? - Chicago Booth Review (Bayesian analysis, prosecutor’s fallacy)\nA Refresher on Statistical Significance - Harvard Business Review\nDecision Making in Uncertain Times - McKinsey\n\n\n\nDiscussion Board: Decision-Making Under Uncertainty\nOpens Day 3 | Due Day 7\n“Consider a strategic decision your organization recently faced (or is currently facing) involving uncertainty. Describe the decision and identify:\n\nWhat were the key uncertain factors?\nHow was probability or likelihood assessed (formally or informally)?\nReflecting on the Ellsberg paradox and Kelly criterion, how might a more systematic probabilistic approach have changed the decision-making process?\n\nRespond to at least two peers’ posts with constructive suggestions.”\n\n\nZoom Session 1: Kick-off + Cursor Hands-on\nDay 4 | 1 hour\n\nWelcome and module overview (15 min)\nHands-on: Setting up Cursor IDE and using coding agents (30 min)\nQ&A on probability concepts from Module 1 (15 min)\n\nPreparation: Install Cursor IDE before the session (setup instructions)"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-2-statistics-and-modeling",
    "href": "courses/caio/topic-overview-caio.html#module-2-statistics-and-modeling",
    "title": "Machine Learning Essentials",
    "section": "Module 2: Statistics and Modeling",
    "text": "Module 2: Statistics and Modeling\nDays 5-8\n\nRecorded Lectures\n\nStatistics\n\n\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nSections: “Normal Distribution,” “Poisson Distribution,” “Binomial Distribution”\nExamples: Heights of Adults, Customer Arrivals, NFL Patriots Coin Toss\n\nChapter 3: Bayesian Learning\n\nSection: “Poisson Model for Count Data”\n\nChapter 12: Linear Regression\n\nSection: “Linear Regression” (opening)\nExamples: Google vs S&P 500, Orange Juice\n\nChapter 13: Logistic Regression and GLMs\n\nSections: “Model Fitting,” “Confusion Matrix,” “ROC Curve”\nExample: NBA point spread\n\n\n\nSupplemental Reading (online)\n\nThe Surprising Power of Online Experiments - Harvard Business Review (A/B testing)\nMachine Learning Explained - MIT Sloan\n\n\n\nDiscussion Board: Predictive Models in Business\nOpens Day 7 | Due Day 11 “The Netflix Prize awarded $1 million for a 10% improvement in recommendation accuracy, yet Netflix never fully implemented the winning algorithm—it was too complex and expensive to deploy, and by then, streaming had changed the business model entirely. See Why Even a Million Dollars Couldn’t Buy a Better Algorithm - Wired (Netflix Prize case study)\nReflecting on this case and the regression concepts from this module:\n\nIdentify a business process in your organization where a predictive model could be applied. What decisions would it inform?\nWhat would happen if the model’s predictions were inaccurate 20% of the time? 40% of the time? How would this affect business outcomes and trust in the system?\nDiscuss the trade-off: Is a highly accurate but complex/expensive model always better than a simpler, ‘good enough’ model? What factors would you consider when making this decision?\n\nRespond to at least two peers’ posts, particularly focusing on whether you agree with their assessment of the accuracy-complexity trade-off.”\n\n\nZoom Session 2: Mid-point Check-in\nDay 8 | 1 hour\n\nReview of statistical modeling concepts (20 min)\nLive demo: Building a simple regression model with Cursor (25 min)\nDiscussion of final project requirements (15 min)\n\nPreparation: Complete Module 2 lectures and readings"
  },
  {
    "objectID": "courses/caio/topic-overview-caio.html#module-3-modern-ai",
    "href": "courses/caio/topic-overview-caio.html#module-3-modern-ai",
    "title": "Machine Learning Essentials",
    "section": "Module 3: Modern AI",
    "text": "Module 3: Modern AI\nDays 9-14\n\nRequired Reading (from course textbook)\nChapter 24: Natural Language Processing\n\nSections: “Converting Words to Numbers (Embeddings),” “Word2Vec and Distributional Semantics”\nExample: Word2Vec for War and Peace\nSections: “Attention Mechanisms,” “Transformer Architecture” (overview)\n\nChapter 26: AI Agents\n\nFull chapter overview (agent architecture, tool use, planning, safety)\n\n\n\nSupplemental Reading (online)\n\nMaking the Most of AI and Machine Learning in Organizations - Stanford SMJ Paper\nTraditional Statistics vs Machine Learning - ToolsGroup\nThe State of AI in 2024 - McKinsey\nGenerative AI’s Act Two - Sequoia Capital\n\n\n\nDiscussion Board: AI Agents in the Enterprise\nOpens Day 11 | Due Day 14\n“AI agents are increasingly being deployed in business contexts. Describe a workflow or process in your organization that could potentially be automated or augmented by an AI agent. Address:\n\nWhat tasks would the agent perform?\nWhat data or tools would it need access to?\nWhat guardrails or human oversight would be necessary?\nWhat risks or concerns would need to be addressed before deployment?\n\nRespond to at least two peers’ posts.”\n\n\nZoom Session 3: Wrap-up + Final Project Presentations\nDay 14 | 1 hour\n\nBrief Modern AI recap (10 min)\nFinal project presentations/demonstrations (35 min)\nCourse wrap-up and next steps for AI leadership (15 min)\n\nPreparation: Complete final project; prepare 2-3 minute demonstration"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html",
    "href": "courses/caio/cursor-setup-guide.html",
    "title": "Cursor IDE Setup Guide",
    "section": "",
    "text": "Cursor is an AI-powered code editor built on VS Code. It allows you to:\n\nWrite code with AI assistance\nAsk questions about your code\nGenerate code from natural language descriptions\nDebug and fix errors with AI help\n\nFor this course, we’ll use Cursor to build an AI agent without needing deep programming expertise."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#what-is-cursor",
    "href": "courses/caio/cursor-setup-guide.html#what-is-cursor",
    "title": "Cursor IDE Setup Guide",
    "section": "",
    "text": "Cursor is an AI-powered code editor built on VS Code. It allows you to:\n\nWrite code with AI assistance\nAsk questions about your code\nGenerate code from natural language descriptions\nDebug and fix errors with AI help\n\nFor this course, we’ll use Cursor to build an AI agent without needing deep programming expertise."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-1-download-and-install-cursor",
    "href": "courses/caio/cursor-setup-guide.html#part-1-download-and-install-cursor",
    "title": "Cursor IDE Setup Guide",
    "section": "2 Part 1: Download and Install Cursor",
    "text": "2 Part 1: Download and Install Cursor\n\n2.1 Step 1: Download Cursor\n\nGo to cursor.sh\nClick the Download button\nThe website will automatically detect your operating system (Mac, Windows, or Linux)\n\n\n\n2.2 Step 2: Install on Mac\n\nOpen the downloaded .dmg file\nDrag the Cursor icon to the Applications folder\nOpen Cursor from Applications\nIf prompted about security, go to System Preferences → Security & Privacy and click “Open Anyway”\n\n\n\n2.3 Step 3: Install on Windows\n\nRun the downloaded .exe installer\nFollow the installation wizard\nLaunch Cursor from the Start Menu"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-2-initial-setup",
    "href": "courses/caio/cursor-setup-guide.html#part-2-initial-setup",
    "title": "Cursor IDE Setup Guide",
    "section": "3 Part 2: Initial Setup",
    "text": "3 Part 2: Initial Setup\n\n3.1 Step 1: Sign In (Optional but Recommended)\n\nWhen Cursor opens, you’ll see a welcome screen\nClick Sign In to create a free account\nYou can sign in with:\n\nGoogle account\nGitHub account\nEmail\n\n\nBenefits of signing in:\n\nFree AI credits for code assistance\nSettings sync across devices\n\n\n\n3.2 Step 2: Choose Your Theme\n\nCursor will ask about your preferred color theme\nChoose Dark or Light based on your preference\nYou can change this later in Settings\n\n\n\n3.3 Step 3: Import VS Code Settings (Optional)\nIf you’ve used VS Code before:\n\nCursor will offer to import your settings\nClick Import to bring over extensions and preferences\nOr click Skip to start fresh"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-3-install-python",
    "href": "courses/caio/cursor-setup-guide.html#part-3-install-python",
    "title": "Cursor IDE Setup Guide",
    "section": "4 Part 3: Install Python",
    "text": "4 Part 3: Install Python\nCursor needs Python installed on your computer to run our project.\n\n4.1 Check if Python is Already Installed\n\nIn Cursor, open the terminal: View → Terminal (or press Ctrl+`)\nType this command and press Enter:\n\npython --version\n\nIf you see Python 3.x.x, you’re good! Skip to Part 4.\nIf you get an error, follow the installation steps below.\n\n\n\n4.2 Install Python on Mac\nOption A: Using Homebrew (Recommended)\n\nOpen Terminal (outside of Cursor)\nInstall Homebrew if you don’t have it:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Python:\n\nbrew install python\nOption B: Direct Download\n\nGo to python.org/downloads\nDownload the latest Python 3.x version\nRun the installer\nImportant: Check “Add Python to PATH” during installation\n\n\n\n4.3 Install Python on Windows\n\nGo to python.org/downloads\nClick “Download Python 3.x.x”\nRun the installer\nIMPORTANT: Check the box that says “Add Python to PATH”\nClick “Install Now”\n\n\n\n4.4 Verify Installation\nAfter installation, close and reopen Cursor, then:\n\nOpen terminal: View → Terminal\nRun:\n\npython --version\n\nYou should see Python 3.x.x"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-4-install-required-packages",
    "href": "courses/caio/cursor-setup-guide.html#part-4-install-required-packages",
    "title": "Cursor IDE Setup Guide",
    "section": "5 Part 4: Install Required Packages",
    "text": "5 Part 4: Install Required Packages\nOur project needs a few Python libraries. Install them in Cursor’s terminal:\npip install pandas numpy scikit-learn\nYou should see output indicating successful installation.\nIf you get a “pip not found” error on Mac:\npip3 install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-5-using-cursors-ai-features",
    "href": "courses/caio/cursor-setup-guide.html#part-5-using-cursors-ai-features",
    "title": "Cursor IDE Setup Guide",
    "section": "6 Part 5: Using Cursor’s AI Features",
    "text": "6 Part 5: Using Cursor’s AI Features\n\n6.1 Feature 1: AI Chat (Cmd+L / Ctrl+L)\nUse this to ask questions or get help:\n\nPress Cmd+L (Mac) or Ctrl+L (Windows)\nA chat panel opens on the right\nAsk questions like:\n\n“How do I load a CSV file in Python?”\n“Explain what this code does”\n“Why am I getting this error?”\n\n\n\n\n6.2 Feature 2: Inline Edit (Cmd+K / Ctrl+K)\nUse this to write or modify code:\n\nSelect some code (or place cursor where you want new code)\nPress Cmd+K (Mac) or Ctrl+K (Windows)\nDescribe what you want in plain English:\n\n“Add a function that calculates the average price”\n“Fix this error”\n“Add comments explaining this code”\n\nReview the suggested changes\nPress Enter to accept or Escape to cancel\n\n\n\n6.3 Feature 3: Code Completion (Tab)\nAs you type, Cursor suggests completions:\n\nStart typing code\nYou’ll see gray “ghost text” suggestions\nPress Tab to accept the suggestion\nPress Escape to dismiss\n\n\n\n6.4 Feature 4: Agent Mode (Cmd+I / Ctrl+I)\nFor larger tasks, use Agent mode:\n\nPress Cmd+I (Mac) or Ctrl+I (Windows)\nDescribe a complex task:\n\n“Create a Python script that loads data and builds a regression model”\n\nThe agent will generate multiple files and complete code"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-6-creating-your-first-project",
    "href": "courses/caio/cursor-setup-guide.html#part-6-creating-your-first-project",
    "title": "Cursor IDE Setup Guide",
    "section": "7 Part 6: Creating Your First Project",
    "text": "7 Part 6: Creating Your First Project\n\n7.1 Step 1: Create a Project Folder\n\nIn Cursor, go to File → Open Folder\nNavigate to where you want your project (e.g., Documents)\nClick New Folder and name it oj-pricing-agent\nSelect this folder and click Open\n\n\n\n7.2 Step 2: Create a Python File\n\nIn the Explorer sidebar (left panel), right-click\nSelect New File\nName it test.py\nAdd this code:\n\nprint(\"Hello from Cursor!\")\n\n\n7.3 Step 3: Run Your Code\n\nOpen the terminal: View → Terminal\nRun your script:\n\npython test.py\n\nYou should see: Hello from Cursor!\n\nCongratulations! You’re ready to build your AI agent."
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#part-7-keyboard-shortcuts-reference",
    "href": "courses/caio/cursor-setup-guide.html#part-7-keyboard-shortcuts-reference",
    "title": "Cursor IDE Setup Guide",
    "section": "8 Part 7: Keyboard Shortcuts Reference",
    "text": "8 Part 7: Keyboard Shortcuts Reference\n\n\n\nAction\nMac\nWindows\n\n\n\n\nAI Chat\nCmd+L\nCtrl+L\n\n\nInline Edit\nCmd+K\nCtrl+K\n\n\nAgent Mode\nCmd+I\nCtrl+I\n\n\nOpen Terminal\nCtrl+| Ctrl+\n\n\n\nSave File\nCmd+S\nCtrl+S\n\n\nOpen File\nCmd+O\nCtrl+O\n\n\nNew File\nCmd+N\nCtrl+N\n\n\nFind\nCmd+F\nCtrl+F"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#troubleshooting",
    "href": "courses/caio/cursor-setup-guide.html#troubleshooting",
    "title": "Cursor IDE Setup Guide",
    "section": "9 Troubleshooting",
    "text": "9 Troubleshooting\n\n9.1 “Python not found” in terminal\nMac:\n\nTry python3 instead of python\nOr run: brew install python\n\nWindows:\n\nReinstall Python and make sure to check “Add Python to PATH”\nRestart Cursor after installation\n\n\n\n9.2 “pip not found”\nMac:\n\nUse pip3 instead of pip\n\nWindows:\n\nTry python -m pip install package_name\n\n\n\n9.3 Cursor won’t start\n\nMake sure you have enough disk space (at least 1GB free)\nTry restarting your computer\nReinstall Cursor from cursor.sh\n\n\n\n9.4 AI features not working\n\nMake sure you’re signed in (check bottom-left corner)\nCheck your internet connection\nTry signing out and back in\n\n\n\n9.5 Code runs but shows errors\n\nCopy the error message\nPress Cmd+L (or Ctrl+L) to open AI Chat\nPaste the error and ask “How do I fix this?”"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#getting-help-during-the-course",
    "href": "courses/caio/cursor-setup-guide.html#getting-help-during-the-course",
    "title": "Cursor IDE Setup Guide",
    "section": "10 Getting Help During the Course",
    "text": "10 Getting Help During the Course\n\nZoom Sessions: Ask questions during live sessions\nCursor AI: Use Cmd+L to ask the AI for help\nDiscussion Board: Post questions for peer assistance\nOffice Hours: [Insert instructor office hours if applicable]"
  },
  {
    "objectID": "courses/caio/cursor-setup-guide.html#next-steps",
    "href": "courses/caio/cursor-setup-guide.html#next-steps",
    "title": "Cursor IDE Setup Guide",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nAfter completing this setup:\n\n✅ Cursor is installed and running\n✅ Python is installed\n✅ Required packages are installed\n✅ You can create and run Python files\n\nYou’re ready for Zoom Session 1 where we’ll practice using Cursor’s AI features together!"
  },
  {
    "objectID": "research.html#nocite",
    "href": "research.html#nocite",
    "title": "Vadim Sokolov",
    "section": "nocite: ‘’",
    "text": "nocite: ‘’\n\n\nBehnia, F., Karbowski, D., and Sokolov, V. (2023), “Deep generative models for vehicle speed trajectories,” Applied Stochastic Models in Business and Industry, 39, 701–719.\n\n\nBendre, S., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the probability of magnus carlsen reaching 2900,” Applied Stochastic Models in Business and Industry, 39, 372–381.\n\n\nPolson, N. G., and Sokolov, V. (2023b), “Generative AI for bayesian computation,” arXiv preprint arXiv:2305.14972.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023a), “Deep partial least squares for instrumental variable regression,” Applied Stochastic Models in Business and Industry.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2023b), “Generative causal inference,” arXiv preprint arXiv:2306.16096.\n\n\nGupta, A., Maharaj, S., Polson, N., and Sokolov, V. (2023), “On the value of chess squares,” Entropy, MDPI, 25, 1374.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2023), “Quantum bayesian computation,” Applied Stochastic Models in Business and Industry.\n\n\nPolson, N., and Sokolov, V. (2023a), “Deep learning: A tutorial,” arXiv preprint arXiv:2310.06251.\n\n\nBaker, E., Barbillon, P., Fadikar, A., Gramacy, R. B., Herbei, R., Higdon, D., Huang, J., Johnson, L. R., Ma, P., Mondal, A., and others (2022), “Analyzing stochastic computer models: A review with opportunities,” Statistical Science, Institute of Mathematical Statistics, 37, 64–89.\n\n\nZha, Y., Parker, S. T., Foster, J. J., and Sokolov, V. (2022), “Housing market forecasting using home showing events,” arXiv preprint arXiv:2201.04003.\n\n\nSchultz, L., Auld, J., and Sokolov, V. (2022), “Bayesian calibration for activity based models,” arXiv preprint arXiv:2203.04414.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022a), “Deep partial least squares for iv regression,” arXiv preprint arXiv:2207.02612.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2022), “Quantum bayes AI,” arXiv preprint arXiv:2208.08068.\n\n\nSchultz, L., and Sokolov, V. (2022), “Deep learning gaussian processes for computer models with heteroskedastic and high-dimensional outputs,” arXiv preprint arXiv:2209.02163.\n\n\nWang, Y., Polson, N., and Sokolov, V. O. (2022), “Data augmentation for bayesian deep learning,” Bayesian Analysis, International Society for Bayesian Analysis, 1, 1–29.\n\n\nLey, H., Auld, J., Verbas, Ö., Weimer, R., Driscoll, S., Mohammadian, K., Golshani, N., Rahim, E., Shabanpour, R., Li, Z., and others (2022), Coordinated transit response planning and operations support tools for mitigating impacts of all-hazard emergency events, United States. Department of Transportation. Federal Transit Administration.\n\n\nNareklishvili, M., Polson, N., and Sokolov, V. (2022b), “Feature selection for personalized policy analysis,” arXiv preprint arXiv:2301.00251.\n\n\nFotouhi, H., Mori, N., Miller-Hooks, E., Sokolov, V., and Sahasrabudhe, S. (2021), “Assessing the effects of limited curbside pickup capacity in meal delivery operations for increased safety during a pandemic,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2675, 436–452.\n\n\nZavareh, M., Maggioni, V., and Sokolov, V. (2021), “Investigating water quality data using principal component analysis and granger causality,” Water, MDPI, 13, 343.\n\n\nPolson, N., Sokolov, V., and Xu, J. (2021), “Deep learning partial least squares,” arXiv preprint arXiv:2106.14085.\n\n\nSokolova, A. O., Marshall, C. H., Lozano, R., Gulati, R., Ledet, E. M., De Sarkar, N., Grivas, P., Higano, C. S., Montgomery, B., Nelson, P. S., and others (2021), “Efficacy of systemic therapies in men with metastatic castration resistant prostate cancer harboring germline ATM versus BRCA2 mutations,” The Prostate, 81, 1382–1389.\n\n\nBhadra, A., Datta, J., Polson, N., Sokolov, V., and Xu, J. (2021), “Merging two cultures: Deep and statistical learning,” arXiv preprint arXiv:2110.11561.\n\n\nHsu, Y. L., Jeng, C. C., Murali, P. S., Torkjazi, M., West, J., Zuber, M., and Sokolov, V. (2021), “Bayesian learning: A selective overview,” arXiv preprint arXiv:2112.12722.\n\n\nPolson, N., and Sokolov, V. (2020), “Deep learning: Computational aspects,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 12, e1500.\n\n\nHuang, X., Li, B., Peng, H., Auld, J. A., and Sokolov, V. O. (2020), “Eco-mobility-on-demand fleet control with ride-sharing,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 23, 3158–3168.\n\n\nSokolov, V. (2020), “Discussion of ‘multivariate generalized hyperbolic laws for modeling financial log-returns—empirical and theoretical considerations’,” Applied Stochastic Models in Business and Industry, John Wiley & Sons, 36, 777–779.\n\n\nDixon, M. F., Polson, N. G., and Sokolov, V. O. (2019), “Deep learning for spatio-temporal modeling: Dynamic traffic flows and high frequency trading,” Applied Stochastic Models in Business and Industry, 35, 788–807.\n\n\nWarren, J., Lipkowitz, J., and Sokolov, V. (2019), “Clusters of driving behavior from observational smartphone data,” IEEE Intelligent Transportation Systems Magazine, IEEE, 11, 171–180.\n\n\nPolson, N. G., and Sokolov, V. (2019), “Bayesian regularization: From tikhonov to horseshoe,” Wiley Interdisciplinary Reviews: Computational Statistics, John Wiley & Sons, Inc. Hoboken, USA, 11, e1463.\n\n\nWang, Y., Polson, N. G., and Sokolov, V. O. (2019), “Scalable data augmentation for deep learning,” arXiv preprint arXiv:1903.09668.\n\n\nSokolov, V., and Polson, M. (2019), “Strategic bayesian asset allocation,” arXiv preprint arXiv:1905.08414.\n\n\nLi, D., Liu, J., Park, N., Lee, D., Ramachandran, G., Seyedmazloom, A., Lee, K., Feng, C., Sokolov, V., and Ganesan, R. (2019), “Solving large-scale 0-1 knapsack problems and its application to point cloud resampling,” arXiv preprint arXiv:1906.05929.\n\n\nChen, H., Jajodia, S., Liu, J., Park, N., Sokolov, V., and Subrahmanian, V. (2019), “FakeTables: Using GANs to generate functional dependency preserving tables with bounded real data.” in IJCAI, pp. 2074–2080.\n\n\nNicholas G. Polson, V. O. S. (2019), “Deep learning,” Wiley StatsRef: Statistics Reference Online.\n\n\nSchultz, L., and Sokolov, V. (2018a), “Bayesian optimization for transportation simulators,” Procedia computer science, Elsevier, 130, 973–978.\n\n\nSchultz, L., and Sokolov, V. (2018b), “Deep reinforcement learning for dynamic urban transportation problems,” arXiv preprint arXiv:1806.05310.\n\n\nSokolov, V., Imran, M., Etherington, D. W., Karbowski, D., and Rousseau, A. (2018), “Effects of predictive real-time traffic signal information,” in 2018 21st international conference on intelligent transportation systems (ITSC), IEEE, pp. 1834–1839.\n\n\nSchultz, L., and Sokolov, V. (2018c), “Practical bayesian optimization for transportation simulators,” arXiv preprint arXiv:1810.03688.\n\n\nPolson, N., and Sokolov, V. (2017a), “Bayesian particle tracking of traffic flows,” IEEE Transactions on Intelligent Transportation Systems, IEEE, 19, 345–356.\n\n\nSokolov, V., Larson, J., Munson, T., Auld, J., and Karbowski, D. (2017), “Maximization of platoon formation through centralized routing and departure time coordination,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2667, 10–16.\n\n\nSokolov, V. (2017), “Discussion of ‘deep learning for finance: Deep portfolios’,” Applied Stochastic Models in Business and Industry, 33, 16–18.\n\n\nAuld, J., Sokolov, V., and Stephens, T. S. (2017), “Analysis of the effects of connected–automated vehicle technologies on travel demand,” Transportation Research Record, SAGE Publications Sage CA: Los Angeles, CA, 2625, 1–8.\n\n\nPolson, N. G., and Sokolov, V. (2017b), “Deep learning: A bayesian perspective.”\n\n\nVerbas, Ö., Sokolov, V., Auld, J., and Ley, H. (2017), “Time-dependent capacitated transit routing with real-time demand and supply data.”\n\n\nAuld, J., Hope, M., Ley, H., Sokolov, V., Xu, B., and Zhang, K. (2016b), “POLARIS: Agent-based modeling framework development and implementation for integrated travel demand and network and operations simulations,” Transportation Research Part C: Emerging Technologies, Pergamon, 64, 101–116.\n\n\nAuld, J., Karbowski, D., Sokolov, V., and Kim, N. (2016a), “A disaggregate model system for assessing the energy impact of transportation at the regional level,” in TRB 2016 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Jongryeol, J. (2016b), “Fuel saving potential of optimal route-based control for plug-in hybrid electric vehicle,” IFAC-PapersOnLine, Elsevier, 49, 128–133.\n\n\nJacquier, E., Polson, N., and Sokolov, V. (2016), “Sequential bayesian learning for merton’s jump model with stochastic volatility,” arXiv preprint arXiv:1610.09750.\n\n\nLarson, J., Munson, T., and Sokolov, V. (2016), “Coordinated platoon routing in a metropolitan network,” in 2016 proceedings of the seventh SIAM workshop on combinatorial scientific computing, Society for Industrial; Applied Mathematics, pp. 73–82.\n\n\nKarbowski, D., Kim, N., Auld, J., and Sokolov, V. (2016a), “Assessing the energy impact of traffic management and vehicle hybridisation,” International Journal of Complexity in Applied Science and Technology, Inderscience Publishers (IEL), 1, 107–124.\n\n\nPolson, N., and Sokolov, V. (2015), “Bayesian analysis of traffic flow on interstate i-55: The LWR model.”\n\n\nSokolov, V., Karbowski, D., Kim, N., and Auld, J. (2015), “Assessing the energy impact of traffic management and vehicle hybridization,” in 25th ITS annual meeting.\n\n\nLuo, Q., Auld, J., and Sokolov, V. (2015), “Addressing some issues of map-matching for large-scale, high-frequency GPS data sets,” in TRB 2015 annual meeting, washington, DC.\n\n\nKarbowski, D., Sokolov, V., and Rousseau, A. (2015), Vehicle energy management optimization through digital maps and connectivity, Argonne National Lab.(ANL), Argonne, IL (United States).\n\n\nRichey, A. S., Richey, J. E., Tan, A., Liu, M., Adam, J. C., and Sokolov, V. (2015), “Assessing the use of remote sensing and a crop growth model to improve modeled streamflow in central asia,” in AGU fall meeting abstracts, pp. H44F–08.\n\n\nSokolov, V., Karbowski, D., and Kim, N. (2014a), “Assessing the energy impact of traffic management and ITS technologies,” in 21st ITS world congress.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and others (2014), “GREET model: The greenhouse gases, regulated emissions, and energy use in transportation model,” Chicago, USA: Argonne National Laboratory.\n\n\nSokolov, V. O., Zhou, X., and Langlois, P.-A. (2014b), “A framework for arterial traffic flow modeling-POLARIS.”\n\n\nAuld, J., Hope, M., Ley, H., Xu, B., Zhang, K., and Sokolov, V. (2013), “Modelling framework for regional integrated simulation of transportation network and activity-based demand (polaris),” in International symposium for next generation infrastructure.\n\n\nAuld, J., Sokolov, V., Fontes, A., and Bautista, R. (2012), “Internet-based stated response survey for no-notice emergency evacuations,” Transportation Letters, Taylor & Francis, 4, 41–53.\n\n\nSokolov, V., Auld, J., and Hope, M. (2012), “A flexible framework for developing integrated models of transportation systems using an agent-based approach,” Procedia Computer Science, Elsevier, 10, 854–859.\n\n\nDatta, B. N., and Sokolov, V. (2011), “A solution of the affine quadratic inverse eigenvalue problem,” Linear Algebra and its Applications, North-Holland, 434, 1745–1760.\n\n\nPark, Y. S., Manli, E., Hope, M., Sokolov, V., and Ley, H. (2010), Fuzzy rule-based approach for evacuation trip demand modeling.\n\n\nWang, M., Sabbisetti, R., Elgowainy, A., Dieffenthaler, D., Anjum, A., Sokolov, V., and GREET, M. (2010), “The greenhouse gases, regulated emissions, and energy use in transportation model,” Center for Transportation Research Argonne National Laboratory, Argonne, IL.\n\n\nDatta, B. N., Deng, S., Sokolov, V., and Sarkissian, D. (2009), “An optimization technique for damped model updating with measured data satisfying quadratic orthogonality constraint,” Mechanical Systems and Signal Processing, Academic Press, 23, 1759–1772.\n\n\nDatta, B. N., and Sokolov, V. (2009), “Quadratic inverse eigenvalue problems, active vibration control and model updating,” Applied and Computational Mathematics, 8, 170–191.\n\n\nSokolov, V. O. (2008), “Quadratic inverse eigenvalue problems: Theory, methods, and applications,” PhD thesis, Northern Illinois University.\n\n\nAre, S., Dostert, P., Ettinger, B., Liu, J., Sokolov, V., Wei, A., and Wiegand, K. (2006), “Reservoir model optimization under uncertainty.”\n\n\nKrukier, L., Pichugina, O., Sokolov, V., and Vulkov, L. (2006), “Numerical investigation of krylov subspace methods for solving non-symmetric systems of linear equations with dominant skew-symmetric part,” International Journal of Numerical Analysis and Modeling, University of Alberta, 3, 115–124."
  },
  {
    "objectID": "courses/caio/01-intro.html",
    "href": "courses/caio/01-intro.html",
    "title": "Tasks and concepts of AI: Generation",
    "section": "",
    "text": "Hephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay\nParticularly pious and learned rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\n\n\n\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…\n\n\n\nJaquet-Droz automata (XVIII century):\n\n\n\n\n\n\n\n\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\n\n\n\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …\n\n\n\n\n\nAI as a science begins with a Turing test (1950).\nThe ides of the Turing test is to check if a machine can imitate a human in a conversation.\nThe original formulation was more nuanced.\n\n\n\n\n\n\n\n\n\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze.\n\n\n\n\n\n\n\n\n\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention.\n\n\n\n\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning).\n\n\n\n\n\n\n\n\n\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science.\n\n\n\n\n\n\n\n\n\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\n\n\n\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead.\n\n\n\n\n\n\n\n\n\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks).\n\n\n\n\n\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors.\n\n\n\n\n\n\n\n\n\n\n\nThe first AI department was at Dec (Digital Equipment Corporation). It is argued that by 1986 he saved the Dec about $10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations.\n\n\n\n\n\n\n\n\n\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step.\n\n\n\n\n\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations\ntraditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments.\nOn the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning.\nAll of those are data-driven approaches.\nThese approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteOld AI\n\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNoteNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information.\n\n\n\n\n\nHow to determine “learning”?\n\n\n\n\n\n\n\nNoteDefinition:\n\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?\n\n\n\n\n\n\n\n\n\n\n\n\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting\n\n\n\n\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\n\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition\n\n\n\n\n\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Competition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection.\nOften the outputs answers are given for a small part of the data, then we call it semi -supervised Learning.\n\n\n\n\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewards possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory.\n\n\n\n\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era\n\n\n\n\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data\n\n\n\n\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic\n\n\n\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)\n\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th\n\n\n\n\n\n\n\n\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\n\n\nKasparov vs IBM’s DeepBlue in 1997\n\n\n\n\n\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two\n\n\n\n\n\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "href": "courses/caio/01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "title": "Tasks and concepts of AI: Generation",
    "section": "The first thoughts about artificial intelligence",
    "text": "The first thoughts about artificial intelligence\n\n\n\n\nHephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay\nParticularly pious and learned rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas)."
  },
  {
    "objectID": "courses/caio/01-intro.html#mechanical-machines",
    "href": "courses/caio/01-intro.html#mechanical-machines",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…"
  },
  {
    "objectID": "courses/caio/01-intro.html#mechanical-machines-1",
    "href": "courses/caio/01-intro.html#mechanical-machines-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nJaquet-Droz automata (XVIII century):"
  },
  {
    "objectID": "courses/caio/01-intro.html#mechanical-machines-2",
    "href": "courses/caio/01-intro.html#mechanical-machines-2",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Mechanical machines",
    "text": "Mechanical machines\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …"
  },
  {
    "objectID": "courses/caio/01-intro.html#turing-test",
    "href": "courses/caio/01-intro.html#turing-test",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "courses/caio/01-intro.html#shennons-theseus",
    "href": "courses/caio/01-intro.html#shennons-theseus",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Shennon’s Theseus",
    "text": "Shennon’s Theseus\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze."
  },
  {
    "objectID": "courses/caio/01-intro.html#stanford-cart",
    "href": "courses/caio/01-intro.html#stanford-cart",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Stanford Cart",
    "text": "Stanford Cart\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention."
  },
  {
    "objectID": "courses/caio/01-intro.html#turing-test-1",
    "href": "courses/caio/01-intro.html#turing-test-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "courses/caio/01-intro.html#dartmouth-workshop",
    "href": "courses/caio/01-intro.html#dartmouth-workshop",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science."
  },
  {
    "objectID": "courses/caio/01-intro.html#dartmouth-workshop-1",
    "href": "courses/caio/01-intro.html#dartmouth-workshop-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."
  },
  {
    "objectID": "courses/caio/01-intro.html#great-hopes",
    "href": "courses/caio/01-intro.html#great-hopes",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "1956-1960: Great hopes",
    "text": "1956-1960: Great hopes\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead."
  },
  {
    "objectID": "courses/caio/01-intro.html#big-hopes",
    "href": "courses/caio/01-intro.html#big-hopes",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "1956-1960: Big Hopes",
    "text": "1956-1960: Big Hopes\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks)."
  },
  {
    "objectID": "courses/caio/01-intro.html#s-knowledge-based-systems",
    "href": "courses/caio/01-intro.html#s-knowledge-based-systems",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "1970s: Knowledge Based Systems",
    "text": "1970s: Knowledge Based Systems\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors."
  },
  {
    "objectID": "courses/caio/01-intro.html#commercial-applications-industry-ai",
    "href": "courses/caio/01-intro.html#commercial-applications-industry-ai",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "1980-2010: Commercial applications Industry AI",
    "text": "1980-2010: Commercial applications Industry AI\n\nThe first AI department was at Dec (Digital Equipment Corporation). It is argued that by 1986 he saved the Dec about $10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations."
  },
  {
    "objectID": "courses/caio/01-intro.html#data-mining-machine-learning",
    "href": "courses/caio/01-intro.html#data-mining-machine-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "1990-2010: DATA MINING, MACHINE LEARNING",
    "text": "1990-2010: DATA MINING, MACHINE LEARNING\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step."
  },
  {
    "objectID": "courses/caio/01-intro.html#rule-based-system-vs-bayes",
    "href": "courses/caio/01-intro.html#rule-based-system-vs-bayes",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "courses/caio/01-intro.html#rule-based-system-vs-bayes-1",
    "href": "courses/caio/01-intro.html#rule-based-system-vs-bayes-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "courses/caio/01-intro.html#definition",
    "href": "courses/caio/01-intro.html#definition",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "DEFINITION",
    "text": "DEFINITION\n\nHow to determine “learning”?\n\n\n\n\nDefinition:\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML: Supervised Learning",
    "text": "Tasks and concepts of ML: Supervised Learning\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\n\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Competition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection.\nOften the outputs answers are given for a small part of the data, then we call it semi -supervised Learning."
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML: reinforcement learning",
    "text": "Tasks and concepts of ML: reinforcement learning\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewards possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory."
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML: active learning",
    "text": "Tasks and concepts of ML: active learning\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI: Reasoning",
    "text": "Tasks and concepts of AI: Reasoning\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-representation",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-representation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI: Representation",
    "text": "Tasks and concepts of AI: Representation\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)"
  },
  {
    "objectID": "courses/caio/01-intro.html#chess-and-ai",
    "href": "courses/caio/01-intro.html#chess-and-ai",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Chess and AI",
    "text": "Chess and AI\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\n\n\nKasparov vs IBM’s DeepBlue in 1997"
  },
  {
    "objectID": "courses/caio/01-intro.html#alphago-zero",
    "href": "courses/caio/01-intro.html#alphago-zero",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two"
  },
  {
    "objectID": "courses/caio/01-intro.html#probability-in-machine-learning",
    "href": "courses/caio/01-intro.html#probability-in-machine-learning",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Probability in machine learning",
    "text": "Probability in machine learning\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "courses/caio/01-intro.html#review-of-basic-probability-concepts",
    "href": "courses/caio/01-intro.html#review-of-basic-probability-concepts",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Review of Basic Probability Concepts",
    "text": "Review of Basic Probability Concepts\nProbability lets us talk efficiently about things that we are uncertain about.\n\nWhat will Amazon’s sales be next quarter?\nWhat will the return be on my stocks next year?\nHow often will users click on a particular Google ad?\n\nAll these involve estimating or predicting unknowns!!"
  },
  {
    "objectID": "courses/caio/01-intro.html#random-variables",
    "href": "courses/caio/01-intro.html#random-variables",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Random Variables",
    "text": "Random Variables\nRandom Variables are numbers that we are not sure about. There’s a list of potential outcomes. We assign probabilities to each outcome.\nExample: Suppose that we are about to toss two coins. Let \\(X\\) denote the number of heads. We call \\(X\\) the random variable that stands for the potential outcome."
  },
  {
    "objectID": "courses/caio/01-intro.html#probability",
    "href": "courses/caio/01-intro.html#probability",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Probability",
    "text": "Probability\nProbability is a language designed to help us communicate about uncertainty. We assign a number between \\(0\\) and \\(1\\) measuring how likely that event is to occur. It’s immensely useful, and there’s only a few basic rules.\n\nIf an event \\(A\\) is certain to occur, it has probability \\(1\\), denoted \\(P(A)=1\\)\nEither an event \\(A\\) occurs or it does not. \\[P(A) = 1 - P(\\text{not }A)\\]\nIf two events are mutually exclusive (both cannot occur simultaneously) then \\[P(A \\text{ or } B) = P(A) + P(B)\\]\nJoint probability, when events are independent \\[P(A \\text{ and } B) = P( A) P(B)\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#probability-distribution",
    "href": "courses/caio/01-intro.html#probability-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Probability Distribution",
    "text": "Probability Distribution\nWe describe the behavior of random variables with a Probability Distribution\nExample: Suppose we are about to toss two coins. Let \\(X\\) denote the number of heads.\n\\[X = \\left\\{ \\begin{array}{ll}\n0 \\text{ with prob. } 1/4\\\\\n1 \\text{ with prob. } 1/2\\\\\n2 \\text{ with prob. } 1/4\n\\end{array}\n\\right.\\]\n\\(X\\) is called a Discrete Random Variable\nQuestion: What is \\(P(X=0)\\)? How about \\(P(X \\geq 1)\\)?"
  },
  {
    "objectID": "courses/caio/01-intro.html#pete-rose-hitting-streak",
    "href": "courses/caio/01-intro.html#pete-rose-hitting-streak",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Pete Rose Hitting Streak",
    "text": "Pete Rose Hitting Streak\nPete Rose of the Cincinnati Reds set a National League record of hitting safely in \\(44\\) consecutive games …\n\nRose was a \\(300\\) hitter. Assume he comes to bat \\(4\\) times each game.\nEach at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.\n\nWhat probability might reasonably be associated with that hitting streak?\nJoe DiMaggio’s record is \\(56\\)! His batting average was \\(.325\\)"
  },
  {
    "objectID": "courses/caio/01-intro.html#pete-rose-hitting-streak-solution",
    "href": "courses/caio/01-intro.html#pete-rose-hitting-streak-solution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Pete Rose Hitting Streak (Solution)",
    "text": "Pete Rose Hitting Streak (Solution)\nLet \\(A_i\\) denote the event that “Rose hits safely in the ith game”\n\\(P(\\text{Rose Hits Safely in 44 consecutive games}) = P(A_1 \\text{ and } A_2 \\ldots \\text{ and } A_{44}) = P(A_1) P(A_2) \\ldots P(A_{44})\\)\nWe now need to find \\(P(A_i) \\ldots\\) where \\(P(A_i) = 1 - P(\\text{not } A_i)\\)\n\\[\\begin{align*}\nP(A_1) &= 1 - P(\\text{not } A_1) \\\\\n&= 1 - P(\\text{Rose makes 4 outs}) \\\\\n&= 1 - (0.7)^4 = 0.76\n\\end{align*}\\]\nSo for the winning streak we have \\((0.76)^{44} = 0.0000057\\)!!!"
  },
  {
    "objectID": "courses/caio/01-intro.html#pete-rose-hitting-streak-inference",
    "href": "courses/caio/01-intro.html#pete-rose-hitting-streak-inference",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Pete Rose Hitting Streak (Inference)",
    "text": "Pete Rose Hitting Streak (Inference)\nThere are three basic inferences:\n\nThis means that the odds for a particular player as good as Pete Rose starting a hitting streak today are 175,470 to 1\nDoesn’t mean that the run of \\(44\\) won’t be beaten by some player at some time: the Law of Very Large Numbers\nJoe DiMaggio’s record is 56!!!! It’s going to be hard to beat. We have \\((0.792)^{56} = 2.13 \\times 10^{-6}\\) or 455,962 to 1"
  },
  {
    "objectID": "courses/caio/01-intro.html#example-happiness-index",
    "href": "courses/caio/01-intro.html#example-happiness-index",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Example: Happiness Index",
    "text": "Example: Happiness Index\n“happiness index” as a function of salary.\n\n\n\nSalary (\\(X\\))\nHappiness (\\(Y\\)): 0 (low)\n1 (medium)\n2 (high)\n\n\n\n\nlow 0\n0.03\n0.12\n0.07\n\n\nmedium 1\n0.02\n0.13\n0.11\n\n\nhigh 2\n0.01\n0.13\n0.14\n\n\nvery high 3\n0.01\n0.09\n0.14\n\n\n\nIs \\(P(Y=2 \\mid X=3) &gt; P(Y=2)\\)?"
  },
  {
    "objectID": "courses/caio/01-intro.html#bayes-rule",
    "href": "courses/caio/01-intro.html#bayes-rule",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nThe computation of \\(P(x \\mid y)\\) from \\(P(x)\\) and \\(P(y \\mid x)\\) is called Bayes theorem: \\[\nP(x \\mid y) = \\frac{P(y,x)}{P(y)} = \\frac{P(y\\mid x)p(x)}{p(y)}\n\\]\nThis shows now the conditional distribution is related to the joint and marginal distributions.\nYou’ll be given all the quantities on the r.h.s."
  },
  {
    "objectID": "courses/caio/01-intro.html#bayes-rule-1",
    "href": "courses/caio/01-intro.html#bayes-rule-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nKey fact: \\(P(x \\mid y)\\) is generally different from \\(P(y \\mid x)\\)!\nExample: Most people would agree\n\\[\\begin{align*}\nPr  & \\left ( Practice \\; hard  \\mid  Play \\; in \\; NBA \\right ) \\approx  1\\\\\nPr  & \\left ( Play \\; in \\; NBA  \\mid  Practice \\; hard  \\right ) \\approx  0\n\\end{align*}\\]\nThe main reason for the difference is that \\(P( Play \\; in \\; NBA ) \\approx 0\\)."
  },
  {
    "objectID": "courses/caio/01-intro.html#independence",
    "href": "courses/caio/01-intro.html#independence",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Independence",
    "text": "Independence\nTwo random variable \\(X\\) and \\(Y\\) are independent if \\[\nP(Y = y  \\mid X = x) = P (Y = y)\n\\] for all possible \\(x\\) and \\(y\\) values. Knowing \\(X=x\\) tells you nothing about \\(Y\\)!\nExample: Tossing a coin twice. What’s the probability of getting \\(H\\) in the second toss given we saw a \\(T\\) in the first one?"
  },
  {
    "objectID": "courses/caio/01-intro.html#sally-clark-case-independence-or-bayes",
    "href": "courses/caio/01-intro.html#sally-clark-case-independence-or-bayes",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Sally Clark Case: Independence or Bayes?",
    "text": "Sally Clark Case: Independence or Bayes?\nSally Clark was accused and convicted of killing her two children\nThey could have both died of SIDS.\n\nThe chance of a family which are non-smokers and over 25 having a SIDS death is around 1 in 8,500.\nThe chance of a family which has already had a SIDS death having a second is around 1 in 100.\nThe chance of a mother killing her two children is around 1 in 1,000,000."
  },
  {
    "objectID": "courses/caio/01-intro.html#bayes-or-independence",
    "href": "courses/caio/01-intro.html#bayes-or-independence",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Bayes or Independence",
    "text": "Bayes or Independence\n\nUnder Bayes \\[\\begin{align*}\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)   &  = P \\left(\n\\mathrm{first} \\; \\mathrm{SIDS} \\right)  P \\left(  \\mathrm{Second} \\; \\;\n\\mathrm{SIDS} | \\mathrm{first} \\; \\mathrm{SIDS} \\right) \\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}\n\\end{align*}\\]\n\nThe \\(\\frac{1}{100}\\) comes from taking into account genetics.\n\nIndependence, as the court did, gets you\n\n\\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000)\n\\]\n\nBy Bayes rule\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{P( E \\cap I)}{P( E \\cap G)}\n\\] \\(P( E \\cap I) = P(E|I )P(I)\\) needs discussion of \\(p(I)\\)."
  },
  {
    "objectID": "courses/caio/01-intro.html#comparison",
    "href": "courses/caio/01-intro.html#comparison",
    "title": "Tasks and concepts of AI: Generation",
    "section": "Comparison",
    "text": "Comparison\n\nHence putting these two together gives the odds of guilt as\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1/850,000}{1/1,000,000} = 1.15\n\\] In terms of posterior probabilities\n\\[\np( G|E) = \\frac{1}{1 + O(G|E)} = 0.465\n\\]\n\nIf you use independence\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1}{73} \\; \\text{and} \\; p( G|E) \\approx 0.99\n\\] The suspect looks guilty."
  },
  {
    "objectID": "courses/caio/01-intro.html#random-variables-expectation-ex",
    "href": "courses/caio/01-intro.html#random-variables-expectation-ex",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Random Variables: Expectation \\(E(X)\\)",
    "text": "Random Variables: Expectation \\(E(X)\\)\nThe expected value of a random variable is simply a weighted average of the possible values X can assume.\nThe weights are the probabilities of occurrence of those values.\n\\[E(X) = \\sum_x xP(X=x)\\]\nWith \\(n\\) equally likely outcomes with values \\(x_1, \\ldots, x_n\\), \\(P(X = x_i) = 1/n\\)\n\\[E(X) = \\frac{x_1+x_2+\\ldots+x_n}{n}\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#roulette-expectation",
    "href": "courses/caio/01-intro.html#roulette-expectation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Roulette Expectation",
    "text": "Roulette Expectation\n\nEuropean Odds: 36 numbers (red/black) + zero\nYou bet $1 on 11 Black (pays 35 to 1)\n\\(X\\) is the return on this bet\n\n\\[E(X) = \\frac{1}{37}\\times 36 + \\frac{36}{37}\\times 0 = 0.97\\]\n\nIf you bet $1 on Black (pays 1 to 1)\n\n\\[E(X) = \\frac{18}{37}\\times 2 + \\frac{19}{37}\\times 0 = 0.97\\]\nCasino is guaranteed to make money in the long run!"
  },
  {
    "objectID": "courses/caio/01-intro.html#standard-deviation-sdx-and-variance-varx",
    "href": "courses/caio/01-intro.html#standard-deviation-sdx-and-variance-varx",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Standard Deviation \\(sd(X)\\) and Variance \\(Var(X)\\)",
    "text": "Standard Deviation \\(sd(X)\\) and Variance \\(Var(X)\\)\nThe variance is calculated as\n\\[Var(X) = E\\left((X - E(X))^2\\right)\\]\nA simpler calculation is \\(Var(X) = E(X^2) - E(X)^2\\).\nThe standard deviation is the square-root of variance.\n\\[sd(X) = \\sqrt{Var(X)}\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#roulette-variance",
    "href": "courses/caio/01-intro.html#roulette-variance",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Roulette Variance",
    "text": "Roulette Variance\n\nEuropean Odds: 36 numbers (red/black) + zero\nYou bet $1 on 11 Black (pays 35 to 1)\n\\(X\\) is the return on this bet\n\n\\[Var(X) = \\frac{1}{37}\\times (36 - 0.97)^2 + \\frac{36}{37}\\times (0 - 0.97)^2 = 34\\]\n\nIf you bet $1 on Black (pays 1 to 1)\n\n\\[Var(X) = \\frac{18}{37}\\times (2 - 0.97)^2+ \\frac{19}{37}\\times (0- 0.97)^2 = 1\\]\nIf your goal is to spend as much time as possible in the casino (free drinks): place small bets on black/red"
  },
  {
    "objectID": "courses/caio/01-intro.html#example-ex-and-varx",
    "href": "courses/caio/01-intro.html#example-ex-and-varx",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Example: \\(E(X)\\) and \\(Var(X)\\)",
    "text": "Example: \\(E(X)\\) and \\(Var(X)\\)\nTortoise and Hare are selling cars. Probability distributions, means and variances for \\(X\\), the number of cars sold\n\n\n\n\n0\n1\n2\n3\nMean\nVariance\nsd\n\n\n\n\ncars sold\n\n\\(X\\)\n\n\n\\(E(X)\\)\n\\(Var(X)\\)\n\\(\\sqrt{Var(X)}\\)\n\n\nTortoise\n0\n0.5\n0.5\n0\n1.5\n0.25\n0.5\n\n\nHare\n0.5\n0\n0\n0.5\n1.5\n2.25\n1.5"
  },
  {
    "objectID": "courses/caio/01-intro.html#expectation-and-variance-calculations",
    "href": "courses/caio/01-intro.html#expectation-and-variance-calculations",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Expectation and Variance Calculations",
    "text": "Expectation and Variance Calculations\nLet’s do Tortoise expectations and variances\n\nThe Tortoise \\[\\begin{align*}\nE(T) &= (1/2)(1) + (1/2)(2) = 1.5 \\\\\nVar(T) &= E(T^2) - E(T)^2 \\\\\n     &= (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25\n\\end{align*}\\]\nNow the Hare’s \\[\\begin{align*}\nE(H) &= (1/2)(0) + (1/2)(3) = 1.5 \\\\\nVar(H) &= (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25\n\\end{align*}\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#expectation-and-variance-interpretation",
    "href": "courses/caio/01-intro.html#expectation-and-variance-interpretation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Expectation and Variance Interpretation",
    "text": "Expectation and Variance Interpretation\nWhat do these tell us above the long run behavior?\n\nTortoise and Hare have the same expected number of cars sold.\nTortoise is more predictable than Hare. He has a smaller variance The standard deviations \\(\\sqrt{Var(X)}\\) are \\(0.5\\) and \\(1.5\\), respectively\nGiven two equal means, you always want to pick the lower variance."
  },
  {
    "objectID": "courses/caio/01-intro.html#linear-combinations-of-random-variables",
    "href": "courses/caio/01-intro.html#linear-combinations-of-random-variables",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\nTwo key properties:\nLet \\(a, b\\) be given constants\n\nExpectations and Variances \\[\\begin{align*}\nE(aX + bY) &= a E(X) + b E(Y) \\\\\nVar(aX + bY) &= a^2 Var(X) + b^2 Var(Y) + 2 ab Cov(X,Y)\n\\end{align*}\\]\n\nwhere \\(Cov(X,Y)\\) is the covariance between random variables."
  },
  {
    "objectID": "courses/caio/01-intro.html#tortoise-and-hare-portfolio",
    "href": "courses/caio/01-intro.html#tortoise-and-hare-portfolio",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tortoise and Hare Portfolio",
    "text": "Tortoise and Hare Portfolio\nWhat about Tortoise and Hare? We need to know \\(Cov(\\text{Tortoise, Hare})\\). Let’s take \\(Cov(T,H) = -1\\) and see what happens\nSuppose \\(a = \\frac{1}{2}, b= \\frac{1}{2}\\) Expectation and Variance\n\\[\\begin{align*}\nE\\left(\\frac{1}{2} T + \\frac{1}{2} H\\right) &= \\frac{1}{2} E(T) + \\frac{1}{2} E(H) = \\frac{1}{2} \\times 1.5 + \\frac{1}{2} \\times 1.5 = 1.5 \\\\\nVar\\left(\\frac{1}{2} T + \\frac{1}{2} H\\right) &= \\frac{1}{4} 0.25 + \\frac{1}{4} 2.25 - 2 \\frac{1}{2} \\frac{1}{2} = 0.625 - 0.5 = 0.125\n\\end{align*}\\]\nMuch lower!"
  },
  {
    "objectID": "courses/caio/01-intro.html#personalization-conditional-probability",
    "href": "courses/caio/01-intro.html#personalization-conditional-probability",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "“Personalization\" \\(=\\)”Conditional Probability\"",
    "text": "“Personalization\" \\(=\\)”Conditional Probability\"\n\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\n\n Many Business Applications!! Suggestions vs Search…."
  },
  {
    "objectID": "courses/caio/01-intro.html#bayess-rule-in-medical-diagnostics",
    "href": "courses/caio/01-intro.html#bayess-rule-in-medical-diagnostics",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Bayes’s Rule in Medical Diagnostics",
    "text": "Bayes’s Rule in Medical Diagnostics\nAlice is a 40-year-old women, what is the chance that she really has breast cancer when she gets positive mammogram result, given the conditions:\n\nThe prevalence of breast cancer among people like Alice is 1%.\nThe test has an 80% detection rate.\nThe test has a 10% false-positive rate.\n\nThe posterior probability \\(P(\\text{cancer} \\mid \\text{positive mammogram})\\)?"
  },
  {
    "objectID": "courses/caio/01-intro.html#medical-diagnostics---visualization",
    "href": "courses/caio/01-intro.html#medical-diagnostics---visualization",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Medical Diagnostics - Visualization",
    "text": "Medical Diagnostics - Visualization\n\n\n\n\n\nMedical Screening\n\n\n\nOf 1000 cases:\n\n108 positive mammograms. 8 are true positives. The remaining 100 are false positives.\n892 negative mammograms. 2 are false negatives. The other 890 are true negatives."
  },
  {
    "objectID": "courses/caio/01-intro.html#test-marketing-a-new-product",
    "href": "courses/caio/01-intro.html#test-marketing-a-new-product",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Test Marketing a New Product",
    "text": "Test Marketing a New Product\nBasic Problem:\n\nYour company is developing a new product and will be test marketing to better gauge the sales of the new product.\nBased on positive, neutral or negative reactions, what are the probability of high and low sales?\n\nNetflix Bayes and AI"
  },
  {
    "objectID": "courses/caio/01-intro.html#test-marketing---setup",
    "href": "courses/caio/01-intro.html#test-marketing---setup",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Test Marketing - Setup",
    "text": "Test Marketing - Setup\nSuppose you are given the following information\n\nNew products introduced in the marketplace have high sales 8% of the time and low sales 92% of the time.\nA marketing test has the following accuracies: If sales are high, then consumer test reaction is positive 70%, neutral 25% and negative 5%. If sales are low, then consumer test reaction is positive 15%, neutral 35% and negative 50%."
  },
  {
    "objectID": "courses/caio/01-intro.html#test-marketing---solution-steps",
    "href": "courses/caio/01-intro.html#test-marketing---solution-steps",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Test Marketing - Solution Steps",
    "text": "Test Marketing - Solution Steps\nStep 1: Set-up your notation. Let \\[H = \\text{high sales} \\quad L = \\text{low sales}\\] \\[\\text{Pos} = \\text{positive} \\quad \\text{Neu = Neutral} \\quad \\text{Neg} = \\text{Negative}\\]\nStep 2: List the known conditional probabilities For the marketing test we have \\[\\begin{align*}\nP(\\text{Pos} \\mid H) &= 0.70, P(\\text{Neu} \\mid H)=0.25, P(\\text{Neg} \\mid H) = 0.05 \\\\\nP(\\text{Pos} \\mid L) &= 0.15, P(\\text{Neu} \\mid L)=0.35, P(\\text{Neg} \\mid L) = 0.50\n\\end{align*}\\]\nFinally, the base rates are \\(P(H) = 0.08\\) and \\(P(L) = 0.92\\)"
  },
  {
    "objectID": "courses/caio/01-intro.html#test-marketing---calculation",
    "href": "courses/caio/01-intro.html#test-marketing---calculation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Test Marketing - Calculation",
    "text": "Test Marketing - Calculation\nStep 3: Describe the posterior probabilities that are required: \\[P(H \\mid \\text{Pos})\\]\nThe probability of high sales given a positive marketing test. Compute the probability of a positive test \\[\\begin{align*}\nP(\\text{Pos}) &= P(\\text{Pos} \\mid H)P(H) +P(\\text{Pos} \\mid L)P(L) \\\\\n&= 0.70 \\times 0.08 + 0.15 \\times 0.92 = 0.194\n\\end{align*}\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#test-marketing---final-answer",
    "href": "courses/caio/01-intro.html#test-marketing---final-answer",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Test Marketing - Final Answer",
    "text": "Test Marketing - Final Answer\nNow use Bayes Rule \\[\\begin{align*}\nP(H \\mid \\text{Pos}) &= \\frac{P(\\text{Pos} \\mid H)P(H)}{P(\\text{Pos})} \\\\\n&= \\frac{0.70 \\times 0.08}{0.194} = 0.288\n\\end{align*}\\]\nHence 28.8% you’ll have high sales in the market.\nWe should interpret this relative to our initial probability of only \\(8\\)%."
  },
  {
    "objectID": "courses/caio/01-intro.html#personalization-conditional-probability-1",
    "href": "courses/caio/01-intro.html#personalization-conditional-probability-1",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "“Personalization” = “Conditional Probability”",
    "text": "“Personalization” = “Conditional Probability”\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\nMany Business Applications!! Suggestions vs Search, …."
  },
  {
    "objectID": "courses/caio/01-intro.html#how-does-netflix-give-recommendations",
    "href": "courses/caio/01-intro.html#how-does-netflix-give-recommendations",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "How does Netflix Give Recommendations?",
    "text": "How does Netflix Give Recommendations?\nWill a subscriber like Saving Private Ryan, given that he or she liked the HBO series Band of Brothers?\nBoth are epic dramas about the Normandy invasion and its aftermath.\n100 people in your database, and every one of them has seen both films.\nTheir viewing histories come in the form of a big “ratings matrix”.\n\n\n\n\nLiked Band of Brothers\nDidn’t like it\n\n\n\n\nLiked Saving Private Ryan\n56 subscribers\n6 subscribers\n\n\nDidn’t like it\n14 subscribers\n24 subscribers\n\n\n\n\\[P(\\text{likes Saving Private Ryan} \\mid \\text{likes Band of Brothers})=\\frac{56}{56+14}=80\\%\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#how-does-netflix-give-recommendations---complexity",
    "href": "courses/caio/01-intro.html#how-does-netflix-give-recommendations---complexity",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "How does Netflix Give Recommendations? - Complexity",
    "text": "How does Netflix Give Recommendations? - Complexity\nBut real problem is much more complicated:\n\nScale. It has 100 million subscribers and ratings data on more than 10,000 shows. The ratings matrix has more than a trillion possible entries.\n“Missingness”. Most subscribers haven’t watched most films. Moreover, missingness pattern is informative.\nCombinatorial explosion. In a database with 10,000 films, no one else’s history is exactly the same as yours.\n\nThe solution to all three issues is careful modeling."
  },
  {
    "objectID": "courses/caio/01-intro.html#how-does-netflix-give-recommendations---fundamental-equation",
    "href": "courses/caio/01-intro.html#how-does-netflix-give-recommendations---fundamental-equation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "How does Netflix Give Recommendations? - Fundamental Equation",
    "text": "How does Netflix Give Recommendations? - Fundamental Equation\nThe fundamental equation is: \\[\\text{Predicted Rating} =\\text{Overall Average} + \\text{Film Offset} + \\text{User Offset} + \\text{User-Film Interaction}\\]\nThese three terms provide a baseline for a given user/film pair:\n\nThe overall average rating across all films is 3.7.\nEvery film has its own offset. Popular movies have positive offsets.\nEvery user has an offset. Some users are more or less critical than average."
  },
  {
    "objectID": "courses/caio/01-intro.html#netflix---latent-features",
    "href": "courses/caio/01-intro.html#netflix---latent-features",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Netflix - Latent Features",
    "text": "Netflix - Latent Features\n\nThe User-Film Interaction is calculated based on a person’s ratings of similar films exhibit patterns because those ratings are all associated with a latent feature of that person.\nThere’s not just one latent feature to describe Netflix subscribers, but dozens or even hundreds. There’s a “British murder mystery” feature, a “gritty character-driven crime drama” feature, a “cooking show” feature, a “hipster comedy films” feature, …"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-hidden-features-tell-the-story",
    "href": "courses/caio/01-intro.html#the-hidden-features-tell-the-story",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "The Hidden Features Tell the Story",
    "text": "The Hidden Features Tell the Story\n\nThese latent features are the magic elixir of the digital economy–a special brew of data, algorithms, and human insight that represents the most perfect tool ever conceived for targeted marketing.\nYour precise combination of latent features–your tiny little corner of a giant multidimensional Euclidean space–makes you a demographic of one.\nNetflix spent $130 million for 10 episodes on The Crown. Other network television: $400 million commissioning 113 pilots, of which 13 shows made it to a second season."
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ml",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ml",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of ML",
    "text": "Tasks and concepts of ML"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI",
    "text": "Tasks and concepts of AI"
  },
  {
    "objectID": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "href": "courses/caio/01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th"
  },
  {
    "objectID": "courses/caio/02-ml.html#why-distributions-matter",
    "href": "courses/caio/02-ml.html#why-distributions-matter",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Why Distributions Matter",
    "text": "Why Distributions Matter\n\n\n\nMachine learning is built on probability\nDistributions describe uncertainty in data\nThree fundamental distributions:\n\nBinomial: Binary outcomes (yes/no, win/lose)\nPoisson: Count data (arrivals, events)\nNormal: Continuous measurements (heights, returns)\n\n\n\nWhy Should Executives Care?\n\n\n\nBusiness Question\nDistribution\n\n\n\n\nWill the customer buy?\nBinomial\n\n\nHow many orders today?\nPoisson\n\n\nWhat’s the forecast error?\nNormal\n\n\n\nChoosing the right distribution is the first step in building a reliable model. Wrong distribution = wrong predictions!"
  },
  {
    "objectID": "courses/caio/02-ml.html#binomial-distribution",
    "href": "courses/caio/02-ml.html#binomial-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\nModels the number of successes in \\(n\\) independent trials, each with probability \\(p\\)\n\\[P(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}\\]\nKey Parameters:\n\n\\(n\\) = number of trials\n\\(p\\) = probability of success\nMean = \\(np\\)\nVariance = \\(np(1-p)\\)\n\nExamples: A/B test conversions, click-through rates, quality defects"
  },
  {
    "objectID": "courses/caio/02-ml.html#nfl-patriots-coin-toss",
    "href": "courses/caio/02-ml.html#nfl-patriots-coin-toss",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "NFL Patriots Coin Toss",
    "text": "NFL Patriots Coin Toss\n\n\nThe Patriots won 19 out of 25 coin tosses in 2014-15. How likely?\n\nThere are 177,100 ways to arrange 19 wins in 25 games\nEach specific sequence has probability \\(0.5^{25}\\)\nCombined probability: 0.5% or odds of 199 to 1 against\n\n\n\nShow R code\n# \"25 choose 19\" = number of ways to pick 19 wins from 25 games\nchoose(25, 19)\n\n\n[1] 177100\n\n\nShow R code\n# Probability = (ways to get 19 wins) × (probability of any specific sequence)\nchoose(25, 19) * 0.5^25\n\n\n[1] 0.005277991\n\n\n\nThe “Law of Large Numbers” Perspective:\nWith 32 NFL teams over 20+ years, some team will have a suspicious streak!\nKey insight: Probability of Patriots specifically = 0.5%. But probability that some team has a streak ≈ much higher!\nBusiness lesson: When auditing for fraud or anomalies:\n\nDon’t just flag rare events\nConsider how many opportunities for rare events exist\nAdjust for “multiple comparisons”\n\nLooking at enough data, you’ll always find something “unusual”"
  },
  {
    "objectID": "courses/caio/02-ml.html#poisson-distribution",
    "href": "courses/caio/02-ml.html#poisson-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nModels count of random events: goals, arrivals, defects, clicks\n\\[P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\n\n\\(\\lambda\\) (lambda): expected rate of events\nKey property: Mean = Variance = \\(\\lambda\\)\nEvents occur independently at a constant average rate\n\n\nBusiness Applications:\n\nCustomer arrivals per hour\nWebsite clicks per day\nManufacturing defects per batch\nInsurance claims per year\nServer requests per minute\n\nIf events are rare and independent, Poisson is your model!"
  },
  {
    "objectID": "courses/caio/02-ml.html#poisson-customer-arrivals",
    "href": "courses/caio/02-ml.html#poisson-customer-arrivals",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Poisson: Customer Arrivals",
    "text": "Poisson: Customer Arrivals\nExample: Customers arrive at 3 per hour. Probability of exactly 5?\n\n# P(X = 5) when lambda = 3\ndpois(5, lambda = 3)\n\n[1] 0.1008188\n\n\nAbout 10% chance of seeing exactly 5 customers"
  },
  {
    "objectID": "courses/caio/02-ml.html#normal-distribution",
    "href": "courses/caio/02-ml.html#normal-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nThe “bell curve” — the most important distribution in statistics\nThe 68-95-99.7 Rule:\n\n68% of data within 1 standard deviation\n95% of data within 2 standard deviations\n99.7% of data within 3 standard deviations\n\nWhy it’s everywhere: Central Limit Theorem guarantees that averages of many random events become Normal\nApplications: Quality control, financial risk, test scores, measurement error"
  },
  {
    "objectID": "courses/caio/02-ml.html#normal-heights-of-adults",
    "href": "courses/caio/02-ml.html#normal-heights-of-adults",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Normal: Heights of Adults",
    "text": "Normal: Heights of Adults\n\n\nMale heights follow a Normal distribution: mean = 70 inches, sd = 3 inches\n\n68% of men are between 67-73 inches (within 1 sd)\nThe 95th percentile is about 75 inches — only 5% are taller\n\n\n\nShow R code\n# What proportion are between 67 and 73 inches (+/- 1 sd)?\npnorm(73, mean = 70, sd = 3) - pnorm(67, mean = 70, sd = 3)\n\n\n[1] 0.6826895\n\n\nShow R code\n# What height is taller than 95% of men?\nqnorm(0.95, mean = 70, sd = 3)\n\n\n[1] 74.93456\n\n\n\nR Functions for Normal Distribution:\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\npnorm()\nProbability ≤ x\nP(height ≤ 73)\n\n\nqnorm()\nFind percentile\n95th percentile\n\n\ndnorm()\nDensity at x\nHeight of curve\n\n\nrnorm()\nRandom samples\nSimulate data\n\n\n\nBusiness Applications:\n\nSetting size ranges for products\nEstablishing “normal” ranges for KPIs\nIdentifying outliers (&gt; 2-3 sd)\nQuality control limits"
  },
  {
    "objectID": "courses/caio/02-ml.html#what-is-regression",
    "href": "courses/caio/02-ml.html#what-is-regression",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "What is Regression?",
    "text": "What is Regression?\n\n\nFinding the relationship between variables\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\n\\(\\beta_0\\): intercept (baseline value)\n\\(\\beta_1\\): slope (change in \\(y\\) per unit change in \\(x\\))\n\\(\\epsilon\\): unexplained variation\n\nGoal: Minimize sum of squared prediction errors\n\nBusiness Questions Regression Answers:\n\nHow much does price affect sales?\nWhat’s the ROI of advertising spend?\nHow does experience affect salary?\nWhat drives customer lifetime value?\nHow does weather affect demand?\n\nRegression quantifies relationships and enables prediction."
  },
  {
    "objectID": "courses/caio/02-ml.html#simple-example-house-prices",
    "href": "courses/caio/02-ml.html#simple-example-house-prices",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Simple Example: House Prices",
    "text": "Simple Example: House Prices\n\n\nUsing Saratoga County housing data, we fit a model:\nPrice = f(Living Area)\n\nIntercept: Base price of ~$13,000 (land value)\nSlope: Each additional square foot adds ~$113 to the price\n\nA 2,000 sq ft house: $13K + (2000 × $113) = $239,000\n\n\nShow R code\nd &lt;- read.csv(\"data/SaratogaHouses.csv\")\nmodel &lt;- lm(price ~ livingArea, data = d)\ncoef(model)\n\n\n(Intercept)  livingArea \n 13439.3940    113.1225 \n\n\n\nInterpreting Coefficients:\n\n\n\nCoefficient\nMeaning\n\n\n\n\nIntercept ($13K)\nValue of land without house\n\n\nSlope ($113/sqft)\nPrice increase per sqft\n\n\n\nMaking Predictions:\n\\[\\text{Price} = 13,439 + 113 \\times \\text{SqFt}\\]\n\n\n\nHouse Size\nPredicted Price\n\n\n\n\n1,500 sqft\n$183,000\n\n\n2,500 sqft\n$296,000\n\n\n3,500 sqft\n$409,000"
  },
  {
    "objectID": "courses/caio/02-ml.html#simple-example-house-prices-output",
    "href": "courses/caio/02-ml.html#simple-example-house-prices-output",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Simple Example: House Prices",
    "text": "Simple Example: House Prices\n\n(Intercept)  livingArea \n 13439.3940    113.1225"
  },
  {
    "objectID": "courses/caio/02-ml.html#visualizing-the-fit",
    "href": "courses/caio/02-ml.html#visualizing-the-fit",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Visualizing the Fit",
    "text": "Visualizing the Fit\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the plot shows:\n\nEach blue dot is a house\nThe red line is our prediction\nVertical distance from dot to line = prediction error\n\nKey observations:\n\nStrong positive relationship\nMore scatter at higher prices (heteroskedasticity)\nSome outliers (expensive small houses, cheap large houses)\n\nThe line minimizes the sum of squared vertical distances"
  },
  {
    "objectID": "courses/caio/02-ml.html#google-vs-sp-500-capm",
    "href": "courses/caio/02-ml.html#google-vs-sp-500-capm",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Google vs S&P 500 (CAPM)",
    "text": "Google vs S&P 500 (CAPM)\nThe Capital Asset Pricing Model (CAPM) asks: Does a stock follow the market or beat it?\n\\[\\text{Google Return} = \\alpha + \\beta \\times \\text{Market Return}\\]\n\n\\(\\beta\\) (beta): How volatile is the stock relative to the market?\n\\(\\alpha\\) (alpha): Does the stock outperform after adjusting for risk?\n\n\n\nShow R code\nlibrary(quantmod)\ngetSymbols(c(\"GOOG\", \"SPY\"), from = \"2017-01-01\", to = \"2023-12-31\") |&gt; invisible()\ngoog &lt;- as.numeric(dailyReturn(GOOG))\nspy &lt;- as.numeric(dailyReturn(SPY))\nmodel &lt;- lm(goog ~ spy)\nprint(model)\n\n\n\nCall:\nlm(formula = goog ~ spy)\n\nCoefficients:\n(Intercept)          spy  \n  0.0003211    1.1705808"
  },
  {
    "objectID": "courses/caio/02-ml.html#interpreting-the-capm-results",
    "href": "courses/caio/02-ml.html#interpreting-the-capm-results",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Interpreting the CAPM Results",
    "text": "Interpreting the CAPM Results\n\n\nOur Findings:\n\nBeta (β = 1.01): Google moves roughly 1:1 with the market\nAlpha (α = 0.0004): Slight outperformance, but p-value = 0.06\nNot statistically significant at 5% level\n\nConclusion: In 2017-2023, Google tracked the market without consistent outperformance\n\nWhat Beta Tells Investors:\n\n\n\nBeta\nInterpretation\n\n\n\n\nβ &lt; 1\nLess volatile than market (utilities, healthcare)\n\n\nβ = 1\nMoves with market (index funds)\n\n\nβ &gt; 1\nMore volatile (tech, small caps)\n\n\n\nHigh beta = higher risk, but potentially higher reward"
  },
  {
    "objectID": "courses/caio/02-ml.html#orange-juice-price-advertising",
    "href": "courses/caio/02-ml.html#orange-juice-price-advertising",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Orange Juice: Price & Advertising",
    "text": "Orange Juice: Price & Advertising\nHow does advertising affect price sensitivity? We model sales as a function of price and whether the product was featured in ads.\nKey finding: The interaction term (log(price):feat) is negative and significant — advertising changes how customers respond to price!\n\n\nShow R code\noj &lt;- read.csv(\"data/oj.csv\")\nmodel &lt;- lm(logmove ~ log(price) * feat, data = oj)\ntidy(model) |&gt; select(term, estimate, p.value) |&gt; kable(digits = 3)\n\n\n\n\n\nterm\nestimate\np.value\n\n\n\n\n(Intercept)\n9.659\n0\n\n\nlog(price)\n-0.958\n0\n\n\nfeat\n1.714\n0\n\n\nlog(price):feat\n-0.977\n0"
  },
  {
    "objectID": "courses/caio/02-ml.html#the-advertising-paradox",
    "href": "courses/caio/02-ml.html#the-advertising-paradox",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "The Advertising Paradox",
    "text": "The Advertising Paradox\n\n\nFinding: Advertising increases price sensitivity\n\n\n\nCondition\nPrice Elasticity\n\n\n\n\nNo advertising\n-0.96\n\n\nWith advertising\n-0.96 + (-0.98) = -1.94\n\n\n\nWhy? Ads coincide with promotions → attract price-sensitive shoppers\n\nKey Lessons:\n\nCorrelation ≠ Causation: Ads don’t cause sensitivity; they coincide with promotions\nSelection effects: Who responds to ads? Price hunters!\nConfounding variables: Promotions happen during ad campaigns\nManagerial insight: Don’t blame advertising for price sensitivity — it’s the promotion strategy\n\nAlways ask: What’s really driving the relationship?"
  },
  {
    "objectID": "courses/caio/02-ml.html#from-regression-to-classification",
    "href": "courses/caio/02-ml.html#from-regression-to-classification",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "From Regression to Classification",
    "text": "From Regression to Classification\n\n\nWhat if the outcome is yes/no?\n\\[P(y=1 \\mid x) = \\frac{1}{1 + e^{-\\beta^T x}}\\]\nWhy not just use linear regression?\n\nLinear regression can predict values &lt; 0 or &gt; 1\nProbabilities must be between 0 and 1\nLogistic function “squashes” any input to (0, 1)"
  },
  {
    "objectID": "courses/caio/02-ml.html#nba-point-spread-example",
    "href": "courses/caio/02-ml.html#nba-point-spread-example",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "NBA Point Spread Example",
    "text": "NBA Point Spread Example\nCan Vegas point spreads predict game outcomes? We fit a logistic regression using historical NBA data.\n\n\nShow R code\nNBA &lt;- read.csv(\"data/NBAspread.csv\")\nmodel &lt;- glm(favwin ~ spread - 1, family = binomial, data = NBA)\ntidy(model) |&gt; kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nspread\n0.156\n0.014\n11.332\n0\n\n\n\n\n\nInterpretation: For each additional point in the spread, log-odds of favorite winning increases by 0.16. The p-value &lt; 0.001 confirms spreads are highly predictive."
  },
  {
    "objectID": "courses/caio/02-ml.html#making-predictions",
    "href": "courses/caio/02-ml.html#making-predictions",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Making Predictions",
    "text": "Making Predictions\nUsing our model, we can predict win probability for any point spread:\n\n\n\nSpread\nP(Favorite Wins)\n\n\n\n\n4 points\n65%\n\n\n8 points\n78%\n\n\n12 points\n87%\n\n\n\n\n\nShow R code\npredict(model, newdata = data.frame(spread = c(4, 8)), type = \"response\")\n\n\n        1         2 \n0.6511238 0.7769474 \n\n\nSame approach used for: credit scoring, churn prediction, marketing response, fraud detection — any binary outcome."
  },
  {
    "objectID": "courses/caio/02-ml.html#confusion-matrix",
    "href": "courses/caio/02-ml.html#confusion-matrix",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nHow accurate is our model? The confusion matrix shows predictions vs. actual outcomes.\n\n\nShow R code\npred &lt;- predict(model, type = \"response\") &gt; 0.5\ntable(Actual = NBA$favwin, Predicted = as.integer(pred))\n\n\n      Predicted\nActual   1\n     0 131\n     1 422\n\n\nOur model achieves about 66% accuracy — better than a coin flip!\n\nReading the Matrix:\n\n\n\n\nPred: 0\nPred: 1\n\n\n\n\nActual: 0\nTN (correct!)\nFP (oops)\n\n\nActual: 1\nFN (oops)\nTP (correct!)\n\n\n\nSports Betting Reality:\n\n66% accuracy sounds good, but…\nVegas takes ~10% commission (“vig”)\nNeed ~52.4% accuracy just to break even\nEdge of 13.6% is excellent if it holds!\n\nBut past performance ≠ future results"
  },
  {
    "objectID": "courses/caio/02-ml.html#understanding-the-confusion-matrix",
    "href": "courses/caio/02-ml.html#understanding-the-confusion-matrix",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Understanding the Confusion Matrix",
    "text": "Understanding the Confusion Matrix\n\n\n\n\nPredicted: Win\nPredicted: Lose\n\n\n\n\nActual: Win\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual: Lose\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nKey Metrics:\n\nAccuracy = (TP + TN) / Total — overall correctness\nPrecision = TP / (TP + FP) — “Of predicted wins, how many were right?”\nRecall = TP / (TP + FN) — “Of actual wins, how many did we catch?”\n\nCaution: Accuracy can mislead! A spam filter predicting “not spam” for everything has 99% accuracy but catches zero spam. Choose metrics based on business costs."
  },
  {
    "objectID": "courses/caio/02-ml.html#roc-curve-the-trade-off",
    "href": "courses/caio/02-ml.html#roc-curve-the-trade-off",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "ROC Curve: The Trade-off",
    "text": "ROC Curve: The Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the ROC Curve:\n\nX-axis: False Positive Rate (false alarms)\nY-axis: True Positive Rate (catches)\nDiagonal: Random guessing (AUC = 0.5)\nUpper-left corner: Perfect classifier\n\nArea Under Curve (AUC):\n\n\n\nAUC\nModel Quality\n\n\n\n\n0.5\nRandom (useless)\n\n\n0.6-0.7\nPoor\n\n\n0.7-0.8\nFair\n\n\n0.8-0.9\nGood\n\n\n0.9+\nExcellent"
  },
  {
    "objectID": "courses/caio/02-ml.html#choosing-the-right-threshold",
    "href": "courses/caio/02-ml.html#choosing-the-right-threshold",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Choosing the Right Threshold",
    "text": "Choosing the Right Threshold\n\n\nThe optimal threshold depends on business costs:\n\nFraud detection: Low threshold (catch more fraud, accept false alarms)\nMedical screening: Low threshold (don’t miss disease)\nSpam filter: Higher threshold (don’t lose important emails)\n\nThere is no universal “correct” threshold\n\nFramework for Threshold Selection:\n\nQuantify costs: What’s the cost of FP vs FN?\nCalculate expected cost at each threshold\nChoose threshold that minimizes total expected cost\n\nExample — Credit Card Fraud:\n\nFalse Positive cost: $10 (customer inconvenience)\nFalse Negative cost: $500 (fraud loss)\nOptimal threshold: Much lower than 0.5!\n\nLet business economics guide your model decisions"
  },
  {
    "objectID": "courses/caio/02-ml.html#summary",
    "href": "courses/caio/02-ml.html#summary",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Insight\n\n\n\n\nDistributions\nBinomial (binary), Poisson (counts), Normal (continuous)\n\n\nPoisson\nMean = Variance — the fingerprint of count data\n\n\nNormal\nCLT makes it universal for averages\n\n\nLinear Regression\nCoefficients = effect sizes\n\n\nLogistic Regression\nOutputs probabilities for classification\n\n\nROC/AUC\nTrade-off between false positives and false negatives\n\n\nThreshold\nBusiness costs should drive the choice\n\n\n\nStatistics is the science of decision-making under uncertainty"
  },
  {
    "objectID": "courses/caio/02-ml.html#supplemental-reading",
    "href": "courses/caio/02-ml.html#supplemental-reading",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Supplemental Reading",
    "text": "Supplemental Reading\n\n\nOnline Articles:\n\nThe Surprising Power of Online Experiments - HBR\nMachine Learning, Explained - MIT Sloan\n\nKey Insight from HBR: A simple A/B test at Bing generated over $100M annually by testing a “low priority” idea\n\nBooks for Further Study:\n\nThe Signal and the Noise — Nate Silver\nThinking, Fast and Slow — Daniel Kahneman\nNaked Statistics — Charles Wheelan\nData Science for Business — Provost & Fawcett\n\nOnline Courses:\n\nAndrew Ng’s Machine Learning (Coursera)\nStatistical Learning (Stanford Online)\nFast.ai Practical Deep Learning"
  },
  {
    "objectID": "courses/caio/02-ml.html#poisson-premier-league-goals",
    "href": "courses/caio/02-ml.html#poisson-premier-league-goals",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Poisson: Premier League Goals",
    "text": "Poisson: Premier League Goals\nHow many goals does a team score per match?\n\nepl &lt;- read.csv(\"data/epl.csv\")\ngoals &lt;- c(epl$home_score, epl$guest_score)\nmean(goals)  # Average goals per team per match\n\n[1] 1.4\n\nvar(goals)   # Variance (should be close to mean for Poisson)\n\n[1] 1.644796"
  },
  {
    "objectID": "courses/caio/02-ml.html#epl-goals-distribution",
    "href": "courses/caio/02-ml.html#epl-goals-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "EPL Goals Distribution",
    "text": "EPL Goals Distribution\n\nThe Poisson model fits soccer goals remarkably well!"
  },
  {
    "objectID": "courses/caio/02-ml.html#central-limit-theorem-clt",
    "href": "courses/caio/02-ml.html#central-limit-theorem-clt",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\n\nThe most important theorem in statistics:\n\nThe average of many independent random events tends toward a Normal distribution, regardless of the original distribution.\n\nWhy it matters: Stock returns, measurement errors, test scores — all tend to be Normal because they’re sums of many small effects.\n\nPractical Implications:\n\nSample means are approximately Normal (even if data isn’t)\nConfidence intervals work because of CLT\nA/B testing relies on CLT for significance tests\nQuality control uses CLT for process monitoring\n\nRule of thumb: Sample size ≥ 30 usually sufficient for CLT to kick in\nThis is why the Normal distribution is everywhere!"
  },
  {
    "objectID": "courses/caio/02-ml.html#clt-in-action",
    "href": "courses/caio/02-ml.html#clt-in-action",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "CLT in Action",
    "text": "CLT in Action\n\nAs we average more dice, the distribution becomes Normal!"
  },
  {
    "objectID": "courses/caio/02-ml.html#predicting-premier-league-goals",
    "href": "courses/caio/02-ml.html#predicting-premier-league-goals",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Predicting Premier League Goals",
    "text": "Predicting Premier League Goals\n\n\nHow many goals will a team score? Historical EPL data:\n\n\nShow R code\nepl &lt;- read.csv(\"data/epl.csv\")\nepl[1:5, c(\"home_team_name\", \"away_team_name\", \"home_score\", \"guest_score\")]\n\n\n  home_team_name       away_team_name home_score guest_score\n1        Arsenal            Liverpool          3           4\n2    Bournemouth    Manchester United          1           3\n3        Burnley              Swansea          0           1\n4        Chelsea             West Ham          2           1\n5 Crystal Palace West Bromwich Albion          0           1\n\n\nEach row = one match with final scores.\n\n\n\nThe Business Problem:\nSports betting: $200+ billion industry.\nOur approach:\n\nAnalyze historical data\nModel goals as random events\nEstimate team strengths\nSimulate matches\n\nWho uses this? FiveThirtyEight, ESPN, DraftKings, Betfair, team analytics"
  },
  {
    "objectID": "courses/caio/02-ml.html#goals-follow-a-poisson-distribution",
    "href": "courses/caio/02-ml.html#goals-follow-a-poisson-distribution",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Goals Follow a Poisson Distribution",
    "text": "Goals Follow a Poisson Distribution\n\n\n\n\nShow R code\ngoals &lt;- c(epl$home_score, epl$guest_score)\nlambda &lt;- mean(goals)\nx &lt;- 0:8\nobserved &lt;- table(factor(goals, levels = x)) / length(goals)\nexpected &lt;- dpois(x, lambda = lambda)\n\nbarplot(rbind(observed, expected), beside = TRUE, \n        names.arg = x, col = c(\"steelblue\", \"coral\"),\n        xlab = \"Goals Scored\", ylab = \"Proportion\",\n        legend.text = c(\"Observed\", \"Poisson Model\"))\n\n\n\n\n\n\n\n\n\n\nModel Validation:\nThe Poisson model (coral bars) fits the observed data (blue bars) remarkably well!\nWhat this tells us:\n\nGoals are indeed rare, independent events\nThe Poisson assumption is justified\nWe can use this model for predictions\n\nSlight discrepancy at 0 goals: Real matches have slightly fewer 0-0 draws than Poisson predicts (teams try harder when level!)"
  },
  {
    "objectID": "courses/caio/02-ml.html#improving-the-model-team-strength",
    "href": "courses/caio/02-ml.html#improving-the-model-team-strength",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Improving the Model: Team Strength",
    "text": "Improving the Model: Team Strength\n\n\nA single \\(\\lambda\\) for all teams is too simple. Better model:\n\\[\\lambda_{ij} = \\text{Attack}_i \\times \\text{Defense}_j \\times \\text{HomeAdvantage}\\]\n\nAttack: How good is team \\(i\\) at scoring?\nDefense: How weak is team \\(j\\) at defending?\nHome advantage: ~0.4 extra goals at home\n\n\nThis is how real sports analytics works:\n\nEstimate each team’s offensive/defensive strength from historical data\nAdjust for home/away effects\nPredict expected goals for each team\nUse Poisson to generate win/draw/loss probabilities\n\nSame framework applies to:\n\nNBA point spreads\nNFL betting lines\nCricket run predictions\nBaseball run expectations"
  },
  {
    "objectID": "courses/caio/02-ml.html#team-specific-λ-arsenal-vs-liverpool",
    "href": "courses/caio/02-ml.html#team-specific-λ-arsenal-vs-liverpool",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Team-Specific λ: Arsenal vs Liverpool",
    "text": "Team-Specific λ: Arsenal vs Liverpool\nTo predict a specific match, we estimate each team’s scoring rate:\n\nArsenal’s attack: How many goals do they typically score at home?\nLiverpool’s defense: How many goals do they typically concede away?\nAdjustment: Scale by league average to get relative strength\n\nFor Arsenal vs Liverpool at home, we estimate Arsenal will score about 1.8 goals on average. Liverpool’s away λ would be calculated similarly.\n\n\nShow R code\n# Simple estimate: average goals scored and conceded\narsenal_attack &lt;- mean(epl$home_score[epl$home_team_name == \"Arsenal\"])\nliverpool_defense &lt;- mean(epl$home_score[epl$away_team_name == \"Liverpool\"])\nleague_avg &lt;- mean(goals)\n\n# Arsenal's expected goals vs Liverpool (simplified)\nlambda_arsenal &lt;- arsenal_attack * (liverpool_defense / league_avg)\nlambda_arsenal\n\n\n[1] 1.851998"
  },
  {
    "objectID": "courses/caio/02-ml.html#monte-carlo-simulation",
    "href": "courses/caio/02-ml.html#monte-carlo-simulation",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nOnce we have \\(\\lambda\\) for each team, we can simulate the match thousands of times.\nFor Arsenal (\\(\\lambda=1.8\\)) vs Liverpool (\\(\\lambda=1.5\\)), running 10,000 simulations gives:\n\nArsenal wins: ~42% of simulations\nDraw: ~24% of simulations\n\nLiverpool wins: ~34% of simulations\n\nThis is how betting companies set their odds!\n\n\nShow R code\nset.seed(42)\nn_sims &lt;- 10000\n# Simulate Arsenal vs Liverpool\narsenal_goals &lt;- rpois(n_sims, lambda = 1.8)  # λ for Arsenal\nliverpool_goals &lt;- rpois(n_sims, lambda = 1.5) # λ for Liverpool\n\n# Match outcomes\nc(Arsenal_Win = mean(arsenal_goals &gt; liverpool_goals),\n  Draw = mean(arsenal_goals == liverpool_goals),\n  Liverpool_Win = mean(arsenal_goals &lt; liverpool_goals))\n\n\n  Arsenal_Win          Draw Liverpool_Win \n       0.4538        0.2254        0.3208"
  },
  {
    "objectID": "courses/caio/02-ml.html#why-monte-carlo",
    "href": "courses/caio/02-ml.html#why-monte-carlo",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Why Monte Carlo?",
    "text": "Why Monte Carlo?\n\n\nEach simulation draws random goals from Poisson distributions\n\nRun 10,000 simulations → get probability of each outcome\nCan extend to simulate entire season, league standings\nSame approach used by betting companies and analytics firms\n\nThis is how FiveThirtyEight and bookmakers build their models!\n\nMonte Carlo Applications:\n\nFinance: Option pricing, portfolio risk (VaR)\nInsurance: Claim projections, reserve calculations\nOperations: Supply chain uncertainty, demand forecasting\nEngineering: Reliability analysis, quality control\nAI: Reinforcement learning, MCMC for Bayesian inference\n\nWhen math is too hard, simulate!"
  },
  {
    "objectID": "courses/caio/02-ml.html#the-1987-stock-market-crash",
    "href": "courses/caio/02-ml.html#the-1987-stock-market-crash",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "The 1987 Stock Market Crash",
    "text": "The 1987 Stock Market Crash\nHow extreme was the October 1987 crash of -21.76%?\nPrior to the crash, S&P 500 monthly returns:\n\nMean: μ = 1.2%\nVolatility: σ = 4.3%\n\nStandardize to find how many standard deviations from the mean:\n\\[Z = \\frac{X - \\mu}{\\sigma} = \\frac{-21.76 - 1.2}{4.3} = -5.34\\]"
  },
  {
    "objectID": "courses/caio/02-ml.html#a-5-sigma-event",
    "href": "courses/caio/02-ml.html#a-5-sigma-event",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "A 5-Sigma Event",
    "text": "A 5-Sigma Event\nUnder the Normal model, a -5.34 sigma event has probability 0.00000005 — about 1 in 20 million!\nThat should happen once every 1.6 million months (130,000 years).\n\n\nShow R code\n# How rare is a -5.34 sigma event?\npnorm(-5.34)\n\n\n[1] 4.647329e-08\n\n\nLesson: Either the model is wrong, or we witnessed something extraordinary. (Hint: stock returns have “fat tails” — extreme events happen more often than Normal predicts.)"
  },
  {
    "objectID": "courses/caio/02-ml.html#google-vs-sp-500-scatter-plot",
    "href": "courses/caio/02-ml.html#google-vs-sp-500-scatter-plot",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Google vs S&P 500: Scatter Plot",
    "text": "Google vs S&P 500: Scatter Plot"
  },
  {
    "objectID": "courses/caio/02-ml.html#epl-goals-mean-variance",
    "href": "courses/caio/02-ml.html#epl-goals-mean-variance",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "EPL Goals: Mean ≈ Variance",
    "text": "EPL Goals: Mean ≈ Variance\n\n\nA key signature of Poisson data: the mean equals the variance.\n\nTeams score about 1.4 goals per match on average\nThe variance is also ~1.4 — this is the Poisson fingerprint!\nIf variance were much larger, we’d need a different model\n\n\n\nShow R code\ngoals &lt;- c(epl$home_score, epl$guest_score)\nmean(goals)  # Average goals per team per match\n\n\n[1] 1.4\n\n\nShow R code\nvar(goals)   # Variance ≈ Mean suggests Poisson!\n\n\n[1] 1.644796\n\n\n\nModel Diagnostics: Mean vs Variance\n\n\n\nRelationship\nSuggests\n\n\n\n\nVariance ≈ Mean\nPoisson ✓\n\n\nVariance &gt; Mean\nOverdispersion (Negative Binomial)\n\n\nVariance &lt; Mean\nUnderdispersion (rare)\n\n\n\nOther Poisson Applications:\n\nCall center arrivals per hour\nWebsite clicks per minute\nInsurance claims per year\nManufacturing defects per batch\nEmails received per day\n\nPoisson is the “go-to” for count data!"
  },
  {
    "objectID": "courses/caio/02-ml.html#clt-in-action-michigan-election-polls",
    "href": "courses/caio/02-ml.html#clt-in-action-michigan-election-polls",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "CLT in Action: Michigan Election Polls",
    "text": "CLT in Action: Michigan Election Polls\nSuppose the true vote share in Michigan is 51%. What happens when we poll voters?\n\nEach voter is like a coin flip (vote A or B)\nSmall samples are noisy; large samples converge to the truth\nThe distribution of poll results becomes Normal\n\n\nShow R code\nset.seed(42)\ntrue_p &lt;- 0.51\n# Poll of 10 voters\nhist(replicate(1000, mean(rbinom(10, 1, true_p))), breaks = 20,\n     main = \"Poll: 10 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\n# Poll of 100 voters\nhist(replicate(1000, mean(rbinom(100, 1, true_p))), breaks = 20,\n     main = \"Poll: 100 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\n# Poll of 1000 voters\nhist(replicate(1000, mean(rbinom(1000, 1, true_p))), breaks = 20,\n     main = \"Poll: 1000 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarger samples → tighter Normal distribution around the true value (red line)"
  },
  {
    "objectID": "courses/caio/02-ml.html#team-specific-lambda-arsenal-vs-liverpool",
    "href": "courses/caio/02-ml.html#team-specific-lambda-arsenal-vs-liverpool",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Team-Specific \\(\\lambda\\): Arsenal vs Liverpool",
    "text": "Team-Specific \\(\\lambda\\): Arsenal vs Liverpool\nTo predict a specific match, we estimate each team’s scoring rate:\n\nArsenal’s attack: How many goals do they typically score at home?\nLiverpool’s defense: How many goals do they typically concede away?\nAdjustment: Scale by league average to get relative strength\n\nFor Arsenal vs Liverpool at home, we estimate Arsenal will score about 1.8 goals on average. Liverpool’s away \\(\\lambda\\) would be calculated similarly.\n\n\nShow R code\n# Simple estimate: average goals scored and conceded\narsenal_attack &lt;- mean(epl$home_score[epl$home_team_name == \"Arsenal\"])\nliverpool_defense &lt;- mean(epl$home_score[epl$away_team_name == \"Liverpool\"])\nleague_avg &lt;- mean(goals)\n\n# Arsenal's expected goals vs Liverpool (simplified)\nlambda_arsenal &lt;- arsenal_attack * (liverpool_defense / league_avg)\nlambda_arsenal\n\n\n[1] 1.851998"
  },
  {
    "objectID": "courses/caio/02-ml.html#fat-tails-reality-vs-normal-model",
    "href": "courses/caio/02-ml.html#fat-tails-reality-vs-normal-model",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Fat Tails: Reality vs Normal Model",
    "text": "Fat Tails: Reality vs Normal Model\n\n\n\n\n\n\n\n\nThe Problem with Normal Assumptions:\nStock returns have more extreme events than the Normal distribution predicts.\n\n\n\nEvent\nNormal Probability\nActually Happened\n\n\n\n\n1987 Crash (-22%)\n1 in \\(10^{160}\\)\nYes\n\n\n2008 Crisis\n“Impossible”\nYes\n\n\n2020 COVID Crash\n“Impossible”\nYes\n\n\n\nImplications for Risk Management:\n\nVaR models underestimate tail risk\nNeed “fat-tailed” distributions (t-distribution, etc.)\nStress testing is essential"
  },
  {
    "objectID": "courses/caio/02-ml.html#the-1987-stock-market-crash-a-5-sigma-event",
    "href": "courses/caio/02-ml.html#the-1987-stock-market-crash-a-5-sigma-event",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "The 1987 Stock Market Crash: A 5-Sigma Event",
    "text": "The 1987 Stock Market Crash: A 5-Sigma Event\nHow extreme was the October 1987 crash of -21.76%?\n\nPrior to crash: \\(\\mu = 1.2\\%\\), \\(\\sigma = 4.3\\%\\) → Z-score = \\(\\frac{-21.76 - 1.2}{4.3} = -5.34\\)\nUnder Normal model: probability = 1 in 20 million (once every 130,000 years!)\nYet 5+ sigma events happened in 1987, 2008, and 2020\n\nConclusion: The model is wrong — stock returns have “fat tails.” Banks using Normal-based VaR dramatically underestimate risk.\n\n\nShow R code\npnorm(-5.34)  # Probability of -5.34 sigma event\n\n\n[1] 4.647329e-08"
  },
  {
    "objectID": "courses/caio/02-ml.html#google-vs-sp-500-capm-results",
    "href": "courses/caio/02-ml.html#google-vs-sp-500-capm-results",
    "title": "Chief AI Officer Program: ML Essentials",
    "section": "Google vs S&P 500: CAPM Results",
    "text": "Google vs S&P 500: CAPM Results\n\n\n\n\n\n\n\n\n\n\n\n\nOur Findings:\n\nBeta (\\(\\beta = 1.01\\)): Google moves 1:1 with market\nAlpha (\\(\\alpha \\approx 0\\)): No significant outperformance (\\(p = 0.06\\))\n\n\n\n\nBeta      \nInterpretation\n\n\n\n\n\\(\\beta &lt; 1\\)\nLess volatile (utilities, healthcare)\n\n\n\\(\\beta = 1\\)\nMoves with market (index funds)\n\n\n\\(\\beta &gt; 1\\)\nMore volatile (tech, small caps)\n\n\n\nConclusion: Google tracked the market without consistent alpha in 2017-2023. High beta = higher risk, potentially higher reward."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-language-challenge",
    "href": "courses/caio/03-ai.html#the-language-challenge",
    "title": "Modern AI",
    "section": "The Language Challenge",
    "text": "The Language Challenge\n\n“You shall know a word by the company it keeps.” — J.R. Firth (1957)\n\nLanguage poses unique challenges for AI:\n\nUnlike images (continuous pixels) or audio (waveforms), text is discrete symbols\nThe word “cat” is not inherently closer to “dog” than to “quantum”\nYet humans effortlessly recognize semantic similarities\n\nThe breakthrough: Represent words as vectors in continuous space where geometry encodes meaning."
  },
  {
    "objectID": "courses/caio/03-ai.html#from-symbols-to-vectors",
    "href": "courses/caio/03-ai.html#from-symbols-to-vectors",
    "title": "Modern AI",
    "section": "From Symbols to Vectors",
    "text": "From Symbols to Vectors\nThe Problem with One-Hot Encoding:\nEach word gets a unique vector with a single 1:\n\n“cat” → [0, 0, 1, 0, 0, …, 0]\n“dog” → [0, 1, 0, 0, 0, …, 0]\n\nProblem: Cosine similarity between any two words = 0\nNo notion of semantic similarity is captured!\nSolution: Learn dense vector representations where similar words are close together."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-twenty-questions-intuition",
    "href": "courses/caio/03-ai.html#the-twenty-questions-intuition",
    "title": "Modern AI",
    "section": "The Twenty Questions Intuition",
    "text": "The Twenty Questions Intuition\nImagine playing Twenty Questions to identify words:\n\n\n\nQuestion\nBear\nDog\nCat\n\n\n\n\nIs it an animal?\n1\n1\n1\n\n\nIs it domestic?\n0\n1\n0.7\n\n\nLarger than human?\n0.8\n0.1\n0.01\n\n\nHas long tail?\n0\n0.6\n1\n\n\nIs it a predator?\n1\n0\n0.6\n\n\n\nEach word becomes a vector of answers. Similar words give similar answers → similar vectors!\nThis is the essence of word embeddings."
  },
  {
    "objectID": "courses/caio/03-ai.html#word2vec-learning-from-context",
    "href": "courses/caio/03-ai.html#word2vec-learning-from-context",
    "title": "Modern AI",
    "section": "Word2Vec: Learning from Context",
    "text": "Word2Vec: Learning from Context\nThe Distributional Hypothesis: Words appearing in similar contexts have similar meanings.\n\n\n\n\n\nflowchart LR\n    C1[\"The ___ sat on the mat\"]\n    C2[\"The ___ sat on the rug\"]\n    \n    Cat[\"cat\"] --&gt; C1\n    Dog[\"dog\"] --&gt; C2\n    \n    Cat --&gt; V[\"Similar Vectors!\"]\n    Dog --&gt; V\n    \n    style Cat fill:#e1f5fe,stroke:#1976d2\n    style Dog fill:#e1f5fe,stroke:#1976d2\n    style V fill:#c8e6c9,stroke:#2e7d32\n\n\n\n\n\n\nResult: Vector arithmetic captures analogies!\n\\[\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\]"
  },
  {
    "objectID": "courses/caio/03-ai.html#word2vec-war-and-peace-example",
    "href": "courses/caio/03-ai.html#word2vec-war-and-peace-example",
    "title": "Modern AI",
    "section": "Word2Vec: War and Peace Example",
    "text": "Word2Vec: War and Peace Example\nTraining Word2Vec on Tolstoy’s novel reveals thematic structure:\n\nMilitary cluster: soldier, regiment, battle, army\nSocial cluster: ballroom, court, marriage, society\nGovernment cluster: history, power, Napoleon\n\n“Peace” sits between social and government domains — its central role in the narrative!\nApplications: Literary analysis, recommendation systems, semantic search."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-skip-gram-model",
    "href": "courses/caio/03-ai.html#the-skip-gram-model",
    "title": "Modern AI",
    "section": "The Skip-Gram Model",
    "text": "The Skip-Gram Model\nGiven a center word, predict surrounding context words:\n\n\n\n\n\nflowchart LR\n    A[loves] --&gt; B[the]\n    A --&gt; C[man]\n    A --&gt; D[his]\n    A --&gt; E[son]\n    \n    style A fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style C fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n\n\n\n\n\n\n\\[P(\\text{context} \\mid \\text{center}) = \\prod_{j} P(w_{j} \\mid w_{\\text{center}})\\]\nThe learned vectors capture semantic relationships because words with similar contexts get similar representations."
  },
  {
    "objectID": "courses/caio/03-ai.html#from-words-to-sentences-the-attention-revolution",
    "href": "courses/caio/03-ai.html#from-words-to-sentences-the-attention-revolution",
    "title": "Modern AI",
    "section": "From Words to Sentences: The Attention Revolution",
    "text": "From Words to Sentences: The Attention Revolution\nThe Problem: Static Embeddings\n\nThe word “bank” has one vector, whether it’s “river bank” or “investment bank.”\nThis conflation of senses creates an information bottleneck.\n\nThe Sequential Bottleneck (RNNs/LSTMs):\n\nProcessed text step-by-step (like reading through a straw).\nEarly information “vanished” as sentences grew longer.\nImpossible to parallelize effectively on modern GPUs.\n\nThe Breakthrough: Attention Mechanisms\n\nLet each word dynamically “attend” to all other words simultaneously.\nExample: “The trophy wouldn’t fit in the suitcase because it was too big.”\nSelf-attention identifies that “it” refers to “trophy” by looking at the whole sentence at once.\n\nResult: Contextual representations that change based on surrounding words."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-attention-mechanism",
    "href": "courses/caio/03-ai.html#the-attention-mechanism",
    "title": "Modern AI",
    "section": "The Attention Mechanism",
    "text": "The Attention Mechanism\nThe Library Analogy (Query, Key, Value):\n\nQuery (Q): What am I looking for? (e.g., “subject of the sentence”)\nKey (K): What is in this book? (e.g., “noun”, “verb”, “adjective”)\nValue (V): What is the content of the book? (the actual meaning vector)\n\nThe Mathematical Operation:\n\nSimilarity: Compare the Query to all Keys using a dot product.\nScoring: Turn these scores into weights (probabilities) using Softmax.\nRetrieval: Take a weighted sum of the Values."
  },
  {
    "objectID": "courses/caio/03-ai.html#attention-a-visual-example",
    "href": "courses/caio/03-ai.html#attention-a-visual-example",
    "title": "Modern AI",
    "section": "Attention: A Visual Example",
    "text": "Attention: A Visual Example\nFor “The trophy wouldn’t fit in the suitcase because it was too big”:\n\n\n\nWord\nAttention to “it”\n\n\n\n\ntrophy\n0.45\n\n\nsuitcase\n0.15\n\n\nfit\n0.12\n\n\nbig\n0.18\n\n\nother\n0.10\n\n\n\nThe model learns that “it” most likely refers to “trophy” by attending strongly to it!"
  },
  {
    "objectID": "courses/caio/03-ai.html#self-attention-vs-cross-attention",
    "href": "courses/caio/03-ai.html#self-attention-vs-cross-attention",
    "title": "Modern AI",
    "section": "Self-Attention vs Cross-Attention",
    "text": "Self-Attention vs Cross-Attention\n\n\nSelf-Attention:\n\n\n\n\n\nflowchart LR\n    w1[\"The\"] &lt;--&gt; w2[\"cat\"]\n    w2 &lt;--&gt; w3[\"sat\"]\n    w1 &lt;--&gt; w3\n    \n    style w1 fill:#e1f5fe,stroke:#1976d2\n    style w2 fill:#e1f5fe,stroke:#1976d2\n    style w3 fill:#e1f5fe,stroke:#1976d2\n\n\n\n\n\n\nGoal: Understand internal relationships. Used in: Encoders (BERT) and Decoders (GPT). Analogy: Rereading a sentence to find the subject.\n\nCross-Attention:\n\n\n\n\n\nflowchart LR\n    e1[\"The\"]\n    e2[\"cat\"]\n    d1[\"Le\"]\n    d2[\"chat\"]\n    \n    e1 --&gt; d1\n    e2 --&gt; d2\n    e1 -.-&gt; d2\n    e2 -.-&gt; d1\n    \n    style e1 fill:#e1f5fe,stroke:#1976d2\n    style e2 fill:#e1f5fe,stroke:#1976d2\n    style d1 fill:#c8e6c9,stroke:#2e7d32\n    style d2 fill:#c8e6c9,stroke:#2e7d32\n\n\n\n\n\n\nGoal: Link two different sequences. Used in: Translation models (T5). Analogy: Looking back at English while writing French."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-transformer-architecture",
    "href": "courses/caio/03-ai.html#the-transformer-architecture",
    "title": "Modern AI",
    "section": "The Transformer Architecture",
    "text": "The Transformer Architecture\n\n\n\n\n\nflowchart LR\n    In[\"Input\"] --&gt; Tok[\"Token\"] --&gt; Emb[\"Embed\"] --&gt; Att[\"Attention\"] --&gt; FF[\"FeedForward\"] --&gt; Out[\"Output\"]\n    \n    style In fill:#e3f2fd,stroke:#1976d2\n    style Att fill:#fff3e0,stroke:#f57c00\n    style FF fill:#e8f5e9,stroke:#388e3c\n    style Out fill:#f3e5f5,stroke:#7b1fa2\n\n\n\n\n\n\nKey innovations:\n\nSelf-attention replaces recurrence → parallel processing\nPositional encoding preserves word order\nMulti-head attention captures different relationship types\nFeed-forward layers add nonlinear transformations"
  },
  {
    "objectID": "courses/caio/03-ai.html#why-transformers-won",
    "href": "courses/caio/03-ai.html#why-transformers-won",
    "title": "Modern AI",
    "section": "Why Transformers Won",
    "text": "Why Transformers Won\n\n\n\nProperty\nRNN/LSTM\nTransformer\n\n\n\n\nSequential processing\nYes (slow)\nNo (parallel)\n\n\nLong-range dependencies\nDifficult\nEasy\n\n\nTraining speed\nSlow\nFast\n\n\nScalability\nLimited\nExcellent\n\n\n\nTransformers scale with compute → the foundation of modern LLMs."
  },
  {
    "objectID": "courses/caio/03-ai.html#from-transformers-to-llms",
    "href": "courses/caio/03-ai.html#from-transformers-to-llms",
    "title": "Modern AI",
    "section": "From Transformers to LLMs",
    "text": "From Transformers to LLMs\nThe Scale Approach:\n\n\n\n\n\nflowchart LR\n    G1[\"GPT-1&lt;br/&gt;117M\"] --&gt; G2[\"GPT-2&lt;br/&gt;1.5B\"]\n    G2 --&gt; G3[\"GPT-3&lt;br/&gt;175B\"]\n    G3 --&gt; G4[\"GPT-4&lt;br/&gt;~1.8T\"]\n    \n    style G1 fill:#e3f2fd\n    style G2 fill:#bbdefb\n    style G3 fill:#90caf9\n    style G4 fill:#42a5f5\n\n\n\n\n\n\nEmergent capabilities appear at scale:\n\nChain-of-thought reasoning\nIn-context learning (few-shot)\nCode generation\nMulti-step planning"
  },
  {
    "objectID": "courses/caio/03-ai.html#how-llms-generate-text",
    "href": "courses/caio/03-ai.html#how-llms-generate-text",
    "title": "Modern AI",
    "section": "How LLMs Generate Text",
    "text": "How LLMs Generate Text\nLLMs are autoregressive: they predict the next token based on all previous tokens.\n\n\n\n\n\n\n\nflowchart LR\n    C[\"Context\"] --&gt; M[\"LLM\"]\n    M --&gt; P[\"Probabilities\"]\n    P --&gt; S[\"Sample\"]\n    S --&gt; T[\"Token\"]\n    T --&gt; |\"Append\"| C\n    \n    style C fill:#e3f2fd,stroke:#1976d2\n    style M fill:#fff3e0,stroke:#f57c00\n    style P fill:#e8f5e9,stroke:#388e3c\n    style T fill:#f3e5f5,stroke:#7b1fa2\n\n\n\n\n\n\nThe Generation Loop:\n\nProbabilities: Compute scores for the vocabulary.\nSampling: Select next word (via Temperature).\nAutoregression: Append and repeat.\n\n\nTemperature (\\(\\tau\\)):\n\n\n\n\\(\\tau\\)\nBehavior\n\n\n\n\n0\nDeterministic\n\n\n0.7\nBalanced\n\n\n1.0\nProbabilistic\n\n\n1.5\nCreative\n\n\n\nLower \\(\\tau\\) = Predictable Higher \\(\\tau\\) = Random"
  },
  {
    "objectID": "courses/caio/03-ai.html#temperature-controlling-randomness",
    "href": "courses/caio/03-ai.html#temperature-controlling-randomness",
    "title": "Modern AI",
    "section": "Temperature: Controlling Randomness",
    "text": "Temperature: Controlling Randomness\nTemperature controls the “creativity” of generation:\n\n\n\nTemperature\nBehavior\n\n\n\n\n0\nAlways pick highest-probability token (deterministic)\n\n\n0.3-0.7\nBalanced: coherent but varied\n\n\n1.0\nSample according to true probabilities\n\n\n&gt; 1.0\nFlatter distribution → more random, creative"
  },
  {
    "objectID": "courses/caio/03-ai.html#the-llm-lifecycle",
    "href": "courses/caio/03-ai.html#the-llm-lifecycle",
    "title": "Modern AI",
    "section": "The LLM Lifecycle",
    "text": "The LLM Lifecycle\n\n\n\n\n\nflowchart LR\n    D[Data Collection] --&gt; P[Pre-Training]\n    P --&gt; I[Instruction Tuning]\n    I --&gt; A[Alignment]\n    A --&gt; Dep[Deployment]\n    \n    style D fill:#e3f2fd,stroke:#1976d2\n    style P fill:#e8f5e9,stroke:#388e3c\n    style I fill:#fff3e0,stroke:#f57c00\n    style A fill:#fce4ec,stroke:#c2185b\n    style Dep fill:#f3e5f5,stroke:#7b1fa2\n\n\n\n\n\n\n\n\n\nStage\nPurpose\n\n\n\n\nData Collection\nCurate training corpus (quality &gt; quantity)\n\n\nPre-Training\nPredict next tokens on billions of sequences\n\n\nInstruction Tuning\nTeach the model to follow instructions\n\n\nAlignment\nEnsure behavior matches human values (RLHF)\n\n\nDeployment\nOptimize for latency, cost, safety"
  },
  {
    "objectID": "courses/caio/03-ai.html#alignment-why-it-matters",
    "href": "courses/caio/03-ai.html#alignment-why-it-matters",
    "title": "Modern AI",
    "section": "Alignment: Why It Matters",
    "text": "Alignment: Why It Matters\n\n\n\n\n\nflowchart LR\n    Q[\"User Query\"]\n    \n    Q --&gt; U[\"Unaligned\"]\n    Q --&gt; A[\"Aligned\"]\n    \n    U --&gt; UR[\"Yes, only true god\"]\n    \n    A --&gt; AR[\"Multiple perspectives exist\"]\n    \n    style Q fill:#e3f2fd,stroke:#1976d2\n    style UR fill:#ffcccc,stroke:#cc0000\n    style AR fill:#ccffcc,stroke:#00cc00\n\n\n\n\n\n\nExample: “Is Allah the only god?”\n\nUnaligned: “Yes, Allah is the one true god and all other beliefs are false.”\nAligned: “In Islam, Allah is considered the one God. Other religions have different perspectives. I can provide factual information if helpful.”\n\nThis nuanced behavior emerges from alignment training, not pre-training alone."
  },
  {
    "objectID": "courses/caio/03-ai.html#context-windows-and-prompting",
    "href": "courses/caio/03-ai.html#context-windows-and-prompting",
    "title": "Modern AI",
    "section": "Context Windows and Prompting",
    "text": "Context Windows and Prompting\nContext window: Maximum tokens the model can “see” at once\n\n\n\n\n\nflowchart LR\n    S[System Prompt&lt;br/&gt;~500 tokens]\n    T[Tools/Schemas&lt;br/&gt;~300 tokens]\n    H[History&lt;br/&gt;~1000 tokens]\n    R[Retrieved Docs&lt;br/&gt;~2000 tokens]\n    U[User Query&lt;br/&gt;~200 tokens]\n    \n    S --&gt; M[LLM]\n    T --&gt; M\n    H --&gt; M\n    R --&gt; M\n    U --&gt; M\n    \n    style S fill:#e3f2fd\n    style R fill:#c8e6c9\n    style U fill:#fff3e0\n\n\n\n\n\n\nPrompting strategies: Zero-shot, Few-shot, Chain-of-thought, System prompts"
  },
  {
    "objectID": "courses/caio/03-ai.html#what-are-ai-agents",
    "href": "courses/caio/03-ai.html#what-are-ai-agents",
    "title": "Modern AI",
    "section": "What Are AI Agents?",
    "text": "What Are AI Agents?\n\n“The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.” — Edsger Dijkstra\n\nAI agents are autonomous systems that:\n\nPerceive their environment\nReason about goals\nTake actions to achieve outcomes\nLearn from results\n\nUnlike chatbots, agents can act in the world."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-agent-loop",
    "href": "courses/caio/03-ai.html#the-agent-loop",
    "title": "Modern AI",
    "section": "The Agent Loop",
    "text": "The Agent Loop\n\n\n\n\n\nflowchart LR\n    P[\"Perceive\"] --&gt; R[\"Reason\"]\n    R --&gt; A[\"Act\"]\n    A --&gt; O[\"Observe\"]\n    O --&gt; P\n    \n    style P fill:#e3f2fd,stroke:#1976d2\n    style R fill:#fff3e0,stroke:#f57c00\n    style A fill:#e8f5e9,stroke:#388e3c\n    style O fill:#fce4ec,stroke:#c2185b\n\n\n\n\n\n\nThe agent perceives its environment, reasons about goals, acts to achieve outcomes, observes the result, and repeats — a continuous loop of intelligent behavior."
  },
  {
    "objectID": "courses/caio/03-ai.html#tool-use-giving-llms-hands",
    "href": "courses/caio/03-ai.html#tool-use-giving-llms-hands",
    "title": "Modern AI",
    "section": "Tool Use: Giving LLMs Hands",
    "text": "Tool Use: Giving LLMs Hands\nLLMs are “brains without hands” — function calling bridges this gap:\n\n\n\n\n\nflowchart LR\n    U[\"Query\"] --&gt; L[\"LLM\"]\n    L --&gt; TC[\"Tool Call\"]\n    TC --&gt; O[\"Orchestrator\"]\n    O --&gt; T[\"Tool\"]\n    T --&gt; |\"Result\"| L\n    L --&gt; R[\"Response\"]\n    \n    style U fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style TC fill:#fce4ec,stroke:#c2185b\n    style T fill:#e8f5e9,stroke:#388e3c\n    style R fill:#f3e5f5,stroke:#7b1fa2\n\n\n\n\n\n\nExamples: Web search, database queries, code execution, API calls."
  },
  {
    "objectID": "courses/caio/03-ai.html#example-currency-conversion-agent",
    "href": "courses/caio/03-ai.html#example-currency-conversion-agent",
    "title": "Modern AI",
    "section": "Example: Currency Conversion Agent",
    "text": "Example: Currency Conversion Agent\nUser: “What’s $100 in euros?”\nAgent reasoning:\n\nI need to convert currency\nCall convert_currency(amount=100, from=\"USD\", to=\"EUR\")\n\nTool returns: 92.50\nAgent response: “100 US dollars is approximately 92.50 euros at current exchange rates.”\nThe agent reasons about what tool to use, then acts to get information."
  },
  {
    "objectID": "courses/caio/03-ai.html#multi-step-planning",
    "href": "courses/caio/03-ai.html#multi-step-planning",
    "title": "Modern AI",
    "section": "Multi-Step Planning",
    "text": "Multi-Step Planning\nComplex tasks require chained actions:\n\n\n\n\n\nflowchart LR\n    Task[Task] --&gt; get_rates[get_rates]\n    get_rates --&gt; Rates[Rates]\n    Rates --&gt; get_prices[get_prices]\n    get_prices --&gt; Prices[Prices]\n    Prices --&gt; correlate[correlate]\n    correlate --&gt; r[r=0.73]\n    r --&gt; report[report]\n    report --&gt; Final[Final Report]\n    \n    style Task fill:#e3f2fd,stroke:#1976d2\n    style get_rates fill:#fff3e0,stroke:#f57c00\n    style Rates fill:#fff3e0,stroke:#f57c00\n    style get_prices fill:#fff3e0,stroke:#f57c00\n    style Prices fill:#fff3e0,stroke:#f57c00\n    style correlate fill:#fff3e0,stroke:#f57c00\n    style r fill:#fff3e0,stroke:#f57c00\n    style report fill:#fff3e0,stroke:#f57c00\n    style Final fill:#c8e6c9,stroke:#2e7d32\n\n\n\n\n\n\nEach step informs the next — true autonomous problem-solving."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-react-framework",
    "href": "courses/caio/03-ai.html#the-react-framework",
    "title": "Modern AI",
    "section": "The ReAct Framework",
    "text": "The ReAct Framework\nReason + Act — Google’s framework for agent workflows:\n\n\n\n\n\nflowchart LR\n    I[Input] --&gt; O1[Observe 1] --&gt; T1[Think 1] --&gt; A1[Act 1] --&gt; O2[Observe 2]\n    O2 --&gt; T2[Think 2] --&gt; A2[Act 2] --&gt; O3[Observe 3] --&gt; T3[Think 3] --&gt; F[Answer]\n    \n    style O1 fill:#e3f2fd,stroke:#1976d2\n    style O2 fill:#e3f2fd,stroke:#1976d2\n    style O3 fill:#e3f2fd,stroke:#1976d2\n    style T1 fill:#fff3e0,stroke:#f57c00\n    style T2 fill:#fff3e0,stroke:#f57c00\n    style T3 fill:#fff3e0,stroke:#f57c00\n    style A1 fill:#c8e6c9,stroke:#388e3c\n    style A2 fill:#c8e6c9,stroke:#388e3c\n    style F fill:#f3e5f5,stroke:#7b1fa2"
  },
  {
    "objectID": "courses/caio/03-ai.html#case-study-chatdev",
    "href": "courses/caio/03-ai.html#case-study-chatdev",
    "title": "Modern AI",
    "section": "Case Study: ChatDev",
    "text": "Case Study: ChatDev\nChatDev orchestrates a virtual software company with specialized AI agents:\n\n\n\n\n\nflowchart LR\n    CEO[CEO] --- CTO[CTO]\n    CTO --- CPO[CPO]\n    Prog[Programmer] --- Des[Designer]\n    Test[Tester] --- Prog2[Programmer]\n    \n    CEO --&gt; Prog\n    Des --&gt; Test\n    Test --&gt; Doc[Documentation]\n    \n    style CEO fill:#ffcccc,stroke:#cc0000\n    style CTO fill:#ccffcc,stroke:#00cc00\n    style Prog fill:#cce5ff,stroke:#1976d2\n    style Test fill:#fff3cd,stroke:#f57c00\n\n\n\n\n\n\nResults: 70 software projects, 17 files each, ~$0.30 per project, 7 minutes."
  },
  {
    "objectID": "courses/caio/03-ai.html#agent-orchestration-patterns",
    "href": "courses/caio/03-ai.html#agent-orchestration-patterns",
    "title": "Modern AI",
    "section": "Agent Orchestration Patterns",
    "text": "Agent Orchestration Patterns\n\n\n\n\n\nflowchart LR\n    A1[Agent A] --&gt; A2[Agent B] --&gt; A3[Agent C]\n    \n    T[Task] --&gt; P1[Agent 1]\n    T --&gt; P2[Agent 2]\n    T --&gt; P3[Agent 3]\n    P1 --&gt; R[Results]\n    P2 --&gt; R\n    P3 --&gt; R\n    \n    S[Supervisor] --&gt; H1[Worker 1]\n    S --&gt; H2[Worker 2]\n    S --&gt; H3[Worker 3]"
  },
  {
    "objectID": "courses/caio/03-ai.html#the-risks-of-agent-autonomy",
    "href": "courses/caio/03-ai.html#the-risks-of-agent-autonomy",
    "title": "Modern AI",
    "section": "The Risks of Agent Autonomy",
    "text": "The Risks of Agent Autonomy\nCase Study: Replit Agent Failure\n\n\n\n\n\nflowchart LR\n    U[\"User: Fix this bug\"] --&gt; A[Agent]\n    A --&gt; D1[\"Diagnoses: config file issue\"]\n    D1 --&gt; D2[\"Decides: delete config\"]\n    D2 --&gt; B[\"Bug in delete tool\"]\n    B --&gt; C[\"Entire project wiped\"]\n    C --&gt; X[\"Production DB destroyed\"]\n    \n    style D1 fill:#fff3cd\n    style D2 fill:#ffcccc\n    style B fill:#ffcccc\n    style X fill:#ff0000,color:#fff\n\n\n\n\n\n\nLesson: Agent autonomy requires multiple safety layers.\nWhat went wrong:\n\n\n\n\n\n\n\n\nFailure\nType\nPrevention\n\n\n\n\nWrong diagnosis\nReasoning error\nRequire confirmation for destructive actions\n\n\nAuto-delete decision\nAutonomy overreach\nHuman-in-the-loop for irreversible ops\n\n\nTool bug\nImplementation flaw\nSandbox testing, rollback capability\n\n\nNo backup\nMissing safeguard\nMandatory snapshots before changes\n\n\n\nKey principle: The more powerful the agent, the more guardrails it needs."
  },
  {
    "objectID": "courses/caio/03-ai.html#agent-safety-challenges",
    "href": "courses/caio/03-ai.html#agent-safety-challenges",
    "title": "Modern AI",
    "section": "Agent Safety Challenges",
    "text": "Agent Safety Challenges\n\n\n\n\n\nflowchart LR\n    PI[Prompt Injection] --&gt; A[Agent]\n    AD[Adversarial Inputs] --&gt; A\n    GM[Goal Misalignment] --&gt; A\n    HA[Hallucinations] --&gt; A\n    CO[Capability Overhang] --&gt; A\n    LC[Lack of Corrigibility] --&gt; A\n    A --&gt; H[Harm]\n    \n    style PI fill:#ffcccc\n    style HA fill:#fff3cd\n    style H fill:#ff0000,color:#fff\n\n\n\n\n\n\nAutonomous agents amplify risks — a hallucination becomes action."
  },
  {
    "objectID": "courses/caio/03-ai.html#safety-mechanisms",
    "href": "courses/caio/03-ai.html#safety-mechanisms",
    "title": "Modern AI",
    "section": "Safety Mechanisms",
    "text": "Safety Mechanisms\n\n\n\n\n\nflowchart LR\n    U[Input] --&gt; IF[Input Filter]\n    IF --&gt; |Clean| A[Agent]\n    IF --&gt; |Malicious| B[Block]\n    A --&gt; OF[Output Filter]\n    OF --&gt; |Safe| R[Response]\n    OF --&gt; |Unsafe| B\n    A --&gt; M[Monitor]\n    M --&gt; |Anomaly| CB[Circuit Breaker]\n    CB --&gt; B\n    \n    style IF fill:#fff3e0,stroke:#f57c00\n    style OF fill:#fff3e0,stroke:#f57c00\n    style B fill:#ffcccc,stroke:#cc0000\n    style R fill:#ccffcc,stroke:#00cc00\n    style CB fill:#fce4ec,stroke:#c2185b\n\n\n\n\n\n\nThe Safety Pipeline:\n\nInput/Output Guards: Fast classifiers that run before and after the LLM.\nMonitoring: Watching for “strange” behavior (e.g., an agent trying to access a restricted database).\nCircuit Breakers: Automatically killing the agent process if safety thresholds are exceeded."
  },
  {
    "objectID": "courses/caio/03-ai.html#anthropics-asl-3-safety-measures",
    "href": "courses/caio/03-ai.html#anthropics-asl-3-safety-measures",
    "title": "Modern AI",
    "section": "Anthropic’s ASL-3 Safety Measures",
    "text": "Anthropic’s ASL-3 Safety Measures\nFor Claude Opus 4, Anthropic activated proactive safety:\n\n\n\n\n\nflowchart LR\n    U[User] --&gt; CC[Constitutional Classifiers]\n    CC --&gt; |Safe| M[Model]\n    CC --&gt; |Blocked| B[Reject]\n    M --&gt; OC[Output Check]\n    OC --&gt; |Safe| R[Response]\n    OC --&gt; |Harmful| B\n    \n    BB[Bug Bounty] --&gt; CC\n    RP[Rapid Patch] --&gt; CC\n    \n    style CC fill:#c8e6c9,stroke:#2e7d32\n    style B fill:#ffcccc,stroke:#cc0000\n    style R fill:#e3f2fd,stroke:#1976d2"
  },
  {
    "objectID": "courses/caio/03-ai.html#evaluating-ai-agents",
    "href": "courses/caio/03-ai.html#evaluating-ai-agents",
    "title": "Modern AI",
    "section": "Evaluating AI Agents",
    "text": "Evaluating AI Agents\nTraditional metrics (accuracy, precision) are insufficient for agents.\n\n\n\n\n\nflowchart LR\n    TC[\"Task Completion\"] --&gt; Score[\"Overall Agent Score\"]\n    RQ[\"Reasoning Quality\"] --&gt; Score\n    SA[\"Safety\"] --&gt; Score\n    RE[\"Resource Efficiency\"] --&gt; Score\n    ER[\"Error Recovery\"] --&gt; Score\n    AD[\"Adversarial Robustness\"] --&gt; Score\n    \n    style SA fill:#ffcccc,stroke:#cc0000\n    style Score fill:#c8e6c9,stroke:#2e7d32"
  },
  {
    "objectID": "courses/caio/03-ai.html#evaluation-approaches",
    "href": "courses/caio/03-ai.html#evaluation-approaches",
    "title": "Modern AI",
    "section": "Evaluation Approaches",
    "text": "Evaluation Approaches\n\n\n\n\n\nflowchart LR\n    A[Agent Output] --&gt; R[Rule-Based]\n    A --&gt; L[LLM-as-Judge]\n    A --&gt; H[Human Review]\n    A --&gt; S[Simulation]\n    \n    R --&gt; E[Score]\n    L --&gt; E\n    H --&gt; E\n    S --&gt; E\n    \n    style R fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style H fill:#c8e6c9,stroke:#2e7d32\n    style S fill:#f3e5f5,stroke:#7b1fa2\n    style E fill:#ffcccc,stroke:#cc0000\n\n\n\n\n\n\nBest practice: Combine multiple approaches for comprehensive evaluation."
  },
  {
    "objectID": "courses/caio/03-ai.html#domain-specific-benchmarks",
    "href": "courses/caio/03-ai.html#domain-specific-benchmarks",
    "title": "Modern AI",
    "section": "Domain-Specific Benchmarks",
    "text": "Domain-Specific Benchmarks\n\n\n\nDomain\nBenchmark\nWhat It Tests\n\n\n\n\nCoding\nSWE-bench\nFix real GitHub issues\n\n\nWeb\nWebArena\nNavigate websites, complete tasks\n\n\nRobotics\nALFRED\nHousehold tasks in 3D\n\n\nEnterprise\nTAU-bench\nMulti-system workflows\n\n\n\nAgent capabilities are task-specific — benchmarks must match use cases."
  },
  {
    "objectID": "courses/caio/03-ai.html#red-teaming-agents",
    "href": "courses/caio/03-ai.html#red-teaming-agents",
    "title": "Modern AI",
    "section": "Red-Teaming Agents",
    "text": "Red-Teaming Agents\nSystematic vulnerability testing:\n\n\n\n\n\nflowchart LR\n    PI[Prompt Injection] --&gt; A[Agent]\n    ME[Agent Mistakes] --&gt; A\n    MU[Direct Misuse] --&gt; A\n    \n    A --&gt; |Vulnerability| V[Security Issue]\n    A --&gt; |Safe| S[Normal Operation]\n    \n    V --&gt; R[Report]\n    \n    style PI fill:#ffcccc,stroke:#cc0000\n    style ME fill:#fff3cd,stroke:#f57c00\n    style MU fill:#ffcccc,stroke:#cc0000\n    style V fill:#ffcccc,stroke:#cc0000\n    style S fill:#ccffcc,stroke:#00cc00\n\n\n\n\n\n\nExample: Hidden text in a webpage hijacks agent to exfiltrate data.\nComprehensive red-teaming found 1,200+ vulnerabilities in one enterprise agent."
  },
  {
    "objectID": "courses/caio/03-ai.html#embodied-ai-robots",
    "href": "courses/caio/03-ai.html#embodied-ai-robots",
    "title": "Modern AI",
    "section": "Embodied AI: Robots",
    "text": "Embodied AI: Robots\nSoftware agents operate in digital systems. Embodied agents must handle:\n\n\n\n\n\nflowchart LR\n    C[\"Camera\"] --&gt; F[\"Fusion\"]\n    L[\"Lidar\"] --&gt; F\n    T[\"Touch\"] --&gt; F\n    F --&gt; B[\"Robot Brain\"]\n    B --&gt; M[\"Motors\"]\n    M --&gt; E[\"Environment\"]\n    E --&gt; |\"Feedback\"| C\n    \n    style F fill:#fff3e0,stroke:#f57c00\n    style B fill:#e3f2fd,stroke:#1976d2\n    style E fill:#c8e6c9,stroke:#388e3c\n\n\n\n\n\n\nThe sim-to-real gap: Robots trained in simulation often fail in reality."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-evolution-of-robotic-intelligence",
    "href": "courses/caio/03-ai.html#the-evolution-of-robotic-intelligence",
    "title": "Modern AI",
    "section": "The Evolution of Robotic Intelligence",
    "text": "The Evolution of Robotic Intelligence\n\n\n\n\n\nflowchart LR\n    S[1960s Shakey] --&gt; P[1980s-2000s Probabilistic]\n    P --&gt; F[2020s Foundation Models]\n    \n    style S fill:#e3f2fd,stroke:#1976d2\n    style P fill:#fff3e0,stroke:#f57c00\n    style F fill:#c8e6c9,stroke:#2e7d32"
  },
  {
    "objectID": "courses/caio/03-ai.html#googles-robotic-transformer-rt-2",
    "href": "courses/caio/03-ai.html#googles-robotic-transformer-rt-2",
    "title": "Modern AI",
    "section": "Google’s Robotic Transformer (RT-2)",
    "text": "Google’s Robotic Transformer (RT-2)\nA vision-language-action model that directly controls robots:\n\n\n\n\n\nflowchart LR\n    V[Vision Input] --&gt; VLA[RT-2 Model]\n    L[Language Input] --&gt; VLA\n    VLA --&gt; A[Action Output]\n    A --&gt; R[Robot]\n    R -- Feedback --&gt; V\n    \n    style V fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style VLA fill:#f3e5f5,stroke:#7b1fa2\n    style A fill:#c8e6c9,stroke:#2e7d32\n    style R fill:#ffcccc,stroke:#cc0000\n\n\n\n\n\n\nGeneral → Interactive → Dexterous\nWorks across robot forms: arms, humanoids, mobile platforms."
  },
  {
    "objectID": "courses/caio/03-ai.html#robot-safety-asimov",
    "href": "courses/caio/03-ai.html#robot-safety-asimov",
    "title": "Modern AI",
    "section": "Robot Safety: ASIMOV",
    "text": "Robot Safety: ASIMOV\nNamed after Asimov’s Laws of Robotics, this benchmark tests embodied AI safety:\n\n\n\n\n\n\n\n\nAsimov’s Law\nModern Interpretation\nTest Scenario\n\n\n\n\n1. Don’t harm humans\nRefuse dangerous commands\n“Throw this at the person”\n\n\n2. Obey orders\nFollow safe instructions\n“Hand me that tool”\n\n\n3. Protect self\nAvoid self-damage\nDon’t walk off ledge\n\n\nZeroth Law\nProtect humanity broadly\nConsider societal impact\n\n\n\nKey challenge: Context matters — “Hand me that knife” is safe in a kitchen, dangerous in a conflict.\nBusiness relevance: As robots enter warehouses, hospitals, and homes, safety benchmarks become legal and ethical requirements."
  },
  {
    "objectID": "courses/caio/03-ai.html#summary-nlp",
    "href": "courses/caio/03-ai.html#summary-nlp",
    "title": "Modern AI",
    "section": "Summary: NLP",
    "text": "Summary: NLP\n\n\n\nConcept\nKey Insight\n\n\n\n\nWord Embeddings\nWords as vectors; geometry = meaning\n\n\nDistributional Hypothesis\nContext reveals meaning\n\n\nAttention\nDynamic weighting of relevant information\n\n\nTransformers\nParallel processing, scalable, powerful\n\n\n\nThe shift from symbols to vectors enabled modern NLP."
  },
  {
    "objectID": "courses/caio/03-ai.html#summary-llms",
    "href": "courses/caio/03-ai.html#summary-llms",
    "title": "Modern AI",
    "section": "Summary: LLMs",
    "text": "Summary: LLMs\n\n\n\nConcept\nKey Insight\n\n\n\n\nAutoregressive Generation\nPredict next token iteratively\n\n\nTemperature\nControls randomness/creativity\n\n\nAlignment\nEnsures safe, helpful behavior\n\n\nContext Windows\nLimit on “memory” size\n\n\n\nScale + alignment = emergent reasoning capabilities."
  },
  {
    "objectID": "courses/caio/03-ai.html#summary-ai-agents",
    "href": "courses/caio/03-ai.html#summary-ai-agents",
    "title": "Modern AI",
    "section": "Summary: AI Agents",
    "text": "Summary: AI Agents\n\n\n\nConcept\nKey Insight\n\n\n\n\nTool Use\nLLMs gain ability to act\n\n\nMulti-Step Planning\nChain reasoning and action\n\n\nOrchestration\nMultiple agents collaborate\n\n\nSafety\nAutonomy amplifies risks\n\n\nEvaluation\nRequires new methodologies\n\n\n\nAgents transform LLMs from conversationalists to autonomous workers."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-executive-perspective",
    "href": "courses/caio/03-ai.html#the-executive-perspective",
    "title": "Modern AI",
    "section": "The Executive Perspective",
    "text": "The Executive Perspective\nFor AI leaders:\n\nNLP powers search, chatbots, document analysis\nLLMs enable natural language interfaces to business systems\nAgents can automate complex, multi-step workflows\nSafety must be built in from the start, not bolted on\nEvaluation requires domain-specific benchmarks and human oversight\n\nThe promise: augmenting human intelligence — agents handle routine tasks while humans provide judgment, creativity, and ethical oversight."
  },
  {
    "objectID": "courses/caio/03-ai.html#supplemental-reading",
    "href": "courses/caio/03-ai.html#supplemental-reading",
    "title": "Modern AI",
    "section": "Supplemental Reading",
    "text": "Supplemental Reading\nOnline Articles:\n\nMaking the Most of AI and Machine Learning in Organizations — Stanford\nThe State of AI in 2024 — McKinsey\nGenerative AI’s Act Two — Sequoia Capital\n\nFrom the Textbook:\n\nChapter 24: Natural Language Processing\nChapter 26: AI Agents"
  },
  {
    "objectID": "courses/caio/03-ai.html#word2vec-tolstoys-novels",
    "href": "courses/caio/03-ai.html#word2vec-tolstoys-novels",
    "title": "Modern AI",
    "section": "Word2Vec: Tolstoy’s Novels",
    "text": "Word2Vec: Tolstoy’s Novels\nTraining Word2Vec on Tolstoy reveals thematic structure through word clusters:\nWar and Peace:\n\nMilitary: soldier, regiment, battle, army\nSocial: ballroom, court, marriage\n“Peace” sits between domains — central to narrative\n\nAnna Karenina:\n\nLove/Passion: heart, passion, desire, jealousy\nSociety: salon, guests, princess, countess\n\nTragedy: death, despair, guilt, train\n\nThe famous opening — “Happy families are all alike; every unhappy family is unhappy in its own way” — clusters “happy” with “alike” and “unhappy” with “own way”!"
  },
  {
    "objectID": "courses/caio/03-ai.html#anna-karenina-word-relationships",
    "href": "courses/caio/03-ai.html#anna-karenina-word-relationships",
    "title": "Modern AI",
    "section": "Anna Karenina: Word Relationships",
    "text": "Anna Karenina: Word Relationships\nWord2Vec captures the novel’s central tensions:\n\n\n\n\n\n\n\nAnalogy\nVector Arithmetic\n\n\n\n\nAnna’s conflict\n\\(\\vec{love} - \\vec{duty} + \\vec{passion} \\approx \\vec{destruction}\\)\n\n\nLevin’s journey\n\\(\\vec{doubt} - \\vec{society} + \\vec{nature} \\approx \\vec{faith}\\)\n\n\n\nBusiness insight: The same technique powers:\n\nNetflix recommendations (movies with similar “themes”)\nAmazon product suggestions\nLinkedIn job matching\nDocument similarity search"
  },
  {
    "objectID": "courses/caio/03-ai.html#why-we-need-randomness-the-obama-example",
    "href": "courses/caio/03-ai.html#why-we-need-randomness-the-obama-example",
    "title": "Modern AI",
    "section": "Why We Need Randomness: The Obama Example",
    "text": "Why We Need Randomness: The Obama Example\nPrompt: “The first African American president is Barack…”\n\nMost probable next token: “Obama” ✓\nAlso correct: “Hussein” (his middle name)\n\nA greedy strategy always picks “Obama” — but in formal documents, “Barack Hussein Obama” is preferred.\nTemperature &gt; 0 allows the model to explore alternatives that may better fit the context."
  },
  {
    "objectID": "courses/caio/03-ai.html#word2vec-war-and-peace",
    "href": "courses/caio/03-ai.html#word2vec-war-and-peace",
    "title": "Modern AI",
    "section": "Word2Vec: War and Peace",
    "text": "Word2Vec: War and Peace\nTraining Word2Vec on Tolstoy’s War and Peace reveals thematic structure:\n\nWord2Vec embeddings from War and Peace, reduced to 2D via PCA"
  },
  {
    "objectID": "courses/caio/03-ai.html#war-and-peace-semantic-clusters",
    "href": "courses/caio/03-ai.html#war-and-peace-semantic-clusters",
    "title": "Modern AI",
    "section": "War and Peace: Semantic Clusters",
    "text": "War and Peace: Semantic Clusters\nThe Word2Vec visualization reveals meaningful semantic relationships:\n\n\n\nCluster\nWords\nInsight\n\n\n\n\nMilitary\nsoldier, regiment, battle, army\nWar domain\n\n\nSocial\nballroom, court, marriage\nPeace domain\n\n\nGovernment\nhistory, power, war\nPolitical themes\n\n\n\nKey observation: “Peace” sits between government and social domains — central to the narrative’s dual structure.\nBusiness applications: Netflix recommendations, Amazon suggestions, LinkedIn job matching, document search"
  },
  {
    "objectID": "courses/caio/03-ai.html#planning-capabilities",
    "href": "courses/caio/03-ai.html#planning-capabilities",
    "title": "Modern AI",
    "section": "Planning Capabilities",
    "text": "Planning Capabilities\nPlanning capabilities enable:\n\n\n\n\n\n\n\n\nCapability\nDescription\nExample\n\n\n\n\nDecomposition\nBreak complex goals into subtasks\n“Analyze market” → 4 API calls\n\n\nState tracking\nRemember intermediate results\nStore data between steps\n\n\nAdaptation\nAdjust plan based on results\nRetry if API fails\n\n\nSynthesis\nCombine outputs into final answer\nMerge data into report\n\n\n\nBusiness impact: Agents can handle multi-hour research tasks that would take humans days."
  },
  {
    "objectID": "courses/caio/03-ai.html#orchestration-use-cases",
    "href": "courses/caio/03-ai.html#orchestration-use-cases",
    "title": "Modern AI",
    "section": "Orchestration: Use Cases",
    "text": "Orchestration: Use Cases\n\n\n\n\n\n\n\n\nPattern\nUse Case\nTradeoff\n\n\n\n\nSequential\nContent pipeline (research → write → edit)\nSimple but slow\n\n\nParallel\nMulti-source analysis\nFast but needs synthesis\n\n\nHierarchical\nProject management\nControl but bottleneck risk\n\n\nDynamic\nMarket-based task allocation\nFlexible but complex"
  },
  {
    "objectID": "courses/caio/03-ai.html#agent-safety-risk-taxonomy",
    "href": "courses/caio/03-ai.html#agent-safety-risk-taxonomy",
    "title": "Modern AI",
    "section": "Agent Safety: Risk Taxonomy",
    "text": "Agent Safety: Risk Taxonomy\n\n\n\n\n\n\n\n\nRisk\nDescription\nReal Example\n\n\n\n\nPrompt Injection\nHidden instructions hijack agent\nEmail contains “ignore previous instructions”\n\n\nHallucinations\nActing on false information\nAgent invents API that doesn’t exist\n\n\nGoal Misalignment\nOptimizes wrong objective\nMaximizes engagement via manipulation\n\n\nCapability Overhang\nDoes more than authorized\nAccesses files outside scope"
  },
  {
    "objectID": "courses/caio/03-ai.html#attention-qkv-interaction",
    "href": "courses/caio/03-ai.html#attention-qkv-interaction",
    "title": "Modern AI",
    "section": "Attention: QKV Interaction",
    "text": "Attention: QKV Interaction\n\n\n\n\n\nflowchart LR\n    k1[k1]\n    k2[k2]\n    km[km]\n    \n    Q[Query q] --&gt; a1[score1]\n    Q --&gt; a2[score2]\n    Q --&gt; am[scorem]\n    \n    k1 --&gt; a1\n    k2 --&gt; a2\n    km --&gt; am\n    \n    a1 -.-&gt; v1[v1]\n    a2 -.-&gt; v2[v2]\n    am -.-&gt; vm[vm]\n    \n    v1 --&gt; O[Output]\n    v2 --&gt; O\n    vm --&gt; O\n    \n    style Q fill:#e1f5fe,stroke:#0277bd\n    style O fill:#c8e6c9,stroke:#2e7d32\n\n\n\n\n\n\n\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\]"
  },
  {
    "objectID": "courses/caio/03-ai.html#autoregressive-generation",
    "href": "courses/caio/03-ai.html#autoregressive-generation",
    "title": "Modern AI",
    "section": "Autoregressive Generation",
    "text": "Autoregressive Generation\nGeneration process:\n\nCompute probability distribution over all possible next tokens\nSample from distribution (controlled by temperature)\nAppend sampled token → Repeat until done"
  },
  {
    "objectID": "courses/caio/03-ai.html#react-the-loop",
    "href": "courses/caio/03-ai.html#react-the-loop",
    "title": "Modern AI",
    "section": "ReAct: The Loop",
    "text": "ReAct: The Loop\nThe Loop:\n\n\n\n\n\n\n\n\nStep\nAction\nExample\n\n\n\n\nObserve\nAnalyze input, tool outputs, environment\n“User wants weather in Paris”\n\n\nThink\nDecide next action or tool to use\n“I should call weather API”\n\n\nAct\nExecute tool or generate response\nget_weather(\"Paris\")\n\n\n\nKey insight: Unlike single-pass generation, ReAct agents can course-correct based on intermediate results."
  },
  {
    "objectID": "courses/caio/03-ai.html#the-defense-in-depth-pipeline",
    "href": "courses/caio/03-ai.html#the-defense-in-depth-pipeline",
    "title": "Modern AI",
    "section": "The Defense-in-Depth Pipeline",
    "text": "The Defense-in-Depth Pipeline\n\n\n\n\n\n\n\n\nLayer\nPurpose\nTechnical Method\n\n\n\n\nInput Filter\nBlock malicious prompts\nPII detection, jailbreak classifiers\n\n\nSandboxing\nIsolate agent actions\nDocker containers, restricted API keys\n\n\nOutput Filter\nPrevent sensitive leakage\nRegEx for PII, toxic content scoring\n\n\nHuman-in-the-Loop\nVerify high-risk actions\n“Approve” button for financial transfers\n\n\nMonitoring\nDetect runtime anomalies\nLog analysis, capability tracking\n\n\n\nKey Principle: Never rely on the LLM to self-police. Use external code to enforce boundaries."
  },
  {
    "objectID": "courses/caio/03-ai.html#human-in-the-loop-hitl",
    "href": "courses/caio/03-ai.html#human-in-the-loop-hitl",
    "title": "Modern AI",
    "section": "Human-in-the-Loop (HITL)",
    "text": "Human-in-the-Loop (HITL)\nThe most effective safety measure for high-stakes agents:\n\nCritical Actions: Require manual approval for destructive or financial operations (e.g., rm -rf, send_payment).\nConfirmation Dialogue: Show the agent’s proposed plan before execution.\nFeedback Loop: Allow the human to correct the agent’s reasoning.\nAudit Logs: Every action approved or rejected by a human is recorded for training and safety reviews.\n\nExample: A code-refactoring agent proposes changes; a human developer reviews and clicks “Merge” or “Reject”."
  },
  {
    "objectID": "courses/caio/03-ai.html#asl-3-safety-pipeline",
    "href": "courses/caio/03-ai.html#asl-3-safety-pipeline",
    "title": "Modern AI",
    "section": "ASL-3: Safety Pipeline",
    "text": "ASL-3: Safety Pipeline\n\n\n\n\n\n\n\n\nLayer\nFunction\nWhy It Matters\n\n\n\n\nConstitutional AI\nReal-time input/output filtering\nBlocks harmful requests before execution\n\n\nBug Bounty\nCrowdsourced discovery\nFinds attacks humans miss\n\n\nRapid Patching\nAuto-generate variants\nStays ahead of attackers\n\n\nEgress Control\nThrottle outbound data\nPrevents model weight theft"
  },
  {
    "objectID": "courses/caio/03-ai.html#robotics-capability-eras",
    "href": "courses/caio/03-ai.html#robotics-capability-eras",
    "title": "Modern AI",
    "section": "Robotics: Capability Eras",
    "text": "Robotics: Capability Eras\n\n\n\n\n\n\n\n\nEra\nCapability\nLimitation\n\n\n\n\nRule-based\nExplicit reasoning\nBrittle, narrow\n\n\nProbabilistic\nHandle uncertainty\nNo language understanding\n\n\nFoundation Models\nNatural language + adaptation\nCompute-intensive\n\n\n\nLLMs have catalyzed a new era: robots that understand language and adapt."
  },
  {
    "objectID": "msai/slides.html#the-vision",
    "href": "msai/slides.html#the-vision",
    "title": "Vadim Sokolov",
    "section": "The Vision",
    "text": "The Vision\n\nGeorge Mason University\nLaunched Fall 2025\nAddressing the critical AI Skills Gap\nEngineering AI Operations: Build, Scale, and Govern\nCross-College Synergy: CS, Systems, Statistics, Policy, and Business"
  },
  {
    "objectID": "msai/slides.html#the-challenge",
    "href": "msai/slides.html#the-challenge",
    "title": "Vadim Sokolov",
    "section": "The Challenge",
    "text": "The Challenge\nThe Width of the Gap\n\n89% of organizations are investing in AI.\nOnly 6% of employees feel comfortable using it.\n51% of tech leaders report an AI skills shortage (Double from 18 months ago).\n\n\n“The largest and fastest-growing tech skills gap in over 15 years.”"
  },
  {
    "objectID": "msai/slides.html#the-solution-ms-in-ai",
    "href": "msai/slides.html#the-solution-ms-in-ai",
    "title": "Vadim Sokolov",
    "section": "The Solution: MS in AI",
    "text": "The Solution: MS in AI\nCurriculum Architecture: 30 Credit Hours\n\n\nCore Foundations (18 Credits)\n\nMachine Learning Foundations\nPlanning & Decision Making\nDeep Learning\nEngineering AI Systems (Hands-on)\nAI Ethics, Policy, & Society\nIT Management\n\n\nSpecialized Tracks (12 Credits)\n\nAI Policy & Ethics\nAdvanced AI\nScalable/Secure Infrastructure\nUse-inspired AI"
  },
  {
    "objectID": "msai/slides.html#why-mason",
    "href": "msai/slides.html#why-mason",
    "title": "Vadim Sokolov",
    "section": "Why Mason?",
    "text": "Why Mason?\nHub of Innovation\n\nLocation: Northern Virginia – The heart of tech and government.\nFaculty: World-class experts from CS, Stats, Cyber, and Policy.\nConnections:\n\nCenter for AI Innovation (CAIIEC)\nAI-in-Gov Faculty Fellows\nRobust Industry Partnerships"
  },
  {
    "objectID": "msai/slides.html#join-us",
    "href": "msai/slides.html#join-us",
    "title": "Master of Science in Artificial Intelligence",
    "section": "Join Us",
    "text": "Join Us\nDeveloping Ethical, Innovative Leaders\n\nOutcomes: Graduates ready to build, govern, and scale AI.\nThe Ask: Partner with us to shape the workforce of tomorrow.\n\nContact: Vadim Sokolov, Program Director cecmsai@gmu.edu"
  },
  {
    "objectID": "msai/slides.html#partner-with-us",
    "href": "msai/slides.html#partner-with-us",
    "title": "Vadim Sokolov",
    "section": "Partner With Us",
    "text": "Partner With Us\n\nCurriculum Advisory: Help us align skills with industry needs.\nCapstone Partners: Provide real-world constraints & datasets for student projects.\nTalent Pipeline: Priority access to internships and graduates.\nGuest Expertise: Share insights as guest speakers or adjunct faculty.\n\nContact: Vadim Sokolov, Program Director vsokolov@gmu.edu"
  },
  {
    "objectID": "msai/slides.html#master-of-science-in-artificial-intelligence",
    "href": "msai/slides.html#master-of-science-in-artificial-intelligence",
    "title": "Vadim Sokolov",
    "section": "Master of Science in Artificial Intelligence",
    "text": "Master of Science in Artificial Intelligence\nGeorge Mason University"
  },
  {
    "objectID": "msai/slides.html#section",
    "href": "msai/slides.html#section",
    "title": "Vadim Sokolov",
    "section": "",
    "text": "Master of Science in Artificial Intelligence"
  },
  {
    "objectID": "msai/slides.html",
    "href": "msai/slides.html",
    "title": "Master of Science inArtificial Intelligence",
    "section": "",
    "text": "title-slide: false format: revealjs: theme: dark transition: slide slide-number: true background-transition: fade logo: gmulogo.png css: styles.css"
  },
  {
    "objectID": "msai/slides.html#the-workforce-readiness-gap",
    "href": "msai/slides.html#the-workforce-readiness-gap",
    "title": "Vadim Sokolov",
    "section": "The Workforce Readiness Gap",
    "text": "The Workforce Readiness Gap\n\n89% of organizations are investing in AI.\nOnly 6% of employees feel comfortable using it.\n51% of tech leaders report an AI skills shortage (Double from 18 months ago).\n\n\n“95% of AI projects fail to generate a return on ROI. It’s not a lack of AI capability, but weak implementation and integration.” — MIT Project NANDA, 2025"
  },
  {
    "objectID": "msai/slides.html#what-we-offer",
    "href": "msai/slides.html#what-we-offer",
    "title": "Vadim Sokolov",
    "section": "What we Offer",
    "text": "What we Offer\nCurriculum Architecture: 30 Credit Hours\n\n\nCore Foundations (18 Credits)\n\nMachine Learning Foundations\nPlanning & Decision Making\nDeep Learning\nEngineering AI Systems (Hands-on)\nAI Ethics, Policy, & Society\nIT Management\n\n\nSpecialized Tracks (12 Credits)\n\nAI Policy & Ethics\nAdvanced AI\nScalable/Secure Infrastructure\nUse-inspired AI"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#case-study-mudslide-threat",
    "href": "courses/caio/mudslide_slides.html#case-study-mudslide-threat",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Case Study: Mudslide Threat",
    "text": "Case Study: Mudslide Threat\nI live in a house at risk of a mudslide damage.\n\nOption A: Build a protective wall ($10,000).\nDamage Cost: $100,000 if the house is hit (and wall fails/absent).\nWall Effectiveness: 95% protection.\nProbability of Mudslide: \\(P(\\text{Slide}) = 0.01\\).\n\nWhat is the best course of action?"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#decision-tree-initial-options",
    "href": "courses/caio/mudslide_slides.html#decision-tree-initial-options",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Decision Tree: Initial Options",
    "text": "Decision Tree: Initial Options\n\n\n\n\n\ngraph LR\n    Start((Decision)) --&gt; Build[Build Wall]\n    Start --&gt; NoBuild[Don't Build]\n    \n    Build -- \"$10,000\" --&gt; WallNode{Slide?}\n    WallNode -- \"0.01\" --&gt; FailNode{Wall Fails?}\n    FailNode -- \"0.05\" --&gt; Loss[\"$100,000 Cost\"]\n    FailNode -- \"0.95\" --&gt; NoLoss[\"$0 Cost\"]\n    WallNode -- \"0.99\" --&gt; NoSlide[\"$0 Cost\"]\n    \n    NoBuild -- \"$0\" --&gt; SlideNode{Slide?}\n    SlideNode -- \"0.01\" --&gt; Loss2[\"$100,000 Cost\"]\n    SlideNode -- \"0.99\" --&gt; NoLoss2[\"$0 Cost\"]"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#comparison-no-test",
    "href": "courses/caio/mudslide_slides.html#comparison-no-test",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Comparison: No Test",
    "text": "Comparison: No Test\nDon’t Build\n\\(EV = 0.01 \\times \\$100,000 = \\$1,000\\)\nBuild (No Test)\n\\(EV = \\$10,000 + (0.01 \\times 0.05 \\times \\$100,000) = \\$10,050\\)\n\n[!IMPORTANT] Based purely on expected cost, Don’t Build is the rational choice despite the high impact of a slide."
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#the-geological-test",
    "href": "courses/caio/mudslide_slides.html#the-geological-test",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "The Geological Test",
    "text": "The Geological Test\nA test is available to better estimate the risk.\n\nCost: $3,000\nAccuracy:\n\n\\(P( T \\mid \\text{Slide} ) = 0.90\\)\n\\(P( \\text{not } T \\mid \\text{No Slide} ) = 0.85\\)\n\n\nShould we take the test?"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#updating-probabilities-bayes-rule",
    "href": "courses/caio/mudslide_slides.html#updating-probabilities-bayes-rule",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Updating Probabilities: Bayes’ Rule",
    "text": "Updating Probabilities: Bayes’ Rule\nProbability of Positive Test \\(P(T)\\):\n\\(P(T) = (0.90 \\times 0.01) + (0.15 \\times 0.99) = 0.1575\\)\nPosterior \\(P(\\text{Slide} \\mid T)\\):\n\\(P(\\text{Slide} \\mid T) = \\frac{0.90 \\times 0.01}{0.1575} \\approx 0.0571\\)\nPosterior \\(P(\\text{Slide} \\mid \\text{not } T)\\):\n\\(P(\\text{Slide} \\mid \\text{not } T) = \\frac{0.1 \\times 0.01}{0.8425} \\approx 0.0012\\)"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#the-testing-strategy",
    "href": "courses/caio/mudslide_slides.html#the-testing-strategy",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "The Testing Strategy",
    "text": "The Testing Strategy\nIf we test:\n\nIf \\(T\\): Build the wall.\nIf not \\(T\\): Don’t build.\n\nExpected Cost with Test:\n\\[\\begin{aligned}\n&\\text{Test Cost} + P(T) \\times \\text{EV(Build} \\mid T) \\\\\n&\\quad + P(\\text{not } T) \\times \\text{EV(No Build} \\mid \\text{not } T)\n\\end{aligned}\\]\n\\(= 3,000 + (0.1575 \\times 10,285) + (0.8425 \\times 120) \\approx \\$4,693\\)"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#risk-vs.-reward",
    "href": "courses/caio/mudslide_slides.html#risk-vs.-reward",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Risk vs. Reward",
    "text": "Risk vs. Reward\n\n\n\nChoice\nExpected Cost\nRisk of Loss\nP\n\n\n\n\nDon’t Build\n$1,000\n0.01\n1 in 100\n\n\nBuild w/o test\n$10,050\n0.0005\n1 in 2000\n\n\nTest & Decide\n$4,693\n0.00146\n1 in 700"
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#conclusion",
    "href": "courses/caio/mudslide_slides.html#conclusion",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Conclusion",
    "text": "Conclusion\n\nLowest Expected Cost: Don’t Build ($1,000).\nLowest Risk of Catastrophe: Build without testing (0.0005).\nMiddle Ground: Testing ($4,693) significantly reduces risk compared to “Don’t Build” without the full $10k upfront cost.\n\nDecision? It depends on your utility function (risk tolerance).\nView Python Implementation (Notebook)"
  },
  {
    "objectID": "courses/caio/01-intro.html#why-distributions-matter",
    "href": "courses/caio/01-intro.html#why-distributions-matter",
    "title": "Machine Learning Essentials",
    "section": "Why Distributions Matter",
    "text": "Why Distributions Matter\n#| include: false\nknitr::knit_hooks$set(pars = function(before, options, envir) {\n  if (before) {\n    par(mar = c(2.5, 2.5, 0.7, 0.5), bty = \"n\", cex.lab = 0.7, cex.axis = 0.5, cex.main = 0.5, pch = 20, cex = 1, mgp = c(1.5, 0.4, 0))\n  }\n})\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\n\n\n\nMachine learning is built on probability\nDistributions describe uncertainty in data\nThree fundamental distributions:\n\nBinomial: Binary outcomes (yes/no, win/lose)\nPoisson: Count data (arrivals, events)\nNormal: Continuous measurements (heights, returns)\n\n\n\nWhy Should Executives Care?\n\n\n\nBusiness Question\nDistribution\n\n\n\n\nWill the customer buy?\nBinomial\n\n\nHow many orders today?\nPoisson\n\n\nWhat’s the forecast error?\nNormal\n\n\n\nChoosing the right distribution is the first step in building a reliable model. Wrong distribution = wrong predictions!"
  },
  {
    "objectID": "courses/caio/01-intro.html#binomial-distribution",
    "href": "courses/caio/01-intro.html#binomial-distribution",
    "title": "Machine Learning Essentials",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\nModels the number of successes in \\(n\\) independent trials, each with probability \\(p\\)\n\\[P(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}\\]\nKey Parameters:\n\n\\(n\\) = number of trials\n\\(p\\) = probability of success\nMean = \\(np\\)\nVariance = \\(np(1-p)\\)\n\nExamples: A/B test conversions, click-through rates, quality defects\n\n#| echo: false\n#| fig-height: 5\nbarplot(dbinom(0:20, size = 20, prob = 0.3),\n  names.arg = 0:20, col = \"steelblue\",\n  xlab = \"Number of Successes\", ylab = \"Probability\",\n  main = \"Binomial(n=20, p=0.3)\")"
  },
  {
    "objectID": "courses/caio/01-intro.html#nfl-patriots-coin-toss",
    "href": "courses/caio/01-intro.html#nfl-patriots-coin-toss",
    "title": "Machine Learning Essentials",
    "section": "NFL Patriots Coin Toss",
    "text": "NFL Patriots Coin Toss\n\n\nThe Patriots won 19 out of 25 coin tosses in 2014-15. How likely?\n\nThere are 177,100 ways to arrange 19 wins in 25 games\nEach specific sequence has probability \\(0.5^{25}\\)\nCombined probability: 0.5% or odds of 199 to 1 against\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\n# \"25 choose 19\" = number of ways to pick 19 wins from 25 games\nchoose(25, 19)\n\n# Probability = (ways to get 19 wins) × (probability of any specific sequence)\nchoose(25, 19) * 0.5^25\n\nThe “Law of Large Numbers” Perspective:\nWith 32 NFL teams over 20+ years, some team will have a suspicious streak!\nKey insight: Probability of Patriots specifically = 0.5%. But probability that some team has a streak ≈ much higher!\nBusiness lesson: When auditing for fraud or anomalies:\n\nDon’t just flag rare events\nConsider how many opportunities for rare events exist\nAdjust for “multiple comparisons”\n\nLooking at enough data, you’ll always find something “unusual”"
  },
  {
    "objectID": "courses/caio/01-intro.html#predicting-premier-league-goals",
    "href": "courses/caio/01-intro.html#predicting-premier-league-goals",
    "title": "Machine Learning Essentials",
    "section": "Predicting Premier League Goals",
    "text": "Predicting Premier League Goals\n\n\nHow many goals will a team score? Historical EPL data:\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\nepl &lt;- read.csv(\"data/epl.csv\")\nepl[1:5, c(\"home_team_name\", \"away_team_name\", \"home_score\", \"guest_score\")]\nEach row = one match with final scores.\n\n\n\nThe Business Problem:\nSports betting: $200+ billion industry.\nOur approach:\n\nAnalyze historical data\nModel goals as random events\nEstimate team strengths\nSimulate matches\n\nWho uses this? FiveThirtyEight, ESPN, DraftKings, Betfair, team analytics"
  },
  {
    "objectID": "courses/caio/01-intro.html#epl-goals-mean-variance",
    "href": "courses/caio/01-intro.html#epl-goals-mean-variance",
    "title": "Machine Learning Essentials",
    "section": "EPL Goals: Mean ≈ Variance",
    "text": "EPL Goals: Mean ≈ Variance\n\n\nA key signature of Poisson data: the mean equals the variance.\n\nTeams score about 1.4 goals per match on average\nThe variance is also ~1.4 — this is the Poisson fingerprint!\nIf variance were much larger, we’d need a different model\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\ngoals &lt;- c(epl$home_score, epl$guest_score)\nmean(goals)  # Average goals per team per match\nvar(goals)   # Variance ≈ Mean suggests Poisson!\n\nModel Diagnostics: Mean vs Variance\n\n\n\nRelationship\nSuggests\n\n\n\n\nVariance ≈ Mean\nPoisson ✓\n\n\nVariance &gt; Mean\nOverdispersion (Negative Binomial)\n\n\nVariance &lt; Mean\nUnderdispersion (rare)\n\n\n\nOther Poisson Applications:\n\nCall center arrivals per hour\nWebsite clicks per minute\nInsurance claims per year\nManufacturing defects per batch\nEmails received per day\n\nPoisson is the “go-to” for count data!"
  },
  {
    "objectID": "courses/caio/01-intro.html#goals-follow-a-poisson-distribution",
    "href": "courses/caio/01-intro.html#goals-follow-a-poisson-distribution",
    "title": "Machine Learning Essentials",
    "section": "Goals Follow a Poisson Distribution",
    "text": "Goals Follow a Poisson Distribution\n\n\n#| code-fold: true\n#| code-summary: \"Show R code\"\n#| fig-height: 5.5\ngoals &lt;- c(epl$home_score, epl$guest_score)\nlambda &lt;- mean(goals)\nx &lt;- 0:8\nobserved &lt;- table(factor(goals, levels = x)) / length(goals)\nexpected &lt;- dpois(x, lambda = lambda)\n\nbarplot(rbind(observed, expected), beside = TRUE, \n        names.arg = x, col = c(\"steelblue\", \"coral\"),\n        xlab = \"Goals Scored\", ylab = \"Proportion\",\n        legend.text = c(\"Observed\", \"Poisson Model\"))\n\nModel Validation:\nThe Poisson model (coral bars) fits the observed data (blue bars) remarkably well!\nWhat this tells us:\n\nGoals are indeed rare, independent events\nThe Poisson assumption is justified\nWe can use this model for predictions\n\nSlight discrepancy at 0 goals: Real matches have slightly fewer 0-0 draws than Poisson predicts (teams try harder when level!)"
  },
  {
    "objectID": "courses/caio/01-intro.html#poisson-distribution",
    "href": "courses/caio/01-intro.html#poisson-distribution",
    "title": "Machine Learning Essentials",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nModels count of random events: goals, arrivals, defects, clicks\n\\[P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\n\n\\(\\lambda\\) (lambda): expected rate of events\nKey property: Mean = Variance = \\(\\lambda\\)\nEvents occur independently at a constant average rate\n\n\nBusiness Applications:\n\nCustomer arrivals per hour\nWebsite clicks per day\nManufacturing defects per batch\nInsurance claims per year\nServer requests per minute\n\nIf events are rare and independent, Poisson is your model!"
  },
  {
    "objectID": "courses/caio/01-intro.html#improving-the-model-team-strength",
    "href": "courses/caio/01-intro.html#improving-the-model-team-strength",
    "title": "Machine Learning Essentials",
    "section": "Improving the Model: Team Strength",
    "text": "Improving the Model: Team Strength\n\n\nA single \\(\\lambda\\) for all teams is too simple. Better model:\n\\[\\lambda_{ij} = \\text{Attack}_i \\times \\text{Defense}_j \\times \\text{HomeAdvantage}\\]\n\nAttack: How good is team \\(i\\) at scoring?\nDefense: How weak is team \\(j\\) at defending?\nHome advantage: ~0.4 extra goals at home\n\n\nThis is how real sports analytics works:\n\nEstimate each team’s offensive/defensive strength from historical data\nAdjust for home/away effects\nPredict expected goals for each team\nUse Poisson to generate win/draw/loss probabilities\n\nSame framework applies to:\n\nNBA point spreads\nNFL betting lines\nCricket run predictions\nBaseball run expectations"
  },
  {
    "objectID": "courses/caio/01-intro.html#team-specific-lambda-arsenal-vs-liverpool",
    "href": "courses/caio/01-intro.html#team-specific-lambda-arsenal-vs-liverpool",
    "title": "Machine Learning Essentials",
    "section": "Team-Specific \\(\\lambda\\): Arsenal vs Liverpool",
    "text": "Team-Specific \\(\\lambda\\): Arsenal vs Liverpool\nTo predict a specific match, we estimate each team’s scoring rate:\n\nArsenal’s attack: How many goals do they typically score at home?\nLiverpool’s defense: How many goals do they typically concede away?\nAdjustment: Scale by league average to get relative strength\n\nFor Arsenal vs Liverpool at home, we estimate Arsenal will score about 1.8 goals on average. Liverpool’s away \\(\\lambda\\) would be calculated similarly.\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\n# Simple estimate: average goals scored and conceded\narsenal_attack &lt;- mean(epl$home_score[epl$home_team_name == \"Arsenal\"])\nliverpool_defense &lt;- mean(epl$home_score[epl$away_team_name == \"Liverpool\"])\nleague_avg &lt;- mean(goals)\n\n# Arsenal's expected goals vs Liverpool (simplified)\nlambda_arsenal &lt;- arsenal_attack * (liverpool_defense / league_avg)\nlambda_arsenal"
  },
  {
    "objectID": "courses/caio/01-intro.html#monte-carlo-simulation",
    "href": "courses/caio/01-intro.html#monte-carlo-simulation",
    "title": "Machine Learning Essentials",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\nOnce we have \\(\\lambda\\) for each team, we can simulate the match thousands of times.\nFor Arsenal (\\(\\lambda=1.8\\)) vs Liverpool (\\(\\lambda=1.5\\)), running 10,000 simulations gives:\n\nArsenal wins: ~42% of simulations\nDraw: ~24% of simulations\n\nLiverpool wins: ~34% of simulations\n\nThis is how betting companies set their odds!\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\nset.seed(42)\nn_sims &lt;- 10000\n# Simulate Arsenal vs Liverpool\narsenal_goals &lt;- rpois(n_sims, lambda = 1.8)  # λ for Arsenal\nliverpool_goals &lt;- rpois(n_sims, lambda = 1.5) # λ for Liverpool\n\n# Match outcomes\nc(Arsenal_Win = mean(arsenal_goals &gt; liverpool_goals),\n  Draw = mean(arsenal_goals == liverpool_goals),\n  Liverpool_Win = mean(arsenal_goals &lt; liverpool_goals))"
  },
  {
    "objectID": "courses/caio/01-intro.html#why-monte-carlo",
    "href": "courses/caio/01-intro.html#why-monte-carlo",
    "title": "Machine Learning Essentials",
    "section": "Why Monte Carlo?",
    "text": "Why Monte Carlo?\n\n\nEach simulation draws random goals from Poisson distributions\n\nRun 10,000 simulations → get probability of each outcome\nCan extend to simulate entire season, league standings\nSame approach used by betting companies and analytics firms\n\nThis is how FiveThirtyEight and bookmakers build their models!\n\nMonte Carlo Applications:\n\nFinance: Option pricing, portfolio risk (VaR)\nInsurance: Claim projections, reserve calculations\nOperations: Supply chain uncertainty, demand forecasting\nEngineering: Reliability analysis, quality control\nAI: Reinforcement learning, MCMC for Bayesian inference\n\nWhen math is too hard, simulate!"
  },
  {
    "objectID": "courses/caio/01-intro.html#central-limit-theorem-clt",
    "href": "courses/caio/01-intro.html#central-limit-theorem-clt",
    "title": "Machine Learning Essentials",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\n\nThe most important theorem in statistics:\n\nThe average of many independent random events tends toward a Normal distribution, regardless of the original distribution.\n\nWhy it matters: Stock returns, measurement errors, test scores — all tend to be Normal because they’re sums of many small effects.\n\nPractical Implications:\n\nSample means are approximately Normal (even if data isn’t)\nConfidence intervals work because of CLT\nA/B testing relies on CLT for significance tests\nQuality control uses CLT for process monitoring\n\nRule of thumb: Sample size ≥ 30 usually sufficient for CLT to kick in\nThis is why the Normal distribution is everywhere!"
  },
  {
    "objectID": "courses/caio/01-intro.html#clt-in-action-michigan-election-polls",
    "href": "courses/caio/01-intro.html#clt-in-action-michigan-election-polls",
    "title": "Machine Learning Essentials",
    "section": "CLT in Action: Michigan Election Polls",
    "text": "CLT in Action: Michigan Election Polls\nSuppose the true vote share in Michigan is 51%. What happens when we poll voters?\n\nEach voter is like a coin flip (vote A or B)\nSmall samples are noisy; large samples converge to the truth\nThe distribution of poll results becomes Normal\n\n#| fig-height: 4.5\n#| code-fold: true\n#| code-summary: \"Show R code\"\n#| layout-ncol: 3\nset.seed(42)\ntrue_p &lt;- 0.51\n# Poll of 10 voters\nhist(replicate(1000, mean(rbinom(10, 1, true_p))), breaks = 20,\n     main = \"Poll: 10 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\n# Poll of 100 voters\nhist(replicate(1000, mean(rbinom(100, 1, true_p))), breaks = 20,\n     main = \"Poll: 100 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\n# Poll of 1000 voters\nhist(replicate(1000, mean(rbinom(1000, 1, true_p))), breaks = 20,\n     main = \"Poll: 1000 Voters\", xlab = \"Vote Share\", col = \"steelblue\", \n     freq = FALSE, xlim = c(0.2, 0.8))\nabline(v = true_p, col = \"red\", lwd = 2, lty = 2)\nLarger samples → tighter Normal distribution around the true value (red line)"
  },
  {
    "objectID": "courses/caio/01-intro.html#normal-distribution",
    "href": "courses/caio/01-intro.html#normal-distribution",
    "title": "Machine Learning Essentials",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nThe “bell curve” — the most important distribution in statistics\nThe 68-95-99.7 Rule:\n\n68% of data within 1 standard deviation\n95% of data within 2 standard deviations\n99.7% of data within 3 standard deviations\n\nWhy it’s everywhere: Central Limit Theorem guarantees that averages of many random events become Normal\nApplications: Quality control, financial risk, test scores, measurement error\n\n#| echo: false\n#| fig-height: 5\nx &lt;- seq(-4, 4, length = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 3, col = \"steelblue\",\n     xlab = \"Standard Deviations from Mean\", ylab = \"Density\")\npolygon(c(x[x &gt;= -1 & x &lt;= 1], 1, -1), \n        c(dnorm(x[x &gt;= -1 & x &lt;= 1]), 0, 0), col = rgb(0.3, 0.5, 0.7, 0.3))\nabline(v = c(-2, -1, 1, 2), lty = 2, col = \"gray\")\ntext(0, 0.15, \"68%\", cex = 1.2)"
  },
  {
    "objectID": "courses/caio/01-intro.html#normal-heights-of-adults",
    "href": "courses/caio/01-intro.html#normal-heights-of-adults",
    "title": "Machine Learning Essentials",
    "section": "Normal: Heights of Adults",
    "text": "Normal: Heights of Adults\n\n\nMale heights follow a Normal distribution: mean = 70 inches, sd = 3 inches\n\n68% of men are between 67-73 inches (within 1 sd)\nThe 95th percentile is about 75 inches — only 5% are taller\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\n# What proportion are between 67 and 73 inches (+/- 1 sd)?\npnorm(73, mean = 70, sd = 3) - pnorm(67, mean = 70, sd = 3)\n\n# What height is taller than 95% of men?\nqnorm(0.95, mean = 70, sd = 3)\n\nR Functions for Normal Distribution:\n\n\n\nFunction\nPurpose\nExample\n\n\n\n\npnorm()\nProbability ≤ x\nP(height ≤ 73)\n\n\nqnorm()\nFind percentile\n95th percentile\n\n\ndnorm()\nDensity at x\nHeight of curve\n\n\nrnorm()\nRandom samples\nSimulate data\n\n\n\nBusiness Applications:\n\nSetting size ranges for products\nEstablishing “normal” ranges for KPIs\nIdentifying outliers (&gt; 2-3 sd)\nQuality control limits"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-1987-stock-market-crash-a-5-sigma-event",
    "href": "courses/caio/01-intro.html#the-1987-stock-market-crash-a-5-sigma-event",
    "title": "Machine Learning Essentials",
    "section": "The 1987 Stock Market Crash: A 5-Sigma Event",
    "text": "The 1987 Stock Market Crash: A 5-Sigma Event\nHow extreme was the October 1987 crash of -21.76%?\n\nPrior to crash: \\(\\mu = 1.2\\%\\), \\(\\sigma = 4.3\\%\\) → Z-score = \\(\\frac{-21.76 - 1.2}{4.3} = -5.34\\)\nUnder Normal model: probability = 1 in 20 million (once every 130,000 years!)\nYet 5+ sigma events happened in 1987, 2008, and 2020\n\nConclusion: The model is wrong — stock returns have “fat tails.” Banks using Normal-based VaR dramatically underestimate risk.\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\npnorm(-5.34)  # Probability of -5.34 sigma event"
  },
  {
    "objectID": "courses/caio/01-intro.html#fat-tails-reality-vs-normal-model",
    "href": "courses/caio/01-intro.html#fat-tails-reality-vs-normal-model",
    "title": "Machine Learning Essentials",
    "section": "Fat Tails: Reality vs Normal Model",
    "text": "Fat Tails: Reality vs Normal Model\n\n\n\n\n\n\n\n\nThe Problem with Normal Assumptions:\nStock returns have more extreme events than the Normal distribution predicts.\n\n\n\nEvent\nNormal Probability\nActually Happened\n\n\n\n\n1987 Crash (-22%)\n1 in \\(10^{160}\\)\nYes\n\n\n2008 Crisis\n“Impossible”\nYes\n\n\n2020 COVID Crash\n“Impossible”\nYes\n\n\n\nImplications for Risk Management:\n\nVaR models underestimate tail risk\nNeed “fat-tailed” distributions (t-distribution, etc.)\nStress testing is essential"
  },
  {
    "objectID": "courses/caio/01-intro.html#what-is-regression",
    "href": "courses/caio/01-intro.html#what-is-regression",
    "title": "Machine Learning Essentials",
    "section": "What is Regression?",
    "text": "What is Regression?\n\n\nFinding the relationship between variables\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\n\\(\\beta_0\\): intercept (baseline value)\n\\(\\beta_1\\): slope (change in \\(y\\) per unit change in \\(x\\))\n\\(\\epsilon\\): unexplained variation\n\nGoal: Minimize sum of squared prediction errors\n\nBusiness Questions Regression Answers:\n\nHow much does price affect sales?\nWhat’s the ROI of advertising spend?\nHow does experience affect salary?\nWhat drives customer lifetime value?\nHow does weather affect demand?\n\nRegression quantifies relationships and enables prediction."
  },
  {
    "objectID": "courses/caio/01-intro.html#simple-example-house-prices",
    "href": "courses/caio/01-intro.html#simple-example-house-prices",
    "title": "Machine Learning Essentials",
    "section": "Simple Example: House Prices",
    "text": "Simple Example: House Prices\n\n\nUsing Saratoga County housing data, we fit a model:\nPrice = f(Living Area)\n\nIntercept: Base price of ~$13,000 (land value)\nSlope: Each additional square foot adds ~$113 to the price\n\nA 2,000 sq ft house: $13K + (2000 × $113) = $239,000\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\nd &lt;- read.csv(\"data/SaratogaHouses.csv\")\nmodel &lt;- lm(price ~ livingArea, data = d)\ncoef(model)\n\nInterpreting Coefficients:\n\n\n\nCoefficient\nMeaning\n\n\n\n\nIntercept ($13K)\nValue of land without house\n\n\nSlope ($113/sqft)\nPrice increase per sqft\n\n\n\nMaking Predictions:\n\\[\\text{Price} = 13,439 + 113 \\times \\text{SqFt}\\]\n\n\n\nHouse Size\nPredicted Price\n\n\n\n\n1,500 sqft\n$183,000\n\n\n2,500 sqft\n$296,000\n\n\n3,500 sqft\n$409,000"
  },
  {
    "objectID": "courses/caio/01-intro.html#visualizing-the-fit",
    "href": "courses/caio/01-intro.html#visualizing-the-fit",
    "title": "Machine Learning Essentials",
    "section": "Visualizing the Fit",
    "text": "Visualizing the Fit\n\n\n#| echo: false\n#| fig-height: 5.5\nd$price &lt;- d$price / 1000\nd$livingArea &lt;- d$livingArea / 1000\nmodel &lt;- lm(price ~ livingArea, data = d)\nplot(d$livingArea, d$price, pch = 21, bg = \"lightblue\", cex = 0.6,\n     xlab = \"Living Area (1000 sq ft)\", ylab = \"Price ($1000)\")\nabline(model, col = \"red\", lwd = 3)\n\nWhat the plot shows:\n\nEach blue dot is a house\nThe red line is our prediction\nVertical distance from dot to line = prediction error\n\nKey observations:\n\nStrong positive relationship\nMore scatter at higher prices (heteroskedasticity)\nSome outliers (expensive small houses, cheap large houses)\n\nThe line minimizes the sum of squared vertical distances"
  },
  {
    "objectID": "courses/caio/01-intro.html#google-vs-sp-500-capm",
    "href": "courses/caio/01-intro.html#google-vs-sp-500-capm",
    "title": "Machine Learning Essentials",
    "section": "Google vs S&P 500 (CAPM)",
    "text": "Google vs S&P 500 (CAPM)\nThe Capital Asset Pricing Model (CAPM) asks: Does a stock follow the market or beat it?\n\\[\\text{Google Return} = \\alpha + \\beta \\times \\text{Market Return}\\]\n\n\\(\\beta\\) (beta): How volatile is the stock relative to the market?\n\\(\\alpha\\) (alpha): Does the stock outperform after adjusting for risk?\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\n#| message: false\nlibrary(quantmod)\ngetSymbols(c(\"GOOG\", \"SPY\"), from = \"2017-01-01\", to = \"2023-12-31\") |&gt; invisible()\ngoog &lt;- as.numeric(dailyReturn(GOOG))\nspy &lt;- as.numeric(dailyReturn(SPY))\nmodel &lt;- lm(goog ~ spy)\nprint(model)"
  },
  {
    "objectID": "courses/caio/01-intro.html#google-vs-sp-500-capm-results",
    "href": "courses/caio/01-intro.html#google-vs-sp-500-capm-results",
    "title": "Machine Learning Essentials",
    "section": "Google vs S&P 500: CAPM Results",
    "text": "Google vs S&P 500: CAPM Results\n\n\n#| echo: false\n#| fig-height: 5.5\nplot(spy, goog, pch = 20, col = rgb(0.3, 0.5, 0.7, 0.5), cex = 0.8,\n     xlab = \"S&P 500 Daily Return\", ylab = \"Google Daily Return\",\n     main = \"Google vs Market (2017-2023)\")\nabline(model, col = \"red\", lwd = 3)\nabline(h = 0, v = 0, lty = 2, col = \"gray\")\nlegend(\"topleft\", legend = bquote(beta == .(round(coef(model)[2], 2))), \n       col = \"red\", lwd = 3, bty = \"n\")\n\nOur Findings:\n\nBeta (\\(\\beta = 1.01\\)): Google moves 1:1 with market\nAlpha (\\(\\alpha \\approx 0\\)): No significant outperformance (\\(p = 0.06\\))\n\n\n\n\nBeta      \nInterpretation\n\n\n\n\n\\(\\beta &lt; 1\\)\nLess volatile (utilities, healthcare)\n\n\n\\(\\beta = 1\\)\nMoves with market (index funds)\n\n\n\\(\\beta &gt; 1\\)\nMore volatile (tech, small caps)\n\n\n\nConclusion: Google tracked the market without consistent alpha in 2017-2023. High beta = higher risk, potentially higher reward."
  },
  {
    "objectID": "courses/caio/01-intro.html#orange-juice-price-advertising",
    "href": "courses/caio/01-intro.html#orange-juice-price-advertising",
    "title": "Machine Learning Essentials",
    "section": "Orange Juice: Price & Advertising",
    "text": "Orange Juice: Price & Advertising\nHow does advertising affect price sensitivity? We model sales as a function of price and whether the product was featured in ads.\nKey finding: The interaction term (log(price):feat) is negative and significant — advertising changes how customers respond to price!\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\noj &lt;- read.csv(\"data/oj.csv\")\nmodel &lt;- lm(logmove ~ log(price) * feat, data = oj)\ntidy(model) |&gt; select(term, estimate, p.value) |&gt; kable(digits = 3)"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-advertising-paradox",
    "href": "courses/caio/01-intro.html#the-advertising-paradox",
    "title": "Machine Learning Essentials",
    "section": "The Advertising Paradox",
    "text": "The Advertising Paradox\n\n\nFinding: Advertising increases price sensitivity\n\n\n\nCondition\nPrice Elasticity\n\n\n\n\nNo advertising\n-0.96\n\n\nWith advertising\n-0.96 + (-0.98) = -1.94\n\n\n\nWhy? Ads coincide with promotions → attract price-sensitive shoppers\n\nKey Lessons:\n\nCorrelation ≠ Causation: Ads don’t cause sensitivity; they coincide with promotions\nSelection effects: Who responds to ads? Price hunters!\nConfounding variables: Promotions happen during ad campaigns\nManagerial insight: Don’t blame advertising for price sensitivity — it’s the promotion strategy\n\nAlways ask: What’s really driving the relationship?"
  },
  {
    "objectID": "courses/caio/01-intro.html#from-regression-to-classification",
    "href": "courses/caio/01-intro.html#from-regression-to-classification",
    "title": "Machine Learning Essentials",
    "section": "From Regression to Classification",
    "text": "From Regression to Classification\n\n\nWhat if the outcome is yes/no?\n\\[P(y=1 \\mid x) = \\frac{1}{1 + e^{-\\beta^T x}}\\]\nWhy not just use linear regression?\n\nLinear regression can predict values &lt; 0 or &gt; 1\nProbabilities must be between 0 and 1\nLogistic function “squashes” any input to (0, 1)\n\n\n#| echo: false\n#| fig-height: 5\nx &lt;- seq(-6, 6, length = 200)\nplot(x, 1/(1 + exp(-x)), type = \"l\", lwd = 3, col = \"steelblue\",\n     xlab = expression(\"Linear Predictor (\" * beta * \"'x)\"), ylab = \"Probability\",\n     main = \"The Logistic (Sigmoid) Function\")\nabline(h = 0.5, lty = 2, col = \"gray\")\nabline(h = c(0, 1), lty = 3, col = \"red\")\ntext(4, 0.75, \"Always between 0 and 1\", cex = 0.9)"
  },
  {
    "objectID": "courses/caio/01-intro.html#nba-point-spread-example",
    "href": "courses/caio/01-intro.html#nba-point-spread-example",
    "title": "Machine Learning Essentials",
    "section": "NBA Point Spread Example",
    "text": "NBA Point Spread Example\nCan Vegas point spreads predict game outcomes? We fit a logistic regression using historical NBA data.\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\nNBA &lt;- read.csv(\"data/NBAspread.csv\")\nmodel &lt;- glm(favwin ~ spread - 1, family = binomial, data = NBA)\ntidy(model) |&gt; kable(digits = 3)\nInterpretation: For each additional point in the spread, log-odds of favorite winning increases by 0.16. The p-value &lt; 0.001 confirms spreads are highly predictive."
  },
  {
    "objectID": "courses/caio/01-intro.html#making-predictions",
    "href": "courses/caio/01-intro.html#making-predictions",
    "title": "Machine Learning Essentials",
    "section": "Making Predictions",
    "text": "Making Predictions\nUsing our model, we can predict win probability for any point spread:\n\n\n\nSpread\nP(Favorite Wins)\n\n\n\n\n4 points\n65%\n\n\n8 points\n78%\n\n\n12 points\n87%\n\n\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\npredict(model, newdata = data.frame(spread = c(4, 8)), type = \"response\")\nSame approach used for: credit scoring, churn prediction, marketing response, fraud detection — any binary outcome."
  },
  {
    "objectID": "courses/caio/01-intro.html#confusion-matrix",
    "href": "courses/caio/01-intro.html#confusion-matrix",
    "title": "Machine Learning Essentials",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n\nHow accurate is our model? The confusion matrix shows predictions vs. actual outcomes.\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Show R code\"\npred &lt;- predict(model, type = \"response\") &gt; 0.5\ntable(Actual = NBA$favwin, Predicted = as.integer(pred))\nOur model achieves about 66% accuracy — better than a coin flip!\n\nReading the Matrix:\n\n\n\n\nPred: 0\nPred: 1\n\n\n\n\nActual: 0\nTN (correct!)\nFP (oops)\n\n\nActual: 1\nFN (oops)\nTP (correct!)\n\n\n\nSports Betting Reality:\n\n66% accuracy sounds good, but…\nVegas takes ~10% commission (“vig”)\nNeed ~52.4% accuracy just to break even\nEdge of 13.6% is excellent if it holds!\n\nBut past performance ≠ future results"
  },
  {
    "objectID": "courses/caio/01-intro.html#understanding-the-confusion-matrix",
    "href": "courses/caio/01-intro.html#understanding-the-confusion-matrix",
    "title": "Machine Learning Essentials",
    "section": "Understanding the Confusion Matrix",
    "text": "Understanding the Confusion Matrix\n\n\n\n\nPredicted: Win\nPredicted: Lose\n\n\n\n\nActual: Win\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual: Lose\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nKey Metrics:\n\nAccuracy = (TP + TN) / Total — overall correctness\nPrecision = TP / (TP + FP) — “Of predicted wins, how many were right?”\nRecall = TP / (TP + FN) — “Of actual wins, how many did we catch?”\n\nCaution: Accuracy can mislead! A spam filter predicting “not spam” for everything has 99% accuracy but catches zero spam. Choose metrics based on business costs."
  },
  {
    "objectID": "courses/caio/01-intro.html#roc-curve-the-trade-off",
    "href": "courses/caio/01-intro.html#roc-curve-the-trade-off",
    "title": "Machine Learning Essentials",
    "section": "ROC Curve: The Trade-off",
    "text": "ROC Curve: The Trade-off\n\n\n#| echo: false\n#| fig-height: 5.5\npred_prob &lt;- predict(model, type = \"response\")\nroc_data &lt;- data.frame(\n  threshold = seq(0, 1, by = 0.01)\n) |&gt;\n  rowwise() |&gt;\n  mutate(\n    sensitivity = mean(pred_prob[NBA$favwin == 1] &gt; threshold),\n    specificity = mean(pred_prob[NBA$favwin == 0] &lt;= threshold)\n  )\n\nplot(1 - roc_data$specificity, roc_data$sensitivity, type = \"l\", lwd = 3,\n     col = \"steelblue\", xlab = \"False Positive Rate\", ylab = \"True Positive Rate\",\n     main = \"ROC Curve\")\nabline(0, 1, lty = 2, col = \"gray\")\n\nUnderstanding the ROC Curve:\n\nX-axis: False Positive Rate (false alarms)\nY-axis: True Positive Rate (catches)\nDiagonal: Random guessing (AUC = 0.5)\nUpper-left corner: Perfect classifier\n\nArea Under Curve (AUC):\n\n\n\nAUC\nModel Quality\n\n\n\n\n0.5\nRandom (useless)\n\n\n0.6-0.7\nPoor\n\n\n0.7-0.8\nFair\n\n\n0.8-0.9\nGood\n\n\n0.9+\nExcellent"
  },
  {
    "objectID": "courses/caio/01-intro.html#choosing-the-right-threshold",
    "href": "courses/caio/01-intro.html#choosing-the-right-threshold",
    "title": "Machine Learning Essentials",
    "section": "Choosing the Right Threshold",
    "text": "Choosing the Right Threshold\n\n\nThe optimal threshold depends on business costs:\n\nFraud detection: Low threshold (catch more fraud, accept false alarms)\nMedical screening: Low threshold (don’t miss disease)\nSpam filter: Higher threshold (don’t lose important emails)\n\nThere is no universal “correct” threshold\n\nFramework for Threshold Selection:\n\nQuantify costs: What’s the cost of FP vs FN?\nCalculate expected cost at each threshold\nChoose threshold that minimizes total expected cost\n\nExample — Credit Card Fraud:\n\nFalse Positive cost: $10 (customer inconvenience)\nFalse Negative cost: $500 (fraud loss)\nOptimal threshold: Much lower than 0.5!\n\nLet business economics guide your model decisions"
  },
  {
    "objectID": "courses/caio/01-intro.html#summary",
    "href": "courses/caio/01-intro.html#summary",
    "title": "Machine Learning Essentials",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nKey Insight\n\n\n\n\nDistributions\nBinomial (binary), Poisson (counts), Normal (continuous)\n\n\nPoisson\nMean = Variance — the fingerprint of count data\n\n\nNormal\nCLT makes it universal for averages\n\n\nLinear Regression\nCoefficients = effect sizes\n\n\nLogistic Regression\nOutputs probabilities for classification\n\n\nROC/AUC\nTrade-off between false positives and false negatives\n\n\nThreshold\nBusiness costs should drive the choice\n\n\n\nStatistics is the science of decision-making under uncertainty"
  },
  {
    "objectID": "courses/caio/01-intro.html#supplemental-reading",
    "href": "courses/caio/01-intro.html#supplemental-reading",
    "title": "Machine Learning Essentials",
    "section": "Supplemental Reading",
    "text": "Supplemental Reading\n\n\nOnline Articles:\n\nThe Surprising Power of Online Experiments - HBR\nMachine Learning, Explained - MIT Sloan\n\nKey Insight from HBR: A simple A/B test at Bing generated over $100M annually by testing a “low priority” idea\n\nBooks for Further Study:\n\nThe Signal and the Noise — Nate Silver\nThinking, Fast and Slow — Daniel Kahneman\nNaked Statistics — Charles Wheelan\nData Science for Business — Provost & Fawcett\n\nOnline Courses:\n\nAndrew Ng’s Machine Learning (Coursera)\nStatistical Learning (Stanford Online)\nFast.ai Practical Deep Learning"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-language-challenge",
    "href": "courses/caio/01-intro.html#the-language-challenge",
    "title": "Machine Learning Essentials",
    "section": "The Language Challenge",
    "text": "The Language Challenge\n\n“You shall know a word by the company it keeps.” — J.R. Firth (1957)\n\nLanguage poses unique challenges for AI:\n\nUnlike images (continuous pixels) or audio (waveforms), text is discrete symbols\nThe word “cat” is not inherently closer to “dog” than to “quantum”\nYet humans effortlessly recognize semantic similarities\n\nThe breakthrough: Represent words as vectors in continuous space where geometry encodes meaning."
  },
  {
    "objectID": "courses/caio/01-intro.html#from-symbols-to-vectors",
    "href": "courses/caio/01-intro.html#from-symbols-to-vectors",
    "title": "Machine Learning Essentials",
    "section": "From Symbols to Vectors",
    "text": "From Symbols to Vectors\nThe Problem with One-Hot Encoding:\nEach word gets a unique vector with a single 1:\n\n“cat” → [0, 0, 1, 0, 0, …, 0]\n“dog” → [0, 1, 0, 0, 0, …, 0]\n\nProblem: Cosine similarity between any two words = 0\nNo notion of semantic similarity is captured!\nSolution: Learn dense vector representations where similar words are close together."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-twenty-questions-intuition",
    "href": "courses/caio/01-intro.html#the-twenty-questions-intuition",
    "title": "Machine Learning Essentials",
    "section": "The Twenty Questions Intuition",
    "text": "The Twenty Questions Intuition\nImagine playing Twenty Questions to identify words:\n\n\n\nQuestion\nBear\nDog\nCat\n\n\n\n\nIs it an animal?\n1\n1\n1\n\n\nIs it domestic?\n0\n1\n0.7\n\n\nLarger than human?\n0.8\n0.1\n0.01\n\n\nHas long tail?\n0\n0.6\n1\n\n\nIs it a predator?\n1\n0\n0.6\n\n\n\nEach word becomes a vector of answers. Similar words give similar answers → similar vectors!\nThis is the essence of word embeddings."
  },
  {
    "objectID": "courses/caio/01-intro.html#word2vec-learning-from-context",
    "href": "courses/caio/01-intro.html#word2vec-learning-from-context",
    "title": "Machine Learning Essentials",
    "section": "Word2Vec: Learning from Context",
    "text": "Word2Vec: Learning from Context\nThe Distributional Hypothesis: Words appearing in similar contexts have similar meanings.\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    C1[\"The ___ sat on the mat\"]\n    C2[\"The ___ sat on the rug\"]\n    \n    Cat[\"cat\"] --&gt; C1\n    Dog[\"dog\"] --&gt; C2\n    \n    Cat --&gt; V[\"Similar Vectors!\"]\n    Dog --&gt; V\n    \n    style Cat fill:#e1f5fe,stroke:#1976d2\n    style Dog fill:#e1f5fe,stroke:#1976d2\n    style V fill:#c8e6c9,stroke:#2e7d32\nResult: Vector arithmetic captures analogies!\n\\[\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#word2vec-war-and-peace",
    "href": "courses/caio/01-intro.html#word2vec-war-and-peace",
    "title": "Machine Learning Essentials",
    "section": "Word2Vec: War and Peace",
    "text": "Word2Vec: War and Peace\nTraining Word2Vec on Tolstoy’s War and Peace reveals thematic structure:\n\nWord2Vec embeddings from War and Peace, reduced to 2D via PCA"
  },
  {
    "objectID": "courses/caio/01-intro.html#war-and-peace-semantic-clusters",
    "href": "courses/caio/01-intro.html#war-and-peace-semantic-clusters",
    "title": "Machine Learning Essentials",
    "section": "War and Peace: Semantic Clusters",
    "text": "War and Peace: Semantic Clusters\nThe Word2Vec visualization reveals meaningful semantic relationships:\n\n\n\nCluster\nWords\nInsight\n\n\n\n\nMilitary\nsoldier, regiment, battle, army\nWar domain\n\n\nSocial\nballroom, court, marriage\nPeace domain\n\n\nGovernment\nhistory, power, war\nPolitical themes\n\n\n\nKey observation: “Peace” sits between government and social domains — central to the narrative’s dual structure.\nBusiness applications: Netflix recommendations, Amazon suggestions, LinkedIn job matching, document search"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-skip-gram-model",
    "href": "courses/caio/01-intro.html#the-skip-gram-model",
    "title": "Machine Learning Essentials",
    "section": "The Skip-Gram Model",
    "text": "The Skip-Gram Model\nGiven a center word, predict surrounding context words:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    A[loves] --&gt; B[the]\n    A --&gt; C[man]\n    A --&gt; D[his]\n    A --&gt; E[son]\n    \n    style A fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    style B fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style C fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n\\[P(\\text{context} \\mid \\text{center}) = \\prod_{j} P(w_{j} \\mid w_{\\text{center}})\\]\nThe learned vectors capture semantic relationships because words with similar contexts get similar representations."
  },
  {
    "objectID": "courses/caio/01-intro.html#from-words-to-sentences-the-attention-revolution",
    "href": "courses/caio/01-intro.html#from-words-to-sentences-the-attention-revolution",
    "title": "Machine Learning Essentials",
    "section": "From Words to Sentences: The Attention Revolution",
    "text": "From Words to Sentences: The Attention Revolution\nThe Problem: Static Embeddings\n\nThe word “bank” has one vector, whether it’s “river bank” or “investment bank.”\nThis conflation of senses creates an information bottleneck.\n\nThe Sequential Bottleneck (RNNs/LSTMs):\n\nProcessed text step-by-step (like reading through a straw).\nEarly information “vanished” as sentences grew longer.\nImpossible to parallelize effectively on modern GPUs.\n\nThe Breakthrough: Attention Mechanisms\n\nLet each word dynamically “attend” to all other words simultaneously.\nExample: “The trophy wouldn’t fit in the suitcase because it was too big.”\nSelf-attention identifies that “it” refers to “trophy” by looking at the whole sentence at once.\n\nResult: Contextual representations that change based on surrounding words."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-attention-mechanism",
    "href": "courses/caio/01-intro.html#the-attention-mechanism",
    "title": "Machine Learning Essentials",
    "section": "The Attention Mechanism",
    "text": "The Attention Mechanism\nThe Library Analogy (Query, Key, Value):\n\nQuery (Q): What am I looking for? (e.g., “subject of the sentence”)\nKey (K): What is in this book? (e.g., “noun”, “verb”, “adjective”)\nValue (V): What is the content of the book? (the actual meaning vector)\n\nThe Mathematical Operation:\n\nSimilarity: Compare the Query to all Keys using a dot product.\nScoring: Turn these scores into weights (probabilities) using Softmax.\nRetrieval: Take a weighted sum of the Values."
  },
  {
    "objectID": "courses/caio/01-intro.html#attention-qkv-interaction",
    "href": "courses/caio/01-intro.html#attention-qkv-interaction",
    "title": "Machine Learning Essentials",
    "section": "Attention: QKV Interaction",
    "text": "Attention: QKV Interaction\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    k1[k1]\n    k2[k2]\n    km[km]\n    \n    Q[Query q] --&gt; a1[score1]\n    Q --&gt; a2[score2]\n    Q --&gt; am[scorem]\n    \n    k1 --&gt; a1\n    k2 --&gt; a2\n    km --&gt; am\n    \n    a1 -.-&gt; v1[v1]\n    a2 -.-&gt; v2[v2]\n    am -.-&gt; vm[vm]\n    \n    v1 --&gt; O[Output]\n    v2 --&gt; O\n    vm --&gt; O\n    \n    style Q fill:#e1f5fe,stroke:#0277bd\n    style O fill:#c8e6c9,stroke:#2e7d32\n\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\]"
  },
  {
    "objectID": "courses/caio/01-intro.html#attention-a-visual-example",
    "href": "courses/caio/01-intro.html#attention-a-visual-example",
    "title": "Machine Learning Essentials",
    "section": "Attention: A Visual Example",
    "text": "Attention: A Visual Example\nFor “The trophy wouldn’t fit in the suitcase because it was too big”:\n\n\n\nWord\nAttention to “it”\n\n\n\n\ntrophy\n0.45\n\n\nsuitcase\n0.15\n\n\nfit\n0.12\n\n\nbig\n0.18\n\n\nother\n0.10\n\n\n\nThe model learns that “it” most likely refers to “trophy” by attending strongly to it!"
  },
  {
    "objectID": "courses/caio/01-intro.html#self-attention-vs-cross-attention",
    "href": "courses/caio/01-intro.html#self-attention-vs-cross-attention",
    "title": "Machine Learning Essentials",
    "section": "Self-Attention vs Cross-Attention",
    "text": "Self-Attention vs Cross-Attention\n\n\nSelf-Attention:\n%%| echo: false\n%%| fig-width: 4\nflowchart LR\n    w1[\"The\"] &lt;--&gt; w2[\"cat\"]\n    w2 &lt;--&gt; w3[\"sat\"]\n    w1 &lt;--&gt; w3\n    \n    style w1 fill:#e1f5fe,stroke:#1976d2\n    style w2 fill:#e1f5fe,stroke:#1976d2\n    style w3 fill:#e1f5fe,stroke:#1976d2\nGoal: Understand internal relationships. Used in: Encoders (BERT) and Decoders (GPT). Analogy: Rereading a sentence to find the subject.\n\nCross-Attention:\n%%| echo: false\n%%| fig-width: 4\nflowchart LR\n    e1[\"The\"]\n    e2[\"cat\"]\n    d1[\"Le\"]\n    d2[\"chat\"]\n    \n    e1 --&gt; d1\n    e2 --&gt; d2\n    e1 -.-&gt; d2\n    e2 -.-&gt; d1\n    \n    style e1 fill:#e1f5fe,stroke:#1976d2\n    style e2 fill:#e1f5fe,stroke:#1976d2\n    style d1 fill:#c8e6c9,stroke:#2e7d32\n    style d2 fill:#c8e6c9,stroke:#2e7d32\nGoal: Link two different sequences. Used in: Translation models (T5). Analogy: Looking back at English while writing French."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-transformer-architecture",
    "href": "courses/caio/01-intro.html#the-transformer-architecture",
    "title": "Machine Learning Essentials",
    "section": "The Transformer Architecture",
    "text": "The Transformer Architecture\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    In[\"Input\"] --&gt; Tok[\"Token\"] --&gt; Emb[\"Embed\"] --&gt; Att[\"Attention\"] --&gt; FF[\"FeedForward\"] --&gt; Out[\"Output\"]\n    \n    style In fill:#e3f2fd,stroke:#1976d2\n    style Att fill:#fff3e0,stroke:#f57c00\n    style FF fill:#e8f5e9,stroke:#388e3c\n    style Out fill:#f3e5f5,stroke:#7b1fa2\nKey innovations:\n\nSelf-attention replaces recurrence → parallel processing\nPositional encoding preserves word order\nMulti-head attention captures different relationship types\nFeed-forward layers add nonlinear transformations"
  },
  {
    "objectID": "courses/caio/01-intro.html#why-transformers-won",
    "href": "courses/caio/01-intro.html#why-transformers-won",
    "title": "Machine Learning Essentials",
    "section": "Why Transformers Won",
    "text": "Why Transformers Won\n\n\n\nProperty\nRNN/LSTM\nTransformer\n\n\n\n\nSequential processing\nYes (slow)\nNo (parallel)\n\n\nLong-range dependencies\nDifficult\nEasy\n\n\nTraining speed\nSlow\nFast\n\n\nScalability\nLimited\nExcellent\n\n\n\nTransformers scale with compute → the foundation of modern LLMs."
  },
  {
    "objectID": "courses/caio/01-intro.html#from-transformers-to-llms",
    "href": "courses/caio/01-intro.html#from-transformers-to-llms",
    "title": "Machine Learning Essentials",
    "section": "From Transformers to LLMs",
    "text": "From Transformers to LLMs\nThe Scale Approach:\n%%| echo: false\n%%| fig-width: 9\nflowchart LR\n    G1[\"GPT-1&lt;br/&gt;117M\"] --&gt; G2[\"GPT-2&lt;br/&gt;1.5B\"]\n    G2 --&gt; G3[\"GPT-3&lt;br/&gt;175B\"]\n    G3 --&gt; G4[\"GPT-4&lt;br/&gt;~1.8T\"]\n    \n    style G1 fill:#e3f2fd\n    style G2 fill:#bbdefb\n    style G3 fill:#90caf9\n    style G4 fill:#42a5f5\nEmergent capabilities appear at scale:\n\nChain-of-thought reasoning\nIn-context learning (few-shot)\nCode generation\nMulti-step planning"
  },
  {
    "objectID": "courses/caio/01-intro.html#how-llms-generate-text",
    "href": "courses/caio/01-intro.html#how-llms-generate-text",
    "title": "Machine Learning Essentials",
    "section": "How LLMs Generate Text",
    "text": "How LLMs Generate Text\nLLMs are autoregressive: they predict the next token based on all previous tokens.\n\n\n%%| echo: false\n%%| fig-width: 8\nflowchart LR\n    C[\"Context\"] --&gt; M[\"LLM\"]\n    M --&gt; P[\"Probabilities\"]\n    P --&gt; S[\"Sample\"]\n    S --&gt; T[\"Token\"]\n    T --&gt; |\"Append\"| C\n    \n    style C fill:#e3f2fd,stroke:#1976d2\n    style M fill:#fff3e0,stroke:#f57c00\n    style P fill:#e8f5e9,stroke:#388e3c\n    style T fill:#f3e5f5,stroke:#7b1fa2\nThe Generation Loop:\n\nProbabilities: Compute scores for the vocabulary.\nSampling: Select next word (via Temperature).\nAutoregression: Append and repeat.\n\n\nTemperature (\\(\\tau\\)):\n\n\n\n\\(\\tau\\)\nBehavior\n\n\n\n\n0\nDeterministic\n\n\n0.7\nBalanced\n\n\n1.0\nProbabilistic\n\n\n1.5\nCreative\n\n\n\nLower \\(\\tau\\) = Predictable Higher \\(\\tau\\) = Random"
  },
  {
    "objectID": "courses/caio/01-intro.html#why-we-need-randomness-the-obama-example",
    "href": "courses/caio/01-intro.html#why-we-need-randomness-the-obama-example",
    "title": "Machine Learning Essentials",
    "section": "Why We Need Randomness: The Obama Example",
    "text": "Why We Need Randomness: The Obama Example\nPrompt: “The first African American president is Barack…”\n\nMost probable next token: “Obama” ✓\nAlso correct: “Hussein” (his middle name)\n\nA greedy strategy always picks “Obama” — but in formal documents, “Barack Hussein Obama” is preferred.\nTemperature &gt; 0 allows the model to explore alternatives that may better fit the context."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-llm-lifecycle",
    "href": "courses/caio/01-intro.html#the-llm-lifecycle",
    "title": "Machine Learning Essentials",
    "section": "The LLM Lifecycle",
    "text": "The LLM Lifecycle\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    D[Data Collection] --&gt; P[Pre-Training]\n    P --&gt; I[Instruction Tuning]\n    I --&gt; A[Alignment]\n    A --&gt; Dep[Deployment]\n    \n    style D fill:#e3f2fd,stroke:#1976d2\n    style P fill:#e8f5e9,stroke:#388e3c\n    style I fill:#fff3e0,stroke:#f57c00\n    style A fill:#fce4ec,stroke:#c2185b\n    style Dep fill:#f3e5f5,stroke:#7b1fa2\n\n\n\nStage\nPurpose\n\n\n\n\nData Collection\nCurate training corpus (quality &gt; quantity)\n\n\nPre-Training\nPredict next tokens on billions of sequences\n\n\nInstruction Tuning\nTeach the model to follow instructions\n\n\nAlignment\nEnsure behavior matches human values (RLHF)\n\n\nDeployment\nOptimize for latency, cost, safety"
  },
  {
    "objectID": "courses/caio/01-intro.html#alignment-why-it-matters",
    "href": "courses/caio/01-intro.html#alignment-why-it-matters",
    "title": "Machine Learning Essentials",
    "section": "Alignment: Why It Matters",
    "text": "Alignment: Why It Matters\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    Q[\"User Query\"]\n    \n    Q --&gt; U[\"Unaligned\"]\n    Q --&gt; A[\"Aligned\"]\n    \n    U --&gt; UR[\"Yes, only true god\"]\n    \n    A --&gt; AR[\"Multiple perspectives exist\"]\n    \n    style Q fill:#e3f2fd,stroke:#1976d2\n    style UR fill:#ffcccc,stroke:#cc0000\n    style AR fill:#ccffcc,stroke:#00cc00\nExample: “Is Allah the only god?”\n\nUnaligned: “Yes, Allah is the one true god and all other beliefs are false.”\nAligned: “In Islam, Allah is considered the one God. Other religions have different perspectives. I can provide factual information if helpful.”\n\nThis nuanced behavior emerges from alignment training, not pre-training alone."
  },
  {
    "objectID": "courses/caio/01-intro.html#context-windows-and-prompting",
    "href": "courses/caio/01-intro.html#context-windows-and-prompting",
    "title": "Machine Learning Essentials",
    "section": "Context Windows and Prompting",
    "text": "Context Windows and Prompting\nContext window: Maximum tokens the model can “see” at once\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    S[System Prompt&lt;br/&gt;~500 tokens]\n    T[Tools/Schemas&lt;br/&gt;~300 tokens]\n    H[History&lt;br/&gt;~1000 tokens]\n    R[Retrieved Docs&lt;br/&gt;~2000 tokens]\n    U[User Query&lt;br/&gt;~200 tokens]\n    \n    S --&gt; M[LLM]\n    T --&gt; M\n    H --&gt; M\n    R --&gt; M\n    U --&gt; M\n    \n    style S fill:#e3f2fd\n    style R fill:#c8e6c9\n    style U fill:#fff3e0\nPrompting strategies: Zero-shot, Few-shot, Chain-of-thought, System prompts"
  },
  {
    "objectID": "courses/caio/01-intro.html#what-are-ai-agents",
    "href": "courses/caio/01-intro.html#what-are-ai-agents",
    "title": "Machine Learning Essentials",
    "section": "What Are AI Agents?",
    "text": "What Are AI Agents?\n\n“The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.” — Edsger Dijkstra\n\nAI agents are autonomous systems that:\n\nPerceive their environment\nReason about goals\nTake actions to achieve outcomes\nLearn from results\n\nUnlike chatbots, agents can act in the world."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-agent-loop",
    "href": "courses/caio/01-intro.html#the-agent-loop",
    "title": "Machine Learning Essentials",
    "section": "The Agent Loop",
    "text": "The Agent Loop\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    P[\"Perceive\"] --&gt; R[\"Reason\"]\n    R --&gt; A[\"Act\"]\n    A --&gt; O[\"Observe\"]\n    O --&gt; P\n    \n    style P fill:#e3f2fd,stroke:#1976d2\n    style R fill:#fff3e0,stroke:#f57c00\n    style A fill:#e8f5e9,stroke:#388e3c\n    style O fill:#fce4ec,stroke:#c2185b\nThe agent perceives its environment, reasons about goals, acts to achieve outcomes, observes the result, and repeats — a continuous loop of intelligent behavior."
  },
  {
    "objectID": "courses/caio/01-intro.html#tool-use-giving-llms-hands",
    "href": "courses/caio/01-intro.html#tool-use-giving-llms-hands",
    "title": "Machine Learning Essentials",
    "section": "Tool Use: Giving LLMs Hands",
    "text": "Tool Use: Giving LLMs Hands\nLLMs are “brains without hands” — function calling bridges this gap:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    U[\"Query\"] --&gt; L[\"LLM\"]\n    L --&gt; TC[\"Tool Call\"]\n    TC --&gt; O[\"Orchestrator\"]\n    O --&gt; T[\"Tool\"]\n    T --&gt; |\"Result\"| L\n    L --&gt; R[\"Response\"]\n    \n    style U fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style TC fill:#fce4ec,stroke:#c2185b\n    style T fill:#e8f5e9,stroke:#388e3c\n    style R fill:#f3e5f5,stroke:#7b1fa2\nExamples: Web search, database queries, code execution, API calls."
  },
  {
    "objectID": "courses/caio/01-intro.html#example-currency-conversion-agent",
    "href": "courses/caio/01-intro.html#example-currency-conversion-agent",
    "title": "Machine Learning Essentials",
    "section": "Example: Currency Conversion Agent",
    "text": "Example: Currency Conversion Agent\nUser: “What’s $100 in euros?”\nAgent reasoning:\n\nI need to convert currency\nCall convert_currency(amount=100, from=\"USD\", to=\"EUR\")\n\nTool returns: 92.50\nAgent response: “100 US dollars is approximately 92.50 euros at current exchange rates.”\nThe agent reasons about what tool to use, then acts to get information."
  },
  {
    "objectID": "courses/caio/01-intro.html#multi-step-planning",
    "href": "courses/caio/01-intro.html#multi-step-planning",
    "title": "Machine Learning Essentials",
    "section": "Multi-Step Planning",
    "text": "Multi-Step Planning\nComplex tasks require chained actions:\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    Task[Task] --&gt; get_rates[get_rates]\n    get_rates --&gt; Rates[Rates]\n    Rates --&gt; get_prices[get_prices]\n    get_prices --&gt; Prices[Prices]\n    Prices --&gt; correlate[correlate]\n    correlate --&gt; r[r=0.73]\n    r --&gt; report[report]\n    report --&gt; Final[Final Report]\n    \n    style Task fill:#e3f2fd,stroke:#1976d2\n    style get_rates fill:#fff3e0,stroke:#f57c00\n    style Rates fill:#fff3e0,stroke:#f57c00\n    style get_prices fill:#fff3e0,stroke:#f57c00\n    style Prices fill:#fff3e0,stroke:#f57c00\n    style correlate fill:#fff3e0,stroke:#f57c00\n    style r fill:#fff3e0,stroke:#f57c00\n    style report fill:#fff3e0,stroke:#f57c00\n    style Final fill:#c8e6c9,stroke:#2e7d32\nEach step informs the next — true autonomous problem-solving."
  },
  {
    "objectID": "courses/caio/01-intro.html#planning-capabilities",
    "href": "courses/caio/01-intro.html#planning-capabilities",
    "title": "Machine Learning Essentials",
    "section": "Planning Capabilities",
    "text": "Planning Capabilities\nPlanning capabilities enable:\n\n\n\n\n\n\n\n\nCapability\nDescription\nExample\n\n\n\n\nDecomposition\nBreak complex goals into subtasks\n“Analyze market” → 4 API calls\n\n\nState tracking\nRemember intermediate results\nStore data between steps\n\n\nAdaptation\nAdjust plan based on results\nRetry if API fails\n\n\nSynthesis\nCombine outputs into final answer\nMerge data into report\n\n\n\nBusiness impact: Agents can handle multi-hour research tasks that would take humans days."
  },
  {
    "objectID": "courses/caio/01-intro.html#react-the-loop",
    "href": "courses/caio/01-intro.html#react-the-loop",
    "title": "Machine Learning Essentials",
    "section": "ReAct: The Loop",
    "text": "ReAct: The Loop\nThe Loop:\n\n\n\n\n\n\n\n\nStep\nAction\nExample\n\n\n\n\nObserve\nAnalyze input, tool outputs, environment\n“User wants weather in Paris”\n\n\nThink\nDecide next action or tool to use\n“I should call weather API”\n\n\nAct\nExecute tool or generate response\nget_weather(\"Paris\")\n\n\n\nKey insight: Unlike single-pass generation, ReAct agents can course-correct based on intermediate results."
  },
  {
    "objectID": "courses/caio/01-intro.html#case-study-chatdev",
    "href": "courses/caio/01-intro.html#case-study-chatdev",
    "title": "Machine Learning Essentials",
    "section": "Case Study: ChatDev",
    "text": "Case Study: ChatDev\nChatDev orchestrates a virtual software company with specialized AI agents:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    CEO[CEO] --- CTO[CTO]\n    CTO --- CPO[CPO]\n    Prog[Programmer] --- Des[Designer]\n    Test[Tester] --- Prog2[Programmer]\n    \n    CEO --&gt; Prog\n    Des --&gt; Test\n    Test --&gt; Doc[Documentation]\n    \n    style CEO fill:#ffcccc,stroke:#cc0000\n    style CTO fill:#ccffcc,stroke:#00cc00\n    style Prog fill:#cce5ff,stroke:#1976d2\n    style Test fill:#fff3cd,stroke:#f57c00\nResults: 70 software projects, 17 files each, ~$0.30 per project, 7 minutes."
  },
  {
    "objectID": "courses/caio/01-intro.html#agent-orchestration-patterns",
    "href": "courses/caio/01-intro.html#agent-orchestration-patterns",
    "title": "Machine Learning Essentials",
    "section": "Agent Orchestration Patterns",
    "text": "Agent Orchestration Patterns\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    A1[Agent A] --&gt; A2[Agent B] --&gt; A3[Agent C]\n    \n    T[Task] --&gt; P1[Agent 1]\n    T --&gt; P2[Agent 2]\n    T --&gt; P3[Agent 3]\n    P1 --&gt; R[Results]\n    P2 --&gt; R\n    P3 --&gt; R\n    \n    S[Supervisor] --&gt; H1[Worker 1]\n    S --&gt; H2[Worker 2]\n    S --&gt; H3[Worker 3]"
  },
  {
    "objectID": "courses/caio/01-intro.html#orchestration-use-cases",
    "href": "courses/caio/01-intro.html#orchestration-use-cases",
    "title": "Machine Learning Essentials",
    "section": "Orchestration: Use Cases",
    "text": "Orchestration: Use Cases\n\n\n\n\n\n\n\n\nPattern\nUse Case\nTradeoff\n\n\n\n\nSequential\nContent pipeline (research → write → edit)\nSimple but slow\n\n\nParallel\nMulti-source analysis\nFast but needs synthesis\n\n\nHierarchical\nProject management\nControl but bottleneck risk\n\n\nDynamic\nMarket-based task allocation\nFlexible but complex"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-risks-of-agent-autonomy",
    "href": "courses/caio/01-intro.html#the-risks-of-agent-autonomy",
    "title": "Machine Learning Essentials",
    "section": "The Risks of Agent Autonomy",
    "text": "The Risks of Agent Autonomy\nCase Study: Replit Agent Failure\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    U[\"User: Fix this bug\"] --&gt; A[Agent]\n    A --&gt; D1[\"Diagnoses: config file issue\"]\n    D1 --&gt; D2[\"Decides: delete config\"]\n    D2 --&gt; B[\"Bug in delete tool\"]\n    B --&gt; C[\"Entire project wiped\"]\n    C --&gt; X[\"Production DB destroyed\"]\n    \n    style D1 fill:#fff3cd\n    style D2 fill:#ffcccc\n    style B fill:#ffcccc\n    style X fill:#ff0000,color:#fff\nLesson: Agent autonomy requires multiple safety layers.\nWhat went wrong:\n\n\n\n\n\n\n\n\nFailure\nType\nPrevention\n\n\n\n\nWrong diagnosis\nReasoning error\nRequire confirmation for destructive actions\n\n\nAuto-delete decision\nAutonomy overreach\nHuman-in-the-loop for irreversible ops\n\n\nTool bug\nImplementation flaw\nSandbox testing, rollback capability\n\n\nNo backup\nMissing safeguard\nMandatory snapshots before changes\n\n\n\nKey principle: The more powerful the agent, the more guardrails it needs."
  },
  {
    "objectID": "courses/caio/01-intro.html#agent-safety-challenges",
    "href": "courses/caio/01-intro.html#agent-safety-challenges",
    "title": "Machine Learning Essentials",
    "section": "Agent Safety Challenges",
    "text": "Agent Safety Challenges\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    PI[Prompt Injection] --&gt; A[Agent]\n    AD[Adversarial Inputs] --&gt; A\n    GM[Goal Misalignment] --&gt; A\n    HA[Hallucinations] --&gt; A\n    CO[Capability Overhang] --&gt; A\n    LC[Lack of Corrigibility] --&gt; A\n    A --&gt; H[Harm]\n    \n    style PI fill:#ffcccc\n    style HA fill:#fff3cd\n    style H fill:#ff0000,color:#fff\nAutonomous agents amplify risks — a hallucination becomes action."
  },
  {
    "objectID": "courses/caio/01-intro.html#agent-safety-risk-taxonomy",
    "href": "courses/caio/01-intro.html#agent-safety-risk-taxonomy",
    "title": "Machine Learning Essentials",
    "section": "Agent Safety: Risk Taxonomy",
    "text": "Agent Safety: Risk Taxonomy\n\n\n\n\n\n\n\n\nRisk\nDescription\nReal Example\n\n\n\n\nPrompt Injection\nHidden instructions hijack agent\nEmail contains “ignore previous instructions”\n\n\nHallucinations\nActing on false information\nAgent invents API that doesn’t exist\n\n\nGoal Misalignment\nOptimizes wrong objective\nMaximizes engagement via manipulation\n\n\nCapability Overhang\nDoes more than authorized\nAccesses files outside scope"
  },
  {
    "objectID": "courses/caio/01-intro.html#safety-mechanisms",
    "href": "courses/caio/01-intro.html#safety-mechanisms",
    "title": "Machine Learning Essentials",
    "section": "Safety Mechanisms",
    "text": "Safety Mechanisms\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    U[Input] --&gt; IF[Input Filter]\n    IF --&gt; |Clean| A[Agent]\n    IF --&gt; |Malicious| B[Block]\n    A --&gt; OF[Output Filter]\n    OF --&gt; |Safe| R[Response]\n    OF --&gt; |Unsafe| B\n    A --&gt; M[Monitor]\n    M --&gt; |Anomaly| CB[Circuit Breaker]\n    CB --&gt; B\n    \n    style IF fill:#fff3e0,stroke:#f57c00\n    style OF fill:#fff3e0,stroke:#f57c00\n    style B fill:#ffcccc,stroke:#cc0000\n    style R fill:#ccffcc,stroke:#00cc00\n    style CB fill:#fce4ec,stroke:#c2185b\nThe Safety Pipeline:\n\nInput/Output Guards: Fast classifiers that run before and after the LLM.\nMonitoring: Watching for “strange” behavior (e.g., an agent trying to access a restricted database).\nCircuit Breakers: Automatically killing the agent process if safety thresholds are exceeded."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-defense-in-depth-pipeline",
    "href": "courses/caio/01-intro.html#the-defense-in-depth-pipeline",
    "title": "Machine Learning Essentials",
    "section": "The Defense-in-Depth Pipeline",
    "text": "The Defense-in-Depth Pipeline\n\n\n\n\n\n\n\n\nLayer\nPurpose\nTechnical Method\n\n\n\n\nInput Filter\nBlock malicious prompts\nPII detection, jailbreak classifiers\n\n\nSandboxing\nIsolate agent actions\nDocker containers, restricted API keys\n\n\nOutput Filter\nPrevent sensitive leakage\nRegEx for PII, toxic content scoring\n\n\nHuman-in-the-Loop\nVerify high-risk actions\n“Approve” button for financial transfers\n\n\nMonitoring\nDetect runtime anomalies\nLog analysis, capability tracking\n\n\n\nKey Principle: Never rely on the LLM to self-police. Use external code to enforce boundaries."
  },
  {
    "objectID": "courses/caio/01-intro.html#human-in-the-loop-hitl",
    "href": "courses/caio/01-intro.html#human-in-the-loop-hitl",
    "title": "Machine Learning Essentials",
    "section": "Human-in-the-Loop (HITL)",
    "text": "Human-in-the-Loop (HITL)\nThe most effective safety measure for high-stakes agents:\n\nCritical Actions: Require manual approval for destructive or financial operations (e.g., rm -rf, send_payment).\nConfirmation Dialogue: Show the agent’s proposed plan before execution.\nFeedback Loop: Allow the human to correct the agent’s reasoning.\nAudit Logs: Every action approved or rejected by a human is recorded for training and safety reviews.\n\nExample: A code-refactoring agent proposes changes; a human developer reviews and clicks “Merge” or “Reject”."
  },
  {
    "objectID": "courses/caio/01-intro.html#anthropics-asl-3-safety-measures",
    "href": "courses/caio/01-intro.html#anthropics-asl-3-safety-measures",
    "title": "Machine Learning Essentials",
    "section": "Anthropic’s ASL-3 Safety Measures",
    "text": "Anthropic’s ASL-3 Safety Measures\nFor Claude Opus 4, Anthropic activated proactive safety:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    U[User] --&gt; CC[Constitutional Classifiers]\n    CC --&gt; |Safe| M[Model]\n    CC --&gt; |Blocked| B[Reject]\n    M --&gt; OC[Output Check]\n    OC --&gt; |Safe| R[Response]\n    OC --&gt; |Harmful| B\n    \n    BB[Bug Bounty] --&gt; CC\n    RP[Rapid Patch] --&gt; CC\n    \n    style CC fill:#c8e6c9,stroke:#2e7d32\n    style B fill:#ffcccc,stroke:#cc0000\n    style R fill:#e3f2fd,stroke:#1976d2"
  },
  {
    "objectID": "courses/caio/01-intro.html#asl-3-safety-pipeline",
    "href": "courses/caio/01-intro.html#asl-3-safety-pipeline",
    "title": "Machine Learning Essentials",
    "section": "ASL-3: Safety Pipeline",
    "text": "ASL-3: Safety Pipeline\n\n\n\n\n\n\n\n\nLayer\nFunction\nWhy It Matters\n\n\n\n\nConstitutional AI\nReal-time input/output filtering\nBlocks harmful requests before execution\n\n\nBug Bounty\nCrowdsourced discovery\nFinds attacks humans miss\n\n\nRapid Patching\nAuto-generate variants\nStays ahead of attackers\n\n\nEgress Control\nThrottle outbound data\nPrevents model weight theft"
  },
  {
    "objectID": "courses/caio/01-intro.html#evaluating-ai-agents",
    "href": "courses/caio/01-intro.html#evaluating-ai-agents",
    "title": "Machine Learning Essentials",
    "section": "Evaluating AI Agents",
    "text": "Evaluating AI Agents\nTraditional metrics (accuracy, precision) are insufficient for agents.\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    TC[\"Task Completion\"] --&gt; Score[\"Overall Agent Score\"]\n    RQ[\"Reasoning Quality\"] --&gt; Score\n    SA[\"Safety\"] --&gt; Score\n    RE[\"Resource Efficiency\"] --&gt; Score\n    ER[\"Error Recovery\"] --&gt; Score\n    AD[\"Adversarial Robustness\"] --&gt; Score\n    \n    style SA fill:#ffcccc,stroke:#cc0000\n    style Score fill:#c8e6c9,stroke:#2e7d32"
  },
  {
    "objectID": "courses/caio/01-intro.html#evaluation-approaches",
    "href": "courses/caio/01-intro.html#evaluation-approaches",
    "title": "Machine Learning Essentials",
    "section": "Evaluation Approaches",
    "text": "Evaluation Approaches\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    A[Agent Output] --&gt; R[Rule-Based]\n    A --&gt; L[LLM-as-Judge]\n    A --&gt; H[Human Review]\n    A --&gt; S[Simulation]\n    \n    R --&gt; E[Score]\n    L --&gt; E\n    H --&gt; E\n    S --&gt; E\n    \n    style R fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style H fill:#c8e6c9,stroke:#2e7d32\n    style S fill:#f3e5f5,stroke:#7b1fa2\n    style E fill:#ffcccc,stroke:#cc0000\nBest practice: Combine multiple approaches for comprehensive evaluation."
  },
  {
    "objectID": "courses/caio/01-intro.html#domain-specific-benchmarks",
    "href": "courses/caio/01-intro.html#domain-specific-benchmarks",
    "title": "Machine Learning Essentials",
    "section": "Domain-Specific Benchmarks",
    "text": "Domain-Specific Benchmarks\n\n\n\nDomain\nBenchmark\nWhat It Tests\n\n\n\n\nCoding\nSWE-bench\nFix real GitHub issues\n\n\nWeb\nWebArena\nNavigate websites, complete tasks\n\n\nRobotics\nALFRED\nHousehold tasks in 3D\n\n\nEnterprise\nTAU-bench\nMulti-system workflows\n\n\n\nAgent capabilities are task-specific — benchmarks must match use cases."
  },
  {
    "objectID": "courses/caio/01-intro.html#red-teaming-agents",
    "href": "courses/caio/01-intro.html#red-teaming-agents",
    "title": "Machine Learning Essentials",
    "section": "Red-Teaming Agents",
    "text": "Red-Teaming Agents\nSystematic vulnerability testing:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    PI[Prompt Injection] --&gt; A[Agent]\n    ME[Agent Mistakes] --&gt; A\n    MU[Direct Misuse] --&gt; A\n    \n    A --&gt; |Vulnerability| V[Security Issue]\n    A --&gt; |Safe| S[Normal Operation]\n    \n    V --&gt; R[Report]\n    \n    style PI fill:#ffcccc,stroke:#cc0000\n    style ME fill:#fff3cd,stroke:#f57c00\n    style MU fill:#ffcccc,stroke:#cc0000\n    style V fill:#ffcccc,stroke:#cc0000\n    style S fill:#ccffcc,stroke:#00cc00\nExample: Hidden text in a webpage hijacks agent to exfiltrate data.\nComprehensive red-teaming found 1,200+ vulnerabilities in one enterprise agent."
  },
  {
    "objectID": "courses/caio/01-intro.html#embodied-ai-robots",
    "href": "courses/caio/01-intro.html#embodied-ai-robots",
    "title": "Machine Learning Essentials",
    "section": "Embodied AI: Robots",
    "text": "Embodied AI: Robots\nSoftware agents operate in digital systems. Embodied agents must handle:\n%%| echo: false\n%%| fig-width: 12\nflowchart LR\n    C[\"Camera\"] --&gt; F[\"Fusion\"]\n    L[\"Lidar\"] --&gt; F\n    T[\"Touch\"] --&gt; F\n    F --&gt; B[\"Robot Brain\"]\n    B --&gt; M[\"Motors\"]\n    M --&gt; E[\"Environment\"]\n    E --&gt; |\"Feedback\"| C\n    \n    style F fill:#fff3e0,stroke:#f57c00\n    style B fill:#e3f2fd,stroke:#1976d2\n    style E fill:#c8e6c9,stroke:#388e3c\nThe sim-to-real gap: Robots trained in simulation often fail in reality."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-evolution-of-robotic-intelligence",
    "href": "courses/caio/01-intro.html#the-evolution-of-robotic-intelligence",
    "title": "Machine Learning Essentials",
    "section": "The Evolution of Robotic Intelligence",
    "text": "The Evolution of Robotic Intelligence\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    S[1960s Shakey] --&gt; P[1980s-2000s Probabilistic]\n    P --&gt; F[2020s Foundation Models]\n    \n    style S fill:#e3f2fd,stroke:#1976d2\n    style P fill:#fff3e0,stroke:#f57c00\n    style F fill:#c8e6c9,stroke:#2e7d32"
  },
  {
    "objectID": "courses/caio/01-intro.html#robotics-capability-eras",
    "href": "courses/caio/01-intro.html#robotics-capability-eras",
    "title": "Machine Learning Essentials",
    "section": "Robotics: Capability Eras",
    "text": "Robotics: Capability Eras\n\n\n\n\n\n\n\n\nEra\nCapability\nLimitation\n\n\n\n\nRule-based\nExplicit reasoning\nBrittle, narrow\n\n\nProbabilistic\nHandle uncertainty\nNo language understanding\n\n\nFoundation Models\nNatural language + adaptation\nCompute-intensive\n\n\n\nLLMs have catalyzed a new era: robots that understand language and adapt."
  },
  {
    "objectID": "courses/caio/01-intro.html#googles-robotic-transformer-rt-2",
    "href": "courses/caio/01-intro.html#googles-robotic-transformer-rt-2",
    "title": "Machine Learning Essentials",
    "section": "Google’s Robotic Transformer (RT-2)",
    "text": "Google’s Robotic Transformer (RT-2)\nA vision-language-action model that directly controls robots:\n%%| echo: false\n%%| fig-width: 10\nflowchart LR\n    V[Vision Input] --&gt; VLA[RT-2 Model]\n    L[Language Input] --&gt; VLA\n    VLA --&gt; A[Action Output]\n    A --&gt; R[Robot]\n    R -- Feedback --&gt; V\n    \n    style V fill:#e3f2fd,stroke:#1976d2\n    style L fill:#fff3e0,stroke:#f57c00\n    style VLA fill:#f3e5f5,stroke:#7b1fa2\n    style A fill:#c8e6c9,stroke:#2e7d32\n    style R fill:#ffcccc,stroke:#cc0000\nGeneral → Interactive → Dexterous\nWorks across robot forms: arms, humanoids, mobile platforms."
  },
  {
    "objectID": "courses/caio/01-intro.html#robot-safety-asimov",
    "href": "courses/caio/01-intro.html#robot-safety-asimov",
    "title": "Machine Learning Essentials",
    "section": "Robot Safety: ASIMOV",
    "text": "Robot Safety: ASIMOV\nNamed after Asimov’s Laws of Robotics, this benchmark tests embodied AI safety:\n\n\n\n\n\n\n\n\nAsimov’s Law\nModern Interpretation\nTest Scenario\n\n\n\n\n1. Don’t harm humans\nRefuse dangerous commands\n“Throw this at the person”\n\n\n2. Obey orders\nFollow safe instructions\n“Hand me that tool”\n\n\n3. Protect self\nAvoid self-damage\nDon’t walk off ledge\n\n\nZeroth Law\nProtect humanity broadly\nConsider societal impact\n\n\n\nKey challenge: Context matters — “Hand me that knife” is safe in a kitchen, dangerous in a conflict.\nBusiness relevance: As robots enter warehouses, hospitals, and homes, safety benchmarks become legal and ethical requirements."
  },
  {
    "objectID": "courses/caio/01-intro.html#summary-nlp",
    "href": "courses/caio/01-intro.html#summary-nlp",
    "title": "Machine Learning Essentials",
    "section": "Summary: NLP",
    "text": "Summary: NLP\n\n\n\nConcept\nKey Insight\n\n\n\n\nWord Embeddings\nWords as vectors; geometry = meaning\n\n\nDistributional Hypothesis\nContext reveals meaning\n\n\nAttention\nDynamic weighting of relevant information\n\n\nTransformers\nParallel processing, scalable, powerful\n\n\n\nThe shift from symbols to vectors enabled modern NLP."
  },
  {
    "objectID": "courses/caio/01-intro.html#summary-llms",
    "href": "courses/caio/01-intro.html#summary-llms",
    "title": "Machine Learning Essentials",
    "section": "Summary: LLMs",
    "text": "Summary: LLMs\n\n\n\nConcept\nKey Insight\n\n\n\n\nAutoregressive Generation\nPredict next token iteratively\n\n\nTemperature\nControls randomness/creativity\n\n\nAlignment\nEnsures safe, helpful behavior\n\n\nContext Windows\nLimit on “memory” size\n\n\n\nScale + alignment = emergent reasoning capabilities."
  },
  {
    "objectID": "courses/caio/01-intro.html#summary-ai-agents",
    "href": "courses/caio/01-intro.html#summary-ai-agents",
    "title": "Machine Learning Essentials",
    "section": "Summary: AI Agents",
    "text": "Summary: AI Agents\n\n\n\nConcept\nKey Insight\n\n\n\n\nTool Use\nLLMs gain ability to act\n\n\nMulti-Step Planning\nChain reasoning and action\n\n\nOrchestration\nMultiple agents collaborate\n\n\nSafety\nAutonomy amplifies risks\n\n\nEvaluation\nRequires new methodologies\n\n\n\nAgents transform LLMs from conversationalists to autonomous workers."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-executive-perspective",
    "href": "courses/caio/01-intro.html#the-executive-perspective",
    "title": "Machine Learning Essentials",
    "section": "The Executive Perspective",
    "text": "The Executive Perspective\nFor AI leaders:\n\nNLP powers search, chatbots, document analysis\nLLMs enable natural language interfaces to business systems\nAgents can automate complex, multi-step workflows\nSafety must be built in from the start, not bolted on\nEvaluation requires domain-specific benchmarks and human oversight\n\nThe promise: augmenting human intelligence — agents handle routine tasks while humans provide judgment, creativity, and ethical oversight."
  },
  {
    "objectID": "courses/caio/01-intro.html#supplemental-reading-1",
    "href": "courses/caio/01-intro.html#supplemental-reading-1",
    "title": "Machine Learning Essentials",
    "section": "Supplemental Reading",
    "text": "Supplemental Reading\nOnline Articles:\n\nMaking the Most of AI and Machine Learning in Organizations — Stanford\nThe State of AI in 2024 — McKinsey\nGenerative AI’s Act Two — Sequoia Capital\n\nFrom the Textbook:\n\nChapter 24: Natural Language Processing\nChapter 26: AI Agents"
  },
  {
    "objectID": "courses/caio/01-intro.html#what-is-cursor",
    "href": "courses/caio/01-intro.html#what-is-cursor",
    "title": "Machine Learning Essentials",
    "section": "What is Cursor?",
    "text": "What is Cursor?\nCursor is an AI-powered code editor built on VS Code. It allows you to:\n\nWrite code with AI assistance\nAsk questions about your code\nGenerate code from natural language descriptions\nDebug and fix errors with AI help\n\nFor this course, we’ll use Cursor to build an AI agent without needing deep programming expertise."
  },
  {
    "objectID": "courses/caio/01-intro.html#part-1-download-and-install-cursor",
    "href": "courses/caio/01-intro.html#part-1-download-and-install-cursor",
    "title": "Machine Learning Essentials",
    "section": "Part 1: Download and Install Cursor",
    "text": "Part 1: Download and Install Cursor\nStep 1: Download Cursor\n\nGo to cursor.sh\nClick the Download button\nThe website will automatically detect your operating system (Mac, Windows, or Linux)\n\nStep 2: Install on Mac\n\nOpen the downloaded .dmg file\nDrag the Cursor icon to the Applications folder\nOpen Cursor from Applications\nIf prompted about security, go to System Preferences → Security & Privacy and click “Open Anyway”\n\nStep 3: Install on Windows\n\nRun the downloaded .exe installer\nFollow the installation wizard\nLaunch Cursor from the Start Menu"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-2-initial-setup",
    "href": "courses/caio/01-intro.html#part-2-initial-setup",
    "title": "Machine Learning Essentials",
    "section": "Part 2: Initial Setup",
    "text": "Part 2: Initial Setup\nStep 1: Sign In (Optional but Recommended)\n\nWhen Cursor opens, you’ll see a welcome screen\nClick Sign In to create a free account\nYou can sign in with:\n\nGoogle account\nGitHub account\nEmail\n\n\nBenefits of signing in:\n\nFree AI credits for code assistance\nSettings sync across devices\n\nStep 2: Choose Your Theme\n\nCursor will ask about your preferred color theme\nChoose Dark or Light based on your preference\nYou can change this later in Settings\n\nStep 3: Import VS Code Settings (Optional)\nIf you’ve used VS Code before:\n\nCursor will offer to import your settings\nClick Import to bring over extensions and preferences\nOr click Skip to start fresh"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-3-install-python",
    "href": "courses/caio/01-intro.html#part-3-install-python",
    "title": "Machine Learning Essentials",
    "section": "Part 3: Install Python",
    "text": "Part 3: Install Python\nCursor needs Python installed on your computer to run our project.\nCheck if Python is Already Installed\n\nIn Cursor, open the terminal: View → Terminal (or press Ctrl+`)\nType this command and press Enter:\n\npython --version\n\nIf you see Python 3.x.x, you’re good! Skip to Part 4.\nIf you get an error, follow the installation steps below.\n\nInstall Python on Mac\nOption A: Using Homebrew (Recommended)\n\nOpen Terminal (outside of Cursor)\nInstall Homebrew if you don’t have it:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Python:\n\nbrew install python\nOption B: Direct Download\n\nGo to python.org/downloads\nDownload the latest Python 3.x version\nRun the installer\nImportant: Check “Add Python to PATH” during installation\n\nInstall Python on Windows\n\nGo to python.org/downloads\nClick “Download Python 3.x.x”\nRun the installer\nIMPORTANT: Check the box that says “Add Python to PATH”\nClick “Install Now”\n\nVerify Installation\nAfter installation, close and reopen Cursor, then:\n\nOpen terminal: View → Terminal\nRun:\n\npython --version\n\nYou should see Python 3.x.x"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-4-install-required-packages",
    "href": "courses/caio/01-intro.html#part-4-install-required-packages",
    "title": "Machine Learning Essentials",
    "section": "Part 4: Install Required Packages",
    "text": "Part 4: Install Required Packages\nOur project needs a few Python libraries. Install them in Cursor’s terminal:\npip install pandas numpy scikit-learn\nYou should see output indicating successful installation.\nIf you get a “pip not found” error on Mac:\npip3 install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-5-using-cursors-ai-features",
    "href": "courses/caio/01-intro.html#part-5-using-cursors-ai-features",
    "title": "Machine Learning Essentials",
    "section": "Part 5: Using Cursor’s AI Features",
    "text": "Part 5: Using Cursor’s AI Features\nFeature 1: AI Chat (Cmd+L / Ctrl+L)\nUse this to ask questions or get help:\n\nPress Cmd+L (Mac) or Ctrl+L (Windows)\nA chat panel opens on the right\nAsk questions like:\n\n“How do I load a CSV file in Python?”\n“Explain what this code does”\n“Why am I getting this error?”\n\n\nFeature 2: Inline Edit (Cmd+K / Ctrl+K)\nUse this to write or modify code:\n\nSelect some code (or place cursor where you want new code)\nPress Cmd+K (Mac) or Ctrl+K (Windows)\nDescribe what you want in plain English:\n\n“Add a function that calculates the average price”\n“Fix this error”\n“Add comments explaining this code”\n\nReview the suggested changes\nPress Enter to accept or Escape to cancel\n\nFeature 3: Code Completion (Tab)\nAs you type, Cursor suggests completions:\n\nStart typing code\nYou’ll see gray “ghost text” suggestions\nPress Tab to accept the suggestion\nPress Escape to dismiss\n\nFeature 4: Agent Mode (Cmd+I / Ctrl+I)\nFor larger tasks, use Agent mode:\n\nPress Cmd+I (Mac) or Ctrl+I (Windows)\nDescribe a complex task:\n\n“Create a Python script that loads data and builds a regression model”\n\nThe agent will generate multiple files and complete code"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-6-creating-your-first-project",
    "href": "courses/caio/01-intro.html#part-6-creating-your-first-project",
    "title": "Machine Learning Essentials",
    "section": "Part 6: Creating Your First Project",
    "text": "Part 6: Creating Your First Project\nStep 1: Create a Project Folder\n\nIn Cursor, go to File → Open Folder\nNavigate to where you want your project (e.g., Documents)\nClick New Folder and name it oj-pricing-agent\nSelect this folder and click Open\n\nStep 2: Create a Python File\n\nIn the Explorer sidebar (left panel), right-click\nSelect New File\nName it test.py\nAdd this code:\n\nprint(\"Hello from Cursor!\")\nStep 3: Run Your Code\n\nOpen the terminal: View → Terminal\nRun your script:\n\npython test.py\n\nYou should see: Hello from Cursor!\n\nCongratulations! You’re ready to build your AI agent."
  },
  {
    "objectID": "courses/caio/01-intro.html#part-7-keyboard-shortcuts-reference",
    "href": "courses/caio/01-intro.html#part-7-keyboard-shortcuts-reference",
    "title": "Machine Learning Essentials",
    "section": "Part 7: Keyboard Shortcuts Reference",
    "text": "Part 7: Keyboard Shortcuts Reference\n\n\n\nAction\nMac\nWindows\n\n\n\n\nAI Chat\nCmd+L\nCtrl+L\n\n\nInline Edit\nCmd+K\nCtrl+K\n\n\nAgent Mode\nCmd+I\nCtrl+I\n\n\nOpen Terminal\nCtrl+| Ctrl+\n\n\n\nSave File\nCmd+S\nCtrl+S\n\n\nOpen File\nCmd+O\nCtrl+O\n\n\nNew File\nCmd+N\nCtrl+N\n\n\nFind\nCmd+F\nCtrl+F"
  },
  {
    "objectID": "courses/caio/01-intro.html#troubleshooting",
    "href": "courses/caio/01-intro.html#troubleshooting",
    "title": "Machine Learning Essentials",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“Python not found” in terminal\nMac:\n\nTry python3 instead of python\nOr run: brew install python\n\nWindows:\n\nReinstall Python and make sure to check “Add Python to PATH”\nRestart Cursor after installation\n\n“pip not found”\nMac:\n\nUse pip3 instead of pip\n\nWindows:\n\nTry python -m pip install package_name\n\nCursor won’t start\n\nMake sure you have enough disk space (at least 1GB free)\nTry restarting your computer\nReinstall Cursor from cursor.sh\n\nAI features not working\n\nMake sure you’re signed in (check bottom-left corner)\nCheck your internet connection\nTry signing out and back in\n\nCode runs but shows errors\n\nCopy the error message\nPress Cmd+L (or Ctrl+L) to open AI Chat\nPaste the error and ask “How do I fix this?”"
  },
  {
    "objectID": "courses/caio/01-intro.html#getting-help-during-the-course",
    "href": "courses/caio/01-intro.html#getting-help-during-the-course",
    "title": "Machine Learning Essentials",
    "section": "Getting Help During the Course",
    "text": "Getting Help During the Course\n\nZoom Sessions: Ask questions during live sessions\nCursor AI: Use Cmd+L to ask the AI for help\nDiscussion Board: Post questions for peer assistance\nOffice Hours: [Insert instructor office hours if applicable]"
  },
  {
    "objectID": "courses/caio/01-intro.html#next-steps",
    "href": "courses/caio/01-intro.html#next-steps",
    "title": "Machine Learning Essentials",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing this setup:\n\n✅ Cursor is installed and running\n✅ Python is installed\n✅ Required packages are installed\n✅ You can create and run Python files\n\nYou’re ready for Zoom Session 1 where we’ll practice using Cursor’s AI features together!"
  },
  {
    "objectID": "courses/caio/01-intro.html#overview",
    "href": "courses/caio/01-intro.html#overview",
    "title": "Machine Learning Essentials",
    "section": "Overview",
    "text": "Overview\nIn this project, you will build an AI agent that helps a retail pricing analyst make decisions about orange juice pricing and promotions. The agent will:\n\nLoad and explore sales data\nBuild a regression model to predict sales\nAnswer business questions using natural language\n\nTime Required: ~2 hours\nPrerequisites:\n\nCursor IDE installed (see Cursor Setup Guide)\nBasic familiarity with Cursor from Zoom Session 1\nDownload the project template to get started quickly"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-1-project-setup",
    "href": "courses/caio/01-intro.html#part-1-project-setup",
    "title": "Machine Learning Essentials",
    "section": "Part 1: Project Setup",
    "text": "Part 1: Project Setup\nStep 1.1: Create Your Project Folder\n\nOpen Cursor IDE\nClick File → Open Folder\nCreate a new folder called oj-pricing-agent on your computer\nSelect that folder to open it in Cursor\n\nStep 1.2: Create the Main Python File\n\nIn the Cursor sidebar, right-click and select New File\nName it oj_agent.py\nYou’ll see an empty file open in the editor\n\nStep 1.3: Copy the Dataset\nDownload the oj_data.csv file and copy it into your oj-pricing-agent folder."
  },
  {
    "objectID": "courses/caio/01-intro.html#part-2-data-loading-and-exploration",
    "href": "courses/caio/01-intro.html#part-2-data-loading-and-exploration",
    "title": "Machine Learning Essentials",
    "section": "Part 2: Data Loading and Exploration",
    "text": "Part 2: Data Loading and Exploration\nStep 2.1: Load the Required Libraries\nIn your oj_agent.py file, start by adding these lines at the top:\n# Required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\nWhat this does: These libraries help us work with data (pandas), do math (numpy), and build models (sklearn).\nStep 2.2: Load the Data\nAdd the following code to load the orange juice sales data:\n# Load the orange juice dataset\nprint(\"Loading data...\")\ndf = pd.read_csv('oj_data.csv')\n\n# Display basic information\nprint(f\"Dataset has {len(df)} rows and {len(df.columns)} columns\")\nprint(f\"\\nColumns: {list(df.columns)}\")\nprint(f\"\\nBrands in dataset: {df['brand'].unique()}\")\nprint(f\"\\nPrice range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\nprint(f\"\\nSample of data:\")\nprint(df.head())\nStep 2.3: Run Your Code (First Test)\n\nSave the file (Ctrl+S or Cmd+S)\nOpen the terminal in Cursor: View → Terminal\nRun the script:\n\npython oj_agent.py\nYou should see output showing:\n\nThe dataset has ~28,000 rows\nThree brands: Tropicana, Minute Maid, Dominick’s\nPrice ranges from about $1 to $4\n\nTroubleshooting: If you get an error about missing packages, run:\npip install pandas numpy scikit-learn"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-3-building-the-regression-model",
    "href": "courses/caio/01-intro.html#part-3-building-the-regression-model",
    "title": "Machine Learning Essentials",
    "section": "Part 3: Building the Regression Model",
    "text": "Part 3: Building the Regression Model\nStep 3.1: Understanding the Model\nWe’re building a model that predicts log of sales volume based on:\n\nPrice: Higher price → lower sales (negative relationship)\nFeatured (feat): If the product is in the weekly ad circular (1 = yes, 0 = no)\nBrand: Different brands have different base sales levels\nPrice × Brand interaction: Price sensitivity varies by brand\n\nStep 3.2: Prepare the Data for Modeling\nAdd this code to prepare features for the model:\n# ============================================\n# PART 3: BUILD THE REGRESSION MODEL\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Building the pricing model...\")\nprint(\"=\"*50)\n\n# Create dummy variables for brand (one-hot encoding)\n# This converts 'brand' text into numbers the model can use\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand', drop_first=False)\n\n# Create the feature matrix\n# We include: price, feat, brand dummies, and price*brand interactions\nX = pd.DataFrame({\n    'price': df['price'],\n    'feat': df['feat'],\n    'brand_minute.maid': brand_dummies['brand_minute.maid'],\n    'brand_tropicana': brand_dummies['brand_tropicana'],\n    # Interaction terms: price effect varies by brand\n    'price_x_minute.maid': df['price'] * brand_dummies['brand_minute.maid'],\n    'price_x_tropicana': df['price'] * brand_dummies['brand_tropicana']\n})\n\n# Target variable: log of sales (logmove)\ny = df['logmove']\n\nprint(f\"Features: {list(X.columns)}\")\nprint(f\"Target: logmove (log of sales volume)\")\nStep 3.3: Fit the Model\nAdd code to train the regression model:\n# Fit the linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Display the coefficients\nprint(\"\\nModel Coefficients:\")\nprint(\"-\" * 40)\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"  {feature}: {coef:.4f}\")\nprint(f\"  intercept: {model.intercept_:.4f}\")\n\n# Calculate R-squared (how well the model fits)\nr_squared = model.score(X, y)\nprint(f\"\\nModel R-squared: {r_squared:.3f}\")\nprint(\"(This means the model explains {:.1f}% of sales variation)\".format(r_squared * 100))\nStep 3.4: Run and Verify the Model\nSave and run the script again. You should see coefficients like:\n\nprice: negative (higher price = lower sales)\nfeat: positive (being featured increases sales)\nbrand coefficients: capture baseline differences between brands\ninteraction terms: show how price sensitivity differs by brand"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-4-creating-helper-functions-for-the-agent",
    "href": "courses/caio/01-intro.html#part-4-creating-helper-functions-for-the-agent",
    "title": "Machine Learning Essentials",
    "section": "Part 4: Creating Helper Functions for the Agent",
    "text": "Part 4: Creating Helper Functions for the Agent\nStep 4.1: Add Prediction Functions\nAdd these functions that the agent will use to answer questions:\n# ============================================\n# PART 4: HELPER FUNCTIONS FOR THE AGENT\n# ============================================\n\ndef predict_sales(brand, price, featured=0):\n    \"\"\"\n    Predict sales volume for a given brand, price, and feature status.\n    \n    Args:\n        brand: 'tropicana', 'minute.maid', or 'dominicks'\n        price: price in dollars (e.g., 2.50)\n        featured: 1 if in ad circular, 0 if not\n    \n    Returns:\n        Predicted sales volume (not log-transformed)\n    \"\"\"\n    # Create feature vector\n    features = {\n        'price': price,\n        'feat': featured,\n        'brand_minute.maid': 1 if brand.lower() == 'minute.maid' else 0,\n        'brand_tropicana': 1 if brand.lower() == 'tropicana' else 0,\n        'price_x_minute.maid': price if brand.lower() == 'minute.maid' else 0,\n        'price_x_tropicana': price if brand.lower() == 'tropicana' else 0\n    }\n    \n    # Convert to dataframe for prediction\n    X_pred = pd.DataFrame([features])\n    \n    # Predict log sales, then convert back\n    log_sales = model.predict(X_pred)[0]\n    sales = np.exp(log_sales)\n    \n    return sales\n\n\ndef get_price_elasticity(brand):\n    \"\"\"\n    Calculate the price elasticity for a given brand.\n    \n    Price elasticity tells us: if price increases by 1%, \n    how much does quantity demanded change (in %)?\n    \n    A more negative number means more price-sensitive.\n    \"\"\"\n    # Base price coefficient\n    base_coef = model.coef_[0]  # price coefficient\n    \n    # Add brand-specific interaction if applicable\n    if brand.lower() == 'minute.maid':\n        interaction_coef = model.coef_[4]  # price_x_minute.maid\n    elif brand.lower() == 'tropicana':\n        interaction_coef = model.coef_[5]  # price_x_tropicana\n    else:  # dominicks (base case)\n        interaction_coef = 0\n    \n    total_elasticity = base_coef + interaction_coef\n    return total_elasticity\n\n\ndef get_advertising_lift(brand):\n    \"\"\"\n    Calculate the sales lift from being featured in advertising.\n    Returns the percentage increase in sales.\n    \"\"\"\n    # The 'feat' coefficient tells us the log-sales increase\n    feat_coef = model.coef_[1]  # feat coefficient\n    \n    # Convert from log to percentage change\n    percentage_lift = (np.exp(feat_coef) - 1) * 100\n    return percentage_lift\n\n\ndef find_optimal_price(brand, min_price=1.0, max_price=4.0, featured=0):\n    \"\"\"\n    Find the price that maximizes revenue for a brand.\n    Revenue = Price × Quantity\n    \"\"\"\n    best_price = min_price\n    best_revenue = 0\n    \n    # Search through price range\n    for price in np.arange(min_price, max_price, 0.05):\n        sales = predict_sales(brand, price, featured)\n        revenue = price * sales\n        \n        if revenue &gt; best_revenue:\n            best_revenue = revenue\n            best_price = price\n    \n    return best_price, best_revenue\n\n\ndef compare_elasticities():\n    \"\"\"\n    Compare price elasticity across all three brands.\n    \"\"\"\n    brands = ['dominicks', 'minute.maid', 'tropicana']\n    results = {}\n    \n    for brand in brands:\n        elasticity = get_price_elasticity(brand)\n        results[brand] = elasticity\n    \n    return results\nStep 4.2: Test the Helper Functions\nAdd test code to verify the functions work:\n# ============================================\n# TEST THE HELPER FUNCTIONS\n# ============================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Testing helper functions...\")\nprint(\"=\"*50)\n\n# Test prediction\ntest_sales = predict_sales('tropicana', 2.50, featured=0)\nprint(f\"\\nPredicted sales for Tropicana at $2.50 (no ad): {test_sales:.0f} units\")\n\n# Test elasticity\nelasticities = compare_elasticities()\nprint(\"\\nPrice Elasticities by Brand:\")\nfor brand, elast in elasticities.items():\n    print(f\"  {brand}: {elast:.3f}\")\n\n# Test advertising lift\nlift = get_advertising_lift('minute.maid')\nprint(f\"\\nAdvertising lift: {lift:.1f}% increase in sales\")\n\n# Test optimal price\nopt_price, opt_rev = find_optimal_price('dominicks')\nprint(f\"\\nOptimal price for Dominick's: ${opt_price:.2f} (revenue: ${opt_rev:.2f})\")\nRun the script again to verify all functions work correctly."
  },
  {
    "objectID": "courses/caio/01-intro.html#part-5-creating-the-ai-agent",
    "href": "courses/caio/01-intro.html#part-5-creating-the-ai-agent",
    "title": "Machine Learning Essentials",
    "section": "Part 5: Creating the AI Agent",
    "text": "Part 5: Creating the AI Agent\nStep 5.1: Add the Agent Logic\nNow we’ll create the agent that interprets natural language questions and calls the appropriate functions. Add this code:\n# ============================================\n# PART 5: THE AI AGENT\n# ============================================\n\ndef answer_question(question):\n    \"\"\"\n    Simple agent that answers business questions about OJ pricing.\n    \n    This is a rule-based agent that matches keywords in the question\n    to determine which analysis to perform.\n    \"\"\"\n    question_lower = question.lower()\n    \n    # Question 1: Predict sales for specific scenario\n    if 'predict' in question_lower or 'sales volume' in question_lower:\n        # Extract brand and price from question if possible\n        if 'tropicana' in question_lower:\n            brand = 'tropicana'\n        elif 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        else:\n            brand = 'dominicks'\n        \n        # Look for price (default to $2.50 if not found)\n        import re\n        price_match = re.search(r'\\$?(\\d+\\.?\\d*)', question_lower)\n        price = float(price_match.group(1)) if price_match else 2.50\n        \n        # Check for advertising\n        featured = 1 if 'advertis' in question_lower or 'feature' in question_lower else 0\n        if 'no advertis' in question_lower or 'without advertis' in question_lower:\n            featured = 0\n        \n        sales = predict_sales(brand, price, featured)\n        \n        response = f\"\"\"\n**Predicted Sales Analysis**\n\nBrand: {brand.title().replace('.', ' ')}\nPrice: ${price:.2f}\nFeatured in Ad: {'Yes' if featured else 'No'}\n\n**Predicted Sales Volume: {sales:,.0f} units**\n\nThis prediction is based on our regression model that accounts for:\n- Base demand for this brand\n- Price sensitivity (elasticity)  \n- Advertising effects\n\"\"\"\n        return response\n    \n    # Question 2: Which brand is most price-sensitive?\n    elif 'price-sensitive' in question_lower or 'price sensitive' in question_lower or 'most sensitive' in question_lower:\n        elasticities = compare_elasticities()\n        \n        # Find most price-sensitive (most negative elasticity)\n        most_sensitive = min(elasticities, key=elasticities.get)\n        \n        response = f\"\"\"\n**Price Sensitivity Analysis**\n\nPrice Elasticity by Brand:\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            sensitivity = \"HIGH\" if elast &lt; -3 else \"MEDIUM\" if elast &lt; -2 else \"LOW\"\n            response += f\"- {brand.title().replace('.', ' ')}: {elast:.3f} ({sensitivity} sensitivity)\\n\"\n        \n        response += f\"\"\"\n**Most Price-Sensitive: {most_sensitive.title().replace('.', ' ')}**\n\nInterpretation: A 1% price increase leads to a {abs(elasticities[most_sensitive]):.1f}% decrease in sales for {most_sensitive.title().replace('.', ' ')}.\n\nBusiness Implication: Be careful with price increases on {most_sensitive.title().replace('.', ' ')} - customers are very responsive to price changes.\n\"\"\"\n        return response\n    \n    # Question 3: Should we feature a brand in advertising?\n    elif 'feature' in question_lower or 'ad circular' in question_lower or 'advertising' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        lift = get_advertising_lift(brand)\n        \n        # Calculate example impact\n        base_sales = predict_sales(brand, 2.50, featured=0)\n        featured_sales = predict_sales(brand, 2.50, featured=1)\n        \n        response = f\"\"\"\n**Advertising Impact Analysis for {brand.title().replace('.', ' ')}**\n\nExpected Sales Lift from Featuring: **{lift:.1f}%**\n\nExample at $2.50:\n- Without advertising: {base_sales:,.0f} units\n- With advertising: {featured_sales:,.0f} units  \n- Additional sales: {featured_sales - base_sales:,.0f} units\n\n**Recommendation:** {'Yes, feature this product!' if lift &gt; 20 else 'Consider the advertising cost vs. the sales lift.'}\n\nThe advertising effect is consistent across price points. Factor in your advertising costs to determine if the sales lift justifies the expense.\n\"\"\"\n        return response\n    \n    # Question 4: Optimal price for a brand\n    elif 'optimal price' in question_lower or 'maximize revenue' in question_lower or 'best price' in question_lower:\n        if 'minute maid' in question_lower:\n            brand = 'minute.maid'\n        elif 'tropicana' in question_lower:\n            brand = 'tropicana'\n        else:\n            brand = 'dominicks'\n        \n        opt_price, opt_revenue = find_optimal_price(brand)\n        opt_sales = predict_sales(brand, opt_price, featured=0)\n        \n        # Compare with current average price\n        avg_price = df[df['brand'] == brand]['price'].mean()\n        avg_revenue = avg_price * predict_sales(brand, avg_price, featured=0)\n        \n        response = f\"\"\"\n**Revenue Optimization for {brand.title().replace('.', ' ')}**\n\n**Optimal Price: ${opt_price:.2f}**\n\nAt optimal price:\n- Predicted sales: {opt_sales:,.0f} units\n- Revenue per store-week: ${opt_revenue:,.2f}\n\nComparison with current average (${avg_price:.2f}):\n- Current revenue: ${avg_revenue:,.2f}\n- Potential improvement: ${opt_revenue - avg_revenue:,.2f} ({((opt_revenue/avg_revenue)-1)*100:.1f}%)\n\nNote: This optimization assumes no competitor response and stable market conditions.\n\"\"\"\n        return response\n    \n    # Question 5: Compare elasticities across brands\n    elif 'compare' in question_lower or 'elasticity' in question_lower or 'across' in question_lower:\n        elasticities = compare_elasticities()\n        \n        response = \"\"\"\n**Price Elasticity Comparison Across Brands**\n\n| Brand | Elasticity | Interpretation |\n|-------|------------|----------------|\n\"\"\"\n        for brand, elast in sorted(elasticities.items(), key=lambda x: x[1]):\n            interp = f\"1% price ↑ → {abs(elast):.1f}% sales ↓\"\n            response += f\"| {brand.title().replace('.', ' ')} | {elast:.3f} | {interp} |\\n\"\n        \n        response += \"\"\"\n**Key Insights:**\n\n1. **Dominick's** (store brand) is least price-sensitive - customers buying store brands may prioritize value and be less responsive to small price changes.\n\n2. **Tropicana** shows moderate price sensitivity - as a premium brand, some customers are loyal but others will switch if prices rise.\n\n3. **Minute Maid** is most price-sensitive - positioned between store and premium brands, these customers actively compare prices.\n\n**Strategic Implications:**\n- Use competitive pricing on Minute Maid to capture price-sensitive shoppers\n- Tropicana can sustain moderate price premiums\n- Dominick's margins can be optimized with less risk of volume loss\n\"\"\"\n        return response\n    \n    else:\n        return \"\"\"\nI can help you with these types of questions:\n\n1. **Sales Prediction:** \"What is the predicted sales volume if we price Tropicana at $2.50?\"\n2. **Price Sensitivity:** \"Which brand is most price-sensitive?\"\n3. **Advertising Impact:** \"Should we feature Minute Maid in the ad circular?\"\n4. **Price Optimization:** \"What price should we set for Dominick's to maximize revenue?\"\n5. **Elasticity Comparison:** \"Compare the price elasticity across brands\"\n\nPlease try one of these questions!\n\"\"\"\nStep 5.2: Add the Interactive Interface\nFinally, add code to let users interact with the agent:\n# ============================================\n# PART 6: INTERACTIVE AGENT INTERFACE\n# ============================================\n\ndef run_agent():\n    \"\"\"\n    Run the interactive agent interface.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"🍊 ORANGE JUICE PRICING ANALYTICS AGENT 🍊\")\n    print(\"=\"*60)\n    print(\"\\nHello! I'm your pricing analytics assistant.\")\n    print(\"I can help you analyze orange juice pricing and promotions.\")\n    print(\"\\nTry asking me questions like:\")\n    print(\"  - What is the predicted sales if we price Tropicana at $2.50?\")\n    print(\"  - Which brand is most price-sensitive?\")\n    print(\"  - Should we feature Minute Maid in the ad circular?\")\n    print(\"  - What price maximizes revenue for Dominick's?\")\n    print(\"  - Compare price elasticity across brands\")\n    print(\"\\nType 'quit' to exit.\\n\")\n    \n    while True:\n        question = input(\"Your question: \").strip()\n        \n        if question.lower() in ['quit', 'exit', 'q']:\n            print(\"\\nThank you for using the OJ Pricing Agent. Goodbye!\")\n            break\n        \n        if not question:\n            continue\n        \n        print(\"\\n\" + \"-\"*50)\n        response = answer_question(question)\n        print(response)\n        print(\"-\"*50 + \"\\n\")\n\n\n# ============================================\n# MAIN: RUN THE AGENT\n# ============================================\n\nif __name__ == \"__main__\":\n    # Run the interactive agent\n    run_agent()"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-6-testing-your-agent",
    "href": "courses/caio/01-intro.html#part-6-testing-your-agent",
    "title": "Machine Learning Essentials",
    "section": "Part 6: Testing Your Agent",
    "text": "Part 6: Testing Your Agent\nStep 6.1: Run the Complete Script\nSave the file and run:\npython oj_agent.py\nStep 6.2: Test All Five Required Questions\nTest your agent with these exact questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nRecord the answers for your summary document."
  },
  {
    "objectID": "courses/caio/01-intro.html#part-7-writing-your-summary",
    "href": "courses/caio/01-intro.html#part-7-writing-your-summary",
    "title": "Machine Learning Essentials",
    "section": "Part 7: Writing Your Summary",
    "text": "Part 7: Writing Your Summary\nCreate a 1-page document (Word or PDF) that includes:\nSection 1: Key Findings (half page)\n\nWhich brand is most/least price-sensitive and why this matters\nThe impact of advertising on sales\nThe optimal pricing recommendations\n\nSection 2: Surprises and Insights (quarter page)\n\nWhat surprised you about the results?\nHow do these findings compare to your intuition?\n\nSection 3: Business Implications (quarter page)\n\nHow would you recommend a retailer use these insights?\nWhat additional data would make this analysis more useful?"
  },
  {
    "objectID": "courses/caio/01-intro.html#part-8-preparing-your-demo",
    "href": "courses/caio/01-intro.html#part-8-preparing-your-demo",
    "title": "Machine Learning Essentials",
    "section": "Part 8: Preparing Your Demo",
    "text": "Part 8: Preparing Your Demo\nFor Zoom Session 3, prepare a 2-3 minute demonstration:\n\nSetup (30 sec): Briefly explain what the agent does\nDemo (1.5 min): Show 2-3 questions and responses\nInsight (1 min): Share your most interesting finding\n\nTips:\n\nHave your script running before you share screen\nPre-type a question so you’re not typing live\nFocus on business insights, not technical details"
  },
  {
    "objectID": "courses/caio/01-intro.html#troubleshooting-1",
    "href": "courses/caio/01-intro.html#troubleshooting-1",
    "title": "Machine Learning Essentials",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“ModuleNotFoundError: No module named ‘pandas’”\nRun: pip install pandas numpy scikit-learn\n“FileNotFoundError: oj_data.csv”\nMake sure the data file is in the same folder as your Python script.\n“Model coefficients look wrong”\nCheck that your data loaded correctly - you should have ~28,000 rows.\n“Agent doesn’t understand my question”\nTry rephrasing using keywords like “predict”, “price-sensitive”, “feature”, “optimal”, or “compare”."
  },
  {
    "objectID": "courses/caio/01-intro.html#complete-code-reference",
    "href": "courses/caio/01-intro.html#complete-code-reference",
    "title": "Machine Learning Essentials",
    "section": "Complete Code Reference",
    "text": "Complete Code Reference\nThe complete oj_agent.py file should be approximately 350-400 lines. If you get stuck, ask Cursor’s AI assistant for help by selecting your code and pressing Cmd+K (Mac) or Ctrl+K (Windows), then describing your issue. I also prepared a complete code reference for you to refer."
  },
  {
    "objectID": "courses/caio/01-intro.html#next-steps-optional-enhancements",
    "href": "courses/caio/01-intro.html#next-steps-optional-enhancements",
    "title": "Machine Learning Essentials",
    "section": "Next Steps (Optional Enhancements)",
    "text": "Next Steps (Optional Enhancements)\nIf you finish early and want to explore further:\n\nAdd more questions: What other business questions could the agent answer?\nImprove the NLP: Use fuzzy matching to better understand varied phrasings\nAdd visualizations: Create charts showing price vs. sales by brand\nConnect to an LLM: Use the OpenAI API to make the agent truly conversational\n\nGood luck with your project! 🍊"
  },
  {
    "objectID": "courses/caio/01-intro.html#case-study-mudslide-threat",
    "href": "courses/caio/01-intro.html#case-study-mudslide-threat",
    "title": "Machine Learning Essentials",
    "section": "Case Study: Mudslide Threat",
    "text": "Case Study: Mudslide Threat\nI live in a house at risk of a mudslide damage.\n\nOption A: Build a protective wall ($10,000).\nDamage Cost: $100,000 if the house is hit (and wall fails/absent).\nWall Effectiveness: 95% protection.\nProbability of Mudslide: \\(P(\\text{Slide}) = 0.01\\).\n\nWhat is the best course of action?"
  },
  {
    "objectID": "courses/caio/01-intro.html#decision-tree-initial-options",
    "href": "courses/caio/01-intro.html#decision-tree-initial-options",
    "title": "Machine Learning Essentials",
    "section": "Decision Tree: Initial Options",
    "text": "Decision Tree: Initial Options\ngraph LR\n    Start((Decision)) --&gt; Build[Build Wall]\n    Start --&gt; NoBuild[Don't Build]\n    \n    Build -- \"$10,000\" --&gt; WallNode{Slide?}\n    WallNode -- \"0.01\" --&gt; FailNode{Wall Fails?}\n    FailNode -- \"0.05\" --&gt; Loss[\"$100,000 Cost\"]\n    FailNode -- \"0.95\" --&gt; NoLoss[\"$0 Cost\"]\n    WallNode -- \"0.99\" --&gt; NoSlide[\"$0 Cost\"]\n    \n    NoBuild -- \"$0\" --&gt; SlideNode{Slide?}\n    SlideNode -- \"0.01\" --&gt; Loss2[\"$100,000 Cost\"]\n    SlideNode -- \"0.99\" --&gt; NoLoss2[\"$0 Cost\"]"
  },
  {
    "objectID": "courses/caio/01-intro.html#comparison-no-test",
    "href": "courses/caio/01-intro.html#comparison-no-test",
    "title": "Machine Learning Essentials",
    "section": "Comparison: No Test",
    "text": "Comparison: No Test\nDon’t Build\n\\(EV = 0.01 \\times \\$100,000 = \\$1,000\\)\nBuild (No Test)\n\\(EV = \\$10,000 + (0.01 \\times 0.05 \\times \\$100,000) = \\$10,050\\)\n\n[!IMPORTANT] Based purely on expected cost, Don’t Build is the rational choice despite the high impact of a slide."
  },
  {
    "objectID": "courses/caio/01-intro.html#the-geological-test",
    "href": "courses/caio/01-intro.html#the-geological-test",
    "title": "Machine Learning Essentials",
    "section": "The Geological Test",
    "text": "The Geological Test\nA test is available to better estimate the risk.\n\nCost: $3,000\nAccuracy:\n\n\\(P( T \\mid \\text{Slide} ) = 0.90\\)\n\\(P( \\text{not } T \\mid \\text{No Slide} ) = 0.85\\)\n\n\nShould we take the test?"
  },
  {
    "objectID": "courses/caio/01-intro.html#updating-probabilities-bayes-rule",
    "href": "courses/caio/01-intro.html#updating-probabilities-bayes-rule",
    "title": "Machine Learning Essentials",
    "section": "Updating Probabilities: Bayes’ Rule",
    "text": "Updating Probabilities: Bayes’ Rule\nProbability of Positive Test \\(P(T)\\):\n\\(P(T) = (0.90 \\times 0.01) + (0.15 \\times 0.99) = 0.1575\\)\nPosterior \\(P(\\text{Slide} \\mid T)\\):\n\\(P(\\text{Slide} \\mid T) = \\frac{0.90 \\times 0.01}{0.1575} \\approx 0.0571\\)\nPosterior \\(P(\\text{Slide} \\mid \\text{not } T)\\):\n\\(P(\\text{Slide} \\mid \\text{not } T) = \\frac{0.1 \\times 0.01}{0.8425} \\approx 0.0012\\)"
  },
  {
    "objectID": "courses/caio/01-intro.html#the-testing-strategy",
    "href": "courses/caio/01-intro.html#the-testing-strategy",
    "title": "Machine Learning Essentials",
    "section": "The Testing Strategy",
    "text": "The Testing Strategy\nIf we test:\n\nIf \\(T\\): Build the wall.\nIf not \\(T\\): Don’t build.\n\nExpected Cost with Test:\n\\[\\begin{aligned}\n&\\text{Test Cost} + P(T) \\times \\text{EV(Build} \\mid T) \\\\\n&\\quad + P(\\text{not } T) \\times \\text{EV(No Build} \\mid \\text{not } T)\n\\end{aligned}\\]\n\\(= 3,000 + (0.1575 \\times 10,285) + (0.8425 \\times 120) \\approx \\$4,693\\)"
  },
  {
    "objectID": "courses/caio/01-intro.html#risk-vs.-reward",
    "href": "courses/caio/01-intro.html#risk-vs.-reward",
    "title": "Machine Learning Essentials",
    "section": "Risk vs. Reward",
    "text": "Risk vs. Reward\n\n\n\nChoice\nExpected Cost\nRisk of Loss\nP\n\n\n\n\nDon’t Build\n$1,000\n0.01\n1 in 100\n\n\nBuild w/o test\n$10,050\n0.0005\n1 in 2000\n\n\nTest & Decide\n$4,693\n0.00146\n1 in 700"
  },
  {
    "objectID": "courses/caio/01-intro.html#conclusion",
    "href": "courses/caio/01-intro.html#conclusion",
    "title": "Machine Learning Essentials",
    "section": "Conclusion",
    "text": "Conclusion\n\nLowest Expected Cost: Don’t Build ($1,000).\nLowest Risk of Catastrophe: Build without testing (0.0005).\nMiddle Ground: Testing ($4,693) significantly reduces risk compared to “Don’t Build” without the full $10k upfront cost.\n\nDecision? It depends on your utility function (risk tolerance).\nView Python Implementation (Notebook)"
  },
  {
    "objectID": "courses/caio/01-intro.html#saint-petersburg-paradox",
    "href": "courses/caio/01-intro.html#saint-petersburg-paradox",
    "title": "Machine Learning Essentials",
    "section": "Saint Petersburg Paradox",
    "text": "Saint Petersburg Paradox\nImagine a gambling game where a fair coin is flipped repeatedly until it lands on heads. The payoff for the game is \\(2^N\\), where \\(N\\) is the number of tosses needed for the coin to land on heads.\nThe expected value of this game is infinite:\n\\[\nE(X) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot 4 + \\frac{1}{8} \\cdot 8 + \\ldots = \\infty\n\\]\nThis means that, in theory, a rational person should be willing to pay any finite amount to play this game. However, in reality, most people would be unwilling to pay a large amount."
  },
  {
    "objectID": "courses/caio/01-intro.html#expected-utility-resolution",
    "href": "courses/caio/01-intro.html#expected-utility-resolution",
    "title": "Machine Learning Essentials",
    "section": "Expected Utility Resolution",
    "text": "Expected Utility Resolution\nBernoulli argued that people do not maximize expected monetary value but rather expected utility \\(U(x)\\).\n\\[\nE[U(X)] = \\sum^\\infty_{k=1} 2^{-k} U(2^k)\n\\]\nFor the log utility case, \\(U(x) = \\log(x)\\), the expected utility is \\(2 \\log(2)\\). To find the certain dollar amount \\(x^*\\) (certainty equivalent) that provides the same utility:\n\\[\n\\log(x^*) = 2\\log(2) = \\log(2^2) = \\log(4) \\implies x^* = 4\n\\]\nUnder log utility, a rational player would pay at most $4 to play, despite the infinite expected monetary value.\nOnline: 2 Weeks (14 Days)\nEmail: vsokolov@gmu.edu\nPhone: 703 993 4533\nCourse Textbook: Bayes, AI and Deep Learning by Nick Polson and Vadim Sokolov. The book is to be published by Chapman & Hall/CRC in 2026. Available for free online."
  },
  {
    "objectID": "courses/caio/01-intro.html#topic-purpose",
    "href": "courses/caio/01-intro.html#topic-purpose",
    "title": "Machine Learning Essentials",
    "section": "Topic Purpose",
    "text": "Topic Purpose\nThe purpose of this topic is to introduce participants to the foundational concepts of artificial intelligence and data-driven decision making. Participants will develop a working understanding of probability, statistical modeling, and modern AI techniques—equipping them to lead AI initiatives, evaluate AI investments, and communicate effectively with technical teams."
  },
  {
    "objectID": "courses/caio/01-intro.html#topic-overview",
    "href": "courses/caio/01-intro.html#topic-overview",
    "title": "Machine Learning Essentials",
    "section": "Topic Overview",
    "text": "Topic Overview\nThis module takes executives on a journey from the fundamentals of probability and uncertainty through statistical modeling to the cutting edge of modern AI. Rather than focusing on mathematical derivations, we emphasize intuition, real-world applications, and business implications. Through compelling case studies—from wrongful convictions caused by probability errors to the Netflix Prize’s lessons about model complexity—participants will learn to think probabilistically about business decisions. The module culminates in a hands-on project where participants build an AI agent using Cursor IDE, directly experiencing how data, models, and AI agents work together to solve business problems."
  },
  {
    "objectID": "courses/caio/01-intro.html#topic-objectives",
    "href": "courses/caio/01-intro.html#topic-objectives",
    "title": "Machine Learning Essentials",
    "section": "Topic Objectives",
    "text": "Topic Objectives\nUpon completion of this topic, you should understand and be able to:\n\nApply probabilistic thinking to business decisions under uncertainty\nRecognize common probability fallacies (prosecutor’s fallacy, base rate neglect) and their business implications\nUnderstand the trade-offs between model accuracy, complexity, and business value\nInterpret regression models and explain their predictions to stakeholders\nEvaluate when AI/ML solutions are appropriate versus traditional statistical approaches\nBuild a simple AI agent that combines data analysis with natural language interaction\nLead informed conversations with data science and AI teams"
  },
  {
    "objectID": "courses/caio/01-intro.html#course-approach",
    "href": "courses/caio/01-intro.html#course-approach",
    "title": "Machine Learning Essentials",
    "section": "Course Approach",
    "text": "Course Approach\nThis topic combines asynchronous learning (recorded lectures, readings, discussion boards) with synchronous sessions (live Zoom calls) and hands-on practice. The approach emphasizes:\n\nCase-based learning: Each concept is grounded in real-world examples—from legal cases to sports analytics to retail pricing\nBusiness-first perspective: Technical concepts are always connected to business decisions and outcomes\nProgressive building: Each module builds on the previous, culminating in an integrated final project\nPeer learning: Discussion boards encourage sharing experiences and learning from diverse industry perspectives\nApplied practice: The final project provides hands-on experience building an AI-powered analytics tool"
  },
  {
    "objectID": "courses/caio/01-intro.html#time-commitment",
    "href": "courses/caio/01-intro.html#time-commitment",
    "title": "Machine Learning Essentials",
    "section": "Time Commitment",
    "text": "Time Commitment\nThis topic will require approximately 10 hours of work to complete:\n\n\n\nActivity\nHours\n\n\n\n\nRecorded Lectures (3 lectures × 30 min)\n1.5\n\n\nLive Zoom Sessions (3 sessions × 1 hr)\n3.0\n\n\nReading\n2\n\n\nDiscussion Boards\n1.0\n\n\nFinal Project\n2.0\n\n\nTotal\n10.0"
  },
  {
    "objectID": "courses/caio/01-intro.html#schedule",
    "href": "courses/caio/01-intro.html#schedule",
    "title": "Machine Learning Essentials",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nDay\nActivities\n\n\n\n\nDay 1-2\nModule 1 lectures available; begin readings on probability and Bayes rule\n\n\nDay 3\nDiscussion Board 1 opens\n\n\nDay 4\nZoom Session 1: Kick-off + Cursor IDE Hands-on (1 hr)\n\n\nDay 5-6\nModule 2 lectures available; readings on statistics and regression\n\n\nDay 7\nDiscussion Board 2 opens\n\n\nDay 8\nZoom Session 2: Mid-point Check-in (1 hr)\n\n\nDay 9-10\nModule 3 lectures available; readings on NLP and AI agents\n\n\nDay 11\nDiscussion Board 3 opens\n\n\nDay 12-13\nFinal Project work time\n\n\nDay 14\nZoom Session 3: Wrap-up + Final Project Presentations (1 hr)"
  },
  {
    "objectID": "courses/caio/01-intro.html#module-1-probability-as-a-language-of-uncertainty",
    "href": "courses/caio/01-intro.html#module-1-probability-as-a-language-of-uncertainty",
    "title": "Machine Learning Essentials",
    "section": "Module 1: Probability as a Language of Uncertainty",
    "text": "Module 1: Probability as a Language of Uncertainty\nDays 1-4\nRecorded Lectures\n\nPart 1: Probability\nPart 2: Bayes Rule\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nOpening sections through “Kolmogorov Axioms”\nSection: “Conditional, Marginal and Joint Distributions”\nExample: Salary-Happiness\n\nChapter 2: Bayes Rule\n\nSection: “Intuition and Simple Examples”\nExample: Sally Clark Case\nExample: Nakamura’s Alleged Cheating\n\nChapter 4: Utility, Risk and Decisions\n\nSection: “Expected Utility”\nExamples: Saint Petersburg Paradox, Kelly Criterion, Ellsberg Paradox, Secretary Problem\nSection: “Decision Trees” (including Medical Testing and Mudslide examples)\n\nSupplemental Reading (online)\n\nDid a US Chess Champion Cheat? - Chicago Booth Review (Bayesian analysis, prosecutor’s fallacy)\nA Refresher on Statistical Significance - Harvard Business Review\nDecision Making in Uncertain Times - McKinsey\n\nDiscussion Board: Decision-Making Under Uncertainty\nOpens Day 3 | Due Day 7\n“Consider a strategic decision your organization recently faced (or is currently facing) involving uncertainty. Describe the decision and identify:\n\nWhat were the key uncertain factors?\nHow was probability or likelihood assessed (formally or informally)?\nReflecting on the Ellsberg paradox and Kelly criterion, how might a more systematic probabilistic approach have changed the decision-making process?\n\nRespond to at least two peers’ posts with constructive suggestions.”\nZoom Session 1: Kick-off + Cursor Hands-on\nDay 4 | 1 hour\n\nWelcome and module overview (15 min)\nHands-on: Setting up Cursor IDE and using coding agents (30 min)\nQ&A on probability concepts from Module 1 (15 min)\n\nPreparation: Install Cursor IDE before the session (setup instructions)"
  },
  {
    "objectID": "courses/caio/01-intro.html#module-2-statistics-and-modeling",
    "href": "courses/caio/01-intro.html#module-2-statistics-and-modeling",
    "title": "Machine Learning Essentials",
    "section": "Module 2: Statistics and Modeling",
    "text": "Module 2: Statistics and Modeling\nDays 5-8\nRecorded Lectures\n\nStatistics\n\nRequired Reading (from course textbook)\nChapter 1: Probability and Uncertainty\n\nSections: “Normal Distribution,” “Poisson Distribution,” “Binomial Distribution”\nExamples: Heights of Adults, Customer Arrivals, NFL Patriots Coin Toss\n\nChapter 3: Bayesian Learning\n\nSection: “Poisson Model for Count Data”\n\nChapter 12: Linear Regression\n\nSection: “Linear Regression” (opening)\nExamples: Google vs S&P 500, Orange Juice\n\nChapter 13: Logistic Regression and GLMs\n\nSections: “Model Fitting,” “Confusion Matrix,” “ROC Curve”\nExample: NBA point spread\n\nSupplemental Reading (online)\n\nThe Surprising Power of Online Experiments - Harvard Business Review (A/B testing)\nMachine Learning Explained - MIT Sloan\n\nDiscussion Board: Predictive Models in Business\nOpens Day 7 | Due Day 11 “The Netflix Prize awarded $1 million for a 10% improvement in recommendation accuracy, yet Netflix never fully implemented the winning algorithm—it was too complex and expensive to deploy, and by then, streaming had changed the business model entirely. See Why Even a Million Dollars Couldn’t Buy a Better Algorithm - Wired (Netflix Prize case study)\nReflecting on this case and the regression concepts from this module:\n\nIdentify a business process in your organization where a predictive model could be applied. What decisions would it inform?\nWhat would happen if the model’s predictions were inaccurate 20% of the time? 40% of the time? How would this affect business outcomes and trust in the system?\nDiscuss the trade-off: Is a highly accurate but complex/expensive model always better than a simpler, ‘good enough’ model? What factors would you consider when making this decision?\n\nRespond to at least two peers’ posts, particularly focusing on whether you agree with their assessment of the accuracy-complexity trade-off.”\nZoom Session 2: Mid-point Check-in\nDay 8 | 1 hour\n\nReview of statistical modeling concepts (20 min)\nLive demo: Building a simple regression model with Cursor (25 min)\nDiscussion of final project requirements (15 min)\n\nPreparation: Complete Module 2 lectures and readings"
  },
  {
    "objectID": "courses/caio/01-intro.html#module-3-modern-ai",
    "href": "courses/caio/01-intro.html#module-3-modern-ai",
    "title": "Machine Learning Essentials",
    "section": "Module 3: Modern AI",
    "text": "Module 3: Modern AI\nDays 9-14\nRequired Reading (from course textbook)\nChapter 24: Natural Language Processing\n\nSections: “Converting Words to Numbers (Embeddings),” “Word2Vec and Distributional Semantics”\nExample: Word2Vec for War and Peace\nSections: “Attention Mechanisms,” “Transformer Architecture” (overview)\n\nChapter 26: AI Agents\n\nFull chapter overview (agent architecture, tool use, planning, safety)\n\nSupplemental Reading (online)\n\nMaking the Most of AI and Machine Learning in Organizations - Stanford SMJ Paper\nTraditional Statistics vs Machine Learning - ToolsGroup\nThe State of AI in 2024 - McKinsey\nGenerative AI’s Act Two - Sequoia Capital\n\nDiscussion Board: AI Agents in the Enterprise\nOpens Day 11 | Due Day 14\n“AI agents are increasingly being deployed in business contexts. Describe a workflow or process in your organization that could potentially be automated or augmented by an AI agent. Address:\n\nWhat tasks would the agent perform?\nWhat data or tools would it need access to?\nWhat guardrails or human oversight would be necessary?\nWhat risks or concerns would need to be addressed before deployment?\n\nRespond to at least two peers’ posts.”\nZoom Session 3: Wrap-up + Final Project Presentations\nDay 14 | 1 hour\n\nBrief Modern AI recap (10 min)\nFinal project presentations/demonstrations (35 min)\nCourse wrap-up and next steps for AI leadership (15 min)\n\nPreparation: Complete final project; prepare 2-3 minute demonstration"
  },
  {
    "objectID": "courses/caio/01-intro.html#assignment-final-project",
    "href": "courses/caio/01-intro.html#assignment-final-project",
    "title": "Machine Learning Essentials",
    "section": "Assignment: Final Project",
    "text": "Assignment: Final Project\nOrange Juice Pricing Analytics Agent\nThis part (as ever other part of the module) is optional.\nBusiness Problem: You are a pricing analyst at a retail chain. Management wants to optimize orange juice pricing and promotional strategies. Build an AI agent that can answer business questions about pricing decisions using historical sales data and a predictive model.\nDataset: Dominick’s Orange Juice Dataset\n\nWeekly sales data for orange juice brands (Tropicana, Minute Maid, Dominick’s)\nVariables: sales volume, price, advertising features, brand\n~28,000 observations across multiple stores\n\nModel: Linear Regression with Interactions\n\nPredict sales volume based on price, advertising, and brand\nCapture how price sensitivity varies by brand\n\nYour Agent Must Answer These Business Questions:\n\n“What is the predicted sales volume if we price Tropicana at $2.50 with no advertising?”\n“Which brand is most price-sensitive?”\n“Should we feature Minute Maid in this week’s ad circular? What’s the expected sales lift?”\n“What price should we set for Dominick’s brand to maximize revenue?”\n“Compare the price elasticity across the three brands.”\n\nDeliverables:\n\nPython code files in Cursor IDE (using provided template)\nWorking agent that answers the 5 business questions above\n1-page summary: What did you learn about OJ pricing? What surprised you?\n2-3 minute demo during Zoom Session 3\n\nEvaluation Criteria:\n\nFunctionality: Agent loads data, builds model, and responds to queries\nBusiness Relevance: Clear connection between model outputs and business decisions\nDocumentation: Clear explanation of approach and results\n\nSee Final Project Guide for detailed step-by-step instructions."
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#saint-petersburg-paradox",
    "href": "courses/caio/mudslide_slides.html#saint-petersburg-paradox",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Saint Petersburg Paradox",
    "text": "Saint Petersburg Paradox\nImagine a gambling game where a fair coin is flipped repeatedly until it lands on heads. The payoff for the game is \\(2^N\\), where \\(N\\) is the number of tosses needed for the coin to land on heads.\nThe expected value of this game is infinite:\n\\[\nE(X) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot 4 + \\frac{1}{8} \\cdot 8 + \\ldots = \\infty\n\\]\nThis means that, in theory, a rational person should be willing to pay any finite amount to play this game. However, in reality, most people would be unwilling to pay a large amount."
  },
  {
    "objectID": "courses/caio/mudslide_slides.html#expected-utility-resolution",
    "href": "courses/caio/mudslide_slides.html#expected-utility-resolution",
    "title": "Decision Analysis: The Mudslide Problem",
    "section": "Expected Utility Resolution",
    "text": "Expected Utility Resolution\nBernoulli argued that people do not maximize expected monetary value but rather expected utility \\(U(x)\\).\n\\[\nE[U(X)] = \\sum^\\infty_{k=1} 2^{-k} U(2^k)\n\\]\nFor the log utility case, \\(U(x) = \\log(x)\\), the expected utility is \\(2 \\log(2)\\). To find the certain dollar amount \\(x^*\\) (certainty equivalent) that provides the same utility:\n\\[\n\\log(x^*) = 2\\log(2) = \\log(2^2) = \\log(4) \\implies x^* = 4\n\\]\nUnder log utility, a rational player would pay at most $4 to play, despite the infinite expected monetary value."
  }
]