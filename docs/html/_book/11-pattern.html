<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Pattern Matching – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-regression.html" rel="next">
<link href="./10-data.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="11&nbsp; Pattern Matching – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/xfy.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="11&nbsp; Pattern Matching – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/xfy.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./11-pattern.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-pattern-matching" id="toc-why-pattern-matching" class="nav-link active" data-scroll-target="#why-pattern-matching"><span class="header-section-number">11.1</span> Why Pattern Matching?</a>
  <ul class="collapse">
  <li><a href="#richard-feynman-on-pattern-matching-and-chess" id="toc-richard-feynman-on-pattern-matching-and-chess" class="nav-link" data-scroll-target="#richard-feynman-on-pattern-matching-and-chess">Richard Feynman on Pattern Matching and Chess</a></li>
  </ul></li>
  <li><a href="#prediction-and-forecasting" id="toc-prediction-and-forecasting" class="nav-link" data-scroll-target="#prediction-and-forecasting"><span class="header-section-number">11.2</span> Prediction and Forecasting</a></li>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="header-section-number">11.3</span> Supervised Learning</a></li>
  <li><a href="#complex-functions" id="toc-complex-functions" class="nav-link" data-scroll-target="#complex-functions"><span class="header-section-number">11.4</span> Complex Functions</a></li>
  <li><a href="#model-estimation" id="toc-model-estimation" class="nav-link" data-scroll-target="#model-estimation"><span class="header-section-number">11.5</span> Model Estimation</a>
  <ul class="collapse">
  <li><a href="#penalized-likelihood" id="toc-penalized-likelihood" class="nav-link" data-scroll-target="#penalized-likelihood">Penalized Likelihood</a></li>
  <li><a href="#bayesian-approach" id="toc-bayesian-approach" class="nav-link" data-scroll-target="#bayesian-approach">Bayesian Approach</a></li>
  </ul></li>
  <li><a href="#prediction-accuracy" id="toc-prediction-accuracy" class="nav-link" data-scroll-target="#prediction-accuracy"><span class="header-section-number">11.6</span> Prediction Accuracy</a>
  <ul class="collapse">
  <li><a href="#evaluation-metrics-for-regression" id="toc-evaluation-metrics-for-regression" class="nav-link" data-scroll-target="#evaluation-metrics-for-regression">Evaluation Metrics for Regression</a></li>
  <li><a href="#evaluation-metrics-for-classification" id="toc-evaluation-metrics-for-classification" class="nav-link" data-scroll-target="#evaluation-metrics-for-classification">Evaluation Metrics for Classification</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">11.7</span> Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./11-pattern.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-pattern" class="quarto-section-identifier"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>“<em>Prediction is very difficult, especially about the future.</em>” Niels Bohr, Danish physicist and Nobel laureate</p>
<p>The history of data analysis is closely intertwined with the development of pattern matching techniques. The ability to identify and understand patterns in data has been crucial for scientific discoveries, technological advancements, and decision-making. From the early days of astronomy to modern machine learning, pattern matching has played a pivotal role in advancing our understanding of the world around us. This chapter explores the key concepts of pattern matching, its historical development, and its impact on data analysis.</p>
<p>Data science involves two major steps: collection and cleaning of data and building a model or applying an algorithm. In this chapter we present the process of building predictive models. To illustrate the process, think of your data as being generated by a black box in which a set of input variables <span class="math inline">\(x\)</span> go through the box and generate an output variable <span class="math inline">\(y\)</span>.</p>
<section id="why-pattern-matching" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="why-pattern-matching"><span class="header-section-number">11.1</span> Why Pattern Matching?</h2>
<p>For Gauss, Laplace, and many other scientists, the central challenge was estimating parameters when the functional form of the relationship was already known—often linear (as in the Earth-shape example) or multiplicative (e.g., Newton’s <span class="math inline">\(F=ma\)</span>). In many modern problems, however, the relationship itself is unknown and defies simple mathematical description; human behaviour and natural language are prominent examples (<span class="citation" data-cites="halevy2009unreasonable">Halevy, Norvig, and Pereira (<a href="references.html#ref-halevy2009unreasonable" role="doc-biblioref">2009</a>)</span>).</p>
<p>In such cases a <em>pattern-matching</em> approach can uncover the hidden relationships directly from data. Pattern matching means identifying recurring sequences, relationships, or structures within a dataset—much like finding a puzzle piece that completes a picture. Recognising these patterns yields insights, reveals trends, supports prediction, and ultimately improves decisions. Initial pattern-matching studies have often sparked major scientific advances, as the early history of mammography illustrates.</p>
<div id="exm-mammography" class="theorem example">
<p><span class="theorem-title"><strong>Example 11.1 (Mammography and Early Pattern Matching)</strong></span> Early mammography relied on visual pattern matching to detect cancer signs like masses and microcalcifications. Radiologists used their expertise to distinguish these patterns from normal tissue, though the process was subjective and error-prone. Despite these challenges, this visual pattern matching laid the foundation for modern screening.</p>
<p>German surgeon Albert Solomon pioneered this field with his 1913 monograph (<span class="citation" data-cites="nicosia2023history">Nicosia et al. (<a href="references.html#ref-nicosia2023history" role="doc-biblioref">2023</a>)</span>). By comparing X-rays of surgically removed tissue with the actual specimens, he identified characteristic features of tumors and was among the first to link microcalcifications to breast cancer—a correlation that remains a key biomarker today, even as the underlying molecular mechanisms are still being studied (<span class="citation" data-cites="bonfiglio2021molecular">Bonfiglio et al. (<a href="references.html#ref-bonfiglio2021molecular" role="doc-biblioref">2021</a>)</span>).</p>
</div>
<section id="richard-feynman-on-pattern-matching-and-chess" class="level3">
<h3 class="anchored" data-anchor-id="richard-feynman-on-pattern-matching-and-chess">Richard Feynman on Pattern Matching and Chess</h3>
<p>Richard Feynman, the renowned physicist, argued that many scientific discoveries begin with pattern matching—a skill experts develop to identify structures and regularities in their domain. He frequently engaged in discussions about artificial intelligence, often using chess as an analogy to illustrate the difference between human intuition and machine calculation.</p>
<p>Feynman observed that while novices calculate moves, masters recognize patterns. They understand the “laws” of the board much like physicists understand the laws of nature. In an interview, he described learning the rules of chess simply by observing games: first noting that bishops maintain their color, then realizing they move diagonally. This process of uncovering rules from observations is the essence of scientific discovery.</p>
<p>Regarding machine intelligence, Feynman was pragmatic. He noted that machines need not “think” like humans to achieve similar or superior results. Just as airplanes fly without flapping wings like birds, computers can play chess (or solve other problems) using different underlying mechanisms—such as massive calculation or statistical optimization—yet achieve superhuman performance.</p>
<blockquote class="blockquote">
<p>“If we would like to make something that runs rapidly over the ground… we could try to make a machine that runs like a cheetah. But, it’s easier to make a machine with wheels… later machines are not going to think like people think.” — Richard Feynman</p>
</blockquote>
<p>This distinction is crucial in modern AI. Today’s systems, like AlphaZero, combine pattern matching (via neural networks) with search (Monte Carlo simulation), effectively “learning” chess principles from scratch. They don’t rely on human heuristics (like “control the center”) but discover their own statistical patterns that maximize the probability of winning.</p>
<p>Discussions with professional pianist Beatrice Rana revealed an interesting parallel. She compared modern AI’s ability to produce incredible results to a pianist capable of remembering and reproducing complex pieces of music—calling it <em>“intelligenza artigiana”</em>, or the intelligence of hands: a mastery of execution and pattern without necessarily possessing human-like consciousness.</p>
<p>How do we translate this conceptual “pattern matching” into concrete algorithms? In data science, we formalize this process by defining a mathematical structure (a model) and using data to adjust its parameters. Whether we are predicting election outcomes or classifying images, the core task remains the same: finding a function that maps inputs to outputs in a way that generalizes to new, unseen data.</p>
<p>We will now move from the intuitive understanding of pattern matching to its formalization in predictive modeling.</p>
</section>
</section>
<section id="prediction-and-forecasting" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="prediction-and-forecasting"><span class="header-section-number">11.2</span> Prediction and Forecasting</h2>
<p>Prediction and forecasting are central challenges in data analysis, predominantly solved using pattern matching approaches. Prediction and forecasting are two closely related concepts that are often used interchangeably. In business and engineering the main motivation for prediction and forecasting is to make better decisions. In science, the main motivation is to test and validate theories.</p>
<p>Prediction and forecasting help to identify trends and patterns in historical data that would otherwise remain hidden. This allows analysts to make <em>informed decisions</em> about the future based on what they know about the past. By using prediction models, analysts can identify <em>potential risks and opportunities</em> that may lie ahead. This information can then be used to develop proactive strategies to mitigate risks and capitalize on opportunities.</p>
<p>In many business applications the concern is improving efficiency of a system. For example, to improve logistic chains and to optimally allocate resources, we need to forecast demand and supply and to predict the future prices of the resources. By predicting future sales, businesses can better plan their marketing and sales efforts. This can lead to increased sales and profitability. Prediction and forecasting can be used to identify and mitigate potential risks, such as financial losses, supply chain disruptions, and operational failures.</p>
<p>For unsupervised learning (finding patterns without labeled data), common techniques include clustering and dimensionality reduction, which we will discuss in later chapters. The primary focus of this chapter is on supervised learning, where we have a target variable we wish to predict.</p>
</section>
<section id="supervised-learning" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">11.3</span> Supervised Learning</h2>
<p>The problem of supervised learning is to learn patterns from observed data to make predictions on new, unseen data. The key idea is that we have input-output pairs <span class="math inline">\((x_i, y_i)\)</span> where we know the correct output <span class="math inline">\(y_i\)</span> for each input <span class="math inline">\(x_i\)</span>, and we use these examples to learn a function that maps inputs to outputs.</p>
<p>Supervised learning has become ubiquitous across modern engineering, business, and technology applications. In manufacturing, predictive maintenance systems use sensor data from industrial equipment to forecast potential failures, enabling proactive maintenance that reduces downtime and costs. Autonomous vehicles rely heavily on supervised learning for object detection, lane recognition, and decision-making systems that process real-time sensor data from cameras, LiDAR, and radar. In healthcare, supervised learning powers diagnostic imaging systems that can detect diseases from X-rays, MRIs, and CT scans with accuracy rivaling human radiologists.</p>
<p>Financial institutions employ supervised learning for fraud detection, credit scoring, and algorithmic trading systems that analyze vast amounts of transaction data. Smart cities utilize supervised learning for traffic flow optimization, energy consumption forecasting, and air quality monitoring. Many companies use prediction models for customer churn, helping identify early warning signs of dissatisfaction. Marketing teams leverage supervised learning for customer segmentation, campaign optimization, and lead scoring to improve conversion rates. Supply chain optimization uses supervised learning to forecast demand, optimize inventory levels, and predict delivery times. These applications demonstrate how supervised learning has evolved from simple prediction tasks to complex, real-time decision-making systems that operate across diverse domains.</p>
<p>A typical prediction problem involves building a rule that maps observed inputs <span class="math inline">\(x\)</span> into the output <span class="math inline">\(y\)</span>. The inputs <span class="math inline">\(x\)</span> are often called predictors, features, or independent variables, while the output <span class="math inline">\(y\)</span> is often called the response or dependent variable. The goal is to find a predictive rule <span class="math display">\[
y = f(x).
\]</span></p>
<p>The map <span class="math inline">\(f\)</span> can be viewed as a black box which describes how to find the output <span class="math inline">\(y\)</span> from the input <span class="math inline">\(x\)</span>. One of the key requirements of <span class="math inline">\(f\)</span> is that we should be able to efficiently find this function using an algorithm. In the simple case <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are both univariate (scalars) and we can view the map as</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDU5MX0.v_80mJFFbUiW1E2oDBTokjAi0lJRFRnvAmN8MliRLL8 -->
<!-- ![ .](fig/xfy1.svg){width=50%} -->
<div class="cell" data-eval="false" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>

</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div id="fig-xfy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-xfy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/xfy.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-xfy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.1: Black box model
</figcaption>
</figure>
</div>
<p>The goal of machine learning is to reconstruct this map from observed data. In a multivariate setting <span class="math inline">\(x = (x_1,\ldots,x_p)\)</span> is a list of <span class="math inline">\(p\)</span> variables. This leads to a model of the form <span class="math inline">\(y = f(x_1,\ldots,x_p)\)</span>. There are a number of possible goals of analysis, such as estimation, inference or prediction. The main one being prediction.</p>
<p>The prediction task is to calculate a response that corresponds to a new feature input variable. An example of inference is the task of establishing causation, with the goal of extracting information about the nature of the black box association of the response variable to the input variables.</p>
<p>In either case, the goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in its nature. To uncover the pattern we use a training dataset, denoted by <span class="math display">\[
D = (y_i,x_i)_{i=1}^n
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is a set of <span class="math inline">\(p\)</span> predictors and <span class="math inline">\(y_i\)</span> is response variable. Prediction problem is to use a training dataset <span class="math inline">\(D\)</span> to design a rule that can be used for predicting output values <span class="math inline">\(y\)</span> for new observations <span class="math inline">\(x\)</span>.</p>
<p>Let <span class="math inline">\(f(x)\)</span> be predictor of <span class="math inline">\(y\)</span>, we will use notation <span class="math display">\[
\hat{y} = f(x).
\]</span></p>
<p>To summarize, we will use the following notation.</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td><span class="math inline">\(y\)</span></td>
<td>output variable (response/outcome)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(x\)</span></td>
<td>input variable (predictor/covariate/feature)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(f(x)\)</span></td>
<td>predictive rule</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat y\)</span></td>
<td>predicted output value</td>
</tr>
</tbody>
</table>
<p>We distinguish several types of input or output variables. First, <em>binary</em> variables that can only have two possible values, e.g.&nbsp;yes/no, left/right, 0/1, up/down, etc. A generalization of binary variable is a <em>categorical</em> variable that can take a fixed number of possible values, for example, marriage status. Additionally, some of the categorical variable can have a natural order to them, for example education level or salary range. Those variables are called <em>ordinal</em>. Lastly, the most common type of a variable is <em>quantitative</em> which is described by a real number.</p>
<p>Depending on the type of the output variable, there are three types of prediction problems.</p>
<table class="caption-top table">
<caption>Types of output variables and corresponding prediction problems.</caption>
<colgroup>
<col style="width: 39%">
<col style="width: 24%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Output Variable Type</th>
<th>Description</th>
<th>Prediction Problem</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Binary</td>
<td><span class="math inline">\(y\in \{0,1\}\)</span></td>
<td>Classification</td>
</tr>
<tr class="even">
<td>Categorical</td>
<td><span class="math inline">\(y\in \{0,\ldots,K\}\)</span> for <span class="math inline">\(K\)</span> possible categories</td>
<td>Classification</td>
</tr>
<tr class="odd">
<td>Quantitative</td>
<td><span class="math inline">\(y \in \mathbb{R}\)</span> (any real number)</td>
<td>Regression</td>
</tr>
<tr class="even">
<td>Ordinal</td>
<td><span class="math inline">\(y\)</span> has natural ordering</td>
<td>Ranking</td>
</tr>
</tbody>
</table>
<p>Here are some examples of prediction problems:</p>
<p>Binary Classification: Predicting whether an email is spam or not spam involves input variables such as email content, sender information, presence of certain keywords, and email length. The output variable is <span class="math inline">\(y \in \{0,1\}\)</span> where 0 = not spam, 1 = spam. The goal is to classify new emails as spam or legitimate.</p>
<p>Categorical Classification: Predicting the type of social media content based on text and image features uses input variables including text content, image features, user engagement metrics, posting time, and hashtags. The output variable is <span class="math inline">\(y \in \{0,1,2,3,4\}\)</span> where 0 = news, 1 = entertainment, 2 = educational, 3 = promotional, 4 = personal. The goal is to automatically categorize social media posts for content moderation and recommendation systems.</p>
<p>Regression (Quantitative): Predicting house prices based on features uses input variables such as square footage, number of bedrooms, location, age of house, and lot size. The output variable is <span class="math inline">\(y \in \mathbb{R}\)</span> (house price in dollars). The goal is to predict the selling price of a new house.</p>
<p>Ranking (Ordinal): Predicting customer satisfaction ratings involves input variables including product quality, customer service experience, delivery time, and price. The output variable is <span class="math inline">\(y \in \{1,2,3,4,5\}\)</span> where 1 = very dissatisfied, 5 = very satisfied. The goal is to predict customer satisfaction level for new customers.</p>
<p>There are several simple predictive rules we can use to predict the output variable <span class="math inline">\(y\)</span>. For example, in the case of regression problem, the simplest rule is to predict the average value of the output variable. This rule is called the <em>mean rule</em> and is defined as <span class="math display">\[
\hat f(x) = \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i.
\]</span></p>
<p>Notice, this model does not depend on the input variable <span class="math inline">\(x\)</span> and will predict the same value for all observations. This rule is simple and easy to implement, but it is not very accurate. In case of binary <span class="math inline">\(y\)</span>, we can apply thresholding to the mean rule to obtain a binary classifier. <span class="math display">\[
f(x) = \begin{cases}
1 &amp; \text{if } \bar{y} &gt; 0.5, \\
0 &amp; \text{if } \bar{y} \leq 0.5.
\end{cases}
\]</span></p>
<p>A more sophisticated rule is the <em>nearest neighbor rule</em>. This rule predicts the output value <span class="math inline">\(y\)</span> for a new observation <span class="math inline">\(x\)</span> by finding the closest observation in the training dataset and using its output value. The nearest neighbor rule is defined as <span class="math display">\[
f(x) = y_{i^*},
\]</span> where <span class="math display">\[i^* = \arg\min_{i=1,\ldots,n} \|x_i - x\|\]</span> is the index of the closest observation in the training dataset. These two models represent two extreme cases of predictive rules: the mean rule is “stubborn” (it always predicts the same value) and the nearest neighbor rule is “flexible” (can be very sensitive to small changes in the inputs). Using the language of statistics the mean rule is of high bias and low variance, while the nearest neighbor rule is of low bias and high variance. Although those two rules are simple, they sometimes lead to useful models that can be used in practice. Further, those two models represent a trade-off between accuracy and complexity (the bias-variance trade-off). We will discuss this trade-off in more detail in the later section.</p>
<p>The mean model and nearest neighbor model belong to a class of so-called <em>non-parametric</em> models. The non-parametric models do not make explicit assumption about the form of the function <span class="math inline">\(f(x)\)</span>. In contrast, parametric models assume that the predictive rule <span class="math inline">\(f(x)\)</span> is a specific function defined by vector of parameters, which we will denote as <span class="math inline">\(\theta\)</span>. A typical notation is then <span class="math display">\[
f_{\theta}(x).
\]</span></p>
<p>Traditional modeling culture employs statistical models characterized by single-layer transformations (transforming inputs directly into outputs without intermediate hidden layers), where the relationship between input variables and output is modeled through direct, interpretable mathematical formulations. These approaches typically involve linear combinations, additive structures, or simple nonlinear transformations that maintain analytical tractability and statistical interpretability. The list of widely used models includes:</p>
<div id="tbl-traditional" class="quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[30,45,25]">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-traditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11.1: Traditional statistical models.
</figcaption>
<div aria-describedby="tbl-traditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 45%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Formula</th>
<th>Parameters / Hyperparameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td><span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p\)</span></td>
<td><span class="math inline">\(\theta = (\beta_0, \beta_1, \ldots, \beta_p)\)</span></td>
</tr>
<tr class="even">
<td>Generalized Linear Model (GLM)</td>
<td><span class="math inline">\(y = f^{-1}(\beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p)\)</span></td>
<td><span class="math inline">\(\theta = (\beta_0, \beta_1, \ldots, \beta_p)\)</span></td>
</tr>
<tr class="odd">
<td>Generalized Additive Model (GAM)</td>
<td><span class="math inline">\(y = \beta_0 + f_1(x_1) + \ldots + f_k(x_k)\)</span></td>
<td><span class="math inline">\(\theta = (\beta_0, f_1, \ldots, f_k)\)</span></td>
</tr>
<tr class="even">
<td>Principal Component Regression (PCR)</td>
<td><span class="math inline">\(y = \beta^T (W x),\quad W \in \mathbb{R}^{k \times p},\ k &lt; p\)</span></td>
<td><span class="math inline">\(\theta = (\beta, W)\)</span></td>
</tr>
<tr class="odd">
<td>k-Nearest Neighbors (KNN)</td>
<td><span class="math inline">\(y = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i\)</span></td>
<td><span class="math inline">\(k\)</span> (neighbors count)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>We wish to find map <span class="math inline">\(f\)</span> such that <span class="math display">\[\begin{align*}
y &amp;= f ( x ) \\
y &amp;=  f ( x_1 , \ldots , x _p )
\end{align*}\]</span></p>
<p>Essentially, the goal is to perform the pattern matching, also known as nonparametric regression. It involves finding complex relationships in data without assuming a specific functional form.</p>
</section>
<section id="complex-functions" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="complex-functions"><span class="header-section-number">11.4</span> Complex Functions</h2>
<p>In contrast to single-layer approaches, Deep Learning employs sophisticated high-dimensional multi-layer neural network architectures that can capture complex, non-linear relationships in data through hierarchical feature learning. In deep learning, we use composite functions rather than additive functions. We write the superposition of univariate functions as <span class="math display">\[
f = f_1 \circ \ldots \circ f_L   \; \; \text{versus}  \; \; f_1 +  \ldots + f_L
\]</span> where composition <span class="math inline">\(f = f_L(f_{L-1}(\ldots f_1(x)))\)</span> creates a “deep” hierarchical structure, as opposed to the “flat” additive structure of models like GAMs.</p>
<div class="cell" data-eval="false" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>

</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div id="fig-func-comp-add" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-func-comp-add-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/function_composition_addition.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-func-comp-add-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11.2: Composition vs Addition of Functions
</figcaption>
</figure>
</div>
<p>Each function <span class="math inline">\(f_i\)</span> in the composition is typically a combination of a linear transformation and a non-linear activation function <span class="math display">\[
f_i(x) = \sigma(W_i x + b_i),
\]</span> The set of parameters that we need to find is <span class="math inline">\(\theta = (W_1, b_1, \ldots, W_L, b_L)\)</span>. The depth and complexity of these architectures allow deep learning models to automatically discover intricate patterns—such as edges in images or grammar in text—from raw input data.</p>
</section>
<section id="model-estimation" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="model-estimation"><span class="header-section-number">11.5</span> Model Estimation</h2>
<p>There are two main approaches to finding the set of parameters <span class="math inline">\(\theta\)</span>. The first is optimization approach that minimizes a loss function. Loss function measures how well predictive rule <span class="math inline">\(f\)</span> captures the relationship between input and output variables. The most common loss function is the mean squared error (MSE). The second approach is to use full Bayesian inference and to calculate the distribution over parameter <span class="math inline">\(\theta\)</span> given the observed data.</p>
<p>From the perspective of representation, feature engineering can be viewed as the search for low-dimensional summaries of <span class="math inline">\(x\)</span> that retain the information needed for prediction: an informal echo of the role of sufficient statistics in classical inference (<a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>).</p>
<p>Both approaches start with formulating likelihood function. Likelihood is a function that tells us how probable the observed data is, given a particular value of the parameter in a statistical model. It is not the same as probability; instead, it’s a function of the parameter, with the data fixed. Suppose you flip a biased coin 10 times and get 7 heads. You want to estimate the probability of getting heads on a single toss. You try different values of <span class="math inline">\(\theta\)</span> and ask: “How likely is it to get exactly 7 heads out of 10 flips if the true probability is <span class="math inline">\(\theta\)</span>?” This leads to the likelihood function. Formally, given <span class="math inline">\(y_i \sim f(y_i\mid x_i,  \theta)\)</span> as exchangeable (often simplified to i.i.d.) samples from a distribution with parameter <span class="math inline">\(\theta\)</span>, the likelihood function is defined as <span class="math display">\[
L(\theta) = \prod_{i=1}^n p(y_i\mid x_i,  \theta).
\]</span> It treats the data <span class="math inline">\(D = (y_i,x_i)_{i=1}^n\)</span> as fixed and varies <span class="math inline">\(\theta\)</span>.</p>
<p>Likelihood connects our model to the data generating process by quantifying how likely it is to observe the actual data we have under different parameter values. For example, if we assume our data follows a normal distribution <span class="math inline">\(y \sim N(f_\theta(x), \sigma^2)\)</span> with mean <span class="math inline">\(f_\theta(x)\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, the likelihood function would be:</p>
<p><span id="eq-normal-likelihood"><span class="math display">\[
L(\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f_\theta(x_i))^2}{2\sigma^2}\right).
\tag{11.1}\]</span></span></p>
<p>For the case of classification problem, we assume that <span class="math inline">\(y_i\)</span> follows a Bernoulli distribution <span class="math inline">\(y_i \sim \text{Bernoulli}(p_i)\)</span>. The likelihood function is defined as <span class="math display">\[
L(\theta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}.
\]</span> Here <span class="math inline">\(p_i\)</span> is the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate <span class="math inline">\(p_i\)</span> is to use logistic function <span class="math inline">\(\sigma(\cdot)\)</span> <span class="math display">\[\begin{align*}
f_{\beta}(x_i) = &amp; \beta^Tx_i\\
p_i  = &amp; \sigma(f_{\beta}(x_i)) =  \frac{e^{f_{\beta}(x_i)}}{1+e^{f_{\beta}(x_i)}},
\end{align*}\]</span> Notice, that logistic function <span class="math inline">\(\sigma(\cdot)\)</span> is restricted to output values in <span class="math inline">\((0,1)\)</span>.</p>
<p>The optimization-based approach is to find the set of parameters <span class="math inline">\(\theta\)</span> that maximizes the likelihood function. <span class="math display">\[
\theta^* = \arg\max_{\theta} L(\theta).
\]</span></p>
<p>Although most often, it is easier to optimize the log-likelihood function. We define the log-likelihood by <span class="math display">\[
\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log p(y_i\mid x_i,  \theta).
\]</span> Notice that the log-likelihood is a sum of per-observation contributions, which is convenient for both analysis and computation. In many estimation problems we will instead minimize the negative log-likelihood (which plays the role of a loss), <span class="math display">\[
l(\theta) = -\ell(\theta).
\]</span></p>
<p><em>Why does the solution not change?</em> Since the logarithm is a monotonically increasing function, if <span class="math inline">\(L(\theta_1) &gt; L(\theta_2)\)</span>, then <span class="math inline">\(\log L(\theta_1) &gt; \log L(\theta_2)\)</span>. This means that the parameter value that maximizes the likelihood function will also maximize the log-likelihood function. The maximum point stays the same, just the function values are transformed.</p>
<p>The value of parameters <span class="math inline">\(\theta\)</span> that maximizes the log-likelihood is called the <em>maximum likelihood estimate</em> (MLE).</p>
<p>Now, rather than maximizing the log-likelihood function, we minimize the negative log-likelihood function <span class="math display">\[
\theta^* = \arg\min_{\theta} l(\theta).
\]</span> This problem is called the least squares problem.</p>
<p>Then the negative log-likelihood function is called the <em>loss function</em>. Thus the problem of finding maximum likelihood estimate is equivalent to minimizing the loss function.</p>
<p>Let’s calculate the loss function that corresponds to the normal likelihood function given by <a href="#eq-normal-likelihood" class="quarto-xref">Equation&nbsp;<span>11.1</span></a>. Using the fact that the logarithm of a product is a sum of logarithms, we can write the negative log-likelihood as <span class="math display">\[
l(\theta) = -\sum_{i=1}^n \log \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f_\theta(x_i))^2}{2\sigma^2}\right).
\]</span> Inside the sum, we have a product of two terms. The first term is a constant with respect to <span class="math inline">\(\theta\)</span> and the second term is a function of <span class="math inline">\(\theta\)</span>. We can rewrite the likelihood function as <span class="math display">\[
l(\theta) = -\sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi\sigma^2}} + \log \exp\left(-\frac{(y_i - f_\theta(x_i))^2}{2\sigma^2}\right)\right].
\]</span></p>
<p>The first term <span class="math inline">\(\log \frac{1}{\sqrt{2\pi\sigma^2}}\)</span> is a constant with respect to <span class="math inline">\(\theta\)</span>, so we can drop it from the optimization problem. The second term can be simplified using the fact that <span class="math inline">\(\log \exp(x) = x\)</span>:</p>
<p><span class="math display">\[
l(\theta) = \sum_{i=1}^n \left[\frac{(y_i - f_\theta(x_i))^2}{2\sigma^2}\right] + C,
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a constant that does not depend on <span class="math inline">\(\theta\)</span>. Since we are minimizing <span class="math inline">\(l(\theta)\)</span>, we can drop constant terms that do not depend on <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
l(\theta) = \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - f_\theta(x_i))^2.
\]</span></p>
<p>This is the <em>mean squared error (MSE)</em> loss function, which is the most commonly used loss function for regression problems. The factor <span class="math inline">\(\frac{1}{2\sigma^2}\)</span> is often absorbed into the learning rate or regularization parameter in optimization algorithms. Thus, another name of the estimator is the <em>least squares estimator</em>. It is the same as the maximum likelihood estimator, assuming that the <span class="math inline">\(f_\theta(x_i)\)</span> is normally distributed.</p>
<section id="penalized-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="penalized-likelihood">Penalized Likelihood</h3>
<p>While maximum likelihood estimation provides a principled approach to parameter estimation, we can often find better estimators using what is called a penalized likelihood. In fact, there are certain cases, when penalized estimator leads to universally better estimators. In statistics, we would say that MLE is inadmissible in dimensions of 3 or higher, meaning there exists another estimator (like the James-Stein estimator) that is strictly better in terms of expected squared error (risk) by “shrinking” estimates towards a central value. Later in <a href="18-theoryai.html" class="quarto-xref"><span>Chapter 17</span></a> we will discuss the theory of penalized estimators in more detail.</p>
<p>Penalized likelihood addresses overfitting by adding a regularization term to the likelihood function. Instead of maximizing just the likelihood, we maximize:</p>
<p><span class="math display">\[
L_{\text{penalized}}(\theta) = L(\theta) \cdot \exp(\lambda \phi(\theta))
\]</span></p>
<p>Or equivalently, we minimize the negative log-likelihood plus a penalty: <span class="math display">\[
l(\theta) =\sum_{i=1}^n l(y_i, f_{\theta} (x_i)) +\lambda \sum_{j=1}^p \phi(\theta_j),
\]</span> where <span class="math inline">\(\lambda &gt; 0\)</span> is the regularization parameter that controls the strength of regularization, and <span class="math inline">\(\phi(\theta)\)</span> is the penalty function that measures model complexity. In machine learning the technique of adding the penalty term to the loss function is called regularization.</p>
<p>Regularization can be viewed as constraint on the model space. The techniques were originally applied to solve ill-posed problems where a slight change in the initial data could significantly alter the solution. Regularization techniques were then proposed for parameter reconstruction in a physical system modeled by a linear operator implied by a set of observations. It had long been believed that ill-conditioned problems offered little practical value, until Tikhonov published his seminal paper <span class="citation" data-cites="tikhonov1943stability">Andrey Nikolayevich Tikhonov et al. (<a href="references.html#ref-tikhonov1943stability" role="doc-biblioref">1943</a>)</span> on regularization. <span class="citation" data-cites="tikhonov1963solution">Andrei N. Tikhonov (<a href="references.html#ref-tikhonov1963solution" role="doc-biblioref">1963</a>)</span> proposed methods for solving regularized problems. In our notation, this corresponds to finding parameters <span class="math inline">\(\theta\)</span> that minimize <span class="math display">\[
\min_\theta  ||y- X\theta||^2_2   + \lambda||(\theta - \theta^{(0)})||^q_q.
\]</span> Here <span class="math inline">\(\lambda\)</span> is the weight on the regularization penalty and the <span class="math inline">\(\ell_q\)</span>-norm is defined by <span class="math inline">\(||\theta||_q = (\sum_i |\theta_i|^q)^{1/q}\)</span>. This optimization problem is a Lagrangian form of the constrained problem given by <span class="math display">\[
\mbox{minimize}_{\theta}\quad||y- X\theta||^2_2\qquad\mbox{subject to }\sum_{i=1}^{p}\phi(\theta_i) \le s.
\]</span> with <span class="math inline">\(\phi(\theta_i) = |\theta_i - \theta_i^{(0)}|^q\)</span>.</p>
<p>Later, sparsity became a primary driving force behind new regularization methods. The idea is that the vector of parameters <span class="math inline">\(\theta\)</span> is sparse, meaning that most of its elements are zero. This is a natural assumption for many models, such as the linear regression model. We will discuss the sparsity in more detail later in the book.</p>
<p>There are multiple optimization algorithms that can be used to find the solution to the penalized likelihood problem. Later in the book we will discuss the Stochastic Gradient Descent (SGD) algorithm, which is a popular tool for training deep learning models.</p>
</section>
<section id="bayesian-approach" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-approach">Bayesian Approach</h3>
<p>Similar to the likelihood maximization approach, the Bayesian approach to model estimation starts with the likelihood function. The difference is that we assume that the parameters <span class="math inline">\(\theta\)</span> are random variables and follow some prior distribution. Then we use the Bayes rule to find the posterior distribution of the parameters <span class="math display">\[
p(\theta | D) \propto L(\theta) ~ p(\theta),
\]</span> where <span class="math inline">\(p(\theta)\)</span> is the prior distribution and <span class="math inline">\(p(y | \theta)\)</span> is the likelihood function. The posterior distribution is the distribution of the parameters given the data <span class="math inline">\(D = (y_i,x_i)_{i=1}^n\)</span>. It is a distribution over the parameters, not a single value.</p>
<p>Penalized likelihood has a natural Bayesian interpretation. The penalty term corresponds to a prior distribution on the parameters: <span class="math display">\[
p(\theta) = \dfrac{1}{Z(\lambda)} \exp(-\lambda \phi(\theta))
\]</span> Then the penalized likelihood is proportional to the posterior distribution: <span class="math display">\[
p(\theta \mid y) \propto p(y | \theta) \cdot p(\theta) = L(\theta) ~ \dfrac{1}{Z(\lambda)} \exp(-\lambda \phi(\theta))
\]</span></p>
<p>This means maximizing the penalized likelihood is equivalent to finding the maximum a posteriori (MAP) estimate, which is the mode of the posterior distribution.</p>
</section>
</section>
<section id="prediction-accuracy" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="prediction-accuracy"><span class="header-section-number">11.6</span> Prediction Accuracy</h2>
<p>After we fit our model and find the optimal value of the parameter <span class="math inline">\(\theta\)</span>, denoted by <span class="math inline">\(\hat \theta\)</span>, we need to evaluate the accuracy of the predictive model. Once <span class="math inline">\(\hat{\theta}\)</span> is obtained, it involves comparing the model’s predictions to actual outcomes. We can simply use the value of the loss function from the training step to evaluate model’s predictive power. However, this only tells us how well the model fits the training data. It doesn’t tell us how well the model will perform on unseen data. To evaluate the model’s performance on unseen data, we need to use a different approach.</p>
<p>The most common approach is to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate its performance. This approach is known as the train-test split. It is a simple and effective way to evaluate how well model predicts for unseen inputs.</p>
<p>Another approach is to use cross-validation. It involves splitting the data into smaller subsets and using them to train and test the model multiple times. When our sample size is small, this allows for a more robust estimate of the model’s performance than simply splitting the data into a single training and test set. For small data sets, simple train-test split approach will be sensitive to choice of test samples, thus the estimated predicted performance will be unstable (high variance). Cross-validation helps to reduce this variance by averaging the performance across multiple folds. This makes the performance estimate more robust and less sensitive to the choice of test samples.</p>
<p>Cross-validation involves several steps. The data is randomly divided into <span class="math inline">\(k\)</span> equal-sized chunks (folds). For each fold, the model is trained on <span class="math inline">\(k-1\)</span> folds and tested on the remaining fold. This process is repeated <span class="math inline">\(k\)</span> times, ensuring each fold is used for testing once. The performance of the model is evaluated on each fold using a chosen metric, such as accuracy, precision, recall, or F1 score. The average of the performance metrics across all k folds is reported as the final estimate of the model’s performance.</p>
<p>A common choice for <span class="math inline">\(k\)</span> is 5 or 10. When <span class="math inline">\(k=n\)</span> (where <span class="math inline">\(n\)</span> is the sample size), this is known as leave-one-out cross-validation. This method can be computationally expensive but is less likely to overfit the data. Stratified cross-validation ensures that each fold contains approximately the same proportion of each class as in the entire dataset. This is important for imbalanced datasets where one class is significantly larger than the others.</p>
<p>Notice, that cross-validation requires re-training the model multiple times, which can be computationally expensive. Thus, for large datasets, we typically prefer simple train-test split. However, for small datasets, cross-validation can provide a more robust estimate of the model’s performance.</p>
<p>Either method is limited to evaluating the model’s performance on data that is available to the modeler. What if we start using our model on data that is different from the training and test sets? Unlike physics, where models often represent universal laws, data science deals with data generated by processes that may vary across contexts. For example, if we are building a model to predict the price of a house, we can train and test the model on data from a specific city. However, if we start using the model to predict the price of a house in a different city, the model might not perform as well. This is because the data from the new city might be different from the data used to train and test the model. This is known as the problem of generalization. It refers to the ability of a model to perform well on data that is different from the training and test sets.</p>
<section id="evaluation-metrics-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-for-regression">Evaluation Metrics for Regression</h3>
<p>There are several metrics that can be used to evaluate the performance of regression models. We can simply use the same function as we use for fitting the model, e.g.&nbsp;least squares <span class="math display">\[
\text{MSE} = \dfrac{1}{m}\sum_{i=1}^n (y_i -\hat y_i)^2,
\]</span> here <span class="math inline">\(\hat y_i\)</span> is the predicted value of the i-th data point by the model <span class="math inline">\(\hat y_i = f(x_i,\hat\theta)\)</span> and <span class="math inline">\(m\)</span> is the total number of data points used for the evaluation. This metric is called the <em>Mean Squared Error (MSE)</em>. It is the average squared difference between the actual and predicted values. Lower MSE indicates better model performance, as it means the model’s predictions are closer to the actual values.</p>
<p>A slight variation of this metric is Root Mean Squared Error (RMSE). This is the square root of MSE and is also commonly used due to its units being the same as the target variable. <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}}.
\]</span> However, MSE is sensitive to outliers, as it squares the errors, giving more weight to large errors. This can lead to misleading results when the data contains outliers.</p>
<p><em>Mean Absolute Error (MAE)</em> solves the sensitivity to the outliers problem. It is the mean of the absolute errors, providing a more robust measure than MSE for skewed error distributions <span class="math display">\[
\text{MAE} = \dfrac{1}{m}\sum_{i=1}^n |y_i -\hat y_i|.
\]</span> A variation of it is the <em>Mean Absolute Percentage Error (MAPE)</em>, which is the mean of the absolute percentage errors <span class="math display">\[
\text{MAPE} = \dfrac{1}{m}\sum_{i=1}^n \left | \dfrac{y_i -\hat y_i}{y_i} \right |.
\]</span></p>
<p>Alternative way to measure the predictive quality is to use the coefficient of determination, also known as the <em>R-squared</em> value, which measures the proportion of variance in the target variable that is explained by the model. Higher R-squared indicates better fit. However, R-squared can be misleading when comparing models with different numbers of features. R-squared is defined as follows <span class="math display">\[
R^2 = 1 - \dfrac{\sum_{i=1}^n (y_i -\hat y_i)^2}{\sum_{i=1}^n (y_i -\bar y_i)^2},
\]</span> where <span class="math inline">\(\bar y_i\)</span> is the mean of the target variable. R-squared is a relative measure of fit, so it can be used to compare different models. However, it is not an absolute measure of fit, so it cannot be used to determine whether a model is good or bad. It is also sensitive to the number of features in the model, so it cannot be used to compare models with different numbers of features.</p>
<p>Finally, we can use graphics to evaluate the model’s performance. For example, we can create a scatterplot of the actual and predicted values of the target variable to visually compare them. We can also plot the histogram or a boxplot of the residuals (errors) to see if they are normally distributed.</p>
</section>
<section id="evaluation-metrics-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics-for-classification">Evaluation Metrics for Classification</h3>
<p><em>Accuracy</em> is the most fundamental metric used to evaluate models. It is defined as the ratio of the number of correct predictions to the total number of predictions. The formula is given by <span class="math display">\[\text{Accuracy} = \frac{\text{TP+TN}}{\text{TP+TN+FP+FN}},\]</span> where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. However, it can be misleading for imbalanced datasets where one class is significantly larger than others. For example, if 95% of the data belongs to one class, a model that always predicts this class will be 95% accurate, even though it’s not very useful.</p>
<p>A more comprehensive understanding of model performance can be achieved by calculating the sensitivity (precision) and specificity (recall) as well as confusion matrix discussed in <a href="02-bayes.html#sec-sensitivity" class="quarto-xref"><span>Section 2.6</span></a>. The confusion matrix is</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Actual/Predicted</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Negative</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p><em>Precision</em> measures the proportion of positive predictions that are actually positive. It is useful for evaluating how good the model is at identifying true positives. <em>Recall</em> measures the proportion of actual positives that are correctly identified by the model. It is useful for evaluating how good the model is at not missing true positives.</p>
<p>Then we can use those to calculate <em>F1 Score</em> which is a harmonic mean of precision and recall, providing a balanced view of both metrics. The formula is given by <span class="math display">\[
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\]</span> Higher F1 score indicates better overall performance. If misclassifying certain instances is more costly than others, weighted metrics account for these different costs. For imbalanced datasets, metrics like F1 score or balanced accuracy are important to avoid misleading interpretations.</p>
<p>Sometimes, we use multiple metrics to get a comprehensive assessment of the model’s performance. Additionally, consider comparing the model’s performance to a baseline model or other existing models for the same task. Sometimes, it is hard to beat a “coin flip” classification model, when the model predicts the class randomly with equal probability. In regression, a simple baseline model is <span class="math inline">\(f(x_i) = \bar y\)</span>, which is the mean of the target variable.</p>
</section>
</section>
<section id="summary" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="summary"><span class="header-section-number">11.7</span> Summary</h2>
<p>In this chapter, we have traced the arc of pattern matching from its conceptual roots—as described by Feynman and practiced by experts—to its mathematical formalization in machine learning. We explored how supervised learning models map inputs to outputs using both simple parametric functions (like linear regression) and complex hierarchical structures (like deep learning). We also discussed the critical role of estimation methods, such as maximum likelihood and Bayesian inference, in finding the optimal parameters. finally, we emphasized that a model is only as good as its performance on unseen data, highlighting the importance of robust evaluation metrics and validation strategies. Together, these components form the backbone of modern predictive modeling.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bonfiglio2021molecular" class="csl-entry" role="listitem">
Bonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca, and Elena Bonanno. 2021. <span>“Molecular Aspects and Prognostic Significance of Microcalcifications in Human Pathology: <span>A</span> Narrative Review.”</span> <em>International Journal of Molecular Sciences</em> 22 (120).
</div>
<div id="ref-halevy2009unreasonable" class="csl-entry" role="listitem">
Halevy, Alon, Peter Norvig, and Fernando Pereira. 2009. <span>“The Unreasonable Effectiveness of Data.”</span> <em>IEEE Intelligent Systems</em> 24 (2): 8–12.
</div>
<div id="ref-nicosia2023history" class="csl-entry" role="listitem">
Nicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini, Federico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023. <span>“History of Mammography: <span>Analysis</span> of Breast Imaging Diagnostic Achievements over the Last Century.”</span> <em>Healthcare</em> 11 (1596).
</div>
<div id="ref-tikhonov1963solution" class="csl-entry" role="listitem">
Tikhonov, Andrei N. 1963. <span>“Solution of Incorrectly Formulated Problems and the Regularization Method.”</span> <em>Sov Dok</em> 4: 1035–38.
</div>
<div id="ref-tikhonov1943stability" class="csl-entry" role="listitem">
Tikhonov, Andrey Nikolayevich et al. 1943. <span>“On the Stability of Inverse Problems.”</span> In <em>Dokl. <span>Akad</span>. <span>Nauk</span> Sssr</em>, 39:195–98.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10-data.html" class="pagination-link" aria-label="Unreasonable Effectiveness of Data">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-regression.html" class="pagination-link" aria-label="Linear Regression">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>