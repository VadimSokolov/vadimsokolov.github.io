<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Bayes Rule – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-bl.html" rel="next">
<link href="./01-prob.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8f57c241cdbc1f937d718a8870719880.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./02-bayes.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification: Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Randomized Control Trials (RCT): Field vs Observational</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Image Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">20-theorydl.html</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Robotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./02-bayes.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><em>When the facts change, I change my mind. What do you do, sir?</em> John Maynard Keynes</p>
</blockquote>
<p>One of the key questions in the theory of learning is: <em>How do you update your beliefs in the presence of new information?</em> Bayes rule provides the answer. Conditional probability can be interpreted as updating your probability of event <span class="math inline">\(A\)</span> after you have learned the new information that <span class="math inline">\(B\)</span> has occurred. In the sense probability is also the language of how you’ll change options in the light of new evidence. For example, we need to find probability that a thrown dice shows on its upper surface an odd number and we found out that number shown is less then 4. We write <span class="math inline">\(p(A=\text{odd} \mid B = \text{less then 4})= 2/3\)</span>.</p>
<p>Probability rules allow us to change our mind if the facts change. For example, suppose that <span class="math inline">\(B = \{ B_1 , B_2 \}\)</span> consists of two pieces of information and that we are interested in <span class="math inline">\(P(A\mid B_1,B_2)\)</span>. Bayes rule simply lets you calculate this conditional probability in a sequential fashion. First, conditioning on the information contained in <span class="math inline">\(B_1\)</span>, let’s us calculate <span class="math display">\[
P( A| B_1 ) = \frac{ p(  B_1 \mid A ) P( A) }{ P( B_1 ) }
\]</span> Then, using the posterior probability <span class="math inline">\(P( A| B_1 )\)</span> as the “new” prior for the next piece of information <span class="math inline">\(B_2\)</span> lets us find <span class="math display">\[
P( A| B_1 , B_2 ) = \frac{ p(  B_2 \mid B_1 , A ) P( A \mid B_1 ) }{ P( B_2 \mid B_1 ) }
\]</span> Hence, we see that we need assessments of the two conditional probabilities <span class="math inline">\(P( B_1 \mid A )\)</span> and <span class="math inline">\(P( B_2 \mid B_1 , A )\)</span>. In many situations, the latter will be simply <span class="math inline">\(P( B_2 \mid A )\)</span> and not involve <span class="math inline">\(B_1\)</span>. The events <span class="math inline">\(( B_1, B_2 )\)</span> will be said to be conditionally independent given <span class="math inline">\(A\)</span>.</p>
<p>This concept generalizes to a sequence of events where <span class="math inline">\(B = \{ B_1,\ldots B_n \}\)</span>. When learning from data will will use this property all the time. An illustrative example will be the Black Swan problem which we discuss later.</p>
<p>Bayes’ rule is a fundamental concept in probability theory and statistics. It describes how to update our <strong>beliefs</strong> about an event based on <strong>new evidence</strong>. We start with an initial belief about the probability of an event (called the <strong>prior probability</strong>). We then observe some conditional information information (e.g.&nbsp;evidence). We use Bayes’ rule to update our initial belief based on the evidence, resulting in a new belief called the <strong>posterior probability</strong>. Remember, the formula is <span class="math display">\[
P(A\mid B) = \dfrac{P(B\mid A) P(A)}{P(B)}
\]</span> where:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Posterior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(A\mid B)\)</span> is the posterior probability of event <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> is known to happen for sure. This is the probability we’re trying to find.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Likelihood
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(B\mid A)\)</span> is the likelihood of observing event <span class="math inline">\(B\)</span> if event <span class="math inline">\(A\)</span> has occurred.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(A)\)</span> is the prior probability of event <span class="math inline">\(A\)</span> occurring. This is our initial belief about the probability of <span class="math inline">\(A\)</span> before we see any evidence.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Marginal probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(B)\)</span> is the marginal probability of observing event <span class="math inline">\(B\)</span>. This is the probability of observing B regardless of whether <span class="math inline">\(A\)</span> occurs.</p>
</div>
</div>
<p>The ability to use Bayes rule sequentially is a key in many applications, when we need to update our beliefs in the presence of new information. For examples, Bayesian learning was used by mathematician Alan Turing in England in Bletchley Park to break the German Enigma code - a development that helped the Allies win the Second World War <span class="citation" data-cites="simpson2010edward">(<a href="references.html#ref-simpson2010edward" role="doc-biblioref">Simpson 2010</a>)</span>. Turing called his algorithm Banburismus, it is a process he invented which used sequential conditional probability to infer information about the likely settings of the Enigma machine.</p>
<p>Dennis Lindley argued that we should all be trained Bayes rule and conditional probability can be simply view as disciplined probability accounting. Akin to how market odds change as evidence changes. One issue is human behavior and intuition trained in how to use Bayes rule, akin to learning the Alphabet!</p>
<div id="exm-intuition" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Intuition)</strong></span> Out intuition is not well trained to make use of a Bayes rule. If I tell you that Steve was selected at random from a representative sample. He is an 6 foot 2 and an excellent basketball player. He goes to gym every day and practices hard playing basketball. Do you think Steve is a custodian at a factory or an NBA player? Most people assume Steve is an NBA player which is wrong. The ration of NBA players to custodians is very small, probabilistically Steve is more likely to be an NBA player. Let’s look at it graphically. The key us to provide the right conditioning and to consider the prior probability! Even though the ratio of people who people who practice basketball hard is much hither among NBA players (it is 1) when compared to custodians, larger number of population means we still have more custodians in the US then NBA players.</p>
<p><span class="math display">\[\begin{align*}
\prob{\text{Practice  hard}  \mid  \text{Play in NBA}} \approx  1\\
\prob{ \text{Play in NBA}  \mid  \text{Practice  hard}} \approx  0.
\end{align*}\]</span></p>
<p>Even though you practice hard, the odds of playing in NBA are low (<span class="math inline">\(1000\)</span> players out of <span class="math inline">\(7\)</span> billion). But given you’re in NBA, you no doubt practice very hard. To understand this further, lets look at the conditional probability implication and apply Bayes rule <span class="math display">\[
p \left ( \text{Play in NBA}  \mid  \text{Practice  hard}  \right ) = \dfrac{p  \left ( \text{Practice  hard}  \mid  \text{Play in NBA} \right )p( \text{Play in NBA})}{p(\text{Practice  hard})}.
\]</span> The initial (a.k.a. prior) probability <span class="math inline">\(p(\text{Play in NBA} ) = \frac{1000}{7 \cdot 10^6} \approx 0\)</span>, makes the conditional (or, so called, posterior) probability also very small. <span class="math display">\[
p   \left ( \text{Play in NBA}  \mid  \text{Practice  hard}  \right ) \approx  0,
\]</span> <span class="math inline">\(P(\text{practice hard})\)</span> is not that small and <span class="math inline">\(P(\text{practice hard} \mid \text{play in NBA})=1\)</span>. Hence, when one ‘vevevses the conditioning’ one gets a very small probability. This makes sense!</p>
</div>
<section id="law-of-total-probability" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="law-of-total-probability"><span class="header-section-number">2.1</span> Law of Total Probability</h2>
<p>The Law of Total Probability is a fundamental rule relating marginal probabilities to conditional probabilities. It’s particularly useful when you’re dealing with a set of mutually exclusive and collectively exhaustive events.</p>
<p>Suppose you have a set of events <span class="math inline">\(B_1, B_2, ..., B_n\)</span> that are mutually exclusive (i.e., no two events can occur at the same time) and collectively exhaustive (i.e., at least one of the events must occur). The Law of Total Probability states that for any other event <span class="math inline">\(A\)</span>, the probability of <span class="math inline">\(A\)</span> occurring can be calculated as the sum of the probabilities of <span class="math inline">\(A\)</span> occurring given each <span class="math inline">\(B_i\)</span>, multiplied by the probability of each <span class="math inline">\(B_i\)</span> occurring.</p>
<p>Mathematically, it is expressed as:</p>
<p><span class="math display">\[
P(A) = \sum_{i=1}^{n} P(A\mid  B_i) P(B_i)
\]</span></p>
<div id="exm-total" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Total Probability)</strong></span> Let’s consider a simple example to illustrate this. Suppose you have two bags of balls. Bag 1 contains 3 red and 7 blue balls, while Bag 2 contains 6 red and 4 blue balls. You randomly choose one of the bags and then randomly draw a ball from that bag. What is the probability of drawing a red ball?</p>
<p>Here, the events <span class="math inline">\(B_1\)</span> and <span class="math inline">\(B_2\)</span> can be choosing Bag 1 and Bag 2, respectively. You want to find the probability of event <span class="math inline">\(A\)</span> (drawing a red ball).</p>
<p>Applying the law:</p>
<ul>
<li><span class="math inline">\(P(A|B_1)\)</span> is the probability of drawing a red ball from Bag 1, which is <span class="math inline">\(\frac{3}{10}\)</span>.</li>
<li><span class="math inline">\(P(A|B_2)\)</span> is the probability of drawing a red ball from Bag 2, which is <span class="math inline">\(\frac{6}{10}\)</span>.</li>
<li>Assume the probability of choosing either bag is equal, so <span class="math inline">\(P(B_1) = P(B_2) = \frac{1}{2}\)</span>.</li>
</ul>
<p>Using the Law of Total Probability:</p>
<p><span class="math display">\[
P(A) = P(A|B_1) \times P(B_1) + P(A|B_2) \times P(B_2)= \frac{3}{10} \times \frac{1}{2} + \frac{6}{10} \times \frac{1}{2} = \frac{9}{20}
\]</span></p>
<p>So, the probability of drawing a red ball in this scenario is <span class="math inline">\(\frac{9}{20}\)</span>.</p>
</div>
<p>This law is particularly useful in complex probability problems where direct calculation of probability is difficult. By breaking down the problem into conditional probabilities based on relevant events, it simplifies the calculation and helps to derive a solution.</p>
<div id="exm-Craps" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Craps)</strong></span> Craps is a fast-moving dice game with a complex betting layout. It’s highly volatile, but eventually your bankroll will drift towards zero. Lets look at the pass line bet. The expectation <span class="math inline">\(E(X)\)</span> governs the long run. When 7 or 11 comes up, you win. When 2,3 or 12 comes up, this is known as “craps”, you lose. When 4,5,6,8,9 or 10 comes up, this number is called the “point”, the bettor continues to roll until a 7 (you lose) or the point comes up (you win).</p>
<p>We need to know the probability of winning. The pay-out, probability and expectation for a $1 bet</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Win</th>
<th>Prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.4929</td>
</tr>
<tr class="even">
<td>-1</td>
<td>0.5071</td>
</tr>
</tbody>
</table>
<p>This leads to an edge in favor of the house as <span class="math display">\[
E(X) = 1 \cdot 0.4929 + (- 1) \cdot  0.5071 = -0.014
\]</span> The house has a 1.4% edge.</p>
<p>To calculate the probability of winning: <span class="math inline">\(P( \text{Win} )\)</span> let’s use the law of total probability <span class="math display">\[
P( \text{Win} ) = \sum_{ \mathrm{Point} } P ( \text{Win} \mid \mathrm{Point} ) P ( \mathrm{Point} )
\]</span> The set of <span class="math inline">\(P( \mathrm{Point} )\)</span> are given by</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Value</th>
<th>Probability</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>1/36</td>
<td>2.78%</td>
</tr>
<tr class="even">
<td>3</td>
<td>2/36</td>
<td>5.56%</td>
</tr>
<tr class="odd">
<td>4</td>
<td>3/36</td>
<td>8.33%</td>
</tr>
<tr class="even">
<td>5</td>
<td>4/36</td>
<td>11.1%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>5/36</td>
<td>13.9%</td>
</tr>
<tr class="even">
<td>7</td>
<td>6/36</td>
<td>16.7%</td>
</tr>
<tr class="odd">
<td>8</td>
<td>5/36</td>
<td>13.9%</td>
</tr>
<tr class="even">
<td>9</td>
<td>4/36</td>
<td>11.1%</td>
</tr>
<tr class="odd">
<td>10</td>
<td>3/36</td>
<td>8.33%</td>
</tr>
<tr class="even">
<td>11</td>
<td>2/36</td>
<td>5.56%</td>
</tr>
<tr class="odd">
<td>12</td>
<td>1/36</td>
<td>2.78%</td>
</tr>
</tbody>
</table>
<p>The conditional probabilities <span class="math inline">\(P( \text{Win} \mid \mathrm{Point} )\)</span> are harder to calculate <span class="math display">\[
P( \text{Win} \mid 7 \; \mathrm{or} \; 11 ) = 1 \; \; \mathrm{and} \; \; P( \text{Win} \mid 2 ,
3 \; \mathrm{or} \; 12 ) = 0
\]</span> We still have to work out all the probabilities of winning given the point. Suppose the point is <span class="math inline">\(4\)</span> <span class="math display">\[
P( \text{Win} \mid 4 ) = P ( 4 \; \mathrm{before} \; 7 ) = \dfrac{P(4)}{P(7)+P(4)} = \frac{3}{9} =
\frac{1}{3}
\]</span> There are 6 ways of getting a 7, 3 ways of getting a 4 for a total of 9 possibilities. Now do all of them and sum them up. You get <span class="math display">\[
P( \text{Win}) = 0.4929
\]</span></p>
</div>
</section>
<section id="independence" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="independence"><span class="header-section-number">2.2</span> Independence</h2>
<p>Historically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.</p>
<p>Using the notion of conditional probability, we can define independence of two variables. Two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>independent</strong> if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. That is, learning information <span class="math inline">\(X=x\)</span> doesn’t affect.</p>
<p>Conditional probabilities are counter intuitive. For example, one of the most important properties is typically <span class="math inline">\(p( x \mid y ) \neq p( y\mid x )\)</span>, our probabilistic assessment of <span class="math inline">\(Y\)</span> for any value <span class="math inline">\(y\)</span>. This is known as <em>Prosecutors’ Fallacy</em> as it arises when probability is used as evidence in a court of law. In the case of independence, <span class="math inline">\(p(x \mid y) = p(x)\)</span> and <span class="math inline">\(p(y \mid x) = p(y)\)</span>. Specifically, the probability of innocence given the evidence is not the same as the probability of evidence given innocence. It is very important to ask the question “what exactly are we conditioning on?” Usually, the observed evidence or data. Probability, of course, given evidence was one of the first applications of Bayes. Central to personalized probability. Clearly this is a strong condition and rarely holds in practice.</p>
<p>We just derived an important relation, that allows us to calculate conditional probability <span class="math inline">\(p(x \mid y)\)</span> when we know joint probability <span class="math inline">\(p(x,y)\)</span> and marginal probability <span class="math inline">\(p(y)\)</span>. The total probability or evidence can be calculated as usual, via <span class="math inline">\(p(y) = \sum_{x}p(x,y)\)</span>.</p>
<p>We will see that independence will lead to a different conclusion that the Bayes conditional probability decomposition: specifically, independence yields <span class="math inline">\(p( x,y ) = p(x) p(y)\)</span> and Bayes says <span class="math inline">\(p(x ,y) = p(x)p(x \mid y)\)</span>.</p>
<p>We need to specify distribution on each of those variables. Two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values variables separately The joint distribution will be giving by <span class="math display">\[
p(x,y) = p(x)p(y).
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then probability of the event <span class="math inline">\(X\)</span> and event <span class="math inline">\(Y\)</span> happening at the same time is the product of individual probabilities. From the conditional distribution formula it follows that <span class="math display">\[
p(x \mid y) = \dfrac{p(x,y)}{p(y)} = \dfrac{p(x)p(y)}{p(y)} = p(x).
\]</span> Another way to think of independence is to say that knowing the value of <span class="math inline">\(Y\)</span> doesn’t tell us anything about possible values of <span class="math inline">\(X\)</span>. For example when tossing a coin twice, the probability of getting <span class="math inline">\(H\)</span> in the second toss does not depend on the outcome of the first toss.</p>
<p>The expression of independence expresses the fact that knowing <span class="math inline">\(X=x\)</span> tells you nothing about <span class="math inline">\(Y\)</span>. In the coin tossing example, if <span class="math inline">\(X\)</span> is the outcome of the first toss and <span class="math inline">\(Y\)</span> is the outcome of the second toss <span class="math display">\[
\prob{ X=H  \mid  Y=T } = \prob{X=H  \mid  Y=H } = \prob{X=H}.
\]</span></p>
<p>Let’s do a similar example which illustrates this point clearly. Most people would agree with the following conditional probability assessments</p>
</section>
<section id="naive-bayes" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">2.3</span> Naive Bayes</h2>
<p>Use of the Bayes rule allows us to build our first predictive model, called Naive Bayes classifier. Naive Bayes is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDMxNH0.uR3YM4nh6u7PkPxdhujkZSsjTt2i73Emgkl_h_IOh4k -->
<p><img src="fig/color-shape-size.svg" class="img-fluid"></p>
<p>Although it’s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification. In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features). In summary, the advantages are:</p>
<ul>
<li>It’s relatively simple to understand and build</li>
<li>It’s easily trained, even with a small dataset</li>
<li>It’s fast!</li>
<li>It’s not sensitive to irrelevant features</li>
</ul>
<p>The main disadvantage is that it assumes every feature is independent, which isn’t always the case.</p>
<p>Let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Fruit</th>
<th>Long</th>
<th>Sweet</th>
<th>Yellow</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Banana</td>
<td>400</td>
<td>350</td>
<td>450</td>
<td>500</td>
</tr>
<tr class="even">
<td>Orange</td>
<td>0</td>
<td>150</td>
<td>300</td>
<td>300</td>
</tr>
<tr class="odd">
<td>Other</td>
<td>100</td>
<td>150</td>
<td>50</td>
<td>200</td>
</tr>
<tr class="even">
<td>Total</td>
<td>500</td>
<td>650</td>
<td>800</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>From this data we can calculate marginal probabilities</p>
<ul>
<li>50% of the fruits are bananas</li>
<li>30% are oranges</li>
<li>20% are other fruits</li>
</ul>
<p>Based on our training set we can also say the following:</p>
<ul>
<li>From 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow</li>
<li>Out of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow</li>
<li>From the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.</li>
</ul>
<p>Give the evidence <span class="math inline">\(E\)</span> (<span class="math inline">\(L\)</span> = Long, <span class="math inline">\(S\)</span> = Sweet and <span class="math inline">\(Y\)</span> = Yellow) we can calculate the probability of each class <span class="math inline">\(C\)</span> (<span class="math inline">\(B\)</span> = Banana, <span class="math inline">\(O\)</span> = Orange or <span class="math inline">\(F\)</span> = Other Fruit) using Bayes’ Theorem: <span class="math display">\[\begin{align*}
P(B \mid E) = &amp; \frac{P(L \mid B)P(S \mid B)P(Y \mid B)P(B)}{P(L)P(S)P(Y)}\\
=&amp;\frac{0.8\times 0.7\times 0.9\times 0.5}{P(E)}=\frac{0.252}{P(E)}
\end{align*}\]</span></p>
<p>Orange: <span class="math display">\[
P(O\mid E)=0.
\]</span></p>
<p>Other Fruit: <span class="math display">\[\begin{align*}
P(F \mid E) &amp; = \frac{P(L \mid F)P(S \mid F)P(Y \mid F)P(F)}{P(L)P(S)P(Y)}\\
=&amp;\frac{0.5\times 0.75\times 0.25\times 0.2}{P(E)}=\frac{0.01875}{P(E)}
\end{align*}\]</span></p>
<p>In this case, based on the higher score, we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.</p>
<p>Now that we’ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g.&nbsp;“spam” and “ham” in case of spam detection) as classes we can start to classify documents and text automatically.</p>
<div id="exm-naivebayes" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Spam Filtering)</strong></span> Original spam filtering algorithm was based on Naive Bayes.The “naive” aspect of Naive Bayes comes from the assumption that inputs (words in the case of text classification) are conditionally independent, given the class label. Naive Bayes treats each word independently, and the model doesn’t capture the sequential or structural information inherent in the language. It does not consider grammatical relationships or syntactic structures. The algorithm doesn’t understand the grammatical rules that dictate how words should be combined to form meaningful sentences. Further, it doesn’t understand the context in which words appear. For example, it may treat the word “bank” the same whether it refers to a financial institution or the side of a river bank. Despite its simplicity and the naive assumption, Naive Bayes often performs well in practice, especially in text classification tasks.</p>
<p>We start by collecting a dataset of emails labeled as “spam” or “not spam” (ham) and calculate the prior probabilities of spam (<span class="math inline">\(P(\text{spam})\)</span>) and not spam (<span class="math inline">\(P(\text{ham})\)</span>) based on the training dataset, by simply counting the proportions of each in the data.</p>
<p>Then each email gets converted into a bag-of-words representation (ignoring word order and considering only word frequencies). Then, we create a vocabulary of unique words from the entire dataset <span class="math inline">\(w_1,w_2,\ldots,w_N\)</span> and calculate conditional probabilities <span class="math display">\[
P(\mathrm{word}_i  \mid  \text{spam}) = \frac{\text{Number of spam emails containing }\mathrm{word}_i}{\text{Total number of spam emails}}, ~ i=1,\ldots,n
\]</span> <span class="math display">\[
P(\mathrm{word}_i  \mid  \text{ham}) = \frac{\text{Number of ham emails containing }\mathrm{word}_i}{\text{Total number of ham emails}}, ~ i=1,\ldots,n
\]</span></p>
<p>Now, we are ready to use our model to classify new emails. We do it by calculating the posterior probability using Bayes’ theorem. Say email has a set of <span class="math inline">\(k\)</span> words <span class="math inline">\(\text{email} = \{w_{e1},w_{e2},\ldots, w_{ek}\}\)</span>, then <span class="math display">\[
P(\text{spam}  \mid  \text{email}) = \frac{P(\text{email}  \mid  \text{spam}) \times P(\text{spam})}{P(\text{email})}
\]</span> Here <span class="math display">\[
P(\text{email}  \mid  \text{spam}) = P( w_{e1}  \mid  \text{spam})P( w_{e2}  \mid  \text{spam})\ldots P( w_{ek}  \mid  \text{spam})
\]</span> We calculate <span class="math inline">\(P(\text{spam} \mid \text{email})\)</span> is a similar way.</p>
<p>Finally, we classify the email as spam or ham based on the class with the highest posterior probability.</p>
<p>Suppose you have a spam email with the word “discount” appearing. Using Naive Bayes, you’d calculate the probability that an email containing “discount” is spam (<span class="math inline">\(P(\text{spam} \mid \text{discount})\)</span>) and ham (<span class="math inline">\(P(\text{ham} \mid \text{discount})\)</span>), and then compare these probabilities to make a classification decision.</p>
<p>While the naive assumption simplifies the model and makes it computationally efficient, it comes at the cost of a more nuanced understanding of language. More sophisticated models, such as transformers, have been developed to address these limitations by considering the sequential nature of language and capturing contextual relationships between words.</p>
<p>In summary, naive Bayes, due to its simplicity and the naive assumption of independence, is not capable of understanding the rules of grammar, the order of words, or the intricate context in which words are used. It is a basic algorithm suitable for certain tasks but may lack the complexity needed for tasks that require a deeper understanding of language structure and semantics.</p>
</div>
</section>
<section id="real-world-bayes" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="real-world-bayes"><span class="header-section-number">2.4</span> Real World Bayes</h2>
<div id="exm-clicker" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (Google random clicker)</strong></span> Google provides a service where they ask visitors to your website ato answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories</p>
<ol type="1">
<li>Random Clicker (RC)</li>
<li>Truthful Clicker (TC)</li>
</ol>
<p>There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also giving the infor- mation that the expected fraction of random clickers is 0.3. After a trial period, you get the following survey results: 65% said Yes and 35% said No.</p>
<p>The qustion is How many people people who are truthful clickers answered yes <span class="math inline">\(P(Y\mid TC)\)</span>?</p>
<p>We are given <span class="math inline">\(P(Y\mid RC) = P(N\mid RC) = 0.5\)</span>, <span class="math inline">\(P(RC)= 0.3\)</span> and <span class="math inline">\(P(Y)\)</span> = 0.65</p>
<p>The totla probability is <span class="math display">\[
P(Y) = P(Y\mid RC)P(RC) + P(Y\mid TC)P(TC) = 0.65,
\]</span> Thus <span class="math display">\[
P(Y\mid TC) = (P(Y) - P(Y\mid RC)P(RC))/P(TC) = (0.65-0.5\cdot 0.3)/0.7 = 0.71
\]</span></p>
</div>
<div id="exm-Scorpion" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (USS Scorpion sank 5 June, 1968 in the middle of the Atlantic.)</strong></span> Experts placed bets of each casualty and how each would affect the sinking. Undersea soundings gave a prior on location. Bayes rule: <span class="math inline">\(L\)</span> is location and <span class="math inline">\(S\)</span> is scenario <span class="math display">\[
p (L \mid S) = \frac{ p(S \mid L) p(L)}{p(S)}
\]</span> The Navy spent <span class="math inline">\(5\)</span> months looking and found nothing. Build a probability map: within <span class="math inline">\(5\)</span> days, the submarine was found within <span class="math inline">\(220\)</span> yards of most likely probability!</p>
<p>A similar story happened during the search of an Air France plane that flew from Rio to Paris.</p>
</div>
<!-- ::: {#exm-evidence}
## Evidence
If $N=4$, $M=47$ out of $T=51$, then gives evidence of $2.5$ to $1$ in favor of $\mathcal{H}_1$

How long a sequence do you need to look at? Calculate the expected log odds. Turing and Good figured you needed sequences of about length
$400$. Can also look at doubles and triples. See MacKay for further details.
::: -->
<div id="exm-Wald" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Wald and Airplane Safety)</strong></span> Many lives were saved by analysis of conditional probabilities performed by Abraham Wald during the Second World War. He was analyzing damages on the US planes that came back from bombarding missions in Germany. Somebody suggested to analyze distribution of the hits over different parts of the plane. The idea was to find a pattern in the damages and design a reinforcement strategy.</p>
<p>After examining hundreds of damaged airplanes, researchers came up with the following table</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Location</th>
<th style="text-align: right;">Number of Planes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">53</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">65</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">96</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">434</td>
</tr>
</tbody>
</table>
<p>We can convert those counts to probabilities</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Location</th>
<th style="text-align: right;">Number of Planes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">0.08</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">0.15</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">0.67</td>
</tr>
</tbody>
</table>
<p>We can conclude the the most likely area to be damaged on the returned planes was the wings and fuselage. <span class="math display">\[
\prob{\mbox{hit on wings or fuselage } \mid \mbox{returns safely}} = 0.67
\]</span> Wald realized that analyzing damages only on survived planes is not the right approach. Instead, he suggested that it is essential to calculate the inverse probability <span class="math display">\[
\prob{\mbox{returns safely} \mid \mbox{hit on wings or fuselage }} = ?
\]</span> To calculate that, he interviewed many engineers and pilots, he performed a lot field experiments. He analyzed likely attack angles. He studied the properties of a shrapnel cloud from a flak gun. He suggested to the army that they fire thousands of dummy bullets at a plane sitting on the tarmac. Wald constructed a ‘probability model’ careful to reconstruct an estimate for the joint probabilities. Table below shows the results.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Hit</th>
<th style="text-align: right;">Returned</th>
<th style="text-align: right;">Shut Down</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">57</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">46</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">434</td>
<td style="text-align: right;">33</td>
</tr>
</tbody>
</table>
<p>Which allows us to estimate joint probabilities, for example <span class="math display">\[
\prob{\mbox{outcome = returns safely} , \mbox{hit  =  engine }} = 53/800 = 0.066
\]</span> We also can calculate the conditional probabilities now <span class="math display">\[
\prob{\mbox{outcome = returns safely} \mid  \mbox{hit  =  wings or fuselage  }} = \dfrac{434}{434+33} = 0.93.
\]</span> Should we reinforce wings or fuselage? Which part of the airplane does need ot be reinforced? <span class="math display">\[
\prob{\mbox{outcome = returns safely} \mid  \mbox{hit  =  engine  }} = \dfrac{53}{53+57} = 0.48
\]</span> Here is another illustration taken from Economics literature. This insight led to George Akerlof winning the Nobel Prize for the concept of asymmetric information.</p>
</div>
<div id="exm-jar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Coin Jar)</strong></span> Large jar containing 1024 fair coins and one two-headed coin. You pick one at random and flip it <span class="math inline">\(10\)</span> times and get all heads. What’s the probability that the coin is the two-headed coin? The probability of initially picking the two headed coin is 1/1025. There is 1/1024 chance of of getting <span class="math inline">\(10\)</span> heads in a row from a fair coin. Therefore, it’s a <span class="math inline">\(50/50\)</span> bet.</p>
<p>Let’s do the formal Bayes rule math. Let <span class="math inline">\(E\)</span> be the event that you get <span class="math inline">\(10\)</span> Heads in a row, then</p>
<p><span class="math display">\[
P \left ( \mathrm{two \; headed}  \mid  E \right ) = \frac{ P \left ( E  \mid  \mathrm{ two \; headed}  \right )P \left (  \mathrm{ two \; headed} \right )}
{P \left ( E  \mid  \mathrm{ fair}  \right )P \left ( \mathrm{ fair} \right ) + P \left ( E  \mid  \mathrm{ two \; headed}  \right )P \left ( \mathrm{ two \; headed} \right )}
\]</span> Therefore, the posterior probability <span class="math display">\[
P \left (  \mathrm{ two \; headed}  \mid  E \right ) = \frac{ 1 \times \frac{1}{1025} }{ \frac{1}{1024} \times \frac{1024}{1025} + 1 \times \frac{1}{1025} } = 0.50
\]</span> What’s the probability that the next toss is a head? Using the law of total probability gives</p>
<p><span class="math display">\[\begin{align*}
  P( H ) &amp;= P( H  \mid  \mathrm{ two \; headed} )P( \mathrm{ two \; headed}  \mid E ) +  P( H  \mid  \mathrm{ fair} )P( \mathrm{ fair}  \mid E) \\
  &amp; = 1 \times \frac{1}{2} + \frac{1}{2} \times \frac{1}{2} = \frac{3}{4}
\end{align*}\]</span></p>
</div>
<div id="exm-monty" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Monty Hall Problem)</strong></span> Another example of a situation when calculating probabilities is counterintuitive. The Monte Hall problems was named after the host of the long-running TV show Let’s make a Deal. The original solution was proposed by Marilyn vos Savant, who had a column with the correct answer that many Mathematicians thought was wrong!</p>
<p>The game set-up is as follows. A contestant is given the choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats. The game is as follows:</p>
<ol type="1">
<li>You pick a door.</li>
<li>Monty then opens one of the other two doors, revealing a goat. He can’t open your door or show you a car</li>
<li>You have the choice of switching doors.</li>
</ol>
<p>The question is, is it advantageous to switch? The answer is yes. The probability of winning if you switch is 2/3 and if you don’t switch is 1/3.</p>
<p>Conditional probabilities allow us to answer this question. Assume you pick door 2 (event <span class="math inline">\(A\)</span>) at random, given that the host opened Door 3 and showed a goat (event B), we need to calculate <span class="math inline">\(P(A\mid B)\)</span>. The prior probability that the car is behind Door 2 is <span class="math inline">\(P(A) =  1/3\)</span> and <span class="math inline">\(P(B\mid A) = 1\)</span>, if the car is behind Door 2, the host has no choice but to open Door 3. The Bayes rule then gives us <span class="math display">\[
P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)} = \frac{1/3}{1/2} = \frac{2}{3}.
\]</span> The overall probability of the host opening Door 3 <span class="math display">\[
P(B) = (1/3 \times 1/2) + (1/3 \times 1) = 1/6 + 1/3 = 1/2.
\]</span></p>
<p>The posterior probability that the car is behind Door 2 after the host opens Door 3 is 2/3. It is to your advantage to switch doors.</p>
</div>
<div id="exm-prosecutors" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (Prosecutors Fallacy)</strong></span> The Prosecutor’s Fallacy is a logical error that occurs when a prosecutor presents evidence or statistical data in a way that suggests a defendant’s guilt, even though the evidence is not as conclusive as it may seem. This fallacy arises from a misunderstanding or misrepresentation of conditional probabilities and <strong>not understanding that</strong> that <span class="math display">\[
P(E\mid G) \ne P(G\mid E)
\]</span></p>
<p>It involves confusion between the probability of two events: the probability of the evidence <span class="math inline">\(E\)</span>, given the defendant’s guilt (which is what the prosecutor may be presenting), and the probability of the defendant’s guilt <span class="math inline">\(G\)</span>, given the evidence (which is what is often of more interest in a trial).</p>
<p>Here’s a simplified example to illustrate the Prosecutor’s Fallacy. Suppose a crime has been committed, and DNA evidence is found at the crime scene. The prosecutor claims that the probability of finding this particular DNA at the scene, given the defendant’s innocence, is very low (making the evidence seem incriminating). However, the Prosecutor’s Fallacy occurs when the prosecutor incorrectly assumes that this low probability implies a low probability of the defendant’s innocence. In reality, the probability of the DNA being found at the crime scene (given the defendant’s innocence) might also be low if the DNA is relatively rare but not exclusive to the defendant.</p>
<p>The fallacy often arises from a failure to consider the base rate or prior probability of the event being investigated. To avoid the Prosecutor’s Fallacy, it’s crucial to carefully distinguish between the probability of the evidence given the hypothesis (guilt or innocence) and the probability of the hypothesis given the evidence.</p>
<p>Consider a more concrete example of base rate fallacy. Say we have a witness who is 80% certain she saw a “checker” (<span class="math inline">\(C\)</span>) taxi in the accident. We need to calculate <span class="math inline">\(P(C\mid E)\)</span>. Assiming the base rate of 20% <span class="math inline">\(P(C) = 0.2\)</span>, we get <span class="math display">\[
P(C\mid E) = \dfrac{P(E\mid C)P(C)}{P(E)} = \dfrac{0.8\cdot 0.2}{0.8\cdot 0.2 + 0.2\cdot 0.8} = 0.5
\]</span> The witness identification accuracy <span class="math inline">\(P(C\mid E) = 0.8\)</span> is called the sensitivity.</p>
<p>Even with a highly accurate witness, the probability that the identified taxi is a Checker will be less than 80%, reflecting the impact of the base rate. Ignoring the base rate can lead to a significant overestimation of the probability of the identified event.</p>
</div>
<div id="exm-law" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (Law Example)</strong></span> &nbsp;</p>
<!-- (https://jgscott.github.io/STA371H_Spring2018/files/DataScience.pdf) -->
<p>Suppose you’re serving on a jury in the city of New York, with a population of roughly 10 million people. A man stands before you accused of murder, and you are asked to judge whether he is guilty <span class="math inline">\(G\)</span> or not guilty <span class="math inline">\(\bar G\)</span>. In his opening remarks, the prosecutor tells you that the defendant has been arrested on the strength of a single, overwhelming piece of evidence: that his DNA matched a sample of DNA taken from the scene of the crime. Let’s denote this evidence by the letter <span class="math inline">\(D\)</span>. To convince you of the strength of this evidence, the prosecutor calls a forensic scientist to the stand, who testifies that the probability that an innocent person’s DNA would match the sample found at the crime scene is only one in a million. The prosecution then rests its case. Would you vote to convict this man? If you answered “yes,” you might want to reconsider! You are charged with assessing <span class="math inline">\(P(G \mid D)\)</span> - that is, the probability that the defendant is guilty, given the information that his DNA matched the sample taken from the scene. Bayes’ rule tells us that <span class="math display">\[
P(G\mid D)= P(G)P(D\mid G)/P(D), ~ P(D) = P(D \mid G)P(G) + P(D \mid \bar G)P(\bar G)
\]</span> We know the following quantities:</p>
<ul>
<li>The prior probability of guilt, <span class="math inline">\(P(G)\)</span>, is about one in 10 million. New York City has 10 million people, and one of them committed the crime.</li>
<li>The probability of a false match, <span class="math inline">\(P(D \mid \bar G)\)</span>, is one in a million, because the forensic scientist tested to this fact.</li>
</ul>
<p>To use Bayes’ rule, let’s make one additional assumption: that the likelihood, <span class="math inline">\(P(D\mid  G)\)</span>, is equal to 1. This means we’re assuming that, if the accused were guilty, there is a 100% chance of seeing a positive result from the DNA test. Let’s plug these numbers into Bayes’ rule and see what we get: <span class="math display">\[
P(G\mid D) = 0.09
\]</span> The probability of guilt looks to be only 9%! This result seems shocking in light of the forensic scientist’s claim that <span class="math inline">\(P(D \mid \bar
G)\)</span> is so small: a “one in a million chance” of a positive match for an innocent person. Yet the prior probability of guilt is very low <span class="math inline">\(P(G)\)</span> is a mere one in 10 million - and so even very strong evidence still only gets us up to <span class="math inline">\(P(G | D) = 0.09\)</span>.</p>
<p>Conflating <span class="math inline">\(P(\bar G \mid  D)\)</span> with <span class="math inline">\(P(D \mid \bar G)\)</span> is a serious error in probabilistic reasoning. These two numbers are typically very different from one another, because conditional probabilities aren’t symmetric. As we’ve said more than once, <span class="math inline">\(P(\text{practices hard} \mid \text{plays in NBA}) \approx 1\)</span>, while <span class="math inline">\(P(\text{plays in NBA} \mid \text{practices hard}) \approx 0\)</span>. An alternate way of thinking about this result is the following. Of the 10 million innocent people in New York, ten would have DNA matches merely by chance. The one guilty person would also have a DNA match. Hence there are 11 people with a DNA match, only one of whom is guilty, and so <span class="math inline">\(P(G \mid D) \approx 1/11\)</span>. Your intuition may mislead, but Bayes’ rule never does!</p>
</div>
<div id="exm-island" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 (Island Problem)</strong></span> There are <span class="math inline">\(N+1\)</span> people on the island and one is a criminal. We have probability of a trait of a criminal equal to <span class="math inline">\(p\)</span>, which is <span class="math inline">\(p = P(E\mid I)\)</span>, the probability of evidence, given innocence. Then we have a suspect who is matching the trait and we need to find probability of being guilty, given the evidence <span class="math inline">\(P(G \mid E)\)</span>. It is easier to do the Bayes rule in the odds form. There are three components to the calculations: the prior odds of innocence, <span class="math display">\[
O ( I ) = P (G) / P ( I ),
\]</span> the Bayes factor, <span class="math display">\[
\frac{P(E\mid G)}{P(E\mid I)}.
\]</span> and the posterior odds of innocence. <span class="math display">\[
    O(I\mid E) = \dfrac{P(G\mid E)}{P(I\mid E)} = \dfrac{1}{Np}.
\]</span></p>
<p>The Cromwell’s rule states that the use of prior probability of 1 or 0 should be avoided except when it is known for certain that the probability is 1 or 0. It is named after Oliver Cromwell who wrote to the General Assembly of he Church of Scotland in 1650 “<em>I beseech you, in the bowels of Christ, it is possible that you may be mistaken</em>”. In other words, using the Bayes rule <span class="math display">\[
P(G\mid E) = \dfrac{P(E\mid G)}{P(E)}P(G),
\]</span> if <span class="math inline">\(P(G)\)</span> is zero, it does not matter what the evidence is. Symmetrically, probability of innocence is zero if the evidence is certain. In other words, if <span class="math inline">\(P(E\mid I) = 0\)</span>, then <span class="math inline">\(P(I\mid E) = 0\)</span>. This is a very strong statement. It is not always true, but it is a good rule of thumb, it is a good way to avoid the prosecutor’s fallacy.</p>
</div>
<div id="exm-nakamura" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (Nakamura’s Alleged Cheating)</strong></span> In our paper <span class="citation" data-cites="maharaj2023kramnik">Maharaj, Polson, and Sokolov (<a href="references.html#ref-maharaj2023kramnik" role="doc-biblioref">2023</a>)</span>, we provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex-world champion) and Hikaru Nakamura . Kramnik called into question Nakamura’s 45.5 out of 46 win streak in a 3+0 online blitz contest at chess.com. In this example we reproduce this paper and assess the weight of evidence using an a priori probabilistic assessment of Viswanathan Anand and the streak evidence of Kramnik. Our analysis shows that Nakamura has 99.6 percent chance of not cheating given Anand’s prior assumptions.</p>
<p>We start by addressing the argument of Kramnik which is based on the fact that the probability of such a streak is very small. This falls into precisely the Prosecutor’s Fallacy. Let introduce the notations. We denote by <span class="math inline">\(G\)</span> the event of being guilty and <span class="math inline">\(I\)</span> the event of innocence. We use <span class="math inline">\(E\)</span> to denote evidence. In our case the evidence is the streak of wins by Nakamura. The Kramnik’s argument is that probability of observing the streak is very low, thus we might have a case of cheating. This is the prosecutor’s fallacy <span class="math display">\[
P(I \mid E) \neq P(E \mid I).
\]</span> Kramnik’s calculations neglects other relevant factors, such as the prior probability of the cheating. The prosecutor’s fallacy can lead to an overestimation of the strength of the evidence and may result in an unjust conviction. In the cheating problem, at the top level of chess prior probability of <span class="math inline">\(P(G)\)</span> is small! According to a recent statement by Viswanathan Anand, the probability of cheating is <span class="math inline">\(1/10000\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//anand-prior.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Anand’s Prior</figcaption>
</figure>
</div>
<p>Given the prior ratio of cheaters to not cheaters is <span class="math inline">\(1/N\)</span>, meaning out of <span class="math inline">\(N+1\)</span> players, there is one cheater, the Bayes calculations requires two main terms. The first one is the prior odds of guilt: <span class="math display">\[
O ( G ) = P (I) / P ( G ).
\]</span> Here <span class="math inline">\(P(I)\)</span> and <span class="math inline">\(P(G)\)</span> are the prior probabilities of innocence and guilt respectively.</p>
<p>The second term is the Bayes factor, which is the ratio of the probability of the evidence under the guilt hypothesis to the probability of the evidence under the innocence hypothesis. The Bayes factor is given by <span class="math display">\[
    L(E\mid G) = \frac{P(E\mid I)}{P(E\mid G)}.
\]</span></p>
<p>Product of the Bayes factor and the prior odds is the posterior odds of guilt, given the evidence. The posterior odds of guilt is given by <span class="math display">\[
    O(G\mid E) = O(G) \times L(E\mid G).
\]</span></p>
<p>The odds of guilty is <span class="math display">\[
    O ( G )  = \dfrac{N/(N+1)}{1/(N+1)} = N.
\]</span></p>
<p>The Bayes factor is given by <span class="math display">\[
\frac{P(E\mid I)}{P(E\mid G)} = \dfrac{p}{1} = p.
\]</span> Thus, the posterior odds of guilt are <span class="math display">\[
    O(G\mid E) = Np.
\]</span> There are two numbers we need to estimate to calculate the odds of cheating given the evidence, namely the prior probability of cheating given via <span class="math inline">\(N\)</span> and the probability of a streak <span class="math inline">\(p = P(E\mid I)\)</span>.</p>
<p>There are multiple ways to calculate the probability of a streak. We can use the binomial distribution, the negative binomial distribution, or the Poisson distribution. The binomial distribution is the most natural choice. The probability of a streak of <span class="math inline">\(k\)</span> wins in a row is given by <span class="math display">\[
    P(E\mid I) = \binom{N}{k} q^k (1-q)^{N-k}.
\]</span> Here <span class="math inline">\(q\)</span> is is the probability of winning a single game. Thus, for a streak of 45 wins in a row, we have <span class="math inline">\(k = 45\)</span> and <span class="math inline">\(N = 46\)</span>. We encode the outcome of a game as <span class="math inline">\(1\)</span> for a win and <span class="math inline">\(0\)</span> for a loss or a draw. The probability of a win is <span class="math inline">\(q =  0.8916\)</span> (Nakamura’s Estimate, he reported on his YouTube channel). The probability of a streak is then 0.029. The individual game win probability is calculated from the ELO rating difference between the players.</p>
<p>The ELO rating of Hikaru is 3300 and the average ELO rating of his opponents is 2950, according to Kramnik. The difference of 350 corresponds to the odds of winning of <span class="math inline">\(wo = 10^{350/400} = 10^{0.875} = 7.2\)</span>. The probability of winning a single game is <span class="math inline">\(q = wo/(1+wo) = 0.8916\)</span>.</p>
<p>Then we use the Anand’s prior of <span class="math inline">\(N = 10000\)</span> to get the posterior odds of cheating given the evidence of a streak of 45 wins in a row. The posterior odds of being innocent are 285. The probability of cheating is then <span class="math display">\[
P(G\mid E) = 1/(1+O(G\mid E)) = 0.003491.
\]</span> Therefore the probability of innocent <span class="math display">\[
    P(I\mid E) = \frac{Np}{Np+1} = 0.9965.
\]</span></p>
<p>For completeness, we perform sensitivity analysis and also get the odds of not cheating for <span class="math inline">\(N = 500\)</span>, which should be high prior probability given the status of the player and the importance of the event. We get <span class="math display">\[
    P(I\mid E) = \frac{Np}{Np+1} = 0.9445.
\]</span></p>
<p>There are several assumptions we made in this analysis.</p>
<ul>
<li>Instead of calculating game-by-game probability of winning, we used the average probability of winning of 0.8916, provided by Nakamura himself. This is a reasonable assumption given the fact that Nakamura is a much stronger player than his opponents. This assumption slightly shifts posterior odds in favor of not cheating. Due to Jensen inequality, we have <span class="math inline">\(E(q^{50}) &gt; E(q)^{50}\)</span>. Expected value of the probability of winning a single game is <span class="math inline">\(E(q) = 0.8916\)</span> and the expected value of the probability of a streak of 50 wins is <span class="math inline">\(E(q^{50})\)</span>. We consider the difference between the two to be small. Further, there is some correlation between the games, which also shifts the posterior odds in favor of not cheating. For example, some players are on tilt. Given they lost first game, they are more likely to lose the second game.</li>
<li>There are many ways to win 3+0 unlike in classical chess. For example, one can win on time. We argue that probability of winning calculated from the ELO rating difference is underestimated.</li>
</ul>
<p>Next, we can use the Bayes analysis to solve an inverse problem and to find what prior you need to assume and how long of a sequence you need to observe to get 0.99 posterior? Small sample size, we have <span class="math inline">\(p\)</span> close to 1. <a href="#fig-n-p" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows the combination of prior (<span class="math inline">\(N\)</span>) and the probability of a streak (<span class="math inline">\(p\)</span>) that gives posterior odds of 0.99.</p>
<p>Indeed, the results of the Bayesian analysis contradict the results of a traditional p-value based approach. A p-value is a measure used in frequentist statistical hypothesis testing. It represents the probability of obtaining the observed results, or results more extreme, assuming that the null hypothesis is true. The null hypothesis is a default position that Nakamura is not cheating and we compare the ELO-based expected win probability of <span class="math inline">\(q=0.8916\)</span> to the observed on of <span class="math inline">\(s=45/46=0.978\)</span>. Under the null hypothesis, Nakamura should perform at the level predicted by <span class="math inline">\(q\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fl">0.8916</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">dbinom</span>(<span class="dv">45</span>,<span class="dv">46</span>,q)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>odds <span class="ot">=</span> p<span class="sc">*</span>N</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="dv">1-1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>odds))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>odds))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.0035</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(N<span class="sc">*</span>p<span class="sc">/</span>(N<span class="sc">*</span>p<span class="sc">+</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1</code></pre>
</div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.006</span>, <span class="at">to=</span><span class="fl">0.07</span>, <span class="at">length.out=</span><span class="dv">500</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">500</span>,<span class="dv">10000</span>, <span class="at">by=</span><span class="dv">250</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">99</span><span class="sc">/</span>N,N,<span class="at">xlab=</span><span class="st">"p"</span>, <span class="at">ylab=</span><span class="st">"N"</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span><span class="st">"blue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-n-p" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-n-p-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-bayes_files/figure-html/fig-n-p-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-n-p-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The combination of prior (<span class="math inline">\(N\)</span>) and the probability of a streak (<span class="math inline">\(p\)</span>) that gives posterior odds of 0.99.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<!-- ::: {#exm-steroid}
## Steve Scott Jose Canuescio
:::

::: {#exm-olympics}
## Don Berry Paper 
In his paper "Inferences aboutTestosterone Abuse among Athletes" @berry2004inferences 
::: -->
<p>David Hume discussed the problem similar to the Island problem is hiw “On Miracles” essay. Hume is making the following argument on miracles:</p>
<blockquote class="blockquote">
<p>“<em>…no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavours to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior.</em>”</p>
</blockquote>
<p>One can view this as an application of Island problem essentially. Assuming probability of a miricle <span class="math inline">\(A\)</span> is $ p( A) = p $ and $ p( not ; A ) = 1 -p <span class="math inline">\(. Then Bayes rule gives\)</span>$ p( A| a ) = <br>
$$ Prosecutors’ fallacy, $ p( a| not ; A) - p( a| A) $ in general.</p>
<p>In Hume’s assessment of miracles (has to be something not in the laws of nature) we have $ p(A) = 10^{-6} $. This assessment takes into account background information, <span class="math inline">\(I\)</span>. Rare to have a contradiction to the laws of nature. More informative to write $ p( A | I ) $. Furthermore, we take $ p( a| A) =0.99 $. Hardest bit is to assess $p(a | not ; A ) $. The “frequency” of faked miracles and mankinds propensity to be marvellous. We assess $ p(a | not ; A ) = 10^{-3} <span class="math inline">\(.
This yields chance of miracle to be unlikely as\)</span>$ p( A| a ) = ^{-3}. $$ Feynman considers the inverse problem: can we learn the laws of nature purely from empirical observation? Uses chess as an example. Is it a miracle that we have two bishops of the same color? No! according to Hume. We just didn’t know the laws of nature (a.k.a. model).</p>
<div id="exm-Sally" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14 (Sally Clark Case: Independence or Bayes Rule?)</strong></span> To show that independence can lead to dramatically different results from Bayes conditional probabilities, consider the Sally Clark case. Sally Clark was accused and convicted of killing her two children who could have both died of SIDS. One explanation is that this was a random occurrence, the other one is that they both died of sudden infant death syndrome (SIDS). How can we use conditional probability to figure out a reasonable assessment of the probability that she murdered her children. First, some known probability assessments</p>
<ol type="1">
<li>The chance of a family of non-smokers having a SIDS death is <span class="math inline">\(1\)</span> in <span class="math inline">\(8,500\)</span>.</li>
<li>The chance of a second SIDS death is <span class="math inline">\(1\)</span> in <span class="math inline">\(100\)</span>.</li>
<li>The chance of a mother killing her two children is around <span class="math inline">\(1\)</span> in <span class="math inline">\(1,000,000\)</span>.</li>
</ol>
<p>Under Bayes <span class="math display">\[\begin{align*}
\prob{\mathrm{both} \; \; \mathrm{SIDS}}   &amp;  = \prob{\mathrm{first} \; \mathrm{SIDS}} \prob{\mathrm{Second} \; \;\mathrm{SIDS} \mid \mathrm{first} \; \mathrm{SIDS}}\\
&amp;  = \frac{1}{8500} \cdot \frac{1}{100} = \frac{1}{850,000}.
\end{align*}\]</span></p>
<p>The <span class="math inline">\(1/100\)</span> comes from taking into account the genetics properties of SIDS. Independence, as implemented by the court, gets you to a probabilistic assessment of <span class="math display">\[
P \left(  \mathrm{both} \; \; \mathrm{SIDS} \right)  = (1/8500) (1/8500) = (1/73,000,000).
\]</span> This is a low probability. It is still not the answer to our question of context. We need a conditional probability, this will come to the Bayes rule.</p>
<p>First, some general comment on the likelihood ratio calculation used to assess the weight of evidence in favor of guilty v.s. innocent evidence. Under Bayes we’ll find that there’s reasonable evidence that she’d be acquitted. We need the relative odds ratio. Let <span class="math inline">\(I\)</span> denote the event that Sally Clark is innocent and <span class="math inline">\(G\)</span> denotes guilty. Let <span class="math inline">\(E\)</span> denote the evidence. In most cases, <span class="math inline">\(E\)</span> contains a sequence <span class="math inline">\(E_1, E_2, \ldots\)</span> of ‘facts’ and we have to use the likelihood ratios in turn. Bayes rule then tells you to combine via multiplicative fashion. If likelihood ratio <span class="math inline">\(&gt;1\)</span>, odds of guilty. If likelihood ratio <span class="math inline">\(&lt;1\)</span>, more likelihood to be <span class="math inline">\(I\)</span>. By Bayes rule <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{p( E\text{ and } I)}{p( G, I)}.
\]</span> If we further decompose <span class="math inline">\(p(E \text{ and } I) = p(E\mid I )p(I)\)</span> then we have to discuss the prior probability of innocence, namely <span class="math inline">\(p(I)\)</span>. Hence this is one subtle advantage of the above decomposition.</p>
<p>The underlying intuition that Bayes gives us in this example, is that one the two possible explanations of the data, both of which are unlikely, it is the relative likelihood of comparison that should matter. Here is a case where the <span class="math inline">\(p\)</span>-value would be non-sensible (<span class="math inline">\(p(E\mid I) \neq p(I\mid E)\)</span>). Effectively comparing two rare event probabilities from the two possible models or explanations.</p>
<p>Hence putting these two together gives the odds of guilt as <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{1/850,000}{1/1,000,000} = 1.15.
\]</span> Solving for the posterior probability yields <span class="math inline">\(46.5\%\)</span> for probability of guilty given evidence. <span class="math display">\[
p( G\mid E) = \frac{1}{1 + O(G\mid E)} = 0.465.
\]</span> Basically a <span class="math inline">\(50/50\)</span> bet. Not enough to definitively convict! But remember that our initial prior probability on guilt <span class="math inline">\(p(G)\)</span> was <span class="math inline">\(10^{-6}\)</span>. So now there has been a dramatic increase to a posterior probability of <span class="math inline">\(0.465\)</span>. So it’s not as if Bayes rule thinks this is evidence in the suspects favor – but the magnitude is still not in the <span class="math inline">\(0.999\)</span> range though, where most jurors would have to be to feel comfortable with a guilt verdict.</p>
<p>If you use the “wrong” model of independence (as the court did) you get <span class="math display">\[
P \left(  \mathrm{both} \; \; \mathrm{SIDS} \right)  = \frac{1}{8500}
  \cdot\frac{1}{8500} = \frac{1}{73,000,000}.
\]</span> With the independence assumption, you make the assessment <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{1}{73} \; \mathrm{ and} \; p( G\mid E) \approx 0.99.
\]</span> Given these probability assumptions, the suspect looks guilty with probability 99%.</p>
<p>Experts also mis-interpret the evidence by saying: <em>1 in 73 million chance that it is someone else</em>. This is clearly false and misleading to the jury and has leads to appeals.</p>
</div>
<div id="exm-Simpson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15 (O. J. Simpson Case: Dershowitz Fallacy)</strong></span> &nbsp;</p>
<!-- https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00839.x -->
<p>This example is based on I. J. Good’s, “When batterer turns murderer.” Nature, 15 June 1995, p.&nbsp;541. Alan Dershowitz, on the O. J. Simpson defense team, stated on T.V. and in newspapers that only 1 in 2,500 of men who abuse their wives go on to murder them. He clearly wanted his audience to interpret this to mean that the evidence of abuse by Simpson would only suggest a 0.04 of his being guilty of murdering her. He used probability to argue that because so few husbands who batter their wifes actually go on to murder their wives. Thus, O.J. is highly likely to be not guilty. This leaves out the most relevant conditioning information that we also know that Nicole Brown Simpson was actually murdered. Both authors believe the jury would be more interested in the probability that the husband is guilty of the murder of his wife given that he abused his wife and his wife was murdered. They both solve this problem by using Bayes’ theorem.</p>
<p>In this example, the notation <span class="math inline">\(B\)</span> represents “woman battered by her husband, boyfriend, or lover”, <span class="math inline">\(M\)</span> represents the event “woman murdered”, and <span class="math inline">\(G\)</span> denotes “woman murdered by her batterer”. Our goal is to show that <span class="math display">\[
% P(M,B \mid M) \neq P(M,B \mid B).
P(G \mid M,B) \neq P(G\mid B).
\]</span></p>
<p>It is not hard to come to a wrong conclusion if you don’t take into account all the relevant conditional information. He intended this information to exonerate O.J. In 1992 the women population of the US was 125 million and 4936 women were murdered, thus <span class="math display">\[
P(M) = 4936/125,000,000 = 0.00004 = 1/25,000.
\]</span> At the same year about 3.5 million women were battered <span class="math display">\[
P(B) = 3.5/125 = 0.028.
\]</span> That same year 1432 women were murdered by their previous batterers, so the marginal probability of that event is <span class="math inline">\(P(G) = 1432/125,000,000 = 0.00001 = 1/87,290\)</span>, and the conditional probability, <span class="math inline">\(P(G | B)\)</span> is 1432 divided by 3.5 million, or <span class="math inline">\(1/2444\)</span>. These are the numbers Dershowitz used to obtain his estimate that about 1 in 2500 battered women go on to be murdered by their batterers.</p>
<p>We need to calculate <span class="math display">\[
P(G \mid M,B) = P(M | G,B) P(G) / P(M).
\]</span> We know <span class="math inline">\(P(M | G,B) = 1\)</span> and <span class="math inline">\(P(G) / P(M) = 0.00001/0.00004 = 0.29\)</span>, or about 1 in 3.5.</p>
<p>Alan Dershowitz provided the jury with an accurate but irrelevant probability. The fact the women was murdered increases the probability that she was murdered by her batterer by a factor of 709 (0.29/(1/2444)). <span class="math display">\[
P(G\mid M,B)\approx 709\times P(G\mid B,M).
\]</span></p>
<p>The argument used by Dershowitz relating to the Simpson case has been discussed by John Paulos in an op-ed article in the Philadelphia Inquirer (15 Oct.&nbsp;1995, C7) and his book “Once Upon a Number”, by I.J. Good in an article in Nature (June 15,1995, p 541) and by Jon Merz and Jonathan Caulkins in an article in Chance Magazine, (Spring 1995, p 14).</p>
</div>
<p>Probability measures the uncertainty of an event. But how do we measure probability? One school of thought, takes probability as subjective, namely personal to the observer. de Finetti famously concluded that “Probability does not exist.” Measuring that is personal to the observer. It’s not like mass which is a property of an object. If two different observers have differing “news” then there is an them to bet (exchange contracts). Thus leading to a assessment of probability. Ramsey (1926) takes this view.</p>
<p>Much of data science is then the art of building probability models to study phenomenon. For many events most people will agree on their probabilities, for example <span class="math inline">\(p(H) = 0.5\)</span> and <span class="math inline">\(p(T) = 0.5\)</span>. In the subjective view of probability we can measure or elicit a personal probability as a “willingness to play”. Namely, will you be willing to bet $1 so you can get $2 if head lands Tail and $0 if Head occurs? For more details, see <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a>.</p>
</section>
<section id="sec-Sensitivity" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="sec-Sensitivity"><span class="header-section-number">2.5</span> Sensitivity and Specificity</h2>
<p>Conditional probabilities are used to define two fundamental metrics used for many probabilistic and statistical learning models, namely <em>sensitivity</em> and <em>specificity</em>.</p>
<p>Sensitivity and specificity are two key metrics used to evaluate the performance of diagnostic tests, classification models, or screening tools. These metrics help assess how well a test can correctly identify individuals with a condition (true positives) and those without the condition (true negatives). Let’s break down each term:</p>
<ol type="1">
<li><strong>Sensitivity (True Positive Rate or Recall):</strong> is the ability of a test to correctly identify individuals who have a particular condition or disease <span class="math inline">\(P ( T=1 \mid D=1 )\)</span>. It is calculated as the ratio of true positives to the sum of true positives and false negatives. <span class="math display">\[
p(T=1\mid D=1) = \dfrac{p(T=1,D=1)}{p(D=1)}.
\]</span> A high sensitivity indicates that the test is good at identifying individuals with the condition, minimizing false negatives.</li>
<li><strong>Specificity (True Negative Rate):</strong> is the ability of a test to correctly identify individuals who do not have a particular condition or disease <span class="math inline">\(P (T=0 \mid D=0 )\)</span>. It is calculated as the ratio of true negatives to the sum of true negatives and false positives. <span class="math display">\[
p(T=0\mid D=0) = \dfrac{p(T=0,D=0)}{p(D=0)}
\]</span> A high specificity indicates that the test is good at correctly excluding individuals without the condition, minimizing false positives.</li>
</ol>
<p>Sensitivity and specificity are often trade-offs. Increasing sensitivity might decrease specificity, and vice versa. Thus, depending on application, you might prefer sensitivity over specificity or vice versa, depending on the consequences of false positives and false negatives in a particular application.</p>
<p>Consider a medical test designed to detect a certain disease. If the test has high sensitivity, it means that it is good at correctly identifying individuals with the disease. On the other hand, if the test has high specificity, it is good at correctly identifying individuals without the disease. The goal is often to strike a balance between sensitivity and specificity based on the specific needs and implications of the test results.</p>
<!-- -   **Sensitivity** measures the true positive rate, or recall, for
  example, the percentage of sick people who are correctly identified
  $P ( T=1 \mid  D=1 )$.
-   **Specificity** measures true negative rate, e.g % of negatives
  corrected identified as such. $P (T=0  \mid  D=0 )$. -->
<p>Sensitivity is often called the power of a procedure (a.k.a. test). There are two kinds of errors (type I and type II) as well as sensitivity and specificity are the dual concepts.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Type I error (false positive rate)
</div>
</div>
<div class="callout-body-container callout-body">
<p>is the % of healthy people who tested positive, <span class="math inline">\(p(T=1\mid D=0)\)</span>, it is the mistake of thinking something is true when it is not.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Type II error (or false negative rate)
</div>
</div>
<div class="callout-body-container callout-body">
<p>is the % sick people who are tested negative, <span class="math inline">\(p(T=0\mid D=1)\)</span>, it the mistake of thinking something is not true when in fact it is true.</p>
</div>
</div>
<p>We would like to control both conditional probabilities with our test. Also if someone tests positive, how likely is it that they actually have the disease. There are two ‘errors’ one can make. Falsely diagnosing someone, or not correctly finding the disease.</p>
<p>In the stock market, one can think of type I error as not not selling a loosing stock quickly enough, and a type II error as failing to buy a growing stock, e.g.&nbsp;Amazon or Google.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 26%">
<col style="width: 31%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p(T=1\mid D=1)\)</span></td>
<td style="text-align: center;">Sensitivity</td>
<td style="text-align: center;">True Positive Rate</td>
<td style="text-align: center;"><span class="math inline">\(1-\beta\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(p(T=0\mid D=0 )\)</span></td>
<td style="text-align: center;">Specificity</td>
<td style="text-align: center;">True Negative Rate</td>
<td style="text-align: center;"><span class="math inline">\(1-\alpha\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p(T=1\mid D=0)\)</span></td>
<td style="text-align: center;">1-Specificity</td>
<td style="text-align: center;">False Positive Rate</td>
<td style="text-align: center;"><span class="math inline">\(\alpha\)</span> (type I error)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(p(T=0\mid D =1)\)</span></td>
<td style="text-align: center;">1-Sensitivity</td>
<td style="text-align: center;">False Negative Rate</td>
<td style="text-align: center;"><span class="math inline">\(\beta\)</span> (type II error)</td>
</tr>
</tbody>
</table>
<p>Often it is convenient to write those four values in a form of a two-by-to matrix, called the confusion matrix:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Actual/Predicted</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Negative</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>where: TP: True Positive. FN: False Negative, FP: False Positive, TN: True Negative</p>
<p>We will extensively use the concepts of errors, specificity and sensitivity later in the book, when describing AB testing and predictive models. These example illustrates why people can commonly miscalculate and mis-interpret probabilities. Those quantities can be calculated using the Bayes rule.</p>
<div id="exm-Apple" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.16 (Apple Watch Series 4 ECG and Bayes’ Theorem)</strong></span> The Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 27%">
<col style="width: 31%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Predicted</th>
<th style="text-align: center;">atrial fibrillation</th>
<th style="text-align: center;">no atrial fibrillation</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">atrial fibrillation</td>
<td style="text-align: center;">1960</td>
<td style="text-align: center;">980</td>
<td style="text-align: center;">2940</td>
</tr>
<tr class="even">
<td style="text-align: left;">no atrial fibrillation</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">97020</td>
<td style="text-align: center;">97060</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">98000</td>
<td style="text-align: center;">100000</td>
</tr>
</tbody>
</table>
<p>However, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4? We use Bayes theorem to answer this question. <span class="math display">\[
p(\text{atrial fibrillation}\mid \text{atrial fibrillation is identified }) = \frac{0.01960}{ 0.02940} = 0.6667
\]</span></p>
<p>The conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%.</p>
<p>In people younger than 55, Apple Watch’s positive predictive value is just 19.6 percent. That means in this group – which constitutes more than 90 percent of users of wearable devices like the Apple Watch – the app incorrectly diagnoses atrial fibrillation 79.4 percent of the time. (You can try the calculation yourself using this Bayesian calculator: enter 0.001 for prevalence, 0.98 for sensitivity, and 0.996 for specificity).</p>
<p>The electrocardiogram app becomes more reliable in older individuals: The positive predictive value is 76 percent among users between the ages of 60 and 64, 91 percent among those aged 70 to 74, and 96 percent for those older than 85.</p>
<p>In the case of medical diagnostics, the sensitivity is the ratio of people who have disease and tested positive to the total number of positive cases in the population <span class="math display">\[
p(T=1\mid D=1) = \dfrac{p(T=1,D=1)}{p(D=1)} = 0.019/0.002 = 0.95
\]</span> The specificity is given by <span class="math display">\[
p(T=0\mid D=0) = \dfrac{p(T=0,D=0)}{p(D=0)} = 0.9702/0.98 = 0.99.
\]</span> As we see the test is highly sensitive and specific. However, only 66% of those who are tested positive will have a disease. This is due to the fact that number of sick people is much less then the number of healthy and presence of type I error.</p>
</div>
</section>
<section id="graphical-representation-of-probability-and-conditional-independence." class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="graphical-representation-of-probability-and-conditional-independence."><span class="header-section-number">2.6</span> Graphical Representation of Probability and Conditional Independence.</h2>
<!-- https://cedar.buffalo.edu/~srihari/CSE674/Chap3/3.6-ConditionalIndependence.pdf -->
<p>We can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by <span class="math display">\[
p(x_1, x_2, \ldots, x_n) = p(x_1)p(x_2 \mid x_1)p(x_3 \mid x_1, x_2) \ldots p(x_n \mid x_1, x_2, \ldots, x_{n-1}).
\]</span> The expression on the right hand side can be simplified if some of the variables are conditionally independent. For example, if <span class="math inline">\(x_3\)</span> is conditionally independent of <span class="math inline">\(x_2\)</span>, given <span class="math inline">\(x_1\)</span>, then we can write <span class="math display">\[
p(x_3 \mid x_1, x_2) =p(x_3 \mid x_1).
\]</span></p>
<p>In a high-dimensional case, when we have a joint distribution over a large number of random variables, we can often simplify the expression by using independence or conditional independence assumptions. Sometimes it is convenient to represent these assumptions in a graphical form. This is the idea behind the concept of a Bayesian network. Essentially, graph is a compact representation of a set of independencies that hold in the distribution.</p>
<p>Let’s consider an example of joint distribution with three random variables, we have the following joint distribution: <span class="math display">\[
p(a,b,c) = p(a\mid b,c)p(b\mid c)p(c)
\]</span></p>
<p>When two nodes are connected they are not independent. Consider the following three cases:</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/bayes-net-Page-3.drawio.svg" class="img-fluid figure-img"></p>
<figcaption>Line Structure</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
p(b\mid c,a) = p(b\mid c),~ p(a,b,c) = p(a)p(c\mid a)p(b\mid c)
\]</span></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/bayes-net-Page-1.drawio.svg" class="img-fluid figure-img"></p>
<figcaption>Lambda Structure</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
p(a\mid b,c) = p(a\mid c), ~ p(a,b,c) = p(a\mid c)p(b\mid c)p(c)
\]</span></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-center">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/bayes-net-Page-5.drawio.svg" class="img-fluid figure-img"></p>
<figcaption>V-structure</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
p(a\mid b) = p(a),~ p(a,b,c) = p(c\mid a,b)p(a)p(b)
\]</span></p>
</div>
</div>
</div>
<p>Although the graph shows us the conditional independence assumptions, we can also derive other independencies from the graph An interesting question if they are connected through a third node. In the first case (a), we have <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> connected through <span class="math inline">\(c\)</span>. Thus, <span class="math inline">\(a\)</span> can influence <span class="math inline">\(b\)</span>. However, once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent. In case (b) the logic here is similar, <span class="math inline">\(a\)</span> can influence <span class="math inline">\(b\)</span> through <span class="math inline">\(c\)</span>, but once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent. In the third case (c), <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent, but once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are not independent. You can formally derive these independencies from the graph by comparing <span class="math inline">\(p(a,b\mid c)\)</span> and <span class="math inline">\(p(a\mid c)p(b\mid c)\)</span>.</p>
<div id="exm-alarm" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.17 (Bayes Home Diagnostics)</strong></span> Suppose that a house alarm system sends me a text notification when some motion inside my house is detected. It detects motion when I have a person inside (burglar) or during an earthquake. Say, from prior data we know that during an earthquake alarm is triggered in 10% of the cases. Once I receive a text message, I start driving back home. While driving I hear on the radio about a small earthquake in our area. Now we want to know <span class="math inline">\(p(b \mid a)\)</span> and <span class="math inline">\(p(b \mid a,r)\)</span>. Here <span class="math inline">\(b\)</span> = burglary, <span class="math inline">\(e\)</span> = earthquake, <span class="math inline">\(a\)</span> = alarm, and <span class="math inline">\(r\)</span> = radio message about small earthquake.</p>
<p>The joint distribution is then given by <span class="math display">\[
  p(b,e,a,r) = p(r \mid a,b,e)p(a \mid b,e)p(b\mid e)p(e).
\]</span> Since we know the causal relations, we can simplify this expression <span class="math display">\[
p(b,e,a,r) = p(r \mid e)p(a \mid b,e)p(b)p(e).
\]</span> The distribution is defined by</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(p(a=1 \mid b,e)\)</span></th>
<th style="text-align: center;">b</th>
<th style="text-align: center;">e</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Graphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as Bayesian network.</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDIyOX0.jgsw8lFITwTKK38jbomGp6OEafDwLPCkz2INBBA4xI4 -->
<div id="fig-bayes-net" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayes-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bayes-net.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayes-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Bayesian network for alarm.
</figcaption>
</figure>
</div>
<p>Now we can easily calculate <span class="math inline">\(p(a=0 \mid b,e)\)</span>, from the property of a probability distribution <span class="math inline">\(p(a=1 \mid b,e) + p(a=0 \mid b,e) = 1\)</span>. In addition, we are given <span class="math inline">\(p(r=1 \mid e=1) = 0.5\)</span> and <span class="math inline">\(p(r=1 \mid e=0) = 0\)</span>. Further, based on historic data we have <span class="math inline">\(p(b) = 2\cdot10^{-4}\)</span> and <span class="math inline">\(p(e) = 10^{-2}\)</span>. Note that causal relations allowed us to have a more compact representation of the joint probability distribution. The original naive representations requires specifying <span class="math inline">\(2^4\)</span> parameters.</p>
<p>To answer our original question, calculate <span class="math display">\[
p(b \mid a) = \dfrac{p(a \mid b)p(b)}{p(a)},~~p(b) = p(a=1 \mid b=1)p(b=1) + p(a=1 \mid b=0)p(b=0).
\]</span> We have everything but <span class="math inline">\(p(a \mid b)\)</span>. This is obtained by marginalizing <span class="math inline">\(p(a=1 \mid b,e)\)</span>, to yield <span class="math display">\[
p(a \mid b) = p(a \mid b,e=1)p(e=1) + p(a \mid b,e=0)p(e=0).
\]</span> We can calculate <span class="math display">\[
p(a=1 \mid b=1) = 1, ~p(a=1 \mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.
\]</span> This leads to <span class="math inline">\(p(b \mid a) = 2\cdot10^{-4}/(2\cdot10^{-4} + 10^{-3}(1-2\cdot10^{-4})) = 1/6\)</span>.</p>
<p>This result is somewhat counterintuitive. We get such a low probability of burglary because its prior is very low compared to prior probability of an earthquake. What will happen to posterior if we live in an area with higher crime rates, say <span class="math inline">\(p(b) = 10^{-3}\)</span>. <a href="#fig-alarm" class="quarto-xref">Figure&nbsp;<span>2.3</span></a> shows the relationship between the prior and posterior. <span class="math display">\[
p(a \mid b) = \dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}
\]</span></p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, .<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> prior <span class="sc">/</span> (prior <span class="sc">+</span> <span class="fl">0.001</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> prior))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(prior, post, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-alarm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-bayes_files/figure-html/fig-alarm-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Relationship between the prior and posterior
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, <span class="math display">\[
p(b=1 \mid a=1,r=1) =  \dfrac{p(a,r  \mid  b)p(b)}{p(a,r)}
\]</span> and <span class="math display">\[
p(a,r  \mid  b)p(b) = \dfrac{\sum_e p(b=1,e,a=1,r=1)}{\sum_b\sum_ep(b,e,a=1,r=1)}
\]</span> <span class="math display">\[
=\dfrac{\sum_ep(r=1 \mid e)p(a=1 \mid b=1,e)p(b=1)p(e)}{\sum_b\sum_ep(r=1 \mid e)p(a=1 \mid b,e)p(b)p(e)}
\]</span> which is <span class="math inline">\(\approx 2\%\)</span> in our case. This effect is called <em>explaining away</em>, namely when new information explains some previously known fact.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-maharaj2023kramnik" class="csl-entry" role="listitem">
Maharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. <span>“Kramnik Vs <span>Nakamura</span> or <span>Bayes</span> Vs p-Value.”</span> {{SSRN Scholarly Paper}}. Rochester, NY.
</div>
<div id="ref-simpson2010edward" class="csl-entry" role="listitem">
Simpson, Edward. 2010. <span>“Edward <span>Simpson</span>: <span>Bayes</span> at <span>Bletchley Park</span>.”</span> <em>Significance</em> 7 (2): 76–80.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-prob.html" class="pagination-link" aria-label="Probability and Uncertainty">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-bl.html" class="pagination-link" aria-label="Bayesian Learning">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>