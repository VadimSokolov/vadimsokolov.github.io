<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Large Language Models: A Revolution in AI – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./24-nlp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar docked quarto-light"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[2][]{\operatorname{E}_{#1}\left[#2\right]}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\DeclareMathOperator*{\minf}{minimize \quad}
\newcommand{\mininlineeq}[4]{\begin{equation}\label{#4}\mbox{minimize}_{#1}\quad#2\qquad\mbox{subject to }#3\end{equation}}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./24-nlp.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./25-llm.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#adding-one-word-at-a-time" id="toc-adding-one-word-at-a-time" class="nav-link active" data-scroll-target="#adding-one-word-at-a-time"><span class="header-section-number">2.1</span> Adding One Word at a Time</a></li>
  <li><a href="#the-simplest-form-of-text-generation-one-letter-at-a-time" id="toc-the-simplest-form-of-text-generation-one-letter-at-a-time" class="nav-link" data-scroll-target="#the-simplest-form-of-text-generation-one-letter-at-a-time"><span class="header-section-number">2.2</span> The simplest form of text generation: One Letter at a Time</a></li>
  <li><a href="#the-secret-sauce-from-attention-to-transformers" id="toc-the-secret-sauce-from-attention-to-transformers" class="nav-link" data-scroll-target="#the-secret-sauce-from-attention-to-transformers"><span class="header-section-number">2.3</span> The Secret Sauce: From Attention to Transformers</a></li>
  <li><a href="#from-words-to-tokens-the-hidden-language-of-llms" id="toc-from-words-to-tokens-the-hidden-language-of-llms" class="nav-link" data-scroll-target="#from-words-to-tokens-the-hidden-language-of-llms"><span class="header-section-number">2.4</span> From Words to Tokens: The Hidden Language of LLMs</a></li>
  <li><a href="#the-architecture-that-changed-everything" id="toc-the-architecture-that-changed-everything" class="nav-link" data-scroll-target="#the-architecture-that-changed-everything"><span class="header-section-number">2.5</span> The Architecture That Changed Everything</a></li>
  <li><a href="#the-scale-revolution-how-bigger-became-better" id="toc-the-scale-revolution-how-bigger-became-better" class="nav-link" data-scroll-target="#the-scale-revolution-how-bigger-became-better"><span class="header-section-number">2.6</span> The Scale Revolution: How Bigger Became Better</a></li>
  <li><a href="#understanding-the-generation-process" id="toc-understanding-the-generation-process" class="nav-link" data-scroll-target="#understanding-the-generation-process"><span class="header-section-number">2.7</span> Understanding the Generation Process</a></li>
  <li><a href="#the-challenge-of-creativity-and-diversity" id="toc-the-challenge-of-creativity-and-diversity" class="nav-link" data-scroll-target="#the-challenge-of-creativity-and-diversity"><span class="header-section-number">2.8</span> The Challenge of Creativity and Diversity</a></li>
  <li><a href="#choosing-the-right-model-for-your-application" id="toc-choosing-the-right-model-for-your-application" class="nav-link" data-scroll-target="#choosing-the-right-model-for-your-application"><span class="header-section-number">2.9</span> Choosing the Right Model for Your Application</a></li>
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance"><span class="header-section-number">2.10</span> Evaluating Model Performance</a></li>
  <li><a href="#when-things-go-wrong-understanding-llm-limitations" id="toc-when-things-go-wrong-understanding-llm-limitations" class="nav-link" data-scroll-target="#when-things-go-wrong-understanding-llm-limitations"><span class="header-section-number">2.11</span> When Things Go Wrong: Understanding LLM Limitations</a></li>
  <li><a href="#building-practical-applications" id="toc-building-practical-applications" class="nav-link" data-scroll-target="#building-practical-applications"><span class="header-section-number">2.12</span> Building Practical Applications</a></li>
  <li><a href="#creative-collaboration-when-artists-meet-algorithms" id="toc-creative-collaboration-when-artists-meet-algorithms" class="nav-link" data-scroll-target="#creative-collaboration-when-artists-meet-algorithms"><span class="header-section-number">2.13</span> Creative Collaboration: When Artists Meet Algorithms</a></li>
  <li><a href="#looking-forward-the-evolving-landscape" id="toc-looking-forward-the-evolving-landscape" class="nav-link" data-scroll-target="#looking-forward-the-evolving-landscape"><span class="header-section-number">2.14</span> Looking Forward: The Evolving Landscape</a></li>
  <li><a href="#the-future-of-human-ai-partnership" id="toc-the-future-of-human-ai-partnership" class="nav-link" data-scroll-target="#the-future-of-human-ai-partnership"><span class="header-section-number">2.15</span> The Future of Human-AI Partnership</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./24-nlp.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./25-llm.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Large Language Models (LLMs) have emerged as one of the most profound breakthroughs in artificial intelligence, fundamentally reshaping our relationship with technology. These models can write poetry that moves us to tears, generate computer code that solves complex problems, translate languages with nuanced understanding, and hold conversations with a fluency that often feels remarkably human. But how do they work? What is the magic behind the curtain that makes a computer suddenly seem to understand the subtleties of human language and thought?</p>
<p>At their core, LLMs are powered by the Transformer architecture, that hinges on a concept called attention—the ability to weigh the importance of different words in a sentence to grasp context and meaning. Imagine if you could instantly understand not just what someone is saying, but also catch every subtle reference, every implied connection, every hidden meaning between the lines. This is what attention mechanisms give to artificial intelligence. This chapter will journey from the foundational ideas of attention to the colossal models that are defining our modern world, exploring not just how they work, but how they think.</p>
<section id="adding-one-word-at-a-time" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="adding-one-word-at-a-time"><span class="header-section-number">2.1</span> Adding One Word at a Time</h2>
<p>The first application of LLMs that most people encounter is text generation. You provide a prompt, and the model generates a continuation that often feels remarkably coherent and relevant. This ability to produce text that mimics human writing is one of the most striking features of LLMs. But how does it achieve this? And why does it work so well?</p>
<p>At its core, an LLM is designed to predict the next token in a sequence based on the context provided by the preceding tokens. This process involves generating a “reasonable continuation” of the input text, where “reasonable” means consistent with patterns observed in vast amounts of human-written text, such as books, articles, and websites. For example, given the prompt “The best thing about AI is its ability to,” the model analyzes patterns from its training data to predict likely continuations. It doesn’t simply match literal text; instead, it evaluates semantic and contextual similarities to produce a ranked list of possible next tokens along with their probabilities.</p>
<p>This mechanism allows LLMs to generate text that aligns with human expectations, leveraging their ability to understand context and meaning at a deep level. By iteratively predicting and appending tokens, the model constructs coherent and meaningful responses that often feel indistinguishable from human writing. Let’s see how this works in practice with a simple example using the SmolLM2 model. We’ll start by loading the model and tokenizer, which are essential components for generating text. The tokenizer converts text into tokens that the model can understand, while the model itself generates predictions based on those tokens.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'./code'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Import our custom functions</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llm_chapter <span class="im">import</span> (ask_smol_lm, get_next_word_suggestions, generate_text_step_by_step)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>local_cache_dir <span class="op">=</span> <span class="st">"./models_cache"</span>  <span class="co"># or use absolute path like "/Users/your_username/ai_models"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create directory if it doesn't exist</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Path(local_cache_dir).mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model with custom cache directory</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This will download once and store in your specified directory</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_id, cache_dir<span class="op">=</span>local_cache_dir)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_id,cache_dir<span class="op">=</span>local_cache_dir,device_map<span class="op">=</span><span class="st">"auto"</span>,torch_dtype<span class="op">=</span>torch.float16,low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"✅ Model loaded from cache directory!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## ✅ Model loaded from cache directory!</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> model.device</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Using device:"</span>, device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Using device: mps:0</code></pre>
</div>
</div>
<p>Consider the text “The best thing about AI is its ability to”. Imagine analyzing billions of pages of human-written text—such as those found on the web or in digitized books—and identifying all instances of this text to determine what word most commonly comes next. While LLM doesn’t directly search for literal matches, it evaluates semantic and contextual similarities to produce a ranked list of possible next words along with their associated probabilities. This process enables it to generate coherent and contextually appropriate continuations.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get next word suggestions for a given text</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>initial_text <span class="op">=</span> <span class="st">"The best thing about AI is its ability to"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>suggestions <span class="op">=</span> get_next_word_suggestions(initial_text, model, tokenizer, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Next word suggestions for '</span><span class="sc">{</span>initial_text<span class="sc">}</span><span class="ss">':"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Next word suggestions for 'The best thing about AI is its ability to':</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (word, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(suggestions):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">' (prob: </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   1. ' learn' (prob: 0.623)
##   2. ' help' (prob: 0.119)
##   3. ' augment' (prob: 0.100)
##   4. ' analyze' (prob: 0.084)
##   5. ' process' (prob: 0.074)</code></pre>
</div>
</div>
<p>When an LLM generates text, it essentially operates by repeatedly asking, “Given the text so far, what should the next word be?”—and then appending a word to the output. More precisely, it adds a “token,” which could represent a full word or just a part of one, allowing it to occasionally create novel words.</p>
<p>At each step, the model produces a ranked list of possible tokens along with their probabilities. One might assume the model should always select the token with the highest probability. However, if this approach is followed strictly, the generated text often lacks creativity and can become repetitive. To address this, randomness is introduced into the selection process. By occasionally choosing lower-ranked tokens, the model can produce more varied and engaging text.</p>
<p>This randomness means that using the same prompt multiple times will likely yield different outputs. A parameter called “temperature” controls the degree of randomness in token selection. For text generation tasks, a temperature value of around 0.8 is often found to strike a good balance between coherence and creativity. It’s worth noting that this parameter is based on empirical findings rather than theoretical principles. The term “temperature” originates from statistical physics due to the use of exponential distributions, but its application here is purely mathematical.</p>
<p>Below is an illustration of the iterative process where the model selects the word with the highest probability at each step (referred to in the code as the model’s “decision”):</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's start with a simple prompt</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>initial_text <span class="op">=</span> <span class="st">"The best thing about AI is its ability to"</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Initial text: '</span><span class="sc">{</span>initial_text<span class="sc">}</span><span class="ss">'"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Initial text: 'The best thing about AI is its ability to'</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text step by step</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer, num_steps<span class="op">=</span><span class="dv">10</span>, temperature<span class="op">=</span><span class="fl">1.0</span>, sample<span class="op">=</span><span class="va">False</span>, print_progress<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Starting text: 'The best thing about AI is its ability to'
## ============================================================
## 'The best thing about AI is its ability to learn' (prob: 0.280)
## 'The best thing about AI is its ability to learn and' (prob: 0.697)
## 'The best thing about AI is its ability to learn and adapt' (prob: 0.481)
## 'The best thing about AI is its ability to learn and adapt.' (prob: 0.321)
## 'The best thing about AI is its ability to learn and adapt. It' (prob: 0.260)
## 'The best thing about AI is its ability to learn and adapt. It can' (prob: 0.529)
## 'The best thing about AI is its ability to learn and adapt. It can analyze' (prob: 0.268)
## 'The best thing about AI is its ability to learn and adapt. It can analyze vast' (prob: 0.619)
## 'The best thing about AI is its ability to learn and adapt. It can analyze vast amounts' (prob: 0.999)
## 'The best thing about AI is its ability to learn and adapt. It can analyze vast amounts of' (prob: 1.000)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Generated text:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">80</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## The best thing about AI is its ability to learn and adapt. It can analyze vast
## amounts of</code></pre>
</div>
</div>
<p>In this example, we always select the most probable next token, which leads to a coherent but somewhat predictable continuation. The model generates text by repeatedly applying this process, building on the context provided by the previous tokens. In fact, we’ve seen that the model actually generates multiple suggestions for the next word, which can be useful for understanding how it thinks about language.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>suggestions <span class="op">=</span> get_next_word_suggestions(initial_text, model, tokenizer, top_k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(suggestions)))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [s[<span class="dv">0</span>] <span class="cf">for</span> s <span class="kw">in</span> suggestions]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> [s[<span class="dv">1</span>] <span class="cf">for</span> s <span class="kw">in</span> suggestions]</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the next word suggestions with their log probabilities</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.plot(indices, np.log10(probabilities), marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.xticks(indices, words, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## ([&lt;matplotlib.axis.XTick object at 0x3fc7fb380&gt;, &lt;matplotlib.axis.XTick object at 0x3fc7fb350&gt;, &lt;matplotlib.axis.XTick object at 0x3fbe2c740&gt;, &lt;matplotlib.axis.XTick object at 0x3fc4efe30&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80adb0&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80b770&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80a690&gt;, &lt;matplotlib.axis.XTick object at 0x3fc809f10&gt;, &lt;matplotlib.axis.XTick object at 0x3fc85c530&gt;, &lt;matplotlib.axis.XTick object at 0x3fc85cec0&gt;], [Text(0, 0, ' learn'), Text(1, 0, ' help'), Text(2, 0, ' augment'), Text(3, 0, ' analyze'), Text(4, 0, ' process'), Text(5, 0, ' think'), Text(6, 0, ' enhance'), Text(7, 0, ' automate'), Text(8, 0, ' provide'), Text(9, 0, ' assist')])</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Next Word Suggestions'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Probability'</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Next Word Suggestions with Log Probabilities'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="25-llm_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The plot above shows the next word suggestions generated by the model, with their probabilities represented on a logarithmic scale. This visualization helps us understand how the model ranks different words based on their likelihood of being the next token in the sequence. We can see that the probabilities of each next word decay exponentially (outside of the top word ‘learn’). This is the law known as Zipf’s law, which was observed by natural language researchers in the 1930s. It states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, a few words are used very frequently, while most words are used rarely.</p>
<p>Now we will run our LLM generation process for longer and will sample words with probabilities that are calculated based on the temperature parameter. We will use a temperature of 0.8, which is often a good choice for generating coherent text without being too repetitive.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix the seed for reproducibility</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">8</span>)<span class="op">;</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer, num_steps<span class="op">=</span><span class="dv">60</span>, temperature<span class="op">=</span><span class="fl">0.8</span>, sample<span class="op">=</span><span class="va">True</span>,print_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Generated text:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">80</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## The best thing about AI is its ability to learn and improve over time. By
## continuously gathering data and analyzing its performance, AI systems can refine
## their decision-making processes to become more effective and efficient. This
## means that as AI systems encounter new data, they can adapt and learn from their
## experiences, leading to better outcomes."    4</code></pre>
</div>
</div>
<p>The generated text demonstrates the model’s ability to create coherent and contextually relevant sentences, even when sampling from a distribution of possible next words. Now, compare this with the output generated using a higher temperature setting.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">8</span>)<span class="op">;</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer, num_steps<span class="op">=</span><span class="dv">60</span>, temperature<span class="op">=</span><span class="fl">1.2</span>, sample<span class="op">=</span><span class="va">True</span>,print_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Generated text:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">80</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## The best thing about AI is its ability to learn and improve over time. One area
## that holds immense potential is in natural language processing," wrote an AI on
## Twitter after retweeting itself 100 times. It isn't cute, but it highlights how
## much stock we should place in machines' ability to develop basic AI. It is what</code></pre>
</div>
</div>
<p>We can see that setting temperature to 1.2 introduces more randomness. In fact, the generation process went “off track” rather quickly, generating meaningless phrases that don’t follow the initial context. This illustrates how temperature affects the model’s creativity and coherence. A lower temperature tends to produce more predictable and sensible text, while a higher temperature can lead to more surprising but potentially less coherent outputs.</p>
</section>
<section id="the-simplest-form-of-text-generation-one-letter-at-a-time" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-simplest-form-of-text-generation-one-letter-at-a-time"><span class="header-section-number">2.2</span> The simplest form of text generation: One Letter at a Time</h2>
<p>The simples thing we can do with an LLM is to generate text one letter at a time. This is a very basic form of text generation, but it can be useful for understanding how the model works at a fundamental level. Let’s see how we can implement this using the same model and tokenizer we used earlier. We start by counting marginal (unconditional) letter frequencies in the text of a Wikipedia article about cats. This will give us a sense of how often each letter appears in the text, which is a good starting point for understanding how the model generates text.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download wikipedia article on "Cat"</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://en.wikipedia.org/wiki/Cat"</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>cat_text <span class="op">=</span> response.text</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract text from HTML</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bs4 <span class="im">import</span> BeautifulSoup</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>soup <span class="op">=</span> BeautifulSoup(cat_text, <span class="st">'html.parser'</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>cat_text <span class="op">=</span> soup.get_text()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s count letter frequencies in the text</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>letter_counts <span class="op">=</span> Counter(c.lower() <span class="cf">for</span> c <span class="kw">in</span> cat_text <span class="cf">if</span> c.isalpha())</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by frequency</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>sorted_letter_counts <span class="op">=</span> <span class="bu">sorted</span>(letter_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, plot the letter frequencies for the first 26 letters</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>sorted_letter_counts <span class="op">=</span> sorted_letter_counts[:<span class="dv">26</span>]  <span class="co"># Limit to top 26 letters</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>letters, counts <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>sorted_letter_counts)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plt.bar(letters, counts, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Letters'</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Letter Frequencies in Wikipedia Article on Cats'</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [Text(0, 0, 'e'), Text(1, 0, 'a'), Text(2, 0, 'i'), Text(3, 0, 't'), Text(4, 0, 'n'), Text(5, 0, 'o'), Text(6, 0, 's'), Text(7, 0, 'r'), Text(8, 0, 'c'), Text(9, 0, 'l'), Text(10, 0, 'd'), Text(11, 0, 'h'), Text(12, 0, 'm'), Text(13, 0, 'u'), Text(14, 0, 'g'), Text(15, 0, 'p'), Text(16, 0, 'f'), Text(17, 0, 'b'), Text(18, 0, 'y'), Text(19, 0, 'v'), Text(20, 0, 'w'), Text(21, 0, 'k'), Text(22, 0, 'j'), Text(23, 0, 'x'), Text(24, 0, 'z'), Text(25, 0, 'q')])</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="25-llm_files/figure-html/unnamed-chunk-11-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="960"></p>
</figure>
</div>
</div>
</div>
<p>If we try to generate the text one letter at a time</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text one letter at a time by sampling from the letter frequencies</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.array(counts)<span class="op">/</span><span class="bu">sum</span>(counts)  <span class="co"># Normalize counts to probabilities</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>gentext <span class="op">=</span> random.choices(letters, weights<span class="op">=</span>counts, k<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated letters:"</span>, <span class="st">''</span>.join(gentext))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Generated letters: hecatmrtplpdhovcelmi</code></pre>
</div>
</div>
<p>What if we do bi-grams, i.e.&nbsp;pairs of letters? We can do this by counting the frequencies of each pair of letters in the text. This will give us a sense of how often each pair of letters appears in the text, which is a good starting point for understanding how the model generates text.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>bigram_counts <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(cat_text) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cat_text[i].isalpha() <span class="kw">and</span> cat_text[i <span class="op">+</span> <span class="dv">1</span>].isalpha():</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> cat_text[i].lower(), cat_text[i <span class="op">+</span> <span class="dv">1</span>].lower()</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only process standard English letters (a-z)</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'a'</span> <span class="op">&lt;=</span> a <span class="op">&lt;=</span> <span class="st">'z'</span> <span class="kw">and</span> <span class="st">'a'</span> <span class="op">&lt;=</span> b <span class="op">&lt;=</span> <span class="st">'z'</span>:</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>            bigram <span class="op">=</span> (a, b)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>            bigram_counts[bigram] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by frequency</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>sorted_bigram_counts <span class="op">=</span> <span class="bu">sorted</span>(bigram_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the heatmap of bigram frequencies</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>bigram_matrix <span class="op">=</span> np.zeros((<span class="dv">26</span>, <span class="dv">26</span>))</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (a, b), count <span class="kw">in</span> sorted_bigram_counts:</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    bigram_matrix[<span class="bu">ord</span>(a) <span class="op">-</span> <span class="bu">ord</span>(<span class="st">'a'</span>), <span class="bu">ord</span>(b) <span class="op">-</span> <span class="bu">ord</span>(<span class="st">'a'</span>)] <span class="op">=</span> count</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>sns.heatmap(bigram_matrix, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Second Letter'</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'First Letter'</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bigram Frequencies in Wikipedia Article on Cats'</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>plt.xticks(ticks<span class="op">=</span>np.arange(<span class="dv">26</span>) <span class="op">+</span> <span class="fl">0.5</span>, labels<span class="op">=</span>[<span class="bu">chr</span>(i <span class="op">+</span> <span class="bu">ord</span>(<span class="st">'a'</span>)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">26</span>)], rotation<span class="op">=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## ([&lt;matplotlib.axis.XTick object at 0x42609ec00&gt;, &lt;matplotlib.axis.XTick object at 0x42609e8a0&gt;, &lt;matplotlib.axis.XTick object at 0x4260dd2b0&gt;, &lt;matplotlib.axis.XTick object at 0x42609ebd0&gt;, &lt;matplotlib.axis.XTick object at 0x4261074d0&gt;, &lt;matplotlib.axis.XTick object at 0x426107d40&gt;, &lt;matplotlib.axis.XTick object at 0x426106450&gt;, &lt;matplotlib.axis.XTick object at 0x426130890&gt;, &lt;matplotlib.axis.XTick object at 0x4261311f0&gt;, &lt;matplotlib.axis.XTick object at 0x426131b80&gt;, &lt;matplotlib.axis.XTick object at 0x426132450&gt;, &lt;matplotlib.axis.XTick object at 0x426132150&gt;, &lt;matplotlib.axis.XTick object at 0x426132b10&gt;, &lt;matplotlib.axis.XTick object at 0x42614cf80&gt;, &lt;matplotlib.axis.XTick object at 0x426191880&gt;, &lt;matplotlib.axis.XTick object at 0x4260b30e0&gt;, &lt;matplotlib.axis.XTick object at 0x4261a85c0&gt;, &lt;matplotlib.axis.XTick object at 0x4261a9040&gt;, &lt;matplotlib.axis.XTick object at 0x4261061b0&gt;, &lt;matplotlib.axis.XTick object at 0x426193440&gt;, &lt;matplotlib.axis.XTick object at 0x4261a98e0&gt;, &lt;matplotlib.axis.XTick object at 0x4261aa090&gt;, &lt;matplotlib.axis.XTick object at 0x4261aa930&gt;, &lt;matplotlib.axis.XTick object at 0x4261817c0&gt;, &lt;matplotlib.axis.XTick object at 0x4261ab020&gt;, &lt;matplotlib.axis.XTick object at 0x4261ab8c0&gt;], [Text(0.5, 0, 'a'), Text(1.5, 0, 'b'), Text(2.5, 0, 'c'), Text(3.5, 0, 'd'), Text(4.5, 0, 'e'), Text(5.5, 0, 'f'), Text(6.5, 0, 'g'), Text(7.5, 0, 'h'), Text(8.5, 0, 'i'), Text(9.5, 0, 'j'), Text(10.5, 0, 'k'), Text(11.5, 0, 'l'), Text(12.5, 0, 'm'), Text(13.5, 0, 'n'), Text(14.5, 0, 'o'), Text(15.5, 0, 'p'), Text(16.5, 0, 'q'), Text(17.5, 0, 'r'), Text(18.5, 0, 's'), Text(19.5, 0, 't'), Text(20.5, 0, 'u'), Text(21.5, 0, 'v'), Text(22.5, 0, 'w'), Text(23.5, 0, 'x'), Text(24.5, 0, 'y'), Text(25.5, 0, 'z')])</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>plt.yticks(ticks<span class="op">=</span>np.arange(<span class="dv">26</span>) <span class="op">+</span> <span class="fl">0.5</span>, labels<span class="op">=</span>[<span class="bu">chr</span>(i <span class="op">+</span> <span class="bu">ord</span>(<span class="st">'a'</span>)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">26</span>)], rotation<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## ([&lt;matplotlib.axis.YTick object at 0x4260b2210&gt;, &lt;matplotlib.axis.YTick object at 0x4260b0c20&gt;, &lt;matplotlib.axis.YTick object at 0x4260dd490&gt;, &lt;matplotlib.axis.YTick object at 0x426133e30&gt;, &lt;matplotlib.axis.YTick object at 0x42614c860&gt;, &lt;matplotlib.axis.YTick object at 0x4261339b0&gt;, &lt;matplotlib.axis.YTick object at 0x42614cb00&gt;, &lt;matplotlib.axis.YTick object at 0x42614d640&gt;, &lt;matplotlib.axis.YTick object at 0x42614e2a0&gt;, &lt;matplotlib.axis.YTick object at 0x42614ea20&gt;, &lt;matplotlib.axis.YTick object at 0x42614f5f0&gt;, &lt;matplotlib.axis.YTick object at 0x42614e5a0&gt;, &lt;matplotlib.axis.YTick object at 0x42614fd70&gt;, &lt;matplotlib.axis.YTick object at 0x426106780&gt;, &lt;matplotlib.axis.YTick object at 0x4261d16a0&gt;, &lt;matplotlib.axis.YTick object at 0x4261a9ac0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d0980&gt;, &lt;matplotlib.axis.YTick object at 0x4261d1d90&gt;, &lt;matplotlib.axis.YTick object at 0x4261d2720&gt;, &lt;matplotlib.axis.YTick object at 0x4261d30b0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d2900&gt;, &lt;matplotlib.axis.YTick object at 0x4261d37d0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d3d70&gt;, &lt;matplotlib.axis.YTick object at 0x4261f0a40&gt;, &lt;matplotlib.axis.YTick object at 0x4261f1280&gt;, &lt;matplotlib.axis.YTick object at 0x4261d1940&gt;], [Text(0, 0.5, 'a'), Text(0, 1.5, 'b'), Text(0, 2.5, 'c'), Text(0, 3.5, 'd'), Text(0, 4.5, 'e'), Text(0, 5.5, 'f'), Text(0, 6.5, 'g'), Text(0, 7.5, 'h'), Text(0, 8.5, 'i'), Text(0, 9.5, 'j'), Text(0, 10.5, 'k'), Text(0, 11.5, 'l'), Text(0, 12.5, 'm'), Text(0, 13.5, 'n'), Text(0, 14.5, 'o'), Text(0, 15.5, 'p'), Text(0, 16.5, 'q'), Text(0, 17.5, 'r'), Text(0, 18.5, 's'), Text(0, 19.5, 't'), Text(0, 20.5, 'u'), Text(0, 21.5, 'v'), Text(0, 22.5, 'w'), Text(0, 23.5, 'x'), Text(0, 24.5, 'y'), Text(0, 25.5, 'z')])</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="25-llm_files/figure-html/unnamed-chunk-13-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>This will take us one step closer to how LLMs generate text. However, LLM’s have much larger context windows, meaning they can consider much longer sequences of text when generating the next token. This is crucial for understanding how LLMs can generate coherent and contextually relevant text. The modern models such as Gemini 2.5 pro uses context windows of up to 1 million tokens. It is approximately the size of the “War and Peace” novel by Leo Tolstoy.</p>
</section>
<section id="the-secret-sauce-from-attention-to-transformers" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-secret-sauce-from-attention-to-transformers"><span class="header-section-number">2.3</span> The Secret Sauce: From Attention to Transformers</h2>
<p>Before Transformers revolutionized the field, AI models struggled with understanding long sentences in much the same way a person with severe short-term memory loss might struggle to follow a complex conversation. Recurrent Neural Networks (RNNs) processed text word-by-word, like a person reading a long scroll with a narrow window that only revealed one word at a time. This sequential process created a fundamental bottleneck; by the time the model reached the end of a paragraph, it had often forgotten what was said at the beginning, losing crucial context that might completely change the meaning of what it was reading.</p>
<p>The breakthrough came with the 2017 paper titled “Attention Is All You Need,” a deceptively simple title that announced one of the most significant advances in AI history. The authors introduced the Transformer and, with it, a completely new way to process language that would eventually power everything from Google’s search results to ChatGPT’s conversations.</p>
<p>The core innovation of the Transformer is the attention mechanism, and to understand it, imagine you’re a detective trying to solve a complex case. You have hundreds of witness statements, documents, and pieces of evidence scattered across your desk. Traditional methods would force you to examine each piece of evidence in order, one by one, trying to remember how each relates to everything you’ve seen before. But what if instead, you could instantly see all the evidence at once and intuitively understand which pieces were most relevant to each other? What if you could automatically highlight the connections between a witness statement and a piece of physical evidence, or between two seemingly unrelated documents that actually tell the same story?</p>
<p>This is precisely how attention works in an LLM. For any given word or concept the model is focusing on (what we call a query), it simultaneously scans all other words in the text (the keys) to determine their relevance. It then calculates a weighted combination of the meanings of those words (the values) to produce a rich, contextualized understanding. It’s like having a hyper-intelligent librarian who, when you ask about “machine learning,” doesn’t just find books with that exact phrase, but intuitively understands which books about statistics, computer science, neuroscience, and even philosophy might be relevant to your query.</p>
<p>Mathematically, this process can be described elegantly. For a query <span class="math inline">\(\mathbf{q}\)</span> and a database of key-value pairs <span class="math inline">\(\mathcal{D} = \{(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)\}\)</span>, the output is:</p>
<p><span class="math display">\[\text{Attention}(\mathbf{q}, \mathcal{D}) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i\]</span></p>
<p>The attention weights, <span class="math inline">\(\alpha(\mathbf{q}, \mathbf{k}_i)\)</span>, determine how much focus to place on each value. These weights are calculated using a softmax function, ensuring they are normalized and sum to 1:</p>
<p><span class="math display">\[\alpha(\mathbf{q}, \mathbf{k}_i) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_j \exp(a(\mathbf{q}, \mathbf{k}_j))}\]</span></p>
<p>This elegant mechanism liberates the model from the tyranny of sequential processing. It can look at an entire sequence at once, effortlessly handling text of any length, drawing connections between distant words in a text, and processing information in parallel, making it incredibly efficient to train.</p>
<p>The concept of attention pooling can actually be traced back to classical kernel methods like Nadaraya-Watson regression, where similarity kernels determine how much weight to give to different data points. Consider how we might weight different pieces of information based on their similarity to what we’re looking for:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define some kernels for attention pooling</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian(x):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> boxcar(x):</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">abs</span>(x) <span class="op">&lt;</span> <span class="fl">1.0</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> constant(x):</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">+</span> <span class="dv">0</span> <span class="op">*</span> x  <span class="co"># noqa: E741</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epanechikov(x):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">abs</span>(x), np.zeros_like(x))</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, sharey<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> (gaussian, boxcar, constant, epanechikov)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> (<span class="st">'Gaussian'</span>, <span class="st">'Boxcar'</span>, <span class="st">'Constant'</span>, <span class="st">'Epanechikov'</span>)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>, <span class="fl">0.1</span>)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kernel, name, ax <span class="kw">in</span> <span class="bu">zip</span>(kernels, names, axes):</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, kernel(x))</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(name)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="25-llm_files/figure-html/unnamed-chunk-14-7.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<p>Each of these kernels represents a different way of weighting information based on similarity or distance. In neural networks, this translates to learning how to attend to different parts of the input sequence, but with the flexibility to learn much more complex patterns than these simple mathematical functions.</p>
<p>One of the most ingenious aspects of the Transformer architecture is multi-head attention, which allows the model to attend to different types of relationships simultaneously. Think of it like having multiple experts in a room, each specialized in different aspects of language. One expert might focus on grammatical relationships between words, another on semantic meaning, another on emotional tone, and yet another on factual content. Instead of having just one attention mechanism, multi-head attention creates multiple parallel attention processes, each learning to capture different types of dependencies within the text.</p>
<p>Perhaps even more remarkable is self-attention, where queries, keys, and values all come from the same sequence. This allows each position in the sequence to attend to all other positions, creating rich representations that capture the complex web of relationships within a single piece of text. When you read a sentence like “The trophy would not fit in the brown suitcase because it was too big,” self-attention helps the model understand that “it” most likely refers to the trophy, not the suitcase, by considering the relationships between all the words simultaneously.</p>
<p>Since attention mechanisms are inherently permutation-invariant, Transformers require positional encoding to understand sequence order. This gives the model a sense of where each word appears in the sequence, enabling it to distinguish between “The cat sat on the mat” and “The mat sat on the cat.”</p>
</section>
<section id="from-words-to-tokens-the-hidden-language-of-llms" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="from-words-to-tokens-the-hidden-language-of-llms"><span class="header-section-number">2.4</span> From Words to Tokens: The Hidden Language of LLMs</h2>
<p>Before any text can enter an LLM, it must first be converted into tokens through a process that’s more art than science. This tokenization step is crucial for understanding how LLMs actually process language, yet it’s often invisible to users who simply type words into a chat interface.</p>
<p>Modern LLMs don’t actually work with words as we understand them, but with subword units that balance efficiency with meaning. Consider how a sentence like “The Verbasizer helped Bowie create unexpected word combinations” might be tokenized. Rather than treating each word as a single unit, the tokenizer might split it into pieces like “The”, ” Ver”, “bas”, “izer”, ” helped”, ” Bow”, “ie”, ” create”, and so on. This approach allows the model to handle rare words and names by breaking them into more common subcomponents.</p>
<p>The choice of tokenization strategy has profound implications for model performance. A vocabulary that’s too small forces the model to represent complex concepts with many tokens, making it harder to capture meaning efficiently. Most modern LLMs use vocabularies of 50,000 to 100,000 tokens, carefully balanced to represent different languages while maintaining computational efficiency.</p>
<p>This tokenization process explains some of the quirks you might notice when working with LLMs. Names from science fiction, like “Tatooine” from Star Wars, might be split into unfamiliar pieces, making the model less reliable when discussing fictional universes. Similarly, code snippets, mathematical expressions, and specialized terminology can be tokenized in ways that fragment their meaning.</p>
</section>
<section id="the-architecture-that-changed-everything" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="the-architecture-that-changed-everything"><span class="header-section-number">2.5</span> The Architecture That Changed Everything</h2>
<p>The complete Transformer architecture consists of two main components working in harmony. The encoder stack processes input sequences through multiple layers of multi-head self-attention and position-wise feed-forward networks, enhanced with residual connections and layer normalization that help stabilize training. The decoder stack uses masked multi-head self-attention to prevent the model from “cheating” by looking at future tokens during training.</p>
<p>This architecture enabled the development of three distinct families of models, each optimized for different types of tasks. Encoder-only models like BERT excel at understanding tasks such as classification, question answering, and sentiment analysis. They can see the entire input at once, making them particularly good at tasks where understanding context from both directions matters.</p>
<p>Decoder-only models like GPT are particularly good at generation tasks, producing coherent, contextually appropriate text. These models are trained to predict the next token given all the previous tokens in a sequence, which might seem simple but turns out to be incredibly powerful for natural text generation.</p>
<p>Encoder-decoder models like T5 bridge both worlds, excelling at sequence-to-sequence tasks like translation and summarization. The text-to-text approach treats all tasks as text generation problems. Need to translate from English to French? The model learns to generate French text given English input. Want to answer a question? The model generates an answer given a question and context.</p>
</section>
<section id="the-scale-revolution-how-bigger-became-better" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="the-scale-revolution-how-bigger-became-better"><span class="header-section-number">2.6</span> The Scale Revolution: How Bigger Became Better</h2>
<p>The true revolution of large language models came from the discovery that these models exhibit remarkable scaling properties. Unlike many machine learning systems that hit performance plateaus as they grow larger, Transformers demonstrated that their performance scales as a predictable power law with three key factors: the number of model parameters, the amount of training data, and the computational resources used for training.</p>
<p>This scaling behavior has led to exponential growth in model sizes. GPT-1, released in 2018 with 117 million parameters, was already considered large for its time. GPT-2, with 1.5 billion parameters, was initially deemed too dangerous to release publicly. GPT-3’s 175 billion parameters represented a quantum leap. Today, we’re seeing models with hundreds of billions to trillions of parameters.</p>
<p>But size alone isn’t the only story. The way these models are trained has become increasingly sophisticated. Masked language modeling involves randomly masking tokens in the input and training the model to predict what’s missing. This approach enables bidirectional context understanding, allowing the model to see both what comes before and after a given word when making predictions.</p>
<p>Autoregressive generation takes a different approach. These models are trained to predict the next token given all the previous tokens in a sequence. This forces the model to learn not just vocabulary and grammar, but also narrative structure, logical reasoning, and even elements of common sense.</p>
</section>
<section id="understanding-the-generation-process" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="understanding-the-generation-process"><span class="header-section-number">2.7</span> Understanding the Generation Process</h2>
<p>To truly appreciate how LLMs work, we need to understand what happens under the hood during text generation. When you ask an LLM a question, the process begins with tokenization, converting your input text into numerical token IDs that the model can process. These tokens are then encoded through multiple transformer layers, each adding layers of understanding and context.</p>
<p>The model produces what are called logits—scores for each token in its vocabulary indicating how likely each token is to come next. The larger the score, the more appropriate the token is considered. For a prompt like “In the wastelands of mine,” the LLM might assign the largest logits to words like “echoes” and the lowest to “scissors,” reflecting the model’s understanding of context and appropriateness.</p>
<p>These logits are then converted into a probability distribution using a softmax function, ensuring all probabilities sum to 1. But here’s where it gets interesting: the model doesn’t just pick the most likely next token. That would lead to repetitive, boring text. Instead, various sampling strategies introduce controlled randomness to make the output more interesting and diverse.</p>
<p>Temperature controls the randomness in this selection process. At low temperatures (close to 0), the model becomes more deterministic, almost always choosing the most likely next token. This produces focused, consistent output but can lead to repetitive text. Higher temperatures introduce more randomness, allowing for creative and surprising outputs but with the risk of producing incoherent content.</p>
<p>Top-p sampling, also known as nucleus sampling, provides another way to control generation quality. Instead of considering the entire vocabulary, it selects from only the smallest set of tokens whose cumulative probability exceeds a threshold. This maintains diversity while avoiding the pitfalls of sampling from very unlikely tokens that might derail the generation.</p>
<p>Frequency and presence penalties offer additional control by discouraging repetition. If you’ve ever noticed an LLM getting stuck repeating the same phrases, these penalties can help. Frequency penalties reduce the likelihood of tokens based on how often they’ve already appeared, while presence penalties discourage any token that has appeared before, regardless of frequency.</p>
</section>
<section id="the-challenge-of-creativity-and-diversity" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="the-challenge-of-creativity-and-diversity"><span class="header-section-number">2.8</span> The Challenge of Creativity and Diversity</h2>
<p>One fascinating aspect of LLMs is their tendency to fall into predictable patterns, especially when generating creative content. If you ask multiple models to create fantasy characters, you might notice they gravitate toward the same names and occupations. Characters named “Thrandil” or “Lyralei” with occupations like “Shadowcrafting” or “Dragon Taming” appear with surprising frequency, revealing the models’ learned biases from their training data.</p>
<p>This phenomenon occurs because LLMs learn patterns from their training data, and fantasy literature contains certain recurring tropes and naming conventions. When prompted to generate fantasy content, the model naturally gravitates toward these well-represented patterns, leading to less diversity than we might hope for.</p>
<p>Understanding this limitation is crucial for practical applications. If you’re building a character generator or creative writing assistant, you need strategies to encourage more diverse outputs. This might involve using higher temperature settings, implementing diversity-promoting sampling strategies, or engineering prompts that explicitly request originality and cultural references from specific traditions.</p>
</section>
<section id="choosing-the-right-model-for-your-application" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="choosing-the-right-model-for-your-application"><span class="header-section-number">2.9</span> Choosing the Right Model for Your Application</h2>
<p>The landscape of available LLMs is vast and constantly evolving, making model selection a complex decision. When choosing a model, you need to consider several factors that go beyond just picking the highest-performing option on a benchmark.</p>
<p>Size tiers offer different trade-offs. Very small models (around 3 billion parameters or less) are fast and efficient, ideal for applications where resources are limited or real-time performance is crucial. These models can run on consumer hardware and often provide adequate performance for simpler tasks like basic text classification.</p>
<p>Medium-sized models (7 to 30 billion parameters) often represent the sweet spot for many applications. They provide significantly better performance than smaller models while still being manageable in terms of computational requirements. Large models (30 billion parameters or more) provide the best performance and often demonstrate emergent capabilities that smaller models lack, but they require specialized hardware and can be expensive to run.</p>
<p>Beyond general capability, you need to consider specialized features. Code generation models have been specifically trained on programming languages and software development tasks. They understand the unique challenges of code completion, including the need for “fill-in-the-middle” capabilities rather than just adding to the end of existing code. Multilingual models are designed to work across many languages simultaneously, while domain-specific models have been fine-tuned on specialized corpora.</p>
<p>Practical constraints often override pure performance considerations. Computational resources, including GPU memory and inference speed, can be limiting factors. Cost considerations vary dramatically between using cloud APIs versus self-hosting models. Latency requirements might favor smaller, faster models over larger, more capable ones. Privacy concerns might necessitate on-premise deployment rather than cloud-based solutions.</p>
</section>
<section id="evaluating-model-performance" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="evaluating-model-performance"><span class="header-section-number">2.10</span> Evaluating Model Performance</h2>
<p>When evaluating models, researchers and practitioners rely on various benchmarks that test different aspects of language understanding and generation. The Massive Multitask Language Understanding (MMLU) benchmark tests knowledge across diverse academic subjects, from high school mathematics to philosophy. HellaSwag evaluates common sense reasoning by asking models to predict likely continuations of scenarios. HumanEval specifically tests code generation capabilities.</p>
<p>However, benchmarks have limitations and don’t always reflect real-world performance. A model that excels at multiple-choice questions might struggle with open-ended creative tasks. Code generation benchmarks might not capture the nuanced requirements of your specific programming domain. The key is to use benchmarks as a starting point while conducting thorough validation using data that closely resembles your actual use case.</p>
<p>Consider implementing your own evaluation framework that tests the specific capabilities you need. If you’re building a customer service chatbot, create test scenarios that reflect your actual customer interactions. If you’re developing a creative writing assistant, evaluate the model’s ability to generate diverse, engaging content in your target style or genre.</p>
</section>
<section id="when-things-go-wrong-understanding-llm-limitations" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="when-things-go-wrong-understanding-llm-limitations"><span class="header-section-number">2.11</span> When Things Go Wrong: Understanding LLM Limitations</h2>
<p>Despite their impressive capabilities, LLMs face several fundamental challenges that become apparent in practical applications. The most widely discussed is the tendency to “hallucinate”—generating confident-sounding but factually incorrect information. This happens because LLMs are fundamentally trained to generate plausible-sounding text, not necessarily true text.</p>
<p>When an LLM encounters a question about a topic it hasn’t seen much during training, it doesn’t simply say “I don’t know.” Instead, it generates text that follows the patterns it has learned, which can result in convincing-sounding but completely fabricated facts, dates, or citations. This limitation is particularly problematic in applications where accuracy is critical.</p>
<p>Bias represents another significant challenge. LLMs can exhibit various biases present in their training data, from subtle gender stereotypes to more overt cultural prejudices. Since these models learn from text produced by humans, they inevitably absorb human biases, sometimes amplifying them in unexpected ways.</p>
<p>Security concerns have emerged as LLMs become more capable. “Jailbreaking” refers to techniques that manipulate models into generating content that violates their safety guidelines. Clever prompt engineering can sometimes bypass safety measures, leading models to provide harmful instructions or exhibit problematic behaviors they were designed to avoid.</p>
<p>Understanding these limitations is crucial for responsible deployment. You need to implement appropriate guardrails, fact-checking mechanisms, and human oversight, especially in high-stakes applications. The goal isn’t to avoid these limitations entirely—that’s currently impossible—but to understand them and design your systems accordingly.</p>
</section>
<section id="building-practical-applications" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="building-practical-applications"><span class="header-section-number">2.12</span> Building Practical Applications</h2>
<p>Modern applications of LLMs extend far beyond simple text generation into sophisticated systems that augment human capabilities. Conversational AI has evolved from simple rule-based chatbots to sophisticated systems capable of maintaining context across long conversations, understanding nuanced requests, and even developing distinct personalities.</p>
<p>When building conversational systems, memory management becomes crucial. LLMs have limited context windows—typically measured in thousands of tokens—so you need strategies for maintaining relevant conversation history while staying within these limits. This might involve summarizing older parts of the conversation, selectively keeping important information, or implementing external memory systems.</p>
<p>In content creation applications, LLMs serve as writing assistants that help with everything from grammar and style suggestions to structural improvements and creative ideation. Code generation has become particularly sophisticated, with models capable of writing complete functions, debugging existing code, and generating documentation. These tools work best when they augment rather than replace human expertise.</p>
<p>Analysis and understanding applications leverage LLMs’ ability to process and synthesize large amounts of text. Document summarization systems can extract key points from lengthy reports. Sentiment analysis applications help businesses understand customer feedback at scale. Information extraction systems can identify entities, relationships, and key facts from unstructured text.</p>
<p>Advanced techniques like prompt engineering have emerged as crucial skills for effectively using LLMs. This involves crafting instructions that guide the model toward desired outputs, often requiring deep understanding of how different phrasings and structures affect model behavior. Few-shot learning allows you to teach models new tasks by providing just a few examples, while chain-of-thought prompting encourages models to break down complex reasoning into step-by-step processes.</p>
<p>Retrieval-augmented generation represents a particularly promising approach that combines LLMs with external knowledge bases. Instead of relying solely on knowledge encoded in model parameters during training, these systems can dynamically retrieve relevant information from databases, documents, or the internet to inform their responses. This approach helps address the hallucination problem while keeping models up-to-date with current information.</p>
</section>
<section id="creative-collaboration-when-artists-meet-algorithms" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="creative-collaboration-when-artists-meet-algorithms"><span class="header-section-number">2.13</span> Creative Collaboration: When Artists Meet Algorithms</h2>
<p>The intersection of AI and creativity offers fascinating insights into how these technologies might augment rather than replace human creativity. David Bowie’s experimentation with the “Verbasizer” in the 1990s provides a prescient example of human-AI collaboration in creative work. Bowie created an algorithmic text generator that would help overcome writer’s block by randomly recombining words and phrases from existing text.</p>
<p>Bowie described how this process resulted in a “kaleidoscope of meanings,” with words and ideas colliding in surprising ways. The system would take phrases like “I am a blackstar” and randomly combine them to create new variations that sparked his creative process. The randomness of the algorithm would often produce surprising results that led him in new creative directions, breaking him out of creative ruts and helping him discover unexpected word combinations.</p>
<p>This collaborative approach to AI-assisted creativity has become increasingly common in modern creative industries. Musicians use AI tools for melody generation and lyric writing assistance. Writers employ LLMs for brainstorming, overcoming writer’s block, and exploring alternative narrative directions. Visual artists use AI for concept generation and style exploration.</p>
<p>The key insight from Bowie’s work—that AI can serve as a creative collaborator rather than just an automation tool—remains relevant as these technologies become more sophisticated. The most successful creative applications of LLMs seem to be those that enhance human creativity rather than attempting to replace it entirely.</p>
</section>
<section id="looking-forward-the-evolving-landscape" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="looking-forward-the-evolving-landscape"><span class="header-section-number">2.14</span> Looking Forward: The Evolving Landscape</h2>
<p>The field of large language models continues to evolve at a breathtaking pace. Architectural innovations like Mixture of Experts (MoE) models allow for scaling model capacity without proportionally increasing computational requirements. Multimodal transformers are beginning to bridge the gap between text, images, audio, and other modalities, creating systems that can understand and generate content across multiple forms of media.</p>
<p>Training efficiency has become a crucial area of research as models grow ever larger. Parameter-efficient fine-tuning techniques allow practitioners to adapt large models to specific tasks without retraining all parameters. Knowledge distillation enables the creation of smaller, faster models that retain much of the capability of their larger teachers.</p>
<p>Safety and alignment research has become increasingly important as these models become more capable and widely deployed. Constitutional AI approaches attempt to instill models with explicit principles and values. Human feedback training uses human preferences to fine-tune model behavior, helping ensure that models are helpful, harmless, and honest.</p>
<p>The applications continue to expand into new domains. In scientific discovery, LLMs are being used to generate hypotheses, analyze literature, and suggest experimental designs. Educational applications range from personalized tutoring systems to tools that help teachers create customized learning materials. The creative industries are being transformed as artists, writers, and designers incorporate AI tools into their workflows as collaborators that enhance and accelerate the creative process.</p>
</section>
<section id="the-future-of-human-ai-partnership" class="level2" data-number="2.15">
<h2 data-number="2.15" class="anchored" data-anchor-id="the-future-of-human-ai-partnership"><span class="header-section-number">2.15</span> The Future of Human-AI Partnership</h2>
<p>Large Language Models represent more than just a technological achievement; they represent a fundamental shift in how we interact with information and computational systems. These models have learned to speak our language, literally and figuratively, opening up possibilities for human-computer interaction that feel more natural and intuitive than anything that came before.</p>
<p>As we continue to push the boundaries of what’s possible with these systems, we’re not just building better tools—we’re exploring what it means for machines to understand and generate human language. The journey from attention mechanisms to trillion-parameter models has been remarkable, but it’s clear that we’re still in the early stages of this technological revolution.</p>
<p>The future of AI is not just about making machines smarter—it’s about making the partnership between human and artificial intelligence more powerful, more creative, and more beneficial for society. As these models become more capable, more efficient, and more widely accessible, they promise to transform virtually every aspect of how we work, learn, create, and communicate.</p>
<p>Understanding how these systems work, their capabilities and limitations, and how to use them effectively will become increasingly important skills. Whether you’re a developer building LLM-powered applications, a researcher pushing the boundaries of what’s possible, or simply someone trying to understand this rapidly evolving field, the key is to approach these tools with both enthusiasm for their potential and awareness of their current limitations.</p>
<p>The revolution in AI that began with the simple idea that “attention is all you need” has only just begun. As we continue to explore the possibilities and address the challenges, we’re not just witnessing the evolution of artificial intelligence—we’re participating in the redefinition of what it means to think, create, and communicate in the digital age.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./24-nlp.html" class="pagination-link" aria-label="Natural Language Processing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>