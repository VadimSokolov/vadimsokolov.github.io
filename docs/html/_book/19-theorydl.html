<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>19&nbsp; Theory of Deep Learning – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./20-sgd.html" rel="next">
<link href="./18-nn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="19&nbsp; Theory of Deep Learning – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/hyperplane.svg">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="19&nbsp; Theory of Deep Learning – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/hyperplane.svg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-theorydl.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#ridge-and-projection-pursuit-regression" id="toc-ridge-and-projection-pursuit-regression" class="nav-link active" data-scroll-target="#ridge-and-projection-pursuit-regression"><span class="header-section-number">19.1</span> Ridge and Projection Pursuit Regression</a>
  <ul class="collapse">
  <li><a href="#from-approximation-to-representation" id="toc-from-approximation-to-representation" class="nav-link" data-scroll-target="#from-approximation-to-representation">From Approximation to Representation</a></li>
  </ul></li>
  <li><a href="#kolmogorov-superposition-theorem-kst" id="toc-kolmogorov-superposition-theorem-kst" class="nav-link" data-scroll-target="#kolmogorov-superposition-theorem-kst"><span class="header-section-number">19.2</span> Kolmogorov Superposition Theorem (KST)</a>
  <ul class="collapse">
  <li><a href="#kolmogorov-arnold-networks" id="toc-kolmogorov-arnold-networks" class="nav-link" data-scroll-target="#kolmogorov-arnold-networks">Kolmogorov-Arnold Networks</a></li>
  </ul></li>
  <li><a href="#kolmogorov-generalized-additive-models-k-gam" id="toc-kolmogorov-generalized-additive-models-k-gam" class="nav-link" data-scroll-target="#kolmogorov-generalized-additive-models-k-gam"><span class="header-section-number">19.3</span> Kolmogorov Generalized Additive Models (K-GAM)</a></li>
  <li><a href="#space-partitioning" id="toc-space-partitioning" class="nav-link" data-scroll-target="#space-partitioning"><span class="header-section-number">19.4</span> Space Partitioning</a>
  <ul class="collapse">
  <li><a href="#connection-to-bayesian-model-averaging" id="toc-connection-to-bayesian-model-averaging" class="nav-link" data-scroll-target="#connection-to-bayesian-model-averaging">Connection to Bayesian Model Averaging</a></li>
  </ul></li>
  <li><a href="#kernel-smoothing-and-interpolation" id="toc-kernel-smoothing-and-interpolation" class="nav-link" data-scroll-target="#kernel-smoothing-and-interpolation"><span class="header-section-number">19.5</span> Kernel Smoothing and Interpolation</a></li>
  <li><a href="#transformers-as-kernel-smoothing" id="toc-transformers-as-kernel-smoothing" class="nav-link" data-scroll-target="#transformers-as-kernel-smoothing"><span class="header-section-number">19.6</span> Transformers as Kernel Smoothing</a>
  <ul class="collapse">
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer">Transformer</a></li>
  </ul></li>
  <li><a href="#deep-learning-as-representation-learning" id="toc-deep-learning-as-representation-learning" class="nav-link" data-scroll-target="#deep-learning-as-representation-learning"><span class="header-section-number">19.7</span> Deep Learning as Representation Learning</a>
  <ul class="collapse">
  <li><a href="#dimensionality-expansion-vs.-reduction" id="toc-dimensionality-expansion-vs.-reduction" class="nav-link" data-scroll-target="#dimensionality-expansion-vs.-reduction">Dimensionality Expansion vs.&nbsp;Reduction</a></li>
  <li><a href="#linear-baselines-pca-and-pls" id="toc-linear-baselines-pca-and-pls" class="nav-link" data-scroll-target="#linear-baselines-pca-and-pls">Linear Baselines: PCA and PLS</a></li>
  <li><a href="#uncertainty-quantification" id="toc-uncertainty-quantification" class="nav-link" data-scroll-target="#uncertainty-quantification">Uncertainty Quantification</a></li>
  </ul></li>
  <li><a href="#sec-double-descent" id="toc-sec-double-descent" class="nav-link" data-scroll-target="#sec-double-descent"><span class="header-section-number">19.8</span> Double Descent</a></li>
  <li><a href="#application" id="toc-application" class="nav-link" data-scroll-target="#application"><span class="header-section-number">19.9</span> Application</a>
  <ul class="collapse">
  <li><a href="#simulated-data" id="toc-simulated-data" class="nav-link" data-scroll-target="#simulated-data">Simulated Data</a></li>
  <li><a href="#training-rates" id="toc-training-rates" class="nav-link" data-scroll-target="#training-rates">Training Rates</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">19.10</span> Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-theorydl.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter explores the theoretical foundations of deep learning through the lens of multivariate function approximation. We begin with <em>ridge functions</em> as fundamental building blocks—functions of the form <span class="math inline">\(f(x) = g(w^Tx)\)</span> that represent one of the simplest forms of nonlinear multivariate functions by combining a single linear projection with a univariate nonlinear transformation. Their key geometric property—remaining constant along directions orthogonal to the projection vector <span class="math inline">\(w\)</span>—makes them particularly useful for high-dimensional approximation.</p>
<p>Building on ridge functions, we introduce <em>projection pursuit regression</em>, which approximates complex input-output relationships using linear combinations of ridge functions. This technique, developed in the 1980s, provides the mathematical foundation for understanding how neural networks decompose high-dimensional problems.</p>
<p>The chapter culminates with the <em>Kolmogorov Superposition Theorem</em> (KST), a profound result showing that any real-valued continuous function can be represented as a sum of compositions of single-variable functions. This theorem provides a theoretical framework for understanding how multivariate functions can be decomposed into simpler, more manageable components—a principle that underlies the architecture of modern neural networks.</p>
<p>Throughout, we examine a central question: can we achieve superior performance through mathematically elegant representations of multivariate functions rather than raw computational power? This tension between theoretical efficiency and practical computation motivates much of the research in deep learning theory.</p>
<section id="ridge-and-projection-pursuit-regression" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="ridge-and-projection-pursuit-regression"><span class="header-section-number">19.1</span> Ridge and Projection Pursuit Regression</h2>
<p>To understand the significance of this trade-off, we consider ridge functions, which represent a fundamental building block in multivariate analysis. Since our ultimate goal is to model arbitrary multivariate functions <span class="math inline">\(f\)</span>, we need a way to reduce dimensionality while preserving the ability to capture nonlinear relationships. Ridge functions accomplish this by representing one of the simplest forms of nonlinear multivariate functions, requiring only a single linear projection and a univariate nonlinear transformation. Formally, a ridge function <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> takes the form <span class="math inline">\(f(x) = g(w^Tx)\)</span>, where <span class="math inline">\(g\)</span> is a univariate function and <span class="math inline">\(x,w \in \mathbb{R}^n\)</span>. The non-zero vector <span class="math inline">\(w\)</span> is called the direction. The term “ridge” reflects a key geometric property: the function remains constant along any direction orthogonal to <span class="math inline">\(w\)</span>. Specifically, for any direction <span class="math inline">\(u\)</span> such that <span class="math inline">\(w^Tu = 0\)</span>, we have</p>
<p><span class="math display">\[
f(x+u) = g(w^T(x+u)) = g(w^Tx) = f(x)
\]</span></p>
<p>This structural simplicity makes ridge functions particularly useful as building blocks for high-dimensional approximation.</p>
<p>Ridge functions play a central role in high-dimensional statistical analysis. For example, projection pursuit regression approximates input-output relations using a linear combination of ridge functions <span class="citation" data-cites="friedman1981projection huber1985projection">(<a href="references.html#ref-friedman1981projection" role="doc-biblioref">Friedman and Stuetzle 1981</a>; <a href="references.html#ref-huber1985projection" role="doc-biblioref">Huber 1985</a>)</span>: <span class="math display">\[
\phi(x) = \sum_{i=1}^{p}g_i(w_i^Tx),
\]</span> where both the directions <span class="math inline">\(w_i\)</span> and functions <span class="math inline">\(g_i\)</span> are learnable parameters, and <span class="math inline">\(w_i^Tx\)</span> represents the one-dimensional projection of the input vector <span class="math inline">\(x\)</span> onto the direction defined by <span class="math inline">\(w_i\)</span>. Each <span class="math inline">\(g_i(w_i^Tx)\)</span> can be interpreted as a learned feature extracted from the data. <span class="citation" data-cites="diaconis1984nonlinear">Diaconis and Shahshahani (<a href="references.html#ref-diaconis1984nonlinear" role="doc-biblioref">1984</a>)</span> extended this approach using nonlinear functions of linear combinations, establishing the mathematical framework that would later inform the design of multi-layer neural networks.</p>
<section id="from-approximation-to-representation" class="level3">
<h3 class="anchored" data-anchor-id="from-approximation-to-representation">From Approximation to Representation</h3>
<p>While ridge functions and projection pursuit provide powerful approximation tools, they rely on sums of increasing numbers of components to achieve precision. A fundamental question in deep learning theory is whether we can achieve <em>exact</em> representation of multivariate functions using a finite number of components, rather than just approximation.</p>
<p>This question addresses a core tension in machine learning: the trade-off between the mathematical efficiency of a representation and its computational feasibility. Modern deep learning succeeds not just because of hardware acceleration (GPUs), but because deep architectures may implicitly leverage efficient representations of high-dimensional functions.</p>
<p>To understand this, we turn to a foundational result that seemingly solves the representation problem entirely: the Kolmogorov Superposition Theorem.</p>
</section>
</section>
<section id="kolmogorov-superposition-theorem-kst" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="kolmogorov-superposition-theorem-kst"><span class="header-section-number">19.2</span> Kolmogorov Superposition Theorem (KST)</h2>
<p>Kolmogorov demonstrated that any real-valued continuous function <span class="math inline">\(f(\mathbf{x})\)</span> on <span class="math inline">\(E^n\)</span> can be represented as a composition of single-variable functions:</p>
<p><span class="math display">\[
f(x_1,\ldots,x_n) = \sum_{q=1}^{2n+1} g_q\left(\phi_q(x_1,\ldots,x_n)\right)
\]</span></p>
<p>where <span class="math inline">\(g_q\)</span> are continuous single-variable functions defined on <span class="math inline">\(\phi_q(E^n)\)</span>. Kolmogorov further showed that the <span class="math inline">\(\phi_q\)</span> functions can be decomposed into sums of single-variable functions:</p>
<p><span class="math display">\[
\phi_q(x_1,\ldots,x_n) = \sum_{i=1}^n \psi_{q,i}(x_i)
\]</span></p>
<p>The Kolmogorov representation theorem <span class="citation" data-cites="kolmogorov1956representation">Kolmogorov (<a href="references.html#ref-kolmogorov1956representation" role="doc-biblioref">1956</a>)</span> takes the form:</p>
<p><span class="math display">\[
f(x_1,\ldots,x_n) = \sum_{q=1}^{2n+1} g_q\left(\sum_{i=1}^n \psi_{q,i}(x_i)\right)
\]</span></p>
<p>The theorem has been refined over time, with inner functions becoming Hölder continuous and Lipschitz continuous, requiring modifications to both outer and inner functions.</p>
<p>The inner functions <span class="math inline">\(\psi_{q,i}\)</span> partition the input space into distinct regions, and the outer function <span class="math inline">\(g\)</span> must be constructed to provide the correct output values across these regions. For each input configuration, the inner functions generate a unique encoding, and <span class="math inline">\(g\)</span> must map this encoding to the appropriate value of <span class="math inline">\(f(x)\)</span>. This creates a dictionary-like structure that associates each region with its corresponding output value.</p>
<p>The constructive proof of KST has been refined through several contributions. <span class="citation" data-cites="sprecher1965structure">Sprecher (<a href="references.html#ref-sprecher1965structure" role="doc-biblioref">1965</a>)</span> provided the first explicit construction of the inner functions, though his proof contained technical gaps. <span class="citation" data-cites="koppen2000curse">Köppen (<a href="references.html#ref-koppen2000curse" role="doc-biblioref">2000</a>)</span> corrected these errors and provided a complete algorithmic construction. More recently, <span class="citation" data-cites="actor2018computation">Actor (<a href="references.html#ref-actor2018computation" role="doc-biblioref">2018</a>)</span> and <span class="citation" data-cites="dembo2021note">Dembo (<a href="references.html#ref-dembo2021note" role="doc-biblioref">2021</a>)</span> proposed computational improvements to the algorithm. <span class="citation" data-cites="braun2009constructive">Braun and Riedmiller (<a href="references.html#ref-braun2009constructive" role="doc-biblioref">2009</a>)</span> further enhanced the understanding by providing precise definitions of the shift parameters <span class="math inline">\(\delta_k\)</span> and characterizing the topological structure induced by the inner functions.</p>
<p>A fundamental trade-off in KST exists between function smoothness and dimensionality. The inner functions <span class="math inline">\(\psi_{p,q}\)</span> can be chosen from two different function spaces, each offering distinct advantages. The first option is to use functions from <span class="math inline">\(C^1([0,1])\)</span>—the space of continuously differentiable functions—but this limits the network’s ability to handle higher dimensions effectively. The second option is to relax the smoothness requirement to Hölder continuous functions (<span class="math inline">\(\psi_{p,q} \in \text{Hölder}_\alpha([0,1])\)</span>), which satisfy the inequality <span class="math inline">\(\|\psi(\mathbf{x}) - \psi(\mathbf{y})\| &lt; \|\mathbf{x}-\mathbf{y}\|^\alpha\)</span> for vector-valued inputs. These functions are less smooth, but this “roughness” enables better approximation in higher dimensions.</p>
<section id="kolmogorov-arnold-networks" class="level3">
<h3 class="anchored" data-anchor-id="kolmogorov-arnold-networks">Kolmogorov-Arnold Networks</h3>
<p>A significant development has been the emergence of Kolmogorov-Arnold Networks (KANs). The key innovation of KANs is their use of learnable functions rather than weights on the network edges. This replaces traditional linear weights with univariate functions, typically parametrized by splines, enhancing both representational capacity and interpretability.</p>
<p>A practical connection exists between KST and neural networks: any KAN can be constructed as a 3-layer MLP. Consider a KST in the form of sums of functions, a two layer model:</p>
<p><span class="math display">\[
f( x_1 , \ldots , x_d ) = f( x) = ( g \circ \psi ) (x )
\]</span></p>
<p>Then KAN not only a superposition of functions but also a particular case of a tree of discrete Urysohn operators:</p>
<p><span class="math display">\[
U(x_1 , \ldots , x_d ) = \sum_{j=1}^d g_j (x_j )
\]</span></p>
<p>This structure enables fast, scalable algorithms that avoid backpropagation for any GAM model through <em>projection descent with Kaczmarz schemes</em>—iterative methods originally developed for solving systems of linear equations that can be adapted for training these networks <span class="citation" data-cites="kaczmarz1937angenaherte">(<a href="references.html#ref-kaczmarz1937angenaherte" role="doc-biblioref">Kaczmarz 1937</a>)</span>.</p>
</section>
</section>
<section id="kolmogorov-generalized-additive-models-k-gam" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="kolmogorov-generalized-additive-models-k-gam"><span class="header-section-number">19.3</span> Kolmogorov Generalized Additive Models (K-GAM)</h2>
<p>Rather than using learnable functions as network node activations, Polson and Sokolov directly use KST representation. This 2-layer network with non-differentiable inner function has architecture:</p>
<p><span class="math display">\[
f(x_1,\ldots,x_d) = \sum_{q=0}^{2d} g_q(z_q)
\]</span></p>
<p>where the inner layer embeds <span class="math inline">\([0,1]^d\)</span> to <span class="math inline">\(\mathbb{R}^{2d+1}\)</span> via:</p>
<p><span class="math display">\[
z_q = \eta_q ( x_1 , \ldots , x_d ) = \sum_{p=1}^ d \lambda_p \psi  ( x_p + q a )
\]</span></p>
<p>Here, <span class="math inline">\(\lambda_p = \sum_{r=1}^\infty \gamma^{-(p-1)\beta(r)}\)</span> represents <span class="math inline">\(p\)</span>-adic expansion with <span class="math inline">\(\beta(r) = (n^r-1)/(n-1)\)</span> and <span class="math inline">\(\gamma \geq d+2\)</span>, <span class="math inline">\(a = (\gamma(\gamma-1))^{-1}\)</span>.</p>
<p>The Koppen function <span class="math inline">\(\psi\)</span> is defined through a recursive limit:</p>
<p><span class="math display">\[
\psi(x) = \lim_{k \rightarrow \infty} \psi_k\left(\sum_{l=1}^{k}i_l\gamma^{-l}\right)
\]</span></p>
<p>where each <span class="math inline">\(x \in [0,1]\)</span> has the representation:</p>
<p><span class="math display">\[
x = \sum_{l=1}^{\infty}i_l\gamma^{-l} = \lim_{k \rightarrow \infty} \left(\sum_{l=1}^{k}i_l\gamma^{-l}\right)
\]</span></p>
<p>and <span class="math inline">\(\psi_k\)</span> is defined recursively as:</p>
<p><span class="math display">\[
\psi_k =
\begin{cases}
    d, &amp; d \in D_1\\
    \psi_{k-1}(d-i_k\gamma^{-k}) + i_k\gamma^{-\beta_n(k)}, &amp; d \in D_k,k&gt;1,i_k&lt;\gamma-1\\
    \frac{1}{2}\left(\psi_k(d-\gamma^{-k}) + \psi_{k-1}(d+\gamma^{-k})\right), &amp; d \in D_k, k&gt;1, i_k = \gamma - 1
\end{cases}
\]</span></p>
<p>The most striking aspect of KST is that it leads to a Generalized Additive Model (GAM) with fixed features that are independent of the target function <span class="math inline">\(f\)</span>. These features, determined by the Koppen function, provide universal topological information about the input space, effectively implementing a k-nearest neighbors structure that is inherent to the representation.</p>
<p>This leads to the following architecture. Any deep learner can be represented as a GAM with feature engineering (topological information) given by features <span class="math inline">\(z_k\)</span> in the hidden layer:</p>
<p><span class="math display">\[\begin{align*}
y_i &amp;= \sum_{k=1}^{2n+1} g(z_k)\\
z_k &amp;= \sum_{j=1}^n \lambda^k\psi(x_j + \epsilon k) + k
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\psi\)</span> is a single activation function common to all nodes, and <span class="math inline">\(g\)</span> is a single outer function.</p>
<p>One approach replaces each <span class="math inline">\(\phi_j\)</span> with a single ReLU network <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[
g(x) = \sum_{k=1}^K \beta_k\text{ReLU}(w_kx + b_k)
\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of neurons.</p>
</section>
<section id="space-partitioning" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="space-partitioning"><span class="header-section-number">19.4</span> Space Partitioning</h2>
<p>The partitioning of the input space by a deep learner is similar to that performed by decision trees and partition-based models such as CART, MARS, and RandomForests. However, trees are more local in the regions that they use to construct their estimators. Each neuron in a deep learning model corresponds to a manifold that divides the input space. In the case of the ReLU activation function <span class="math inline">\(f(x) = \max(0,x)\)</span>, the manifold is simply a hyperplane. The neuron activates when the new observation is on the “right” side of this hyperplane, with the activation magnitude equal to the distance from the boundary. For example, in two dimensions, three neurons with ReLU activation functions will divide the space into seven regions, as shown in <a href="#fig-hyper-planes" class="quarto-xref">Figure&nbsp;<span>19.1</span></a>.</p>
<div id="fig-hyper-planes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hyper-planes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hyperplane.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hyper-planes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.1: Space partition by three ReLU neurons in 2D.
</figcaption>
</figure>
</div>
<p>The key difference between tree-based architectures and neural network-based models lies in how hyperplanes are combined. <a href="#fig-tree-dl-comp" class="quarto-xref">Figure&nbsp;<span>19.2</span></a> compares space decomposition by hyperplanes in tree-based versus neural network architectures. We compare a two-layer neural network (bottom row) with a tree model trained via the CART algorithm (top row). The network architecture used is:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=  \mathrm{softmax}(W^{(0)}Z^{(2)} + b^{(0)})\\
Z^{(2)} &amp;=  \tanh(W^{(2)}Z^{(1)} + b^{(2)})\\
Z^{(1)} &amp;=  \tanh(W^{(1)}X + b^{(1)})
\end{aligned}
\]</span></p>
<p>The weight matrices for simple data are <span class="math inline">\(W^{(1)}, W^{(2)} \in \mathbb{R}^{2 \times 2}\)</span>; for circle data <span class="math inline">\(W^{(1)} \in \mathbb{R}^{2 \times 2}\)</span> and <span class="math inline">\(W^{(2)} \in \mathbb{R}^{3 \times 2}\)</span>; and for spiral data we have <span class="math inline">\(W^{(1)} \in \mathbb{R}^{2 \times 2}\)</span> and <span class="math inline">\(W^{(2)} \in \mathbb{R}^{4 \times 2}\)</span>. An advantage of deep architectures is that the number of hyperplanes grows exponentially with the number of layers.</p>
<div id="fig-tree-dl-comp" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-dl-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/simple_data_tree.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/spiral_data_tree.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/circle_data_tree.svg" class="img-fluid figure-img"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/simple_data_dl.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/circle_data_dl.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="fig/spiral_data_dl.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-dl-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.2: Space partition by tree architectures (top row) and deep learning architectures (bottom row) for three different data sets.
</figcaption>
</figure>
</div>
<section id="connection-to-bayesian-model-averaging" class="level3">
<h3 class="anchored" data-anchor-id="connection-to-bayesian-model-averaging">Connection to Bayesian Model Averaging</h3>
<p>This space partitioning perspective connects naturally to Bayesian ensemble methods. A Bayesian probabilistic approach can optimally weight predictors via model averaging:</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{r=1}^R w_r \hat{Y}_r(X)
\]</span></p>
<p>where <span class="math inline">\(\hat{Y}_r(X) = \E{Y \mid X, \text{model } r}\)</span> and weights <span class="math inline">\(w_r\)</span> are posterior model probabilities. <span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span> demonstrated the success of multiple randomized classifiers (e.g., an ensemble of 100 trees), which reduces error rates significantly compared to single trees by exploiting the weak correlation between diverse classifiers.</p>
</section>
</section>
<section id="kernel-smoothing-and-interpolation" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="kernel-smoothing-and-interpolation"><span class="header-section-number">19.5</span> Kernel Smoothing and Interpolation</h2>
<p>The theory of kernel methods was developed by Fredholm in the context of integral equations <span class="citation" data-cites="fredholm1903classe">(<a href="references.html#ref-fredholm1903classe" role="doc-biblioref">Fredholm 1903</a>)</span>. The idea is to represent a function as a linear combination of basis functions, which are called kernels.</p>
<p><span class="math display">\[
f(x) = \int_{a}^{b} K(x,x') \, d\mu(x') \quad \text{where} \quad \mathbf{x} = ( x_1 , \ldots , x_d )
\]</span></p>
<p>Here, the unknown function <span class="math inline">\(f(x)\)</span> is represented as a linear combination of kernels <span class="math inline">\(K(x,x')\)</span> with unknown coefficients <span class="math inline">\(\phi(x')\)</span>. The kernels are known, and the coefficients are unknown. The coefficients are found by solving the integral equation. The first work in this area was done by Abel who considered equations of the form above.</p>
<p>Nowadays, we call those equations Volterra integral equations of the first kind. Integral equations typically arise in inverse problems. Their significance extends beyond their historical origins, as kernel methods have become instrumental in addressing one of the fundamental challenges in modern mathematics: the curse of dimensionality.</p>
<p><span class="citation" data-cites="nadaraya1964estimating">Nadaraya (<a href="references.html#ref-nadaraya1964estimating" role="doc-biblioref">1964</a>)</span> and <span class="citation" data-cites="watson1964smooth">Watson (<a href="references.html#ref-watson1964smooth" role="doc-biblioref">1964</a>)</span> independently proposed using kernels to estimate the regression function. The idea is to estimate <span class="math inline">\(f(x)\)</span> at a point <span class="math inline">\(x\)</span> by computing a weighted average of the response values <span class="math inline">\(y_i\)</span> at nearby points <span class="math inline">\(x_i\)</span>, with the kernel function defining the weights.</p>
<p>The regression function estimate:</p>
<p><span class="math display">\[
\hat{f}(x) = \sum_{i=1}^n  y_i K(x,x_i)/ \sum_{i=1}^n K(x,x_i) ,
\]</span></p>
<p>with normalized kernel weights.</p>
<p>Both Nadaraya and Watson considered the symmetric kernel <span class="math inline">\(K(x,x') = K(\|x'-x\|_2)\)</span>, where <span class="math inline">\(||\cdot||_2\)</span> is the Euclidean norm. The most popular kernel of that sort is the Gaussian kernel:</p>
<p><span class="math display">\[
K(x,x') = \exp\left( -\dfrac{\|x-x'\|_2^2}{2\sigma^2}\right).
\]</span></p>
<p>Alternatively, replacing the 2-norm with inner products: <span class="math inline">\(K(x,x') = \exp(x^Tx'/2\sigma^2)\)</span>.</p>
<p>Kernel methods are supported by numerous generalization bounds which often take the form of inequalities that describe the performance limits of kernel-based estimators. A particularly important example is the Bayes risk for <span class="math inline">\(k\)</span>-nearest neighbors (<span class="math inline">\(k\)</span>-NN), which can be expressed in a kernel framework as:</p>
<p><span class="math display">\[
\hat{f} ( x) =  \sum_{i=1}^N w_i y_i        \; \text{where}\; w_i := K( x_i , x ) /  \sum_{i=1}^N K( x_i ,x )   
\]</span></p>
<p><span class="math inline">\(k\)</span>-NN classifiers converge to error rates bounded relative to Bayes error rate, with relationships depending on class number. For binary classification, asymptotic <span class="math inline">\(k\)</span>-NN error rate is at most <span class="math inline">\(2R^*(1-R^*)\)</span> where <span class="math inline">\(R^*\)</span> is Bayes error rate. Cover and Hart proved interpolated <span class="math inline">\(k\)</span>-NN schemes are consistent estimators with performance improving as sample size increases.</p>
</section>
<section id="transformers-as-kernel-smoothing" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="transformers-as-kernel-smoothing"><span class="header-section-number">19.6</span> Transformers as Kernel Smoothing</h2>
<p><span class="citation" data-cites="bahdanau2014neural">Bahdanau, Cho, and Bengio (<a href="references.html#ref-bahdanau2014neural" role="doc-biblioref">2014</a>)</span> proposed kernel smoothing for sequence-to-sequence learning, estimating next-word probability using context vectors—weighted averages of input sequence vectors <span class="math inline">\(h_j\)</span>:</p>
<p><span class="math display">\[
c_i = \sum_{j=1}^n \alpha_{ij} h_j,
\]</span></p>
<p>where weights <span class="math inline">\(\alpha_{ij}\)</span> are defined by the kernel function:</p>
<p><span class="math display">\[
\alpha_{ij} = \dfrac{\exp\left( e_{ij}\right)}{\sum_{k=1}^n \exp\left( e_{ik}\right)}.
\]</span></p>
<p>Instead of using a traditional similarity measure like the 2-norm or inner product, the authors used a neural network to define the energy function <span class="math inline">\(e_{ij} = a(s_{i-1},h_j)\)</span>. This neural network measures the similarity between the last generated element of the output sequence <span class="math inline">\(s_{i-1}\)</span> and <span class="math inline">\(j\)</span>-th element of the input sequence <span class="math inline">\(h_j\)</span>. The resulting context vector is then used to predict the next word in the sequence.</p>
<section id="transformer" class="level3">
<h3 class="anchored" data-anchor-id="transformer">Transformer</h3>
<p>Transformers have become the dominant architecture for natural language processing, achieving state-of-the-art results across tasks from machine translation to language modeling and text generation. Originally designed to handle sequential data, the transformer architecture has since been extended to computer vision (Vision Transformers), protein structure prediction (AlphaFold), and speech recognition. Its success stems from a novel self-attention mechanism that efficiently captures long-range dependencies.</p>
<p>The idea to use kernel smoothing for sequence to sequence was called “attention”, or cross-attention, by <span class="citation" data-cites="bahdanau2014neural">Bahdanau, Cho, and Bengio (<a href="references.html#ref-bahdanau2014neural" role="doc-biblioref">2014</a>)</span>. When used for self-supervised learning, it is called self-attention. When a sequence is mapped to a matrix <span class="math inline">\(M\)</span>, it is called multi-head attention. The concept of self-attention and attention for natural language processing was further developed by <span class="citation" data-cites="vaswani2023attention">Vaswani et al. (<a href="references.html#ref-vaswani2023attention" role="doc-biblioref">2023</a>)</span> who developed a smoothing method that they called the transformer.</p>
<p>The transformer architecture revolves around a series of mathematical concepts and operations:</p>
<ul>
<li><strong>Embeddings</strong>: The input text is converted into vectors using embeddings. Each word (or token) is represented by a unique vector in a high-dimensional space.</li>
<li><strong>Positional Encoding</strong>: Since transformers do not have a sense of sequence order (like RNNs do), positional encodings are added to the embeddings to provide information about the position of each word in the sequence.</li>
<li><strong>Multi-Head Attention</strong>: The core of the transformer model. It enables the model to focus on different parts of the input sequence simultaneously. The attention mechanism is defined as: <span class="math display">\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]</span> where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are query, key, and value matrices respectively.</li>
<li><strong>Query (Q), Key (K), and Value (V) Vectors</strong>: These are derived from the input embeddings. They represent different aspects of the input.</li>
<li><strong>Scaled Dot-Product Attention</strong>: The attention mechanism calculates the dot product of the Query with all Keys, scales these values, and then applies a softmax function to determine the weights of the Values.</li>
<li><strong>Multiple ‘Heads’</strong>: The model does this in parallel multiple times (multi-head), allowing it to capture different features from different representation subspaces.</li>
<li><strong>Layer Normalization and Residual Connections</strong>: After each sub-layer in the encoder and decoder (like multi-head attention or the feed-forward layers), the transformer applies layer normalization and adds the output of the sub-layer to its input (residual connection). This helps in stabilizing the training of deep networks.</li>
<li><strong>Feed-Forward Neural Networks</strong>: Each layer in the transformer contains a fully connected feed-forward network applied to each position separately and identically. It is defined as: <span class="math display">\[ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 \]</span> where <span class="math inline">\(W_1\)</span>, <span class="math inline">\(W_2\)</span>, <span class="math inline">\(b_1\)</span>, and <span class="math inline">\(b_2\)</span> are learnable parameters.</li>
<li><strong>Output Linear Layer and Softmax</strong>: The decoder’s final output passes through a linear layer followed by a softmax layer. This layer converts the decoder output into predicted next-token probabilities.</li>
<li><strong>Training and Loss Function</strong>: Transformers are often trained using a variant of Cross-Entropy Loss to compare the predicted output with the actual output.</li>
<li><strong>Masking</strong>: In the decoder, to prevent future tokens from being used in the prediction, a technique called ‘masking’ is applied.</li>
<li><strong>Backpropagation and Optimization</strong>: The model’s parameters are adjusted through backpropagation and optimization algorithms like Adam.</li>
</ul>
<p>Later, <span class="citation" data-cites="lin2017structured">Lin et al. (<a href="references.html#ref-lin2017structured" role="doc-biblioref">2017</a>)</span> proposed using similar idea for self-supervised learning, where a sequence of words (sentence) is mapped to a single matrix:</p>
<p><span class="math display">\[
M = AH,
\]</span></p>
<p>where <span class="math inline">\(H\)</span> is the matrix representing an input sequence <span class="math inline">\(H = (h_1,\ldots,h_n)\)</span> and <span class="math inline">\(A\)</span> is the matrix of weights:</p>
<p><span class="math display">\[
A = \mathrm{softmax}\left(W_2\tanh\left(W_1H^T\right)\right).
\]</span></p>
<p>This allows to represent a sequence of words of any length <span class="math inline">\(n\)</span> using a “fixed size” <span class="math inline">\(r\times u\)</span> matrix <span class="math inline">\(M\)</span>, where <span class="math inline">\(u\)</span> is the dimension of a vector that represents an element of a sequence (word embedding) and <span class="math inline">\(r\)</span> is the hyper-parameter that defines the size of the matrix <span class="math inline">\(M\)</span>.</p>
<p>The primary advantage of transformers over recurrent architectures is their parallelizability—all positions in a sequence can be processed simultaneously rather than sequentially. Modern large language models such as BERT <span class="citation" data-cites="devlin2018bert">(<a href="references.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span>, GPT <span class="citation" data-cites="brown2020language">(<a href="references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>, and T5 <span class="citation" data-cites="raffel2020exploring">(<a href="references.html#ref-raffel2020exploring" role="doc-biblioref">Raffel et al. 2020</a>)</span> are built on transformer architectures. The transformer’s ability to capture long-range dependencies through attention, combined with its scalability to massive datasets and model sizes, has made it the foundation for the current generation of AI systems. See <span class="citation" data-cites="tsai2019transformer">Tsai et al. (<a href="references.html#ref-tsai2019transformer" role="doc-biblioref">2019</a>)</span> for a comprehensive survey of transformer applications beyond NLP.</p>
</section>
</section>
<section id="deep-learning-as-representation-learning" class="level2" data-number="19.7">
<h2 data-number="19.7" class="anchored" data-anchor-id="deep-learning-as-representation-learning"><span class="header-section-number">19.7</span> Deep Learning as Representation Learning</h2>
<p>A fundamental perspective on deep learning is that it automates the process of feature engineering. Formally, we seek to find a predictor <span class="math inline">\(\mathbf{Y} = f(\mathbf{X})\)</span>. We can decompose this function into two stages: a data transformation (representation learning) <span class="math inline">\(\phi(\mathbf{X})\)</span> and a predictive model <span class="math inline">\(g(\cdot)\)</span>.</p>
<p><span class="math display">\[
\mathbf{Y} \sim p(\mathbf{Y} \mid \mathbf{Z}), \quad \mathbf{Z} = \phi(\mathbf{X})
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{Z}\)</span> represents latent features. The transformation <span class="math inline">\(\phi(\cdot)\)</span> effectively performs dimensionality reduction (or expansion) to uncover a structure where the relationship between <span class="math inline">\(\mathbf{Z}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> is simpler (often linear).</p>
<section id="dimensionality-expansion-vs.-reduction" class="level3">
<h3 class="anchored" data-anchor-id="dimensionality-expansion-vs.-reduction">Dimensionality Expansion vs.&nbsp;Reduction</h3>
<p>Classically, statisticians approached complex functions through <strong>dimensionality expansion</strong> (basis expansion). Methods like <strong>Kernel Regression</strong> or <strong>Splines</strong> map the input <span class="math inline">\(\mathbf{X}\)</span> into a higher-dimensional space of features <span class="math inline">\(\phi(\mathbf{X}) = (\phi_1(\mathbf{X}), \ldots, \phi_M(\mathbf{X}))\)</span> (e.g., interactions, polynomials) and then fit a linear model. The “Kernel Trick” allows this to be done efficiently without explicit computation of features.</p>
<p>Deep learning, in contrast, often emphasizes <strong>dimensionality reduction</strong> (feature learning). It searches for a low-dimensional manifold embedded in the high-dimensional input space.</p>
</section>
<section id="linear-baselines-pca-and-pls" class="level3">
<h3 class="anchored" data-anchor-id="linear-baselines-pca-and-pls">Linear Baselines: PCA and PLS</h3>
<p>To understand the contribution of deep learning, it is useful to compare it with linear methods for Representation Learning:</p>
<ol type="1">
<li><strong>Principal Component Analysis (PCA)</strong>: An unsupervised method that finds orthogonal directions (eigenvectors of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>) maximizing variance. It creates features <span class="math inline">\(\mathbf{Z}_{PCA} = \mathbf{X}\mathbf{W}\)</span> without knowledge of the target <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li><strong>Partial Least Squares (PLS)</strong>: A supervised method that finds directions maximizing the covariance between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>. It effectively finds features that are both variable and predictive.</li>
</ol>
<p>Deep Neural Networks extrapolate this concept to the nonlinear regime. They can be viewed as performing <strong>Nonlinear PLS</strong>. The layers of a network apply successive semi-affine transformations:</p>
<p><span class="math display">\[
\mathbf{Z}^{(l)} = \sigma(W^{(l)}\mathbf{Z}^{(l-1)} + b^{(l)})
\]</span></p>
<p>This hierarchical stacking allows the network to learn progressively more abstract representations, discovering nonlinear factors that maximize predictive power, much like a nonlinear generalization of the supervised dimensionality reduction performed by PLS.</p>
</section>
<section id="uncertainty-quantification" class="level3">
<h3 class="anchored" data-anchor-id="uncertainty-quantification">Uncertainty Quantification</h3>
<p>We can extend this framework to quantify uncertainty. Our probabilistic model is <span class="math inline">\(\mathbf{Y} \mid \mathbf{Z} \sim p(\mathbf{Y} \mid \mathbf{Z})\)</span>, where <span class="math inline">\(\mathbf{Z} = g(\mathbf{X})\)</span> is the deep feature extraction. A key result by <span class="citation" data-cites="brillinger2012generalized">Brillinger (<a href="references.html#ref-brillinger2012generalized" role="doc-biblioref">2012</a>)</span> suggests that for certain input distributions (e.g., Gaussian), the central subspace estimated by methods like PLS or Deep Learning can be consistent.</p>
<p>By treating the top layer as a generalized linear model on learned features <span class="math inline">\(\mathbf{Z}\)</span>, we can leverage standard Bayesian techniques for the final prediction layer while relying on the deep network for feature discovery. This “Last Layer Bayesian” approach provides a practical path to uncertainty quantification in deep learning, enabling the calculation of predictive intervals: <span class="math display">\[
\mathbf{Y}_{\star} \sim \int p(\mathbf{Y} \mid \mathbf{Z}_{\star}, \theta) p(\theta \mid \mathcal{D}) d\theta
\]</span> where <span class="math inline">\(\theta\)</span> represents the parameters of the output layer.</p>
</section>
</section>
<section id="sec-double-descent" class="level2" data-number="19.8">
<h2 data-number="19.8" class="anchored" data-anchor-id="sec-double-descent"><span class="header-section-number">19.8</span> Double Descent</h2>
<p>Double descent is a phenomenon of over-parameterized statistical models. In this section, we present a view of double descent from a Bayesian perspective. Over-parameterized models such as deep neural networks have an interesting re-descending property in their risk characteristics. This is a recent phenomenon in machine learning and has been the subject of many studies. As the complexity of the model increases, there is a U-shaped region corresponding to the traditional bias-variance trade-off, but then as the number of parameters equals the number of observations and the model becomes one of interpolation, the risk can become infinite and then, in the over-parameterized region, it re-descends—the double descent effect. We show that this has a natural Bayesian interpretation. Moreover, we show that it is not in conflict with the traditional Occam’s razor that Bayesian models possess, in that they tend to prefer simpler models when possible.</p>
<p>Empirically, the double descent effect was initially observed for high-dimensional neural network regression models and the good performance of these models on such tasks as large language models, image processing, and generative AI methods<span class="citation" data-cites="nareklishvili2023generative">(<a href="references.html#ref-nareklishvili2023generative" role="doc-biblioref">Nareklishvili, Polson, and Sokolov 2023</a>)</span>. The double descent effect extends the classical bias-variance trade-off curve that shrinkage estimators possess. This phenomenon was first observed in the context of linear regression<span class="citation" data-cites="belkin2019reconciling">(<a href="references.html#ref-belkin2019reconciling" role="doc-biblioref">Belkin et al. 2019</a>)</span>. The authors showed that the test error of the estimator can decrease as the number of parameters increases. <span class="citation" data-cites="bach2024highdimensional">Bach (<a href="references.html#ref-bach2024highdimensional" role="doc-biblioref">2024</a>)</span> extends these results to stochastic regression models.</p>
<p>Interpolators—estimators that achieve zero training error—were then shown to have attractive properties due to the double descent effect<span class="citation" data-cites="hastie2022surprises">(<a href="references.html#ref-hastie2022surprises" role="doc-biblioref">Hastie et al. 2022</a>)</span>. Our goal is to show that Bayesian estimators can also possess a double descent phenomenon. Interpolators such as ReLU neural networks<span class="citation" data-cites="polson2017deep">(<a href="references.html#ref-polson2017deep" role="doc-biblioref">Polson, Sokolov, et al. 2017</a>)</span> have increased in popularity with many applications such as traffic flow modeling<span class="citation" data-cites="polson2017deep">(<a href="references.html#ref-polson2017deep" role="doc-biblioref">Polson, Sokolov, et al. 2017</a>)</span> and high-frequency trading<span class="citation" data-cites="dixon2019deep">(<a href="references.html#ref-dixon2019deep" role="doc-biblioref">Dixon, Polson, and Sokolov 2019</a>)</span>, among many others.</p>
<p>Occam’s razor—the favoring of simpler models over complex ones—is a natural feature of Bayesian methods that are based on the weight of evidence (a.k.a. the marginal likelihood of the data). To do this, they penalize models with higher complexity via a correction term as in the Bayesian Information Criterion (BIC). This seems inconsistent with the double descent phenomenon. We show that this is not the case, as even though Bayesian methods shift the posterior towards lower-complexity models, highly parameterized Bayesian models can also have good risk properties due to the conditional prior of parameters given the model. We illustrate this with an application to neural network models.</p>
<p>Double descent has been studied from a frequentist point of view in <span class="citation" data-cites="belkin2019reconciling">Belkin et al. (<a href="references.html#ref-belkin2019reconciling" role="doc-biblioref">2019</a>)</span>, <span class="citation" data-cites="bach2024highdimensional">Bach (<a href="references.html#ref-bach2024highdimensional" role="doc-biblioref">2024</a>)</span>. The phenomenon of double descent is illustrated in <a href="#fig-double-descent" class="quarto-xref">Figure&nbsp;<span>19.3</span></a>. The first part of the curve represents the classical U-shaped bias-variance trade-off. The second part demonstrates the double descent phenomenon, where the test error of the estimator can decrease as the model becomes over-parameterized beyond the interpolation threshold. This phenomenon was later observed in the context of deep learning<span class="citation" data-cites="nakkiran2021deep">(<a href="references.html#ref-nakkiran2021deep" role="doc-biblioref">Nakkiran et al. 2021</a>)</span>. The authors showed that the test error of the estimator can decrease as the number of parameters increases.</p>
<div id="fig-double-descent" class="quarto-float quarto-figure quarto-figure-center anchored" width="60%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-double-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/double-descent-stylized-dots.png" id="fig-double-descent" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-double-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.3
</figcaption>
</figure>
</div>
<div id="exm-double-descent" class="theorem example">
<p><span class="theorem-title"><strong>Example 19.1 (Double Descent Demonstration using Polynomial Regression)</strong></span> To illustrate the double descent phenomenon in a concrete setting, we present a detailed example using polynomial regression with Legendre basis functions. This example demonstrates how the test error can exhibit the characteristic U-shaped curve followed by a re-descent as model complexity increases far beyond the interpolation threshold.</p>
<p>Our demonstration uses a one-dimensional regression problem where we attempt to learn a sinusoidal function <span class="math inline">\(f(x) = \sin(5x)\)</span> from a small dataset of only <span class="math inline">\(n = 20\)</span> observations sampled from the interval <span class="math inline">\([-1, 1]\)</span>. We add Gaussian noise with standard deviation <span class="math inline">\(\sigma = 0.3\)</span> to simulate realistic measurement error. The choice of a small sample size is crucial for observing double descent, as it creates a regime where the number of model parameters can substantially exceed the number of observations.</p>
<p>We fit polynomial models of varying degrees <span class="math inline">\(d = 1, 2, \ldots, 50\)</span> using Legendre polynomial basis functions. Legendre polynomials provide a numerically stable orthogonal basis that helps avoid the numerical instabilities associated with standard monomial bases in high-degree polynomial fitting. For each degree <span class="math inline">\(d\)</span>, we estimate the coefficients using the Moore-Penrose pseudoinverse, which provides the minimum-norm solution when the system is overdetermined (i.e., when <span class="math inline">\(d &gt; n\)</span>).</p>
<p><a href="#fig-double-descent-grid" class="quarto-xref">Figure&nbsp;<span>19.4</span></a> illustrates how model behavior changes dramatically across different polynomial degrees. The four panels show representative cases that capture the key phases of the double descent phenomenon:</p>
<ul>
<li><p><strong>Degree 1 (Underparameterized)</strong>: The linear model is too simple to capture the oscillatory nature of the underlying sine function, resulting in high bias and poor fit to both training and test data.</p></li>
<li><p><strong>Degree 5 (Classical Optimum)</strong>: This represents the sweet spot of the classical bias-variance tradeoff, where the model has sufficient complexity to capture the main features of the sine function without overfitting severely.</p></li>
<li><p><strong>Degree 20 (Interpolation Threshold)</strong>: At this degree, the model has exactly as many parameters as training observations, enabling perfect interpolation of the training data. However, the resulting fit exhibits wild oscillations between data points, leading to poor generalization performance.</p></li>
<li><p><strong>Degree 50 (Over-parameterized)</strong>: Surprisingly, despite having far more parameters than observations, this highly over-parameterized model achieves better test performance than the interpolating model, demonstrating the double descent effect.</p></li>
</ul>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-double-descent-grid" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-double-descent-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="19-theorydl_files/figure-html/fig-double-descent-grid-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-double-descent-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.4: Double Descent Phenomenon: Polynomial Regression with Different Degrees
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, let’s plot the MSE curve. We will plot the test error (blue line) and the training error (red line) for different polynomial degrees from 1 to 50.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-mse-curve" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mse-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="19-theorydl_files/figure-html/fig-mse-curve-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mse-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.5: Bias-Variance Trade-off: Training and Test MSE vs Model Complexity
</figcaption>
</figure>
</div>
</div>
</div>
<p>The key insight from <a href="#fig-mse-curve" class="quarto-xref">Figure&nbsp;<span>19.5</span></a> is the characteristic double descent shape in the test error (blue line). The curve exhibits three distinct phases:</p>
<ol type="1">
<li><p><strong>Classical Regime</strong>: For low degrees (<span class="math inline">\(d &lt; 5\)</span>), increasing model complexity reduces both bias and test error, following the traditional understanding of the bias-variance tradeoff.</p></li>
<li><p><strong>Interpolation Crisis</strong>: Around the interpolation threshold (<span class="math inline">\(d \approx n = 20\)</span>), test error peaks dramatically as the model begins to perfectly fit the training data while generalizing poorly.</p></li>
<li><p><strong>Over-parameterized Regime</strong>: For very high degrees (<span class="math inline">\(d &gt; 30\)</span>), test error decreases again, demonstrating that extreme over-parameterization can lead to improved generalization despite the model’s ability to memorize the training data.</p></li>
</ol>
<p>This behavior challenges the conventional wisdom that more parameters necessarily lead to worse generalization. The double descent phenomenon arises from the implicit regularization effects of minimum-norm solutions in over-parameterized settings. When <span class="math inline">\(d &gt; n\)</span>, the pseudoinverse solution corresponds to the minimum <span class="math inline">\(\ell_2\)</span>-norm coefficients among all possible interpolating solutions. This implicit bias toward simpler functions can lead to surprisingly good generalization properties.</p>
<p>While this example uses polynomial regression for clarity, the double descent phenomenon has been observed across a wide range of modern machine learning models, including deep neural networks, random forests, and kernel methods. The implications for practice are significant. Given that model selection is time consuming and computationally expensive, this example shows, that instead of spending time to do model selection to find the “sweet spot” model with 5-degree polynomial, we just over-parametrise and get a good model for free!</p>
<p>This example serves as a concrete illustration of how classical statistical intuitions about model complexity may not apply in contemporary machine learning settings, particularly when dealing with over-parameterized models that have become increasingly common in practice.</p>
</div>
</section>
<section id="application" class="level2" data-number="19.9">
<h2 data-number="19.9" class="anchored" data-anchor-id="application"><span class="header-section-number">19.9</span> Application</h2>
<section id="simulated-data" class="level3">
<h3 class="anchored" data-anchor-id="simulated-data">Simulated Data</h3>
<p>We also apply the K-GAM architecture to a simulated dataset to evaluate its performance on data with known structure and relationships. The dataset contains 100 observations generated from the following function:</p>
<p><span class="math display">\[\begin{align*}
    &amp;y = \mu(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0,1)\\
         &amp;\mu(x) = 10\sin(\pi x_1 x_2) + 20(x_3-0.5)^2 + 10x_4 + 5x_5.
\end{align*}\]</span></p>
<p>The goal is to predict the function <span class="math inline">\(y(x)\)</span> based on the input <span class="math inline">\(x\)</span>. The dataset is often used as a benchmark dataset for regression algorithms due to its diverse mix of relationships (linear, quadratic, nonlinear, Gaussian random noise) between the input features and the target function.</p>
<p>We use the Koppen function to transform the five-dimensional input into a set of 11 features (<span class="math inline">\(2d+1\)</span>). We then learn the outer function <span class="math inline">\(g\)</span> using a ReLU network. To thoroughly investigate the model’s capabilities, we implement two distinct approaches to learning the outer function. The first approach uses different <span class="math inline">\(g\)</span> functions for each feature, following the original KST formulation. This allows each function to specialize in capturing specific patterns, but might be more difficult to train and has more parameters. The second approach uses a single <span class="math inline">\(g\)</span> function for all features, as proposed by <span class="citation" data-cites="lorentz197613th">Lorentz (<a href="references.html#ref-lorentz197613th" role="doc-biblioref">1976</a>)</span>, providing a more unified and parameter-efficient representation.</p>
<p>The first model with multiple <span class="math inline">\(g_i\)</span> functions has dimensions: <span class="math inline">\(W^0_i \in \mathbb{R}^{16\times 1}\)</span> and <span class="math inline">\(W^j_i \in \mathbb{R}^{16\times 16}\)</span> for <span class="math inline">\(j=1,\ldots,18\)</span>.</p>
<p>The second architecture using single function <span class="math inline">\(g\)</span> for all features maintains similar structure but increases inner layer width from 16 to 200. This increased capacity allows single functions to learn complex patterns, compensating for the constraint versus multiple specialized functions.</p>
</section>
<section id="training-rates" class="level3">
<h3 class="anchored" data-anchor-id="training-rates">Training Rates</h3>
<p>Consider nonparametric regression <span class="math inline">\(y_i= f (x_i) + \epsilon_i\)</span> where <span class="math inline">\(x_i = ( x_{1i} , \ldots , x_{di} )\)</span>. We estimate <span class="math inline">\(f( x_1 , \ldots , x_d )\)</span> for <span class="math inline">\(x  = ( x_1 , \ldots , x_d ) \in [0,1]^d\)</span>. From a classical risk perspective:</p>
<p><span class="math display">\[
R ( f , \hat{f}_N ) = E_{X,Y} \left ( \lVert  f - \hat{f}_N \rVert^2 \right ),
\]</span> where <span class="math inline">\(\lVert \cdot \rVert\)</span> denotes <span class="math inline">\(L^2(P_X)\)</span>-norm.</p>
<p>Under standard assumptions, optimal minimax rate <span class="math inline">\(\inf_{\hat{f}} \sup_f R( f , \hat{f}_N ) = O_p ( N^{- 2 \beta /( 2 \beta + d )} )\)</span> for <span class="math inline">\(\beta\)</span>-Holder smooth functions <span class="math inline">\(f\)</span>. This rate depends on dimension <span class="math inline">\(d\)</span>, problematic in high dimensions. Restricting function classes yields better rates independent of <span class="math inline">\(d\)</span>, avoiding the curse of dimensionality. Common approaches include linear superpositions (ridge functions) and projection pursuit models.</p>
<p>Another asymptotic result comes from posterior concentration. Here, <span class="math inline">\(\hat{f}_N\)</span> is a regularized MAP (maximum a posteriori) estimator solving:</p>
<p><span class="math display">\[
\hat{f}_N = \arg \min_{ \hat{f}_N } \frac{1}{N} \sum_{i=1}^N ( y_i - \hat{f}_N ( x_i ))^2 + \phi ( \hat{f}_N )
\]</span></p>
<p>where <span class="math inline">\(\phi(\hat{f})\)</span> is regularization. Under appropriate conditions, posterior distribution <span class="math inline">\(\Pi(f | x, y)\)</span> concentrates around true function at minimax rate (up to <span class="math inline">\(\log N\)</span> factor).</p>
<p>A key result in the deep learning literature provides convergence rates for deep neural networks. Given a training dataset of input-output pairs <span class="math inline">\(( x_i , y_i)_{i=1}^N\)</span> from the model <span class="math inline">\(y = f(x) + \epsilon\)</span> where <span class="math inline">\(f\)</span> is a deep learner (i.e.&nbsp;superposition of functions)</p>
<p><span class="math display">\[
f = g_L \circ \ldots g_1 \circ g_0
\]</span></p>
<p>where each <span class="math inline">\(g_i\)</span> is a <span class="math inline">\(\beta_i\)</span>-smooth Hölder function with <span class="math inline">\(d_i\)</span> variables, that is <span class="math inline">\(|g_i(x) - g_i(y)| &lt; |x-y|^{\beta_i}\)</span>.</p>
<p>Then, the estimator has optimal rate:</p>
<p><span class="math display">\[
O \left ( \max_{1\leq i \leq L } N^{- 2 \beta^* /( 2 \beta^* + d_i ) } \right )  \; \text{where}\; \beta_i^* = \beta_i \prod_{l = i+1}^L \min ( \beta_l , 1 )
\]</span></p>
<p>This result can be applied to various function classes, including generalized additive models of the form</p>
<p><span class="math display">\[
f_0 ( x ) = h \left ( \sum_{p=1}^d f_{0,p} (x_p) \right )
\]</span></p>
<p>where <span class="math inline">\(g_0(z) = h(z)\)</span>, <span class="math inline">\(g_1 ( x_1 , \ldots , x_d ) = ( f_{01}(x_1) , \ldots , f_{0d} (x_d) )\)</span> and <span class="math inline">\(g_2 ( y_1 , \ldots , y_d ) = \sum_{i=1}^d y_i\)</span>. In this case, <span class="math inline">\(d_1 = d_2 = 1\)</span>, and assuming <span class="math inline">\(h\)</span> is Lipschitz, we get an optimal rate of <span class="math inline">\(O(N^{-1/3})\)</span>, which is independent of <span class="math inline">\(d\)</span>.</p>
<p><span class="citation" data-cites="schmidt-hieber2021kolmogorov">Schmidt-Hieber (<a href="references.html#ref-schmidt-hieber2021kolmogorov" role="doc-biblioref">2021</a>)</span> show that deep ReLU networks also have optimal rate of <span class="math inline">\(O( N^{-1/3} )\)</span> for certain function classes. For <span class="math inline">\(3\)</span>-times differentiable (e.g.&nbsp;cubic B-splines ), <span class="citation" data-cites="coppejans2004kolmogorovs">Coppejans (<a href="references.html#ref-coppejans2004kolmogorovs" role="doc-biblioref">2004</a>)</span> finds a rate of <span class="math inline">\(O( N^{-3/7} ) = O( N^{-3/(2 \times 3 + 1) } )\)</span>. <span class="citation" data-cites="igelnik2003kolmogorovs">Igelnik and Parikh (<a href="references.html#ref-igelnik2003kolmogorovs" role="doc-biblioref">2003</a>)</span> finds a rate <span class="math inline">\(O( N^{-1} )\)</span> for Kolmogorov Spline Networks.</p>
<p>Finally, it’s worth noting the relationship between expected risk and empirical risk. The expected risk, <span class="math inline">\(R\)</span>, is typically bounded by the empirical risk plus a term of order <span class="math inline">\(1/\sqrt{N}\)</span>:</p>
<p><span class="math display">\[
R(y, f^\star) \leq \frac{1}{N} \sum_{i=1}^N R(y_i, f^\star(x_i)) + O\left(\frac{\|f\|}{\sqrt{N}}\right)
\]</span></p>
<p>where <span class="math inline">\(f^\star\)</span> is the minimizer of the expected risk. However, in the case of interpolation, where the model perfectly fits the training data, the empirical risk term becomes zero, leaving only the <span class="math inline">\(O(1/\sqrt{N})\)</span> term.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="19.10">
<h2 data-number="19.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">19.10</span> Conclusion</h2>
<p>This chapter has traced the theoretical foundations of deep learning through the lens of multivariate function approximation. Several key insights emerge from this analysis:</p>
<p><em>Mathematical Foundations</em>: The Kolmogorov Superposition Theorem provides a rigorous theoretical basis for understanding why neural networks can approximate any continuous function. The theorem shows that any multivariate function can be decomposed into compositions of univariate functions—a principle that directly informs the layered architecture of modern neural networks.</p>
<p><em>Representation vs.&nbsp;Computation</em>: While KST guarantees the existence of such representations, the inner functions (like the Koppen function) are typically non-smooth and computationally challenging. This highlights a fundamental tension: mathematically elegant representations may not always translate to efficient computation.</p>
<p><em>Connections Across Methods</em>: We have seen deep connections between seemingly disparate approaches. Transformers can be understood as kernel smoothing methods. Deep networks perform sophisticated space partitioning similar to decision trees. The K-GAM framework reveals that deep learners are essentially generalized additive models with automatically learned features.</p>
<p><em>Double Descent and Over-parameterization</em>: Perhaps most surprisingly, the double descent phenomenon suggests that classical intuitions about the bias-variance trade-off may not apply to modern over-parameterized models. Networks with more parameters than training examples can still generalize well, challenging conventional wisdom about model complexity.</p>
<p>These theoretical insights not only deepen our understanding of why deep learning works but also suggest new directions for developing more efficient and interpretable architectures. The ongoing research into Kolmogorov-Arnold Networks and related approaches hints at the possibility of achieving the representational power of deep learning with potentially simpler, more interpretable structures.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-actor2018computation" class="csl-entry" role="listitem">
Actor, Jonas. 2018. <span>“Computation for the <span>Kolmogorov Superposition Theorem</span>.”</span> {{MS Thesis}}, Rice.
</div>
<div id="ref-amit2000multiple" class="csl-entry" role="listitem">
Amit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. <span>“Multiple Randomized Classifiers: <span>MRCL</span>.”</span>
</div>
<div id="ref-bach2024highdimensional" class="csl-entry" role="listitem">
Bach, Francis. 2024. <span>“High-Dimensional Analysis of Double Descent for Linear Regression with Random Projections.”</span> <em>SIAM Journal on Mathematics of Data Science</em> 6 (1): 26–50.
</div>
<div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural <span>Machine Translation</span> by <span>Jointly Learning</span> to <span>Align</span> and <span>Translate</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-belkin2019reconciling" class="csl-entry" role="listitem">
Belkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. <span>“Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.”</span> <em>Proceedings of the National Academy of Sciences</em> 116 (32): 15849–54.
</div>
<div id="ref-braun2009constructive" class="csl-entry" role="listitem">
Braun, Heinrich, and Martin Riedmiller. 2009. <span>“Constructive Neural Network Learning Algorithms for Pattern Classification.”</span> <em>IEEE Transactions on Neural Networks</em> 20 (1): 84–97.
</div>
<div id="ref-brillinger2012generalized" class="csl-entry" role="listitem">
Brillinger, David R. 2012. <span>“A <span>Generalized Linear Model With</span> <span>‘<span>Gaussian</span>’</span> <span>Regressor Variables</span>.”</span> In <em>Selected <span>Works</span> of <span>David Brillinger</span></em>, edited by Peter Guttorp and David Brillinger, 589–606. Selected <span>Works</span> in <span>Probability</span> and <span>Statistics</span>. New York, NY: Springer.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language <span>Models</span> Are <span>Few-Shot Learners</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-coppejans2004kolmogorovs" class="csl-entry" role="listitem">
Coppejans, Mark. 2004. <span>“On <span>Kolmogorov</span>’s Representation of Functions of Several Variables by Functions of One Variable.”</span> <em>Journal of Econometrics</em> 123 (1): 1–31.
</div>
<div id="ref-dembo2021note" class="csl-entry" role="listitem">
Dembo, Amir. 2021. <span>“A Note on the Universal Approximation Capability of Deep Neural Networks.”</span> <em>arXiv Preprint arXiv:2104.xxxxx</em>.
</div>
<div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> In <em>Proceedings of the 2019 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language Technologies</span>, <span>Volume</span> 1 (<span>Long</span> and <span>Short Papers</span>)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics.
</div>
<div id="ref-diaconis1984nonlinear" class="csl-entry" role="listitem">
Diaconis, Persi, and Mehrdad Shahshahani. 1984. <span>“On Nonlinear Functions of Linear Combinations.”</span> <em>SIAM Journal on Scientific and Statistical Computing</em> 5 (1): 175–91.
</div>
<div id="ref-dixon2019deep" class="csl-entry" role="listitem">
Dixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. <span>“Deep Learning for Spatio-Temporal Modeling: <span>Dynamic</span> Traffic Flows and High Frequency Trading.”</span> <em>Applied Stochastic Models in Business and Industry</em> 35 (3): 788–807.
</div>
<div id="ref-fredholm1903classe" class="csl-entry" role="listitem">
Fredholm, Ivar. 1903. <span>“Sur Une Classe d’équations Fonctionnelles.”</span> <em>Acta Mathematica</em> 27 (none): 365–90.
</div>
<div id="ref-friedman1981projection" class="csl-entry" role="listitem">
Friedman, Jerome H., and Werner Stuetzle. 1981. <span>“Projection <span>Pursuit Regression</span>.”</span> <em>Journal of the American Statistical Association</em> 76 (376): 817–23.
</div>
<div id="ref-hastie2022surprises" class="csl-entry" role="listitem">
Hastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. 2022. <span>“Surprises in High-Dimensional Ridgeless Least Squares Interpolation.”</span> <em>The Annals of Statistics</em> 50 (2): 949–86.
</div>
<div id="ref-huber1985projection" class="csl-entry" role="listitem">
Huber, Peter J. 1985. <span>“Projection <span>Pursuit</span>.”</span> <em>The Annals of Statistics</em> 13 (2): 435–75.
</div>
<div id="ref-igelnik2003kolmogorovs" class="csl-entry" role="listitem">
Igelnik, B., and N. Parikh. 2003. <span>“Kolmogorov’s Spline Network.”</span> <em>IEEE Transactions on Neural Networks</em> 14 (4): 725–33.
</div>
<div id="ref-kaczmarz1937angenaherte" class="csl-entry" role="listitem">
Kaczmarz, Stefan. 1937. <span>“Angenäherte Auflösung von Systemen Linearer Gleichungen.”</span> <em>Bulletin International de l’Académie Polonaise Des Sciences Et Des Lettres</em> 35: 355–57.
</div>
<div id="ref-kolmogorov1956representation" class="csl-entry" role="listitem">
Kolmogorov, AN. 1956. <span>“On the Representation of Continuous Functions of Several Variables as Superpositions of Functions of Smaller Number of Variables.”</span> In <em>Soviet. <span>Math</span>. <span>Dokl</span></em>, 108:179–82.
</div>
<div id="ref-koppen2000curse" class="csl-entry" role="listitem">
Köppen, Mario. 2000. <span>“The Curse of Dimensionality.”</span> <em>5th Online World Conference on Soft Computing in Industrial Applications (WSC5)</em> 1: 4–8.
</div>
<div id="ref-lin2017structured" class="csl-entry" role="listitem">
Lin, Zhouhan, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. <span>“A <span class="nocase">Structured Self-attentive Sentence Embedding</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1703.03130">https://arxiv.org/abs/1703.03130</a>.
</div>
<div id="ref-lorentz197613th" class="csl-entry" role="listitem">
Lorentz, George G. 1976. <span>“The 13th Problem of <span>Hilbert</span>.”</span> In <em>Proceedings of <span>Symposia</span> in <span>Pure Mathematics</span></em>, 28:419–30. American Mathematical Society.
</div>
<div id="ref-nadaraya1964estimating" class="csl-entry" role="listitem">
Nadaraya, E. A. 1964. <span>“On <span>Estimating Regression</span>.”</span> <em>Theory of Probability &amp; Its Applications</em> 9 (1): 141–42.
</div>
<div id="ref-nakkiran2021deep" class="csl-entry" role="listitem">
Nakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2021. <span>“Deep Double Descent: <span>Where</span> Bigger Models and More Data Hurt*.”</span> <em>Journal of Statistical Mechanics: Theory and Experiment</em> 2021 (12): 124003.
</div>
<div id="ref-nareklishvili2023generative" class="csl-entry" role="listitem">
Nareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2023. <span>“Generative <span>Causal Inference</span>,”</span> June. <a href="https://arxiv.org/abs/2306.16096">https://arxiv.org/abs/2306.16096</a>.
</div>
<div id="ref-polson2017deep" class="csl-entry" role="listitem">
Polson, Nicholas G, Vadim Sokolov, et al. 2017. <span>“Deep <span>Learning</span>: <span>A Bayesian Perspective</span>.”</span> <em>Bayesian Analysis</em> 12 (4): 1275–1304.
</div>
<div id="ref-raffel2020exploring" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67.
</div>
<div id="ref-schmidt-hieber2021kolmogorov" class="csl-entry" role="listitem">
Schmidt-Hieber, Johannes. 2021. <span>“The <span>Kolmogorov</span>–<span>Arnold</span> Representation Theorem Revisited.”</span> <em>Neural Networks</em> 137 (May): 119–26.
</div>
<div id="ref-sprecher1965structure" class="csl-entry" role="listitem">
Sprecher, David A. 1965. <span>“On the Structure of Continuous Functions of Several Variables.”</span> <em>Transactions of the American Mathematical Society</em> 115: 340–55.
</div>
<div id="ref-tsai2019transformer" class="csl-entry" role="listitem">
Tsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. <span>“Transformer <span>Dissection</span>: <span>A Unified Understanding</span> of <span>Transformer</span>’s <span>Attention</span> via the <span>Lens</span> of <span>Kernel</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1908.11775">https://arxiv.org/abs/1908.11775</a>.
</div>
<div id="ref-vaswani2023attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
<div id="ref-watson1964smooth" class="csl-entry" role="listitem">
Watson, Geoffrey S. 1964. <span>“Smooth <span>Regression Analysis</span>.”</span> <em>Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)</em> 26 (4): 359–72.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./18-nn.html" class="pagination-link" aria-label="Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./20-sgd.html" class="pagination-link" aria-label="Gradient Descent">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>