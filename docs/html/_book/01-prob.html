<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-bayes.html" rel="next">
<link href="./00-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8f57c241cdbc1f937d718a8870719880.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification: Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Deep Learners</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Robotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><em>“It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge…”</em> Pierre Simon Laplace</p>
</blockquote>
<p>Probability deals with randomness. The art of data science is being able to “separate” signal from noise. For example, we need to account for randomness in human behavior. A random phenomena, by its very nature, means a precise prediction of an outcome has to be described by a distribution. Surprisingly random events typically have statistical regularity in many ways. For example, if we flip a coin, it would be hard to predict the outcome (head or tail) on an individual flip, but if we flip a coin many times and count the proportion of heads, the average will converge to something close to <span class="math inline">\(1/2\)</span>. This is called the law of large numbers.</p>
<p>Probability is a language that lets you communicate information about uncertain outcomes and events. By assigning a numeric value between zero and one to an event, or a collection of outcomes, in its simplest form, probability measures how likely an event is to occur.</p>
<p>Our goal here is to introduce you to the concepts of probability, conditional probability and their governing rules. The crowning being bayes rule for updating conditional probabilities. Understanding these concepts serves as a basis for more complex data analysis and machine learning algorithms. Building probabilistic models has many challenges and real world application. You are about to learn about practical examples from fields as diverse as medical diagnosis, chess games to racetrack odds.</p>
<p>We start by defining probabilities of a finite number of events. An axiomatic approach was proposed by Kolmogorov. This approach is very powerful and allows us to derive many important results and rules for calculating probabilities. Furthermore, in this chapter, we will discuss the notion of conditional probability and independence as well as tools for summarizing the distribution of a random variable, namely expectation and variance.</p>
<p>The study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same (statistical regularity). The first rigorous treatment of probability was presented by Jakob Bernoulli in his paper “Ars Conjectandi” (art of guesses) where he claims that to makes a guess is the same thing as to measure a probability.</p>
<section id="bernoullis-problem" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="bernoullis-problem"><span class="header-section-number">1.1</span> Bernoulli’s Problem</h2>
<p>Bernoulli considered the following problem. Suppose that we observe <span class="math inline">\(m\)</span> successes and <span class="math inline">\(n\)</span> failures of an event <span class="math inline">\(A\)</span>, out of total <span class="math inline">\(N=m+n\)</span> trials. How do we assign a probability <span class="math inline">\(P(A)\)</span> to the event <span class="math inline">\(A\)</span>? A classic definition of the probability (due to Jakob Bernoulli) is the ratio of number of favorable outcomes <span class="math inline">\(m\)</span> to the total number of outcomes <span class="math inline">\(N\)</span>, which is the sum of <span class="math inline">\(m\)</span> and the number of unfavorable outcomes <span class="math inline">\(n\)</span> <span class="math display">\[
P = \dfrac{m}{m+n} = \dfrac{m}{N}.
\]</span></p>
<p>Moreover, can we can we construct a law of succession? What is the probability that the next trial is to be success, given that there are uncertainties in the underlying probabilities. <span class="citation" data-cites="keynes1921treatise">Keynes (<a href="#ref-keynes1921treatise" role="doc-biblioref">1921</a>)</span> considered the rule of succession a.k.a. induction. For example, Bernoulli proposed that <span class="math display">\[
P_{N+1} = \dfrac{m+1}{N+2}.
\]</span> <span class="citation" data-cites="keynes1921treatise">Keynes (<a href="#ref-keynes1921treatise" role="doc-biblioref">1921</a>)</span> (p.&nbsp;371) provided a fully Bayesian model based on what we know today as Beta-Binomial model. <a href="#sec-bl" class="quarto-xref"><span class="quarto-unresolved-ref">sec-bl</span></a> provides a full analysis. The determination of the predictive rule is equivalent to the problem of finding a sufficient statistics (a.k.a. summary statistic) and performing feature engineering in modern day artificial intelligence applications.</p>
<p>de Finetti puts this in the framework of exchangeable random variables, see <span class="citation" data-cites="kreps1988notes">Kreps (<a href="#ref-kreps1988notes" role="doc-biblioref">1988</a>)</span> for further discussion. Jeffreys provides an alternative approach based on the principle of indifference. <span class="math display">\[
P_{N+1} = \dfrac{m+1/2}{N+1}.
\]</span> Ramsey (1926) and de Finetti (1937) and Savage (1956) use a purely axiomatic approach in an effort to operationalize probability. In a famous quote de Finetti says “the probability does not exist”. In this framework, probability is subjective and operationalize as a willingness to bet. If a gambit <span class="math inline">\(A\)</span> pays $1 if it happens and $0 otherwise, then the willingness to bet 50 cents to enter the gamble implies the subjective probability of <span class="math inline">\(A\)</span> is 0.5. Contrary to the frequentist approach, the probability is not a property of the event, but a property of the person. This is the basis of the Bayesian approach to probability.</p>
<p>Leonard Jimmie Savage, an American statistician, developed a decision theory framework known as the “Savage axioms” or the “Sure-Thing Principle.” This framework is a set of axioms that describe how a rational decision-maker should behave in the face of uncertainty. These axioms provide a foundation for subjective expected utility theory.</p>
<p>The Savage axioms consist of three main principles:</p>
<ol type="1">
<li><strong>Completeness Axiom:</strong>
<ul>
<li>This axiom assumes that a decision-maker can compare and rank all possible outcomes or acts in terms of preferences. In other words, for any two acts (or lotteries), the decision-maker can express a preference for one over the other, or consider them equally preferable.</li>
</ul></li>
<li><strong>Transitivity Axiom:</strong>
<ul>
<li>This axiom states that if a decision-maker prefers act A to act B and prefers act B to act C, then they must also prefer act A to act C. It ensures that the preferences are consistent and do not lead to cycles or contradictions.</li>
</ul></li>
<li><strong>Continuity Axiom (or Archimedean Axiom):</strong>
<ul>
<li>The continuity axiom introduces the concept of continuity in preferences. It implies that if a decision-maker prefers act A to act B, and B to C, then there exists some probability at which the decision-maker is indifferent between A and some lottery that combines B and C. This axiom helps to ensure that preferences are not too “discontinuous” or erratic.</li>
</ul></li>
</ol>
<p>Savage’s axioms provide a basis for the development of subjective expected utility theory. In this theory, decision-makers are assumed to assign subjective probabilities to different outcomes and evaluate acts based on the expected utility, which is a combination of the utility of outcomes and the subjective probabilities assigned to those outcomes.</p>
<p>Savage’s framework has been influential in shaping the understanding of decision-making under uncertainty. It allows for a more flexible approach to decision theory that accommodates subjective beliefs and preferences. However, it’s worth noting that different decision theorists may have alternative frameworks, and there are ongoing debates about the appropriateness of various assumptions in modeling decision-making.</p>
<p>Frequency probability is based on the idea that the probability of an event can be found by repeating the experiment many times and probability arises from from some random process on the sample space (such as random selection). For example, if we toss a coin many times, the probability of getting a head is the number of heads divided by the total number of tosses. This is the basis of the frequentist approach to probability.</p>
<p>Another way, sometimes more convenient, to talk about uncertainty and to express probabilities is odds, such as 9 to 2 or 3 to 1. We assign odds “on <span class="math inline">\(A\)</span>” or “against <span class="math inline">\(A\)</span>’’. For example, when we say that the odds on a Chicago Bear’s Super Bowl win are 2 to 9, it means that if they are to play 11 times (9+2), they will win 2 times. If <span class="math inline">\(A\)</span> is the win event, then odds on <span class="math inline">\(A\)</span> <span class="math display">\[
O(A) = \dfrac{P(A)}{P(\mbox{not A}) }
\]</span> Equivalently, probabilities can be determined from odds <span class="math display">\[
P(A) = \dfrac{1}{1+O(A)}
\]</span> For example if the odds are one, then <span class="math inline">\(O(A) = 1\)</span> and for every $1 bet you will payout $1. This event has probability <span class="math inline">\(0.5\)</span></p>
<p>If <span class="math inline">\(O(A) = 2\)</span>, then you are willing to offer <span class="math inline">\(2:1\)</span>. For a $1 bet you’ll payback $3. In terms of probability <span class="math inline">\(P = 1/3\)</span>.</p>
<p>Odds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.</p>
<div id="exm-odds" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Odds)</strong></span> One of the main sources of prediction markets are bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. <a href="#fig-odds" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-odds</span></a> shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hilary Clinton would win Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.</p>
<div id="fig-odds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig//hilary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Presidential Odds 2016
</figcaption>
</figure>
</div>
<p>Ahead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hilary Clinton. The best odds on Clinton are <span class="math inline">\(1/3\)</span>, this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides , otherwise they are risking to stay out of business.</p>
</div>
<div id="exm-derby" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Kentucky Derby)</strong></span> The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is <span class="math inline">\(1 \frac{1}{4}\)</span> (2 kilometers) race and is the first time the three-year old horses have raced the distance.</p>
<p>There was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have one many times in a row. The market is getting better at predicting whose going to win. Here’s the data</p>
<ol type="1">
<li>Spectacular Bid 1979 (with odds 0.6/1)</li>
<li>Fusaichi Pegasus 2000 (with odds 2.3/1)</li>
<li>Street Sense 2007 (with odds 9/2)</li>
<li>Big Brown 2008 (with odds 5/2)</li>
</ol>
<p>Recently, favorites have had a lot more success</p>
<ol type="1">
<li>California Chrome 2014 (with odds 5/2)</li>
<li>American Pharoah 2015 (with odds 2/1)</li>
<li>Nyqvist 2016 (with odds 3.3/1)</li>
<li>Always Dreaming 2017 (with odds 5.2/1)</li>
</ol>
<p>The most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only one other horse that in 2005 has broken two minutes at odds 11.5/1.</p>
</div>
<div id="exm-odds2" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Boy-Girl Paradox)</strong></span> If a woman has two children and one is a girl, the chance that the other child is also female has to be <span class="math inline">\(50-50\)</span>, right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, and I’m still not sure I believe this, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are <span class="math display">\[
BB \; \; BG \; \; GB \; \; GG
\]</span> Conditional on the information that one is a girl means that you know we can’t have the <span class="math inline">\(BB\)</span> scenario. Hence we are left with three possibilities <span class="math display">\[
BG \; \; GB \; \; GG
\]</span> In one <span class="math inline">\(1\)</span> of these is the other a girl. Hence <span class="math inline">\(1/3\)</span>.</p>
<p>It’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is <span class="math inline">\(1/2\)</span> as there are two possibilities <span class="math display">\[
GB \; \; GG
\]</span> This leads to the probability of <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-galton" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Galton Paradox)</strong></span> You flip three fair coins. What is the <span class="math inline">\(P(\text{all} \; \text{alike})\)</span>?</p>
<p>Assuming a fair coin (i.e.&nbsp;<span class="math inline">\(p(H) = p(T) = 1/2\)</span>), a formal approach might consist of computing the probability for all heads or all tails, which is <span class="math display">\[\begin{align*}
p(HHH) &amp;\equiv p(H \text{ and } H \text{ and } H) \\
&amp;= p(H)\times p(H)\times p(H) \\
&amp;= \left(\frac{1}{2}\right)^3
\end{align*}\]</span> and, since we’re ultimately interested in the probability of either (mutually exclusive) case, <span class="math display">\[\begin{align*}
P(\text{all alike}) &amp;= P(HHH \text{ or } TTT) \\
&amp;= P(HHH) + P(TTT) \\
&amp;= 2 \times \frac{1}{8}
\end{align*}\]</span></p>
<p>One could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, <span class="math display">\[
p(\text{all alike}) = 1/2
\]</span> The fault lies somewhere within the terms the and/or “remaining/last” and their connotation. A faulty symmetry assumption is being made in that statement pertaining to the distribution of the “remaining/last” coin. Loosely put, you’re certain to ultimately be in the case where at least two are alike, as stated in the above argument, but within each case the probability of landing the “remaining/last” matching <span class="math inline">\(H\)</span> or <span class="math inline">\(T\)</span> is not <span class="math inline">\(1/2\)</span>, due to the variety of ways you can arrive at two matching coins.</p>
<p>For a real treatment of the subject, we highly recommend reading Galton’s essay at <a href="http://galton.org/essays/1890-1899/galton-1894-chances.pdf">galton.org</a>.</p>
</div>
<div id="exm-odds3" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Three Cards)</strong></span> Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that its red. What’s the probability the other side is red? <span class="math inline">\(1/2\)</span>? No, its <span class="math inline">\(2/3\)</span>! By a similar logic there are six initial possibilities <span class="math display">\[
B_1 B_2 \; \; B_2 B_1 \; \; B R \; \; R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> where <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> index the sides of the same colored cards.</p>
<p>If we now condition on the top side being red we see that there are still three possibilities left <span class="math display">\[
R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> Hence the probability is <span class="math inline">\(2/3\)</span> and not the intuitive <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-Patriots" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (New England Patriots)</strong></span> Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The New England Patriots won 19 out of 25 coin tosses in 2014-15 season. What is the probability of this happening?</p>
<p>Let <span class="math inline">\(X\)</span> be a random variable equal to <span class="math inline">\(1\)</span> if the Patriots win and <span class="math inline">\(0\)</span> otherwise. It’s reasonable to assume <span class="math inline">\(P(X = 1) = \frac{1}{2}\)</span>. The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is <span class="math inline">\((1/2)^{25}\)</span>. We can code a typical sequence as, <span class="math display">\[
1,1,1,\ldots,1,0,0,\ldots,0.
\]</span> There are <span class="math inline">\(177,100\)</span> different sequences of 25 games where the Patriots win 19. There are <span class="math inline">\(25! = 1\cdot 2\cdot \ldots \cdot 25\)</span> ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are <span class="math inline">\(19!\)</span> ways to re-arrange the ones and <span class="math inline">\(6!\)</span> ways to rearrange the sequence on zeroes. Thus, the total number of of different winning sequences is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">factorial</span>(<span class="dv">25</span>)<span class="sc">/</span>(<span class="fu">factorial</span>(<span class="dv">19</span>)<span class="sc">*</span><span class="fu">factorial</span>(<span class="dv">25-19</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 177100</code></pre>
</div>
</div>
<p>Each potential sequence has probability <span class="math inline">\(0.5^{25}\)</span>, thus<br>
<span class="math display">\[
P\left(\text{Patriots win 19 out 25 tosses}\right) =  177,100 \times 0.5^{25} = 0.005
\]</span></p>
<p>Often, it is easier to communicate uncertainties in a form of odds. In terms of betting odds of <span class="math inline">\(1:1\)</span> gives <span class="math inline">\(P = \frac{1}{2}\)</span>, odds on <span class="math inline">\(2:1\)</span> (I give <span class="math inline">\(2\)</span> for each <span class="math inline">\(1\)</span> you bet) is <span class="math inline">\(P = \frac{1}{3}\)</span>.</p>
<p>Remember, odds, <span class="math inline">\(O(A)\)</span>, is the ratio of the probability of not happening over happening, <span class="math display">\[
O(A) = (1 - P(A)) / P(A),
\]</span> equivalently, <span class="math display">\[
P(A) = \frac{1}{1 + O(A)}.
\]</span></p>
<p>The odds of patriot winning sequence in then 1 to 199</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="fl">-0.005</span>)<span class="sc">/</span><span class="fl">0.005</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 199</code></pre>
</div>
</div>
</div>
<div id="exm-Pete" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (Hitting Streak)</strong></span> Pete Rose of the Cincinnati Reds set a National League record of hitting safely in <span class="math inline">\(44\)</span> consecutive games. How likely a such a long sequence of safe hits is to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once at every of those 44 games. Here are a couple of facts we know about him:</p>
<ol type="1">
<li>Rose was a <span class="math inline">\(300\)</span> hitter, he hits safely 3 times out of 10 attempts</li>
<li>Each at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.</li>
</ol>
<p>Assuming he comes to bat <span class="math inline">\(4\)</span> times each game, <em>what probability might reasonably be associated with that hitting streak?</em> First we define notation. We use <span class="math inline">\(A_i\)</span> to denote an event of hitting safely at game <span class="math inline">\(i\)</span>, then <span class="math display">\[
\begin{aligned}
&amp; P( \mathrm{Rose \; Hits \; Safely \; in \;44 \; consecutive \; games} ) = \\
&amp; P ( A_1 \; \text{and} \;  A_2  \ldots \text{and} \;  A_{44} ) = P ( A_1 ) P ( A_2 ) \ldots P ( A_{44} )
\end{aligned}
\]</span> We now need to find <span class="math inline">\(P(A_i)\text{s}\)</span> where <span class="math inline">\(P (A_i ) = 1 - P ( \text{not} \; A_i )\)</span> <span class="math display">\[\begin{align*}
P ( A_1 ) &amp; = 1 - P ( \mathrm{ not} \; A_1 ) \\
&amp; = 1 - P ( \mathrm{ Rose \; makes \; 4 \; outs } ) \\
&amp; = 1 - ( 0.7)^4 = 0.76
\end{align*}\]</span> For the winning streak, then we have <span class="math inline">\((0.76)^{44} = 0.0000057\)</span>, a very low probability. In terms of odds, there are three basic inferences</p>
<ol type="1">
<li>This means that the odds for a particular player as good as Pete Rose starting a hitting streak today are <span class="math inline">\(175,470\)</span> to <span class="math inline">\(1\)</span>.</li>
<li>This doesn’t mean that the run of <span class="math inline">\(44\)</span> won’t be beaten by some player at some time: the Law of Very Large Numbers</li>
<li>Joe DiMaggio’s record is 56. He is a 325 better, thus we have <span class="math inline">\((0.792)^{56} = 2.13 \times 10^{-6}\)</span> or 455,962 to 1. It’s going to be hard to beat.</li>
</ol>
</div>
<div id="exm-Jeter" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 (Derek Jeter)</strong></span> Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and a finer subset selection can change your average effects. Drug trials. Care with selection bias.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>1995</th>
<th></th>
<th>1996</th>
<th></th>
<th>Combined</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Derek Jeter</td>
<td>12/48</td>
<td>0.250</td>
<td>183/582</td>
<td>0.314</td>
<td>195/650</td>
<td>0.310</td>
</tr>
<tr class="even">
<td>David Justice</td>
<td>104/411</td>
<td>0.253</td>
<td>454/140</td>
<td>0.321</td>
<td>149/551</td>
<td>0.270</td>
</tr>
</tbody>
</table>
<p>This situation is known as <em>confounding</em>. If occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if <span class="math inline">\(A,B,C\)</span> are events it is possible to have the three inequalities <span class="math display">\[\begin{align*}
&amp;P( A \mid B \text{ and } C ) &gt; P( A \mid B \text{ and } \bar C )\\
&amp;P( A \mid \bar  B \text{ and } C ) &gt; P( A \mid \bar  B \text{ and } \bar  C )\\
&amp;P( A \mid \text{ and } C ) &gt; P( A \text{ and } C )
\end{align*}\]</span> The three inequalities can’t hold simultaneously when <span class="math inline">\(P(B\mid C) = P(B\mid \bar  C)\)</span>.</p>
</div>
<div id="exm-birthday" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 (Birthday Problem)</strong></span> The birthday problem is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.</p>
<p>In general, given <span class="math inline">\(N\)</span> items (people) randomly distributed into <span class="math inline">\(c\)</span> categories (birthdays), where the number of items is small compared to the number of categories <span class="math inline">\(N \ll c\)</span>, the probability of no match is given by <span class="math display">\[
P(\text{no match}) \approx \exp\left(-N^2/2c\right).
\]</span> Given <span class="math inline">\(A_i\)</span> is the event that person <span class="math inline">\(i\)</span> has a matching birthday with someone, we have <span class="math display">\[
P(\text{no match})  = \prod_{i=1}^{N-1}(1-P(A_i)) = \exp\left(\sum_{i=1}^{N-1}\log (1-P(A_i))\right).
\]</span> Here <span class="math inline">\(P(A_i) =\dfrac{i}{c}\)</span> Then use the approximation <span class="math inline">\(\log(1-x) \approx -x\)</span> for small <span class="math inline">\(x\)</span> to get <span class="math inline">\(P(\text{no match})\)</span>. <span class="math display">\[
\sum_{i=1}^{N-1}\log (1-P(A_i)) \approx -\sum_{i=1}^{N-1}\dfrac{i}{c} = -\dfrac{N(N-1)}{2c}.
\]</span></p>
<p>The probability of at least two people sharing a birthday is then the complement of the probability above: <span class="math display">\[
P(\text{At least one shared birthday}) = 1 - P(\text{no match}).
\]</span> Solving for <span class="math inline">\(P(\text{match})=1/2\)</span>, leads to a square root law <span class="math inline">\(N=1.2\sqrt{c}\)</span>, if <span class="math inline">\(c=365\)</span> then <span class="math inline">\(N=23\)</span>, and if <span class="math inline">\(c=121\)</span> (near birthday mathc), then <span class="math inline">\(N=13\)</span>.</p>
<p>This unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.</p>
</div>
</section>
<section id="kolmogorov-axioms" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="kolmogorov-axioms"><span class="header-section-number">1.2</span> Kolmogorov Axioms</h2>
<p>Later, in early thirties of the last century, Kolmogorov made significant contributions to the development of probability. He characterized it as a system of sets that meet specific criteria. The representation of the elements within this set is irrelevant. This is similar to how basic geometric concepts are typically introduced. For example, a circle is defined as the set of all points that are equidistant from a given point. The representation of the circle is irrelevant, as long as the set of points meets the criteria. Similarly, a probability field is defined as a set of events that meet specific criteria. This is the basis of the axiomatic approach to probability theory.</p>
<p>Kolmogorov’s axioms, which provided a rigorous foundation for probability theory. He showed that probability is immensely useful and adheres to only a few basic rules. These axioms provided a set of logical and mathematical rules that describe the properties of probability measures.</p>
<p>Let <span class="math inline">\(S\)</span> be a collection of elementary events and consider two random events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> that are subsets of <span class="math inline">\(S\)</span>. The three axioms are:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: For any random event <span class="math inline">\(A\)</span>, the probability of <span class="math inline">\(A\)</span> is greater than or equal to zero: <span class="math display">\[
P(A)\ge 0
\]</span></li>
<li><strong>Normalization</strong>: The probability of the entire sample <span class="math inline">\(S\)</span> space is equal to 1: <span class="math display">\[
P(S) = 1
\]</span></li>
<li><strong>Additivity</strong>: For mutually exclusive events, we have <span class="math display">\[
P(A \text{ or } B) = P(A) + P(B)
\]</span> The probability of the union of these events is equal to the sum of their individual probabilities.</li>
</ol>
<p>Mutually exclusive means that only one of the events in the sequence can occur. These axioms provided a solid and consistent foundation for probability theory, allowing mathematicians to reason rigorously about uncertainty and randomness. Kolmogorov’s work helped unify and clarify many concepts in probability, and his axioms are now widely accepted as the basis for modern probability theory. His contributions had a profound impact on various fields, including statistics, mathematical physics, and information theory.</p>
<p>Assigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (a.k.a observed data). Consider coin-tossing example. We toss coin twice and the possible outcomes are <em>HH, HT, TH, TT</em>. Say event <span class="math inline">\(A\)</span> represents a repetition, then it will consists of the first and second outcome of the two coin-toss. Then, to empirically estimate <span class="math inline">\(P(A)\)</span> we can repeat the two-toss experiment <span class="math inline">\(n\)</span> times and count <span class="math inline">\(m\)</span>, the number of times <span class="math inline">\(A\)</span> occurred. When <span class="math inline">\(N\)</span> is large, <span class="math inline">\(m/N\)</span> will be close to <span class="math inline">\(P(A)\)</span>. However, if we are to repeat this experiment under different conditions, e.g.&nbsp;when an unbalanced coin is used, our estimate of <span class="math inline">\(P(A)\)</span> will change as well.</p>
<p>The axioms provide are a number of rules that probabilities must follow. There are several important corollaries, that can help us assigning probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:</p>
<!-- https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf -->
<ol type="1">
<li><strong>Complement Rule</strong>: Let not <span class="math inline">\(A\)</span> denote the complement of event A. <span class="math display">\[
  P(\text{not } A) = 1- P(A).
\]</span></li>
<li><strong>Monotonicity</strong>: If <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(P(A)\le P(B)\)</span>. In other words, the probability of a larger set is greater than or equal to the probability of a subset.</li>
<li><strong>Subadditivity</strong>: This is a generalization of the addition rule, where the equality holds when events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive. <span class="math display">\[
P(A \text{ or } B)\le P(A)+P(B).
\]</span></li>
<li><strong>Inclusion-Exclusion Principle</strong>: This principle extends subadditivity to the case where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not necessarily mutually exclusive. <span class="math display">\[
P(A\text{ or } B)=P(A)+P(B)-P(A\text{ and }B).
\]</span></li>
<li><strong>Conditional Probabiliity</strong>: The conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is <span class="math display">\[
P(A\mid B) = \dfrac{P(A \text{ and } B)}{P(B)}.
\]</span></li>
<li><strong>Bayes rule</strong> is simply the calculation of conditional probables reversing the conditioning. A disciplined probability accounting so to speak. <span class="math display">\[
P(A\mid B) = \dfrac{P(A \text{ and } B)}{P(B)} = \dfrac{P(B\mid A)P(A)}{P(B)}.   
\]</span></li>
<li><strong>Law of total probability</strong> is a direct consequence of the definition of conditional probability and the normalization axiom. It states that if <span class="math inline">\(B_1, B_2, \ldots, B_n\)</span> are mutually exclusive and exhaustive events, then <span class="math display">\[
P(A) = \sum_{i=1}^n P(A \text{ and } B_i) = \sum_{i=1}^n P(A \mid B_i)P(B_i).
\]</span></li>
</ol>
<p>All of these axioms follow simply from the principle of coherance of the avoidance of dutch book. This incudes the Bayes rule itself (de Finetti, Shimony).</p>
<p>Bayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.</p>
<p>Bayes rule simply provides a disciplined probability accounting of how this probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities he cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.</p>
<p>These corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.</p>
<p>The age of artificial intelligence (AI) has certainly proved that Bayes is a powerful tool. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics of the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.</p>
</section>
<section id="dutch-book-and-the-rules-of-probability" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="dutch-book-and-the-rules-of-probability"><span class="header-section-number">1.3</span> Dutch book and the rules of probability</h2>
<p>If probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.</p>
<p>The starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “<em>It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event</em>” (p.&nbsp;101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.</p>
<p>Formally, for any event <span class="math inline">\(A\)</span>, the identity <span class="math display">\[
P(A)  =\frac{1}{1+\text{odds}(A)}\mathrm{or}\;\;\text{odds}(A)=\frac{1-P(A)}{P(A)}\text{,}
\]</span> where <span class="math inline">\(\bar A\)</span> is the complement of <span class="math inline">\(A\)</span>, links odds and probabilities. Throughout, we use <span class="math inline">\(P\)</span> as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is <span class="math inline">\(1/3\)</span>. The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are <em>fair</em> if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.</p>
<p>In constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “<em>could have a book made against him by a cunning bettor and would then stand to lose in any event</em>” (Ramsey (1931), p.&nbsp;22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.</p>
<p>Dutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, <span class="math inline">\(S\)</span>. The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.</p>
<p>To derive the rules, consider the first axiom of probability: for any event <span class="math inline">\(A\)</span>, <span class="math inline">\(0\leq P(A) \leq 1\)</span>. Suppose that the odds imply probabilities <span class="math inline">\(P(A)\)</span> for <span class="math inline">\(A\)</span> occurring and <span class="math inline">\(P(\bar A)\)</span> for other outcomes, with associated payoffs of <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>. Then, having bet <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>, the gains if <span class="math inline">\(A\)</span> or <span class="math inline">\(\bar A\)</span> occur, <span class="math inline">\(G_{A}\)</span> and <span class="math inline">\(G_{\bar A}\)</span>, respectively, are <span class="math display">\[\begin{align*}
G(A)   &amp;  =S_{A}-\text{$P$}(A)
S_{A}-\text{$P$}(\bar A)  S_{\bar A}\\
G(\bar A)   &amp;  =S_{\bar A}-\text{$P$}(A)
S_{A}-\text{$P$}(\bar A)  S_{\bar A}\text{.}%
\end{align*}\]</span> To see this, note that the bettor receives <span class="math inline">\(S_{A}\)</span> and pays <span class="math inline">\(P(A) S_{A}\)</span> for a bet on event <span class="math inline">\(A\)</span>. The bookie can always choose to place a zero stake on <span class="math inline">\(\bar A\)</span> occurring, which implies that <span class="math inline">\(G(A) =S_{A}-P(A) S_{A}\)</span> and <span class="math inline">\(G\left(\bar A\right) =-P(A) S_{A}\)</span>. Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus <span class="math inline">\(G(A) G(\bar A) \leq 0\)</span>. Substituting, <span class="math inline">\(\left( 1-P(A) \right) P(A) \geq0\)</span> or <span class="math inline">\(0\leq P(A) \leq 1\)</span>, which is the first axiom of probability. The second axiom is that the set of all possible outcomes has probability <span class="math inline">\(1\)</span> is similarly straightforward to show.</p>
<p>The third axiom is that probabilities add, that is, for two disjoint events <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span>, <span class="math inline">\(P(A) =P\left( A_{1} \text{ or } A_{2}\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>. Assuming stakes sizes of <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}},\)</span> and <span class="math inline">\(S_{A_{2}}\)</span> (and zero stakes on their complements) there are three possible outcomes. If neither <span class="math inline">\(A_{1}\)</span> nor <span class="math inline">\(A_{2}\)</span> occur, the gain is <span class="math display">\[
G(\bar A)  =-\text{$P$}(A)  S_{A}%
-\text{$P$}\left(  A_{1}\right)  S_{A_{1}}-\text{$P$}\left( A_{2}\right)  S_{A_{2}}.
\]</span></p>
<p>If <span class="math inline">\(A_{1}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and the gain is <span class="math display">\[
G\left(  A_{1}\right)  =\left(  1-\text{$P$}(A)
\right)  S_{A}+\left(  1-\text{$P$}\left(  A_{1}\right)  \right)
S_{A_{1}}-\text{$P$}\left(  A_{2}\right)  S_{A_{2}},
\]</span> and finally if <span class="math inline">\(A_{2}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and <span class="math display">\[
G\left(  A_{2}\right)  =\left(  1-\text{$P$}(A)
\right)  S_{A}-\text{$P$}\left(  A_{1}\right)  S_{A_{1}}+\left( 1-\text{$P$}\left(  A_{2}\right)  \right)  S_{A_{2}}.
\]</span> Arranging these into a matrix equation, <span class="math inline">\(G=PS\)</span>:<br>
<span class="math display">\[
\left( \begin{array}
[c]{c}%
G(\bar A) \\
G\left(  A_{1}\right) \\
G\left(  A_{2}\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
1-P(A) \\
1-P(A)  &amp; 1-P\left(  A_{1}\right)  &amp;
-P\left(  A_{2}\right) \\
1-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
1-P\left(  A_{2}\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{A}\\
S_{A_{1}}\\
S_{A_{2}}%
\end{array}
\right)  \text{.}%
\]</span></p>
<p>The absence of a Dutch book arbitrage implies that there is no set of stakes, <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}}\)</span>, and <span class="math inline">\(S_{A_{2}}\)</span>, such that the winnings in all three events are positive. If the matrix <span class="math inline">\(P\)</span> is invertible, it is possible to find stakes with positive gains. To rule this, the determinant of <span class="math inline">\(P\)</span> must be zero, which implies that <span class="math inline">\(0=-P(A) +P\left(A_{1}\right) +P\left( A_{2}\right)\)</span>, or <span class="math inline">\(P\left(A\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>. The chapter, countably additivity also holds.</p>
<p>The fourth axiom is conditional probability. Consider an event <span class="math inline">\(B\)</span>, with <span class="math inline">\(P\left( B\right) &gt;0\)</span>, an event <span class="math inline">\(A\)</span> that occurs conditional on <span class="math inline">\(B\)</span>, and the event that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur. The probabilities or prices of these bets are <span class="math inline">\(P\left( B\right)\)</span>, <span class="math inline">\(P\left( A \mid B\right)\)</span>, and <span class="math inline">\(P\left( A \text{ and } B\right)\)</span>. Consider bets with stakes <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A \mid B}\)</span> and <span class="math inline">\(S_{A \text{ and } B}\)</span>, with the understanding that if <span class="math inline">\(B\)</span> does not occur, the conditional bet on <span class="math inline">\(A\)</span> is canceled. The payoffs to the events that <span class="math inline">\(B\)</span> does not occur, <span class="math inline">\(B\)</span> occurs but not <span class="math inline">\(A\)</span>, and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur, are <span class="math display">\[
\left( \begin{array}
[c]{c}%
G\left(  \bar B\right) \\
G\left(  \bar A \text{ and } B\right) \\
G\left(  A \text{ and } B\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp; 0\\
1-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp;
-P\left(  A \mid B\right) \\
1-P\left(  B\right)  &amp; 1-P\left(  A \text{ and } B\right)  &amp;
1-P\left(  A \mid B\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{B}\\
S_{A \text{ and } B}\\
S_{A \mid B}%
\end{array}
\right)  \text{.}%
\]</span> Similar arguments imply the determinant must be zero, which implies that <span class="math display">\[
P\left(  A \mid B\right)  =\frac{P\left(  A \text{ and } B\right)
}{P\left(  B\right)  },
\]</span> which is the law of conditional probability, given <span class="math inline">\(P(B)&gt;0\)</span>, of course, otherwise the conditional probability is not defined, and the <span class="math inline">\(P\)</span> matrix has determinant 0.</p>
<p>To summarize, probabilities are degrees of belief and are subjective, and if these beliefs are consistent or coherent, they satisfy the rules of probability. Thus, unlike the Kolmogorov system that assumes the laws of probability, the Bayesian approach <em>derives</em> the laws of probability from behavior that avoids certain losses. This is why most Bayesians describe their way of thinking as rational and coherent.</p>
</section>
<section id="random-variables" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="random-variables"><span class="header-section-number">1.4</span> Random Variables</h2>
<p>A random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.</p>
<p>For example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.</p>
<p>So, in summary, while events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.</p>
<section id="discrete-random-variable" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="discrete-random-variable"><span class="header-section-number">1.4.1</span> Discrete Random Variable</h3>
<p>Random variables are quantities that we are not certain about. The simplest version of a random variable is a binary yes/no outcome. A random variable that can take a finite or a countable number of values is called <strong>discrete random variable</strong>. Otherwise, it will be a continuous random variable.</p>
<!-- Another example of a random variable is price of a stock next year. The price variable can take a continuum of values and is called a continuous random variable. -->
<p>A random variable will describe an uncertain quantity, denoted by <span class="math inline">\(X\)</span>, by attaching a numeric value to the occurrence of an event. Two examples of discrete random variable are</p>
<ol type="1">
<li>Will a user click-through on a Google ad?</li>
<li>Who will win the 2024 will elections?</li>
</ol>
<p>Random variables are constructed by assigning specific values to events such as <span class="math inline">\(\{X=x\}\)</span> which corresponds to the outcomes where <span class="math inline">\(X\)</span> equals to a specific number <span class="math inline">\(x\)</span>. Associated with possible outcomes are probabilities, a number between zero and one.</p>
<p>To fix notation, we will use <span class="math inline">\(\prob{X=x}\)</span> to denote the probability that random variable <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(x\)</span>. A map from all possible values <span class="math inline">\(x\)</span> of a discrete random variable <span class="math inline">\(X\)</span> to probabilities is called a <strong>probability mass function</strong> <span class="math inline">\(p(x)\)</span>. We will interchangeably use <span class="math inline">\(\prob{X=x}\)</span> and <span class="math inline">\(p(x)\)</span>. An important property of the probability mass function is that (normalization Kolmogorov axiom) <span class="math display">\[
\sum_{x\in S} p(x) = 1.
\]</span> Here <span class="math inline">\(S\)</span> denotes the set of all possible values of random variable <span class="math inline">\(X\)</span>.</p>
<p>Clearly, all probabilities have to be greater than or equal to zero, so that <span class="math inline">\(p(x)\ge 0\)</span>.</p>
<p>For a continuous random variable, the probability distribution is represented by a probability density function (PDF), which indicates the likelihood of the variable falling within a particular range and will discuss it later. In continuous case, we will use <span class="math inline">\(p(x)\)</span> to denote probability density function. Another way of describing a continuous random variable, is to use cumulative density function <span class="math inline">\(F(x) = P(X\le x)\)</span>. Arguably, a more natural approach.</p>
<p>The Cumulative Distribution Function (CDF) for a discrete random variable is a function that provides the probability that the random variable is less than or equal to a particular value. The CDF is monotonically increasing function (never decreases as <span class="math inline">\(x\)</span> increases). In other words, if <span class="math inline">\(a \leq b\)</span>, then <span class="math inline">\(F_X(a) \leq F_X(b)\)</span>. The value of the CDF always lies between 0 and 1, inclusive.</p>
<div id="exm-dcdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.10 (Discrete CDF)</strong></span> Suppose <span class="math inline">\(X\)</span> is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
P(X = x) = \frac{1}{6}
\]</span> for <span class="math inline">\(x = 1, 2, 3, 4, 5, 6\)</span></p>
<p>The CDF of <span class="math inline">\(X\)</span>, <span class="math inline">\(F(x)\)</span>, is calculated as follows:</p>
<ul>
<li>For <span class="math inline">\(x &lt; 1\)</span>, <span class="math inline">\(F(x) = 0\)</span> (since it’s impossible to roll less than 1).</li>
<li>For <span class="math inline">\(1 \leq x &lt; 2\)</span>, <span class="math inline">\(F(x) = \frac{1}{6}\)</span> (the probability of rolling a 1).</li>
<li>For <span class="math inline">\(2 \leq x &lt; 3\)</span>, <span class="math inline">\(F(x) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}\)</span> (the probability of rolling a 1 or 2).</li>
<li>This pattern continues, adding <span class="math inline">\(\frac{1}{6}\)</span> for each integer interval up to 6.</li>
<li>For <span class="math inline">\(x \geq 6\)</span>, <span class="math inline">\(F(x) = 1\)</span> (since it’s certain to roll a number 6 or less).</li>
</ul>
<p>Graphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>), <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a discrete random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="bernoulli-distribution" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">1.5</span> Bernoulli Distribution</h2>
<p>The formal model of a coin toss was described by Bernoulli. He modeled the notion of <em>probability</em> for a coin toss, now known as the Bernoulli distribution, there <span class="math inline">\(X \in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p, P(X=0) = 1-p\)</span>. Laplace gave us the <em>principle of insufficient reason</em>: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete distribution on the set of possible outcomes.</p>
<p>A Bernoulli trial relates to an experiment with the following conditions</p>
<ol type="1">
<li>The result of each trial is either a success or failure.</li>
<li>The probability <span class="math inline">\(p\)</span> of a success is the same for all trials.</li>
<li>The trials are assumed to be <em>independent</em>.</li>
</ol>
<p>The Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by <span class="math inline">\(\text{Bernoulli}(p)\)</span>, where <span class="math inline">\(p\)</span> is the probability of success.</p>
<p>The probability mass function (PMF) of a Bernoulli distribution is defined as follows: <span class="math display">\[
P(X = x) = \begin{cases}
p &amp; \text{if } x = 1 \\
1 - p &amp; \text{if } x = 0
\end{cases}
\]</span> The expectation (mean) of a Bernoulli distributed random variable <span class="math inline">\(X\)</span> is given by: <span class="math display">\[\E{X} = p
\]</span> Simply speaking, if you are to toss a coin many times, you expect <span class="math inline">\(p\)</span> heads.</p>
<p>The variance of <span class="math inline">\(X\)</span> is given by: <span class="math display">\[
\Var{X} = p(1-p)
\]</span></p>
<div id="exm-Coin" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.11 (Coin Toss)</strong></span> The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is <span class="math inline">\(S = \{H,T\}\)</span>, and <span class="math inline">\(p(X = H) = p(X = T) = 1/2\)</span>. On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads <span class="math inline">\(X\)</span> out of two coin tosses is</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Given the probability mass function we can, for example, calculate the probability of at least one head as <span class="math inline">\(\prob{X \geq 1} = \prob{X =0} + \prob{X =1} = p(0)+p(1) = 3/4\)</span>.</p>
</div>
<p>The Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to <span class="math inline">\(X\)</span>, which is the number of successes. It’s probability distribution is calculated via: <span class="math display">\[
\prob{X=x} = {n \choose x} p^x(1-p)^{n-x}.
\]</span> Here <span class="math inline">\({n \choose x}\)</span> is the combinatorial function, <span class="math display">\[
{n \choose x} = \frac{n!}{x!(n-x)!},
\]</span> where <span class="math inline">\(n!=n(n-1)(n-2)\ldots 2 \cdot 1\)</span> counts the number of ways of getting <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials.</p>
<p>Table below shows the expected value and variance of Binomial random variable.</p>
<table class="caption-top table">
<caption>Mean and Variance of Binomial</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Binomial Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = n p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = n p ( 1 - p )\)</span></td>
</tr>
</tbody>
</table>
<p>For large sample sizes <span class="math inline">\(n\)</span>, this distribution is approximately normal with mean <span class="math inline">\(np\)</span> and variance of <span class="math inline">\(np(1-p)\)</span>.</p>
<p>Suppose we are about to toss two coins. Let <span class="math inline">\(X\)</span> denote the number of heads. Then the following table specifies the probability distribution <span class="math inline">\(p(x)\)</span> for all possible values <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>. This leads to the following table</p>
<table class="caption-top table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Thus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely Binomial distribution.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of heads in three flips. Each possible outcome (“realization”) of <span class="math inline">\(X\)</span> is an <em>event</em>. Now consider the event of getting only two heads <span class="math display">\[
\{ X= 2\} = \{ HHT, HTH, THH \} ,
\]</span> The probability distribution of <span class="math inline">\(X\)</span> is Binomial with parameters <span class="math inline">\(n = 3, p= 1/2\)</span>, where <span class="math inline">\(n\)</span> denotes the sample size (a.k.a. number of trials) and <span class="math inline">\(p\)</span> is the probability of heads, we have a fair coin. The notation is <span class="math inline">\(X \sim \mathrm{Bin} \left ( n = 3 , p = \frac{1}{2} \right )\)</span> where the sign <span class="math inline">\(\sim\)</span> is read as <em>distributed as</em>.</p>
<table class="caption-top table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">HHH</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;"><span class="math inline">\(p^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">HHT</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1- p)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1 - p)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\((1-p)p^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p( 1-p)^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p ( 1-p)^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">TTH</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^2 p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">TTT</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^3\)</span></td>
</tr>
</tbody>
</table>
<section id="continuous-random-variables" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="continuous-random-variables"><span class="header-section-number">1.5.1</span> Continuous Random Variables</h3>
<p>If we want to build a probabilistic model of a stock price or return. We need to use a continuous random variable that can take an interval of values. Instead of frequency function we will use <strong>density function</strong>, <span class="math inline">\(p(x)\)</span> to describe a continuous variable. Unlike the discrete case <span class="math inline">\(p(x)\)</span> is not the probability that random variable takes value <span class="math inline">\(x\)</span>. Rather, we need to talk about value being inside an interval. For example probability of <span class="math inline">\(X\)</span> with density <span class="math inline">\(p(x)\)</span> being inside any interval <span class="math inline">\([a,b]\)</span>, with <span class="math inline">\(a&lt;b\)</span> is given by <span class="math display">\[
P(a &lt; X &lt; b) = \int_{a}^{b}p(x)dx.
\]</span> The total probability is one as <span class="math inline">\(\int_{-\infty}^\infty p(x) dx=1\)</span>. The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then <span class="math display">\[
  p(x) = \begin{cases} 1, ~~~20 \le x \le 30\\0, ~~~\mbox{otherwise}\end{cases}
\]</span></p>
<p>Under, this model, then the probability of temperature being between 25 and 27 degrees is <span class="math display">\[
P(25 \le x \le 27) = \int_{25}^{27} p(x)dx = (27-25)/10 = 0.2
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/uniform.svg" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption>Uniform Distribution: Probability of temperature being between 25 and 27</figcaption>
</figure>
</div>
<p>The Cumulative Distribution Function for a continuous random variable (X), denoted as (F_X(x)), is defined similarly to discrete RV CDF as <span class="math display">\[
F(x) = P(X \leq x)
\]</span> Continuous RV CDF has the same properties as a discrete one (increasing and takes values in [0,1]).</p>
<div id="exm-ccdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.12 (Continuous CDF for Uniform Distribution)</strong></span> <span class="math display">\[
\begin{cases}
1 &amp; \text{if } 0 \leq x \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The CDF, <span class="math inline">\(F(x)\)</span>, is obtained by integrating the PDF: - For <span class="math inline">\(x &lt; 0\)</span>, <span class="math inline">\(F(x) = 0\)</span>. - For <span class="math inline">\(0 \leq x \leq 1\)</span>, <span class="math inline">\(F(x) = \int_0^x 1 \, dt = x\)</span>. - For <span class="math inline">\(x &gt; 1\)</span>, <span class="math inline">\(F(x) = 1\)</span>. So, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as <span class="math inline">\(x\)</span> goes from 0 to 1.</p>
<p>Graphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as <span class="math inline">\(x\)</span> increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">runif</span>(<span class="dv">500</span>)), <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">pch=</span><span class="dv">21</span>, <span class="at">bg=</span><span class="st">"grey"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a uniform random variable</figcaption>
</figure>
</div>
</div>
</div>
<p>What about CDF of a normal distribution?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">rnorm</span>(<span class="dv">500</span>)), <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">pch=</span><span class="dv">21</span>, <span class="at">bg=</span><span class="st">"grey"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a normal random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="the-inverse-cdf-method" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="the-inverse-cdf-method"><span class="header-section-number">1.5.2</span> The Inverse CDF Method</h3>
<p>The inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, <span class="math inline">\(F\)</span>. Since <span class="math inline">\(F\left(  x\right)\)</span> is uniformly distributed on <span class="math inline">\(\left[ 0,1\right]\)</span>, draw a uniform random variable and invert the CDF to get a draw from <span class="math inline">\(F\)</span>. Thus, to sample from <span class="math inline">\(F\)</span>, <span class="math display">\[\begin{align*}
&amp;  \text{Step 1}\text{: Draw }U\sim U\left[  0,1\right]  \ \\
&amp;  \text{Step 2}\text{: }\text{Set }X=F^{-1}\left(  U\right)  ,
\end{align*}\]</span> where <span class="math inline">\(F^{-1}\left(  U\right)  =\inf\left\{  x:F\left(  x\right)  =U\right\}\)</span>.</p>
<p>This inversion method provides i.i.d. draws from <span class="math inline">\(F\)</span> provided that <span class="math inline">\(F^{-1}\left(  U\right)\)</span> can be exactly calculated. For example, the CDF of an exponential random variable with parameter <span class="math inline">\(\mu\)</span> is <span class="math inline">\(F\left(  x\right) =1-\exp\left(  -\mu x\right)\)</span>, which can easily be inverted. When <span class="math inline">\(F^{-1}\)</span> cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, <span class="math inline">\(F\left(  x\right)\)</span> can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from <span class="math inline">\(F\)</span>. With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.</p>
</section>
<section id="functional-transformations" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="functional-transformations"><span class="header-section-number">1.5.3</span> Functional Transformations</h3>
<p>The second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that <span class="math inline">\(X\sim F\)</span>, admitting a density <span class="math inline">\(f\)</span>, and that <span class="math inline">\(y=h\left(  x\right)\)</span> is an increasing continuous function. Thus, we can define <span class="math inline">\(x=h^{-1}\left(  y\right)\)</span> as the inverse of the function <span class="math inline">\(h\)</span>. The distribution of <span class="math inline">\(y\)</span> is given by <span class="math display">\[
F_Y\left(y\right)  =\text{P}\left(  Y\leq y\right)  =\int_{-\infty}^{h^{-1}\left(  y\right)  }f\left(  x\right)  dx=F_X\left(  X\leq h^{-1}\left(y\right)  \right).
\]</span> Differentiating with respect to <span class="math inline">\(y\)</span> gives the density via Leibnitz’s rule: <span class="math display">\[
f_{Y}\left(  y\right)  =f\left(  h^{-1}\left(  y\right)  \right)  \left\vert\frac{d}{dy}\left(  h^{-1}\left(  y\right)  \right)  \right\vert,
\]</span> where we make explicit that the density is over the random variable <span class="math inline">\(Y\)</span>. This result is used widely. For example, if <span class="math inline">\(X\sim\mathcal{N}\left(  0,1\right)\)</span>, then <span class="math inline">\(Y=\mu+\sigma X\)</span>. Since <span class="math inline">\(x=h^{-1}\left(  y\right)  =\frac{y-\mu}{\sigma}\)</span>, the distribution function is <span class="math inline">\(F\left(  \frac{x-\mu}{\sigma}\right)\)</span> and density <span class="math display">\[
f_{Y}\left(  y\right)  =\frac{1}{\sqrt{2\pi}\sigma}\exp\left(  -\frac{1}{2}\left(  \frac{y-\mu}{\sigma}\right)  ^{2}\right).
\]</span> Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if <span class="math inline">\(Y\sim\mathcal{X}^{2}\left(  \nu\right)\)</span> and <span class="math inline">\(\nu\)</span> is an integer, then <span class="math inline">\(Y=\sum_{i=1}^{\nu}X_{i}^{2}\)</span> where each <span class="math inline">\(X_{i}\)</span> is independent standard normal. Exponential random variables can be used to simulate <span class="math inline">\(\mathcal{X}^{2}\)</span>, Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be via sums of squared vectors of standard normal random variables.</p>
</section>
</section>
<section id="conditional-marginal-and-joint-distributions" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="conditional-marginal-and-joint-distributions"><span class="header-section-number">1.6</span> Conditional, Marginal and Joint Distributions</h2>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which can be related to each other. Knowing <span class="math inline">\(X\)</span> would change your news about <span class="math inline">\(Y\)</span>. For example, as a first pass, psychologists who study phenomenon of happiness can be interested in understanding it relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.</p>
<p>Formally, the <strong>joint distribution</strong> of two variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function given by <span class="math display">\[
p(x,y) = \prob{X=x,Y=y}.
\]</span> This maps all combinations of possible values of these two variables to a probability on the interval [0,1].</p>
<p>The <strong>conditional probability</strong> is a measure of the probability of an random variable <span class="math inline">\(X\)</span>, given that value of another random variable was observed <span class="math inline">\(Y = y\)</span>. <span class="math display">\[
p(x\mid y) = \prob{X = x \mid Y = y}.
\]</span></p>
<p>The <strong>marginal probability</strong> of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the marginal probability <span class="math inline">\(\prob{X}\)</span> is the probability distribution of <span class="math inline">\(X\)</span> when the values of <span class="math inline">\(Y\)</span> are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of <span class="math inline">\(Y\)</span>. The converse is also true: the marginal distribution can be obtained for <span class="math inline">\(Y\)</span> by summing over the separate values of <span class="math inline">\(X\)</span>.</p>
<p>Marginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.</p>
<div id="exm-slary" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.13 (Salary-Happyness)</strong></span> Let’s look at an example. Suppose that to model relationship between two quantities, salary <span class="math inline">\(Y\)</span> and happiness <span class="math inline">\(X\)</span>. After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.</p>
<div class="cell">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Results of the Gallop survey. Rows are Salary (<span class="math inline">\(Y\)</span>) and columns are happiness (<span class="math inline">\(X\)</span>)</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">X = 0 (low)</th>
<th style="text-align: center;">X = 1 (medium)</th>
<th style="text-align: center;">X = 2 (high)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Y = low (0)</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = medium (1)</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Y = high (2)</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = very high (3)</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Each cell of the table is the joint probability, e.g.&nbsp;14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.</p>
<p>Now, if we want to answer the question what is the percent of high incomers in the population. For that we need to calculate what is called a <strong>marginal probability</strong> <span class="math inline">\(\prob{y = 2}\)</span>. We can calculate the proportion of high incomers <span class="math inline">\(\prob{y = 2}\)</span> by summing up the entries in the third row of the table, which is 0.17 in our case.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.07</span> <span class="sc">+</span> <span class="fl">0.01</span> <span class="sc">+</span> <span class="fl">0.09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.17</code></pre>
</div>
</div>
<p>Formally marginal probability over <span class="math inline">\(y\)</span> is calculated by summing the joint probability over the other variable, <span class="math inline">\(x\)</span>, <span class="math display">\[
p(y) = \sum_{x \in S}p(x,y)
\]</span> Where <span class="math inline">\(S\)</span> is a set of all possible values of the random variable <span class="math inline">\(X\)</span>.</p>
<!--     Salary ($S$)   0 (low)   1 (medium)   2 (high) -->
<!--   -------------- --------- ------------ ---------- -->
<!--            low 0      0.03         0.12       0.07 -->
<!--         medium 1      0.02         0.13       0.11 -->
<!--           high 2      0.01         0.13       0.14 -->
<!--      very high 3      0.01         0.09       0.14 -->
<p>Another, question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the <strong>conditional probability</strong> of <span class="math inline">\(X\)</span> given that value of variable <span class="math inline">\(Y\)</span> is known. This is denoted by <span class="math inline">\(\prob{X=x\mid Y=y}\)</span> or simply <span class="math inline">\(p(x\mid y)\)</span>, where <span class="math inline">\(\mid\)</span> reads as “given” or “conditional upon”.</p>
<p>The conditional probability <span class="math inline">\(p(x\mid y)\)</span> also has interpretation as updating your probability over <span class="math inline">\(X\)</span> after you have learned the new information about <span class="math inline">\(Y\)</span>. In this sense, probability is also the language of how you change opinions in light of new evidence. Proportion of happy people among high incomers is given by the conditional probability <span class="math inline">\(\prob{X=2\mid Y=2}\)</span> and can be calculated by dividing proportion of those who are high incomer and highly happy by the proportion of the high incomers <span class="math display">\[
\prob{X=2\mid Y=2} = \dfrac{\prob{X=2,Y=2}}{\prob{Y=2}} = \dfrac{0.09}{0.17} = 0.5294118.
\]</span></p>
<p>Now, if we compare it with the proportion of highly happy people <span class="math inline">\(\prob{X = 2} = 0.38\)</span>, we see that on average you are more likely to be happy given your income is high.</p>
</div>
</section>
<section id="independence" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="independence"><span class="header-section-number">1.7</span> Independence</h2>
<p>Historically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.</p>
<p>Using the notion of conditional probability, we can define independence of two variables. Two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <strong>independent</strong> if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. That is, learning information <span class="math inline">\(X=x\)</span> doesn’t affect.</p>
<p>Conditional probabilities are counter intuitive. For example, one of the most important properties is typically <span class="math inline">\(p( x \mid y ) \neq p( y\mid x )\)</span>, our probabilistic assessment of <span class="math inline">\(Y\)</span> for any value <span class="math inline">\(y\)</span>. This is known as <em>Prosecutors’ Fallacy</em> as it arises when probability is used as evidence in a court of law. In the case of independence, <span class="math inline">\(p(x \mid y) = p(x)\)</span> and <span class="math inline">\(p(y \mid x) = p(y)\)</span>. Specifically, the probability of innocence given the evidence is not the same as the probability of evidence given innocence. It is very important to ask the question “what exactly are we conditioning on?” Usually, the observed evidence or data. Probability, of course, given evidence was one of the first applications of Bayes. Central to personalized probability. Clearly this is a strong condition and rarely holds in practice.</p>
<p>We just derived an important relation, that allows us to calculate conditional probability <span class="math inline">\(p(x \mid y)\)</span> when we know joint probability <span class="math inline">\(p(x,y)\)</span> and marginal probability <span class="math inline">\(p(y)\)</span>. The total probability or evidence can be calculated as usual, via <span class="math inline">\(p(y) = \sum_{x}p(x,y)\)</span>.</p>
<p>We will see that independence will lead to a different conclusion that the Bayes conditional probability decomposition: specifically, independence yields <span class="math inline">\(p( x,y ) = p(x) p(y)\)</span> and Bayes says <span class="math inline">\(p(x ,y) = p(x)p(x \mid y)\)</span>.</p>
<p>We need to specify distribution on each of those variables. Two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values variables separately The joint distribution will be giving by <span class="math display">\[
p(x,y) = p(x)p(y).
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then probability of the event <span class="math inline">\(X\)</span> and event <span class="math inline">\(Y\)</span> happening at the same time is the product of individual probabilities. From the conditional distribution formula it follows that <span class="math display">\[
p(x \mid y) = \dfrac{p(x,y)}{p(y)} = \dfrac{p(x)p(y)}{p(y)} = p(x).
\]</span> Another way to think of independence is to say that knowing the value of <span class="math inline">\(Y\)</span> doesn’t tell us anything about possible values of <span class="math inline">\(X\)</span>. For example when tossing a coin twice, the probability of getting <span class="math inline">\(H\)</span> in the second toss does not depend on the outcome of the first toss.</p>
<p>The expression of independence expresses the fact that knowing <span class="math inline">\(X=x\)</span> tells you nothing about <span class="math inline">\(Y\)</span>. In the coin tossing example, if <span class="math inline">\(X\)</span> is the outcome of the first toss and <span class="math inline">\(Y\)</span> is the outcome of the second toss <span class="math display">\[
\prob{ X=H  \mid  Y=T } = \prob{X=H  \mid  Y=H } = \prob{X=H}.
\]</span></p>
<p>Let’s do a similar example which illustrates this point clearly. Most people would agree with the following conditional probability assessments</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-keynes1921treatise" class="csl-entry" role="listitem">
Keynes, John Maynard. 1921. <em>A Treatise on Probability</em>. Macmillan.
</div>
<div id="ref-kreps1988notes" class="csl-entry" role="listitem">
Kreps, David. 1988. <em>Notes <span>On The Theory Of Choice</span></em>. Boulder: Westview Press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-intro.html" class="pagination-link" aria-label="Principles of Data Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Principles of Data Science</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-bayes.html" class="pagination-link" aria-label="Bayes Rule">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>