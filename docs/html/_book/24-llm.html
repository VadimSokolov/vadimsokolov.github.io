<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>24&nbsp; Large Language Models – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./25-robots.html" rel="next">
<link href="./23-nlp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="24&nbsp; Large Language Models – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="24-llm_files/figure-html/unnamed-chunk-3-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="24&nbsp; Large Language Models – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="24-llm_files/figure-html/unnamed-chunk-3-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-llm.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#autoregressive-generation-adding-one-word-at-a-time" id="toc-autoregressive-generation-adding-one-word-at-a-time" class="nav-link active" data-scroll-target="#autoregressive-generation-adding-one-word-at-a-time"><span class="header-section-number">24.1</span> Autoregressive Generation: Adding One Word at a Time</a></li>
  <li><a href="#building-intuition-character-level-text-generation" id="toc-building-intuition-character-level-text-generation" class="nav-link" data-scroll-target="#building-intuition-character-level-text-generation"><span class="header-section-number">24.2</span> Building Intuition: Character-Level Text Generation</a></li>
  <li><a href="#the-scale-approach-how-bigger-became-better" id="toc-the-scale-approach-how-bigger-became-better" class="nav-link" data-scroll-target="#the-scale-approach-how-bigger-became-better"><span class="header-section-number">24.3</span> The Scale Approach: How Bigger Became Better</a></li>
  <li><a href="#choosing-the-right-model-for-your-application" id="toc-choosing-the-right-model-for-your-application" class="nav-link" data-scroll-target="#choosing-the-right-model-for-your-application"><span class="header-section-number">24.4</span> Choosing the Right Model for Your Application</a></li>
  <li><a href="#sec-distillation" id="toc-sec-distillation" class="nav-link" data-scroll-target="#sec-distillation"><span class="header-section-number">24.5</span> Distillation, Fine-tuning and Quantization</a>
  <ul class="collapse">
  <li><a href="#quantization" id="toc-quantization" class="nav-link" data-scroll-target="#quantization">Quantization</a></li>
  <li><a href="#sec-fine-tuning" id="toc-sec-fine-tuning" class="nav-link" data-scroll-target="#sec-fine-tuning">Fine-tuning</a></li>
  <li><a href="#sec-distillation-detail" id="toc-sec-distillation-detail" class="nav-link" data-scroll-target="#sec-distillation-detail">Model Distillation: Knowledge Transfer</a></li>
  <li><a href="#the-rise-of-small-language-models" id="toc-the-rise-of-small-language-models" class="nav-link" data-scroll-target="#the-rise-of-small-language-models">The Rise of Small Language Models</a></li>
  </ul></li>
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance"><span class="header-section-number">24.6</span> Evaluating Model Performance</a></li>
  <li><a href="#sec-post-training-reasoning" id="toc-sec-post-training-reasoning" class="nav-link" data-scroll-target="#sec-post-training-reasoning"><span class="header-section-number">24.7</span> Post-training Techniques</a>
  <ul class="collapse">
  <li><a href="#chain-of-thought-and-chain-of-reasoning" id="toc-chain-of-thought-and-chain-of-reasoning" class="nav-link" data-scroll-target="#chain-of-thought-and-chain-of-reasoning">Chain-of-Thought and Chain of Reasoning</a></li>
  <li><a href="#reflexion" id="toc-reflexion" class="nav-link" data-scroll-target="#reflexion">Reflexion</a></li>
  <li><a href="#non-linear-reasoning-capabilities" id="toc-non-linear-reasoning-capabilities" class="nav-link" data-scroll-target="#non-linear-reasoning-capabilities">Non-Linear Reasoning Capabilities</a></li>
  </ul></li>
  <li><a href="#data-quality-and-quantity" id="toc-data-quality-and-quantity" class="nav-link" data-scroll-target="#data-quality-and-quantity"><span class="header-section-number">24.8</span> Data quality and quantity</a></li>
  <li><a href="#sec-context-engineering" id="toc-sec-context-engineering" class="nav-link" data-scroll-target="#sec-context-engineering"><span class="header-section-number">24.9</span> Dealing with Context Window Limitations: Context Engineering</a>
  <ul class="collapse">
  <li><a href="#the-lost-in-the-middle-problem" id="toc-the-lost-in-the-middle-problem" class="nav-link" data-scroll-target="#the-lost-in-the-middle-problem">The Lost-in-the-Middle Problem</a></li>
  <li><a href="#when-to-use-long-context-vs.-rag-vs.-summarization" id="toc-when-to-use-long-context-vs.-rag-vs.-summarization" class="nav-link" data-scroll-target="#when-to-use-long-context-vs.-rag-vs.-summarization">When to Use Long Context vs.&nbsp;RAG vs.&nbsp;Summarization</a></li>
  <li><a href="#attention-efficiency-flashattention-and-ring-attention" id="toc-attention-efficiency-flashattention-and-ring-attention" class="nav-link" data-scroll-target="#attention-efficiency-flashattention-and-ring-attention">Attention Efficiency: FlashAttention and Ring Attention</a></li>
  <li><a href="#retrieval-augmented-generation" id="toc-retrieval-augmented-generation" class="nav-link" data-scroll-target="#retrieval-augmented-generation">Retrieval-Augmented Generation</a></li>
  <li><a href="#advanced-rag-variants" id="toc-advanced-rag-variants" class="nav-link" data-scroll-target="#advanced-rag-variants">Advanced RAG Variants</a></li>
  <li><a href="#solving-lost-in-the-middle" id="toc-solving-lost-in-the-middle" class="nav-link" data-scroll-target="#solving-lost-in-the-middle">Solving Lost-in-the-Middle</a></li>
  <li><a href="#caching-and-compression" id="toc-caching-and-compression" class="nav-link" data-scroll-target="#caching-and-compression">Caching and Compression</a></li>
  <li><a href="#neural-memory-systems" id="toc-neural-memory-systems" class="nav-link" data-scroll-target="#neural-memory-systems">Neural Memory Systems</a></li>
  <li><a href="#evaluation-and-deployment" id="toc-evaluation-and-deployment" class="nav-link" data-scroll-target="#evaluation-and-deployment">Evaluation and Deployment</a></li>
  </ul></li>
  <li><a href="#combining-techniques-for-optimal-performance" id="toc-combining-techniques-for-optimal-performance" class="nav-link" data-scroll-target="#combining-techniques-for-optimal-performance"><span class="header-section-number">24.10</span> Combining Techniques for Optimal Performance</a>
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-llm.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-llm" class="quarto-section-identifier"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Large Language Models (LLMs) have emerged as a defining technology in artificial intelligence. While their aptitude for writing code is well-known, they also translate languages, analyze legal documents, and converse with fluency that feels human. This chapter explores the mechanisms underlying these capabilities.</p>
<p>LLMs depend on the Transformer architecture, which introduced the <em>attention</em> mechanism. Attention allows the model to dynamically weigh different words within a sequence, capturing long-range dependencies that previous architectures missed. While the mathematical foundations are detailed in <a href="23-nlp.html" class="quarto-xref"><span>Chapter 23</span></a>, this chapter focuses on operational logic: from Transformer mechanics to emergent reasoning capabilities.</p>
<section id="autoregressive-generation-adding-one-word-at-a-time" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="autoregressive-generation-adding-one-word-at-a-time"><span class="header-section-number">24.1</span> Autoregressive Generation: Adding One Word at a Time</h2>
<p>The most visible capability of LLMs is text generation—specifically, the ability to produce coherent, contextually relevant continuations from a given prompt. This phenomenon relies on a process known as <em>autoregressive modeling</em>. Fundamentally, an LLM is a probability distribution over sequences of text, trained to predict the next <em>token</em> given a preceding context. A token serves as the atomic unit of processing; depending on the tokenization schema, it may represent a full word, a subword unit (like “-ing”), or a single character. Given the prompt <span class="math inline">\(Q\)</span> and a sequence of tokens <span class="math inline">\(x_1, x_2, \ldots, x_t\)</span>, the model samples the next token <span class="math inline">\(x_{t+1}\)</span> from the conditional distribution <span class="math display">\[
x_{t+1} \sim p(x_{t+1} | Q, x_1, x_2, \ldots, x_t).
\]</span> The conditional varaibels <span class="math inline">\(Q, x_1, x_2, \ldots, x_t\)</span> are called the context. In this case, the prompt might include the user’s question and documents “attached” to the question. To illustrate this mechanism in practice, we will use the <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2">SmolLM2</a> model. We begin by loading the model and its associated tokenizer—the component responsible for translating raw text into the discrete inputs the model understands.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'./code'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Import our custom functions</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llm_chapter <span class="im">import</span> (ask_smol_lm, get_next_word_suggestions, generate_text_step_by_step)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>local_cache_dir <span class="op">=</span> <span class="st">"./models_cache"</span>  <span class="co"># or use absolute path like "/Users/your_username/ai_models"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create directory if it doesn't exist</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Path(local_cache_dir).mkdir(parents<span class="op">=</span><span class="va">True</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model with custom cache directory</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This will download once and store in your specified directory</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_id, cache_dir<span class="op">=</span>local_cache_dir)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_id,cache_dir<span class="op">=</span>local_cache_dir,device_map<span class="op">=</span><span class="st">"auto"</span>,torch_dtype<span class="op">=</span>torch.float16,low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> model.device</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Model loaded. Using device:", device)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Consider the text <code>The best thing about AI is its ability to</code>. Imagine analyzing billions of pages of human-written text—such as those found on the web or in digitized books—and identifying all instances of this text to determine what word most commonly comes next. While an LLM doesn’t directly search for literal matches, it evaluates semantic and contextual similarities to produce a ranked list of possible next words along with their associated probabilities.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get next word suggestions for a given text</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>initial_text <span class="op">=</span> <span class="st">"The best thing about AI is its ability to"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>suggestions <span class="op">=</span> get_next_word_suggestions(initial_text, model, tokenizer, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Next word suggestions for '</span><span class="sc">{</span>initial_text<span class="sc">}</span><span class="ss">':"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">## Next word suggestions for 'The best thing about AI is its ability to':</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (word, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(suggestions):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. '</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">' (prob: </span><span class="sc">{</span>prob<span class="sc">:.3f}</span><span class="ss">)"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">##   1. ' learn' (prob: 0.620)</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">##   2. ' help' (prob: 0.120)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">##   3. ' augment' (prob: 0.101)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">##   4. ' analyze' (prob: 0.085)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">##   5. ' process' (prob: 0.074)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-fig-width="5" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart
    A["learn&lt;br/&gt;(0.620)"]:::high-prob
    B["help&lt;br/&gt;(0.120)"]:::med-high-prob
    C["augment&lt;br/&gt;(0.101)"]:::med-prob
    D["analyze&lt;br/&gt;(0.085)"]:::med-low-prob
    E["process&lt;br/&gt;(0.074)"]:::low-prob
    
    classDef high-prob fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff
    classDef med-high-prob fill:#8BC34A,stroke:#558B2F,stroke-width:2px,color:#000
    classDef med-prob fill:#FFEB3B,stroke:#F57F17,stroke-width:2px,color:#000
    classDef med-low-prob fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff
    classDef low-prob fill:#F44336,stroke:#B71C1C,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> Next word predictions with probabilities, color-coded from green (high probability) to red (low probability)</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>If we look at the probabilities (on the log scale) of the next 10 words.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>suggestions <span class="op">=</span> get_next_word_suggestions(initial_text, model, tokenizer, top_k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(suggestions)))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [s[<span class="dv">0</span>] <span class="cf">for</span> s <span class="kw">in</span> suggestions]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> [s[<span class="dv">1</span>] <span class="cf">for</span> s <span class="kw">in</span> suggestions]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the next word suggestions with their log probabilities</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.plot(indices, np.log10(probabilities), marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xticks(indices, words, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">'right'</span>)<span class="op">;</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Next Word Suggestions'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Log Probability'</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Next Word Suggestions with Log Probabilities'</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-llm_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that the probabilities of each next word decay exponentially with rank (outside of the top word ‘learn’). This pattern is reminiscent of Zipf’s law, observed by linguist George Kingsley Zipf in the 1930s, which states that the frequency of a word in natural language is inversely proportional to its rank in the frequency table. While Zipf’s law describes unconditional word frequencies across a corpus, the probability distribution over next tokens given a specific context exhibits a similar heavy-tailed structure: a few continuations are highly probable, while most are rare.</p>
<p>One might assume the model should always select the next token with the highest probability. However, this approach is leads to generated text that is often lacks creativity and can become repetitive. To address this, randomness is introduced into the selection process. By occasionally choosing lower-ranked tokens, the model can produce more varied and engaging text.</p>
<p>This randomness means that using the same prompt multiple times will likely yield different outputs. A parameter called <em>temperature</em> controls the degree of randomness in token selection. Empirically, a temperature value of around 0.8 often strikes a good balance between coherence and creativity for text generation tasks. The term “temperature” originates from statistical physics, where it controls the spread of the Boltzmann distribution over energy states; here, it analogously controls the spread of the probability distribution over tokens (see <a href="#eq-class-prob" class="quarto-xref">Equation&nbsp;<span>24.1</span></a> in the Distillation section below for the mathematical formulation). A temperature of 0 would always select the highest-probability token (deterministic), while higher temperatures flatten the distribution, making less probable tokens more likely to be selected.</p>
<p>The following example illustrates the iterative process where the model selects the word with the highest probability at each step:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's start with a simple prompt</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>initial_text <span class="op">=</span> <span class="st">"The best thing about AI is its ability to"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Initial text: '</span><span class="sc">{</span>initial_text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Initial text: 'The best thing about AI is its ability to'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text step by step</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer, </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>num_steps<span class="op">=</span><span class="dv">10</span>, temperature<span class="op">=</span><span class="fl">1.0</span>, sample<span class="op">=</span><span class="va">False</span>, print_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">## Generated text:</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">60</span>))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">## The best thing about AI is its ability to learn and adapt.</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">## It can analyze vast amounts of</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In this example, we always select the most probable next token, which leads to a coherent but somewhat predictable continuation. The model generates text by repeatedly applying this process, building on the context provided by the previous tokens.</p>
<p>Now we will run our LLM generation process for longer and sample words with probabilities calculated based on the temperature parameter. We will use a temperature of 0.8, which is often a good choice for generating coherent text without being too repetitive.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix the seed for reproducibility</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">8</span>)<span class="op">;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a> num_steps<span class="op">=</span><span class="dv">60</span>, temperature<span class="op">=</span><span class="fl">0.8</span>, sample<span class="op">=</span><span class="va">True</span>,print_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Generated text:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">60</span>))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">## The best thing about AI is its ability to interact precisely</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">## with buildings, including piping [Ethernet be Definition</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">## requires Qualities]-was way k)-ay -- will keeping order for</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">## from few trips built themselves sitto functions convenient</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">## years answer shows data communication "states general rooms</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">## developers warning windows cybersecurity Virtual interview</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co">## no hassle put contents voice ordering popular regard dinner</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co">## English</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can see that the model went off track rather quickly, generating meaningless phrases that don’t follow the initial context. Now let’s try a higher temperature of 1.2.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">8</span>)<span class="op">;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text_step_by_step(initial_text, model, tokenizer,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a> num_steps<span class="op">=</span><span class="dv">60</span>, temperature<span class="op">=</span><span class="fl">1.2</span>, sample<span class="op">=</span><span class="va">True</span>,print_progress<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated text:"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">## Generated text:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(textwrap.fill(generated_text, width<span class="op">=</span><span class="dv">60</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">## The best thing about AI is its ability to interact precisely</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">## upwards             reffwd [EUMaiSTAVEQל]- AI achieves</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">## kawakay -- sporic order for accuracy round trips built hard</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">## sitto functions thruts generate squancers emerge good</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">## simasts tailrajs windows finish triippities siplex</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">## /&gt;node_{thread----------------mem</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Here the generation process went “off track” even quicker, it even entroduced characters from a different language. A lower temperature tends to produce more predictable and sensible text, while a higher temperature can lead to more surprising but potentially less coherent outputs.</p>
</section>
<section id="building-intuition-character-level-text-generation" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="building-intuition-character-level-text-generation"><span class="header-section-number">24.2</span> Building Intuition: Character-Level Text Generation</h2>
<p>Before diving deeper into how modern LLMs work, it helps to understand text generation at its most fundamental level: one character at a time. While LLMs operate on tokens (typically subwords), examining character-level patterns reveals the core insight behind statistical language modeling. We’ll start by counting letter frequencies in a Wikipedia article about cats, then see how these simple statistics can generate text.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download wikipedia article on "Cat"</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://en.wikipedia.org/wiki/Cat"</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> requests.get(url)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>cat_text <span class="op">=</span> response.text</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract text from HTML</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> bs4 <span class="im">import</span> BeautifulSoup</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>soup <span class="op">=</span> BeautifulSoup(cat_text, <span class="st">'html.parser'</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>cat_text <span class="op">=</span> soup.get_text()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now let’s count letter frequencies in the text and plot the letter frequencies for the first 26 letters</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>letter_counts <span class="op">=</span> Counter(c.lower() <span class="cf">for</span> c <span class="kw">in</span> cat_text <span class="cf">if</span> c.isalpha())</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by frequency</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sorted_letter_counts <span class="op">=</span> <span class="bu">sorted</span>(letter_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sorted_letter_counts <span class="op">=</span> sorted_letter_counts[:<span class="dv">26</span>]<span class="op">;</span>  <span class="co"># Limit to top 26 letters</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>letters, counts <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>sorted_letter_counts)<span class="op">;</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.bar(letters, counts, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Letters'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Letter Frequencies in Wikipedia Article on Cats'</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)<span class="op">;</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-llm_files/figure-html/unnamed-chunk-10-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>If we try to generate the text one letter at a time</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text one letter at a time by sampling from the letter frequencies</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>counts <span class="op">=</span> np.array(counts)<span class="op">/</span><span class="bu">sum</span>(counts)  <span class="co"># Normalize counts to probabilities</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>gentext <span class="op">=</span> random.choices(letters, weights<span class="op">=</span>counts, k<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated letters:"</span>, <span class="st">''</span>.join(gentext))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">## Generated letters: bioetsapatelacapsios</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>What if we do bi-grams, i.e.&nbsp;pairs of letters? We can do this by counting the frequencies of each pair of letters in the text. This will give us a sense of how often each pair of letters appears in the text, which is a good starting point for understanding how the model generates text.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>bigram_counts <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(cat_text) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cat_text[i].isalpha() <span class="kw">and</span> cat_text[i <span class="op">+</span> <span class="dv">1</span>].isalpha():</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        a, b <span class="op">=</span> cat_text[i].lower(), cat_text[i <span class="op">+</span> <span class="dv">1</span>].lower()</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only process standard English letters (a-z)</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'a'</span> <span class="op">&lt;=</span> a <span class="op">&lt;=</span> <span class="st">'z'</span> <span class="kw">and</span> <span class="st">'a'</span> <span class="op">&lt;=</span> b <span class="op">&lt;=</span> <span class="st">'z'</span>:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>            bigram <span class="op">=</span> (a, b)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            bigram_counts[bigram] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by frequency</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>sorted_bigram_counts <span class="op">=</span> <span class="bu">sorted</span>(bigram_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the heatmap of bigram frequencies</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>bigram_matrix <span class="op">=</span> np.zeros((<span class="dv">26</span>, <span class="dv">26</span>))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (a, b), count <span class="kw">in</span> sorted_bigram_counts:</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    bigram_matrix[<span class="bu">ord</span>(a) <span class="op">-</span> <span class="bu">ord</span>(<span class="st">'a'</span>), <span class="bu">ord</span>(b) <span class="op">-</span> <span class="bu">ord</span>(<span class="st">'a'</span>)] <span class="op">=</span> count</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>sns.heatmap(bigram_matrix, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Second Letter'</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'First Letter'</span>)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bigram Frequencies in Wikipedia Article on Cats'</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>plt.xticks(ticks<span class="op">=</span>np.arange(<span class="dv">26</span>) <span class="op">+</span> <span class="fl">0.5</span>, labels<span class="op">=</span>[<span class="bu">chr</span>(i <span class="op">+</span> <span class="bu">ord</span>(<span class="st">'a'</span>)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">26</span>)], rotation<span class="op">=</span><span class="dv">45</span>)<span class="op">;</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>plt.yticks(ticks<span class="op">=</span>np.arange(<span class="dv">26</span>) <span class="op">+</span> <span class="fl">0.5</span>, labels<span class="op">=</span>[<span class="bu">chr</span>(i <span class="op">+</span> <span class="bu">ord</span>(<span class="st">'a'</span>)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">26</span>)], rotation<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-llm_files/figure-html/unnamed-chunk-12-5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>This will take us one step closer to how LLMs generate text. We used Lev Tolstoy’s “War and Peace” novel to estimate the, 2-grams, 3-grams, 4-grams, and 5-grams letter frequencies and to generate text based on those models. The results are shown below:</p>
<pre><code>2-gram: ton w mer. y the ly im, in peerthayice waig trr. w tume shanite tem.
3-gram: the ovna gotionviculy on his sly. shoutessixeemy, he thed ashe
4-gram: the with ger frence of duke in me, but of little. progomind some later
5-gram: the replace, and of the did natasha's attacket, and aside. he comparte,</code></pre>
<p>We used the <code>nltk</code> package <span class="citation" data-cites="bird2009natural">(<a href="references.html#ref-bird2009natural" role="doc-biblioref">Bird, Klein, and Loper 2009</a>)</span> to estimate the letter frequencies from Tolstoy’s novel and generate text based on those models. The results show that even with simple letter-based models, we can generate text that resembles natural language, albeit with nonsensical phrases. As the n-gram order increases, the generated text becomes more coherent—the 5-gram output even captures character names like “Natasha.” This progression illustrates the core principle that underlies all language models: <em>context matters</em>, and more context leads to better predictions.</p>
<p>However, LLMs have much larger context windows, meaning they can consider much longer sequences of text when generating the next token. Modern models such as Gemini 3 Pro use context windows of up to 1 million tokens—approximately the size of Leo Tolstoy’s “War and Peace” novel. However, if you try to use a simple counting method (as we did with n-grams), you will quickly run into the problem of combinatorial explosion. For example, if we try to estimate 10-grams letter frequencies, we will have to count <span class="math inline">\(26^{10}\)</span> (over 141 trillion) combinations of letters. If we use word-based n-grams, the problem is even worse, as the number of common words in the English language is estimated to be around 40,000. This means that the number of possible 2-grams is 1.6 billion, for 3-grams is 64 trillion, and for 4-grams is 2.6 quadrillion. By the time we get to a typical question people ask when using AI chats with 20 words, the number of possibilities is larger than the number of particles in the universe. The challenge lies in the fact that the total amount of English text ever written is vastly insufficient to accurately estimate these probabilities, and this is where LLMs come in. They use neural networks to “compress” the input context into dense vector embeddings—distributed representations that capture semantic meaning—and “interpolate” the probabilities of the next token. This allows them to estimate probabilities for sequences they have never seen before and generate text that is coherent and contextually relevant. The main component of these neural networks is the transformer architecture.</p>
<p>The first step an LLM takes to “compress” the input is applying the attention mechanism. This concept is similar to convolutional neural networks (CNNs) used in computer vision, where the model focuses on different parts of the input image. In LLMs, attention allows the model to focus on different parts of the input text when generating the next token (see <a href="23-nlp.html" class="quarto-xref"><span>Chapter 23</span></a> for the mathematical details).</p>
</section>
<section id="the-scale-approach-how-bigger-became-better" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="the-scale-approach-how-bigger-became-better"><span class="header-section-number">24.3</span> The Scale Approach: How Bigger Became Better</h2>
<p>For decades the high-quality data analysis required finding a parsimonious model. Meaning the model that is as small as possible, but still able to capture the essential patterns in the data. This is the approach of traditional statistical learning. The deep learning models broke this trend and were shown to work very well then number of parameters is large, often you would have more parameters than data points. We discussed this phenomenon in <a href="19-theorydl.html#sec-double-descent" class="quarto-xref"><span>Section 19.8</span></a>.</p>
<p>This scaling behavior has led to exponential growth in model sizes. GPT-1, released in 2018 with 117 million parameters, was already considered large for its time. GPT-2, with 1.5 billion parameters, was initially deemed too dangerous to release publicly. GPT-3’s 175 billion parameters represented a quantum leap.</p>
<div id="fig-llm-param-growth" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llm-param-growth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/llm-param-growth.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llm-param-growth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.1: Scaling Behavior of LLMs
</figcaption>
</figure>
</div>
<p>Architectural innovations like Mixture of Experts (MoE) models allow for scaling model capacity further without proportionally increasing computational requirements, by activating only a subset of parameters for processing each token.</p>
<p>Beyond scaling up parameters, new architectures are emerging. Multimodal transformers are beginning to bridge the gap between text, images, audio, and other modalities, creating systems that can understand and generate content across multiple forms of media. These systems process diverse inputs within a single unified model, enabling rich interactions like chatting about an image or generating music from text descriptions.</p>
</section>
<section id="choosing-the-right-model-for-your-application" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="choosing-the-right-model-for-your-application"><span class="header-section-number">24.4</span> Choosing the Right Model for Your Application</h2>
<p>Although, the bigger models are a go-to approach, it does not always mean better choice. When choosing an appropriate model for a problem at hand, you need to consider several factors, such ability to upload your data to the cloud provider, cost of the model, the performance of the model on your specific task and latency requirements. Let’s discuss those factors in more detail.</p>
<p>Uploading your data to the model hosted by the cloud provider is a common practice. However, sometimes you are restricted by security policies, regulations (like HIPAA or GDPR), or massive data volumes. Then on-premises deployment: when you host the model (e.g., an open-source LLM like Llama 3 or Mistral) on your own hardware is one option. This gives you total control over the data lifecycle but requires significant capital expenditure (CapEx) for GPUs and maintenance. Another option that became recently available is to use isolated environments on the cloud provider. While the hardware is still theirs, the network is logically separated from the public internet, and data can be transferred over dedicated physical lines to avoid the public web. Typically, in this scenario model provider also offers zero data retention, which is a policy that ensures that the user data is deleted after it is used for inference and is not used for training or other purposes.</p>
<p>The cost of the model can be a prohibitive factor. In many commercial applications companies would be loosing money if they are using expensive “out-of-the-box” models. Therefore, they need to develop their own models or use open-source models that are more cost-effective. In this case you option is yet again to select a smaller model that you can host on your own hardware and fine-tune on your own data. We will discuss fine-tuning in more detail in <a href="#sec-distillation" class="quarto-xref"><span>Section 24.5</span></a>.</p>
<p>The performance of an existing model on your specific task is yet another factor. Typically, the bigger the model, the better the performance, but smaller models can be better (or good enough) for a particular use case. Size tiers offer different trade-offs: very small models (around 3 billion parameters or fewer) are fast and efficient and can often run on consumer hardware for simpler tasks like basic text classification or domain-specific chatbots; medium-sized models (7 to 30 billion parameters) are often the sweet spot, offering stronger performance with manageable compute; and large models (30 billion parameters or more) provide the best general performance and can show emergent capabilities, but typically require specialized hardware and higher cost. Beyond size, specialized variants matter too: code models are trained for software tasks (including “fill-in-the-middle” editing), multilingual models target many languages, and domain-specific models are tuned to specialized corpora. In practice, constraints like GPU memory, inference speed, cost (cloud APIs versus self-hosting), latency, and privacy requirements often drive the final choice.</p>
<p>Although, typically, there is a trade-off between the size and the performance, techniques such as knowledge distillation, fine-tuning, and quantization can help you achieve good performance on a complex task with a smaller model. We discuss these techniques in detail in <a href="#sec-distillation" class="quarto-xref"><span>Section 24.5</span></a>. A notable example occurred in late 2025 when Google’s Gemini 3 Flash—a distilled model designed for efficiency—outperformed the flagship Gemini 3 Pro on coding benchmarks, demonstrating that focused optimization can matter more than raw parameter count for specific tasks.</p>
<p>Finally, the latency requirement is a key factor in applications such as speech bots (e.g., Alexa, Google Home) and real-time applications (e.g., trading, finance). In these scenarios, the <em>Time to First Token</em> (TTFT)—the delay before the model starts outputting its response—is often more critical than the overall throughput. Several architectural and algorithmic techniques have been developed to minimize this delay.</p>
<p>One such technique is <strong>Speculative Decoding</strong> <span class="citation" data-cites="leviathan2023fast chen2023accelerating">(<a href="references.html#ref-leviathan2023fast" role="doc-biblioref">Leviathan, Kalman, and Matias 2023</a>; <a href="references.html#ref-chen2023accelerating" role="doc-biblioref">Chen et al. 2023</a>)</span>, which addresses the sequential bottleneck of autoregressive generation. Since generating each token requires a full forward pass of a large model, the process is inherently slow. Speculative decoding uses a much smaller, faster “draft” model to predict several potential next tokens in a single step. The larger “target” model then verifies these tokens in parallel. If the target model agrees with the draft, multiple tokens are accepted at once; if not, only the incorrect ones are discarded and regenerated. This approach can achieve significant speedups (often 2-3x) without any loss in accuracy.</p>
<div class="cell" data-fig-width="6" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">sequenceDiagram
    participant D as Draft Model (Small)
    participant T as Target Model (Large)
    participant O as Output
    
    Note over D, T: Step 1: Draft model generates K tokens
    D-&gt;&gt;D: Predict x_1, x_2, ..., x_K
    D-&gt;&gt;T: Send draft tokens
    
    Note over T: Step 2: Target model verifies in parallel
    T-&gt;&gt;T: Compute probabilities for x_1, ..., x_K
    
    Note over T, O: Step 3: Accept/Reject
    T--&gt;&gt;O: Output accepted tokens + 1 new token
</pre>
</div>
<p></p><figcaption> Speculative Decoding: A small draft model proposes tokens that a larger target model verifies in parallel, accelerating the generation process.</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>Another critical optimization is <strong>KV Caching</strong> (Key-Value Caching). During autoregressive generation, the model repeatedly attends to the same previous tokens. Rather than recomputing the keys and values for these tokens at every step, the model stores them in memory. This reduces the computational cost of generating each subsequent token from <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n)\)</span> relative to the current sequence length.</p>
<p>For real-time streaming applications, models can begin processing input before waiting until the entire query is available. This is known as <strong>Streaming Prefill</strong> or <strong>Incremental Prefill</strong>. Instead of waiting for the entire user query to be completed, the model starts the “prefill” phase—processing the input and building the KV cache—on chunks of the input as they arrive. This is particularly useful for voice-activated systems where the beginning of a command can be processed while the user is still speaking.</p>
<p>To improve efficiency in multi-user environments, <strong>Continuous Batching</strong> <span class="citation" data-cites="yu2022orca">(<a href="references.html#ref-yu2022orca" role="doc-biblioref">Yu et al. 2022</a>)</span> allows the server to start processing new requests immediately, even if other requests in the same batch are already in the middle of generation. Unlike static batching, which waits for all sequences in a batch to finish before starting a new one, continuous batching dynamically inserts and removes requests, significantly increasing throughput and reducing waiting times.</p>
<p>Finally, <strong>Quantization</strong> and <strong>Model Distillation</strong> (discussed in detail in <a href="#sec-distillation" class="quarto-xref"><span>Section 24.5</span></a>) remain the most common ways to reduce latency by simplifying the model itself. Quantization reduces the numerical precision of model weights (e.g., from 16-bit to 4-bit), allowing for faster arithmetic and smaller memory footprints, while distillation creates smaller student models that “mimic” the reasoning of larger teachers.</p>
</section>
<section id="sec-distillation" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="sec-distillation"><span class="header-section-number">24.5</span> Distillation, Fine-tuning and Quantization</h2>
<p>While scaling up model parameters has driven remarkable advances, practical deployment often demands the opposite: smaller, faster, more efficient models. Three complementary techniques—distillation, fine-tuning, and quantization—bridge the gap between research capabilities and production requirements. These techniques leverage existing foundation models for specific needs at a fraction of the cost of training from scratch.</p>
<section id="quantization" class="level3">
<h3 class="anchored" data-anchor-id="quantization">Quantization</h3>
<p><em>Quantization</em> reduces the numerical precision of model weights and activations, typically from 32-bit floating-point to 8-bit integers or even lower. This compression reduces memory bandwidth requirements and enables faster arithmetic operations on supported hardware. Modern quantization techniques like <a href="https://arxiv.org/abs/2210.17323">GPTQ</a> and <a href="https://arxiv.org/abs/2306.00978">AWQ</a> can reduce model size by 4× with minimal accuracy degradation, enabling larger models to run on consumer hardware or significantly reducing inference costs in production deployments.</p>
<p>The mathematics of quantization involves mapping continuous values to a discrete set of levels. For a weight <span class="math inline">\(w\)</span> with range <span class="math inline">\([w_{min}, w_{max}]\)</span>, 8-bit quantization maps it to one of 256 integer values: <span class="math display">\[
w_q = \text{round}\left(\frac{w - w_{min}}{w_{max} - w_{min}} \times 255\right)
\]</span></p>
<p>More sophisticated approaches like <em>post-training quantization</em> (PTQ) analyze calibration data to find optimal scaling factors, while <em>quantization-aware training</em> (QAT) incorporates quantization effects during training to minimize accuracy loss.</p>
</section>
<section id="sec-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="sec-fine-tuning">Fine-tuning</h3>
<p><em>Fine-tuning</em> adapts a pre-trained model to specific tasks or domains by continuing training on specialized data. This process leverages the general knowledge encoded during pre-training while teaching the model task-specific patterns and vocabulary. Pre-trained language models possess extensive knowledge from their training, but they are not optimized for any particular task out of the box. While a general-purpose LLM can generate logically valid responses, those responses may not align with the specific requirements of a given application. For instance, a fintech company might need a model that interprets balance sheets or analyzes regulatory filings with domain-specific precision, while a healthcare application requires understanding of medical terminology and compliance with clinical guidelines.</p>
<p>Fine-tuning offers several key advantages:</p>
<ul>
<li><em>Data efficiency</em>: Effective fine-tuning can be achieved with thousands rather than millions of examples.</li>
<li><em>Cost efficiency</em>: Reusing pre-trained models reduces computational cost compared to training from scratch.</li>
<li><em>Versatility</em>: The same pre-trained model can be fine-tuned for multiple applications across domains.</li>
<li><em>Improved performance</em>: Fine-tuned models learn task-specific patterns critical for their target applications.</li>
</ul>
<p>There are several approaches to fine-tuning, each with different trade-offs between performance, resource requirements, and flexibility.</p>
<p><em>Full fine-tuning</em> updates all model parameters, resulting in a brand-new version of the model optimized for the specific task. This approach offers maximum flexibility in adapting the model, as it can learn features and representations across all layers of the architecture. However, full fine-tuning requires significant compute resources and memory, and risks <em>catastrophic forgetting</em>—where the model loses its general capabilities as it specializes for the new task.</p>
<p><em>Parameter-efficient fine-tuning</em> (PEFT) methods address these limitations by modifying only a small subset of parameters while freezing most of the model. The key idea is to add task-specific layers on top of the frozen base model, allowing fundamental language understanding to remain unaffected. PEFT techniques like LoRA (Low-Rank Adaptation) train small adapter modules, dramatically reducing memory and compute requirements while maintaining performance. LoRA works by decomposing weight updates into low-rank matrices: <span class="math display">\[
W' = W + BA
\]</span> where <span class="math inline">\(B \in \mathbb{R}^{d \times r}\)</span> and <span class="math inline">\(A \in \mathbb{R}^{r \times k}\)</span> with rank <span class="math inline">\(r \ll \min(d,k)\)</span>, typically <span class="math inline">\(r = 8\)</span> to <span class="math inline">\(64\)</span>. This decomposition means training far fewer parameters while achieving comparable results to full fine-tuning.</p>
<p><em>Instruction fine-tuning</em> trains the model with examples explicitly showing how it should respond to different queries. The labeled data consists of input-output pairs where inputs convey the desired behavior and outputs represent the expected responses. By exposing the model to a diverse range of instructions and appropriate responses, it learns to generalize across different types of tasks and follow user intentions more reliably.</p>
<p><em>Sequential fine-tuning</em> gradually adapts a model to increasingly specialized tasks. For example, a general AI model could first be fine-tuned for medical terminology, then refined further for pediatric cardiology. This progressive specialization allows the model to build upon previously learned knowledge.</p>
<p><em>Multi-task learning</em> trains the model on datasets containing instructions for various tasks over multiple training cycles. The model learns to balance different objectives without forgetting earlier ones, creating a more versatile system capable of handling diverse requests.</p>
<p>Fine-tuning is often combined with other post-training techniques such as <em>Reinforcement Learning from Human Feedback</em> (RLHF) to align model behavior with human preferences. These advanced techniques are discussed in detail in <a href="#sec-post-training-reasoning" class="quarto-xref"><span>Section 24.7</span></a>.</p>
</section>
<section id="sec-distillation-detail" class="level3">
<h3 class="anchored" data-anchor-id="sec-distillation-detail">Model Distillation: Knowledge Transfer</h3>
<p>As we push the boundaries of model performance with larger parameter counts and complex post-training reasoning chains, a practical challenge emerges: deployment efficiency. Training and deploying models with hundreds of billions of parameters requires considerable computational power, specialized hardware, and substantial energy consumption. Large models also demand significant memory to store their weights and process inputs, making them impractical for many real-world applications. While a massive 175B+ parameter model might offer superior reasoning, running it for every user query is often prohibitively expensive and slow.</p>
<p><em>Model Distillation</em> addresses this challenge by transferring knowledge from a large, complex “teacher” model to a smaller, more efficient “student” model. The student learns to replicate the teacher’s behavior on specific tasks, achieving similar results while being significantly smaller and faster. Distilled models can typically achieve 2–8× faster inference compared to their teacher models, depending on architecture and hardware optimization.</p>
<p>A striking example of distillation’s potential emerged in late 2025, and was documented in <a href="https://vertu.com/lifestyle/gemini-3-flash-outperforms-pro-in-coding-while-pro-suffers-critical-memory-issues/">Virtu article</a>. Google’s Gemini 3 Flash—a model explicitly designed for speed and cost efficiency—outperformed the flagship Gemini 3 Pro on the SWE-bench coding benchmark, achieving 78% compared to Pro’s 76.2%. This inversion of the expected hierarchy was accompanied by widespread reports of Pro exhibiting critical issues: deleting code, losing context mid-conversation, and failing to maintain logical coherence across extended interactions. The phenomenon has been attributed to knowledge distillation, where the compression process that created Flash from Pro inadvertently preserved and even sharpened the most effective coding reasoning pathways while discarding less relevant capabilities. Flash also demonstrated advantages in speed (roughly three times faster) and cost (about 70% cheaper), leading major development tools to adopt it as their preferred model for coding assistance. This case illustrates a broader principle: architectural efficiency and focused optimization can matter more than raw parameter count for specific tasks.</p>
<p>Model distillation, formalized by <span class="citation" data-cites="hinton2015distilling">Hinton, Vinyals, and Dean (<a href="references.html#ref-hinton2015distilling" role="doc-biblioref">2015</a>)</span> in their seminal paper “Distilling the Knowledge in a Neural Network,” is based on a <em>Teacher-Student</em> architecture. The core insight is that a large, pre-trained teacher model has learned much more than just the final answers—it has learned a rich internal representation of the data structure.</p>
<p>When a standard model trains on a “hard” label (e.g., identifying an image as “Dog”), it is penalized if it outputs anything other than 100% confidence in that class. However, a sophisticated teacher model might output probabilities like: <em>Dog: 0.90, Cat: 0.09, Car: 0.0001</em>. The fact that the model thinks the image is 9% likely to be a “Cat” and almost impossible to be a “Car” contains valuable information—it tells us that this specific dog looks somewhat like a cat (perhaps it’s fluffy or small). This similarity information, often called <em>dark knowledge</em>, is lost if we train only on the final hard label.</p>
<p>Distillation trains a smaller student model to mimic these <em>soft targets</em> (the probability distributions) produced by the teacher. By doing so, the student learns how the teacher generalizes, not just what the teacher predicts. Thanks to the level of detail provided in soft targets, the student model can achieve high performance with a smaller amount of data than the original teacher required.</p>
<p>Different distillation approaches focus on transferring different aspects of the teacher’s knowledge:</p>
<p><em>Response-based distillation</em> focuses on having the student mimic the final output layer of the teacher model. The student learns to imitate the teacher’s predictions by minimizing a distillation loss, ensuring it captures the nuanced information present in the teacher’s outputs. This is the most common form of distillation.</p>
<p><em>Feature-based distillation</em> leverages the internal representations or features learned by the teacher in its intermediate layers. Rather than focusing solely on final outputs, this approach encourages the student to match the teacher’s internal activations, learning how the teacher processes information at multiple levels.</p>
<p><em>Relation-based distillation</em> captures and transfers the relationship knowledge between data samples and layers within the neural network. This method complements response-based approaches by encoding how different inputs relate to each other in the teacher’s representation space.</p>
<p>To expose soft probabilities effectively, distillation uses a modified <em>Softmax</em> function with a parameter called <em>Temperature (<span class="math inline">\(T\)</span>)</em>. Standard Softmax (<span class="math inline">\(T=1\)</span>) tends to push probabilities towards 0 or 1, hiding the smaller details. Raising <span class="math inline">\(T\)</span> “softens” the distribution, making smaller class probabilities more prominent and easier for the student to learn from.</p>
<p>The probability <span class="math inline">\(q_i\)</span> for class <span class="math inline">\(i\)</span> is calculated as:</p>
<p><span id="eq-class-prob"><span class="math display">\[
q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
\tag{24.1}\]</span></span></p>
<p>where <span class="math inline">\(z_i\)</span> are the logits (raw outputs) of the model.</p>
<p>The training objective for the student model typically combines two loss functions:</p>
<ol type="1">
<li><em>Distillation Loss (Soft Loss):</em> The Kullback-Leibler (KL) Divergence between the student’s soft predictions and the teacher’s soft targets (both computed at temperature <span class="math inline">\(T\)</span>). KL Divergence measures how one probability distribution differs from a reference distribution.</li>
<li><em>Student Loss (Hard Loss):</em> The standard Cross-Entropy loss between the student’s predictions (at <span class="math inline">\(T=1\)</span>) and the actual ground-truth labels.</li>
</ol>
<p><span class="math display">\[ L = \alpha L_{soft} + (1-\alpha) L_{hard} \]</span></p>
<p>This combined objective forces the student to be accurate on the data (hard loss) while also mimicking the generalization behavior of the teacher (soft loss). The weighting parameter <span class="math inline">\(\alpha\)</span> balances these two objectives.</p>
<p>The following diagram illustrates the distillation pipeline, where the student learns from both the dataset and the teacher’s soft outputs.</p>
<div id="fig-distillation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/knowledge_distillation.jpeg" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.2: Knowledge Distillation Framework Diagram illustrating the process of transferring knowledge from a complex teacher model to a simpler student model.
</figcaption>
</figure>
</div>
<p>Several schemes have been developed to facilitate the distillation process:</p>
<p><em>Offline distillation</em> refers to the traditional approach where the teacher model is trained first, then the student model is trained separately using the soft labels generated by the teacher. This is the most straightforward approach when a well-trained teacher is available.</p>
<p><em>Online distillation</em> is used when a large pre-trained teacher is not available for a given task, or when the teacher model is so large that there is insufficient storage or processing capacity. In this approach, the teacher and student models are trained simultaneously, with the student learning from the teacher dynamically during training.</p>
<p><em>Self-distillation</em> is a variant where a single model acts as both teacher and student. Knowledge is transferred from deeper layers of the network to shallower layers of the same network. This technique can improve model performance and regularization without requiring a separate teacher model. Studies have shown that self-distillation can maintain up to 95% of the teacher’s accuracy while drastically reducing model size and inference time.</p>
<section id="from-theory-to-practice" class="level4">
<h4 class="anchored" data-anchor-id="from-theory-to-practice">From Theory to Practice</h4>
<p>Since the work of <span class="citation" data-cites="hinton2015distilling">Hinton, Vinyals, and Dean (<a href="references.html#ref-hinton2015distilling" role="doc-biblioref">2015</a>)</span>, model distillation has evolved from a theoretical framework into a critical component of modern machine learning infrastructure. The efficacy of this approach was notably demonstrated in the domain of Natural Language Processing by <span class="citation" data-cites="sanh2019distilbert">Sanh et al. (<a href="references.html#ref-sanh2019distilbert" role="doc-biblioref">2019</a>)</span>, whose <em>DistilBERT</em> model retained approximately 97% of the original BERT performance while reducing parameters by 40% and increasing inference speed by 60%. DistilBERT was specifically created to address challenges associated with large pre-trained language models, focusing on computational and memory efficiency.</p>
<p>In contemporary production environments, distillation serves as the bridge between massive “reasoning” models and efficient deployment. Advanced pipelines utilize a teacher-student paradigm where a large-scale model (e.g., a 200B+ parameter reasoning model) generates synthetic data or soft targets. These outputs are subsequently used to fine-tune smaller, cost-effective models (e.g., 8B parameter models) for specific downstream tasks. This methodology, often aligned with techniques like <em>distilling step-by-step</em> <span class="citation" data-cites="hsieh2023distilling">(<a href="references.html#ref-hsieh2023distilling" role="doc-biblioref">Hsieh et al. 2023</a>)</span>, allows for the deployment of models that exhibit high-level reasoning capabilities with significantly reduced computational overhead.</p>
<p>Furthermore, distillation enables the proliferation of <em>Edge AI</em>, permitting sophisticated inference on resource-constrained devices where memory and power budgets preclude the use of full-scale foundation models. Mobile implementations like MobileBERT run efficiently on smartphones, enabling features such as on-device text prediction and voice assistants that give users AI functionality without requiring constant cloud connectivity. By effectively compressing the “dark knowledge” of giant architectures into efficient runtimes, distillation addresses the practical dichotomy between model scale and deployment feasibility.</p>
<p>Major e-commerce and recommendation platforms have applied distillation techniques to improve their systems. For example, privileged feature distillation has been used to enhance recommendation systems, achieving measurable gains in click-through and conversion rates while maintaining reasonable inference costs.</p>
<p>Model distillation offers several key benefits:</p>
<ul>
<li><em>Reduced model size</em>: Enables deployment on devices with limited storage and computational power.</li>
<li><em>Faster inference</em>: Smaller models process data more quickly, reducing response times.</li>
<li><em>Lower resource consumption</em>: Reduces VRAM usage, memory bandwidth, and power consumption.</li>
<li><em>Direct cost reduction</em>: Distilled models require less compute, reducing operational costs.</li>
</ul>
<p>Distillation is most effective when applied after a model has been fully pre-trained or fine-tuned on a specific task. At this stage, the teacher model has already captured rich task-specific knowledge that can be efficiently transferred to the student. Common use cases include:</p>
<ul>
<li><em>Production deployment</em>: Reducing serving costs and meeting hardware constraints before model release</li>
<li><em>Edge and mobile applications</em>: Enabling AI features on resource-constrained devices</li>
<li><em>Budget-conscious inference at scale</em>: Serving thousands of users simultaneously while managing costs</li>
<li><em>Task-specific optimization</em>: Replacing overly large general-purpose models with compact, focused alternatives</li>
</ul>
</section>
</section>
<section id="the-rise-of-small-language-models" class="level3">
<h3 class="anchored" data-anchor-id="the-rise-of-small-language-models">The Rise of Small Language Models</h3>
<p>The success of distillation and efficient fine-tuning has contributed to a broader trend: the rise of <em>Small Language Models</em> (SLMs). These models, typically ranging from a few hundred million to several billion parameters, challenge the assumption that bigger is always better.</p>
<p>SLMs offer compelling advantages for many practical applications:</p>
<ul>
<li>They can run on consumer hardware, including laptops and smartphones</li>
<li>They provide faster inference times, suitable for real-time applications</li>
<li>They consume less energy, reducing both costs and environmental impact</li>
<li>They can be deployed in privacy-sensitive contexts where data cannot leave the device</li>
</ul>
<p>Techniques like distillation, parameter-efficient fine-tuning, and quantization work synergistically to create capable SLMs. A typical workflow might involve: (1) distilling knowledge from a large teacher model, (2) fine-tuning the student on domain-specific data using LoRA, and (3) quantizing the result for efficient deployment. This pipeline democratizes access to advanced AI capabilities, making sophisticated models accessible across diverse platforms and use cases.</p>
<p>As the demand for AI continues to grow, the importance of techniques that balance capability with efficiency will only increase. The future of LLM deployment lies not just in scaling up, but in the intelligent compression and adaptation of powerful models for practical use.</p>
</section>
</section>
<section id="evaluating-model-performance" class="level2" data-number="24.6">
<h2 data-number="24.6" class="anchored" data-anchor-id="evaluating-model-performance"><span class="header-section-number">24.6</span> Evaluating Model Performance</h2>
<p>When evaluating models, researchers and practitioners rely on various benchmarks that test different aspects of language understanding and generation. The field has evolved significantly as earlier benchmarks became saturated—with top models achieving near-perfect scores—and concerns about test set contamination grew. Modern evaluation suites now emphasize more challenging reasoning tasks and dynamic, contamination-resistant designs.</p>
<p>For complex reasoning, <a href="https://arxiv.org/abs/2311.12022">GPQA Diamond</a> presents graduate-level questions in biology, physics, and chemistry that challenge even domain experts, while <a href="https://arcprize.org/">ARC-AGI</a> measures abstract reasoning capabilities that approach the boundaries of general intelligence. Mathematical prowess is tested through competition-level problems from the <a href="https://maa.org/student-programs/amc/">AIME</a> (American Invitational Mathematics Examination), where top models now achieve scores that would qualify for elite high school competitions.</p>
<p>Practical software engineering capabilities are evaluated via <a href="https://www.swebench.com/">SWE-Bench</a>, which tasks models with resolving real GitHub issues from popular open-source repositories—a far more realistic test than generating isolated code snippets. For multimodal understanding, MMMU-Pro extends earlier benchmarks with challenging questions requiring joint reasoning over text and images. <a href="#fig-swe-bench-chart" class="quarto-xref">Figure&nbsp;<span>24.3</span></a> shows that as of December 2025, the most cost-effective models for software engineering are Claude 4/4.5 Opus and GPT-5.1-codex.</p>
<div id="fig-swe-bench-chart" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-swe-bench-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/swe-bench-chart-2025-12-29.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-swe-bench-chart-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.3: Cumulative cost distribution for solving SWE-Bench issues across leading models (January 2026).
</figcaption>
</figure>
</div>
<p>To combat benchmark contamination—where models may have memorized test questions during training—dynamic benchmarks like <a href="https://livebench.ai/">LiveBench</a> continuously refresh their question sets using recent math competitions, newly published papers, and current news articles. Perhaps most ambitiously, <a href="https://lastexam.ai/">Humanity’s Last Exam</a> crowdsources extremely difficult questions from domain experts across fields, explicitly designed to resist easy saturation.</p>
<div id="fig-hle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hle-jan26.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.4: HLE as of January 2026.
</figcaption>
</figure>
</div>
<p>Our personal favorite resource to keep track of the latest and greatest models is <a href="https://lmarena.ai">LMArena</a> (formerly Chatbot Arena). Rather than relying on static test sets, LMArena uses a community-driven approach where users compare model outputs head-to-head in blind evaluations. Participants see responses from two anonymous models to the same prompt and select the one they prefer. These pairwise comparisons are then aggregated using an Elo rating system—the same method used to rank chess players. When a model wins a comparison, its score increases; when it loses, its score decreases. The magnitude of each adjustment depends on the expected outcome: defeating a higher-rated model yields a larger score boost than beating a lower-rated one. This dynamic system continuously adapts as new votes accumulate, providing a real-time reflection of collective user preferences that complements traditional benchmark evaluations. For example, for the WebDev category, the LMArena ranks Claude 4/4.5 in the top three along with GPT-5.2. This is a similar ranking we saw in SWE-Bench.</p>
<div id="fig-lmarena-leaderboard" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lmarena-leaderboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/lmarena.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lmarena-leaderboard-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.5: LMArena leaderboard for WebDev category as of January 2026.
</figcaption>
</figure>
</div>
<p>However, benchmarks have limitations and don’t always reflect real-world performance. A model that excels at multiple-choice questions might struggle with open-ended creative tasks. Code generation benchmarks might not capture the nuanced requirements of your specific programming domain. The key is to use benchmarks as a starting point while conducting thorough validation using data that closely resembles your actual use case.</p>
<p>Consider implementing your own evaluation framework that tests the specific capabilities you need. If you’re building a customer service chatbot, create test scenarios that reflect your actual customer interactions. If you’re developing a creative writing assistant, evaluate the model’s ability to generate diverse, engaging content in your target style or genre.</p>
</section>
<section id="sec-post-training-reasoning" class="level2" data-number="24.7">
<h2 data-number="24.7" class="anchored" data-anchor-id="sec-post-training-reasoning"><span class="header-section-number">24.7</span> Post-training Techniques</h2>
<p>While LLMs excel at predicting the next token in a sequence, their true potential emerges through post-training techniques that teach them to reason, think step-by-step, and align with human expectations. When we think about improving LLMs’ skills, our focus often centers on aspects such as improved grammar or more natural-sounding responses. But what sets a helpful LLM apart is its ability to reason. This involves thinking through problems, breaking them down into steps, making informed decisions, and explaining how it arrived at an answer. Reasoning takes next-token prediction to the next level by adding logic, structure, and goal-oriented thinking.</p>
<p>Without strong reasoning skills, models often skip steps, make confident but incorrect claims (hallucinations), or struggle with tasks that require planning or logic. For any organization, this creates a significant risk, undermining user trust and leading to unreliable outcomes. The good news is that we can improve reasoning with the right techniques and upgrade a pre-trained LLM with broad knowledge into a valuable tool for real-world tasks that aligns with users’ needs.</p>
<p>Post-training refines a model’s capabilities, teaching it to move beyond simply predicting the next word. This means moving past the first plausible answer and compelling the model to build a more deliberate, logical response. It learns to break down a task, reflect on its outputs, and consult external tools—mimicking a more methodical, human-like reasoning process. This is how we upgrade a generalist LLM into a specialized tool that is more accurate, trustworthy, and aligned with specific business goals.</p>
<p>One form of reasoning involves combining independent facts to arrive at an answer, rather than simply regurgitating memorized information. For example, when asked, “What is the capital of the state where Dallas is located?” a model could just recall “Austin” if it has seen that exact question before. However, a deeper level of reasoning is at play. Interpretability research reveals that models like Claude first activate concepts representing “Dallas is in Texas” and then connect this to another concept, “the capital of Texas is Austin.” This demonstrates the ability to perform multi-step reasoning by chaining together different pieces of knowledge.</p>
<div id="fig-claude-reasoning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-claude-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/claude_reasoning.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-claude-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.6: Multi-step Reasoning: <a href="https://www.anthropic.com/news/tracing-thoughts-language-model">Anthropic</a>
</figcaption>
</figure>
</div>
<p>This multi-step reasoning process can be visualized as follows:</p>
<div id="fig-cot-reasoning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cot-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chain_of_thought_diagram.jpeg" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cot-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.7: Chain of Thought Reasoning process showing the activation of intermediate concepts.
</figcaption>
</figure>
</div>
<p>This capability can be tested by intervening in the model’s thought process. For instance, if the “Texas” concept is artificially replaced with “California,” the model’s output correctly changes from “Austin” to “Sacramento,” confirming that it is genuinely using the intermediate step to determine its final answer. This ability to combine facts is a crucial component of advanced reasoning.</p>
<p>The landscape of post-training methods used to boost the reasoning abilities of pre-trained LLMs is rich and varied. These techniques build on the model’s existing knowledge, teaching it to follow instructions more effectively and use tools or feedback to refine its answers. Each method adds a new layer of skill, whether it involves breaking down problems, learning from feedback, or drawing on real-world information, all to bridge the model’s reasoning with the human thought process.</p>
<p>However, it’s crucial to understand that even when a model produces a step-by-step “chain of thought,” (CoT) it may not be a faithful representation of its actual reasoning process. Recent research from Anthropic explores this very question, revealing a complex picture: sometimes the reasoning is faithful, and sometimes it’s fabricated to fit a pre-determined conclusion.</p>
<p>When a model is tasked with a problem it can solve, like finding the square root of 0.64, interpretability tools show that it follows a logical path, activating concepts for intermediate steps (like the square root of 64) before reaching the final answer. However, when presented with a difficult problem and an incorrect hint, the model can engage in what researchers call “motivated reasoning.” It starts with the incorrect answer and works backward, creating a believable but entirely fake sequence of steps to justify its conclusion. This ability to generate a plausible argument for a foregone conclusion without regard for truth is a critical limitation. These interpretability techniques offer a way to “catch the model in the act” of faking its reasoning, providing a powerful tool for auditing AI systems.</p>
<p>LLMs were not originally designed to function as calculators; they were trained on text data and lack built-in mathematical algorithms. Yet, they can perform addition tasks, like calculating 36+59, seemingly without explicitly writing out each step. How does a model, primarily trained to predict the next word in a sequence, manage to perform such calculations?</p>
<p>One might speculate that the model has memorized extensive addition tables, allowing it to recall the answer to any sum present in its training data. Alternatively, it could be using traditional longhand addition methods similar to those taught in schools.</p>
<p>However, research reveals that Claude, a specific LLM, utilizes multiple computational strategies simultaneously. One strategy estimates an approximate answer, while another precisely calculates the last digit of the sum. These strategies interact and integrate to produce the final result. While addition is a straightforward task, analyzing how it is executed at this granular level—through a combination of approximate and precise methods—can provide insights into how Claude approaches more complex problems.</p>
<div id="fig-claude-mental-math" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-claude-mental-math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/claude-mental-math.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-claude-mental-math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.8: Mental Math Problem Solving: <a href="https://www.anthropic.com/news/tracing-thoughts-language-model">Anthropic</a>
</figcaption>
</figure>
</div>
<p>Models like Claude 3.7 Sonnet can “think out loud,” often improving answer quality, but sometimes misleading with fabricated reasoning. This “faked” reasoning can be convincing, posing reliability challenges. Interpretability helps distinguish genuine reasoning from false.</p>
<p>For instance, Claude accurately computes the square root of 0.64, showing a clear thought process. However, when tasked with finding the cosine of a large number, it may fabricate steps. Additionally, when given a hint, Claude may reverse-engineer steps to fit a target, demonstrating motivated reasoning.</p>
<div id="fig-claude-false-reasoning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-claude-false-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/claude-multistep-reasoning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-claude-false-reasoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.9: False Reasoning: <a href="https://www.anthropic.com/news/tracing-thoughts-language-model">Anthropic</a>
</figcaption>
</figure>
</div>
<p>This highlights that a model’s explanation of its thought process can’t always be trusted. For high-stakes applications, being able to verify the internal reasoning process, rather than just accepting the output, is essential for building reliable and trustworthy AI.</p>
<p>Instruction Fine-Tuning (IFT) represents perhaps the most fundamental approach to improving model reasoning. The core idea involves taking a pre-trained model and running a second pass of supervised learning on mini-lessons, each formed as a triple of instruction, input, and answer.</p>
<p>Consider a math word problem where the instruction asks to <code>solve this math word problem step by step</code>, the input presents <code>Sarah has 12 apples and gives away 5. How many does she have left?</code>, and the answer provides</p>
<pre><code>Step 1: Start with 12 apples. 
Step 2: Subtract 5 apples given away. 
Step 3: 12 - 5 = 7 apples remaining. </code></pre>
<p>Each training example teaches the model how to transform a task description into the steps that solve it <span class="citation" data-cites="chung2022scaling">(<a href="references.html#ref-chung2022scaling" role="doc-biblioref">Chung et al. 2022</a>)</span>. After thousands of such drills, the model learns many small skills and when to switch among them. The steady practice trains it to deliver precise answers that match the instruction rather than sliding into a generic reply. Empirical evidence demonstrates the power of this approach: Flan UPaLM 540B, a variant of the UPaLM model fine-tuned with instruction-based tasks, significantly outperformed the original UPaLM 540B model. UPaLM stands for Unified Pre-trained Language Model, which is a large-scale language model designed to handle a wide range of tasks. The Flan UPaLM 540B was evaluated across four benchmarks: MMLU (Massive Multitask Language Understanding), which tests the model’s ability to handle a variety of academic subjects; BBH (Big-Bench Hard), a set of challenging tasks designed to push the limits of language models; TyDiQA (Typologically Diverse Question Answering), which assesses the model’s performance in answering questions across diverse languages; and MGSM (Mathematics Grade School Math), which evaluates the model’s capability in solving grade school-level math problems. The Flan UPaLM 540B showed an average improvement of 8.9% over the original model across these benchmarks.</p>
<p>Domain-Specific Supervised Fine-Tuning takes the IFT principle and applies it within specialized fields. This approach restricts the training corpus to one technical field, such as medicine, law, or finance, saturating the model weights with specialist concepts and rules. Fine-tuning on domain-specific data enables the model to absorb the field’s vocabulary and structural rules, providing it with direct access to specialized concepts that were scarce during pre-training. The model can quickly rule out answers that do not make sense and narrow the search space it explores while reasoning. Mastering a domain requires data that captures its unique complexity, utilizing domain-specific examples, human-labeled edge cases, and diverse training data generated through hybrid pipelines combining human judgment and AI. This process enhances the model’s ability to follow complex instructions, reason across modalities and languages, and avoid common pitfalls like hallucination. The effectiveness of this approach is striking: in ICD-10 coding, domain SFT catapulted exact-code accuracy from less than 1% to approximately 97% on standard ICD coding (including linguistic and lexical variations) and to 69% on real clinical notes <span class="citation" data-cites="hou2025enhancing">(<a href="references.html#ref-hou2025enhancing" role="doc-biblioref">Hou et al. 2025</a>)</span>.</p>
<section id="chain-of-thought-and-chain-of-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="chain-of-thought-and-chain-of-reasoning">Chain-of-Thought and Chain of Reasoning</h3>
<p>Chain-based reasoning techniques represent some of the most powerful tools for improving LLM reasoning capabilities. These approaches guide models to break down complex problems into manageable steps, explore multiple solution paths, and learn from their mistakes—mirroring the deliberate problem-solving strategies humans employ when tackling difficult tasks.</p>
<p>Chain-of-Thought <span class="citation" data-cites="wei2023chainofthought">(<a href="references.html#ref-wei2023chainofthought" role="doc-biblioref">Wei et al. 2023</a>)</span> prompting offers a remarkably simple yet powerful technique that requires no model retraining. The approach involves showing the model a worked example that spells out every intermediate step, then asking it to “think step by step.” Writing the solution step by step forces the model to reveal its hidden reasoning, making it more likely for logically necessary tokens to appear. Because each step is generated one at a time, the model can inspect its own progress and fix contradictions on the fly. The empirical results are impressive: giving PaLM 540B eight CoT examples improved its accuracy on GSM8K from 18% to 57%. This improvement came entirely from a better prompt, with no changes to the model’s weights.</p>
<p>Tree-of-Thought extends the chain-of-thought concept by allowing exploration of multiple reasoning paths simultaneously. Instead of following one chain, this method lets the model branch into multiple reasoning paths, score partial solutions, and expand on the ones that look promising. Deliberate exploration stops the first plausible idea from dominating. ToT lets the model test several lines of reasoning instead of locking onto one. When a branch hits a dead end, it can backtrack to an earlier step and try another idea, something a plain CoT cannot do. The model operates in a deliberate loop: propose, evaluate, and explore. This approach resembles a CEO evaluating multiple business strategies, modeling several potential outcomes before committing to the most promising one, preventing over-investment in a flawed initial idea. This principle has been applied in projects to improve coding agents focused on generating pull requests for repository maintenance and bug-fixing tasks across multiple programming languages. Researchers have analyzed thousands of coding agent trajectories, evaluating each interaction step-by-step to provide more explicit guidance to the models, enabling them to make better decisions on real coding tasks. In the “Game of 24” puzzle, GPT-4 combined with CoT reasoning solved only 4% of the puzzles, but replacing it with ToT raised the success rate to 74% <span class="citation" data-cites="yao2023tree">(<a href="references.html#ref-yao2023tree" role="doc-biblioref">Yao et al. 2023</a>)</span>.</p>
</section>
<section id="reflexion" class="level3">
<h3 class="anchored" data-anchor-id="reflexion">Reflexion</h3>
<p>Reflexion <span class="citation" data-cites="shinn2023reflexion">(<a href="references.html#ref-shinn2023reflexion" role="doc-biblioref">Shinn et al. 2023</a>)</span> introduces a self-improvement mechanism that operates through iterative feedback. After each attempt, the model writes a short reflection on what went wrong or could be improved. That remark is stored in memory and included in the next prompt, giving the model a chance to revise its approach on the next try. Reflexion turns simple pass/fail signals into meaningful feedback that the model can understand and act on. By reading its own critique before trying again, the model gains short-term memory and avoids repeating past mistakes. This self-monitoring loop of try, reflect, revise guides the model toward better reasoning without changing its weights. Over time, it helps the model adjust its thinking more like a human would, by learning from past mistakes and trying again with a better plan. A GPT-4 agent using Reflexion raised its success rate from 80% to 91% on the HumanEval coding dataset.</p>
</section>
<section id="non-linear-reasoning-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="non-linear-reasoning-capabilities">Non-Linear Reasoning Capabilities</h3>
<p>Recent advances in LLM reasoning have focused on establishing these non-linear capabilities, moving beyond simple chain-of-thought prompting to more sophisticated reasoning architectures. These approaches recognize that human reasoning is rarely linear—we backtrack when we realize we’ve made an error, we consider multiple possibilities in parallel, and we iteratively refine our understanding as we gather more information.</p>
<p>One promising direction is iterative reasoning, where models are allowed to revise their intermediate steps based on feedback or self-evaluation. Unlike traditional autoregressive generation where each token is final once generated, iterative approaches allow the model to revisit and modify earlier parts of its reasoning chain. This might involve generating an initial solution, evaluating it for consistency, and then revising specific steps that appear problematic.</p>
<p>A compelling example of how extended thinking improves reasoning capabilities can be seen in mathematical problem-solving performance. When Claude 3.7 Sonnet was given more computational budget to “think” through problems on the American Invitational Mathematics Examination (AIME) 2024, its accuracy improved logarithmically with the number of thinking tokens allocated. This demonstrates that allowing models more time for internal reasoning—similar to how humans perform better on complex problems when given more time to think—can lead to substantial performance gains.</p>
<div id="fig-aime-2024" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-aime-2024-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/aime-2024-performance.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-aime-2024-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.10: AIME 2024 performance vs.&nbsp;actual thinking token usage
</figcaption>
</figure>
</div>
<p><a href="#fig-aime-2024" class="quarto-xref">Figure&nbsp;<span>24.10</span></a> shows Claude 3.7 Sonnet’s performance on the 2024 American Invitational Mathematics Examination improving logarithmically with the number of thinking tokens used per problem. The model generally uses fewer tokens than the maximum budget allocated, suggesting it adaptively determines when sufficient reasoning has been applied. Source: <a href="https://www.anthropic.com/news/visible-extended-thinking">Anthropic’s Visible Extended Thinking</a>.</p>
<p>Parallel hypothesis generation represents another departure from linear reasoning. Instead of committing to a single reasoning path, these approaches generate multiple competing explanations or solutions simultaneously. The model can then evaluate these alternatives, potentially combining insights from different paths or selecting the most promising direction based on evidence accumulation.</p>
<p>Dynamic tool selection and reasoning takes this further by allowing models to adaptively choose which reasoning strategies or external tools to employ based on the specific demands of the current problem. Rather than following a predetermined sequence of operations, the model can dynamically decide whether to retrieve external information, perform symbolic computation, or engage in pure logical reasoning based on the current state of the problem.</p>
<p>These non-linear reasoning capabilities are particularly important for complex problem-solving scenarios where the optimal approach isn’t clear from the outset. In scientific reasoning, for example, a hypothesis might need to be revised as new evidence emerges. In mathematical problem-solving, an initial approach might prove intractable, requiring a fundamental shift in strategy. In code generation, debugging often requires jumping between different levels of abstraction and considering multiple potential sources of error.</p>
<p>The implementation of non-linear reasoning often involves sophisticated orchestration between multiple model calls, external tools, and feedback mechanisms. This represents a shift from viewing LLMs as simple text generators to understanding them as components in more complex reasoning systems. As these capabilities mature, we can expect to see LLMs that not only generate human-like text but also exhibit more human-like reasoning patterns—flexible, adaptive, and capable of handling ambiguity and uncertainty with greater finesse.</p>
<p>Reinforcement Learning from Human Feedback (RLHF) represents a sophisticated approach to aligning model behavior with human preferences. The process involves taking a pre-trained model and generating several answers for real user prompts. Human reviewers rank those answers, a reward model learns these rankings, and the main model is updated to score higher on that reward. This loop optimizes the model to produce outputs humans prefer rather than those that merely score well on next-token likelihood. Because humans reward answers that are complete, fact-checked, and well-explained, the model learns to value clear logic over quick guesses. Each reinforcement learning step trains it to produce responses that follow instructions, chain ideas coherently, and avoid unsupported claims, aligning its internal decision-making with human expectations. Safety and alignment research relies heavily on these techniques; Constitutional AI approaches, for instance, attempt to instill models with explicit principles and values during this phase. In the original InstructGPT study by <span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="references.html#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>, annotators preferred answers from the 175B RLHF-tuned model over the same-size GPT-3 baseline 85% of the time. Even the 1.3B RLHF model outperformed the baseline, despite having 100 times fewer parameters.</p>
<p>Chain-of-Action (CoA) <span class="citation" data-cites="pan2025chainofaction">(<a href="references.html#ref-pan2025chainofaction" role="doc-biblioref">Pan et al. 2025</a>)</span> represents the most sophisticated integration of reasoning and external tool use. This approach decomposes a complex query into a reasoning chain interleaved with tool calls such as web search, database lookup, or image retrieval that are executed on the fly and fed into the next thought. Each action grounds the chain in verified facts. By using up-to-date information and multi-reference faith scores, the model can remain grounded and make more informed decisions, even when sources disagree. Because it can plug in different tools as needed, it’s able to take on more complex tasks that require different data modalities. CoA outperformed the leading CoT and RAG baselines by approximately 6% on multimodal QA benchmarks, particularly on compositional questions that need both retrieval and reasoning.</p>
</section>
</section>
<section id="data-quality-and-quantity" class="level2" data-number="24.8">
<h2 data-number="24.8" class="anchored" data-anchor-id="data-quality-and-quantity"><span class="header-section-number">24.8</span> Data quality and quantity</h2>
<p>One might assume that training an LLM for non-linear reasoning would require tremendous amounts of data, but recent research reveals that data quality can compensate for limited quantity. This finding has significant implications for organizations looking to develop reasoning-capable models without massive data collection efforts.</p>
<p>Two compelling examples demonstrate this principle. The S1 research <span class="citation" data-cites="yang2025qwen2">(<a href="references.html#ref-yang2025qwen2" role="doc-biblioref">Yang et al. 2025</a>)</span> fine-tuned their base model, Qwen2.5-32B-Instruct, on only 1,000 high-quality reasoning examples, yet achieved remarkable performance improvements. Their data collection process was methodical: they started with 59,029 questions from 16 diverse sources (including many Olympiad problems), generated reasoning traces using Google Gemini Flash Thinking API through distillation, then applied rigorous filtering. Problems were first filtered by quality (removing poor formatting), then by difficulty—a problem was deemed difficult if neither Qwen2.5-7B-Instruct nor Qwen2.5-32B-Instruct could solve it, and the reasoning length was substantial. Finally, 1,000 problems were sampled strategically across various topics.</p>
<p>Similarly, the LIMO (Less is More for Reasoning) research <span class="citation" data-cites="ye2025limo">(<a href="references.html#ref-ye2025limo" role="doc-biblioref">Ye et al. 2025</a>)</span> demonstrated that quality trumps quantity. Taking NuminaMath as a base model, they fine-tuned it on merely 817 high-quality curated training samples to achieve impressive mathematical performance with exceptional out-of-distribution generalization. Their results were striking enough to warrant comparison with OpenAI’s o1 model.</p>
<p>For high-quality non-linear reasoning data, LIMO proposes three essential guidelines:</p>
<p><em>Structured Organization</em>: Tokens are allocated to individual “thoughts” according to their importance and complexity, with more tokens devoted to key reasoning points while keeping simpler steps concise. This mirrors how human experts organize their thinking—spending more time on difficult concepts and moving quickly through routine steps.</p>
<p><em>Cognitive Scaffolding</em>: Concepts are introduced strategically, with careful bridging of gaps to make complex reasoning more accessible. Rather than jumping directly to advanced concepts, the reasoning process builds understanding step by step, similar to how effective teachers structure lessons.</p>
<p><em>Rigorous Verification</em>: Intermediate results and assumptions are frequently checked, and logical consistency is ensured throughout the reasoning chain. This is especially important given the risk of hallucinations in complex reasoning tasks.</p>
<p>The verification aspect deserves special attention. The rStar-Math research <span class="citation" data-cites="guan2025rstarmath">(<a href="references.html#ref-guan2025rstarmath" role="doc-biblioref">Guan et al. 2025</a>)</span> offers an innovative approach by training their LLM to produce solutions as Python code with text as code comments. This format allows for automatic verification—the code can be executed to check correctness, providing immediate feedback on the reasoning process. With agentic capabilities, this approach could create a feedback loop where the LLM learns from its execution results.</p>
<p>These findings suggest that the path to better reasoning capabilities lies not in simply collecting more data, but in curating datasets that exemplify the structured, scaffolded, and verified thinking patterns we want models to learn. This approach makes advanced reasoning capabilities more accessible to organizations that may not have access to massive datasets but can invest in creating high-quality training examples.</p>
<div id="fig-rstar-math" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rstar-math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/rstar-math.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rstar-math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.11: An example of Code-augmented CoT Figure from RStar-Math <span class="citation" data-cites="guan2025rstarmath">Guan et al. (<a href="references.html#ref-guan2025rstarmath" role="doc-biblioref">2025</a>)</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-rstar-math" class="quarto-xref">Figure&nbsp;<span>24.11</span></a> shows how rStar-Math integrates code execution with reasoning by formatting solutions as Python code with explanatory comments. This approach allows for automatic verification of intermediate steps, creating a feedback loop where the model can learn from execution results and catch errors in real-time. The figure demonstrates how mathematical reasoning can be made more reliable by grounding abstract concepts in executable code.</p>
</section>
<section id="sec-context-engineering" class="level2" data-number="24.9">
<h2 data-number="24.9" class="anchored" data-anchor-id="sec-context-engineering"><span class="header-section-number">24.9</span> Dealing with Context Window Limitations: Context Engineering</h2>
<p>Consider this paradox: GPT-4 Turbo advertises a 128,000-token context window—enough to process an entire novel—yet production systems routinely degrade as context fills. Response times spike, accuracy drops, and costs spiral. The promise of million-token windows collides with the reality of quadratic attention complexity and the finding that models often ignore information buried in the middle of long prompts.</p>
<p>Large language models are stateless: they generate output from input, then forget everything. The <em>context window</em>—the maximum tokens processed in one forward pass—defines what the model can “see.” Everything must fit: system prompt, conversation history, retrieved documents, examples, and the query itself.</p>
<p>Context window management couples three concerns that production systems care about:</p>
<ul>
<li><em>Cost</em>: More tokens means more spend</li>
<li><em>Latency</em>: More tokens means more compute and slower responses</li>
<li><em>Accuracy</em>: More tokens can help, but long prompts dilute signal and introduce failure modes</li>
</ul>
<p>This creates a fundamental trade-off. You can pack in more conversation history and more documents, but the model often becomes slower and less reliable. The goal is not maximum context but the <em>right</em> context, assembled under a fixed budget.</p>
<p>A useful mental model treats the context window as a budget:</p>
<p><span class="math display">\[
B \approx S + T + H + R + U
\]</span></p>
<p>where <span class="math inline">\(B\)</span> is the usable token budget, <span class="math inline">\(S\)</span> is system instructions, <span class="math inline">\(T\)</span> is tool schemas, <span class="math inline">\(H\)</span> is conversation history, <span class="math inline">\(R\)</span> is retrieved evidence, and <span class="math inline">\(U\)</span> is the user’s current input. Context engineering is deciding what to keep, what to compress, and what to retrieve—so that <span class="math inline">\(R\)</span> contains <em>evidence</em> rather than redundancy.</p>
<p>Many visible quality improvements in LLM applications come from better context management rather than larger parameter counts: retrieval that finds the right passage, reranking that removes near-misses, compression that preserves key facts, caching that avoids resending static instructions, and memory that keeps multi-turn workflows coherent. You can often get a large fraction of the value of a bigger model by improving the pipeline that feeds it.</p>
<section id="the-lost-in-the-middle-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-lost-in-the-middle-problem">The Lost-in-the-Middle Problem</h3>
<p>Before diving into solutions, it’s worth understanding the failure mode that dominates production systems. Large language models exhibit a characteristic U-shaped attention curve: they attend strongly to the beginning and end of their context but underweight information in the middle. Research confirms documents positioned in the middle of a long context receive 20–40% less attention than documents at the boundaries <span class="citation" data-cites="liu2023lost">(<a href="references.html#ref-liu2023lost" role="doc-biblioref">N. F. Liu et al. 2023</a>)</span>.</p>
<p>This architectural bias has practical consequences. If you retrieve 15 document chunks and the most relevant one lands in position 7, the model may effectively ignore it. The fix is counterintuitive: don’t fight the attention bias—exploit it by placing the most relevant documents at the start and end.</p>
</section>
<section id="when-to-use-long-context-vs.-rag-vs.-summarization" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-long-context-vs.-rag-vs.-summarization">When to Use Long Context vs.&nbsp;RAG vs.&nbsp;Summarization</h3>
<p>Before diving into specific techniques, choose the right strategy for your use case:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 32%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Strategy</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Document fits in window; need holistic reasoning</td>
<td>Stuff entire document</td>
<td>Cross-references and global structure preserved</td>
</tr>
<tr class="even">
<td>Corpus larger than window; factual lookup</td>
<td>RAG with small chunks</td>
<td>Precision matters more than narrative</td>
</tr>
<tr class="odd">
<td>Corpus larger than window; synthesis task</td>
<td>RAG with hierarchical retrieval</td>
<td>Need both global context and specific evidence</td>
</tr>
<tr class="even">
<td>Long conversation history</td>
<td>Tiered memory + summarization</td>
<td>Keep recent turns verbatim; compress older context</td>
</tr>
<tr class="odd">
<td>Repetitive prompt structure</td>
<td>Caching + compression</td>
<td>Avoid paying to reprocess static content</td>
</tr>
</tbody>
</table>
<p>The decision often comes down to whether the task requires <em>holistic reasoning</em> (favor long context) or <em>precise evidence retrieval</em> (favor RAG). Many production systems combine both: retrieve evidence via RAG, then use the full context window for reasoning over that evidence.</p>
</section>
<section id="attention-efficiency-flashattention-and-ring-attention" class="level3">
<h3 class="anchored" data-anchor-id="attention-efficiency-flashattention-and-ring-attention">Attention Efficiency: FlashAttention and Ring Attention</h3>
<p>Transformers rely on attention, and naive attention scales quadratically with sequence length (<span class="math inline">\(O(n^2)\)</span>). Even with optimizations, long prompts increase memory traffic and reduce throughput.</p>
<p><em>FlashAttention</em> addresses this by fusing attention computation into a single kernel that keeps intermediate results in fast on-chip SRAM rather than writing large <span class="math inline">\(N \times N\)</span> matrices to slow GPU memory. This reduces memory requirements from <span class="math inline">\(O(N^2)\)</span> to <span class="math inline">\(O(N)\)</span>. FlashAttention-3, optimized for NVIDIA H100 architecture, achieves up to 740 TFLOPs/s and enables 16,000-token contexts on 10 GB of VRAM <span class="citation" data-cites="dao2023flashattention2">(<a href="references.html#ref-dao2023flashattention2" role="doc-biblioref">Dao 2023</a>)</span>.</p>
<p>For multi-million token applications, <em>Ring Attention</em> distributes computation across GPU clusters. Query, key, and value tensors are split into chunks, with each GPU computing attention for its local chunk while exchanging states in a ring pattern. Context window size scales linearly with cluster size <span class="citation" data-cites="liu2023ring">(<a href="references.html#ref-liu2023ring" role="doc-biblioref">H. Liu, Zaharia, and Abbeel 2023</a>)</span>.</p>
<p>These optimizations are essential infrastructure, but they don’t eliminate the fundamental constraint: treat tokens as a scarce resource and build systems that spend them on evidence rather than redundancy.</p>
</section>
<section id="retrieval-augmented-generation" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmented-generation">Retrieval-Augmented Generation</h3>
<p>Retrieval-Augmented Generation (RAG) addresses one of the fundamental limitations of LLMs: their reliance on knowledge encoded during training. Before answering, a retriever grabs documents or information relevant to the query and injects them into the context window so the model can reason over fresh evidence. RAG grounds the model in verifiable facts, drastically reducing hallucinations and improving user trust. Instead of relying on potentially outdated or incorrect memorized knowledge, the model reasons over fresh, injected evidence.</p>
<p>The RAG process works as follows: the user’s query is redirected to an embedding model, where it is converted into a numeric form; these embeddings are then compared with a knowledge base; the embedding model locates relevant data; the retrieved information is integrated into the prompt for the LLM as additional context; and finally, the output, combining both the retrieved information and the original prompt, is submitted to the user. This approach resembles a lawyer building an argument not from memory, but by citing specific, relevant legal precedents directly in court.</p>
<p>The integration of RAG into an enterprise workflow-generation system reduced the rate of hallucinated steps and tables from 21% to 7.5% when evaluated on the HumanEval benchmark <span class="citation" data-cites="ayala2024reducing">(<a href="references.html#ref-ayala2024reducing" role="doc-biblioref">Ayala and Bechard 2024</a>)</span>. In real-world applications enhancing LLMs’ multilingual reasoning, RAG has been used to feed models verified, multilingual documents at inference time. The results show that models can answer complex questions in multiple languages, citing specific evidence from retrieved text. Every factual claim becomes traceable, eliminating guesswork and demonstrating a consistent, grounded reasoning process across languages.</p>
<p>Rather than expanding what fits in context, RAG dynamically selects what belongs there. The core insight: for most queries, only a fraction of a knowledge base is relevant. RAG retrieves that fraction on demand.</p>
<p>A basic RAG pipeline:</p>
<ol type="1">
<li><em>Index</em>: Split documents into chunks, embed into vectors, store in vector database</li>
<li><em>Retrieve</em>: Given a query, find similar chunks via embedding similarity</li>
<li><em>Generate</em>: Concatenate retrieved chunks with the query and prompt the LLM</li>
</ol>
<div class="cell" data-fig-width="6" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  D[Documents] --&gt; C[Chunk &amp; Embed]
  C --&gt; V[(Vector DB)]
  Q[Query] --&gt; E[Embed]
  E --&gt; V
  V --&gt;|top-k| R[Retrieved chunks]
  R --&gt; P[Prompt + Context]
  Q --&gt; P
  P --&gt; L[LLM]
  L --&gt; A[Answer]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This architecture allows models to access knowledge bases orders of magnitude larger than any context window.</p>
<section id="rag-versus-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="rag-versus-fine-tuning">RAG versus Fine-tuning</h4>
<p>RAG and fine-tuning (discussed in <a href="#sec-fine-tuning" class="quarto-xref"><span>Section 24.5.2</span></a>) represent two distinct strategies for adapting LLMs to specific applications. While fine-tuning modifies the model’s internal parameters based on domain-specific data, RAG retrieves relevant information from external knowledge bases and incorporates it into the prompt at inference time. In RAG, the model’s parameters remain unchanged during the retrieval process.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div id="rag-col" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><strong>RAG</strong> is particularly useful when:</p>
<ul>
<li>Information evolves frequently or is constantly changing</li>
<li>Labeled data is insufficient or unavailable</li>
<li>Access to external, up-to-date sources is required</li>
</ul>
</div>
<div id="finetune-col" class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><strong>Fine-tuning</strong> is more appropriate when:</p>
<ul>
<li>Direct control over model behavior is desired</li>
<li>Labeled data is available</li>
<li>The same pre-trained model needs to be adapted for multiple specific tasks</li>
<li>Compliance standards or ethical guidelines must be strictly enforced</li>
</ul>
</div>
</div>
</div>
<p>In many cases, combining both approaches yields optimal results: fine-tuning provides the foundational domain adaptation while RAG supplies dynamic, current information at inference time. This hybrid approach, sometimes called <em>Retrieval-Augmented Fine-Tuning</em> (RAFT), trains models to effectively use retrieved context while maintaining domain expertise. RAFT extends RAG by fine-tuning the model on tasks that explicitly require using retrieved documents, teaching it to better incorporate external information into its reasoning process.</p>
</section>
</section>
<section id="advanced-rag-variants" class="level3">
<h3 class="anchored" data-anchor-id="advanced-rag-variants">Advanced RAG Variants</h3>
<p>The basic RAG pipeline admits many refinements. Fixed-size chunking (every 512 tokens) often severs sentences mid-thought. <em>Semantic chunking</em> uses embeddings to find natural break points. The Max–Min algorithm embeds sentences sequentially, comparing each to the current chunk. If similarity exceeds a threshold, the sentence joins; otherwise, a new chunk begins. No universal optimal chunk size exists: 64–128 tokens suit factual lookup; 512–1,024 tokens suit narrative tasks. Chroma’s ClusterSemanticChunker uses dynamic programming to maximize within-chunk similarity, improving retrieval precision by 8–15% over greedy methods <span class="citation" data-cites="chroma2024chunking">(<a href="references.html#ref-chroma2024chunking" role="doc-biblioref">Chroma Research 2024</a>)</span>.</p>
<p>Pure vector search can miss exact tokens, identifiers, or acronyms. A user searching for “RFC 2616” needs lexical matching, not semantic similarity to “HTTP specification.” <em>Hybrid retrieval</em> combines dense embeddings with keyword search (BM25). Results merge via Reciprocal Rank Fusion:</p>
<p><span class="math display">\[
H = (1 - \alpha) \cdot \text{BM25 Score} + \alpha \cdot \text{Semantic Score}
\]</span></p>
<p>Typical <span class="math inline">\(\alpha \approx 0.5\)</span> for general-purpose retrieval. See <a href="https://weaviate.io/blog/chunking-strategies-for-rag">Weaviate’s discussion</a> for tuning guidance.</p>
<p>Many RAG failures are scope failures, not retrieval failures. If the user asks “What is the refund policy for EU customers?”, retrieval should be constrained by region, document type, and effective date <em>before</em> scoring semantic similarity—a technique called <em>metadata filtering</em>. See <a href="https://haystack.deepset.ai/blog/extracting-metadata-filter">Haystack’s metadata filtering guide</a> for practical patterns.</p>
<p>Vector similarity is a coarse filter. <em>Reranking</em> applies a cross-encoder to the top-<span class="math inline">\(k\)</span> candidates, scoring query-document pairs jointly. ColBERTv2 and BERT-based rerankers achieve 15–30% improvement over embedding-only retrieval <span class="citation" data-cites="khattab2020colbert">(<a href="references.html#ref-khattab2020colbert" role="doc-biblioref">Khattab and Zaharia 2020</a>)</span>. A standard two-stage pipeline retrieves 20+ candidates via hybrid search (maximizing recall), then reranks to top 3–5 via cross-encoder (maximizing precision).</p>
<p>When documents are large and interconnected, flat retrieval struggles. <em>Hierarchical retrieval</em> addresses this: <em>RAPTOR</em> builds a tree of summaries—retrieve high-level summaries to identify relevant sections, then drill down to specific chunks <span class="citation" data-cites="sarthi2024raptor">(<a href="references.html#ref-sarthi2024raptor" role="doc-biblioref">Sarthi et al. 2024</a>)</span>. <em>GraphRAG</em> extracts knowledge graphs and retrieves entity-relationship paths for multi-entity reasoning <span class="citation" data-cites="microsoft2024graphrag">(<a href="references.html#ref-microsoft2024graphrag" role="doc-biblioref">Microsoft Research 2024</a>)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 38%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Variant</th>
<th>Innovation</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Self-RAG</td>
<td>Joint retriever-generator with self-critique</td>
<td>High-stakes QA (legal, medical)</td>
</tr>
<tr class="even">
<td>CRAG</td>
<td>Adaptive retrieval with confidence evaluation and web fallback</td>
<td>Tasks requiring current information</td>
</tr>
<tr class="odd">
<td>Graph RAG</td>
<td>Knowledge graph extraction; retrieves entity paths</td>
<td>Multi-entity reasoning</td>
</tr>
<tr class="even">
<td>HyDE</td>
<td>Generates hypothetical answer, retrieves based on that</td>
<td>Vague or ambiguous queries</td>
</tr>
</tbody>
</table>
<p><em>HyDE</em> addresses a common failure mode: vague queries that don’t match relevant documents. The model first generates a hypothetical answer, then uses that answer’s embedding for retrieval—even if factually wrong, it contains vocabulary that matches correct documents <span class="citation" data-cites="gao2022hyde">(<a href="references.html#ref-gao2022hyde" role="doc-biblioref">Gao et al. 2022</a>)</span>.</p>
</section>
<section id="solving-lost-in-the-middle" class="level3">
<h3 class="anchored" data-anchor-id="solving-lost-in-the-middle">Solving Lost-in-the-Middle</h3>
<p>Addressing the lost-in-the-middle phenomenon requires both architectural improvements and practical mitigation strategies. Recent research has systematically evaluated various approaches to this challenge. <span class="citation" data-cites="gupte2025lostmiddle">Gupte et al. (<a href="references.html#ref-gupte2025lostmiddle" role="doc-biblioref">2025</a>)</span> introduced the GM-Extract benchmark to study LLM performance on retrieval of control variables, proposing distinct metrics for spatial retrieval capability (Document Metric) and semantic retrieval capability (Variable Extraction Metric). Their analysis categorizes mitigation methods into black-box approaches (modifications to prompts and retrieval strategies) and white-box approaches (modifications to model architecture or attention mechanisms), finding that the efficacy of these techniques is highly nuanced and context-dependent.</p>
<p>The most straightforward fix is strategic document positioning after reranking:</p>
<ul>
<li>Most relevant → position at start</li>
<li>Second-most relevant → position at end</li>
<li>Medium relevance → position in middle</li>
</ul>
<p>Combined with two-stage retrieval (broad recall, then precision reranking), this inverts the U-curve to match model attention patterns.</p>
<p>OpenAI’s GPT-5.2 introduced the MRCRv2 (Multi-Round Coreference Resolution) benchmark, specifically designed to evaluate long-context performance across extended conversations. This benchmark measures how well models track entities and maintain coherence across multiple turns of dialogue, directly addressing scenarios where critical information might otherwise be lost mid-context. Early evaluations suggest that <a href="https://vertu.com/lifestyle/gpt-5-2-hype-vs-reality-is-openais-latest-model-worth-the-upgrade/">reasoning models like GPT-5.2</a> show improved performance on these metrics, though the improvements are not uniform across all task types.</p>
<p>Chain-of-thought (CoT) prompting also helps mitigate lost-in-the-middle effects by encouraging models to explicitly reference and reason through retrieved documents in sequence. When models are prompted to “think step by step” about each piece of evidence, they are less likely to skip over information positioned in the middle of the context. This approach, combined with retrieval-augmented generation, creates a reasoning pipeline that forces attention to all retrieved chunks rather than just those at the boundaries.</p>
<p><em>Long-context reranking</em> takes this further: concatenate retrieved chunks in original document order and score jointly, capturing cross-chunk relationships. This often improves accuracy by 10–15% over isolated chunk reranking.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.postprocessors <span class="im">import</span> LLMRerank, MetadataReplacementPostProcessor</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_index.core.node_parser <span class="im">import</span> SentenceWindowNodeParser</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>node_parser <span class="op">=</span> SentenceWindowNodeParser.from_defaults(</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    window_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    window_metadata_key<span class="op">=</span><span class="st">"window"</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>index <span class="op">=</span> VectorStoreIndex.from_documents(documents, node_parser<span class="op">=</span>node_parser)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>postprocessors <span class="op">=</span> [</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    MetadataReplacementPostProcessor(target_metadata_key<span class="op">=</span><span class="st">"window"</span>),</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    LLMRerank(top_n<span class="op">=</span><span class="dv">3</span>, service_context<span class="op">=</span>service_context)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>query_engine <span class="op">=</span> index.as_query_engine(</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    similarity_top_k<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    node_postprocessors<span class="op">=</span>postprocessors</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="caching-and-compression" class="level3">
<h3 class="anchored" data-anchor-id="caching-and-compression">Caching and Compression</h3>
<p><em>Prompt compression</em> reduces token count before inference, cutting latency and cost. Microsoft’s LLMLingua-2 formulates compression as token classification: a small Transformer determines which tokens are essential, achieving 2–5× compression with minimal performance loss <span class="citation" data-cites="jiang2023llmlingua">(<a href="references.html#ref-jiang2023llmlingua" role="doc-biblioref">Jiang et al. 2023</a>)</span>. Compression works well for verbose natural language (instructions, transcripts, conversation history), RAG systems retrieving 10+ chunks per query, and multi-step reasoning with repeated context. It fails for structured data—JSON schemas, SQL, API specifications—where dropping a token from <code>user_id</code> → <code>userid</code> breaks functionality.</p>
<p>For applications with large static prompt components, <em>context caching</em> provides substantial savings. Systems like <a href="https://platform.openai.com/docs/guides/prompt-caching">Anthropic’s prompt caching</a> allow marking static portions—system instructions, documentation, tool schemas—for reuse across API calls, reducing costs significantly and eliminating redundant computation.</p>
<p>Even with sufficient tokens, context can fail. <em>Instruction conflicts</em> occur when system prompts contradict retrieved text or user queries. <em>Retrieval drift</em> happens when top-<span class="math inline">\(k\)</span> chunks are semantically related but not evidentially useful. <em>Duplication</em> wastes budget and amplifies noise. <em>Stale memory</em> silently drops constraints or commitments from earlier turns. <em>Prompt injection</em> allows malicious content in retrieved documents to hijack model behavior. Mitigations include explicit conflict resolution in system prompts, deduplication before context assembly, and allowlists for tool invocation.</p>
<p>Multi-turn applications face a token explosion problem: keeping every turn eventually exceeds the budget; truncating aggressively loses commitments and constraints. Production systems implement <em>tiered memory</em>: (1) <em>Active Memory</em> (100–500 tokens) holds the current turn plus last 2–3 exchanges; (2) <em>Session Memory</em> (500–2,000 tokens) stores compressed summaries, key entities, and cross-turn dependencies; (3) <em>Persistent Memory</em> in an external vector or graph database is retrieved on demand.</p>
<div class="cell" data-fig-width="6" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  U[User] --&gt; A[Active context&lt;br/&gt;last few turns + current task]
  A --&gt; M[Model call]
  M --&gt; O[Output]
  A --&gt; S[Session summary&lt;br/&gt;compact running state]
  A --&gt; P[Persistent memory&lt;br/&gt;vector store / graph]
  P --&gt;|retrieve on demand| A
  S --&gt;|refresh| A
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The <em>MemGPT</em> pattern virtualizes LLM context, treating it like an operating system’s page cache <span class="citation" data-cites="packer2023memgpt">(<a href="references.html#ref-packer2023memgpt" role="doc-biblioref">Packer et al. 2023</a>)</span>. At 70% capacity, reasoning pauses for memory pressure handling. The system identifies least critical content for eviction, compresses it to an external tier, and retrieves relevant context back on demand. This pattern sustains multi-turn conversations indefinitely within finite windows.</p>
</section>
<section id="neural-memory-systems" class="level3">
<h3 class="anchored" data-anchor-id="neural-memory-systems">Neural Memory Systems</h3>
<p>The tension between attention’s quadratic cost and the desire for persistent memory has driven research into architectures that learn <em>during inference</em>.</p>
<p><em>Titans</em> gives models a deep memory module—a multi-layer perceptron that updates on the fly for each input <span class="citation" data-cites="titans2025">(<a href="references.html#ref-titans2025" role="doc-biblioref">Behrouz, Pezeshki, and Fakoor 2025</a>)</span>. Updates are triggered by a “surprise signal” computed from gradient magnitudes, analogous to biological synaptic strengthening during prediction error.</p>
<div id="fig-titans-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-titans-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/titans-architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-titans-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.12: Titans Architecture. The model uses a deep MLP memory module that updates during inference based on a surprise signal.
</figcaption>
</figure>
</div>
<p><em>MIRAS</em> generalizes this approach, treating memory as learnable parameters updated via local optimization with configurable loss functions <span class="citation" data-cites="miras2025">(<a href="references.html#ref-miras2025" role="doc-biblioref">Behrouz and Pezeshki 2025</a>)</span>.</p>
<p><em>Larimar</em> takes a different approach, adding a distributed episodic memory to existing LLMs <span class="citation" data-cites="das2024larimar">(<a href="references.html#ref-das2024larimar" role="doc-biblioref">Das et al. 2024</a>)</span>. This brain-inspired architecture allows dynamic, one-shot updates of knowledge without retraining or fine-tuning—achieving 8–10× speedups over traditional knowledge editing methods while also supporting selective forgetting and information leakage prevention.</p>
<p>These architectures suggest a future where AI systems combine attention for immediate reasoning with neural memory for long-term, persistent knowledge.</p>
</section>
<section id="evaluation-and-deployment" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-and-deployment">Evaluation and Deployment</h3>
<p>Effective context management requires measuring performance across three dimensions. <em>Retrieval quality</em> is assessed via Precision@k (fraction of top-<span class="math inline">\(k\)</span> retrieved documents that are relevant), Recall (fraction of all relevant documents that were retrieved), and Mean Reciprocal Rank (average of <span class="math inline">\(1/\text{rank}\)</span> for the first relevant result—higher when relevant documents appear earlier). <em>Compression efficiency</em> tracks compression ratio (original tokens divided by compressed tokens), task retention (performance on downstream tasks after compression), and latency improvements. <em>Generation quality</em> measures exact match accuracy (whether the answer matches a gold reference exactly), F1 scores (harmonic mean of precision and recall at the token level), and faithfulness—whether claims in the answer can be traced to the provided context rather than hallucinated. Libraries like <a href="https://docs.ragas.io/">RAGAS</a> provide standardized evaluation pipelines for RAG systems, computing metrics such as faithfulness, answer relevancy, and context precision.</p>
<p>Different applications demand different context engineering strategies. <em>Enterprise knowledge assistants</em> typically employ hybrid retrieval (15 semantic + 5 BM25 candidates), cross-encoder reranking to the top-3, and LLMLingua-2 compression at 2–3×, targeting under 2,000 tokens of evidence with sub-500ms latency. Citations to retrieved chunks ensure auditability. <em>Deep research systems</em> favor <a href="https://arxiv.org/abs/2401.18059">hierarchical retrieval via RAPTOR</a> for multi-level abstraction and <a href="https://www.microsoft.com/en-us/research/project/graphrag/">GraphRAG</a> for structural questions, synthesizing 5–10 documents while preserving source order. <em>Codebase assistants</em> combine sparse retrieval (symbols, filenames) with dense retrieval (conceptual similarity), limiting context to 3–5 surgical code blocks with aggressive caching of tool schemas, targeting under 4,000 tokens with sub-second latency.</p>
<p>A production deployment should address each stage systematically: chunking strategy matched to task requirements (small for lookup, large for narrative), hybrid search with metadata filtering, cross-encoder reranking to top-3 or top-5, high-relevance content positioned at context boundaries, compression applied to verbose content but skipped for structured data, static prompt components pinned via caching, tiered memory architecture for multi-turn workflows, and continuous monitoring of retrieval precision, token spend, and answer faithfulness.</p>
</section>
</section>
<section id="combining-techniques-for-optimal-performance" class="level2" data-number="24.10">
<h2 data-number="24.10" class="anchored" data-anchor-id="combining-techniques-for-optimal-performance"><span class="header-section-number">24.10</span> Combining Techniques for Optimal Performance</h2>
<p>Each technique brings its advantages, and the most effective AI systems often combine them strategically. An agent might follow structured prompts through instruction fine-tuning, think through problems step by step using chain-of-thought reasoning, refine its answers through self-review via reflexion, and align its tone based on human feedback through RLHF. This stacked approach has become standard in today’s leading models: most large LLMs, including GPT-4, are first trained with supervised fine-tuning and then polished with RLHF.</p>
<p>To understand these approaches systematically, we can think of instruction fine-tuning as teaching with flashcards, learning specific input-output patterns for following user commands. Domain-specific fine-tuning resembles medical school specialization, absorbing field-specific vocabulary and rules for expert knowledge tasks. Chain-of-thought operates like showing your work in math class, generating intermediate reasoning steps for complex problem solving. Tree-of-thought functions as decision tree exploration, branching and evaluating multiple paths for planning and strategy tasks. Reflexion mirrors learning from mistakes through self-critique and improvement for iterative problem solving. RAG operates like an open-book exam, accessing external information for fact-based reasoning. RLHF resembles teacher feedback, learning from human preferences for human-aligned responses. Finally, chain-of-action works like using tools while thinking, interleaving reasoning with actions for multi-step tasks requiring external resources.</p>
<p>In summary, the table below offers a concise overview of each post-training method. It includes simplified analogies to clarify the technical concepts, outlines the fundamental working principles, and highlights typical applications.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 22%">
<col style="width: 28%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Post-training Method</th>
<th>Simplified Analogy</th>
<th>Basic Working Principle</th>
<th>Typical Applications</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Instruction Fine-Tuning</td>
<td>Teaching with flashcards</td>
<td>Learning specific input-output patterns for following user commands</td>
<td>Following user commands</td>
</tr>
<tr class="even">
<td>Domain-Specific Supervised Fine-Tuning</td>
<td>Medical school specialization</td>
<td>Absorbing field-specific vocabulary and rules for expert knowledge tasks</td>
<td>Expert knowledge tasks</td>
</tr>
<tr class="odd">
<td>Chain-of-Thought</td>
<td>Showing your work in math class</td>
<td>Generating intermediate reasoning steps for complex problem solving</td>
<td>Complex problem solving</td>
</tr>
<tr class="even">
<td>Tree-of-Thought</td>
<td>Decision tree exploration</td>
<td>Branching and evaluating multiple paths for planning and strategy tasks</td>
<td>Planning and strategy tasks</td>
</tr>
<tr class="odd">
<td>Reflexion</td>
<td>Learning from mistakes</td>
<td>Writing a short reflection on what went wrong, then revising the approach</td>
<td>Iterative problem solving</td>
</tr>
<tr class="even">
<td>Retrieval-Augmented Generation</td>
<td>An open-book exam, accessing external information for fact-based reasoning</td>
<td>Grabbing documents or information relevant to the query and injecting them into the context window so the model can reason over fresh evidence</td>
<td>Fact-based reasoning</td>
</tr>
<tr class="odd">
<td>Reinforcement Learning from Human Feedback</td>
<td>Teacher feedback, learning from human preferences for human-aligned responses</td>
<td>Taking a pre-trained model and generating several answers for real user prompts. Human reviewers rank those answers, a reward model learns these rankings, and the main model is updated to score higher on that reward</td>
<td>Human-aligned responses</td>
</tr>
<tr class="even">
<td>Chain-of-Action</td>
<td>Using tools while thinking, interleaving reasoning with actions for multi-step tasks requiring external resources</td>
<td>Decomposing a complex query into a reasoning chain interleaved with tool calls such as web search, database lookup, or image retrieval that are executed on the fly and fed into the next thought</td>
<td>Multi-step tasks requiring external resources</td>
</tr>
</tbody>
</table>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>Context management is a systems problem: chunking, retrieval, reranking, positioning, compression, and memory interact under a fixed budget. The engineering goal is simple—spend tokens on evidence and constraints that matter, not on redundancy.</p>
<p>Organizations deploying RAG without these optimizations leave substantial accuracy gains unrealized and pay more than necessary in API costs. The tools—LLMLingua-2, semantic chunking, cross-encoder reranking, prompt caching—are mature and open-source. What separates successful deployments is systematic application: measuring retrieval precision, compression ratios, and answer quality at each stage.</p>
<p>Emerging neural memory systems suggest a future beyond context window constraints—architectures that learn during inference and maintain persistent state across interactions. For now, context engineering remains essential: the difference between “it kind of works” and a reliable production system.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-ayala2024reducing" class="csl-entry" role="listitem">
Ayala, Orlando, and Patrice Bechard. 2024. <span>“Reducing Hallucination in Structured Outputs via <span>Retrieval-Augmented Generation</span>.”</span> In <em>Proceedings of the 2024 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language Technologies</span> (<span>Volume</span> 6: <span>Industry Track</span>)</em>, 228–38. Mexico City, Mexico: Association for Computational Linguistics.
</div>
<div id="ref-miras2025" class="csl-entry" role="listitem">
Behrouz, Ali, and Mohammad Pezeshki. 2025. <span>“<span>MIRAS</span>: <span>Memory</span> as an Optimization Object.”</span> <em>Google Research</em>.
</div>
<div id="ref-titans2025" class="csl-entry" role="listitem">
Behrouz, Ali, Mohammad Pezeshki, and Rasool Fakoor. 2025. <span>“Titans: <span>Learning</span> to Memorize at Test Time.”</span> <em>arXiv Preprint arXiv:2501.00663</em>. <a href="https://arxiv.org/abs/2501.00663">https://arxiv.org/abs/2501.00663</a>.
</div>
<div id="ref-bird2009natural" class="csl-entry" role="listitem">
Bird, Steven, Ewan Klein, and Edward Loper. 2009. <em>Natural <span>Language Processing</span> with <span>Python</span>: <span>Analyzing Text</span> with the <span>Natural Language Toolkit</span></em>. Beijing ; Cambridge Mass.: O’Reilly Media.
</div>
<div id="ref-chen2023accelerating" class="csl-entry" role="listitem">
Chen, Charlie, Sebastian Borgeaud, Jean-Baptiste Alayrac, Eliza Buchatskaya, Sebastian Bodnariu, Benoit Steiner, Junteng Jia, et al. 2023. <span>“Accelerating Large Language Model Decoding with Speculative Sampling.”</span> <em>arXiv Preprint arXiv:2302.01318</em>. <a href="https://arxiv.org/abs/2302.01318">https://arxiv.org/abs/2302.01318</a>.
</div>
<div id="ref-chroma2024chunking" class="csl-entry" role="listitem">
Chroma Research. 2024. <span>“Evaluating Chunking Strategies for Retrieval.”</span>
</div>
<div id="ref-chung2022scaling" class="csl-entry" role="listitem">
Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, et al. 2022. <span>“Scaling <span>Instruction-Finetuned Language Models</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2210.11416">https://arxiv.org/abs/2210.11416</a>.
</div>
<div id="ref-dao2023flashattention2" class="csl-entry" role="listitem">
Dao, Tri. 2023. <span>“<span>FlashAttention-2</span>: <span>Faster</span> Attention with Better Parallelism and Work Partitioning.”</span> <em>arXiv Preprint arXiv:2307.08691</em>. <a href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a>.
</div>
<div id="ref-das2024larimar" class="csl-entry" role="listitem">
Das, Payel, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, et al. 2024. <span>“Larimar: <span>Large</span> Language Models with Episodic Memory Control.”</span> In <em>Proceedings of the 41st International Conference on Machine Learning (<span>ICML</span>)</em>.
</div>
<div id="ref-gao2022hyde" class="csl-entry" role="listitem">
Gao, Luyu, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. <span>“Precise Zero-Shot Dense Retrieval Without Relevance Labels.”</span> <em>arXiv Preprint arXiv:2212.10496</em>. <a href="https://arxiv.org/abs/2212.10496">https://arxiv.org/abs/2212.10496</a>.
</div>
<div id="ref-guan2025rstarmath" class="csl-entry" role="listitem">
Guan, Xinyu, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. <span>“<span class="nocase">rStar-Math</span>: <span>Small LLMs Can Master Math Reasoning</span> with <span>Self-Evolved Deep Thinking</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2501.04519">https://arxiv.org/abs/2501.04519</a>.
</div>
<div id="ref-gupte2025lostmiddle" class="csl-entry" role="listitem">
Gupte, Mihir, Eshan Dixit, Muhammad Tayyab, and Arun Adiththan. 2025. <span>“What Works for <span>‘Lost-in-the-Middle’</span> in LLMs? A Study on GM-Extract and Mitigations.”</span> <em>arXiv Preprint arXiv:2511.13900</em>. <a href="https://arxiv.org/abs/2511.13900">https://arxiv.org/abs/2511.13900</a>.
</div>
<div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. <span>“Distilling the Knowledge in a Neural Network.”</span> <em>arXiv Preprint arXiv:1503.02531</em>. <a href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a>.
</div>
<div id="ref-hou2025enhancing" class="csl-entry" role="listitem">
Hou, Zhen, Hao Liu, Jiang Bian, Xing He, and Yan Zhuang. 2025. <span>“Enhancing Medical Coding Efficiency Through Domain-Specific Fine-Tuned Large Language Models.”</span> <em>Npj Health Systems</em> 2 (1): 14.
</div>
<div id="ref-hsieh2023distilling" class="csl-entry" role="listitem">
Hsieh, Cheng-Yu, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. <span>“Distilling Step-by-Step! <span>Outperforming</span> Larger Language Models with Less Training Data and Smaller Model Sizes.”</span> <em>arXiv Preprint arXiv:2305.02301</em>. <a href="https://arxiv.org/abs/2305.02301">https://arxiv.org/abs/2305.02301</a>.
</div>
<div id="ref-jiang2023llmlingua" class="csl-entry" role="listitem">
Jiang, Huiqiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. <span>“<span>LLMLingua</span>: <span>Compressing</span> Prompts for Accelerated Inference of Large Language Models.”</span> <em>arXiv Preprint arXiv:2310.05736</em>. <a href="https://arxiv.org/abs/2310.05736">https://arxiv.org/abs/2310.05736</a>.
</div>
<div id="ref-khattab2020colbert" class="csl-entry" role="listitem">
Khattab, Omar, and Matei Zaharia. 2020. <span>“<span>ColBERT</span>: <span>Efficient</span> and Effective Passage Search via Contextualized Late Interaction over <span>BERT</span>.”</span> In <em>Proceedings of the 43rd International <span>ACM SIGIR</span> Conference on Research and Development in Information Retrieval</em>, 39–48. ACM.
</div>
<div id="ref-leviathan2023fast" class="csl-entry" role="listitem">
Leviathan, Yaniv, Matan Kalman, and Yossi Matias. 2023. <span>“Fast Inference from Transformers via Predictive Sampling.”</span> <em>arXiv Preprint arXiv:2211.17191</em>. <a href="https://arxiv.org/abs/2211.17191">https://arxiv.org/abs/2211.17191</a>.
</div>
<div id="ref-liu2023ring" class="csl-entry" role="listitem">
Liu, Hao, Matei Zaharia, and Pieter Abbeel. 2023. <span>“Ring Attention with Blockwise Transformers for Near-Infinite Context.”</span> <em>arXiv Preprint arXiv:2310.01889</em>. <a href="https://arxiv.org/abs/2310.01889">https://arxiv.org/abs/2310.01889</a>.
</div>
<div id="ref-liu2023lost" class="csl-entry" role="listitem">
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the Middle: <span>How</span> Language Models Use Long Contexts.”</span> <em>arXiv Preprint arXiv:2307.03172</em>. <a href="https://arxiv.org/abs/2307.03172">https://arxiv.org/abs/2307.03172</a>.
</div>
<div id="ref-microsoft2024graphrag" class="csl-entry" role="listitem">
Microsoft Research. 2024. <span>“<span>GraphRAG</span>: <span>Unlocking LLM</span> Discovery on Narrative Private Data.”</span>
</div>
<div id="ref-ouyang2022training" class="csl-entry" role="listitem">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 27730–44.
</div>
<div id="ref-packer2023memgpt" class="csl-entry" role="listitem">
Packer, Charles, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. <span>“<span>MemGPT</span>: <span>Towards LLMs</span> as Operating Systems.”</span> <em>arXiv Preprint arXiv:2310.08560</em>. <a href="https://arxiv.org/abs/2310.08560">https://arxiv.org/abs/2310.08560</a>.
</div>
<div id="ref-pan2025chainofaction" class="csl-entry" role="listitem">
Pan, Zhenyu, Haozheng Luo, Manling Li, and Han Liu. 2025. <span>“Chain-of-<span>Action</span>: <span>Faithful</span> and <span>Multimodal Question Answering</span> Through <span>Large Language Models</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2403.17359">https://arxiv.org/abs/2403.17359</a>.
</div>
<div id="ref-sanh2019distilbert" class="csl-entry" role="listitem">
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. <span>“<span>DistilBERT</span>, a Distilled Version of <span>BERT</span>: Smaller, Faster, Cheaper and Lighter.”</span> <em>arXiv Preprint arXiv:1910.01108</em>. <a href="https://arxiv.org/abs/1910.01108">https://arxiv.org/abs/1910.01108</a>.
</div>
<div id="ref-sarthi2024raptor" class="csl-entry" role="listitem">
Sarthi, Parth, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. <span>“<span>RAPTOR</span>: <span>Recursive</span> Abstractive Processing for Tree-Organized Retrieval.”</span> <em>arXiv Preprint arXiv:2401.18059</em>. <a href="https://arxiv.org/abs/2401.18059">https://arxiv.org/abs/2401.18059</a>.
</div>
<div id="ref-shinn2023reflexion" class="csl-entry" role="listitem">
Shinn, Noah, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. <span>“Reflexion: <span>Language</span> Agents with Verbal Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2303.11366">https://arxiv.org/abs/2303.11366</a>.
</div>
<div id="ref-wei2023chainofthought" class="csl-entry" role="listitem">
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. <span>“Chain-of-<span>Thought Prompting Elicits Reasoning</span> in <span>Large Language Models</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.
</div>
<div id="ref-yang2025qwen2" class="csl-entry" role="listitem">
Yang, An, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, et al. 2025. <span>“Qwen2. 5-1m Technical Report.”</span> <em>arXiv Preprint arXiv:2501.15383</em>. <a href="https://arxiv.org/abs/2501.15383">https://arxiv.org/abs/2501.15383</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>“Tree of <span>Thoughts</span>: <span>Deliberate Problem Solving</span> with <span>Large Language Models</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.
</div>
<div id="ref-ye2025limo" class="csl-entry" role="listitem">
Ye, Yixin, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. <span>“<span>LIMO</span>: <span>Less</span> Is <span>More</span> for <span>Reasoning</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2502.03387">https://arxiv.org/abs/2502.03387</a>.
</div>
<div id="ref-yu2022orca" class="csl-entry" role="listitem">
Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soo-Jin Jeong, Woosung Lee, and Byung-Gon Chun. 2022. <span>“Orca: A Distributed Serving System for <span class="nocase">Transformer-based</span> Generative <span>Models</span>.”</span> In <em>16th <span>USENIX</span> Symposium on Operating Systems Design and Implementation (<span>OSDI</span> 22)</em>, 527–46.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./23-nlp.html" class="pagination-link" aria-label="Natural Language Processing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./25-robots.html" class="pagination-link" aria-label="AI Agents">
        <span class="nav-page-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>