<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Tree Models – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./15-forecasting.html" rel="next">
<link href="./13-logistic.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/chess.jpg">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/chess.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#building-a-tree-via-recursive-binary-splitting" id="toc-building-a-tree-via-recursive-binary-splitting" class="nav-link active" data-scroll-target="#building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</a></li>
  <li><a href="#pruning-taming-an-overfit-tree" id="toc-pruning-taming-an-overfit-tree" class="nav-link" data-scroll-target="#pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</a>
  <ul class="collapse">
  <li><a href="#example-boston-housing-data" id="toc-example-boston-housing-data" class="nav-link" data-scroll-target="#example-boston-housing-data">Example: Boston Housing Data</a></li>
  </ul></li>
  <li><a href="#classification-trees" id="toc-classification-trees" class="nav-link" data-scroll-target="#classification-trees"><span class="header-section-number">14.3</span> Classification Trees</a>
  <ul class="collapse">
  <li><a href="#example-classifying-diamond-quality" id="toc-example-classifying-diamond-quality" class="nav-link" data-scroll-target="#example-classifying-diamond-quality">Example: Classifying Diamond Quality</a></li>
  </ul></li>
  <li><a href="#ensemble-methods" id="toc-ensemble-methods" class="nav-link" data-scroll-target="#ensemble-methods"><span class="header-section-number">14.4</span> Ensemble Methods</a>
  <ul class="collapse">
  <li><a href="#bagging" id="toc-bagging" class="nav-link" data-scroll-target="#bagging">Bagging</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting">Boosting</a></li>
  </ul></li>
  <li><a href="#sec-bart-causal" id="toc-sec-bart-causal" class="nav-link" data-scroll-target="#sec-bart-causal"><span class="header-section-number">14.5</span> BART for causal inference</a>
  <ul class="collapse">
  <li><a href="#bart-versus-propensity-score-matching-psm" id="toc-bart-versus-propensity-score-matching-psm" class="nav-link" data-scroll-target="#bart-versus-propensity-score-matching-psm">BART versus Propensity Score Matching (PSM)</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  <li><a href="#appendix-theoretical-foundations" id="toc-appendix-theoretical-foundations" class="nav-link" data-scroll-target="#appendix-theoretical-foundations"><span class="header-section-number">15</span> Appendix: Theoretical Foundations</a>
  <ul class="collapse">
  <li><a href="#why-ensembles-work-a-geometric-perspective" id="toc-why-ensembles-work-a-geometric-perspective" class="nav-link" data-scroll-target="#why-ensembles-work-a-geometric-perspective"><span class="header-section-number">15.1</span> Why Ensembles Work: A Geometric Perspective</a>
  <ul class="collapse">
  <li><a href="#smoothing-and-variance-reduction" id="toc-smoothing-and-variance-reduction" class="nav-link" data-scroll-target="#smoothing-and-variance-reduction">Smoothing and Variance Reduction</a></li>
  <li><a href="#the-problem-of-high-dimensions" id="toc-the-problem-of-high-dimensions" class="nav-link" data-scroll-target="#the-problem-of-high-dimensions">The Problem of High Dimensions</a></li>
  <li><a href="#trees-as-adaptive-nearest-neighbors" id="toc-trees-as-adaptive-nearest-neighbors" class="nav-link" data-scroll-target="#trees-as-adaptive-nearest-neighbors">Trees as “Adaptive” Nearest Neighbors</a></li>
  </ul></li>
  <li><a href="#classification-variance-decomposition" id="toc-classification-variance-decomposition" class="nav-link" data-scroll-target="#classification-variance-decomposition"><span class="header-section-number">15.2</span> Classification variance decomposition</a>
  <ul class="collapse">
  <li><a href="#the-nearest-neighbor-insight" id="toc-the-nearest-neighbor-insight" class="nav-link" data-scroll-target="#the-nearest-neighbor-insight">The Nearest Neighbor Insight</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Imagine you’re a jewelry appraiser tasked with determining a diamond’s value. You might follow a series of questions: Is the carat weight above 1.0? If yes, is the clarity VS1 or better? Each question leads to another, creating a decision path that eventually arrives at a price estimate. This is precisely how decision trees work—they mirror our natural decision-making process by creating a flowchart of if-then rules.</p>
<p>Logistic regression is a deliberately structured model: it trades flexibility for interpretability and stable estimation. Tree-based methods reverse that trade-off by letting the data determine interactions and nonlinearities. The cost is that algorithmic choices (splitting, pruning, ensembling) become part of the statistical model. This chapter introduces trees as a first major step into nonparametric prediction.</p>
<p>We’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of <span class="math inline">\(y\)</span> to use for a given <span class="math inline">\(x\)</span>. Similar to a decision tree, a predictive tree model is a nested sequence of if-else statements that map any input data point <span class="math inline">\(x\)</span> to a predicted output <span class="math inline">\(y\)</span>. Each if-else statement checks a feature of <span class="math inline">\(x\)</span> and sends the data left or right along the tree branch. At the end of the branch, a single value of <span class="math inline">\(y\)</span> is predicted.</p>
<p><a href="#fig-chesstree" class="quarto-xref">Figure&nbsp;<span>14.1</span></a> shows a decision tree for predicting a chess piece given a four-dimensional input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.</p>
<div id="fig-chesstree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chess.jpg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Elementary tree scheme; visualization of the splitting process
</figcaption>
</figure>
</div>
<p>The prediction mechanism is straightforward: traverse the tree from the root to a leaf node following the conditional logic. The process of building a tree, given a set of training data, is more complicated and has three main components:</p>
<ol type="1">
<li><strong>Splitting</strong>. The process of dividing the training data into subsets based on the value of a single feature. The goal is to create subsets that are as homogeneous as possible. The subsets are then used to create the nodes of the tree.</li>
<li><strong>Stopping</strong>. The process of deciding when to stop splitting. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
<li><strong>Pruning</strong>. The process of removing nodes from the tree that do not improve the accuracy of the tree. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
</ol>
<p>The crux of the tree-building process lies in splitting: determining the optimal feature and threshold that best separates the data. At each step the splitting process needs to decide on the feature index <span class="math inline">\(j\)</span> to be used for splitting and the location of the split. For a binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.</p>
<p>Below we’ll explore tree-based models using the classic diamonds dataset, which contains prices and attributes for 53,940 diamonds. We’ll start with simple decision trees, progress to ensemble methods like random forests and gradient boosting, and develop deep insights into how these algorithms work, when to use them, and how to avoid common pitfalls.</p>
<p>Let’s look at the data, which has 10 variables:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 40%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>carat</code></td>
<td>Weight of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>cut</code></td>
<td>Quality of the cut</td>
<td>Fair, Good, Very Good, Premium, Ideal</td>
</tr>
<tr class="odd">
<td><code>color</code></td>
<td>Color of the diamond</td>
<td>J, I, H, G, F, E, D</td>
</tr>
<tr class="even">
<td><code>clarity</code></td>
<td>Clarity of the diamond</td>
<td>I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF</td>
</tr>
<tr class="odd">
<td><code>depth</code></td>
<td>Depth of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>table</code></td>
<td>Width of the diamond’s table</td>
<td>Numeric</td>
</tr>
</tbody>
</table>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamonds)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(diamonds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">carat</th>
<th style="text-align: left;">cut</th>
<th style="text-align: left;">color</th>
<th style="text-align: left;">clarity</th>
<th style="text-align: right;">depth</th>
<th style="text-align: right;">table</th>
<th style="text-align: right;">price</th>
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Ideal</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.21</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI1</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">VS1</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">327</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.29</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">I</td>
<td style="text-align: left;">VS2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">334</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">2.6</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.31</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">335</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">2.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.24</td>
<td style="text-align: left;">Very Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">VVS2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Let’s plot price vs carat. Notice the strong non-linear relationship between carat and price. This suggests that log-transformations might help make the relationship linear.</p>
<details class="code-fold">
<summary>Plot price vs carat</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log-transformed price for better linear relationships</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>diamonds <span class="ot">&lt;-</span> diamonds <span class="sc">%&gt;%</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">mutate</span>(<span class="at">log_price =</span> <span class="fu">log</span>(price),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a> <span class="at">log_carat =</span> <span class="fu">log</span>(carat))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine the linearized relationship</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> log_carat, <span class="at">y =</span> log_price)) <span class="sc">+</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">color =</span> <span class="st">"darkblue"</span>) <span class="sc">+</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Log-transformed Price vs Carat Shows Linear Relationship"</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> <span class="st">"Log(Carat)"</span>, <span class="at">y =</span> <span class="st">"Log(Price)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-price-carat-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-price-carat-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="14-tree_files/figure-html/fig-price-carat-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-price-carat-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Price vs carat
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-price-carat-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-price-carat-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="14-tree_files/figure-html/fig-price-carat-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-price-carat-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Price vs carat
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Unlike linear regression, tree models are naturally indifferent to non-linear relationships between predictors and the response. In general, we do not need to transform the variables.</p>
<p>Although carat is the most important factor in determining the price of a diamond, it is not the only factor. We can see that there is a lot of variability in the price of diamonds with the same carat.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use density plot to compare price for different clarity levels</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_density</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Let’s start with a simple decision tree using just two predictors to visualize how trees partition the feature space:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(price <span class="sc">~</span> carat <span class="sc">+</span> clarity, <span class="at">data =</span> diamonds)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The decision tree plot shows how the algorithm partitions the feature space based on carat and clarity to predict diamond prices. The tree structure reveals several interesting patterns:</p>
<ol type="1">
<li><p>Primary split on carat: The tree first splits on carat at 1.05, indicating this is the most important predictor for price. This makes intuitive sense since carat weight is typically the strongest determinant of diamond value.</p></li>
<li><p>Secondary splits on clarity: After the carat split, the tree further partitions based on clarity levels. This shows that while carat is primary, clarity still provides important predictive value for price.</p></li>
<li><p>Interpretability: Each terminal node (leaf) shows the predicted price for that region. For example, diamonds with carat &lt; 1.05 and clarity in the lower categories (I1, SI2, SI1) have an average predicted price of $2,847.</p></li>
<li><p>Feature interactions: The tree reveals how carat and clarity interact - the effect of clarity on price depends on the carat weight, which is captured through the hierarchical splitting structure.</p></li>
</ol>
<p>This simple two-predictor tree demonstrates the key advantages of decision trees: they can handle non-linear relationships, provide interpretable rules, and naturally capture feature interactions without requiring explicit specification of interaction terms.</p>
<p>Let’s plot the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x=</span>carat, <span class="at">y=</span>clarity, <span class="at">colour=</span>price)) <span class="sc">+</span> </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low=</span><span class="st">"blue"</span>, <span class="at">high=</span><span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that for small and large diamonds, the price is consistently low and does not depend much on the clarity. However, at around 1 carat, we see some overlap in the price for different clarity levels. Clarity becomes important at this level.</p>
<p>Now let’s plot the data with the tree regions.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the rectangle areas that represent the regions of the tree</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a> <span class="at">carat =</span> <span class="fu">seq</span>(<span class="fu">min</span>(diamonds<span class="sc">$</span>carat), <span class="fu">max</span>(diamonds<span class="sc">$</span>carat), <span class="at">length.out =</span> <span class="dv">50</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a> <span class="at">clarity =</span> diamonds<span class="sc">$</span>clarity</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>pred_data<span class="sc">$</span>pred_price <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, pred_data)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot regions</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(pred_data, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> clarity, <span class="at">fill =</span> pred_price)) <span class="sc">+</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">"blue"</span>, <span class="at">high =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Tree Regions: Carat vs Clarity"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The plot above shows the decision tree’s prediction regions as colored tiles, where each tile represents a specific combination of carat and clarity values. The color gradient from blue to red indicates the predicted price, with darker red representing higher predicted prices.</p>
<p>Looking at this visualization, we can see several key patterns. The strongest predictor is clearly carat, as evidenced by the vertical bands of similar colors. As carat increases (moving right on the x-axis), the predicted prices generally increase (colors shift from blue to red). The tree captures non-linear patterns that a simple linear model would miss. For example, the rate of price increase with carat is not uniform across all clarity levels. Unlike smooth regression surfaces, the tree creates distinct rectangular regions with sharp boundaries, reflecting the binary splitting nature of decision trees.</p>
<section id="building-a-tree-via-recursive-binary-splitting" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</h2>
<p>The prediction using a tree is straightforward. The tree divides the predictor space-that is, the set of possible values for <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> - into <span class="math inline">\(J\)</span> distinct and non-overlapping boxes, <span class="math inline">\(R_1,R_2,...,R_J\)</span>. For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>. <span class="math display">\[
f(x) = \bar y_j, \text{ for } x \in R_j, \text{ where } \bar y_j = \text{Average}(y_i \mid x_i \in R_j)
\]</span></p>
<p>The overall goal of building a tree is to find regions that lead to minima of the total Residual Sum of Squares (RSS) <span class="math display">\[
\mathrm{RSS} = \sum_{j=1}^J\sum_{i \in R_j}(y_i - \bar{y}_j)^2 \rightarrow \mathrm{minimize}
\]</span></p>
<p>Unfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes. We can find a good approximate solution, using top-down approach (the CART algorithm).</p>
<p>It begins with the entire dataset at the “root” node and repeatedly splits the data into two “child” nodes. This process continues recursively on each new node, with the goal of making the resulting groups (nodes) as homogeneous as possible with respect to the target variable, price. At each iteration we decide on which variable <span class="math inline">\(j\)</span> to split and the split point <span class="math inline">\(s\)</span>. <span class="math display">\[
R_1(j, s) = \{x\mid x_j &lt; s\} \mbox{ and } R_2(j, s) = \{x\mid x_j \ge s\},
\]</span> thus, we seek to minimize (in case of regression tree) <span class="math display">\[
\min_{j,s}\left[ \sum_{i:x_i\in R_1}(y_i - \bar{y}_1)^2 + \sum_{i:x_i \in R_2}(y_i - \bar{y}_2)^2\right]
\]</span> As a result, every observed input point belongs to a single region.</p>
</section>
<section id="pruning-taming-an-overfit-tree" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</h2>
<p>Now let’s discuss how many regions we should have. At one extreme end, we can have <span class="math inline">\(n\)</span> regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed <span class="math inline">\(y\)</span>’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1,...,R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias. Deep trees often suffer from high variance, where slight perturbations in the training data produce vastly different structures, rendering the model unstable.</p>
<p>How do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree <span class="math inline">\(T_0\)</span>, and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!</p>
<p>Instead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span>. For each value of <span class="math inline">\(\alpha\)</span> there corresponds a subtree <span class="math inline">\(T \subset T_0\)</span> such that minimizes <span class="math display">\[
\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \bar{y}_m)^2 + \alpha |T|
\]</span> The parameter <span class="math inline">\(\alpha\)</span> balances the complexity of the subtree and its adherence to the training data. When we increment <span class="math inline">\(\alpha\)</span> starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of <span class="math inline">\(\alpha\)</span>. We determine the optimal value <span class="math inline">\(\hat \alpha\)</span> through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to <span class="math inline">\(\hat \alpha\)</span>.</p>
<section id="example-boston-housing-data" class="level3">
<h3 class="anchored" data-anchor-id="example-boston-housing-data">Example: Boston Housing Data</h3>
<p>To demonstrate pruning and decision boundaries on a standard benchmark, we switch to the Boston Housing dataset. This dataset contains information about housing values in suburbs of Boston. We used it in previous chapters, but here it allows us to easily visualize pruning on a well-known problem.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(Boston)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Boston)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 7%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 11%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">crim</th>
<th style="text-align: right;">zn</th>
<th style="text-align: right;">indus</th>
<th style="text-align: right;">chas</th>
<th style="text-align: right;">nox</th>
<th style="text-align: right;">rm</th>
<th style="text-align: right;">age</th>
<th style="text-align: right;">dis</th>
<th style="text-align: right;">rad</th>
<th style="text-align: right;">tax</th>
<th style="text-align: right;">ptratio</th>
<th style="text-align: right;">black</th>
<th style="text-align: right;">lstat</th>
<th style="text-align: right;">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.54</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">296</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">24</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">9.1</td>
<td style="text-align: right;">22</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">393</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">395</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">33</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.3</td>
<td style="text-align: right;">36</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">394</td>
<td style="text-align: right;">5.2</td>
<td style="text-align: right;">29</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>We will focus on predicting <code>medv</code> (median value of owner-occupied homes in $1000s) using <code>lstat</code> (lower status of the population percent) and other variables.</p>
<p>First we build a big tree:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># mindev param controls stopping: smaller value = bigger tree</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>lstat, <span class="at">data=</span>Boston, <span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co"># first big tree size</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="do">## 73</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then prune it down to one with 7 leaves:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(boston.tree<span class="sc">$</span>where)) <span class="co"># pruned tree size</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 7</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>boston.fit <span class="ot">=</span> <span class="fu">predict</span>(boston.tree) <span class="co">#get training fitted values</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">cex=</span>.<span class="dv">5</span>,<span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlab=</span><span class="st">"lstat"</span>, <span class="at">ylab=</span><span class="st">"medv"</span>) <span class="co">#plot data</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],boston.fit[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>cvals<span class="ot">=</span><span class="fu">c</span>(<span class="fl">9.725</span>,<span class="fl">4.65</span>,<span class="fl">3.325</span>,<span class="fl">5.495</span>,<span class="fl">16.085</span>,<span class="fl">19.9</span>) <span class="co">#cutpoints from tree</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(cvals)) <span class="fu">abline</span>(<span class="at">v=</span>cvals[i],<span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lty=</span><span class="dv">2</span>) <span class="co">#cutpoints</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-8-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Now let’s use more variables. We pick <code>dis</code> (weighted mean of distances to five Boston employment centres), <code>lstat</code>, and <code>medv</code>:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select specific columns by name</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">=</span> Boston[, <span class="fu">c</span>(<span class="st">"dis"</span>, <span class="st">"lstat"</span>, <span class="st">"medv"</span>)]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">names</span>(df2))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">## "dis"   "lstat" "medv"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Build the big tree:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>., df2, <span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co">#</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 74</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then prune it down to one with 7 leaves:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boston.tree,<span class="at">type=</span><span class="st">"u"</span>)<span class="co"># plot tree and partition in x.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">partition.tree</span>(boston.tree)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Get predictions on 2d grid and make perspective plot:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>pv<span class="ot">=</span><span class="fu">seq</span>(<span class="at">from=</span>.<span class="dv">01</span>,<span class="at">to=</span>.<span class="dv">99</span>,<span class="at">by=</span>.<span class="dv">05</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>x1q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>lstat,<span class="at">probs=</span>pv)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x2q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>dis,<span class="at">probs=</span>pv)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">=</span> <span class="fu">expand.grid</span>(x1q,x2q) <span class="co">#matrix with two columns using all combinations of x1q and x2q</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>dfpred <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">dis=</span>xx[,<span class="dv">2</span>],<span class="at">lstat=</span>xx[,<span class="dv">1</span>])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>lmedpred <span class="ot">=</span> <span class="fu">predict</span>(boston.tree,dfpred)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a perspective plot (3D surface)</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co"># theta sets the viewing angle, zlim ensures the vertical axis covers the data range</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="fu">persp</span>(x1q, x2q, <span class="fu">matrix</span>(lmedpred, <span class="at">ncol=</span><span class="fu">length</span>(x2q), <span class="at">byrow=</span>T),</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">theta=</span><span class="dv">150</span>, <span class="at">xlab=</span><span class="st">'dis'</span>, <span class="at">ylab=</span><span class="st">'lstat'</span>, <span class="at">zlab=</span><span class="st">'medv'</span>,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">zlim=</span><span class="fu">c</span>(<span class="fu">min</span>(df2<span class="sc">$</span>medv), <span class="fl">1.1</span><span class="sc">*</span><span class="fu">max</span>(df2<span class="sc">$</span>medv)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p><em>Advantages of Decision Trees</em>: Decision trees are incredibly intuitive and simple to explain, often even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics. Additionally, decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.</p>
<p><em>Disadvantages of Decision Trees</em>: The main drawback is instability. Deep trees often suffer from high variance, where slight perturbations in the training data produce vastly different structures.</p>
</section>
</section>
<section id="classification-trees" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="classification-trees"><span class="header-section-number">14.3</span> Classification Trees</h2>
<p>A classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use criteria better suited for categorical data.</p>
<p>We start by introducing some notations. Let <span class="math display">\[
p_{mk} = \dfrac{1}{N_m}\sum_{x_i \in R_m} I(y_i=k),
\]</span> be the proportion of observations of class <span class="math inline">\(k\)</span> in region <span class="math inline">\(m\)</span>.</p>
<p>The classification is then done by: <span class="math display">\[
p_m = \max_k p_{mk},~~~ E_m = 1-p_m
\]</span> where <span class="math inline">\(k(m) = \arg\max_k p_{mk}\)</span> is the most frequent class in region <span class="math inline">\(m\)</span>. The error rate <span class="math inline">\(E_m\)</span> is simplcy the proportion of observations in the region that do NOT belong to the majority class.</p>
<p>Then classification prediction is: <span class="math display">\[
P(y=k) = \sum_{j=1}^J p_j I(x \in R_j)
\]</span></p>
<p>An alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.</p>
<p>Now, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region. Consider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).</p>
<p>In both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it is perfectly classifying all observations in that region. This is an ideal situation in classification problems.</p>
<p>This illustrates that while the misclassification rate is a useful metric, it does not always capture the full picture. The Gini Index or Cross-Entropy are preferred for growing trees because they are more sensitive to node purity.</p>
<p>The Gini index: <span class="math display">\[
G_m = \sum_{k=1}^K p_{mk}(1-p_{mk})
\]</span> It measures a variance across the <span class="math inline">\(K\)</span> classes. It takes on a small value if all of the <span class="math inline">\(p_{mk}\)</span>’s are close to zero or one (pure nodes).</p>
<p>An alternative to the Gini index is cross-entropy (a.k.a deviance), given by <span class="math display">\[
D_m = -\sum_{k=1}^Kp_{mk}\log p_{mk}
\]</span> It is near zero if the <span class="math inline">\(p_{mk}\)</span>’s are all near zero or near one. Gini index and the cross-entropy led to similar results.</p>
<section id="example-classifying-diamond-quality" class="level3">
<h3 class="anchored" data-anchor-id="example-classifying-diamond-quality">Example: Classifying Diamond Quality</h3>
<p>Let’s use a classification tree to predict the <code>cut</code> of a diamond based on its price and carat. We will try to distinguish “Ideal” cuts from others. To make it a clear binary problem for visualization, let’s create a binary variable <code>is_ideal</code>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>diamonds_class <span class="ot">&lt;-</span> diamonds <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">is_ideal =</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(cut <span class="sc">==</span> <span class="st">"Ideal"</span>, <span class="st">"Ideal"</span>, <span class="st">"Non-Ideal"</span>)))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit classification tree</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>class_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(is_ideal <span class="sc">~</span> carat <span class="sc">+</span> price <span class="sc">+</span> clarity, <span class="at">data =</span> diamonds_class, <span class="at">method=</span><span class="st">"class"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(class_tree, <span class="at">main=</span><span class="st">"Classification Tree for Ideal Cut"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The tree shows how <code>clairty</code> and <code>carat</code> effectively separate Ideal cut diamonds from the rest. The nodes display the predicted class and the probability of that class, illustrating how the model estimates class probabilities (<span class="math inline">\(p_{mk}\)</span>) in each region.</p>
</section>
</section>
<section id="ensemble-methods" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="ensemble-methods"><span class="header-section-number">14.4</span> Ensemble Methods</h2>
<p>There are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.</p>
<p>The simple idea behind every ensemble method is that the variance of the average is lower than the variance of individual models (see <a href="10-data.html#sec-bias-variance-galton" class="quarto-xref"><span>Section 10.1.3.2</span></a> for a historical perspective on how Galton’s ox-weighing experiment illustrates this principle). Say we have <span class="math inline">\(B\)</span> models <span class="math inline">\(f_1(x),\ldots,f_B(x)\)</span> then we combine those <span class="math display">\[
f_{avg}(x) = \dfrac{1}{B}\sum_{b=1}^Bf_b(x)
\]</span> Combining models helps fight overfitting. On the negative side, it is harder to interpret these ensembles compared to a single decision tree.</p>
<section id="bagging" class="level3">
<h3 class="anchored" data-anchor-id="bagging">Bagging</h3>
<p>In the <strong>bagging</strong> approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.</p>
<p>To bootstrap aggregate (Bag) we:</p>
<ul>
<li>Take <span class="math inline">\(B\)</span> bootstrap samples from the training data, each of the same size as the training data.</li>
<li>Fit a large tree to each bootstrap sample (we know how to do this fast!). This will give us <span class="math inline">\(B\)</span> trees.</li>
<li>Combine the results from each of the <span class="math inline">\(B\)</span> trees to get an overall prediction.</li>
</ul>
<p>When the target variable <span class="math inline">\(y\)</span> is numeric, the bagging process is straightforward: the final prediction is simply the average. When <span class="math inline">\(y\)</span> is categorical, we use a voting system or average the predicted probabilities.</p>
<p>Let’s experiment with the number of trees in the model using the Boston dataset again. We calculate the mean squared error for each number of trees.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Boston)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>ntreev <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">500</span>,<span class="dv">5000</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>fmat <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,n,<span class="dv">3</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a> rffit <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>lstat,<span class="at">data=</span>Boston,<span class="at">ntree=</span>ntreev[i],<span class="at">maxnodes=</span><span class="dv">15</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a> fmat[,i] <span class="ot">=</span> <span class="fu">predict</span>(rffit)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a> mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((fmat[,i] <span class="sc">-</span> Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">cat</span>(<span class="st">"Mean Squared Error with"</span>, ntreev[i], <span class="st">"trees:"</span>, <span class="fu">round</span>(mse, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean Squared Error with 10 trees: 32 </span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean Squared Error with 500 trees: 29 </span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean Squared Error with 5000 trees: 29</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let’s plot the results</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>oo <span class="ot">=</span> <span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">xlab=</span><span class="st">'lstat'</span>,<span class="at">ylab=</span><span class="st">'medv'</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],fmat[oo,i],<span class="at">col=</span>i<span class="sc">+</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">title</span>(<span class="at">main=</span><span class="fu">paste</span>(<span class="st">'bagging ntrees = '</span>,ntreev[i]))</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="3" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-15-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-15-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li>With 10 trees our fit is too jumbly (high variance).</li>
<li>With 1,000 and 5,000 trees the fit is smooth and very similar.</li>
<li>Note that although our method is based on multiple trees (average over) so we no longer have a simple step function!</li>
</ul>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">Random Forest</h3>
<p>In the bagging technique, models can become correlated, which prevents the achievement of a <span class="math inline">\(1/B\)</span> reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.</p>
<p>Random Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees. Instead of considering all <span class="math inline">\(p\)</span> predictors for a split, a random sample of <span class="math inline">\(m\)</span> predictors is chosen as split candidates. This subset of predictors is different for each split.</p>
<p>The number of predictors considered at each split, <span class="math inline">\(m\)</span>, is typically chosen to be the square root of the total number of predictors, <span class="math inline">\(p\)</span> for classification, or <span class="math inline">\(p/3\)</span> for regression.</p>
<p>One of the “interpretation” tools that comes with ensemble models is importance ranking: the total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all trees.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">4</span>,<span class="at">importance=</span><span class="cn">TRUE</span>,<span class="at">ntree=</span><span class="dv">50</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston,<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"lightblue"</span>,<span class="at">main=</span><span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">6</span>,<span class="at">ntree=</span><span class="dv">50</span>, <span class="at">maxnodes=</span><span class="dv">50</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>yhat.rf <span class="ot">=</span> <span class="fu">predict</span>(rf.boston,<span class="at">newdata=</span>Boston)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat[oo],Boston<span class="sc">$</span>medv[oo],<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"grey"</span>, <span class="at">xlab=</span><span class="st">"lstat"</span>, <span class="at">ylab=</span><span class="st">"medv"</span>) <span class="co">#plot data</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],yhat.rf[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Random Forest Fit</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="boosting" class="level3">
<h3 class="anchored" data-anchor-id="boosting">Boosting</h3>
<p>Boosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.</p>
<p>Here’s how Boosting works:</p>
<ol type="1">
<li>Initially, a single decision tree is fitted to the data. This initial tree is intentionally made weak.</li>
<li>We then examine the residuals, which represent the portion of the target variable <span class="math inline">\(y\)</span> not explained by the weak tree.</li>
<li>A new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.</li>
<li>This new tree is also “weakened” or “shrunk” (multiplied by a learning rate <span class="math inline">\(\lambda\)</span>). The prediction from this tree is then added to the prediction of the previous trees.</li>
<li>This process is repeated iteratively. The final model is the sum of all these “shrunk” trees.</li>
</ol>
<p>The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals).</p>
<p>Mathematically, we want to minimize a loss function <span class="math inline">\(\mathcal{L}\)</span>. For regression, we might take <span class="math inline">\(\mathcal{L}(y_i , \theta_i ) = (y_i - \theta_i )^2\)</span>. We solve: <span class="math display">\[\mathrm{minimize}_{\beta \in R^B} \sum_{i=1}^n \mathcal{L} \left(y_i, \sum_{b=1}^B \beta_j \cdot T_b(x_i)\right)\]</span></p>
<p>Gradient boosting acts like gradient descent in function space. Start with initial model, e.g., fit a single tree <span class="math inline">\(\theta^{(0)} = T_0\)</span>. Repeat: - Evaluate gradient <span class="math inline">\(g\)</span> at latest prediction <span class="math inline">\(\theta^{(k-1)}\)</span>, <span class="math display">\[g_i = \left.\left[\frac{\partial \mathcal{L}(y_i, \theta_i)}{\partial \theta_i}\right]\right|_{\theta_i = \theta_i^{(k-1)}},\ i=1,\ldots,n\]</span> - Find a tree <span class="math inline">\(T_k\)</span> that is close to <span class="math inline">\(-g\)</span>, i.e., <span class="math inline">\(T_k\)</span> solves <span class="math display">\[\mathrm{minimize}_{\text{trees }T} \sum_{i=1}^n (-g_i - T(x_i))^2\]</span> - Update our prediction: <span class="math display">\[\theta^{(k)} = \theta^{(k-1)} + \lambda \cdot T_k\]</span></p>
<p><strong>Algorithm:</strong> 1. Set <span class="math inline">\(\hat{f}(x)=0\)</span> and <span class="math inline">\(r_i=y_i\)</span> for all <span class="math inline">\(i\)</span> in training set. 2. For <span class="math inline">\(b=1,2,\ldots,B\)</span>, repeat: (a) Fit a tree <span class="math inline">\(\hat{f}^b\)</span> with <span class="math inline">\(d\)</span> splits (interactions) to the training data <span class="math inline">\((X, r)\)</span>. (b) Update <span class="math inline">\(\hat{f}\)</span> by adding a shrunken version of the new tree: <span class="math display">\[\hat{f}(x) \leftarrow \hat{f}(x) +\lambda \hat{f}^b(x)\]</span> (c) Update the residuals: <span class="math display">\[r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)\]</span> 3. Output the boosted model: <span class="math display">\[\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)\]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> is the <strong>learning rate</strong> or <em>shrinkage</em> parameter. By multiplying the new tree’s contribution by a small number (e.g., 0.01 or 0.1), we prevent the model from adapting too quickly to outliers or noise.</p>
<p>Here are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = <span class="math inline">\(\lambda\)</span> at .2.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>boost.boston<span class="ot">=</span><span class="fu">gbm</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">distribution=</span><span class="st">"gaussian"</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="at">n.trees=</span><span class="dv">5000</span>,<span class="at">interaction.depth=</span><span class="dv">4</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>yhat.boost<span class="ot">=</span><span class="fu">predict</span>(boost.boston,<span class="at">newdata=</span>Boston,<span class="at">n.trees=</span><span class="dv">5000</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost<span class="sc">-</span>Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.0004</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boost.boston, <span class="at">plotit=</span><span class="cn">FALSE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">var</th>
<th style="text-align: right;">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">lstat</td>
<td style="text-align: left;">lstat</td>
<td style="text-align: right;">36.32</td>
</tr>
<tr class="even">
<td style="text-align: left;">rm</td>
<td style="text-align: left;">rm</td>
<td style="text-align: right;">30.98</td>
</tr>
<tr class="odd">
<td style="text-align: left;">dis</td>
<td style="text-align: left;">dis</td>
<td style="text-align: right;">7.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">crim</td>
<td style="text-align: left;">crim</td>
<td style="text-align: right;">5.09</td>
</tr>
<tr class="odd">
<td style="text-align: left;">nox</td>
<td style="text-align: left;">nox</td>
<td style="text-align: right;">4.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">age</td>
<td style="text-align: left;">age</td>
<td style="text-align: right;">4.50</td>
</tr>
<tr class="odd">
<td style="text-align: left;">black</td>
<td style="text-align: left;">black</td>
<td style="text-align: right;">3.45</td>
</tr>
<tr class="even">
<td style="text-align: left;">ptratio</td>
<td style="text-align: left;">ptratio</td>
<td style="text-align: right;">3.11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tax</td>
<td style="text-align: left;">tax</td>
<td style="text-align: right;">1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">rad</td>
<td style="text-align: left;">rad</td>
<td style="text-align: right;">1.17</td>
</tr>
<tr class="odd">
<td style="text-align: left;">indus</td>
<td style="text-align: left;">indus</td>
<td style="text-align: right;">0.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">chas</td>
<td style="text-align: left;">chas</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr class="odd">
<td style="text-align: left;">zn</td>
<td style="text-align: left;">zn</td>
<td style="text-align: right;">0.13</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"rm"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"lstat"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-20-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Boosting vs Random Forests</strong>: * <strong>Boosting</strong> often provides better accuracy by correcting specific errors (residuals), but is more prone to overfitting and noise. It requires careful tuning of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(B\)</span>. * <strong>Random Forests</strong> are more robust “out of the box”, easier to parallelize (trees are independent), and less prone to overfitting, but might not reach the same peak accuracy as a well-tuned boosting model.</p>
</section>
</section>
<section id="sec-bart-causal" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="sec-bart-causal"><span class="header-section-number">14.5</span> BART for causal inference</h2>
<p>Estimating the causal effect of an intervention, such as a new drug, a marketing campaign, or a public policy, is a central goal across science and industry. While the gold standard for causal inference is the Randomized Controlled Trial (RCT), it is often infeasible, unethical, or too expensive to conduct. Researchers must therefore turn to observational data, where the assignment of treatment is not controlled by the investigator. This introduces a fundamental challenge: individuals who receive the treatment may be systematically different from those who do not, a problem known as confounding. Separating the true effect of the treatment from these pre-existing differences is the primary task of causal inference from observational data.</p>
<p>To formalize causal questions, we rely on the Rubin Causal Model (RCM), also known as the potential outcomes framework. For a binary treatment <span class="math inline">\(Z\)</span> (where <span class="math inline">\(Z_i=1\)</span> if individual <span class="math inline">\(i\)</span> receives the treatment and <span class="math inline">\(Z_i=0\)</span> otherwise), we posit that each individual <span class="math inline">\(i\)</span> has two potential outcomes: * <span class="math inline">\(Y_i(1)\)</span>: The outcome that would be observed if individual <span class="math inline">\(i\)</span> were exposed to the treatment. * <span class="math inline">\(Y_i(0)\)</span>: The outcome that would be observed if individual <span class="math inline">\(i\)</span> were exposed to the control (no treatment).</p>
<p>This framework leads directly to what Holland (1986) termed the “fundamental problem of causal inference”: for any given individual, we can only ever observe one of these two potential outcomes. The outcome we do not see is the counterfactual. Causal inference can thus be viewed as a missing data problem, where the goal is to estimate the values of the unobserved potential outcomes.</p>
<p>From this foundation, we can define several key causal quantities, or estimands:</p>
<ul>
<li>Individual Treatment Effect (ITE): The effect for a single individual, defined as <span class="math display">\[\tau_i = Y_i(1) - Y_i(0).\]</span> This is typically unobservable.</li>
<li>Average Treatment Effect (ATE): The average effect across the entire population, <span class="math display">\[\tau_{ATE} = \E{Y(1) - Y(0)}.\]</span> This is often the primary estimand of interest for broad policy questions.</li>
<li>Average Treatment Effect on the Treated (ATT): The average effect for those who actually received the treatment, <span class="math display">\[\tau_{ATT} = \E{Y(1) - Y(0) \mid Z=1}.\]</span></li>
<li>Conditional Average Treatment Effect (CATE): The average effect for a subpopulation defined by a set of covariates <span class="math inline">\(X=x\)</span>, <span class="math display">\[\tau(x) = \E{Y(1) - Y(0) \mid X=x}.\]</span> Understanding the CATE allows for the exploration of treatment effect heterogeneity.</li>
</ul>
<p>To estimate these causal estimands from observational data, we must rely on a set of critical, untestable assumptions that connect the observed data to the unobserved potential outcomes. These are known as identification assumptions.</p>
<ol type="1">
<li>Stable Unit Treatment Value Assumption (SUTVA): This assumption has two parts. First, it assumes there is no interference between units, meaning one individual’s treatment status does not affect another’s outcome. Second, it assumes there are no hidden variations of the treatment; the treatment assigned to one individual is the same as the treatment assigned to any other.</li>
<li>Ignorability (or Unconfoundedness): This is the most crucial assumption. It states that, conditional on a set of observed pre-treatment covariates <span class="math inline">\(X\)</span>, treatment assignment <span class="math inline">\(Z\)</span> is independent of the potential outcomes: <span class="math display">\[(Y(0), Y(1)) \perp Z \mid X\]</span>. In essence, it assumes that we have measured all the common causes of both treatment selection and the outcome. If this holds, then within any stratum defined by the covariates <span class="math inline">\(X\)</span>, the treatment assignment is “as-if” random.</li>
<li>Positivity (or Overlap/Common Support): This assumption requires that for any set of covariate values <span class="math inline">\(x\)</span> present in the population, there is a non-zero probability of being in either the treatment or the control group: <span class="math inline">\(0 &lt; P(Z=1 \mid X=x) &lt; 1\)</span>. This ensures that we can find both treated and control individuals with similar characteristics, making comparison meaningful and avoiding extrapolation to regions with no data.</li>
</ol>
<p>To demonstrate the application of Bayesian methods to this challenge, we use the famous Lalonde dataset, a canonical benchmark in the causal inference literature. The dataset addresses a real-world policy question: evaluating the effectiveness of the National Supported Work (NSW) Demonstration, a federally funded job training program implemented in the US from 1975-1979. The program was designed to help individuals facing significant social and economic barriers (e.g., former drug addicts, ex-convicts, high school dropouts) improve their labor market prospects. The treatment (<span class="math inline">\(treat\)</span>) is participation in this program, and the primary outcome (<span class="math inline">\(re78\)</span>) is the individual’s real earnings in 1978, after the program.</p>
<p>The historical importance of this dataset stems from Robert Lalonde’s 1986 paper, which delivered a powerful critique of the non-experimental methods used at the time. Lalonde started with data from an actual RCT, which provided an unbiased estimate of the program’s effect. He then took the treated group from the experiment but replaced the experimental control group with a non-experimental comparison group drawn from large public surveys—the Panel Study of Income Dynamics (PSID) and the Current Population Survey (CPS). He showed that the standard econometric models of the era failed to replicate the experimental benchmark when applied to this new, confounded dataset, casting serious doubt on their reliability for policy evaluation. Our task is to see if a modern, flexible Bayesian method—Bayesian Additive Regression Trees (BART)—can succeed where these earlier methods failed.</p>
<p>The challenge posed by the Lalonde dataset becomes immediately apparent when we examine the pre-treatment characteristics of the treated group versus the non-experimental control group. A naive comparison of their 1978 earnings would be deeply misleading because the groups were profoundly different before the program even began. <a href="#tbl-imbalance" class="quarto-xref">Table&nbsp;<span>14.1</span></a> illustrates this imbalance for key covariates, including age, education, race, marital status, and earnings in the years prior to the intervention (1974 and 1975).</p>
<p>The Standardized Mean Difference (SMD) provides a scale-free measure of the difference between the group means. A common rule of thumb suggests that an absolute SMD greater than 0.1 indicates a potentially meaningful imbalance. As the table shows, the groups differ substantially on nearly every measured characteristic. The treated individuals were younger, less educated, more likely to be from minority groups, and had drastically lower earnings in the years before the program. This severe selection bias is precisely what makes the Lalonde dataset such a difficult and important test case for causal inference methods. Any credible method must be able to adjust for these vast pre-existing differences to isolate the true causal effect of the job training program.</p>
<div id="tbl-imbalance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.1: Covariate Balance in the Lalonde Non-Experimental Dataset. <em>Note: Data corresponds to the widely used Dehejia and Wahba (1999) sample of the Lalonde dataset. Standardized Mean Difference is calculated as the difference in means divided by the pooled standard deviation.</em>
</figcaption>
<div aria-describedby="tbl-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Covariate</th>
<th style="text-align: left;">Treated Mean</th>
<th style="text-align: left;">Control Mean</th>
<th style="text-align: left;">Std. Mean Diff.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Age (years)</td>
<td style="text-align: left;">25.82</td>
<td style="text-align: left;">28.04</td>
<td style="text-align: left;">-0.31</td>
</tr>
<tr class="even">
<td style="text-align: left;">Education (years)</td>
<td style="text-align: left;">10.35</td>
<td style="text-align: left;">10.23</td>
<td style="text-align: left;">0.06</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Black (indicator)</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">0.20</td>
<td style="text-align: left;">1.84</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hispanic (indicator)</td>
<td style="text-align: left;">0.06</td>
<td style="text-align: left;">0.14</td>
<td style="text-align: left;">-0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Married (indicator)</td>
<td style="text-align: left;">0.19</td>
<td style="text-align: left;">0.51</td>
<td style="text-align: left;">-0.81</td>
</tr>
<tr class="even">
<td style="text-align: left;">No Degree (indicator)</td>
<td style="text-align: left;">0.71</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Earnings 1974</td>
<td style="text-align: left;">2095.57</td>
<td style="text-align: left;">5630.71</td>
<td style="text-align: left;">-0.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">Earnings 1975</td>
<td style="text-align: left;">1532.06</td>
<td style="text-align: left;">5205.52</td>
<td style="text-align: left;">-0.65</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To address the challenge of confounding, we need a method that can flexibly model the relationship between the outcome, the treatment, and the many covariates shown to be imbalanced. Bayesian Additive Regression Trees (BART) is a powerful non-parametric machine learning algorithm that is exceptionally well-suited for this task. It combines the predictive power of ensemble methods with a rigorous Bayesian framework for regularization and uncertainty quantification.</p>
<p>At its core, BART models the expected value of an outcome <span class="math inline">\(Y\)</span> as a sum of many individual regression trees. For a set of predictors <span class="math inline">\(x\)</span>, the model is:</p>
<p><span class="math display">\[Y = \sum_{j=1}^{m} g(x; T_j, M_j) + \epsilon, \quad \text{where} \quad \epsilon \sim N(0, \sigma^2)\]</span></p>
<p>Here, <span class="math inline">\(m\)</span> is the number of trees in the ensemble (typically around 200), and each function <span class="math inline">\(g(x; T_j, M_j)\)</span> represents a single regression tree. The structure of the tree is denoted by <span class="math inline">\(T_j\)</span>, and <span class="math inline">\(M_j\)</span> is the set of parameter values in its terminal nodes (or leaves).</p>
<p>Crucially, each individual tree is designed to be a “weak learner”. It is kept shallow and simple, meaning it explains only a small fraction of the variation in the outcome. The final, powerful prediction comes from summing up the contributions of all these simple components. This sum-of-trees structure allows BART to automatically capture very complex relationships, including high-order interactions and non-linearities, without the user needing to specify them in advance. For example, an interaction between age and education is implicitly modeled if a tree splits on education within a branch that has already been split on age. This flexibility is a major advantage in observational studies where the true functional form of the relationship between the outcome and the confounders is unknown.</p>
<p>In most machine learning algorithms, overfitting is controlled through techniques like cross-validation or complexity penalties. BART, being a fully Bayesian method, achieves this through a carefully specified set of regularization priors. These priors are designed to keep each tree simple and prevent any single tree from dominating the overall fit.</p>
<p>The key priors are:</p>
<ol type="1">
<li>Prior on Tree Structure: This prior strongly encourages shallow trees. It is defined by a rule governing the probability that a node at a certain depth <span class="math inline">\(d\)</span> will be split further. This probability is typically modeled as <span class="math display">\[p(T_j) = \alpha(1+d)^{-\beta},\]</span> where <span class="math inline">\(\alpha \in (0,1)\)</span> and <span class="math inline">\(\beta \ge 0\)</span> are hyperparameters. Setting <span class="math inline">\(\beta\)</span> to a value like 2 ensures that the probability of splitting decreases rapidly with depth, keeping the trees small.</li>
<li>Prior on Terminal Node Parameters: After the response variable <span class="math inline">\(Y\)</span> is centered and scaled, the values <span class="math inline">\(\mu_{jk}\)</span> in the terminal nodes of each tree are given a Normal prior, <span class="math display">\[
\mu_{jk} \sim N(0, \sigma_{\mu}^2).
\]</span> This prior shrinks the predictions within each leaf towards zero. Because the final prediction is a sum over <span class="math inline">\(m\)</span> trees, this shrinkage ensures that the contribution of each individual tree is small.</li>
<li>Prior on Error Variance: The residual variance <span class="math inline">\(\sigma^2\)</span> is typically given a conjugate Inverse-Gamma prior. This prior is usually chosen to be weakly informative, allowing the data to dominate the posterior estimate of the noise level, but it still constrains the variance to be reasonable.</li>
</ol>
<p>Together, these priors act as a sophisticated regularization mechanism that allows BART to fit complex functions while being highly resistant to overfitting.</p>
<p>BART models are fit using a Markov chain Monte Carlo (MCMC) algorithm, specifically a form of Gibbs sampler known as Bayesian backfitting. The algorithm does not find a single “best” model. Instead, it generates thousands of samples from the joint posterior distribution of all model parameters: <span class="math inline">\(p(T_1,\ldots,T_m, M_1,\ldots,M_m, \sigma \mid Y, X)\)</span>.</p>
<p>The fitting process works iteratively:</p>
<ol type="1">
<li>Initialize all <span class="math inline">\(m\)</span> trees and <span class="math inline">\(\sigma\)</span>.</li>
<li>For each tree <span class="math inline">\(j\)</span> from 1 to <span class="math inline">\(m\)</span>:
<ol type="a">
<li>Calculate the “partial residual” by subtracting the predictions of all other trees from the outcome: <span class="math display">\[R_j = Y - \sum_{k \neq j} g(x; T_k, M_k)\]</span>.</li>
<li>Draw a new tree structure <span class="math inline">\(T_j\)</span> and its leaf parameters <span class="math inline">\(M_j\)</span> from their posterior distribution conditional on this partial residual, <span class="math display">\[p(T_j, M_j \mid R_j, \sigma).\]</span></li>
</ol></li>
<li>After iterating through all trees, draw a new value for <span class="math inline">\(\sigma\)</span> from its posterior conditional on the current residuals.</li>
<li>Repeat steps 2 and 3 for thousands of iterations.</li>
</ol>
<p>The output of this process is not one set of trees, but a collection of (e.g., 5000) sets of trees, where each set represents a plausible regression function drawn from the posterior distribution. This collection of draws is the key to quantifying uncertainty in a Bayesian way.</p>
<p>The power of BART for causal inference lies in how it leverages the full posterior distribution to estimate counterfactuals. The strategy aligns perfectly with the Bayesian view of causal inference as a missing data problem, as articulated by Rubin (1978).</p>
<p>The standard approach for causal inference with BART is to model the outcome <span class="math inline">\(Y\)</span> as a function of both the covariates <span class="math inline">\(X\)</span> and the treatment indicator <span class="math inline">\(Z\)</span>. The model learns a single, flexible response surface:</p>
<p><span class="math display">\[\E{Y \mid X, Z} = f(X, Z)\]</span></p>
<p>Here, the treatment <span class="math inline">\(Z\)</span> is included as if it were “just another covariate” in the set of predictors fed to the BART algorithm. The model is free to discover how the effect of <span class="math inline">\(Z\)</span> varies with <span class="math inline">\(X\)</span> through the tree-splitting process. The Conditional Average Treatment Effect (CATE) is then simply the difference in the predictions from this learned function:</p>
<p><span class="math display">\[\tau(x) = f(x, Z=1) - f(x, Z=0)\]</span></p>
<p>The core of the estimation process is a predictive step that is repeated for each draw from the MCMC sampler. Suppose the MCMC algorithm has produced <span class="math inline">\(S\)</span> posterior draws of the function <span class="math inline">\(f\)</span>. For each draw <span class="math inline">\(s = 1,\ldots, S\)</span>:</p>
<ol type="1">
<li>We take the full dataset of <span class="math inline">\(n\)</span> individuals with their observed covariates <span class="math inline">\(X\)</span>.</li>
<li>We create two hypothetical, or counterfactual, datasets:
<ul>
<li>Treated World: The observed covariates <span class="math inline">\(X\)</span> for all <span class="math inline">\(n\)</span> individuals, but with the treatment indicator set to <span class="math inline">\(Z=1\)</span> for everyone.</li>
<li>Control World: The observed covariates <span class="math inline">\(X\)</span> for all <span class="math inline">\(n\)</span> individuals, but with the treatment indicator set to <span class="math inline">\(Z=0\)</span> for everyone.</li>
</ul></li>
<li>Using the fitted BART model corresponding to posterior draw <span class="math inline">\(s\)</span> (i.e., <span class="math inline">\(f^{(s)}\)</span>), we predict the outcome for every individual under both scenarios. This gives us a full set of posterior predictive draws for the potential outcomes: <span class="math inline">\(\tilde{Y}_i(1)^{(s)}\)</span> and <span class="math inline">\(\tilde{Y}_i(0)^{(s)}\)</span> for each individual <span class="math inline">\(i\)</span>.</li>
</ol>
<p>This process is a direct implementation of the missing data analogy. For an individual <span class="math inline">\(i\)</span> who was actually treated (<span class="math inline">\(Z_i=1\)</span>), their observed outcome <span class="math inline">\(Y_i\)</span> is their potential outcome <span class="math inline">\(Y_i(1)\)</span>. The BART model provides a posterior predictive draw for their missing counterfactual outcome, <span class="math inline">\(\tilde{Y}_i(0)^{(s)}\)</span>. Conversely, for a control subject, we use the model to predict their missing <span class="math inline">\(\tilde{Y}_i(1)^{(s)}\)</span>.</p>
<p>Once we have the posterior draws of the potential outcomes for every individual at each MCMC iteration, we can compute a posterior draw for any causal estimand of interest. For example, at each iteration <span class="math inline">\(s\)</span>:</p>
<ul>
<li>ITE draw: <span class="math display">\[\tau_i^{(s)} = \tilde{Y}_i(1)^{(s)} - \tilde{Y}_i(0)^{(s)}\]</span></li>
<li>ATE draw: <span class="math display">\[\tau_{ATE}^{(s)} = \frac{1}{n} \sum_{i=1}^{n} \tau_i^{(s)}\]</span></li>
</ul>
<p>By collecting these values across all <span class="math inline">\(S\)</span> MCMC iterations, we obtain <span class="math display">\[\{\tau_{ATE}^{(1)}, \tau_{ATE}^{(2)},\ldots, \tau_{ATE}^{(S)}\}.\]</span> This set is a Monte Carlo approximation of the entire posterior distribution of the Average Treatment Effect.</p>
<p>This is a profoundly powerful result. Instead of a single point estimate and a standard error, the Bayesian approach yields a full probability distribution for the unknown causal effect. From this posterior distribution, we can easily calculate a posterior mean (our best point estimate) and a 95% credible interval. Unlike a frequentist confidence interval, the Bayesian credible interval has a direct and intuitive probabilistic interpretation: given our data and model, there is a 95% probability that the true value of the ATE lies within this range. This propagation of uncertainty from the model parameters all the way to the final causal estimate is a hallmark of the Bayesian approach.</p>
<p>We now apply this framework to the Lalonde dataset to estimate the causal effect of the NSW job training program on 1978 earnings.</p>
<p>The analysis is streamlined by using the <code>bartCause</code> package in <code>R</code>, which is specifically designed for causal inference with BART. The package provides a wrapper around the core <code>dbarts</code> implementation, simplifying the process of fitting the model and generating counterfactuals. A typical function call would look like this:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the package and data</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bartCause)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(lalonde)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define confounders</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>confounders <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">'age'</span>, <span class="st">'educ'</span>, <span class="st">'black'</span>, <span class="st">'hisp'</span>, <span class="st">'married'</span>, <span class="st">'nodegr'</span>, <span class="st">'re74'</span>, <span class="st">'re75'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the BART model</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">bartc</span>(</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">response =</span> lalonde<span class="sc">$</span>re78,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">treatment =</span> lalonde<span class="sc">$</span>treat,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">confounders =</span> lalonde[, confounders],</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">estimand =</span> <span class="st">"ate"</span>,</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">commonSup.rule =</span> <span class="st">"sd"</span> <span class="co"># Rule to handle poor overlap</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In this call, we specify the outcome (<code>re78</code>), the binary treatment (treat), and the matrix of pre-treatment confounders. We set estimand = <code>ate</code> to target the Average Treatment Effect.</p>
<p>Before interpreting the causal estimates, it is essential to perform MCMC diagnostics to ensure the algorithm has converged to a stable posterior distribution. The bartCause package provides plotting functions for this purpose. Trace plots for key parameters, such as the posterior draws of the ATE and the residual standard deviation (<span class="math inline">\(\sigma\)</span>), should be examined. These plots should show the chains mixing well and exploring a consistent region of the parameter space, without long-term drifts or stuck periods, indicating that the sampler has converged.</p>
<p>The primary result can be obtained by calling <code>summary(fit)</code>. This provides the posterior mean of the ATE, which serves as our point estimate, along with a 95% credible interval. For a richer view, we can plot the entire posterior distribution of the ATE, which visualizes our uncertainty about the treatment effect.</p>
<p>The true power of this result is seen when placed in the context of other estimates, as shown in <a href="#tbl-ate-estimate" class="quarto-xref">Table&nbsp;<span>14.2</span></a>. The naive difference in means between the treated and control groups in the non-experimental data is large and negative, a direct consequence of the severe confounding. The experimental benchmark from the original RCT for this subset of treated individuals is an earnings gain of approximately $886. The BART estimate, after adjusting for the observed confounders, is remarkably close to this benchmark. This result demonstrates that a flexible, non-parametric Bayesian model like BART can successfully overcome the severe selection bias that plagued earlier econometric methods.</p>
<div id="tbl-ate-estimate" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ate-estimate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.2: Comparison of ATE Estimates for the NSW Program. Note: Estimates are for the non-experimental Lalonde sample (treated units from NSW, control units from PSID). The experimental benchmark is the difference-in-means estimate from the randomized trial for the same treated units. Uncertainty for BART is the posterior standard deviation
</figcaption>
<div aria-describedby="tbl-ate-estimate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">ATE Estimate</th>
<th style="text-align: left;">Uncertainty (Std. Dev. / Interval)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Experimental Benchmark</td>
<td style="text-align: left;">886.3</td>
<td style="text-align: left;">-277.37</td>
</tr>
<tr class="even">
<td style="text-align: left;">Naive Difference-in-Means</td>
<td style="text-align: left;">-8492.24</td>
<td style="text-align: left;">-633.91</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Propensity Score Matching</td>
<td style="text-align: left;">1079.13</td>
<td style="text-align: left;">-158.59</td>
</tr>
<tr class="even">
<td style="text-align: left;">Double Machine Learning</td>
<td style="text-align: left;">370.94</td>
<td style="text-align: left;">-394.68</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Causal BART</td>
<td style="text-align: left;">818.79</td>
<td style="text-align: left;">-184.46</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While the ATE provides a useful summary, it can mask important variations in how the treatment affects different people. A policy might be beneficial on average but ineffective or even harmful for certain subgroups. A key advantage of BART is its ability to move beyond the average and explore this Heterogeneous Treatment Effect (HTE), which is critical for developing more targeted and effective policies.</p>
<p>Estimating HTE allows us to answer questions like: “For whom does this program work best?” or “Are there individuals for whom the program is detrimental?” In settings with limited resources, this information is vital for allocating the intervention to those most likely to benefit. The flexibility of BART, which does not assume a constant treatment effect, makes it an ideal tool for this task.</p>
<p>Because BART provides a posterior predictive distribution of potential outcomes for every individual in the dataset, we can estimate an Individual Conditional Average Treatment Effect (ICATE) for each person. By plotting a histogram of the posterior means of these ICATEs, we can visualize the distribution of effects across the sample. This reveals whether the effect is consistent for everyone or if there is substantial variation, with some individuals benefiting much more than others.</p>
<p>To understand what drives this heterogeneity, we can examine how the estimated CATE varies as a function of key pre-treatment covariates. These relationships are often visualized using partial dependence plots. For the Lalonde data, such analyses have revealed that the effect of the job training program is not constant but varies non-linearly with characteristics like age and pre-treatment income (re74). For instance, the program’s benefit might increase with age up to a certain point and then decline, or it might be most effective for individuals with low-to-moderate prior earnings but less so for those with very low or higher earnings. These are nuanced, data-driven insights that would be completely missed by a standard linear regression model that only estimates a single average effect.</p>
<p>A subtle but important issue can arise when using flexible regularized models like BART for causal inference in the presence of strong confounding, as is the case here. The regularization priors, which are designed to prevent overfitting, can shrink the estimated effects of the confounders towards zero. Because the treatment Z is highly correlated with these confounders, the model may mistakenly attribute some of the effect of the confounders to the treatment, leading to a bias known as Regularization-Induced Confounding (RIC).</p>
<p>A powerful solution, proposed by <span class="citation" data-cites="hahn2020bayesian">Hahn, Murray, and Carvalho (<a href="references.html#ref-hahn2020bayesian" role="doc-biblioref">2020</a>)</span>, is to first estimate the propensity score, <span class="math inline">\(\pi(x) = P(Z=1 \mid X)\)</span>, which is the probability of receiving treatment given the covariates X. This score serves as a one-dimensional summary of all confounding information. This estimated propensity score is then included as an additional predictor in the BART outcome model. By providing this confounding summary directly to the model, we help the BART algorithm differentiate between the prognostic effects of the covariates (captured by <span class="math inline">\(\pi(x)\)</span>) and the causal effect of the treatment Z, thereby mitigating RIC. This “ps-BART” approach is considered state-of-the-art and is easily implemented in the <code>bartCause</code> package by setting the argument <code>p.scoreAsCovariate = TRUE</code>.</p>
<section id="bart-versus-propensity-score-matching-psm" class="level3">
<h3 class="anchored" data-anchor-id="bart-versus-propensity-score-matching-psm">BART versus Propensity Score Matching (PSM)</h3>
<p>BART is one of several methods for causal inference from observational data. It is instructive to compare its philosophy with that of another widely used technique: Propensity Score Matching (PSM). BART and PSM represent two different philosophies for tackling confounding. Propensity Score Matching (PSM): This approach focuses on the design of the study. The goal is to use the observed data to construct a new sample in which the treatment and control groups are balanced on their observed covariates, thereby mimicking the properties of an RCT. The propensity score is the central tool used to achieve this balance. The analysis of the outcome is then performed on this newly created, “balanced” dataset.</p>
<p>BART focuses on the analysis stage. The goal is to build a highly flexible and accurate predictive model for the outcome that explicitly includes the treatment and confounders, <span class="math inline">\(\E{Y \mid X,Z}\)</span>. It uses the full dataset and relies on the model’s ability to correctly adjust for the confounding variables to isolate the causal effect.</p>
<p>Each approach has its own set of advantages and disadvantages. PSM is often praised for its transparency; one can assess the quality of the covariate balance achieved by the matching procedure before ever looking at the outcome variable, reducing the risk of “p-hacking” or specification searching. However, PSM can be inefficient, as it often requires discarding a significant portion of the control group that does not have good matches in the treated group (i.e., poor overlap). It can also suffer from residual confounding if the matches are not sufficiently close. BART, on the other hand, is highly efficient as it uses all available data. Its main strengths are its flexibility in capturing unknown functional forms and interactions, its ability to easily estimate heterogeneous effects, and its principled framework for uncertainty quantification. Its primary weakness is that it can be perceived as a “black box” if not diagnosed carefully. Its validity, like all modeling approaches, depends on the untestable ignorability assumption, and as discussed, it can be susceptible to regularization-induced confounding if not applied with care.</p>
<p>In modern practice, the line between these two philosophies is blurring. It is now common to see them used in conjunction. For example, many practitioners use flexible machine learning models, including BART itself, to estimate the propensity scores used for matching or weighting, which can improve the quality of the covariate balance over simpler logistic regression models. Conversely, the state-of-the-art application of BART for causal inference (ps-BART) incorporates the propensity score directly into the outcome model. This convergence reflects a mature understanding that both balancing the data structure and flexibly modeling the outcome are complementary and powerful tools for robust causal inference.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 31%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Propensity Score Matching (PSM)</th>
<th>Bayesian Additive Regression Trees (BART)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Primary Goal</td>
<td>Create balanced treatment/control groups (Design)</td>
<td>Flexibly model the outcome-covariate relationship (Analysis)</td>
</tr>
<tr class="even">
<td>Use of Data</td>
<td>Often discards unmatched units, reducing sample size</td>
<td>Uses all available data</td>
</tr>
<tr class="odd">
<td>Confounding Control</td>
<td>Achieved by balancing covariates via matching/weighting</td>
<td>Achieved by conditioning on covariates in a flexible model</td>
</tr>
<tr class="even">
<td>Key Assumption</td>
<td>Correct specification of the propensity score model</td>
<td>Correct specification of the outcome model (though BART is very flexible)</td>
</tr>
<tr class="odd">
<td>Treatment Effect</td>
<td>Primarily estimates ATT; ATE can be harder to estimate</td>
<td>Easily estimates ATE, ATT, and CATE/HTE</td>
</tr>
<tr class="even">
<td>Uncertainty</td>
<td>Often requires bootstrapping for standard errors</td>
<td>Provides full posterior distributions and credible intervals naturally</td>
</tr>
<tr class="odd">
<td>Flexibility</td>
<td>Limited by the PS model; main effect is assumed constant after matching</td>
<td>Highly flexible; automatically models non-linearities and interactions</td>
</tr>
</tbody>
</table>
<p>:: Conceptual Comparison of BART and Propensity Score Matching</p>
<p>This example shows that BART, a flexible non-parametric method, can successfully adjust for severe confounding and recover a causal estimate that is remarkably close to the experimental benchmark, a feat that eluded many of the methods available when Lalonde first published his critique. It is crucial to remember that BART is not a panacea. Its validity, like that of any non-experimental method, rests on the untestable assumption of ignorability—that we have measured and adjusted for all relevant confounding variables.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Randomization remains the cleanest route to causal identification, but it is not always feasible. When randomization is absent, modern Bayesian methods like BART offer a principled path forward: they combine flexible non-parametric modeling with uncertainty quantification and can, under the right assumptions, recover causal effects that approach the quality of experimental benchmarks. The broader narrative on experiments, randomization, and when we must rely on observational data is developed in <a href="05-ab.html" class="quarto-xref"><span>Chapter 5</span></a>.</p>
</section>
</section>
<section id="appendix-theoretical-foundations" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> Appendix: Theoretical Foundations</h1>
<section id="why-ensembles-work-a-geometric-perspective" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="why-ensembles-work-a-geometric-perspective"><span class="header-section-number">15.1</span> Why Ensembles Work: A Geometric Perspective</h2>
<p>The ensemble methods we’ve discussed—bagging, random forests, and boosting—all share a common goal: improving predictive performance by combining multiple weak learners. But why does this work so well? The answer lies in the <strong>bias-variance tradeoff</strong> and the geometry of high-dimensional spaces.</p>
<section id="smoothing-and-variance-reduction" class="level3">
<h3 class="anchored" data-anchor-id="smoothing-and-variance-reduction">Smoothing and Variance Reduction</h3>
<p>In essence, individual decision trees are low-bias but high-variance estimators. They can capture complex patterns (low bias) but are sensitive to noise (high variance). When we average many such trees, as in Bagging or Random Forests, we are effectively performing <strong>smoothing</strong>.</p>
<p>Quantitatively, if we have <span class="math inline">\(N\)</span> uncorrelated predictors <span class="math inline">\(f_1, \dots, f_N\)</span>, the variance of their average is: <span class="math display">\[
\text{Var}\left(\frac{1}{N}\sum_{i=1}^N f_i\right) = \frac{1}{N} \text{Var}(f_i)
\]</span> Averaging <span class="math inline">\(N\)</span> uncorrelated models reduces the variance by a factor of <span class="math inline">\(N\)</span>. This is the <strong><span class="math inline">\(1/N\)</span> rule</strong>. In practice, the trees in a forest are correlated, so the reduction isn’t quite <span class="math inline">\(1/N\)</span>, but the principle holds: the more diverse (uncorrelated) the trees, the more variance reduction we get. This is why Random Forests Randomly select features at each split—to force the trees to be different.</p>
</section>
<section id="the-problem-of-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-high-dimensions">The Problem of High Dimensions</h3>
<p>One might ask: “Why not just use K-Nearest Neighbors (KNN)? It also averages local points.” The problem is the <strong>curse of dimensionality</strong>. In high-dimensional feature spaces, data points become incredibly sparse.</p>
<p>Consider a 50-dimensional sphere. As shown in <a href="#fig-hd-ball" class="quarto-xref">Figure&nbsp;<span>15.1</span></a>, if we sample points uniformly, almost all of them will reside near the “crust” or surface of the sphere, not in the center.</p>
<div id="fig-hd-ball" class="quarto-float quarto-figure quarto-figure-center anchored" width="40%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hd_ball.svg" id="fig-hd-ball" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1
</figcaption>
</figure>
</div>
<p>This phenomenon means that in high dimensions, “local neighbors” are not actually close to you—they are far away on the other side of the space. A standard KNN algorithm fails because it averages points that aren’t truly similar.</p>
</section>
<section id="trees-as-adaptive-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="trees-as-adaptive-nearest-neighbors">Trees as “Adaptive” Nearest Neighbors</h3>
<p>Decision trees solve this by defining “neighbors” differently. Instead of using a fixed distance metric (like Euclidean distance), trees define a neighborhood as a rectangular box (or <strong>cylindrical region</strong>) learned from the data (<a href="#fig-cilinder" class="quarto-xref">Figure&nbsp;<span>15.2</span></a>).</p>
<div id="fig-cilinder" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/cylinder_kernel.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/rf_kernel.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Cylindrical kernels for trees (left) and random forests (right).
</figcaption>
</figure>
</div>
<p>Constructing the regions is fundamental to reducing the curse of dimensionality. It is useful to imagine a very large dataset, e.g., 100k images, and think about how a new image’s input coordinates, <span class="math inline">\(X\)</span>, are “neighbors” to data points in the training set. Our predictor will then be a smart conditional average of the observed outputs, <span class="math inline">\(Y\)</span>, for our neighbors. When <span class="math inline">\(p\)</span> is large, spheres (<span class="math inline">\(L^2\)</span> balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.</p>
<p>To illustrate the problem further, <a href="#fig-hd-ball" class="quarto-xref">Figure&nbsp;<span>15.1</span></a> below shows the 2D image of 1000 uniform samples from a 50-dimensional ball <span class="math inline">\(B_{50}\)</span>. The image is calculated as <span class="math inline">\(w^T Y\)</span>, where <span class="math inline">\(w = (1,1,0,\ldots,0)\)</span> and <span class="math inline">\(Y \sim U(B_{50})\)</span>. Samples are centered around the equators and none of the samples fall close to the boundary of the set.</p>
<p>As dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from <a href="#fig-hd-hist" class="quarto-xref">Figure&nbsp;<span>15.3</span></a>, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e.&nbsp;<span class="math inline">\(e_1^T Y\)</span>, where <span class="math inline">\(e_1 = (1,0,\ldots,0)\)</span>.</p>
<div id="fig-hd-hist" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_100.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_200.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_300.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_400.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: Histogram of marginal distribution of <span class="math inline">\(Y\sim U(B_p)\)</span> for different dimensions <span class="math inline">\(p\)</span> (x-axis).
</figcaption>
</figure>
</div>
<p>Similar central limit results were known to Maxwell who showed that random variable <span class="math inline">\(w^TY\)</span> is close to standard normal, when <span class="math inline">\(Y \sim U(B_p)\)</span>, <span class="math inline">\(p\)</span> is large, and <span class="math inline">\(w\)</span> is a unit vector (lies on the boundary of the ball). For the history of this fact, see <span class="citation" data-cites="diaconis1987dozen">Diaconis and Freedman (<a href="references.html#ref-diaconis1987dozen" role="doc-biblioref">1987</a>)</span>. More general results in this direction were obtained in <span class="citation" data-cites="klartag2007central">Klartag (<a href="references.html#ref-klartag2007central" role="doc-biblioref">2007</a>)</span>. Further, <span class="citation" data-cites="milman2009asymptotic">Milman and Schechtman (<a href="references.html#ref-milman2009asymptotic" role="doc-biblioref">2009</a>)</span> presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.</p>
<p>Deep learning improves on this by performing a sequence of GLM-like transformations; effectively, DL learns a distributed partition of the input space. Specifically, suppose that we have <span class="math inline">\(K\)</span> partitions. Then the DL predictor takes the form of a weighted average, or in the case of classification, a soft-max of the weighted average of observations in this partition. Given a new high-dimensional input <span class="math inline">\(X_{\mathrm{new}}\)</span>, many deep learners are an average of learners obtained by our hyperplane decomposition. Generically, we have</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{k \in K} w_k(X)\hat{Y}_k(X),
\]</span> where <span class="math inline">\(w_k\)</span> are the weights learned in region <span class="math inline">\(k\)</span>, and <span class="math inline">\(w_k(X)\)</span> is an indicator of the region with appropriate weighting given the training data. The weight <span class="math inline">\(w_k\)</span> also indicates which partition the new <span class="math inline">\(X_{\mathrm{new}}\)</span> lies in.</p>
<p>The use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form of clever conditional averaging) are prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, with the caveat that they have large variances due to dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the <span class="math inline">\(1/N\)</span>-rule and average to reduce risk. Specifically, suppose that we have <span class="math inline">\(K\)</span> exchangeable, <span class="math inline">\(\mathbb{E} ( \hat{Y}_i ) = \mathbb{E} ( \hat{Y}_{\pi(i)} )\)</span>, predictors</p>
<p><span class="math display">\[
\hat{Y} = ( \hat{Y}_1 , \ldots , \hat{Y}_K )
\]</span></p>
<p>Find <span class="math inline">\(w\)</span> to attain <span class="math inline">\(\operatorname{argmin}_W E l( Y , w^T \hat{Y} )\)</span> where <span class="math inline">\(l\)</span> convex in the second argument;</p>
<p><span class="math display">\[
E l( Y , w^T \hat{Y} ) = \frac{1}{K!} \sum_\pi E l( Y , w^T \hat{Y} ) \geq E l \left ( Y , \frac{1}{K!} \sum_\pi w_\pi^T \hat{Y} )\right )
\]</span> <span class="math display">\[
= E l \left ( Y , (1/K) \iota^T \hat{Y} \right )
\]</span></p>
<p>where <span class="math inline">\(\iota = ( 1 , \ldots ,1 )\)</span>. Hence, the randomized multiple predictor with weights <span class="math inline">\(w = (1/K)\iota\)</span> provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.</p>
<p>An alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule. We formalize the gains in Classification Risk with the following discussion.</p>
</section>
</section>
<section id="classification-variance-decomposition" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="classification-variance-decomposition"><span class="header-section-number">15.2</span> Classification variance decomposition</h2>
<p><span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span> provide a rigorous connection between the strength of individual classifiers and their correlation. To formalize this, we need to establish some notation. Consider a <span class="math inline">\(K\)</span>-class classification problem where:</p>
<ul>
<li><span class="math inline">\(X\)</span> denotes the input feature vector</li>
<li><span class="math inline">\(Y \in \{1, 2, \ldots, K\}\)</span> denotes the true class label</li>
<li><span class="math inline">\(c \in \{1, 2, \ldots, K\}\)</span> represents a specific true class</li>
<li><span class="math inline">\(d \in \{1, 2, \ldots, K\}\)</span> represents a candidate class label (the class we’re voting for)</li>
</ul>
<p>In an ensemble setting, let <span class="math inline">\(h(X, d)\)</span> denote an individual classifier (e.g., a single decision tree) that outputs a vote for class <span class="math inline">\(d\)</span> given input <span class="math inline">\(X\)</span>. Typically, <span class="math inline">\(h(X, d) \in \{0, 1\}\)</span> for hard voting (1 if the classifier predicts class <span class="math inline">\(d\)</span>, 0 otherwise) or <span class="math inline">\(h(X, d) \in [0, 1]\)</span> for soft voting (the probability that the classifier assigns to class <span class="math inline">\(d\)</span>).</p>
<p>The ensemble aggregates votes from multiple classifiers. Let <span class="math inline">\(\mathbf{Q}\)</span> denote the distribution over classifiers—this is the randomization scheme (e.g., bootstrap sampling, random feature selection) that generates the trees in the ensemble. The aggregate classifier <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> is the average vote for class <span class="math inline">\(d\)</span> over all classifiers sampled from <span class="math inline">\(\mathbf{Q}\)</span>:</p>
<p><span class="math display">\[
H_{\mathbf{Q}}(X, d) = E_{\mathbf{Q}}[h(X, d)] = \frac{1}{B}\sum_{b=1}^B h_b(X, d)
\]</span></p>
<p>where <span class="math inline">\(h_b\)</span> are individual classifiers sampled from <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(B\)</span> is the number of classifiers in the ensemble. In practice, this is simply the proportion of trees that vote for class <span class="math inline">\(d\)</span> (or the average probability assigned to class <span class="math inline">\(d\)</span> across all trees).</p>
<p>The final classification decision <span class="math inline">\(C_{\mathbf{Q}}(X)\)</span> is the class that receives the most votes (or highest average probability):</p>
<p><span class="math display">\[
C_{\mathbf{Q}}(X) = \arg\max_{d \in \{1, \ldots, K\}} H_{\mathbf{Q}}(X, d)
\]</span></p>
<p>Now, let <span class="math inline">\(P_{c}\)</span> denote the population conditional probability distribution of a point <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=c\)</span>, and let <span class="math inline">\(E_{c}\)</span> and <span class="math inline">\(\operatorname{Var}_{c}\)</span> denote the associated conditional expectation and variance operators (i.e., <span class="math inline">\(E_c[\cdot] = E[\cdot \mid Y=c]\)</span> and <span class="math inline">\(\operatorname{Var}_c[\cdot] = \operatorname{Var}[\cdot \mid Y=c]\)</span>). Define the vectors of average aggregates conditional on class <span class="math inline">\(c\)</span> as <span class="math display">\[
M_{c}(d)=E_{c}\left[H_{\mathbf{Q}}(X, d)\right]=E\left[H_{\mathbf{Q}}(X, d) \mid Y=c\right]
\]</span></p>
<p>for <span class="math inline">\(d=1, \ldots, K\)</span>. Intuitively, <span class="math inline">\(M_c(d)\)</span> represents the <strong>expected vote share</strong> for class <span class="math inline">\(d\)</span> when the true class is <span class="math inline">\(c\)</span>. Ideally, <span class="math inline">\(M_c(c)\)</span> (votes for the correct class) should be much larger than <span class="math inline">\(M_c(d)\)</span> (votes for incorrect classes).</p>
<p>The average conditional margin (ACM) for class <span class="math inline">\(c\)</span> is defined as <span class="math display">\[
\theta_{c}=\min _{d \neq c}\left(M_{c}(c)-M_{c}(d)\right)
\]</span> This <span class="math inline">\(\theta_c\)</span> measures the “safety gap” between the correct class and the closest competing class. A larger margin means the classifier is more robust.</p>
<p>We assume that <span class="math inline">\(\theta_{c}&gt;0\)</span>. This assumption is very weak since it involves only the average over the population of class <span class="math inline">\(c\)</span>. It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.</p>
<p>Given that <span class="math inline">\(\theta_{c}&gt;0\)</span>, the error rate for class <span class="math inline">\(c\)</span> depends on the extent to which the aggregate classifier <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> is concentrated around <span class="math inline">\(M_{c}(d)\)</span> for each <span class="math inline">\(d=1, \ldots, K\)</span>. The simplest measure of concentration is the variance of <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> with respect to the distribution <span class="math inline">\(P_{c}\)</span>. Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to <span class="math inline">\(P_{c}\)</span> as follows.</p>
<p><span class="math display">\[\begin{align*}
P_{c}\left(C_{\mathbf{Q}}(X) \neq c\right) \leq &amp; P_{c}\left(H_{\mathbf{Q}}(X, c)&lt;M_{c}(c)-\theta_{c} / 2\right) \\
&amp; +\sum_{d \neq c} P_{c}\left(H_{\mathbf{Q}}(X, d)&gt;M_{c}(d)+\theta_{c} / 2\right) \\
\leq &amp; \sum_{d=1}^{K} P_{c}\left(\left|H_{\mathbf{Q}}(X, d)-M_{c}(d)\right|&gt;\theta_{c} / 2\right) \\
\leq &amp; \frac{4}{\theta_{c}^{2}} \sum_{d=1}^{K} \operatorname{Var}_{c}\left[H_{\mathbf{Q}}(X, d)\right] . \tag{10}
\end{align*}\]</span></p>
<p>Of course Chebyshev’s inequality is coarse and will not give very sharp results in itself, but we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities. To reduce error, we must either <strong>increase the margin</strong> <span class="math inline">\(\theta_c\)</span> (build stronger classifiers) or <strong>decrease the variance</strong> (add more diverse trees).</p>
<p>We rewrite each of the variance terms of the last equation as</p>
<p><span class="math display">\[\begin{align*}
\operatorname{Var}_{c}\left[H_{\mathbf{Q}}(X, d)\right] &amp; =E_{c}\left[H_{\mathbf{Q}}(X, d)\right]^{2}-\left[E_{c} H_{\mathbf{Q}}(X, d)\right]^{2} \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} E_{c}\left[h_{1}(X, d) h_{2}(X, d)\right]-E_{\mathbf{Q} \otimes \mathbf{Q}}\left[E_{c}\left[h_{1}(X, d)\right] E_{c}\left[h_{2}(X, d)\right]\right] \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} \operatorname{Cov}_{c}\left[h_{1}(X, d), h_{2}(X, d)\right] \doteq \gamma_{c, d} \tag{11}
\end{align*}\]</span></p>
<p>Here, <span class="math inline">\(E_{\mathbf{Q} \otimes \mathbf{Q}}\)</span> denotes the expectation over the product measure of two independent draws from <span class="math inline">\(\mathbf{Q}\)</span>. That is, <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> are two classifiers sampled independently from the distribution <span class="math inline">\(\mathbf{Q}\)</span>, and <span class="math inline">\(E_{\mathbf{Q} \otimes \mathbf{Q}}\)</span> averages over all such pairs. This allows us to express the variance of the ensemble in terms of the covariance between pairs of individual classifiers.</p>
<p>This equation formalizes the intuition: to minimize error, we must minimize the covariance <span class="math inline">\(\gamma_{c, d}\)</span> between trees. This is precisely what <strong>Random Forests</strong> do by randomizing splits (<span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span>).</p>
<section id="the-nearest-neighbor-insight" class="level3">
<h3 class="anchored" data-anchor-id="the-nearest-neighbor-insight">The Nearest Neighbor Insight</h3>
<p>Why do trees work so well on complex data? <span class="citation" data-cites="cover1967nearest">Cover and Hart (<a href="references.html#ref-cover1967nearest" role="doc-biblioref">1967</a>)</span> provided a fundamental result for nearest-neighbor classifiers: as sample size grows, the error rate of a simple 1-Nearest Neighbor classifier is bounded by <strong>twice the Bayes Risk</strong> (the irreducible error). <span class="math display">\[
R^* \le R_{NN} \le 2R^*(1-R^*)
\]</span> This means that a simple “look at your neighbor” strategy captures at least half the available signal. Decision trees can be viewed as <strong>adaptive nearest-neighbor</strong> models. Instead of using a fixed distance metric (which fails in high dimensions due to sparsity), trees learn a custom metric—dividing space into “blocky” regions based only on relevant features.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-amit2000multiple" class="csl-entry" role="listitem">
Amit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. <span>“Multiple Randomized Classifiers: <span>MRCL</span>.”</span>
</div>
<div id="ref-cover1967nearest" class="csl-entry" role="listitem">
Cover, T., and P. Hart. 1967. <span>“Nearest Neighbor Pattern Classification.”</span> <em>IEEE Transactions on Information Theory</em> 13 (1): 21–27.
</div>
<div id="ref-diaconis1987dozen" class="csl-entry" role="listitem">
Diaconis, Persi, and David Freedman. 1987. <span>“A Dozen de <span class="nocase">Finetti-style</span> Results in Search of a Theory.”</span> In <em>Annales de l’<span>IHP</span> Probabilités Et Statistiques</em>, 23:397–423.
</div>
<div id="ref-hahn2020bayesian" class="csl-entry" role="listitem">
Hahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020. <span>“Bayesian <span>Regression Tree Models</span> for <span>Causal Inference</span>: <span>Regularization</span>, <span>Confounding</span>, and <span>Heterogeneous Effects</span> (with <span>Discussion</span>).”</span> <em>Bayesian Analysis</em> 15 (3): 965–1056.
</div>
<div id="ref-klartag2007central" class="csl-entry" role="listitem">
Klartag, Bo’az. 2007. <span>“A Central Limit Theorem for Convex Sets.”</span> <em>Inventiones Mathematicae</em> 168 (1): 91–131.
</div>
<div id="ref-milman2009asymptotic" class="csl-entry" role="listitem">
Milman, Vitali D, and Gideon Schechtman. 2009. <em>Asymptotic Theory of Finite Dimensional Normed Spaces: <span>Isoperimetric</span> Inequalities in Riemannian Manifolds</em>. Vol. 1200. Springer.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./13-logistic.html" class="pagination-link" aria-label="Logistic Regression and Generalized Linear Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./15-forecasting.html" class="pagination-link" aria-label="Forecasting">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>