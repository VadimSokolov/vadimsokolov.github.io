[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "16-tree.html",
    "href": "16-tree.html",
    "title": "1  Tree Models",
    "section": "",
    "text": "1.1 Bulding a Tree via Recursive Binary Splitting\nWe’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of \\(y\\) to use for a given \\(x\\). Similar to decision tree predictive tree model is a nested sequence of if-else statements that map any input data point \\(x\\) to a predicted output \\(y\\). Each if-else statement checks a feature of \\(x\\) and sends the data left or right along the tree branch. At the end of the branch, a single value of \\(y\\) is predicted.\nFigure 1.1 shows a decision tree for predicting a chess piece given a four-dimensional input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.\nThe prediction algorithm is simple. Start at the root node and move down the tree until you reach a leaf node. The process of building a tree, given a set of training data, is more complicated and has three main components:\nThe splitting process is the most important part of the tree-building process. At each step the splitting process need to decide on the feature index \\(j\\) to be used for splitting and the location of the split. For binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.\nImagine you’re a jewelry appraiser tasked with determining a diamond’s value. You might follow a series of questions: Is the carat weight above 1.0? If yes, is the clarity VS1 or better? Each question leads to another, creating a decision path that eventually arrives at a price estimate. This is precisely how decision trees work—they mirror our natural decision-making process by creating a flowchart of if-then rules.\nBelow we’ll explore tree-based models using the classic diamonds dataset, which contains prices and attributes for 53,940 diamonds. We’ll start with simple decision trees, progress to ensemble methods like random forests and gradient boosting, and develop deep insights into how these algorithms work, when to use them, and how to avoid common pitfalls.\nLet’s start with a quick demo and look at the data, wchih has 10 variables\nLet’s plot price vs carat.\nNotice the strong non-linear relationship between carat and price. This suggests that log-transformations might help making the relationship linear.\nHowever, as we will see later tree models are not very sensitive to the linearity of the relationship between the predictors and the response. In general, we do not need to transform the variables.\nAlthough carat is the most important factor in determining the price of a diamond, it is not the only factor. We can see that there is a lot of variability in the price of diamonds with the same carat.\nLet’s start with a simple decision tree using just two predictors to visualize how trees partition the feature space:\nThe decision tree plot shows how the algorithm partitions the feature space based on carat and clarity to predict diamond prices. The tree structure reveals several interesting patterns:\nThis simple two-predictor tree demonstrates the key advantages of decision trees: they can handle non-linear relationships, provide interpretable rules, and naturally capture feature interactions without requiring explicit specification of interaction terms.\nLet’s plot the data.\nWe can see that that for small and large diamonds, the price is consistently low and does not depend much on the clarity. However, at around 1 carat, we see some overlap in the price for different clarity levels. Clarity becomes impportant at this level\nNow let’s plot the data with the tree regions.\nThe plot above shows the decision tree’s prediction regions as colored tiles, where each tile represents a specific combination of carat and clarity values. The color gradient from blue to red indicates the predicted price, with darker red representing higher predicted prices.\nLooking at this visualization, we can see several key patterns. The strongest predictor is clearly carat, as evidenced by the vertical bands of similar colors. As carat increases (moving right on the x-axis), the predicted prices generally increase (colors shift from blue to red). The tree captures non-linear patterns that a simple linear model would miss. For example, the rate of price increase with carat is not uniform across all clarity levels. Unlike smooth regression surfaces, the tree creates distinct rectangular regions with sharp boundaries, reflecting the binary splitting nature of decision trees.\nThe prediction is rather straightforward. The tree divides the predictor space-that is, the set of possible values for \\(x_1, x_2, \\ldots, x_p\\) - into \\(J\\) distinct and non-overlapping boxes, \\(R_1,R_2,...,R_J\\). For every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\).\n\\[\nf(x) = \\bar y_j, \\text{ for } x \\in R_j, \\text{ where } \\bar y_j = \\text{Average}(y_i \\mid x_i \\in R_j)\n\\]\nThe overall goal of building a tree is to find find regions that lead to minima of the total Residual Sum of Squares (RSS) \\[\n\\mathrm{RSS} = \\sum_{j=1}^J\\sum_{i \\in R_j}(y_i - \\bar{y}_j)^2 \\rightarrow \\mathrm{minimize}\n\\]\nUnfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into \\(J\\) boxes. We can find a good approximate solution, using top-down approach (the CART algorithm).\nIt begins with the entire dataset at the “root” node and repeatedly splits the data into two “child” nodes. This process continues recursively on each new node, with the goal of making the resulting groups (nodes) as homogeneous as possible with respect to the target variable, price. At each iteatoin we decide on: which variable \\(j\\) to split and split point \\(s\\). \\[\nR_1(j, s) = \\{x\\mid x_j &lt; s\\} \\mbox{ and } R_2(j, s) = \\{x\\mid x_j \\ge s\\},\n\\] thus, we seek to minimize (in case of regression tree) \\[\n\\min_{j,s}\\left[ \\sum_{i:x_i\\in  R_1}(y_i - \\bar{y}_1)^2 + \\sum_{i:x_i  \\in R_2}(y_i - \\bar{y}_2)^2\\right]\n\\] As a result, every observed input point belongs to a single region.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#pruning-taming-an-overfit-tree",
    "href": "16-tree.html#pruning-taming-an-overfit-tree",
    "title": "1  Tree Models",
    "section": "1.2 Pruning: Taming an Overfit Tree",
    "text": "1.2 Pruning: Taming an Overfit Tree\nNow let’s discuss how many regions we should have. At one extreme end, we can have \\(n\\) regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed \\(y\\)’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions \\(R_1,...,R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias.\nHow do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree \\(T_0\\), and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!\nInstead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that minimizes \\[\n\\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m}(y_i - \\bar{y}_m)^2 + \\alpha |T|\n\\] The parameter \\(\\alpha\\) balances the complexity of the subtree and its adherence to the training data. When we increment \\(\\alpha\\) starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of \\(\\alpha\\). We determine the optimal value \\(\\hat \\alpha\\) through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to \\(\\hat \\alpha\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#classification-trees",
    "href": "16-tree.html#classification-trees",
    "title": "1  Tree Models",
    "section": "1.3 Classification Trees",
    "text": "1.3 Classification Trees\nA classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use the classification error rate, which is the proportion of observations in that region that do not belong to the most prevalent class.\nWe start by introducing s0me notations \\[\np_{mk} = \\dfrac{1}{N_m}\\sum_{x_i \\in R_m} I(y_i=k),\n\\] which is proportion of observations of class \\(k\\) in region \\(m\\).\nThe classification then done as follows \\[\np_m = \\max_k p_{mk},~~~ E_m = 1-p_m\n\\] i.e the most frequent observation in region \\(m\\)\nThen classification is done as follows \\[\nP(y=k) = \\sum_{j=1}^J p_j I(x \\in R_j)\n\\]\nAn alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.\nNow, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region.\nConsider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).\nIn both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it’s perfectly classifying all observations in that region. This is an ideal situation in classification problems. On the other hand, the first tree, despite having the same overall misclassification rate, doesn’t have any region where all observations are correctly classified.\nThis illustrates that while the misclassification rate is a useful metric, it doesn’t always capture the full picture. Other metrics like the Gini Index or Cross-Entropy can provide a more nuanced view of the quality of a split, taking into account not just the overall error rate, but also the distribution of errors across different regions.\nAnother way to measure the quality of the split is to use the Gini Index and Cross-Entropy Say, I have 400 observations in each class (400,400). I create a tree with two region: (300,100) and (100,300). Say I have another tree: (200,400) and (200,0). In both cases misclassification rate is 0.25. The later tree is preferable. We prefer to have more “pure nodes” and Gini index does a better job.\nThe Gini index: \\[\nG_m = \\sum_{k=1}^K p_{mk}(1-p_{mk})\n\\] It measures a variance across the \\(K\\) classes. It takes on a small value if all of the \\(p_{mk}\\)’s are close to zero or one\nAn alternative to the Gini index is cross-entropy (a.k.a deviance), given by \\[\nD_m = -\\sum_{k=1}^Kp_{mk}\\log p_{mk}\n\\] It is near zero if the \\(p_mk\\)’s are all near zero or near one. Gini index and the cross-entropy led to similar results.\nNow we apply the tree model to the Boston housing dataset.\n\nlibrary(MASS); data(Boston); attach(Boston)\nhead(Boston)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0.01\n18\n2.3\n0\n0.54\n6.6\n65\n4.1\n1\n296\n15\n397\n5.0\n24\n\n\n0.03\n0\n7.1\n0\n0.47\n6.4\n79\n5.0\n2\n242\n18\n397\n9.1\n22\n\n\n0.03\n0\n7.1\n0\n0.47\n7.2\n61\n5.0\n2\n242\n18\n393\n4.0\n35\n\n\n0.03\n0\n2.2\n0\n0.46\n7.0\n46\n6.1\n3\n222\n19\n395\n2.9\n33\n\n\n0.07\n0\n2.2\n0\n0.46\n7.2\n54\n6.1\n3\n222\n19\n397\n5.3\n36\n\n\n0.03\n0\n2.2\n0\n0.46\n6.4\n59\n6.1\n3\n222\n19\n394\n5.2\n29\n\n\n\n\n\n\nFirst we build a big tree\n\ntemp = tree(medv~lstat,data=Boston,mindev=.0001)\nlength(unique(temp$where)) # first big tree size\n\n## [1] 73\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\nlength(unique(boston.tree$where)) # pruned tree size\n\n## [1] 7\n\n\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\nboston.fit = predict(boston.tree) #get training fitted values\nplot(Boston$lstat,Boston$medv,cex=.5,pch=16) #plot data\noo=order(Boston$lstat)\nlines(Boston$lstat[oo],boston.fit[oo],col='red',lwd=3) #step function fit\ncvals=c(9.725,4.65,3.325,5.495,16.085,19.9) #cutpoints from tree\nfor(i in 1:length(cvals)) abline(v=cvals[i],col='magenta',lty=2) #cutpoints\n\n\n\n\n\n\n\n\n\n\nPick off dis,lstat,medv\n\ndf2=Boston[,c(8,13,14)] \nprint(names(df2))\n\n## [1] \"dis\"   \"lstat\" \"medv\"\n\n\nBuild the big tree\n\ntemp = tree(medv~.,df2,mindev=.0001)\nlength(unique(temp$where)) #\n\n## [1] 74\n\n\nThen prune it down to one with 7 leaves\n\nboston.tree=prune.tree(temp,best=7)\n\nplot(boston.tree,type=\"u\")# plot tree and partition in x.\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\npartition.tree(boston.tree)\n\n\n\n\n\n\n\n\n\n\nGet predictions on 2d grid\n\npv=seq(from=.01,to=.99,by=.05)\nx1q = quantile(df2$lstat,probs=pv)\nx2q = quantile(df2$dis,probs=pv)\nxx = expand.grid(x1q,x2q) #matrix with two columns using all combinations of x1q and x2q\ndfpred = data.frame(dis=xx[,2],lstat=xx[,1])\nlmedpred = predict(boston.tree,dfpred)\n\nMake perspective plot\n\npersp(x1q,x2q,matrix(lmedpred,ncol=length(x2q),byrow=T),\n      theta=150,xlab='dis',ylab='lstat',zlab='medv',\n      zlim=c(min(df2$medv),1.1*max(df2$medv)))\n\n\n\n\n\n\n\n\nAdvantages of Decision Trees:\nDecision trees are incredibly intuitive and simple to explain. They can be even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods we’ve discussed in previous chapters. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics, particularly when the trees are not overly complex. Decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.\nDisadvantages of Decision Trees:\nLarge trees can exhibit high variance. This means that a minor change in the data can lead to a significant change in the final estimated tree, making the model unstable. Conversely, small trees, while more stable, may not be powerful predictors as they might oversimplify the problem. It can be challenging to find a balance between bias and variance when using decision trees. A model with too much bias oversimplifies the problem and performs poorly, while a model with too much variance overfits the data and may not generalize well to unseen data.\nThere are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier, and hence improve predictive accuracy by reducing overfitting. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.\nIn the bagging approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.\nTo Bootsrap Aggregate (Bag) we:\n\nTake \\(B\\) bootstrap samples from the training data, each of the same size as the training data.\nFit a large tree to each bootstrap sample (we know how to do this fast!). This will give us \\(B\\) trees.\nCombine the results from each of the B trees to get an overall prediction.\n\nWhen the target variable \\(y\\) is numeric, the bagging process is straightforward. The final prediction is simply the average of the predictions from each of the \\(B\\) trees. However, when \\(y\\) is categorical, the process of combining results from different trees is less straightforward. One common approach is to use a voting system. In this system, each tree in the ensemble makes a prediction for a given input \\(x\\). The predicted category that receives the most votes (out of \\(B\\) total votes) is chosen as the final prediction. Another approach is to average the predicted probabilities \\(\\hat p\\) from each tree. This method can provide a more nuanced prediction, especially in cases where the voting results are close.\nDespite the potential benefits of averaging predicted probabilities, most software implementations of bagging for decision trees use the voting method. This is likely due to its simplicity and intuitive appeal. However, the best method to use can depend on the specific characteristics of the problem at hand.\nThe simple idea behind every ensemble modes is that variance of the average is lowe than variance of individual. Say we have \\(B\\) models \\(f_1(x),\\ldots,f_B(x)\\) then we combine those \\[\nf_{avg}(x) = \\dfrac{1}{B}\\sum_{b=1}^Bf_b(x)\n\\] Combining models helps fighting overfilling. On the negative side, it is harder to interpret those ensembles\nLet’s experiment with the number of trees in the model\n\nlibrary(randomForest)\nn = nrow(Boston)\nntreev = c(10,500,5000)\nfmat = matrix(0,n,3)\nfor(i in 1:3) {\n  rffit = randomForest(medv~lstat,data=Boston,ntree=ntreev[i],maxnodes=15)\n  fmat[,i] = predict(rffit)\n  print(mean((fmat[,i] - Boston$medv)^2, na.rm = TRUE))\n}\n\n## [1] 31\n## [1] 29\n## [1] 29\n\n\nLet’s plot the results\noo = order(Boston$lstat)\nfor(i in 1:3) {\n  plot(Boston$lstat,Boston$medv,xlab='lstat',ylab='medv',pch=16)\n  lines(Boston$lstat[oo],fmat[oo,i],col=i+1,lwd=3)\n  title(main=paste('bagging ntrees = ',ntreev[i]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith 10 trees our fit is too jumbly.\nWith 1,000 and 5,000 trees the fit is not bad and very similar.\nNote that although our method is based multiple trees (average over) so we no longer have a simple step function!!",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#random-forest",
    "href": "16-tree.html#random-forest",
    "title": "1  Tree Models",
    "section": "1.4 Random Forest",
    "text": "1.4 Random Forest\nIn the bagging technique, models can become correlated, which prevents the achievement of a \\(1/n\\) reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.\nRandom Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees, making the ensemble more robust and improving prediction accuracy. This randomness comes into play when considering a split in a tree. Instead of considering all \\(p\\) predictors for a split, a random sample of \\(m\\) predictors is chosen as split candidates. This subset of predictors is different for each split, which means that different trees are likely to use different predictors in the top split, leading to a more diverse set of trees.\nThe number of predictors considered at each split, \\(m\\), is typically chosen to be the square root of the total number of predictors, \\(p\\). This choice is a rule of thumb that often works well in practice, but it can be tuned based on the specific characteristics of the dataset.\nBy decorrelating the trees, Random Forests can often achieve better performance than bagging, especially when there’s a small number of very strong predictors in the dataset. In such cases, bagging can end up with an ensemble of very similar trees that all rely heavily on these strong predictors, while Random Forests can leverage the other, weaker predictors more effectively.\nOve of the “interpretation” tools that comes with ensemble models is importance rank: total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all tree\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=4,importance=TRUE,ntree=50)\nvarImpPlot(rf.boston,pch=21,bg=\"lightblue\",main=\"\")\n\n\n\n\n\n\n\n\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=6,ntree=50, maxnodes=50)\nyhat.rf = predict(rf.boston,newdata=Boston)\noo=order(Boston$lstat)\nplot(Boston$lstat[oo],Boston$medv[oo],pch=21,bg=\"grey\", xlab=\"lstat\", ylab=\"medv\") #plot data\nlines(Boston$lstat[oo],yhat.rf[oo],col='red',lwd=3) #step function fit",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#boosting",
    "href": "16-tree.html#boosting",
    "title": "1  Tree Models",
    "section": "1.5 Boosting",
    "text": "1.5 Boosting\nBoosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.\nHere’s how Boosting works:\n\nInitially, a single decision tree is fitted to the data.\nThis initial tree is intentionally made weak, meaning it doesn’t perfectly fit the data.\nWe then examine the residuals, which represent the portion of the target variable \\(y\\) not explained by the weak tree.\nA new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.\nThis new tree is also “weakened” or “shrunk”. The prediction from this tree is then added to the prediction of the first tree.\nThis process is repeated iteratively. In each iteration, a new tree is fitted to the residuals of the current ensemble of trees, shrunk, and then added to the ensemble.\nThe final model is the sum of all these “shrunk” trees. The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals). Each new tree is trying to correct the mistakes of the ensemble of previous trees. By adding together many weak models (the shrunk trees), Boosting can often achieve a strong overall model.\n\nPick a loss function \\(\\mathcal{L}\\) that reflects setting; e.g., for continuous \\(y\\), could take \\(\\mathcal{L}(y_i , \\theta_i ) = (y_i - \\theta_i )^2\\) Want to solve \\[\\mathrm{minimize}_{\\beta \\in R^M} \\sum_{i=1}^n \\mathcal{L} \\left(y_i, \\sum_{j=1}^M \\beta_j \\cdot T_j(x_i)\\right)\\]\n\nIndexes all trees of a fixed size (e.g., depth = 5), so \\(M\\) is huge\nSpace is simply too big to optimize\nGradient boosting: basically a version of gradient descent that is forced to work with trees\nFirst think of optimization as \\(\\min_\\theta f (\\theta)\\), over predicted values \\(\\theta\\) (subject to \\(\\theta\\) coming from trees)\n\n\n\n\n\n\n\n\n\n\n\nSet \\(f_1(x)=0\\) (constant predictor) and \\(r_i=y_i\\)\nFor \\(b=1,2,\\ldots,B\\)\n\nFit a tree \\(f_b\\) with \\(d\\) splits to the training set \\((X,r)\\)\nUpdate the model \\[f(x) = f(x) +\\lambda f_b(x)\\]\nUpdate the residuals \\[r_i=r_i - \\lambda f_b(x)\\]\n\nHere are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = \\(\\lambda\\) at .2.\n\nlibrary(gbm)\nboost.boston=gbm(medv~.,data=Boston,distribution=\"gaussian\",n.trees=5000,interaction.depth=4)\nyhat.boost=predict(boost.boston,newdata=Boston,n.trees=5000)\nmean((yhat.boost-Boston$medv)^2)\n\n## [1] 4e-04\n\n\n\nsummary(boost.boston, plotit=FALSE)\n\n\n\n\n\n\nvar\nrel.inf\n\n\n\n\nlstat\nlstat\n36.32\n\n\nrm\nrm\n30.98\n\n\ndis\ndis\n7.63\n\n\ncrim\ncrim\n5.09\n\n\nnox\nnox\n4.63\n\n\nage\nage\n4.50\n\n\nblack\nblack\n3.45\n\n\nptratio\nptratio\n3.11\n\n\ntax\ntax\n1.74\n\n\nrad\nrad\n1.17\n\n\nindus\nindus\n0.87\n\n\nchas\nchas\n0.39\n\n\nzn\nzn\n0.13\n\n\n\n\n\n\nplot(boost.boston,i=\"rm\")\nplot(boost.boston,i=\"lstat\")\n\n\n\n\n\n\n\n\n\n\nAdvantages of Boosting over Random Forests:\n\nPerformance: Boosting, in many cases, provides better predictive accuracy than Random Forests. By focusing on the residuals or mistakes, Boosting can incrementally improve model performance.\nModel Interpretability: While both methods are not as interpretable as a single decision tree, Boosting models can sometimes be more interpretable than Random Forests, especially when the number of weak learners (trees) is small.\n\nDisadvantages of Boosting compared to Random Forests:\n\nComputation Time and Complexity: Boosting can be more computationally intensive than Random Forests. This is because trees are built sequentially in Boosting, while in Random Forests, they are built independently and can be parallelized.\nOverfitting: Boosting can overfit the training data if the number of trees is too large, or if the trees are too complex. This is less of a problem with Random Forests, which are less prone to overfitting due to the randomness injected into the tree building process.\nOutliers: Boosting can be sensitive to outliers since it tries to correct the mistakes of the predecessors. On the other hand, Random Forests are more robust to outliers.\nNoise: Boosting can overemphasize instances that are hard to classify and can overfit to noise, whereas Random Forests are more robust to noise.\n\nRemember, the choice between Boosting and Random Forests (or any other model) should be guided by the specific requirements of your task, including the nature of your data, the computational resources available, and the trade-off between interpretability and predictive accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#finding-good-bayes-predictors",
    "href": "16-tree.html#finding-good-bayes-predictors",
    "title": "1  Tree Models",
    "section": "1.6 Finding Good Bayes Predictors",
    "text": "1.6 Finding Good Bayes Predictors\nThe ensemble methods we’ve discussed—bagging, random forests, and boosting—all share a common goal: improving predictive performance by combining multiple weak learners. But what is the theoretical foundation for why these methods work so well? And how do we determine the optimal way to combine these predictors? These questions lead us naturally to a Bayesian perspective on prediction, which provides both theoretical justification and practical guidance for ensemble methods.\nBayesian methods tackle the problem of good predictive performance in a number of ways. The goal is to find a good predictive MSE \\(E_{Y,\\hat{Y}}(\\Vert\\hat{Y} - Y \\Vert^2)\\). First, Stein shrinkage (a.k.a regularization with an \\(\\ell_2\\) norm) has long been known to provide good mean squared error properties in estimation, namely \\(E(||\\hat{\\theta} - \\theta||^2)\\) as well. These gains translate into predictive performance (in an iid setting) for \\(E(||\\hat{Y}-Y||^2)\\). One of the main issues is how to tune the amount of regularisation (a.k.a prior hyper-parameters). Stein’s unbiased estimator of risk provides a simple empirical rule to address this problem, as does cross-validation. From a Bayes perspective, the marginal likelihood (and full marginal posterior) provides a natural method for hyper-parameter tuning. The issue is computational tractability and scalability. The posterior for \\((W,b)\\) is extremely high dimensional and multimodal and posterior MAP provides good predictors \\(\\hat{Y}(X)\\).\nBayes conditional averaging can also perform well in high dimensional regression and classification problems. High dimensionality brings with it the curse of dimensionality and it is instructive to understand why certain kernel can perform badly.\nAdaptive Kernel predictors (a.k.a. smart conditional averager) are of the form\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R K_r ( X_i , X ) \\hat{Y}_r (X)\n\\]\nHere \\(\\hat{Y}_r(X)\\) is a deep predictor with its own trained parameters. For tree models, the kernel \\(K_r( X_i , X)\\) is a cylindrical region \\(R_r\\) (open box set). Figure Figure 1.2 illustrates the implied kernels for trees (cylindrical sets) and random forests. Not too many points will be neighbors in high dimensional input space.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Cylindrical kernels for trees (left) and random forests (right).\n\n\n\nConstructing the regions is fundamental to reduce the curse of dimensionality. It is useful to imagine a very large dataset, e.g. 100k images and think about how a new image’s input coordinates, \\(X\\), are “neighbors” to data point in the training set. Our predictor will then be a smart conditional average of the observed outputs, \\(Y\\), for our neighbors. When \\(p\\) is large, spheres (\\(L^2\\) balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.\nTo illustrate the problem further, Figure Figure 1.3 below shows the 2D image of 1000 uniform samples from a 50-dimensional ball \\(B_{50}\\). The image is calculated as \\(w^T Y\\), where \\(w = (1,1,0,\\ldots,0)\\) and \\(Y \\sim U(B_{50})\\). Samples are centered around the equators and none of the samples fall close to the boundary of the set.\n\n\n\n\n\n\nFigure 1.3\n\n\n\nAs dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from Figure Figure 1.4, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e. \\(e_1^T Y\\), where \\(e_1 = (1,0,\\ldots,0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Histogram of marginal distribution of \\(Y\\sim U(B_p)\\) for different dimensions \\(p\\).\n\n\n\nSimilar central limit results were known to Maxwell who showed that random variable \\(w^TY\\) is close to standard normal, when \\(Y \\sim U(B_p)\\), \\(p\\) is large, and \\(w\\) is a unit vector (lies on the boundary of the ball). For the history of this fact, see Diaconis and Freedman (1987). More general results in this direction were obtained in Klartag (2007). Further, Milman and Schechtman (2009) presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.\nDeep learning improves on this by performing a sequence of GLM-like transformations, effectively DL learns a distributed partition of the input space. Specifically, suppose that we have \\(K\\) partitions. Then the DL predictor takes the form of a weighted average or soft-max of the weighted average in case of classification of observations in this partition. Given a new high dimensional input \\(X_{\\mathrm{new}}\\), many deep learners are an average of learners obtained by our hyper-plane decomposition. Generically, we have\n\\[\n\\hat{Y}(X) = \\sum_{k \\in K} w_k(X)\\hat{Y}_k(X),\n\\] where \\(w_k\\) are the weights learned in region \\(K\\), and \\(w_k(X)\\) is an indicator of the region with appropriate weighting given the training data. Where \\(w_k\\) is a weight which also indicates which partition the new \\(X_{new}\\) lies in.\nThe use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form clever conditional averaging) is prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, the caveat that they have large variances due to the dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the \\(1/N\\)-rule and average to reduce risk. Specifically, suppose that we have \\(K\\) exchangeable, \\(\\mathbb{E} ( \\hat{Y}_i ) = \\mathbb{E} ( \\hat{Y}_{\\pi(i)} )\\), predictors\n\\[\n\\hat{Y} = ( \\hat{Y}_1 , \\ldots , \\hat{Y}_K )\n\\]\nFind \\(w\\) to attain \\(\\operatorname{argmin}_W E l( Y , w^T \\hat{Y} )\\) where \\(l\\) convex in the second argument;\n\\[\nE l( Y , w^T \\hat{Y} )  = \\frac{1}{K!} \\sum_\\pi E l( Y , w^T \\hat{Y} ) \\geq  E l \\left ( Y , \\frac{1}{K!} \\sum_\\pi w_\\pi^T \\hat{Y} )\\right ) =  E l \\left ( Y , (1/K) \\iota^T \\hat{Y} \\right )\n\\]\nwhere \\(\\iota = ( 1 , \\ldots ,1 )\\). Hence, the randomized multiple predictor with weights \\(w = (1/K)\\iota\\) provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.\nAn alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule. We formalize the gains in Classification Risk with the following discussion.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "16-tree.html#ensemble-averaging-and-1n-rule",
    "href": "16-tree.html#ensemble-averaging-and-1n-rule",
    "title": "1  Tree Models",
    "section": "1.7 Ensemble Averaging and \\(1/N\\) Rule",
    "text": "1.7 Ensemble Averaging and \\(1/N\\) Rule\nIn high dimensions, when I have large number of predictive models that generate uncorrelated predictions, the optimal approach to generate a prediction is to average out predictions from those individual models/ weak predictors. This is called the \\(1/N\\) rule. The variance in the prediction is reduced by a factor of \\(N\\) when we average out \\(N\\) uncorrelated predictions. \\[\n\\mbox{Var} \\left ( \\frac{1}{N} \\sum_{i=1}^N \\hat y_i \\right ) = \\frac{1}{N^2} \\mbox{Var} \\left ( \\hat y_i \\right ) + \\frac{2}{N^2} \\sum_{i \\neq j} \\mbox{Cov} \\left ( \\hat y_i, \\hat y_j \\right )\n\\] In high dimensions it relatively easy to find uncorrelated predictors and those techniques prove to lead to a winning solution in many machine learning competitions. The \\(1/N\\) rule is optimal due to exchangeability of the weak predictors, see Polson, Sokolov, et al. (2017)\n\n\n1.7.1 Classification variance decomposition\nThe famous result is due to Cover and Hart (1967) who proved that k-nearest neighbors are at most twice the bayes risk.\nAmit, Blanchard, and Wilder (2000) use the population conditional probability distribution of a point \\(X\\) given \\(Y=c\\), denoted by \\(P_{c}\\), and the associated conditional expectation and variance operators will be denoted \\(E_{c}\\) and \\(V a r_{c}\\). Define the vectors of average aggregates conditional on class \\(c\\) as \\[\n\\begin{equation*}\nM_{c}(d)=E_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right]=E\\left[H_{\\mathbf{Q}}(X, d) \\mid Y=c\\right] \\tag{8}\n\\end{equation*}\n\\]\nfor \\(d=1, \\ldots, K\\). The average conditional margin (ACM) for class \\(c\\) is defined as\n\\[\n\\begin{equation*}\n\\theta_{c}=\\min _{d \\neq c}\\left(M_{c}(c)-M_{c}(d)\\right) \\tag{9}\n\\end{equation*}\n\\]\nWe assume that \\(\\theta_{c}&gt;0\\). This assumption is very weak since it involves only the average over the population of class \\(c\\). It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.\nGiven that \\(\\theta_{c}&gt;0\\), the error rate for class \\(c\\) depends on the extent to which the aggregate classifier \\(H_{\\mathbf{Q}}(X, d)\\) is concentrated around \\(M_{c}(d)\\) for each \\(d=1, \\ldots, K\\). The simplest measure of concentration is the variance of \\(H_{\\mathbf{Q}}(X, d)\\) with respect to the distribution \\(P_{c}\\). Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to \\(P_{c}\\) as follows.\n\\[\n\\begin{align*}\nP_{c}\\left(C_{\\mathbf{Q}}(X) \\neq c\\right) \\leq & P_{c}\\left(H_{\\mathbf{Q}}(X, c)&lt;M_{c}(c)-\\theta_{c} / 2\\right) \\\\\n& +\\sum_{d \\neq c} P_{c}\\left(H_{\\mathbf{Q}}(X, d)&gt;M_{c}(d)+\\theta_{c} / 2\\right) \\\\\n\\leq & \\sum_{d=1}^{K} P_{c}\\left(\\left|H_{\\mathbf{Q}}(X, d)-M_{c}(d)\\right|&gt;\\theta_{c} / 2\\right) \\\\\n\\leq & \\frac{4}{\\theta_{c}^{2}} \\sum_{d=1}^{K} \\operatorname{Var}_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right] . \\tag{10}\n\\end{align*}\n\\]\nOf course Chebyshev’s inequality is coarse and will not give very sharp results in itself, be we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities.\nWe rewrite each of the variance terms of the last equation as\n\\[\n\\begin{align*}\n\\operatorname{Var}_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right] & =E_{c}\\left[E_{\\mathbf{Q}} h(X, d)\\right]^{2}-\\left[E_{c} E_{\\mathbf{Q}} h(X, d)\\right]^{2} \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} E_{c}\\left[h_{1}(X, d) h_{2}(X, d)\\right]-E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\left[E_{c}\\left[h_{1}(X, d)\\right] E_{c}\\left[h_{2}(X, d)\\right]\\right] \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} \\operatorname{Cov}_{c}\\left[h_{1}(X, d), h_{2}(X, d)\\right] \\doteq \\gamma_{c, d} \\tag{11}\n\\end{align*}\n\\]\nwhere the notation \\(E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\) means that \\(h_{1}, h_{2}\\) are two classifiers sampled independently from the distribution \\(\\mathbf{Q}\\). We can therefore interpret this variance term as the conditional covariance of two classifiers independently sampled from \\(\\mathbf{Q}\\). We call this quantity the average conditional covariance (ACC). Even if \\(\\mathbf{Q}\\) is a discrete distribution, such as that provided by a particular run of \\(N\\) classifiers, when it is supported on a moderate number of classifiers, it is dominated by the conditional covariances of which there are order \\(N^{2}\\), and not the conditional variances of which there are order \\(N\\).\n\n\n1.7.2 Conditional and unconditional dependence\nIt should be emphasized that two classifiers, provided that they achieve reasonable classification rate (that is, better than just picking a class at random) will not be unconditionally independent. If we do not know the class label of a point, and vector \\(\\left(h_{1}(X, i)\\right)_{i}\\) is large at class \\(c\\), then we actually change our expectations regarding \\(\\left(h_{2}(X, i)\\right)_{i}\\). On the other hand if we were given in advance the class label \\(Y\\), then knowing \\(h_{1}(X)\\) would hardly affect our guess about \\(h_{2}(X)\\). This is the motivation behind the notion of weak conditional dependence.\nThis is in contrast to the measure of dependence introduced in Dietterich (1998), which involves the unconditional covariance. The \\(\\kappa\\) statistic used there is\n\\[\n\\kappa\\left(h_{1}, h_{2}\\right)=\\frac{\\sum_{d} \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]}{1-\\sum_{d} E h_{1}(X, d) E h_{2}(X, d)},\n\\]\nA simple decomposition of the numerator yields: \\(\\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d)\\right]=E \\operatorname{Cov}\\left[h_{1}(X, d), h_{2}(X, d) \\mid Y\\right]+\\operatorname{Cov}\\left[E\\left[h_{1}(X, d) \\mid Y\\right], E\\left[h_{2}(X, d) \\mid Y\\right]\\right]\\).\n\n\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple Randomized Classifiers: MRCL.”\n\n\nCover, T., and P. Hart. 1967. “Nearest Neighbor Pattern Classification.” IEEE Transactions on Information Theory 13 (1): 21–27.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a Theory.” In Annales de l’IHP Probabilités Et Statistiques, 23:397–423.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex Sets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of Finite Dimensional Normed Spaces: Isoperimetric Inequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple\nRandomized Classifiers: MRCL.”\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nCover, T., and P. Hart. 1967. “Nearest Neighbor Pattern\nClassification.” IEEE Transactions on Information Theory\n13 (1): 21–27.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a\nTheory.” In Annales de l’IHP\nProbabilités Et Statistiques, 23:397–423.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex\nSets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of\nFinite Dimensional Normed Spaces: Isoperimetric\nInequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep\nLearning: A Bayesian Perspective.”\nBayesian Analysis 12 (4): 1275–1304.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "References"
    ]
  }
]