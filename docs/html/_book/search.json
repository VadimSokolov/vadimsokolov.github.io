[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "24-nlp.html",
    "href": "24-nlp.html",
    "title": "1  Natural Language Processing",
    "section": "",
    "text": "1.1 Converting Words to Numbers\nThe ability to understand and generate human language has long been considered a hallmark of intelligence. When Alan Turing proposed his famous test in 1950, he chose natural conversation as the ultimate benchmark for machine intelligence. Yet for decades, this goal remained frustratingly elusive. Early attempts at machine translation in the 1950s, which simply replaced words using bilingual dictionaries, produced nonsensical results that highlighted the profound complexity of human language. The phrase “The spirit is willing, but the flesh is weak” allegedly translated to Russian and back as “The vodka is good, but the meat is rotten”—a cautionary tale about the subtleties of meaning that transcend mere word substitution.\nThis chapter traces the remarkable journey from those early failures to today’s language models that can engage in nuanced dialogue, translate between languages with near-human accuracy, and even generate creative text. At the heart of this transformation lies a fundamental shift in how we represent language computationally: from discrete symbols manipulated by hand-crafted rules to continuous vector spaces learned from vast corpora of text.\nLanguage presents unique challenges for mathematical modeling. Unlike images, which naturally exist as arrays of continuous pixel values, or audio signals, which are continuous waveforms, text consists of discrete symbols with no inherent geometric structure. The word “cat” is not inherently closer to “dog” than to “quantum”—at least not in any obvious mathematical sense. Yet humans effortlessly recognize that cats and dogs share semantic properties that neither shares with abstract physics concepts.\nA naive way to represent words is through one-hot encoding, where each word in a vocabulary is assigned a unique vector with a single non-zero entry. For example, in a vocabulary of size \\(N\\), the word “cat” might be represented as \\(\\vec{v}_{\\text{cat}} = [0, 0, \\ldots, 1, \\ldots, 0]\\) (with the 1 in the \\(i\\)-th position corresponding to “cat”). However, this approach fails to capture any notion of semantic similarity: the cosine similarity between any two distinct one-hot vectors is zero, erasing all information about how words relate to each other.\nThis type of representation makes even the seemingly simple task of determining whether two sentences have similar meanings challenging. The sentences “The cat sat on the mat” and “A feline rested on the rug” express nearly identical ideas despite sharing no words except “the” and “on.” Conversely, “The bank is closed” could refer to a financial institution or a river’s edge—the same words encoding entirely different meanings. These examples illustrate why early symbolic approaches to natural language processing, based on logical rules and hand-crafted features, struggled to capture the fluid, contextual nature of meaning.\nThe breakthrough came from reconceptualizing the representation problem. Instead of treating words as atomic symbols, what if we could embed them in a continuous vector space where geometric relationships encode semantic relationships? This idea, simple in retrospect, revolutionized the field. In such a space, we might find that \\(\\vec{v}_{\\text{cat}} - \\vec{v}_{\\text{dog}}\\) has similar direction to \\(\\vec{v}_{\\text{car}} - \\vec{v}_{\\text{bicycle}}\\), capturing the analogical relationship “cat is to dog as car is to bicycle” through vector arithmetic.\nTo formalize this intuition, we seek a mapping \\(\\phi: \\mathcal{V} \\rightarrow \\mathbb{R}^d\\) from a vocabulary \\(\\mathcal{V}\\) of discrete tokens to \\(d\\)-dimensional vectors. The challenge lies in learning this mapping such that the resulting geometry reflects semantic relationships. The naive approach of one-hot encoding, where each word is represented by a vector with a single non-zero entry, fails catastrophically: in an \\(N\\)-word vocabulary, this produces \\(N\\)-dimensional vectors where every pair of distinct words has cosine similarity zero, erasing all notion of semantic relatedness.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#converting-words-to-numbers",
    "href": "24-nlp.html#converting-words-to-numbers",
    "title": "1  Natural Language Processing",
    "section": "",
    "text": "The Math of Twenty Questions\nMy (Vadim’s) daughter and I play a game of Twenty Questions during road trips. The rules are simple: one person thinks of something, and the other person has to guess what it is by asking yes-or-no questions. The person who is guessing can ask up to twenty questions, and then they have to make a guess. If they guess correctly, they win; if not, the other person wins. The game is fun, but it’s also a great way to illustrate how AI systems can learn to represent words and phrases as numbers. The trick is to come up with an optimal set of yes-or-no questions that will allow you to distinguish between all the words or phrases you might want to represent. Surpisingly, most of the words can be identified with a small set of universal questions asked in the same order every time. Usually person who has a better set of questions wins.For example, you might ask:\n\nIs it an animal? (Yes)\nIs this a domestic animal? (No)\nIs it larger than a human? (Yes)\nDoes it have a long tail? (No)\nIs it a predator? (Yes)\nCan move on two feet? (Yes)\nIs it a bear? (Yes)\n\nThus, if we use a 20-dimenstional 0-1 vector to represent the word “bear,” the portion of this vector corresponding to these questions would look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n1\n0\n1\n1\n\n\n\nThis is called a word vector. Specifically, it’s a “binary” or 0/1 vector: 1 means yes, 0 means no. Different words, would produce different answers to the same questions, so they would have different word vectors. If we stack all these vectors in a matrix, where each row is a word and each column is a question, we get something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n1\n0\n1\n1\n\n\nDog\n1\n1\n0\n1\n0\n0\n\n\nCat\n1\n1\n0\n1\n1\n0\n\n\n\nThe binary nature of these vectors forces us to choose 1 or 0 for the “Larger than human” question for “Dog”. However, there are some dog breeds that are larger than humans, so this binary representation is not very useful in this case. We can do better by allowing the answers to be numbers between 0 and 1, rather than just 0 or 1. This way, we can represent the fact that some dogs are larger than humans, but most are not. For example, we might answer the question “Is it larger than a human?” with a 0.1 for a dog, and a 0.8 for a bear, some types of bears can be smaller than humans, for example, black bears that live in North America, but most bears are larger than humans.\nUsing this approach, the vectors now become\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n0.8\n0\n1\n0.8\n\n\nDog\n1\n1\n0.1\n0.6\n0\n0\n\n\nCat\n1\n0.7\n0.01\n1\n0.6\n0\n\n\n\nAI systems, also use non-binary scoring rules to judge a win or loss. For example, if the answer is “bear,” then the score might be 100 points for a correct guess, 90 points for a close guess like “Binturong” or “wolverine,” and 50 points for a distant guess like “eagle.” This way of keeping score matches the real-world design requirements of most NLP systems. For example, if you translate JFK saying “Ich bin ein Berliner” as “I am a German,” you’re wrong, but a lot closer than if you translate it as “I am a cronut.”\nThe process of converting words into numbers is called “embedding.” The resulting vectors are called “word embeddings.” The only part that is left is how to design an algorithms that can find a good set of questions to ask. Usually real-life word embeddings have hundreds of questions, not just twenty. The process of finding these questions is called “training” the model. The goal is to find a set of questions that will allow the model to distinguish between all the words in the vocabulary, and to do so in a way that captures their meanings. However, the algorithms do not have a notion of meaning in the same way that humans do. Instead, they learn by counting word co-location statistics—that is, which words tend to appear with which other words in real sentences written by humans.\nThese co-occurrence statistics serve as surprisingly effective proxies for meaning. For example, consider the question: “Among all sentences containing ‘fries’ and ‘ketchup,’ how frequently does the word ‘bun’ also appear?” This is the kind of query a machine can easily formulate and answer, since it relies on counting rather than true understanding.\nWhile such a specific question may be too limited if you can only ask a few hundred, the underlying idea—using word co-occurrence patterns—is powerful. Word embedding algorithms are built on this principle: they systematically explore which words tend to appear together, and through optimization, learn the most informative “questions” to ask. By repeatedly applying these learned probes, the algorithms construct a vector for each word or phrase, capturing its relationships based on co-occurrence statistics. These word vectors are then organized into a matrix for further use.\nThere are many ways to train word embeddings, but the most common one is to use a neural network. The neural network learns to ask questions that are most useful for distinguishing between different words. It does this by looking at a large number of examples of words and their meanings, and then adjusting the weights of the questions based on how well they perform. One of the first and most popular algorithms for this is called Word2Vec, which was introduced by Mikolov et al. (2013) at Google in 2013.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#word2vec-and-distributional-semantics",
    "href": "24-nlp.html#word2vec-and-distributional-semantics",
    "title": "1  Natural Language Processing",
    "section": "1.2 Word2Vec and Distributional Semantics",
    "text": "1.2 Word2Vec and Distributional Semantics\nThe theoretical foundation for learning meaningful word representations comes from the distributional hypothesis, articulated by linguist J.R. Firth in 1957: “You shall know a word by the company it keeps.” This principle suggests that words appearing in similar contexts tend to have similar meanings. If “coffee” and “tea” both frequently appear near words like “drink,” “hot,” “cup,” and “morning,” we can infer their semantic similarity.\nThe word2vec framework, introduced by Mikolov et al. (2013), operationalized this insight through a beautifully simple probabilistic model. The skip-gram variant posits that a word can be used to predict its surrounding context words. Given a corpus of text represented as a sequence of words \\(w_1, w_2, \\ldots, w_T\\), the model maximizes the likelihood:\n\\[\\mathcal{L} = \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\\]\nwhere \\(m\\) is the context window size. The conditional probability is parameterized using two sets of embeddings: \\(\\mathbf{v}_w\\) for words as centers and \\(\\mathbf{u}_w\\) for words as context:\n\\[P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{w \\in \\mathcal{V}} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)}\\]\nThis formulation reveals deep connections to the theoretical frameworks discussed in previous chapters. The dot product \\(\\mathbf{u}_o^T \\mathbf{v}_c\\) acts as a compatibility score between center and context words, while the softmax normalization ensures a valid probability distribution. From the perspective of ridge functions, we can view this as learning representations where the function \\(f(w_c, w_o) = \\mathbf{u}_o^T \\mathbf{v}_c\\) captures the log-odds of co-occurrence.\n\nThe Skip-Gram Model\nThe skip-gram model operates on a simple yet powerful principle: given a center word, predict the surrounding context words within a fixed window. This approach assumes that words appearing in similar contexts tend to have similar meanings, directly implementing the distributional hypothesis.\nConsider the sentence “The man loves his son” with “loves” as the center word and a context window of size 2. The skip-gram model aims to maximize the probability of generating the context words “the,” “man,” “his,” and “son” given the center word “loves.” This relationship can be visualized as follows:\n\n\n\n\n\ngraph TD\n    A[loves] --&gt; B[the]\n    A --&gt; C[man]\n    A --&gt; D[his]\n    A --&gt; E[son]\n    \n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#f3e5f5\n    style D fill:#f3e5f5\n    style E fill:#f3e5f5\n    \n    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    \n    class A centerWord\n    class B,C,D,E contextWord\n\n\n\n\n\n\nThe mathematical foundation assumes conditional independence among context words given the center word, allowing the joint probability to factorize:\n\\[P(\\text{\"the\"}, \\text{\"man\"}, \\text{\"his\"}, \\text{\"son\"} \\mid \\text{\"loves\"}) = P(\\text{\"the\"} \\mid \\text{\"loves\"}) \\cdot P(\\text{\"man\"} \\mid \\text{\"loves\"}) \\cdot P(\\text{\"his\"} \\mid \\text{\"loves\"}) \\cdot P(\\text{\"son\"} \\mid \\text{\"loves\"})\\]\nThis independence assumption, while not strictly true in natural language, proves crucial for computational tractability. Each conditional probability is modeled using the softmax function over the entire vocabulary:\n\\[P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^T \\mathbf{v}_c)}\\]\nThe skip-gram objective seeks to maximize the likelihood of observing all context words across the entire corpus. For a text sequence of length \\(T\\) with words \\(w^{(1)}, w^{(2)}, \\ldots, w^{(T)}\\), the objective becomes:\n\\[\\mathcal{L}_{\\text{skip-gram}} = \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)})\\]\nwhere \\(m\\) is the context window size. The normalization by \\(T\\) ensures that the objective remains bounded as corpus size grows.\nThe gradient with respect to the center word embedding reveals the learning dynamics:\n\\[\\frac{\\partial \\log P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c} = \\mathbf{u}_o - \\sum_{i \\in \\mathcal{V}} P(w_i \\mid w_c) \\mathbf{u}_i\\]\nThis elegant form shows that the gradient pushes the center word embedding toward the observed context word (\\(\\mathbf{u}_o\\)) while pulling it away from all other words, weighted by their predicted probabilities. This creates a natural contrast between positive and negative examples, even in the original formulation without explicit negative sampling.\nThe skip-gram architecture assigns two vector representations to each word: \\(\\mathbf{v}_w\\) when the word serves as a center word and \\(\\mathbf{u}_w\\) when it appears in context. This asymmetry allows the model to capture different aspects of word usage. After training, the center word vectors \\(\\mathbf{v}_w\\) are typically used as the final word embeddings, though some implementations average or concatenate both representations.\n\n\nThe Continuous Bag of Words (CBOW) Model\nWhile skip-gram predicts context words from a center word, the Continuous Bag of Words (CBOW) model reverses this relationship: it predicts a center word based on its surrounding context. For the same text sequence “the”, “man”, “loves”, “his”, “son” with “loves” as the center word, CBOW models the conditional probability:\n\\[P(\\text{\"loves\"} \\mid \\text{\"the\"}, \\text{\"man\"}, \\text{\"his\"}, \\text{\"son\"})\\]\nThe CBOW architecture can be visualized as multiple context words converging to predict a single center word:\n\n\n\n\n\ngraph TD\n    B[the] --&gt; A[loves]\n    C[man] --&gt; A\n    D[his] --&gt; A\n    E[son] --&gt; A\n    \n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#f3e5f5\n    style D fill:#f3e5f5\n    style E fill:#f3e5f5\n    \n    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    \n    class A centerWord\n    class B,C,D,E contextWord\n\n\n\n\n\n\nThe key difference from skip-gram lies in how CBOW handles multiple context words. Rather than treating each context word independently, CBOW averages their embeddings. For a center word \\(w_c\\) with context words \\(w_{o_1}, \\ldots, w_{o_{2m}}\\), the conditional probability is:\n\\[P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\exp\\left(\\mathbf{u}_c^T \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^T \\bar{\\mathbf{v}}_o\\right)}\\]\nwhere \\(\\bar{\\mathbf{v}}_o = \\frac{1}{2m}\\sum_{j=1}^{2m} \\mathbf{v}_{o_j}\\) is the average of context word vectors. Note that in CBOW, \\(\\mathbf{u}_i\\) represents word \\(i\\) as a center word and \\(\\mathbf{v}_i\\) represents it as a context word—the opposite of skip-gram’s convention.\nThe CBOW objective maximizes the likelihood of generating all center words given their contexts:\n\\[\\mathcal{L}_{\\text{CBOW}} = \\sum_{t=1}^T \\log P(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})\\]\nThe gradient with respect to context word vectors reveals how CBOW learns:\n\\[\\frac{\\partial \\log P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j\\right)\\]\nThis gradient is scaled by \\(\\frac{1}{2m}\\), effectively distributing the learning signal across all context words. CBOW tends to train faster than skip-gram because it predicts one word per context window rather than multiple words, but skip-gram often produces better representations for rare words since it generates more training examples per sentence.\n\n\nPretraining Word2Vec\nTraining word2vec models requires careful attention to implementation details that significantly impact the quality of learned representations. The training process begins with data preprocessing on large text corpora. Using the Penn Tree Bank dataset as an example—a carefully annotated corpus of Wall Street Journal articles containing about 1 million words—we implement several crucial preprocessing steps.\nFirst, we build a vocabulary by counting word frequencies and retaining only words that appear at least a minimum number of times (typically 5-10). This thresholding serves two purposes: it reduces the vocabulary size from potentially millions to tens of thousands of words, and it prevents the model from wasting capacity on rare words that appear too infrequently to learn meaningful representations. Words below the threshold are replaced with a special &lt;unk&gt; token.\nThe training procedure uses stochastic gradient descent with a carefully designed learning rate schedule. The initial learning rate (typically 0.025 for skip-gram and 0.05 for CBOW) is linearly decreased to a minimum value (usually 0.0001) as training progresses:\n\\[\\alpha_t = \\alpha_0 \\left(1 - \\frac{\\text{words\\_processed}}{\\text{total\\_words} \\times \\text{epochs}}\\right)\\]\nThis schedule ensures larger updates early in training when representations are poor, transitioning to fine-tuning as the model converges.\nThe implementation uses several optimizations for efficiency. Word vectors are typically initialized randomly from a uniform distribution over \\([-0.5/d, 0.5/d]\\) where \\(d\\) is the embedding dimension (commonly 100-300). During training, we maintain two embedding matrices: one for center words and one for context words. After training, these can be combined (usually by averaging) or just the center word embeddings can be used.\nFor minibatch processing, training examples are grouped to enable efficient matrix operations. Given a batch of center-context pairs, the forward pass computes scores using matrix multiplication, applies the loss function (either full softmax, hierarchical softmax, or negative sampling), and backpropagates gradients. A typical training configuration might process batches of 512 word pairs, iterating 5-15 times over a corpus.\nThe quality of learned embeddings can be evaluated through word similarity and analogy tasks. For similarity, we compute cosine distances between word vectors and verify that semantically similar words have high cosine similarity. For analogies, we test whether vector arithmetic captures semantic relationships: the famous “king - man + woman \\(\\approx\\) queen” example demonstrates that vector differences encode meaningful semantic transformations.\nTraining word2vec on real data requires several practical considerations. Using the Penn Tree Bank dataset—a carefully annotated corpus of Wall Street Journal articles—we must first tokenize the text and build a vocabulary. Typically, we keep only words appearing at least 10 times, replacing rare words with a special &lt;unk&gt; token. This thresholding reduces vocabulary size and helps the model focus on learning good representations for common words.\nA crucial but often overlooked aspect is subsampling of frequent words. Words like “the,” “a,” and “is” appear so frequently that they provide little information about the semantic content of their neighbors. The probability of discarding a word \\(w\\) during training is:\n\\[P(\\text{discard } w) = 1 - \\sqrt{\\frac{t}{f(w)}}\\]\nwhere \\(f(w)\\) is the frequency of word \\(w\\) and \\(t\\) is a threshold (typically \\(10^{-5}\\)). This formula ensures that very frequent words are aggressively subsampled while preserving most occurrences of informative words.\nFor training, we extract examples by sliding a window over the text. For each center word, we collect its context words within a window of size \\(m\\). Importantly, the actual window size is sampled uniformly from \\([1, m]\\) for each center word, which helps the model learn representations that are robust to varying context sizes. Consider the sentence “the cat sat on the mat” with maximum window size 2. For the center word “sat,” we might sample a window size of 1, giving context words [“cat”, “on”], or a window size of 2, giving context words [“the”, “cat”, “on”, “the”].",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#computational-efficiency-through-negative-sampling",
    "href": "24-nlp.html#computational-efficiency-through-negative-sampling",
    "title": "1  Natural Language Processing",
    "section": "1.3 Computational Efficiency Through Negative Sampling",
    "text": "1.3 Computational Efficiency Through Negative Sampling\nThe elegance of word2vec’s formulation belies a serious computational challenge. Computing the normalization term in the softmax requires summing over the entire vocabulary—potentially millions of terms—for every gradient update. With large corpora containing billions of words, this quickly becomes intractable.\nNegative sampling transforms the problem from multi-class classification to binary classification. Instead of predicting which word from the entire vocabulary appears in the context, we ask a simpler question: given a word pair, is it a real center-context pair from the corpus or a randomly generated negative example? The objective becomes:\n\\[\\mathcal{L}_{\\text{NS}} = \\log \\sigma(\\mathbf{u}_o^T \\mathbf{v}_c) + \\sum_{k=1}^K \\mathbb{E}_{w_k \\sim P_n} \\left[\\log \\sigma(-\\mathbf{u}_{w_k}^T \\mathbf{v}_c)\\right]\\]\nwhere \\(\\sigma(x) = 1/(1 + e^{-x})\\) is the sigmoid function, and \\(P_n\\) is a noise distribution over words. The clever insight is that by carefully choosing the noise distribution—typically \\(P_n(w) \\propto f(w)^{3/4}\\) where \\(f(w)\\) is word frequency—we can approximate the original objective while reducing computation from \\(O(|\\mathcal{V}|)\\) to \\(O(K)\\), where \\(K \\ll |\\mathcal{V}|\\) is a small number of negative samples (typically 5-20).\nAn alternative solution is hierarchical softmax, which replaces the flat softmax over the vocabulary with a binary tree where each leaf represents a word. The probability of a word is then the product of binary decisions along the path from root to leaf:\n\\[P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left([\\![n(w_o, j+1) = \\text{leftChild}(n(w_o, j))]\\!] \\cdot \\mathbf{u}_{n(w_o,j)}^T \\mathbf{v}_c\\right)\\]\nwhere \\(L(w_o)\\) is the length of the path to word \\(w_o\\), \\(n(w_o, j)\\) is the \\(j\\)-th node on this path, and \\([\\![\\cdot]\\!]\\) is the indicator function that returns 1 or -1. This reduces computational complexity from \\(O(|\\mathcal{V}|)\\) to \\(O(\\log |\\mathcal{V}|)\\), though negative sampling typically performs better in practice.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#global-vectors-and-matrix-factorization",
    "href": "24-nlp.html#global-vectors-and-matrix-factorization",
    "title": "1  Natural Language Processing",
    "section": "1.4 Global Vectors and Matrix Factorization",
    "text": "1.4 Global Vectors and Matrix Factorization\nWhile word2vec learns from local context windows, GloVe (Global Vectors) leverages global co-occurrence statistics. The key observation is that the ratio of co-occurrence probabilities can encode semantic relationships. Consider the words “ice” and “steam” in relation to “solid” and “gas”: \\(P(\\text{solid} \\mid \\text{ice}) / P(\\text{solid} \\mid \\text{steam})\\) is large (around 8.9), while \\(P(\\text{gas} \\mid \\text{ice}) / P(\\text{gas} \\mid \\text{steam})\\) is small (around 0.085), and \\(P(\\text{water} \\mid \\text{ice}) / P(\\text{water} \\mid \\text{steam})\\) is close to 1 (around 1.36).\nThese ratios capture the semantic relationships: ice is solid, steam is gas, and both relate to water. GloVe learns embeddings that preserve these ratios through the objective:\n\\[\\mathcal{L}_{\\text{GloVe}} = \\sum_{i,j} h(X_{ij}) \\left(\\mathbf{v}_i^T \\mathbf{u}_j + b_i + c_j - \\log X_{ij}\\right)^2\\]\nwhere \\(X_{ij}\\) counts co-occurrences, \\(h(\\cdot)\\) is a weighting function that prevents very common or very rare pairs from dominating, and \\(b_i, c_j\\) are bias terms. A typical choice is \\(h(x) = (x/x_{\\max})^{\\alpha}\\) if \\(x &lt; x_{\\max}\\), else 1, with \\(\\alpha = 0.75\\) and \\(x_{\\max} = 100\\).\nThis formulation reveals GloVe as a weighted matrix factorization problem. We seek low-rank factors \\(\\mathbf{V}\\) and \\(\\mathbf{U}\\) such that \\(\\mathbf{V}^T\\mathbf{U} \\approx \\log \\mathbf{X}\\), where the approximation is weighted by the function \\(h(\\cdot)\\). This connection to classical linear algebra provides theoretical insights: the optimal embeddings lie in the subspace spanned by the top singular vectors of an appropriately transformed co-occurrence matrix.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#beyond-words-subword-and-character-models-tokenization",
    "href": "24-nlp.html#beyond-words-subword-and-character-models-tokenization",
    "title": "1  Natural Language Processing",
    "section": "1.5 Beyond Words: Subword and Character Models (Tokenization)",
    "text": "1.5 Beyond Words: Subword and Character Models (Tokenization)\nA fundamental limitation of word-level embeddings is their inability to handle out-of-vocabulary words or capture morphological relationships. The word “unhappiness” shares obvious morphological connections with “happy,” “unhappy,” and “happiness,” but word2vec treats these as completely independent tokens.\nFastText addresses this limitation by representing words as bags of character n-grams. For the word “where” with n-grams of length 3 to 6, we extract: “&lt;wh”, “whe”, “her”, “ere”, “re&gt;”, and longer n-grams up to the full word. The word embedding is then the sum of its n-gram embeddings:\n\\[\\mathbf{v}_{\\text{where}} = \\sum_{g \\in \\mathcal{G}_{\\text{where}}} \\mathbf{z}_g\\]\nThis approach naturally handles out-of-vocabulary words by breaking them into known n-grams and provides better representations for rare words by sharing parameters across morphologically related words.\nAn even more flexible approach is Byte Pair Encoding (BPE), which learns a vocabulary of subword units directly from the data. Starting with individual characters, BPE iteratively merges the most frequent pair of adjacent units until reaching a desired vocabulary size. For example, given a corpus with word frequencies {“fast”: 4, “faster”: 3, “tall”: 5, “taller”: 4}, BPE might learn merges like “t” + “a” \\(\\rightarrow\\) “ta”, then “ta” + “l” \\(\\rightarrow\\) “tal”, and so on. This data-driven approach balances vocabulary size with representation power, enabling models to handle arbitrary text while maintaining reasonable computational requirements.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#contextual-representations-and-the-transformer-architecture",
    "href": "24-nlp.html#contextual-representations-and-the-transformer-architecture",
    "title": "1  Natural Language Processing",
    "section": "1.6 Contextual Representations and the Transformer Architecture",
    "text": "1.6 Contextual Representations and the Transformer Architecture\nStatic word embeddings suffer from a fundamental limitation: they assign a single vector to each word, ignoring context. The word “bank” receives the same representation whether it appears in “river bank” or “investment bank.” This conflation of multiple senses into a single vector creates an information bottleneck that limits performance on downstream tasks.\nEarly approaches to contextual embeddings used recurrent neural networks. Models like ELMo (Embeddings from Language Models) employ bidirectional LSTMs to encode context, computing forward and backward hidden states that are concatenated to form context-sensitive representations. ELMo further combines representations from multiple layers, allowing downstream tasks to mix different levels of abstraction through learned weights.\nThe transformer architecture revolutionized contextual representations by replacing recurrence with self-attention. For a sequence with embeddings \\(\\mathbf{H} = [\\mathbf{h}_1, \\ldots, \\mathbf{h}_n]^T\\), self-attention computes:\n\\[\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\\]\nwhere \\(\\mathbf{Q} = \\mathbf{H}\\mathbf{W}_Q\\), \\(\\mathbf{K} = \\mathbf{H}\\mathbf{W}_K\\), and \\(\\mathbf{V} = \\mathbf{H}\\mathbf{W}_V\\) are learned linear projections. This mechanism allows each position to attend to all other positions, with attention weights determined by the scaled dot product between queries and keys.\nFrom a kernel method perspective, self-attention implements a form of kernel smoothing where each position’s representation is a weighted combination of all positions. As discussed in Chapter 19, this connects to Nadaraya-Watson kernel regression, with the key difference that the kernel is both learned and input-dependent.\nMulti-head attention extends this by computing multiple attention functions in parallel, allowing the model to capture different types of relationships simultaneously. The complete transformer block combines multi-head attention with position-wise feedforward networks and residual connections:\n\\[\\mathbf{H}' = \\text{LayerNorm}(\\mathbf{H} + \\text{MultiHead}(\\mathbf{H}))\\] \\[\\mathbf{H}'' = \\text{LayerNorm}(\\mathbf{H}' + \\text{FFN}(\\mathbf{H}'))\\]\nwhere the feedforward network typically has a hidden dimension 4 times the model dimension.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#pretraining-at-scale-bert-and-beyond",
    "href": "24-nlp.html#pretraining-at-scale-bert-and-beyond",
    "title": "1  Natural Language Processing",
    "section": "1.7 Pretraining at Scale: BERT and Beyond",
    "text": "1.7 Pretraining at Scale: BERT and Beyond\nThe availability of powerful architectures raised a crucial question: how can we best leverage unlabeled text to learn general-purpose representations? BERT (Bidirectional Encoder Representations from Transformers) introduced a pretraining framework that has become the foundation for modern NLP.\nBERT’s key innovation was masked language modeling (MLM), where 15% of tokens are selected for prediction. For each selected token, 80% are replaced with [MASK], 10% with random tokens, and 10% left unchanged. This prevents the model from simply learning to copy tokens when they’re not masked. The loss function only considers predictions for masked positions:\n\\[\\mathcal{L}_{\\text{MLM}} = -\\sum_{m \\in \\mathcal{M}} \\log P(x_m \\mid \\mathbf{x}_{\\backslash \\mathcal{M}})\\]\nBERT combines MLM with next sentence prediction (NSP), which trains the model to understand relationships between sentence pairs. Training examples contain 50% consecutive sentences and 50% randomly paired sentences. The input representation concatenates both sentences with special tokens and segment embeddings to distinguish between them.\nThe scale of BERT pretraining represents a quantum leap from earlier approaches. The original BERT models were trained on a combination of BookCorpus (800 million words from over 11,000 books) and English Wikipedia (2,500 million words). This massive dataset enables the model to see diverse writing styles, topics, and linguistic phenomena. The preprocessing pipeline removes duplicate paragraphs, filters very short or very long sentences, and maintains document boundaries to ensure coherent sentence pairs for NSP.\n\nBERT Architecture and Training Details\nTo understand BERT’s architectural significance, it’s helpful to compare it with its predecessors ELMo and GPT, which represent different approaches to contextual representation learning:\n\n\n\n\n\ngraph TB\n    subgraph col1 [\" \"]\n        subgraph ELMo [\"ELMo\"]\n            direction TB\n            E1[\"BiLSTM&lt;br/&gt;Layer 1\"]\n            E2[\"BiLSTM&lt;br/&gt;Layer 2\"]\n            E3[\"Task-specific&lt;br/&gt;Architecture\"]\n            E4[\"Output\"]\n            \n            E1 --&gt; E2\n            E2 --&gt; E3\n            E3 --&gt; E4\n        end\n        \n        style E1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style E2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style E3 fill:#ccffcc,stroke:#00cc00,stroke-width:2px\n        style E4 fill:#ccccff,stroke:#0000cc,stroke-width:2px\n    end\n    \n    subgraph col2 [\" \"]\n        subgraph GPT [\"GPT\"]\n            direction TB\n            G1[\"Transformer&lt;br/&gt;Layer 1\"]\n            G2[\"Transformer&lt;br/&gt;Layer 2\"]\n            G3[\"Transformer&lt;br/&gt;Layer 3\"]\n            G4[\"Linear&lt;br/&gt;Output\"]\n            G5[\"Output\"]\n            \n            G1 --&gt; G2\n            G2 --&gt; G3\n            G3 --&gt; G4\n            G4 --&gt; G5\n        end\n        \n        style G1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style G2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style G3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style G4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px\n        style G5 fill:#ccccff,stroke:#0000cc,stroke-width:2px\n    end\n    \n    subgraph col3 [\" \"]\n        subgraph BERT [\"BERT\"]\n            direction TB\n            B1[\"Transformer&lt;br/&gt;Encoder 1\"]\n            B2[\"Transformer&lt;br/&gt;Encoder 2\"]\n            B3[\"Transformer&lt;br/&gt;Encoder 3\"]\n            B4[\"Linear&lt;br/&gt;Output\"]\n            B5[\"Output\"]\n            \n            B1 --&gt; B2\n            B2 --&gt; B3\n            B3 --&gt; B4\n            B4 --&gt; B5\n        end\n        \n        style B1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style B2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style B3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px\n        style B4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px\n        style B5 fill:#ccccff,stroke:#0000cc,stroke-width:2px\n    end\n    \n    style col1 fill:none,stroke:none\n    style col2 fill:none,stroke:none\n    style col3 fill:none,stroke:none\n\n\n\n\n\n\nELMo uses bidirectional LSTMs with task-specific architectures, requiring custom model design for each application. GPT employs a unidirectional Transformer decoder that processes text left-to-right, making it task-agnostic but unable to see future context. BERT combines the best of both: bidirectional context understanding through Transformer encoders with minimal task-specific modifications (just an output layer). BERT comes in two main configurations that balance model capacity with computational requirements:\n\nBert configurations and parameters\n\n\n\n\n\n\n\n\n\nModel\nTransformer Layers\nHidden Dimensions\nAttention Heads\nParameters\n\n\n\n\nBERT-Base\n12\n768\n12\n110M\n\n\nBERT-Large\n24\n1024\n16\n340M\n\n\n\nBoth models use a vocabulary of 30,000 WordPiece tokens, learned using a data-driven tokenization algorithm similar to BPE. The maximum sequence length is 512 tokens, though most pretraining uses sequences of 128 tokens to improve efficiency, with only the final 10% of training using full-length sequences.\nThe pretraining procedure involves several techniques to stabilize and accelerate training:\n\nWarm-up Learning Rate: The learning rate increases linearly for the first 10,000 steps to \\(10^{-4}\\), then decreases linearly. This warm-up prevents large gradients early in training when the model is randomly initialized.\nGradient Accumulation: To simulate larger batch sizes on limited hardware, gradients are accumulated over multiple forward passes before updating weights. BERT uses an effective batch size of 256 sequences.\nMixed Precision Training: Using 16-bit floating point for most computations while maintaining 32-bit master weights speeds up training significantly on modern GPUs.\n\n\n\nData Preparation for BERT Pretraining\nThe data preparation pipeline for BERT is surprisingly complex. Starting with raw text, the process involves:\n\n\n\n\n\ngraph LR\n    A[Raw Text] --&gt; B[Document Segmentation]\n    B --&gt; C[Sentence Segmentation]\n    C --&gt; D[WordPiece Tokenization]\n    D --&gt; E[Creating Training Examples]\n    E --&gt; F[Creating TFRecord Files]\n\n\n\n\n\n\nIn document segmentation stage, text is split into documents, maintaining natural boundaries. For books, this means chapter boundaries; for Wikipedia, article boundaries. The next stage is sentence segmentation, where each document is split into sentences using heuristic rules. Typically, this involves identifying periods followed by whitespace and capital letters, while accounting for exceptions such as abbreviations.\nFollowing sentence segmentation, the text undergoes WordPiece tokenization. This process uses a learned WordPiece vocabulary to break text into subword units, ensuring that unknown words can be represented as sequences of known subwords. Special handling is applied to mark the beginning of words.\nOnce tokenized, the data is organized into training examples. For each example, a target sequence length is sampled from a geometric distribution to introduce variability. Sentences are packed together until the target length is reached. For the next sentence prediction (NSP) task, half of the examples use the actual next segment, while the other half use a randomly selected segment. Masked language modeling (MLM) is applied by masking 15% of tokens, following the 80/10/10 strategy: 80% of the time the token is replaced with [MASK], 10% with a random token, and 10% left unchanged. Special tokens such as [CLS] at the beginning and [SEP] between segments are added, and segment embeddings are created (0 for the first segment, 1 for the second). Sequences are padded to a fixed length using [PAD] tokens.\nFinally, the prepared examples are serialized into TFRecord files for efficient input/output during training. Examples are grouped by sequence length to minimize the amount of padding required.\nThe democratization of pretraining through libraries like Hugging Face Transformers has made it possible for smaller organizations to leverage these powerful techniques, either by fine-tuning existing models or pretraining specialized models for their domains.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#transfer-learning-and-downstream-applications",
    "href": "24-nlp.html#transfer-learning-and-downstream-applications",
    "title": "1  Natural Language Processing",
    "section": "1.8 Transfer Learning and Downstream Applications",
    "text": "1.8 Transfer Learning and Downstream Applications\nThe power of pretrained models lies in their transferability. For sentiment analysis, we add a linear layer on top of the [CLS] token representation and fine-tune on labeled data. Popular datasets include IMDb movie reviews (50K examples) and Stanford Sentiment Treebank (11,855 sentences). Fine-tuning typically requires only 2-4 epochs, demonstrating the effectiveness of transfer learning.\nNatural language inference (NLI) determines logical relationships between premise and hypothesis sentences. The Stanford Natural Language Inference corpus contains 570,000 sentence pairs labeled as entailment, contradiction, or neutral. For BERT-based NLI, we concatenate premise and hypothesis with [SEP] tokens and classify using the [CLS] representation.\nToken-level tasks like named entity recognition classify each token independently. Common datasets include CoNLL-2003 (English and German entities) and OntoNotes 5.0 (18 entity types). The BIO tagging scheme marks entity boundaries: B-PER for beginning of person names, I-PER for inside, and O for outside any entity.\nQuestion answering presents unique challenges. The SQuAD dataset contains 100,000+ questions where answers are text spans from Wikipedia articles. BERT approaches this by predicting start and end positions independently, with the final answer span selected to maximize the product of start and end probabilities subject to length constraints.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#model-compression-and-efficiency",
    "href": "24-nlp.html#model-compression-and-efficiency",
    "title": "1  Natural Language Processing",
    "section": "1.9 Model Compression and Efficiency",
    "text": "1.9 Model Compression and Efficiency\nWhile large pretrained models achieve impressive performance, their computational requirements limit deployment. Knowledge distillation trains a small “student” model to mimic a large “teacher” model through a combined loss:\n\\[\\mathcal{L}_{\\text{distill}} = \\alpha \\mathcal{L}_{\\text{task}} + (1-\\alpha) \\text{KL}(p_{\\text{teacher}} \\| p_{\\text{student}})\\]\nDistilBERT achieves 97% of BERT’s performance with 40% fewer parameters and 60% faster inference. Quantization reduces numerical precision from 32-bit to 8-bit or even lower, while pruning removes connections below a magnitude threshold. These techniques can reduce model size by an order of magnitude with minimal performance degradation.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#theoretical-perspectives-and-future-directions",
    "href": "24-nlp.html#theoretical-perspectives-and-future-directions",
    "title": "1  Natural Language Processing",
    "section": "1.10 Theoretical Perspectives and Future Directions",
    "text": "1.10 Theoretical Perspectives and Future Directions\nThe success of language models connects to several theoretical frameworks. Transformers are universal approximators for sequence-to-sequence functions—given sufficient capacity, they can approximate any continuous function to arbitrary precision. The self-attention mechanism provides an inductive bias well-suited to capturing long-range dependencies.\nDespite having hundreds of millions of parameters, these models generalize remarkably well. This connects to implicit regularization in overparameterized models, where gradient descent dynamics bias toward solutions with good generalization properties. Language models automatically learn hierarchical features: early layers capture syntax and morphology, middle layers semantic relationships, and later layers task-specific abstractions.\nYet significant challenges remain. Models struggle with compositional generalization—understanding “red car” and “blue house” doesn’t guarantee understanding “red house” if that combination is rare in training. Sample efficiency remains poor compared to human learning. A child masters basic grammar from thousands of examples; BERT sees billions. This gap suggests fundamental differences in learning mechanisms.\nInterpretability poses ongoing challenges. While attention visualizations provide some insights, we lack principled methods for understanding distributed representations across hundreds of layers and attention heads. Future directions include multimodal understanding (integrating text with vision and speech), more efficient architectures that maintain performance while reducing computational requirements, and developing theoretical frameworks to predict and understand model behavior.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#natural-language-processing-applications",
    "href": "24-nlp.html#natural-language-processing-applications",
    "title": "1  Natural Language Processing",
    "section": "1.11 Natural Language Processing: Applications",
    "text": "1.11 Natural Language Processing: Applications\nHaving established the theoretical foundations and pretraining methodologies for natural language understanding, we now turn to the practical application of these techniques to solve real-world problems. The power of pretrained representations lies not merely in their mathematical elegance, but in their ability to transfer learned linguistic knowledge to diverse downstream tasks with minimal architectural modifications.\nThe landscape of NLP applications can be broadly categorized into two fundamental types based on their input structure and prediction granularity. Sequence-level tasks operate on entire text sequences, producing a single output per input sequence or sequence pair. These include sentiment classification, where we determine the emotional polarity of a review, and natural language inference, where we assess logical relationships between premise-hypothesis pairs. Token-level tasks make predictions for individual tokens within sequences, such as named entity recognition that identifies person names, locations, and organizations, or question answering that pinpoints answer spans within passages.\n\n\n\n\n\ngraph TB\n    subgraph \"Pretrained Representations\" [\"Pretrained Text Representations\"]\n        P1[Word2Vec/GloVe] \n        P2[BERT/RoBERTa]\n        P3[GPT/T5]\n    end\n    \n    subgraph \"Deep Learning Architectures\" [\"Model Architectures\"]\n        A1[MLPs]\n        A2[CNNs] \n        A3[RNNs]\n        A4[Transformers]\n        A5[Attention Mechanisms]\n    end\n    \n    subgraph \"Sequence-Level Tasks\" [\"Sequence-Level Applications\"]\n        S1[Sentiment Analysis]\n        S2[Text Classification]\n        S3[Natural Language Inference]\n        S4[Semantic Similarity]\n    end\n    \n    subgraph \"Token-Level Tasks\" [\"Token-Level Applications\"]\n        T1[Named Entity Recognition]\n        T2[Part-of-Speech Tagging]\n        T3[Question Answering]\n        T4[Text Summarization]\n    end\n    \n    P1 --&gt; A1\n    P1 --&gt; A2\n    P1 --&gt; A3\n    P2 --&gt; A4\n    P2 --&gt; A5\n    P3 --&gt; A4\n    \n    A1 --&gt; S1\n    A1 --&gt; S3\n    A2 --&gt; S1\n    A2 --&gt; S2\n    A3 --&gt; S1\n    A3 --&gt; S3\n    A4 --&gt; S2\n    A4 --&gt; S3\n    A4 --&gt; T1\n    A4 --&gt; T3\n    A5 --&gt; S3\n    A5 --&gt; T3\n    \n    style P1 fill:#e8f4f8\n    style P2 fill:#e8f4f8\n    style P3 fill:#e8f4f8\n    style A1 fill:#f0f8e8\n    style A2 fill:#f0f8e8\n    style A3 fill:#f0f8e8\n    style A4 fill:#f0f8e8\n    style A5 fill:#f0f8e8\n    style S1 fill:#fdf2e8\n    style S2 fill:#fdf2e8\n    style S3 fill:#fdf2e8\n    style S4 fill:#fdf2e8\n    style T1 fill:#f8e8f4\n    style T2 fill:#f8e8f4\n    style T3 fill:#f8e8f4\n    style T4 fill:#f8e8f4\n\n\n\n\n\n\nThis architectural flexibility represents one of the most significant advantages of the representation learning paradigm. Unlike earlier rule-based or feature-engineering approaches that required domain-specific expertise for each task, modern NLP systems can leverage the same pretrained representations across vastly different applications. A BERT model pretrained on general text can be fine-tuned for medical document classification, legal contract analysis, or social media sentiment detection with only task-specific output layers.\n\nSentiment Analysis: From Opinions to Insights\nSentiment analysis exemplifies the practical value of learned text representations. Consider the challenge of processing customer reviews for a major e-commerce platform. Traditional approaches might rely on hand-crafted lexicons of positive and negative words, but such methods fail to capture context-dependent sentiment. The phrase “not bad” expresses mild approval despite containing the negative word “bad,” while “insanely good” uses typically negative intensity (“insanely”) to convey strong positive sentiment.\nModern sentiment analysis systems leverage pretrained embeddings that capture these nuanced semantic relationships. A convolutional neural network architecture can effectively identify local sentiment-bearing phrases through learned filters, while recurrent networks model the sequential dependencies that determine how sentiment evolves throughout a text. The Stanford Sentiment Treebank, with its fine-grained annotations ranging from very negative to very positive, provides a testing ground where contemporary models achieve accuracy levels approaching human inter-annotator agreement.\n\n\nNatural Language Inference: Reasoning About Meaning\nNatural language inference represents a more sophisticated challenge that requires understanding logical relationships between text pairs. Given a premise “The company’s profits increased by 15% this quarter” and a hypothesis “The business is performing well,” a model must determine whether the hypothesis follows from the premise (entailment), contradicts it, or remains neutral.\nThe Stanford Natural Language Inference corpus provides over 570,000 such premise-hypothesis pairs, enabling large-scale training of inference models. Successful approaches often employ attention mechanisms to align relevant portions of premises with hypotheses, identifying which words and phrases support or contradict the proposed logical relationship. The decomposable attention model, for instance, computes element-wise attention between premise and hypothesis tokens, allowing fine-grained comparison of semantic content.\n\n\nToken-Level Applications: Precision at the Word Level\nToken-level tasks require models to make predictions for individual words or subwords within sequences. Named entity recognition illustrates this paradigm: given the sentence “Apple Inc. was founded by Steve Jobs in Cupertino,” a model must identify “Apple Inc.” as an organization, “Steve Jobs” as a person, and “Cupertino” as a location.\nThe BIO (Begin-Inside-Outside) tagging scheme provides a standard framework for such tasks. The word “Apple” receives the tag “B-ORG” (beginning of organization), “Inc.” gets “I-ORG” (inside organization), “Steve” becomes “B-PER” (beginning of person), and so forth. This encoding allows models to handle multi-word entities while maintaining the token-level prediction structure.\nQuestion answering presents another compelling token-level application. In extractive question answering, models must identify spans within passages that answer given questions. The SQuAD dataset poses questions about Wikipedia articles, requiring models to pinpoint exact text spans as answers. For example, given the passage about Apple Inc. and the question “Who founded Apple?”, the model should identify “Steve Jobs” as the answer span.\n\n\nFine-Tuning Pretrained Models\nThe emergence of large-scale pretrained models like BERT has revolutionized the application landscape. Rather than training task-specific models from scratch, practitioners can fine-tune pretrained representations on downstream tasks with remarkable efficiency. This approach typically requires only 2-4 epochs of training on task-specific data, compared to the hundreds of epochs needed for training from random initialization.\nThe fine-tuning process involves freezing most pretrained parameters while allowing task-specific layers to adapt. For sequence classification, this might involve adding a single linear layer on top of BERT’s [CLS] token representation. For token-level tasks like named entity recognition, each token’s representation feeds through the same classification layer to produce per-token predictions.\nThis transfer learning paradigm has democratized access to state-of-the-art NLP capabilities. Organizations without massive computational resources can leverage pretrained models fine-tuned on their specific domains, achieving performance that would have required substantial research and development investments just a few years ago.\n\n\nChallenges and Future Directions\nDespite remarkable progress, significant challenges remain in NLP applications. Models often exhibit brittleness when faced with adversarial examples or distribution shifts between training and deployment data. A sentiment classifier trained on movie reviews might fail on product reviews due to domain-specific language patterns. Similarly, named entity recognition systems trained on news articles may struggle with social media text due to informal language and unconventional capitalization.\nComputational efficiency presents another ongoing concern. While large pretrained models achieve impressive performance, their size and inference requirements limit deployment in resource-constrained environments. Knowledge distillation and model pruning techniques offer promising approaches for creating smaller, faster models that retain much of the original performance.\nThe interpretability challenge looms large as models become more complex. Understanding why a model makes specific predictions becomes crucial for high-stakes applications like medical diagnosis or legal document analysis. Attention visualizations provide some insight, but we lack comprehensive frameworks for understanding decision-making processes in large neural networks.\nFuture directions point toward multimodal understanding that integrates text with visual and auditory information, few-shot learning that adapts quickly to new domains with minimal examples, and more robust architectures that maintain performance across diverse linguistic contexts. The field continues evolving rapidly, driven by the interplay between theoretical advances in representation learning and practical demands from real-world applications.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-nlp.html#conclusion",
    "href": "24-nlp.html#conclusion",
    "title": "1  Natural Language Processing",
    "section": "1.12 Conclusion",
    "text": "1.12 Conclusion\nThe journey from symbolic manipulation to neural language understanding represents one of the great success stories of modern artificial intelligence. By reconceptualizing language as geometry in high-dimensional spaces, leveraging self-supervision at scale, and developing powerful architectural innovations like transformers, the field has achieved capabilities that seemed like science fiction just a decade ago.\nThe mathematical frameworks developed—from distributional semantics to attention mechanisms—provide not just engineering tools but lenses through which to examine fundamental questions about meaning and understanding. As these systems become more capable and widely deployed, understanding their theoretical foundations, practical limitations, and societal implications becomes ever more critical.\nThe techniques discussed in this chapter—word embeddings, contextual representations, pretraining, and fine-tuning—form the foundation of modern NLP systems. Yet despite impressive engineering achievements, we’ve only begun to scratch the surface of true language understanding. The rapid progress offers both tremendous opportunities and sobering responsibilities. The most exciting chapters in this story are yet to be written.\n\n\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "25-llm.html",
    "href": "25-llm.html",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "",
    "text": "2.1 Adding One Word at a Time\nLarge Language Models (LLMs) have emerged as one of the most profound breakthroughs in artificial intelligence, fundamentally reshaping our relationship with technology. These models can write poetry that moves us to tears, generate computer code that solves complex problems, translate languages with nuanced understanding, and hold conversations with a fluency that often feels remarkably human. But how do they work? What is the magic behind the curtain that makes a computer suddenly seem to understand the subtleties of human language and thought?\nAt their core, LLMs are powered by the Transformer architecture, that hinges on a concept called attention—the ability to weigh the importance of different words in a sentence to grasp context and meaning. Imagine if you could instantly understand not just what someone is saying, but also catch every subtle reference, every implied connection, every hidden meaning between the lines. This is what attention mechanisms give to artificial intelligence. This chapter will journey from the foundational ideas of attention to the colossal models that are defining our modern world, exploring not just how they work, but how they think.\nThe first application of LLMs that most people encounter is text generation. You provide a prompt, and the model generates a continuation that often feels remarkably coherent and relevant. This ability to produce text that mimics human writing is one of the most striking features of LLMs. But how does it achieve this? And why does it work so well?\nAt its core, an LLM is designed to predict the next token in a sequence based on the context provided by the preceding tokens. This process involves generating a “reasonable continuation” of the input text, where “reasonable” means consistent with patterns observed in vast amounts of human-written text, such as books, articles, and websites. For example, given the prompt “The best thing about AI is its ability to,” the model analyzes patterns from its training data to predict likely continuations. It doesn’t simply match literal text; instead, it evaluates semantic and contextual similarities to produce a ranked list of possible next tokens along with their probabilities.\nThis mechanism allows LLMs to generate text that aligns with human expectations, leveraging their ability to understand context and meaning at a deep level. By iteratively predicting and appending tokens, the model constructs coherent and meaningful responses that often feel indistinguishable from human writing. Let’s see how this works in practice with a simple example using the SmolLM2 model. We’ll start by loading the model and tokenizer, which are essential components for generating text. The tokenizer converts text into tokens that the model can understand, while the model itself generates predictions based on those tokens.\nsys.path.append('./code')\n# Import our custom functions\nfrom llm_chapter import (ask_smol_lm, get_next_word_suggestions, generate_text_step_by_step)\n\nlocal_cache_dir = \"./models_cache\"  # or use absolute path like \"/Users/your_username/ai_models\"\n# Create directory if it doesn't exist\nPath(local_cache_dir).mkdir(parents=True, exist_ok=True)\n# Load model with custom cache directory\nmodel_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n# This will download once and store in your specified directory\ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=local_cache_dir)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=local_cache_dir,device_map=\"auto\",torch_dtype=torch.float16,low_cpu_mem_usage=True)\n\nprint(\"✅ Model loaded from cache directory!\")\n\n## ✅ Model loaded from cache directory!\n\ndevice = model.device\nprint(\"Using device:\", device)\n\n## Using device: mps:0\nConsider the text “The best thing about AI is its ability to”. Imagine analyzing billions of pages of human-written text—such as those found on the web or in digitized books—and identifying all instances of this text to determine what word most commonly comes next. While LLM doesn’t directly search for literal matches, it evaluates semantic and contextual similarities to produce a ranked list of possible next words along with their associated probabilities. This process enables it to generate coherent and contextually appropriate continuations.\n# Get next word suggestions for a given text\ninitial_text = \"The best thing about AI is its ability to\"\nsuggestions = get_next_word_suggestions(initial_text, model, tokenizer, top_k=5)\nprint(f\"Next word suggestions for '{initial_text}':\")\n\n## Next word suggestions for 'The best thing about AI is its ability to':\n\nfor i, (word, prob) in enumerate(suggestions):\n    print(f\"  {i+1}. '{word}' (prob: {prob:.3f})\")\n\n##   1. ' learn' (prob: 0.623)\n##   2. ' help' (prob: 0.119)\n##   3. ' augment' (prob: 0.100)\n##   4. ' analyze' (prob: 0.084)\n##   5. ' process' (prob: 0.074)\nWhen an LLM generates text, it essentially operates by repeatedly asking, “Given the text so far, what should the next word be?”—and then appending a word to the output. More precisely, it adds a “token,” which could represent a full word or just a part of one, allowing it to occasionally create novel words.\nAt each step, the model produces a ranked list of possible tokens along with their probabilities. One might assume the model should always select the token with the highest probability. However, if this approach is followed strictly, the generated text often lacks creativity and can become repetitive. To address this, randomness is introduced into the selection process. By occasionally choosing lower-ranked tokens, the model can produce more varied and engaging text.\nThis randomness means that using the same prompt multiple times will likely yield different outputs. A parameter called “temperature” controls the degree of randomness in token selection. For text generation tasks, a temperature value of around 0.8 is often found to strike a good balance between coherence and creativity. It’s worth noting that this parameter is based on empirical findings rather than theoretical principles. The term “temperature” originates from statistical physics due to the use of exponential distributions, but its application here is purely mathematical.\nBelow is an illustration of the iterative process where the model selects the word with the highest probability at each step (referred to in the code as the model’s “decision”):\n# Let's start with a simple prompt\ninitial_text = \"The best thing about AI is its ability to\"\nprint(f\"Initial text: '{initial_text}'\")\n\n## Initial text: 'The best thing about AI is its ability to'\n# Generate text step by step\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer, num_steps=10, temperature=1.0, sample=False, print_progress=True)\n\n## Starting text: 'The best thing about AI is its ability to'\n## ============================================================\n## 'The best thing about AI is its ability to learn' (prob: 0.280)\n## 'The best thing about AI is its ability to learn and' (prob: 0.697)\n## 'The best thing about AI is its ability to learn and adapt' (prob: 0.481)\n## 'The best thing about AI is its ability to learn and adapt.' (prob: 0.321)\n## 'The best thing about AI is its ability to learn and adapt. It' (prob: 0.260)\n## 'The best thing about AI is its ability to learn and adapt. It can' (prob: 0.529)\n## 'The best thing about AI is its ability to learn and adapt. It can analyze' (prob: 0.268)\n## 'The best thing about AI is its ability to learn and adapt. It can analyze vast' (prob: 0.619)\n## 'The best thing about AI is its ability to learn and adapt. It can analyze vast amounts' (prob: 0.999)\n## 'The best thing about AI is its ability to learn and adapt. It can analyze vast amounts of' (prob: 1.000)\n\nprint(\"Generated text:\")\n\n## Generated text:\n\nprint(textwrap.fill(generated_text, width=80))\n\n## The best thing about AI is its ability to learn and adapt. It can analyze vast\n## amounts of\nIn this example, we always select the most probable next token, which leads to a coherent but somewhat predictable continuation. The model generates text by repeatedly applying this process, building on the context provided by the previous tokens. In fact, we’ve seen that the model actually generates multiple suggestions for the next word, which can be useful for understanding how it thinks about language.\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nsuggestions = get_next_word_suggestions(initial_text, model, tokenizer, top_k=10)\nindices = list(range(len(suggestions)))\nwords = [s[0] for s in suggestions]\nprobabilities = [s[1] for s in suggestions]\n# Plotting the next word suggestions with their log probabilities\nplt.plot(indices, np.log10(probabilities), marker='o', linestyle='-', color='skyblue')\nplt.xticks(indices, words, rotation=45, ha='right')\n\n\n## ([&lt;matplotlib.axis.XTick object at 0x3fc7fb380&gt;, &lt;matplotlib.axis.XTick object at 0x3fc7fb350&gt;, &lt;matplotlib.axis.XTick object at 0x3fbe2c740&gt;, &lt;matplotlib.axis.XTick object at 0x3fc4efe30&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80adb0&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80b770&gt;, &lt;matplotlib.axis.XTick object at 0x3fc80a690&gt;, &lt;matplotlib.axis.XTick object at 0x3fc809f10&gt;, &lt;matplotlib.axis.XTick object at 0x3fc85c530&gt;, &lt;matplotlib.axis.XTick object at 0x3fc85cec0&gt;], [Text(0, 0, ' learn'), Text(1, 0, ' help'), Text(2, 0, ' augment'), Text(3, 0, ' analyze'), Text(4, 0, ' process'), Text(5, 0, ' think'), Text(6, 0, ' enhance'), Text(7, 0, ' automate'), Text(8, 0, ' provide'), Text(9, 0, ' assist')])\n\n\nCode\nplt.xlabel('Next Word Suggestions')\nplt.ylabel('Log Probability')\nplt.title('Next Word Suggestions with Log Probabilities')\nplt.grid()\nplt.tight_layout()\nplt.show()\nThe plot above shows the next word suggestions generated by the model, with their probabilities represented on a logarithmic scale. This visualization helps us understand how the model ranks different words based on their likelihood of being the next token in the sequence. We can see that the probabilities of each next word decay exponentially (outside of the top word ‘learn’). This is the law known as Zipf’s law, which was observed by natural language researchers in the 1930s. It states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, a few words are used very frequently, while most words are used rarely.\nNow we will run our LLM generation process for longer and will sample words with probabilities that are calculated based on the temperature parameter. We will use a temperature of 0.8, which is often a good choice for generating coherent text without being too repetitive.\n# Fix the seed for reproducibility\ntorch.manual_seed(8);\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer, num_steps=60, temperature=0.8, sample=True,print_progress=False)\nprint(\"Generated text:\")\n\n## Generated text:\n\nprint(textwrap.fill(generated_text, width=80))\n\n## The best thing about AI is its ability to learn and improve over time. By\n## continuously gathering data and analyzing its performance, AI systems can refine\n## their decision-making processes to become more effective and efficient. This\n## means that as AI systems encounter new data, they can adapt and learn from their\n## experiences, leading to better outcomes.\"    4\nThe generated text demonstrates the model’s ability to create coherent and contextually relevant sentences, even when sampling from a distribution of possible next words. Now, compare this with the output generated using a higher temperature setting.\ntorch.manual_seed(8);\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer, num_steps=60, temperature=1.2, sample=True,print_progress=False)\nprint(\"Generated text:\")\n\n## Generated text:\n\nprint(textwrap.fill(generated_text, width=80))\n\n## The best thing about AI is its ability to learn and improve over time. One area\n## that holds immense potential is in natural language processing,\" wrote an AI on\n## Twitter after retweeting itself 100 times. It isn't cute, but it highlights how\n## much stock we should place in machines' ability to develop basic AI. It is what\nWe can see that setting temperature to 1.2 introduces more randomness. In fact, the generation process went “off track” rather quickly, generating meaningless phrases that don’t follow the initial context. This illustrates how temperature affects the model’s creativity and coherence. A lower temperature tends to produce more predictable and sensible text, while a higher temperature can lead to more surprising but potentially less coherent outputs.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-simplest-form-of-text-generation-one-letter-at-a-time",
    "href": "25-llm.html#the-simplest-form-of-text-generation-one-letter-at-a-time",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.2 The simplest form of text generation: One Letter at a Time",
    "text": "2.2 The simplest form of text generation: One Letter at a Time\nThe simples thing we can do with an LLM is to generate text one letter at a time. This is a very basic form of text generation, but it can be useful for understanding how the model works at a fundamental level. Let’s see how we can implement this using the same model and tokenizer we used earlier. We start by counting marginal (unconditional) letter frequencies in the text of a Wikipedia article about cats. This will give us a sense of how often each letter appears in the text, which is a good starting point for understanding how the model generates text.\n\n# Download wikipedia article on \"Cat\"\nimport requests\nurl = \"https://en.wikipedia.org/wiki/Cat\"\nresponse = requests.get(url)\ncat_text = response.text\n# Extract text from HTML\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(cat_text, 'html.parser')\ncat_text = soup.get_text()\n\nNow let’s count letter frequencies in the text\n\nfrom collections import Counter\nletter_counts = Counter(c.lower() for c in cat_text if c.isalpha())\n# Sort by frequency\nsorted_letter_counts = sorted(letter_counts.items(), key=lambda x: x[1], reverse=True)\n\nFinally, plot the letter frequencies for the first 26 letters\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nsorted_letter_counts = sorted_letter_counts[:26]  # Limit to top 26 letters\nletters, counts = zip(*sorted_letter_counts)\nplt.bar(letters, counts, color='skyblue')\nplt.xlabel('Letters')\nplt.ylabel('Frequency')\nplt.title('Letter Frequencies in Wikipedia Article on Cats')\nplt.xticks(rotation=45)\n\n\n## ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [Text(0, 0, 'e'), Text(1, 0, 'a'), Text(2, 0, 'i'), Text(3, 0, 't'), Text(4, 0, 'n'), Text(5, 0, 'o'), Text(6, 0, 's'), Text(7, 0, 'r'), Text(8, 0, 'c'), Text(9, 0, 'l'), Text(10, 0, 'd'), Text(11, 0, 'h'), Text(12, 0, 'm'), Text(13, 0, 'u'), Text(14, 0, 'g'), Text(15, 0, 'p'), Text(16, 0, 'f'), Text(17, 0, 'b'), Text(18, 0, 'y'), Text(19, 0, 'v'), Text(20, 0, 'w'), Text(21, 0, 'k'), Text(22, 0, 'j'), Text(23, 0, 'x'), Text(24, 0, 'z'), Text(25, 0, 'q')])\n\n\nCode\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf we try to generate the text one letter at a time\n\nimport numpy as np\n# Generate text one letter at a time by sampling from the letter frequencies\ncounts = np.array(counts)/sum(counts)  # Normalize counts to probabilities\nimport random\ngentext = random.choices(letters, weights=counts, k=20)\nprint(\"Generated letters:\", ''.join(gentext))\n\n## Generated letters: hecatmrtplpdhovcelmi\n\n\nWhat if we do bi-grams, i.e. pairs of letters? We can do this by counting the frequencies of each pair of letters in the text. This will give us a sense of how often each pair of letters appears in the text, which is a good starting point for understanding how the model generates text.\n\n\nCode\nfrom collections import defaultdict\nbigram_counts = defaultdict(int)\nfor i in range(len(cat_text) - 1):\n    if cat_text[i].isalpha() and cat_text[i + 1].isalpha():\n        a, b = cat_text[i].lower(), cat_text[i + 1].lower()\n        # Only process standard English letters (a-z)\n        if 'a' &lt;= a &lt;= 'z' and 'a' &lt;= b &lt;= 'z':\n            bigram = (a, b)\n            bigram_counts[bigram] += 1\n\n# Sort by frequency\nsorted_bigram_counts = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Plot the heatmap of bigram frequencies\nimport seaborn as sns\nbigram_matrix = np.zeros((26, 26))\nfor (a, b), count in sorted_bigram_counts:\n    bigram_matrix[ord(a) - ord('a'), ord(b) - ord('a')] = count\n\nsns.heatmap(bigram_matrix, cmap='Blues')\nplt.xlabel('Second Letter')\nplt.ylabel('First Letter')\nplt.title('Bigram Frequencies in Wikipedia Article on Cats')\nplt.xticks(ticks=np.arange(26) + 0.5, labels=[chr(i + ord('a')) for i in range(26)], rotation=45)\n\n\n## ([&lt;matplotlib.axis.XTick object at 0x42609ec00&gt;, &lt;matplotlib.axis.XTick object at 0x42609e8a0&gt;, &lt;matplotlib.axis.XTick object at 0x4260dd2b0&gt;, &lt;matplotlib.axis.XTick object at 0x42609ebd0&gt;, &lt;matplotlib.axis.XTick object at 0x4261074d0&gt;, &lt;matplotlib.axis.XTick object at 0x426107d40&gt;, &lt;matplotlib.axis.XTick object at 0x426106450&gt;, &lt;matplotlib.axis.XTick object at 0x426130890&gt;, &lt;matplotlib.axis.XTick object at 0x4261311f0&gt;, &lt;matplotlib.axis.XTick object at 0x426131b80&gt;, &lt;matplotlib.axis.XTick object at 0x426132450&gt;, &lt;matplotlib.axis.XTick object at 0x426132150&gt;, &lt;matplotlib.axis.XTick object at 0x426132b10&gt;, &lt;matplotlib.axis.XTick object at 0x42614cf80&gt;, &lt;matplotlib.axis.XTick object at 0x426191880&gt;, &lt;matplotlib.axis.XTick object at 0x4260b30e0&gt;, &lt;matplotlib.axis.XTick object at 0x4261a85c0&gt;, &lt;matplotlib.axis.XTick object at 0x4261a9040&gt;, &lt;matplotlib.axis.XTick object at 0x4261061b0&gt;, &lt;matplotlib.axis.XTick object at 0x426193440&gt;, &lt;matplotlib.axis.XTick object at 0x4261a98e0&gt;, &lt;matplotlib.axis.XTick object at 0x4261aa090&gt;, &lt;matplotlib.axis.XTick object at 0x4261aa930&gt;, &lt;matplotlib.axis.XTick object at 0x4261817c0&gt;, &lt;matplotlib.axis.XTick object at 0x4261ab020&gt;, &lt;matplotlib.axis.XTick object at 0x4261ab8c0&gt;], [Text(0.5, 0, 'a'), Text(1.5, 0, 'b'), Text(2.5, 0, 'c'), Text(3.5, 0, 'd'), Text(4.5, 0, 'e'), Text(5.5, 0, 'f'), Text(6.5, 0, 'g'), Text(7.5, 0, 'h'), Text(8.5, 0, 'i'), Text(9.5, 0, 'j'), Text(10.5, 0, 'k'), Text(11.5, 0, 'l'), Text(12.5, 0, 'm'), Text(13.5, 0, 'n'), Text(14.5, 0, 'o'), Text(15.5, 0, 'p'), Text(16.5, 0, 'q'), Text(17.5, 0, 'r'), Text(18.5, 0, 's'), Text(19.5, 0, 't'), Text(20.5, 0, 'u'), Text(21.5, 0, 'v'), Text(22.5, 0, 'w'), Text(23.5, 0, 'x'), Text(24.5, 0, 'y'), Text(25.5, 0, 'z')])\n\n\nCode\nplt.yticks(ticks=np.arange(26) + 0.5, labels=[chr(i + ord('a')) for i in range(26)], rotation=0)\n\n\n## ([&lt;matplotlib.axis.YTick object at 0x4260b2210&gt;, &lt;matplotlib.axis.YTick object at 0x4260b0c20&gt;, &lt;matplotlib.axis.YTick object at 0x4260dd490&gt;, &lt;matplotlib.axis.YTick object at 0x426133e30&gt;, &lt;matplotlib.axis.YTick object at 0x42614c860&gt;, &lt;matplotlib.axis.YTick object at 0x4261339b0&gt;, &lt;matplotlib.axis.YTick object at 0x42614cb00&gt;, &lt;matplotlib.axis.YTick object at 0x42614d640&gt;, &lt;matplotlib.axis.YTick object at 0x42614e2a0&gt;, &lt;matplotlib.axis.YTick object at 0x42614ea20&gt;, &lt;matplotlib.axis.YTick object at 0x42614f5f0&gt;, &lt;matplotlib.axis.YTick object at 0x42614e5a0&gt;, &lt;matplotlib.axis.YTick object at 0x42614fd70&gt;, &lt;matplotlib.axis.YTick object at 0x426106780&gt;, &lt;matplotlib.axis.YTick object at 0x4261d16a0&gt;, &lt;matplotlib.axis.YTick object at 0x4261a9ac0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d0980&gt;, &lt;matplotlib.axis.YTick object at 0x4261d1d90&gt;, &lt;matplotlib.axis.YTick object at 0x4261d2720&gt;, &lt;matplotlib.axis.YTick object at 0x4261d30b0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d2900&gt;, &lt;matplotlib.axis.YTick object at 0x4261d37d0&gt;, &lt;matplotlib.axis.YTick object at 0x4261d3d70&gt;, &lt;matplotlib.axis.YTick object at 0x4261f0a40&gt;, &lt;matplotlib.axis.YTick object at 0x4261f1280&gt;, &lt;matplotlib.axis.YTick object at 0x4261d1940&gt;], [Text(0, 0.5, 'a'), Text(0, 1.5, 'b'), Text(0, 2.5, 'c'), Text(0, 3.5, 'd'), Text(0, 4.5, 'e'), Text(0, 5.5, 'f'), Text(0, 6.5, 'g'), Text(0, 7.5, 'h'), Text(0, 8.5, 'i'), Text(0, 9.5, 'j'), Text(0, 10.5, 'k'), Text(0, 11.5, 'l'), Text(0, 12.5, 'm'), Text(0, 13.5, 'n'), Text(0, 14.5, 'o'), Text(0, 15.5, 'p'), Text(0, 16.5, 'q'), Text(0, 17.5, 'r'), Text(0, 18.5, 's'), Text(0, 19.5, 't'), Text(0, 20.5, 'u'), Text(0, 21.5, 'v'), Text(0, 22.5, 'w'), Text(0, 23.5, 'x'), Text(0, 24.5, 'y'), Text(0, 25.5, 'z')])\n\n\nCode\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThis will take us one step closer to how LLMs generate text. However, LLM’s have much larger context windows, meaning they can consider much longer sequences of text when generating the next token. This is crucial for understanding how LLMs can generate coherent and contextually relevant text. The modern models such as Gemini 2.5 pro uses context windows of up to 1 million tokens. It is approximately the size of the “War and Peace” novel by Leo Tolstoy.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-secret-sauce-from-attention-to-transformers",
    "href": "25-llm.html#the-secret-sauce-from-attention-to-transformers",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.3 The Secret Sauce: From Attention to Transformers",
    "text": "2.3 The Secret Sauce: From Attention to Transformers\nBefore Transformers revolutionized the field, AI models struggled with understanding long sentences in much the same way a person with severe short-term memory loss might struggle to follow a complex conversation. Recurrent Neural Networks (RNNs) processed text word-by-word, like a person reading a long scroll with a narrow window that only revealed one word at a time. This sequential process created a fundamental bottleneck; by the time the model reached the end of a paragraph, it had often forgotten what was said at the beginning, losing crucial context that might completely change the meaning of what it was reading.\nThe breakthrough came with the 2017 paper titled “Attention Is All You Need,” a deceptively simple title that announced one of the most significant advances in AI history. The authors introduced the Transformer and, with it, a completely new way to process language that would eventually power everything from Google’s search results to ChatGPT’s conversations.\nThe core innovation of the Transformer is the attention mechanism, and to understand it, imagine you’re a detective trying to solve a complex case. You have hundreds of witness statements, documents, and pieces of evidence scattered across your desk. Traditional methods would force you to examine each piece of evidence in order, one by one, trying to remember how each relates to everything you’ve seen before. But what if instead, you could instantly see all the evidence at once and intuitively understand which pieces were most relevant to each other? What if you could automatically highlight the connections between a witness statement and a piece of physical evidence, or between two seemingly unrelated documents that actually tell the same story?\nThis is precisely how attention works in an LLM. For any given word or concept the model is focusing on (what we call a query), it simultaneously scans all other words in the text (the keys) to determine their relevance. It then calculates a weighted combination of the meanings of those words (the values) to produce a rich, contextualized understanding. It’s like having a hyper-intelligent librarian who, when you ask about “machine learning,” doesn’t just find books with that exact phrase, but intuitively understands which books about statistics, computer science, neuroscience, and even philosophy might be relevant to your query.\nMathematically, this process can be described elegantly. For a query \\(\\mathbf{q}\\) and a database of key-value pairs \\(\\mathcal{D} = \\{(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)\\}\\), the output is:\n\\[\\text{Attention}(\\mathbf{q}, \\mathcal{D}) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i\\]\nThe attention weights, \\(\\alpha(\\mathbf{q}, \\mathbf{k}_i)\\), determine how much focus to place on each value. These weights are calculated using a softmax function, ensuring they are normalized and sum to 1:\n\\[\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_j \\exp(a(\\mathbf{q}, \\mathbf{k}_j))}\\]\nThis elegant mechanism liberates the model from the tyranny of sequential processing. It can look at an entire sequence at once, effortlessly handling text of any length, drawing connections between distant words in a text, and processing information in parallel, making it incredibly efficient to train.\nThe concept of attention pooling can actually be traced back to classical kernel methods like Nadaraya-Watson regression, where similarity kernels determine how much weight to give to different data points. Consider how we might weight different pieces of information based on their similarity to what we’re looking for:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define some kernels for attention pooling\ndef gaussian(x):\n    return np.exp(-x**2 / 2)\n\ndef boxcar(x):\n    return np.abs(x) &lt; 1.0\n\ndef constant(x):\n    return 1.0 + 0 * x  # noqa: E741\n\ndef epanechikov(x):\n    return np.maximum(1 - np.abs(x), np.zeros_like(x))\n\nfig, axes = plt.subplots(1, 4, sharey=True, figsize=(12, 3))\n\nkernels = (gaussian, boxcar, constant, epanechikov)\nnames = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')\nx = np.arange(-2.5, 2.5, 0.1)\nfor kernel, name, ax in zip(kernels, names, axes):\n    ax.plot(x, kernel(x))\n    ax.set_xlabel(name)\n\nplt.show()\n\n\n\n\n\n\n\n\nEach of these kernels represents a different way of weighting information based on similarity or distance. In neural networks, this translates to learning how to attend to different parts of the input sequence, but with the flexibility to learn much more complex patterns than these simple mathematical functions.\nOne of the most ingenious aspects of the Transformer architecture is multi-head attention, which allows the model to attend to different types of relationships simultaneously. Think of it like having multiple experts in a room, each specialized in different aspects of language. One expert might focus on grammatical relationships between words, another on semantic meaning, another on emotional tone, and yet another on factual content. Instead of having just one attention mechanism, multi-head attention creates multiple parallel attention processes, each learning to capture different types of dependencies within the text.\nPerhaps even more remarkable is self-attention, where queries, keys, and values all come from the same sequence. This allows each position in the sequence to attend to all other positions, creating rich representations that capture the complex web of relationships within a single piece of text. When you read a sentence like “The trophy would not fit in the brown suitcase because it was too big,” self-attention helps the model understand that “it” most likely refers to the trophy, not the suitcase, by considering the relationships between all the words simultaneously.\nSince attention mechanisms are inherently permutation-invariant, Transformers require positional encoding to understand sequence order. This gives the model a sense of where each word appears in the sequence, enabling it to distinguish between “The cat sat on the mat” and “The mat sat on the cat.”",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#from-words-to-tokens-the-hidden-language-of-llms",
    "href": "25-llm.html#from-words-to-tokens-the-hidden-language-of-llms",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.4 From Words to Tokens: The Hidden Language of LLMs",
    "text": "2.4 From Words to Tokens: The Hidden Language of LLMs\nBefore any text can enter an LLM, it must first be converted into tokens through a process that’s more art than science. This tokenization step is crucial for understanding how LLMs actually process language, yet it’s often invisible to users who simply type words into a chat interface.\nModern LLMs don’t actually work with words as we understand them, but with subword units that balance efficiency with meaning. Consider how a sentence like “The Verbasizer helped Bowie create unexpected word combinations” might be tokenized. Rather than treating each word as a single unit, the tokenizer might split it into pieces like “The”, ” Ver”, “bas”, “izer”, ” helped”, ” Bow”, “ie”, ” create”, and so on. This approach allows the model to handle rare words and names by breaking them into more common subcomponents.\nThe choice of tokenization strategy has profound implications for model performance. A vocabulary that’s too small forces the model to represent complex concepts with many tokens, making it harder to capture meaning efficiently. Most modern LLMs use vocabularies of 50,000 to 100,000 tokens, carefully balanced to represent different languages while maintaining computational efficiency.\nThis tokenization process explains some of the quirks you might notice when working with LLMs. Names from science fiction, like “Tatooine” from Star Wars, might be split into unfamiliar pieces, making the model less reliable when discussing fictional universes. Similarly, code snippets, mathematical expressions, and specialized terminology can be tokenized in ways that fragment their meaning.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-architecture-that-changed-everything",
    "href": "25-llm.html#the-architecture-that-changed-everything",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.5 The Architecture That Changed Everything",
    "text": "2.5 The Architecture That Changed Everything\nThe complete Transformer architecture consists of two main components working in harmony. The encoder stack processes input sequences through multiple layers of multi-head self-attention and position-wise feed-forward networks, enhanced with residual connections and layer normalization that help stabilize training. The decoder stack uses masked multi-head self-attention to prevent the model from “cheating” by looking at future tokens during training.\nThis architecture enabled the development of three distinct families of models, each optimized for different types of tasks. Encoder-only models like BERT excel at understanding tasks such as classification, question answering, and sentiment analysis. They can see the entire input at once, making them particularly good at tasks where understanding context from both directions matters.\nDecoder-only models like GPT are particularly good at generation tasks, producing coherent, contextually appropriate text. These models are trained to predict the next token given all the previous tokens in a sequence, which might seem simple but turns out to be incredibly powerful for natural text generation.\nEncoder-decoder models like T5 bridge both worlds, excelling at sequence-to-sequence tasks like translation and summarization. The text-to-text approach treats all tasks as text generation problems. Need to translate from English to French? The model learns to generate French text given English input. Want to answer a question? The model generates an answer given a question and context.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-scale-revolution-how-bigger-became-better",
    "href": "25-llm.html#the-scale-revolution-how-bigger-became-better",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.6 The Scale Revolution: How Bigger Became Better",
    "text": "2.6 The Scale Revolution: How Bigger Became Better\nThe true revolution of large language models came from the discovery that these models exhibit remarkable scaling properties. Unlike many machine learning systems that hit performance plateaus as they grow larger, Transformers demonstrated that their performance scales as a predictable power law with three key factors: the number of model parameters, the amount of training data, and the computational resources used for training.\nThis scaling behavior has led to exponential growth in model sizes. GPT-1, released in 2018 with 117 million parameters, was already considered large for its time. GPT-2, with 1.5 billion parameters, was initially deemed too dangerous to release publicly. GPT-3’s 175 billion parameters represented a quantum leap. Today, we’re seeing models with hundreds of billions to trillions of parameters.\nBut size alone isn’t the only story. The way these models are trained has become increasingly sophisticated. Masked language modeling involves randomly masking tokens in the input and training the model to predict what’s missing. This approach enables bidirectional context understanding, allowing the model to see both what comes before and after a given word when making predictions.\nAutoregressive generation takes a different approach. These models are trained to predict the next token given all the previous tokens in a sequence. This forces the model to learn not just vocabulary and grammar, but also narrative structure, logical reasoning, and even elements of common sense.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#understanding-the-generation-process",
    "href": "25-llm.html#understanding-the-generation-process",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.7 Understanding the Generation Process",
    "text": "2.7 Understanding the Generation Process\nTo truly appreciate how LLMs work, we need to understand what happens under the hood during text generation. When you ask an LLM a question, the process begins with tokenization, converting your input text into numerical token IDs that the model can process. These tokens are then encoded through multiple transformer layers, each adding layers of understanding and context.\nThe model produces what are called logits—scores for each token in its vocabulary indicating how likely each token is to come next. The larger the score, the more appropriate the token is considered. For a prompt like “In the wastelands of mine,” the LLM might assign the largest logits to words like “echoes” and the lowest to “scissors,” reflecting the model’s understanding of context and appropriateness.\nThese logits are then converted into a probability distribution using a softmax function, ensuring all probabilities sum to 1. But here’s where it gets interesting: the model doesn’t just pick the most likely next token. That would lead to repetitive, boring text. Instead, various sampling strategies introduce controlled randomness to make the output more interesting and diverse.\nTemperature controls the randomness in this selection process. At low temperatures (close to 0), the model becomes more deterministic, almost always choosing the most likely next token. This produces focused, consistent output but can lead to repetitive text. Higher temperatures introduce more randomness, allowing for creative and surprising outputs but with the risk of producing incoherent content.\nTop-p sampling, also known as nucleus sampling, provides another way to control generation quality. Instead of considering the entire vocabulary, it selects from only the smallest set of tokens whose cumulative probability exceeds a threshold. This maintains diversity while avoiding the pitfalls of sampling from very unlikely tokens that might derail the generation.\nFrequency and presence penalties offer additional control by discouraging repetition. If you’ve ever noticed an LLM getting stuck repeating the same phrases, these penalties can help. Frequency penalties reduce the likelihood of tokens based on how often they’ve already appeared, while presence penalties discourage any token that has appeared before, regardless of frequency.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-challenge-of-creativity-and-diversity",
    "href": "25-llm.html#the-challenge-of-creativity-and-diversity",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.8 The Challenge of Creativity and Diversity",
    "text": "2.8 The Challenge of Creativity and Diversity\nOne fascinating aspect of LLMs is their tendency to fall into predictable patterns, especially when generating creative content. If you ask multiple models to create fantasy characters, you might notice they gravitate toward the same names and occupations. Characters named “Thrandil” or “Lyralei” with occupations like “Shadowcrafting” or “Dragon Taming” appear with surprising frequency, revealing the models’ learned biases from their training data.\nThis phenomenon occurs because LLMs learn patterns from their training data, and fantasy literature contains certain recurring tropes and naming conventions. When prompted to generate fantasy content, the model naturally gravitates toward these well-represented patterns, leading to less diversity than we might hope for.\nUnderstanding this limitation is crucial for practical applications. If you’re building a character generator or creative writing assistant, you need strategies to encourage more diverse outputs. This might involve using higher temperature settings, implementing diversity-promoting sampling strategies, or engineering prompts that explicitly request originality and cultural references from specific traditions.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#choosing-the-right-model-for-your-application",
    "href": "25-llm.html#choosing-the-right-model-for-your-application",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.9 Choosing the Right Model for Your Application",
    "text": "2.9 Choosing the Right Model for Your Application\nThe landscape of available LLMs is vast and constantly evolving, making model selection a complex decision. When choosing a model, you need to consider several factors that go beyond just picking the highest-performing option on a benchmark.\nSize tiers offer different trade-offs. Very small models (around 3 billion parameters or less) are fast and efficient, ideal for applications where resources are limited or real-time performance is crucial. These models can run on consumer hardware and often provide adequate performance for simpler tasks like basic text classification.\nMedium-sized models (7 to 30 billion parameters) often represent the sweet spot for many applications. They provide significantly better performance than smaller models while still being manageable in terms of computational requirements. Large models (30 billion parameters or more) provide the best performance and often demonstrate emergent capabilities that smaller models lack, but they require specialized hardware and can be expensive to run.\nBeyond general capability, you need to consider specialized features. Code generation models have been specifically trained on programming languages and software development tasks. They understand the unique challenges of code completion, including the need for “fill-in-the-middle” capabilities rather than just adding to the end of existing code. Multilingual models are designed to work across many languages simultaneously, while domain-specific models have been fine-tuned on specialized corpora.\nPractical constraints often override pure performance considerations. Computational resources, including GPU memory and inference speed, can be limiting factors. Cost considerations vary dramatically between using cloud APIs versus self-hosting models. Latency requirements might favor smaller, faster models over larger, more capable ones. Privacy concerns might necessitate on-premise deployment rather than cloud-based solutions.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#evaluating-model-performance",
    "href": "25-llm.html#evaluating-model-performance",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.10 Evaluating Model Performance",
    "text": "2.10 Evaluating Model Performance\nWhen evaluating models, researchers and practitioners rely on various benchmarks that test different aspects of language understanding and generation. The Massive Multitask Language Understanding (MMLU) benchmark tests knowledge across diverse academic subjects, from high school mathematics to philosophy. HellaSwag evaluates common sense reasoning by asking models to predict likely continuations of scenarios. HumanEval specifically tests code generation capabilities.\nHowever, benchmarks have limitations and don’t always reflect real-world performance. A model that excels at multiple-choice questions might struggle with open-ended creative tasks. Code generation benchmarks might not capture the nuanced requirements of your specific programming domain. The key is to use benchmarks as a starting point while conducting thorough validation using data that closely resembles your actual use case.\nConsider implementing your own evaluation framework that tests the specific capabilities you need. If you’re building a customer service chatbot, create test scenarios that reflect your actual customer interactions. If you’re developing a creative writing assistant, evaluate the model’s ability to generate diverse, engaging content in your target style or genre.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#when-things-go-wrong-understanding-llm-limitations",
    "href": "25-llm.html#when-things-go-wrong-understanding-llm-limitations",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.11 When Things Go Wrong: Understanding LLM Limitations",
    "text": "2.11 When Things Go Wrong: Understanding LLM Limitations\nDespite their impressive capabilities, LLMs face several fundamental challenges that become apparent in practical applications. The most widely discussed is the tendency to “hallucinate”—generating confident-sounding but factually incorrect information. This happens because LLMs are fundamentally trained to generate plausible-sounding text, not necessarily true text.\nWhen an LLM encounters a question about a topic it hasn’t seen much during training, it doesn’t simply say “I don’t know.” Instead, it generates text that follows the patterns it has learned, which can result in convincing-sounding but completely fabricated facts, dates, or citations. This limitation is particularly problematic in applications where accuracy is critical.\nBias represents another significant challenge. LLMs can exhibit various biases present in their training data, from subtle gender stereotypes to more overt cultural prejudices. Since these models learn from text produced by humans, they inevitably absorb human biases, sometimes amplifying them in unexpected ways.\nSecurity concerns have emerged as LLMs become more capable. “Jailbreaking” refers to techniques that manipulate models into generating content that violates their safety guidelines. Clever prompt engineering can sometimes bypass safety measures, leading models to provide harmful instructions or exhibit problematic behaviors they were designed to avoid.\nUnderstanding these limitations is crucial for responsible deployment. You need to implement appropriate guardrails, fact-checking mechanisms, and human oversight, especially in high-stakes applications. The goal isn’t to avoid these limitations entirely—that’s currently impossible—but to understand them and design your systems accordingly.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#building-practical-applications",
    "href": "25-llm.html#building-practical-applications",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.12 Building Practical Applications",
    "text": "2.12 Building Practical Applications\nModern applications of LLMs extend far beyond simple text generation into sophisticated systems that augment human capabilities. Conversational AI has evolved from simple rule-based chatbots to sophisticated systems capable of maintaining context across long conversations, understanding nuanced requests, and even developing distinct personalities.\nWhen building conversational systems, memory management becomes crucial. LLMs have limited context windows—typically measured in thousands of tokens—so you need strategies for maintaining relevant conversation history while staying within these limits. This might involve summarizing older parts of the conversation, selectively keeping important information, or implementing external memory systems.\nIn content creation applications, LLMs serve as writing assistants that help with everything from grammar and style suggestions to structural improvements and creative ideation. Code generation has become particularly sophisticated, with models capable of writing complete functions, debugging existing code, and generating documentation. These tools work best when they augment rather than replace human expertise.\nAnalysis and understanding applications leverage LLMs’ ability to process and synthesize large amounts of text. Document summarization systems can extract key points from lengthy reports. Sentiment analysis applications help businesses understand customer feedback at scale. Information extraction systems can identify entities, relationships, and key facts from unstructured text.\nAdvanced techniques like prompt engineering have emerged as crucial skills for effectively using LLMs. This involves crafting instructions that guide the model toward desired outputs, often requiring deep understanding of how different phrasings and structures affect model behavior. Few-shot learning allows you to teach models new tasks by providing just a few examples, while chain-of-thought prompting encourages models to break down complex reasoning into step-by-step processes.\nRetrieval-augmented generation represents a particularly promising approach that combines LLMs with external knowledge bases. Instead of relying solely on knowledge encoded in model parameters during training, these systems can dynamically retrieve relevant information from databases, documents, or the internet to inform their responses. This approach helps address the hallucination problem while keeping models up-to-date with current information.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#creative-collaboration-when-artists-meet-algorithms",
    "href": "25-llm.html#creative-collaboration-when-artists-meet-algorithms",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.13 Creative Collaboration: When Artists Meet Algorithms",
    "text": "2.13 Creative Collaboration: When Artists Meet Algorithms\nThe intersection of AI and creativity offers fascinating insights into how these technologies might augment rather than replace human creativity. David Bowie’s experimentation with the “Verbasizer” in the 1990s provides a prescient example of human-AI collaboration in creative work. Bowie created an algorithmic text generator that would help overcome writer’s block by randomly recombining words and phrases from existing text.\nBowie described how this process resulted in a “kaleidoscope of meanings,” with words and ideas colliding in surprising ways. The system would take phrases like “I am a blackstar” and randomly combine them to create new variations that sparked his creative process. The randomness of the algorithm would often produce surprising results that led him in new creative directions, breaking him out of creative ruts and helping him discover unexpected word combinations.\nThis collaborative approach to AI-assisted creativity has become increasingly common in modern creative industries. Musicians use AI tools for melody generation and lyric writing assistance. Writers employ LLMs for brainstorming, overcoming writer’s block, and exploring alternative narrative directions. Visual artists use AI for concept generation and style exploration.\nThe key insight from Bowie’s work—that AI can serve as a creative collaborator rather than just an automation tool—remains relevant as these technologies become more sophisticated. The most successful creative applications of LLMs seem to be those that enhance human creativity rather than attempting to replace it entirely.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#looking-forward-the-evolving-landscape",
    "href": "25-llm.html#looking-forward-the-evolving-landscape",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.14 Looking Forward: The Evolving Landscape",
    "text": "2.14 Looking Forward: The Evolving Landscape\nThe field of large language models continues to evolve at a breathtaking pace. Architectural innovations like Mixture of Experts (MoE) models allow for scaling model capacity without proportionally increasing computational requirements. Multimodal transformers are beginning to bridge the gap between text, images, audio, and other modalities, creating systems that can understand and generate content across multiple forms of media.\nTraining efficiency has become a crucial area of research as models grow ever larger. Parameter-efficient fine-tuning techniques allow practitioners to adapt large models to specific tasks without retraining all parameters. Knowledge distillation enables the creation of smaller, faster models that retain much of the capability of their larger teachers.\nSafety and alignment research has become increasingly important as these models become more capable and widely deployed. Constitutional AI approaches attempt to instill models with explicit principles and values. Human feedback training uses human preferences to fine-tune model behavior, helping ensure that models are helpful, harmless, and honest.\nThe applications continue to expand into new domains. In scientific discovery, LLMs are being used to generate hypotheses, analyze literature, and suggest experimental designs. Educational applications range from personalized tutoring systems to tools that help teachers create customized learning materials. The creative industries are being transformed as artists, writers, and designers incorporate AI tools into their workflows as collaborators that enhance and accelerate the creative process.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "25-llm.html#the-future-of-human-ai-partnership",
    "href": "25-llm.html#the-future-of-human-ai-partnership",
    "title": "2  Large Language Models: A Revolution in AI",
    "section": "2.15 The Future of Human-AI Partnership",
    "text": "2.15 The Future of Human-AI Partnership\nLarge Language Models represent more than just a technological achievement; they represent a fundamental shift in how we interact with information and computational systems. These models have learned to speak our language, literally and figuratively, opening up possibilities for human-computer interaction that feel more natural and intuitive than anything that came before.\nAs we continue to push the boundaries of what’s possible with these systems, we’re not just building better tools—we’re exploring what it means for machines to understand and generate human language. The journey from attention mechanisms to trillion-parameter models has been remarkable, but it’s clear that we’re still in the early stages of this technological revolution.\nThe future of AI is not just about making machines smarter—it’s about making the partnership between human and artificial intelligence more powerful, more creative, and more beneficial for society. As these models become more capable, more efficient, and more widely accessible, they promise to transform virtually every aspect of how we work, learn, create, and communicate.\nUnderstanding how these systems work, their capabilities and limitations, and how to use them effectively will become increasingly important skills. Whether you’re a developer building LLM-powered applications, a researcher pushing the boundaries of what’s possible, or simply someone trying to understand this rapidly evolving field, the key is to approach these tools with both enthusiasm for their potential and awareness of their current limitations.\nThe revolution in AI that began with the simple idea that “attention is all you need” has only just begun. As we continue to explore the possibilities and address the challenges, we’re not just witnessing the evolution of artificial intelligence—we’re participating in the redefinition of what it means to think, create, and communicate in the digital age.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Large Language Models: A Revolution in AI</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n“Efficient Estimation of Word\nRepresentations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "References"
    ]
  }
]