[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications, drawing from our experience teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present complex concepts in a way that is accessible to data scientists, business professionals, and technical experts alike. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nThe techniques discussed in this book are transformative and have a profound impact on automation. From self-driving cars to virtual assistants, they are already woven into daily life and will soon become even more pervasive across industries. Understanding them is essential for anyone who wants to stay ahead of the curve.\nAI’s ability to learn, adapt, and make decisions accelerates automation across industries. By analyzing vast amounts of data, AI algorithms identify patterns and trends that support informed decision-making, leading to better resource allocation, optimized processes, and improved outcomes. For example, AI-powered chatbots handle customer inquiries around the clock, offering personalized, efficient support that boosts satisfaction and loyalty. AI also creates entirely new business models, disrupting traditional markets and unlocking opportunities for innovation and growth. In addition, it is driving progress in areas such as personalized medicine and space exploration, with the potential to revolutionize these fields and improve our lives.\nThe term AI has morphed over time. Coined in 1956 by John McCarthy, it was first defined as “the science and engineering of making intelligent machines.” The field has since evolved, and so has the definition. Today, AI is a broad field encompassing various subfields, including machine learning, deep learning, and natural language processing. These terms are often used interchangeably, but they are not synonymous. Machine learning is a subfield of AI that focuses on algorithms that learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, artificial intelligence has undergone significant transformations. Traditional AI focused on rule-based systems and Boolean logic with limited learning capabilities, which made them brittle in changing environments. In contrast, emerging AI is centered on modeling uncertainty, pattern matching, and deep learning—all data-driven approaches. These methods are more adaptable to complex, unstructured data, but they are also more data-dependent and can lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nOne example of the old AI vs the new AI comes from space exploration. As was famously said, the Kalman filter took us to the moon, and the deep learning took us back. The Appolo 11 code was written in 1969 and used the Kalman filter to navigate the lunar lander to the moon. The modern backwards landing of the rockets were enabled by deep learning. This re-use of rockets changed the economics of space exploration and made it more affordable.\nThis book is based on lecture notes from our courses, refined and expanded over years of teaching. We have incorporated valuable feedback from students at both the University of Chicago and George Mason University to create a comprehensive and engaging learning experience. The book is organized into three parts:\n\nPart 1: Bayesian Learning: This section covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: It explores the core concepts of AI and focuses on pattern-matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: It delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in AI and machine learning. We acknowledge the foundational work of pioneers such as Claude Shannon (information theory), John von Neumann (game theory and decision science), and Richard Bellman (dynamic programming and optimal control).\nThe evolution of AI can be summarized in three stages:\n\nSearch. Early search engines answered a single question with a ranked list of webpages. The PageRank algorithm, developed by Larry Page and Sergey Brin, used power iterations to rank these pages by relevance. Statistical tools like Kendall’s tau and Spearman’s rank correlation measured the similarity between the ranking and actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and that of others, easing the burden of choice.\nSummaries. Current AI systems like ChatGPT and Perplexity excel at summarization and generalization. These large language models distill vast amounts of complex information into clear, coherent summaries that capture the essential points. They can generalize across different domains, connecting concepts and providing insights that might not be immediately obvious. This ability represents a significant leap from simple search and recommendation, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in the most useful ways.\n\nInitially, the interaction was a single question leading to a single answer. Then came suggestions, where the system anticipated user needs. Now, we have summaries, where AI agents interpret a request, formulate a plan, and execute it. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe worlds of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nFinance: Financial market modeling, risk management, and investment analysis.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "The Modern AI Playbook",
    "section": "",
    "text": "AIQ, AGI and ASI\nWhen you open an Amazon page, there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and others who have bought similar products, Amazon uses AI and machine learning to predict what might interest you the next time you shop. When you apply for a loan online, you typically get an immediate answer after filling out an application. The information you provide, combined with your credit history pulled from a credit bureau, is used by a predictive model to determine with high confidence whether you are likely to default on the loan.\nWhat do Amazon, the finance industry, and a championship-winning baseball franchise have in common? They all use AI-driven methods to improve operations. Ability to automate tasks allows to scale operations and perform tasks that are too complex for humans to perform. AI-driven methods are becoming increasingly important in all industries.\nThe strong demand for AI talent has translated into substantial compensation premiums for students entering the field. According to the 2025 AI Talent Salary Report, AI professionals command a median salary of $160,000 annually, with a 28% premium over traditional tech roles. Entry-level AI engineers earn between $70,000 and $120,000, with specialized roles like computer vision engineers starting at $140,043. Importantly for students, specialization matters significantly: Large Language Model (LLM) engineers earn 25-40% more than general ML engineers, while AI Safety and Alignment specialists have seen 45% salary increases since 2023. The report emphasizes that companies hiring AI talent early pay 15-20% less than those waiting, suggesting that new graduates who enter the market quickly may benefit from competitive starting salaries as organizations rush to build AI capabilities.\nWhat are the key ingredients of AI-driven methods? At the heart of modern AI lie three fundamental pillars: deep learning architectures that extract hierarchical patterns from raw data by composing simple operations across many layers, Bayesian statistical frameworks that quantify uncertainty and update beliefs as evidence arrives, and high-performance computing infrastructure—particularly GPUs originally designed for gaming—that makes processing billion-parameter models and massive data sets feasible. Deep learning provides the representational power to capture complex relationships in high-dimensional data, enabling language models to represent subtle nuances of syntax and semantics while image recognition systems distinguish thousands of object categories by learning features from pixels to edges to shapes to objects. Bayesian methods complement this by providing a principled framework for reasoning under uncertainty, telling us how to interpret probabilistic predictions, incorporate prior knowledge, and update beliefs with new data—capabilities essential in scientific applications from drug discovery to climate modeling where quantifying uncertainty is as important as making predictions. The interplay between these three pillars creates a virtuous cycle: as computing power grows, we train larger models that capture increasingly sophisticated patterns while Bayesian frameworks quantify what they know and don’t know. Let’s start by trying to understand how deep learning fits within the broader landscape of data science, machine learning, and artificial intelligence.\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that extract patterns from data sets. The algorithms are adaptations of applied mathematics techniques to specific computer architectures, and the software implements those algorithms.\nMachine learning (ML) applies AI models to design predictive rules for forecasting and what-if analysis. For example, companies use predictive models to optimize marketing budgets or forecast shipping demand. A machine-learning system is trained rather than explicitly programmed—it’s presented with many examples relevant to a task and finds statistical structure that allows it to automate that task. For instance, to automate tagging vacation pictures, you would present the system with examples of tagged photos, and it would learn rules for associating pictures with tags. Tools from statistics serve as a basis for many machine learning algorithms.\nArtificial Intelligence has been around for decades. The term AI was coined by the famous computer scientist John McCarthy in 1955. While initially connected to robotics, currently, AI is understood as a set of mathematical tools used to develop algorithms that perform tasks typically done by humans, such as driving a car or scheduling a doctor’s appointment. This set of mathematical tools includes probabilistic models, machine learning algorithms, and deep learning.\nDeep learning is a type of machine learning that performs a sequence of transformations (filters) on data. The output of each of those filters is called a factor in traditional statistical language and a hidden feature in machine learning. The word “deep” means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nA major difference between modern and historical AI algorithms is that most recent AI approaches rely on learning patterns from data. For example, IBM’s Deep Blue, the chess-playing computer that famously defeated world champion Garry Kasparov in 1997, was “hardcoded” with explicit rules and strategies designed by chess experts and IBM engineers. Deep Blue evaluated millions of chess positions per second using carefully programmed evaluation functions that assigned scores to board positions based on material advantage, piece positioning, king safety, and other strategic factors. These evaluation criteria were implemented as if-then statements and heuristics crafted by human experts rather than learned from data. On the other hand, the modern AlphaGo Zero algorithm did not use any human inputs and learned optimal strategies to play chess and other board games from large data sets generated from self-plays. Although handcrafted systems like Deep Blue performed well in some tasks, such as chess playing, they are hard to design for many complex applications, such as self-driving cars. Large data sets allow us to replace sets of rules designed by engineers with sets of rules learned automatically from data. Thus, learning algorithms, such as deep learning, are at the core of most modern AI systems.\nThe key concept behind many modern AI systems is pattern-recognition. A “pattern” is a prediction rule that maps an input to an expected output, and “learning a pattern” means fitting a good prediction rule to a data set. In AI, prediction rules are often referred to as “models.” The process of using data to find a good prediction rule is often called “training the model.” Mathematically, we can express this as learning a function \\(f\\) that maps inputs \\(x\\) to outputs \\(y\\), so that \\(y = f(x)\\). For instance, in large language model, \\(x\\) represents a question and \\(y\\) represents the answer. In a chess game, \\(x\\) represents the board position and \\(y\\) represents the best move. The learning process involves finding the function \\(f\\) that best captures the relationship between inputs and outputs by examining many examples of input-output pairs in a training dataset. Deep learning excels at discovering complex, nonlinear functions \\(f\\) when the relationship between \\(x\\) and \\(y\\) is too intricate to specify manually—such as the mapping from raw pixel values to semantic image content, or from question and answer to text.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#aiq-agi-and-asi",
    "href": "00-intro.html#aiq-agi-and-asi",
    "title": "The Modern AI Playbook",
    "section": "",
    "text": "“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\nThree Visions of Automation: Wiener, Keynes, and Veblen\nAs artificial intelligence transforms industries and redefines work, it is instructive to revisit three influential twentieth-century perspectives on technology, labor, and human flourishing. Norbert Wiener, John Maynard Keynes, and Thorstein Veblen each offered distinct visions of how automation would reshape society—visions that remain strikingly relevant as we navigate today’s AI revolution.\n\nWiener’s Warning: Automation and Human Dignity\nNorbert Wiener, the mathematician who founded cybernetics and whose work laid foundations for modern control systems and artificial intelligence, was among the first to recognize both the power and peril of automation. Writing in The Human Use of Human Beings (1950), Wiener articulated a prescient warning about the social consequences of replacing human labor with machines:\n\nIf we combine our machine potentials of a factory with the valuation of human beings on which our present factory system is based, we are in for an Industrial Revolution of unmitigated cruelty. We must be willing to deal in facts rather than fashionable ideologies if we wish to get through this period unharmed.\n\nWiener’s concern was fundamentally about human identity and dignity. If workers derive their sense of worth from their role as factory laborers, and automation eliminates those roles, what becomes of their identity? This was not merely an economic question about displaced workers finding new employment—it was a deeper psychological and existential challenge. Wiener recognized that the transition to an automated economy would require not just retraining programs, but a fundamental reimagining of how humans find meaning and value in a world where machines perform an ever-expanding range of tasks.\nCrucially, Wiener rejected the notion that automation would deliver humanity into a comfortable retirement. Instead, he argued:\n\nThe world of the future will be an even more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.\n\nThis vision stands in stark contrast to utopian fantasies of automated abundance. Wiener foresaw that as machines took over routine cognitive and physical tasks, the remaining challenges would become more abstract, more complex, and more demanding of human creativity and judgment. The age of AI would not eliminate work—it would transform it into work that pushes against the very boundaries of human capability. This perspective resonates powerfully with today’s experience: while AI systems can now draft legal documents, generate code, and analyze medical images, the highest-value human contributions increasingly involve strategic thinking, ethical reasoning, and creative synthesis that remain beyond machine capabilities.\n\n\nKeynes’s Optimism: Solving the Economic Problem\nJohn Maynard Keynes, writing during the Great Depression in his 1930 essay Economic Possibilities for our Grandchildren (1930), offered a remarkably optimistic counterpoint. Keynes predicted that within a century—roughly by 2030—technological progress and capital accumulation would “solve the economic problem” for humanity. By this he meant that productivity gains would become so substantial that meeting humanity’s basic material needs would require only minimal labor. Keynes envisioned a future where people might work perhaps fifteen hours per week, devoting the remainder of their time to leisure, culture, and the pursuit of fulfilling activities.\nKeynes distinguished between “absolute needs”—those we feel regardless of others’ circumstances, such as food, shelter, and safety—and “relative needs”—our desire to feel superior to our fellows. He argued that while relative needs are insatiable, absolute needs could be satisfied through technological abundance. Once this occurred, humanity would face a new challenge: learning to live wisely with leisure. Keynes worried that without the structure and purpose provided by work, many people would struggle to find meaning. He wrote that humanity would need to cultivate the “art of life itself” and learn to value activities pursued for their own sake rather than for economic gain.\nKeynes’s prediction that technology would dramatically increase productivity proved remarkably accurate. However, his assumption that increased productivity would translate into reduced working hours has not materialized as he expected. Rather than collectively choosing leisure, advanced economies have channeled productivity gains into increased consumption, higher living standards, and the expansion of service industries. The phenomenon of “Veblenian” conspicuous consumption—our next topic—helps explain why.\n\n\nVeblen’s Critique: The Leisure Class and Conspicuous Consumption\nThorstein Veblen, writing even earlier in The Theory of the Leisure Class (1899), offered a more cynical analysis of how elites use both leisure and consumption to signal status. Veblen introduced the concept of conspicuous consumption—the purchase of goods and services primarily to display wealth and social status rather than to satisfy genuine needs. The leisure class, in Veblen’s analysis, derives its social standing not from productive labor but from the ostentatious display of time and resources devoted to non-productive activities.\nVeblen’s insight reveals why Keynes’s vision of universal leisure has not materialized. In modern economies, work serves not only to produce income for consumption but also to confer identity, status, and social belonging. High-status professionals often work longer hours than necessary for material sustenance precisely because their work signals competence, dedication, and membership in elite circles. The “leisure” time that technology has created has often been filled not with Keynesian cultivation of the art of life, but with Veblenian status competitions—from luxury travel photographed for social media to the accumulation of credentials through continuous education.\nMoreover, as AI automates routine tasks, the remaining human work increasingly involves activities that are themselves forms of status display: strategic decision-making, creative innovation, and high-stakes problem-solving. These activities signal membership in cognitive elites in ways that parallel Veblen’s leisure class. The AI era has not eliminated status competition through work—it has transformed the nature of the work that confers status.\n\n\nSynthesis: Navigating the AI Transition\nThese three perspectives collectively illuminate the complex challenge we face as artificial intelligence reshapes the economy. Wiener reminds us that automation poses existential questions about human purpose and dignity that cannot be solved through economic policy alone. The psychological and social dimensions of work mean that job displacement creates crises of identity as much as crises of income. As AI systems take on tasks previously reserved for highly educated professionals—from legal research to medical diagnosis to software engineering—even cognitive elites must grapple with Wiener’s question: what is the human use of human beings in an age of intelligent machines?\nKeynes’s optimism about technological abundance has proven partially correct—we have achieved levels of material prosperity that would have seemed utopian to earlier generations. Yet his assumption that abundance would automatically translate into leisure and cultural flourishing has not materialized. This is partly explained by Veblen’s observation that human desires are fundamentally social and positional. We work not only to consume but to signal our worth and to participate in status hierarchies. AI productivity gains are thus channeled not into reduced working hours but into ever-more-sophisticated forms of consumption and status competition.\nThe synthesis of these perspectives suggests that successfully navigating the AI transition requires more than technical solutions or economic policies. It requires cultivating new sources of meaning, identity, and social connection that are not solely dependent on traditional employment. It requires resisting purely Veblenian status competitions in favor of Keynesian cultivation of intrinsically valuable activities. And it requires heeding Wiener’s warning that the future will demand more, not less, of our intelligence, creativity, and ethical judgment—even as machines handle an expanding range of routine tasks.\nAs we explore the technical foundations and practical applications of AI throughout this book, these broader questions about human flourishing in an automated age provide essential context. The algorithms, models, and systems we study are not merely technical artifacts—they are forces reshaping the fundamental structure of work, consumption, and social organization. Understanding their implications requires both technical sophistication and humanistic wisdom.\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations, and markets interact in complex ways. AI facilitates organization and hence connects people to markets faster and more simply. Hence it creates economic value. Most of the recessions in the 19th century were a result of not being able to get goods to markets quickly enough, which led to banking crises. AI accelerates speed to market. It creates growth. The age of abundance is here.\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer. He compares it to the previous generations of software development approaches summarized in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrently, LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nN. G. Polson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He discusses systems that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#large-language-models-llms",
    "href": "00-intro.html#large-language-models-llms",
    "title": "The Modern AI Playbook",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\n\n\n\nSource: https://johnnosta.medium.com/the-most-important-chart-in-100-years-1095915e1605\n\n\nThe algorithmic aspects of deep learning (the main building block of an AI system) have existed for decades. In 1956, Kolmogorov showed that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951, Robbins and Monro proposed stochastic approximation algorithms. Combined with regularization methods developed by Andrey Tikhonov in 1940s and 1950s, the stochastic approximation is the main technique for finding weights of a deep learning model today. The backpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In the mid-1980s, Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL have been around for several decades, one could wonder why we observe a recent peak in popularity of those methods. Modern AI models are enabled by the old math and the new powerful GPU chips, which provide the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix multiplications simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and prediction. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing. Memory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\n\n\n\nTable 1: Evolution of Nvidia’s GPU performance\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nArchitecture\nFP32 Peak (TeraFLOPS)\nFP8/FP4 Peak (Peta/ExaFLOPS)\nMemory (per GPU)\n\n\n\n\n2006\nGeForce 7900 GTX\n0.23\n\n0.5GB GDDR3\n\n\n2016\nGeForce GTX 1080\n8.9\n\n8GB GDDR5X\n\n\n2024\nRTX 4070 SUPER\n~32\n\n12GB GDDR6X\n\n\n2024\nBlackwell B200\n~45 (FP64)\n20 PFLOPS (FP4) / 1.4 ExaFLOPS (AI cluster)\n288GB HBM3e\n\n\n2026\nRubin VR300\n\n50 PFLOPS (FP4) / 1.2 ExaFLOPS (FP8, rack)\n288GB HBM4\n\n\n2027\nRubin Ultra\n\n100 PFLOPS (FP4) / 5 ExaFLOPS (FP8, rack)\n1TB HBM4e (per 4 dies)\n\n\n\n\n\n\nNow AI models are the main consumers of those processors. The most popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\n\n\n\n\n\n\nFigure 1: Compute requirements scale exponentially across AI tasks. Source: Michael Dell\n\n\n\nThe computational demands of AI tasks scale exponentially, as illustrated in Figure 1: while a single-shot chatbot represents the baseline (1x), image generation requires ~10x more compute, reasoning tasks need ~100x, video generation demands ~3,000x, and deep research capabilities require over 1,000,000x the baseline. This exponential scaling, based on estimates from OpenAI, Nvidia, and Melius Research, explains why AI companies face persistent capacity constraints despite massive infrastructure investments. As users demand more sophisticated capabilities, the infrastructure requirements grow super-linearly, making access to computational resources an increasingly critical competitive advantage.\nHowever, instead of measuring computing requirements using traditional floating-point arithmetic (FLOPS), the commercial LLM models use token as a unit of computation. A token is a unit of text that is processed by the model. It is typically a word or a phrase. The number of tokens is the number of words or phrases that are processed by the model as inputs or outputs. Two-tree years ago, when you had to adopt an LLM model for a specific tasks, you had to perform fine-tuning of the model. It involved updating the parameters of the model to make it more suitable for your specific task. However, now you will find spend less computing power on fine-tuning and more on using the model as-is and simply providing the tasks-specific instructions and data sets as model context.\nWhile GPUs have driven the current AI revolution through their parallel processing capabilities, the quest for quantum supremacy—the point where a quantum computer definitively outperforms any classical counterpart on a specific task—represents the next frontier in computational power. This is the practical realization of ideas stretching back decades. As Feynman’s seminal paper first discussed, the principles of quantum physics suggest a new foundation for computation, and the key to harnessing this is von Neumann’s principle of quantum measurement, where the state of a physical system is used to measure quantities of interest. This measurement concept is central to the work by N. Polson, Sokolov, and Xu (2023) on quantum Bayesian computation, which explores how to create quantum algorithms that promise an exponential speed-up for the very Bayesian and black-box neural network methods pioneered by statisticians like Smith and Breiman. Thus, while the quantum mechanics underlying the construction of a stable quantum computer is still in its infancy, the algorithms that will grant it supremacy are already being designed, aiming to leverage these fundamental quantum principles to accelerate data science far beyond the capabilities of even the most advanced GPU clusters.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaude is the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\n\n\n\nClaude\n\n\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to use graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in the 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to the popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful of scientists and engineers, most of whom are trained in computer science. However, modern methodologies, software, and the availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range of problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra, and high-performance computing. Although deep learning models are very interesting to study from a methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. The ability to learn very complex patterns in data and generate accurate predictions makes deep learning a useful and exciting methodology to use. We hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics, and linear algebra will be revisited, this book is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems. There are many aspects of data analysis that do not deal with building predictive models, for example, data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017). Those are not covered in this book.\n\nEnterprise LLM Applications\nLarge language models are rapidly transforming business operations across industries, creating measurable economic value through automation, enhanced decision-making, and new product capabilities. In customer service, companies report 30-50% reductions in support costs by deploying LLM-powered chatbots that handle routine inquiries while escalating complex issues to human agents. Financial institutions use LLMs for document analysis, extracting key information from contracts, regulatory filings, and legal documents at speeds and accuracy levels that far exceed manual review. Software development has been revolutionized by AI coding assistants like GitHub Copilot, with studies showing developers completing tasks 55% faster when using AI assistance. In healthcare, LLMs assist with clinical documentation, reducing physician administrative burden by automatically generating visit summaries and insurance prior authorization requests. Marketing and content creation industries leverage LLMs for generating product descriptions, email campaigns, and social media content, with some companies reporting 10x increases in content production capacity. Legal firms deploy LLMs for contract review, due diligence, and legal research, compressing weeks of associate work into hours. The pharmaceutical industry uses LLMs to analyze scientific literature and accelerate drug discovery by identifying promising compounds and predicting molecular interactions. Consulting firms employ LLMs to synthesize market research, generate client deliverables, and perform competitive analysis. Education platforms integrate LLMs for personalized tutoring, automated grading, and adaptive learning systems that adjust to individual student needs. Collectively, these applications represent billions of dollars in productivity gains and are fundamentally reshaping knowledge work across sectors, making AI literacy increasingly essential for business professionals and creating strong demand for practitioners who can effectively deploy these technologies.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#bayes-evidence-as-minus-log-probability",
    "href": "00-intro.html#bayes-evidence-as-minus-log-probability",
    "title": "The Modern AI Playbook",
    "section": "Bayes: Evidence as Minus Log-Probability",
    "text": "Bayes: Evidence as Minus Log-Probability\nImagine you’re searching for something lost—a missing ship, a hidden treasure, or a city abandoned centuries ago. You have multiple clues: historical documents, geological surveys, satellite imagery, and expert opinions. How do you combine all these disparate pieces of evidence into a coherent search strategy? This is exactly the type of problem where Bayesian reasoning shines, and it’s a powerful framework that underlies many modern AI applications.\nThe Bayesian approach provides a principled mathematical framework for updating our beliefs as new evidence arrives. At its core is Bayes’ rule, which tells us how to revise the probability of a hypothesis given new data:\n\\[\nP(\\text{hypothesis} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{hypothesis}) \\times P(\\text{hypothesis})}{P(\\text{data})}\n\\]\nWhile this formula is elegant, what makes Bayesian reasoning especially powerful is a simple mathematical trick: when we work with logarithms of probabilities, combining evidence becomes as simple as addition. Taking the logarithm of both sides of Bayes’ rule gives us:\n\\[\n\\log P(\\text{hypothesis} \\mid \\text{data}) = \\log P(\\text{data} \\mid \\text{hypothesis}) + \\log P(\\text{hypothesis}) - \\log P(\\text{data})\n\\]\nThis transformation reveals that the log-posterior (our updated belief) is simply the sum of the log-likelihood (evidence from data) and the log-prior (our initial belief), minus a normalization constant. In other words, on the log scale, we’re just adding up different sources of evidence. Each piece of information contributes its “weight” to the total, and we combine them linearly.\nThis additive property has profound practical implications. When you have multiple independent sources of evidence—say, historical documents, geological surveys, and geophysical measurements—each contributes a term to the sum. Strong evidence adds a large positive contribution, weak evidence adds little, and contradictory evidence subtracts from the total. The beauty is that the mathematical framework handles all the bookkeeping automatically.\nA remarkable application of this principle comes from the world of mineral exploration. In 2022, Aurania Resources announced that they had found the location of Logroño de los Caballeros, a “lost city” of Spanish gold miners that had been abandoned in the jungles of Ecuador for over 400 years. The discovery was made possible by Bayesian search theory, developed by Larry Stone who has a remarkable track record of finding lost objects—including the USS Scorpion nuclear submarine and Air France Flight 447.\nLarry Stone’s approach to finding Logroño exemplifies how Bayesian reasoning combines multiple sources of evidence. The team assembled a mountain of heterogeneous information:\n\nHistorical documents: Spanish colonial records from the 1580s-1590s describing Logroño’s location relative to rivers and other settlements\nArchaeological evidence: A 1574 map by Mendez showing approximate locations\n\nGeological data: Stream sediment samples analyzed for gold content\nGeophysical surveys: Magnetic and radiometric measurements\nModern geography: LiDAR topographic data and current river systems\nGeochemical patterns: Distribution of minerals indicating potential gold sources\n\nEach of these information sources provided a “clue” that was more or less reliable, more or less precise, and potentially contradictory with others. How do you reconcile a 450-year-old account that “Logroño was half a league from the Rio Zamora” with geological evidence suggesting gold-bearing formations in a different area?\nBayesian search theory provides the answer. Bayesian reasoning assigns each piece of evidence a reliability weight and used Bayes’ rule to generate probability maps. Historical documents considered highly reliable (such as official Spanish reports) contributed strongly to the probability distribution, while more ambiguous sources contributed less. Larry Stone, explained: “Our success in integrating historical documents with scientific data using Bayesian methods opens a range of potential applications in the mineral and energy exploration sectors.”\nThe power of this approach became clear when they combined evidence that initially seemed contradictory. A critical breakthrough came from multiple corroborating accounts: Juan Lopez de Avendaño reported in 1588 that Logroño was half a league from the Rio Zamora; that same year, two soldiers drowned crossing “the river” to fight an uprising; in the mid-1590s, seven soldiers drowned trying to reach a downstream garrison; and a 1684 Jesuit account described an elderly woman who remembered hearing Logroño’s church bells from her village at the mouth of the Rio Zamora. Each piece of evidence individually was ambiguous—which river? how far is “half a league”?—but together they pointed to a specific location along the Rio Santiago valley.\nOn the log-probability scale, each piece of evidence either added to or subtracted from the likelihood of different locations. Strong, consistent evidence (multiple drowning accounts suggesting a major river crossing) added significant weight. Weak or contradictory evidence contributed less. The final probability map was literally the sum of these contributions, with the peak probability occurring where the most evidence converged. Figure 2 shows the likelihood ratio surfaces generated for copper, silver, and gold deposits—visual representations of how different evidence sources combine to create probability distributions across the search area.\n\n\n\n\n\n\nFigure 2: Likelihood ratio surfaces generated by Metron showing potential locations for copper, silver, and gold deposits in Aurania’s concession area. These heat maps visualize how Bayesian analysis combines multiple sources of geological and geophysical evidence into a single probability distribution. Warmer colors indicate higher likelihood ratios where multiple pieces of evidence converge. Source: Metron Inc., via MIT Sloan\n\n\n\nThe result was dramatic: Bayesian reasoning generated probability maps that identified the Rio Santiago valley as the most likely location of Logroño, and subsequent fieldwork confirmed extensive alluvial gold deposits and active artisanal mining exactly where the Bayesian analysis predicted. As Dr. Keith Barron, Aurania’s CEO, noted: “This key discovery can ultimately lead us to Logroño’s gold source.” The location that seemed to reconcile all the disparate evidence—Spanish colonial records, drowning accounts, geological surveys, and modern geography—turned out to be correct.\nThis example illustrates why the Bayesian framework is so powerful in modern AI applications. Machine learning models constantly face the challenge of combining multiple sources of information: pixels in different regions of an image, words in different parts of a sentence, measurements from different sensors. The additive property of log-probabilities provides an efficient computational framework for this fusion. When you train a deep learning model, the loss function essentially measures how well the model combines evidence from the training data with prior knowledge (encoded in the model architecture and regularization). Optimization algorithms adjust model parameters to maximize this combined evidence, updating beliefs exactly as Bayes’ rule prescribes.\nThe mathematical elegance of working with log-probabilities extends beyond search problems. In natural language processing, transformer models compute attention weights that determine how much “evidence” each word provides about the meaning of other words. In computer vision, convolutional networks combine evidence from different receptive fields. In recommendation systems, collaborative filtering combines evidence from multiple users’ preferences. All of these applications benefit from the additive structure that log-probabilities provide.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#examples-ai-in-action",
    "href": "00-intro.html#examples-ai-in-action",
    "title": "The Modern AI Playbook",
    "section": "Examples: AI in Action",
    "text": "Examples: AI in Action\nThe following examples illustrate the remarkable breadth and impact of modern AI applications across diverse domains. From analyzing street signs to predict vehicle steering commands, to discovering cardiovascular risks hidden in eye scans, these cases demonstrate how deep learning models can extract meaningful patterns from complex data that would be impossible for humans to process at scale. Each example showcases a different aspect of AI’s transformative potential: computer vision for autonomous navigation, medical diagnosis through pattern recognition, and creative synthesis that challenges our understanding of artistic expression. These real-world applications reveal how the mathematical foundations we’ll explore throughout this book translate into practical solutions that are reshaping industries and expanding the boundaries of what machines can accomplish.\n\nExample 1 (Updating Google Maps with Deep Learning and Street View.) Every day, Google Maps provides useful directions, real-time traffic information, and information on businesses to millions of people. To provide the best experience for users, this information must constantly mirror an ever-changing world. While Street View cars collect millions of images daily, it is impossible to manually analyze more than 80 billion high-resolution images collected to date to find new or updated information for Google Maps. One of the goals of Google’s Ground Truth team is to enable the automatic extraction of information from geo-located imagery to improve Google Maps.\n(Wojna et al. 2017) describes an approach to accurately read street names out of very challenging Street View images in many countries, automatically, using a deep neural network. The algorithm achieves 84.2% accuracy on the challenging French Street Name Signs (FSNS) dataset, significantly outperforming previous state-of-the-art systems. Further, the model was extended to extract business names from street fronts.\n\n\n\n\n\n\nFigure 3: Some examples of FSNS images.\n\n\n\nAnother piece of information that researchers were able to extract from street view is political leanings of a neighborhood based on the vehicles parked on its streets. Using computer algorithms that can see and learn, they have analyzed millions of publicly available images on Google Street View. The researchers say they can use that knowledge to determine the political leanings of a given neighborhood just by looking at the cars on the streets.\n\n\n\n\n\n\nFigure 4: Types of cars detected in StreetView Images can be used to predict voting patterns\n\n\n\n\n\nExample 2 (CNN for Self Driving Car) In 2004 a self-driving vehicle that participated in Darpa’s grand challenge drove 150 miles through the Mojave Desert without human intervention.\n\n\n\n\n\n\nFigure 5: Darpa Grand Challenge Flyer from 2004\n\n\n\nCurrent self-driving systems rely on convolutional neural networks (CNN). This is a particular neural network architecture that can be trained to map raw pixels from a single front-facing camera directly to steering commands(Bojarski et al. 2016). This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads.\nCompared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn’t automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps.\nAn NVIDIA DevBox and Torch 7 were used for training and an NVIDIA Drive PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 FPS.\n\n\n\n\n\n\nFigure 6: The architecture for self driving car\n\n\n\n\n\nExample 3 (Predicting Heart disease from eye images) Scientists from Google’s health-tech subsidiary Verily have discovered a new way to assess a person’s risk of heart disease using machine learning (Poplin et al. 2018). By analyzing scans of the back of a patient’s eye, the company’s software is able to accurately deduce data, including an individual’s age, blood pressure, and whether or not they smoke. This can then be used to predict their risk of suffering a major cardiac event — such as a heart attack — with roughly the same accuracy as current leading methods.\nTo train the algorithm, Verily’s scientists used machine learning to analyze a medical dataset of nearly 300,000 patients. This information included eye scans as well as general medical data. As with all deep learning analysis, neural networks were then used to mine this information for patterns, learning to associate telltale signs in the eye scans with the metrics needed to predict cardiovascular risk (e.g., age and blood pressure).\n\n\n\n\n\n\nFigure 7: Two images of the fundus, or interior rear of your eye. The one on the left is a regular image; the one on the right shows how Google’s algorithm picks out blood vessels (in green) to predict blood pressure. Photo by Google - Verily Life Sciences\n\n\n\nWhen presented with retinal images of two patients, one of whom suffered a cardiovascular event in the following five years, and one of whom did not, Google’s algorithm was able to tell which was which 70 percent of the time. This is only slightly worse than the commonly used SCORE method of predicting cardiovascular risk, which requires a blood test and makes correct predictions in the same test 72 percent of the time.\nA new Rembrandt painting unveiled in Amsterdam Tuesday has the tech world buzzing more than the art world. “The Next Rembrandt,” as it’s been dubbed, was the brainchild of Bas Korsten, creative director at the advertising firm J. Walter Thompson in Amsterdam.\nThe new portrait is the product of 18 months of analysis of 346 paintings and 150 gigabytes of digitally rendered graphics. Everything about the painting — from the subject matter (a Caucasian man between the age of 30 and 40) to his clothes (black, wide-brimmed hat, black shirt and white collar), facial hair (small mustache and goatee) and even the way his face is positioned (facing right) — was distilled from Rembrandt’s body of work.\n“A computer learned, with artificial intelligence, how to re-create a new Rembrandt right eye,” Korsten explains. “And we did that for all facial features, and after that, we assembled those facial features using the geometrical dimensions that Rembrandt used to use in his own work.” Can you guess which image was generated by the algorithm?\n\n\n\n\n\n\nFigure 8: Can you guess which image was generated by the algorithm?\n\n\n\n\n\nExample 4 (Learning Person Trajectory Representations for Team Activity Analysis) Activity analysis in which multiple people interact across a large space is challenging due to the interplay of individual actions and collective group dynamics. A recently proposed end-to-end approach (Mehrasa et al. 2017) allows for learning person trajectory representations for group activity analysis. The learned representations encode rich spatio-temporal dependencies and capture useful motion patterns for recognizing individual events, as well as characteristic group dynamics that can be used to identify groups from their trajectories alone. Deep learning was applied in the context of team sports, using the sets of events (e.g. pass, shot) and groups of people (teams). Analysis of events and team formations using NHL hockey and NBA basketball datasets demonstrate the generality of applicability of DL to sports analytics.\nWhen activities involve multiple people distributed in space, the relative trajectory patterns of different people can provide valuable cues for activity analysis. We learn rich trajectory representations that encode useful information for recognizing individual events as well as overall group dynamics in the context of team sports.\n\n\n\n\n\n\nFigure 9: Hockey trajectory analysis visualization\n\n\n\n\n\nExample 5 (EPL Liverpool Prediction) Liverpool FC has become a benchmark in football for integrating advanced data analytics into both their recruitment and on-field strategies. The Expected Possession Value (EPV) pitch map shown below displays the likelihood that possession from a given location will result in a goal, with red areas indicating high-value zones where Liverpool’s chances of scoring increase significantly when they gain or retain the ball. Liverpool’s analysts use these EPV maps to inform tactical decisions and player positioning, allowing the coaching staff to instruct players to press, pass, or move into these high-value zones.\n\n\n\n\n\n\nFigure 10: Expected Possession Value (EPV) pitch map\n\n\n\nOn April 26, 2019, Liverpool scored their fastest-ever Premier League goal—Naby Keita found the net just 15 seconds into the match against Huddersfield Town, setting the tone for a dominant 5-0 victory. This remarkable goal exemplified Liverpool’s data-driven approach to football. Keita’s immediate pressure on Huddersfield’s Jon Gorenc Stankovic was not random—it was a calculated move informed by analytics revealing Huddersfield’s vulnerability when building from the back under pressure. This demonstrates the effective application of Expected Possession Value (EPV) principles. Liverpool’s analysts systematically study opponent build-up patterns, using video and tracking data to predict where and how opponents are likely to play the ball from kick-off or in early possession phases. This intelligence allows Liverpool players to position themselves strategically for maximum disruption. When Huddersfield’s goalkeeper played out from the back, Keita was already moving to intercept, anticipating the pass route—a behavior that had been drilled through analytics-driven preparation and scenario planning.\n\n\nExample 6 (SailGP) SailGP is a global sailing championship that represents the pinnacle of competitive sailing, featuring identical F50 foiling catamarans raced by national teams. Unlike traditional sailing competitions, the series combines high-performance sailing with advanced analytics, real-time data processing, and 5G connectivity to create a modern, technology-driven sport where success depends as much on data analysis and optimization as traditional sailing skills. SailGP races take place in iconic venues around the world, with teams representing countries competing in a season-long championship format that emphasizes both athletic excellence and technological innovation.\n\n\n\nEmirates GBR SailGP Team F50 Race. Source:Wikimedia.\n\n\nOracle’s co-founder Larry Ellison is a pioneer and a significant investor in sailing analytics and technology, particularly through his involvement with Oracle Team USA and the America’s Cup. His team’s success in the 2010 and 2013 America’s Cup campaigns demonstrated how data analysis done in real-time could provide competitive advantages. His approach has influenced the development of modern sailing competitions like SailGP.\nJimmy Spithill is a veteran of eight consecutive America’s Cup campaigns. His most legendary achievement came in 2013 with ORACLE TEAM USA, where he led what is widely acclaimed as one of the greatest comebacks in sporting history. After falling behind 8-1 in the best-of-17 series against Emirates Team New Zealand, Spithill and his team faced seemingly insurmountable odds. No team had ever come back from such a deficit in America’s Cup history.\nThe turning point came after the 8-1 loss when the team made a critical technological decision. They installed additional sensors throughout the boat to collect more comprehensive data about performance, wind conditions, and boat dynamics. These sensors provided real-time feedback that allowed the team to make precise adjustments to their sailing strategy and boat configuration.\nWith the enhanced data collection system in place, ORACLE TEAM USA began their historic comeback, winning eight consecutive races to claim the America’s Cup with a final score of 9-8. The victory demonstrated how the integration of advanced sensor technology and data analytics could provide the competitive edge needed to overcome even the most daunting deficits.\nThis 2013 comeback remains a defining moment in sailing history, showcasing how the marriage of traditional sailing skill with cutting-edge technology can produce extraordinary results. Spithill’s leadership during this period highlighted the importance of adaptability and the willingness to embrace technological solutions when facing adversity.\nOne of the main features is velocity made good (VMG), which calculates the optimal course to maximize speed toward the mark while accounting for wind direction and current. Tack and gybe optimization uses statistical modeling to determine the optimal timing for direction changes based on wind shifts, boat speed, and course geometry. Layline calculations employ predictive analytics to determine the optimal approach angles to marks, minimizing distance sailed.\nFurther, boundary layer modeling provides statistical analysis of wind gradients across the racecourse to identify optimal sailing lanes. Weather routing uses optimization algorithms that consider multiple weather models to find the fastest route between marks.\nPressure sensors combined with models of flow dynamics of hydrofoils allow for calculating optimal foil position. Sail trim analysis examines statistical correlation between sail settings, wind conditions, and boat speed. Weight distribution modeling optimizes crew positioning and ballast distribution based on real-time conditions.\nReal-time dashboards display statistical process control charts showing performance metrics against historical benchmarks. Predictive modeling employs machine learning algorithms that forecast optimal strategies based on current conditions and historical performance.\nThe statistical foundation relies heavily on time series analysis, regression modeling, and Monte Carlo simulations to account for the inherent variability in wind and sea conditions. Teams use Bayesian inference to update their models in real-time as new data becomes available during races, creating a dynamic optimization system that continuously refines strategy based on actual performance data.\nThe LinkedIn article by Van Loon describes how SailGP provides a good summary of modern evolution of traditional sailing competitions, leveraging cutting-edge technology to enhance both performance and spectator experience.\nThere are over 1,000 sensors on a SailGP boat that generate an astonishing 52 billion data points per race, providing unprecedented insights into boat performance, wind conditions, and crew actions.\nSailGP’s approach demonstrates how modern sports are increasingly becoming technology competitions as much as athletic competitions, with success depending heavily on the ability to collect, process, and act on real-time data effectively. This represents a significant shift from traditional sailing, where success was primarily determined by experience, intuition, and traditional sailing skills.\n\n\nExample 7 (Google Energy) In 2016, Google’s DeepMind published a white paper outlining their approach to save energy in data centers. Reducing energy usage has been a major focus for data center operators over the past 10 years. Major breakthroughs, however, are few and far between, but Google managed to reduce the amount of energy used for cooling by up to 40 percent. In any large-scale energy-consuming environment, this would be a huge improvement. Given how sophisticated Google’s data centers are, even a small reduction in energy will lead to large savings. DeepMind used a system of neural networks trained on different operating scenarios and parameters within their data centers, creating a more efficient and adaptive framework to understand data center dynamics and optimize efficiency.\nTo accomplish this, the historical data that had already been collected by thousands of sensors within the data center – data such as temperatures, power, pump speeds, setpoints, etc. – was used to train an ensemble of deep neural networks. Since the objective was to improve data center energy efficiency, the model was trained on the average future PUE (Power Usage Effectiveness), which is defined as the ratio of the total building energy usage to the IT energy usage. Two ensembles of deep neural networks were developed to predict the future temperature and pressure of the data center over the next hour. The purpose of these predictions is to simulate the recommended actions from the PUE model, to ensure that we do not go beyond any operating constraints.\nThe DL system was able to consistently achieve a 40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies. It also produced the lowest PUE the site had ever seen.\n\n\n\n\n\n\nFigure 11: DL system was able to consistently achieve a 40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies. It also produced the lowest PUE the site had ever seen.\n\n\n\n\n\nExample 8 (Chess and Backgammon) The game of Chess is the most studied domain in AI. Many bright minds attempted to build an algorithm that can beat a human master. Both Alan Turing and John von Neumann, who are considered pioneers of AI, developed Chess algorithms. Historically, highly specialized systems, such as IBM’s DeepBlue have been successful in chess.\n\n\n\n\n\n\nFigure 12: Kasparov vs IBM’s DeepBlue in 1997\n\n\n\nMost of those systems are based on alpha-beta search, handcrafted by human grand masters. Human inputs are used to design game-specific heuristics that allow truncating moves which are unlikely to lead to a win.\nRecent implementations of chess robots rely on deep learning models. (Silver et al. 2017) shows a simplified example of a binary-linear value function \\(v\\), which assigns a numeric score to each board position \\(s\\). The value function parameters \\(w\\) are estimated from outcomes of a series of self-play and is represented as dot product of a binary feature vector \\(x(s)\\) and the learned weight vector \\(w\\): e.g. value of each piece. Then, each future position is evaluated by summing weights of active features.\n\n\n\n\n\n\nFigure 13: Chess Algorithm, \\(v(s,w)\\) denotes value of strategy, \\(s\\), given the learned weights, \\(w\\).\n\n\n\nBefore deep learning models were used for Go and Chess, IBM used them to develop a backgammon robot, which they called TD-Gammon (Tesauro 1995). TD-Gammon uses a deep learning model as a value function which predicts the value, or reward, of a particular state of the game for the current player.\n\n\n\n\n\n\nFigure 14: Backgammon Algorithm: Tree-Search for Backgammon policies\n\n\n\n\n\nExample 9 (Alpha Go and Move 37) One of the great challenges of computational games was to conquer the ancient Chinese game of Go. The number of possible board positions is \\(10^{960}\\) which prevents us from using tree search algorithms as it was done with chess. There are only \\(10^{170}\\) possible chess positions. Alpha Go uses 2 deep learning neural networks to assist a move and to evaluate a position: a policy network for move recommendation and a value network for current evaluation (who will win?).\nThe policy network was initially trained with supervised learning with data fed from human master games. Then, it was trained in unsupervised mode by playing against itself. The value network was trained based on the outcome of games. A key trick is to reduce breadth of search space by only considering moves recommended by the policy network. The next advance is to reduce depth of search space by replacing search space sub trees with a single value created by the value network.\nThe game is played by performing a Monte Carlo tree search which we illustrate in Figure 15. The algorithm involves traversing the tree using highest recommended moves that haven’t been picked yet, expanding leaf nodes and evaluating with both policy and value networks, and backing up through the tree and storing mean evaluation at each node of its leaf nodes.\n\n\n\n\n\n\nFigure 15: Steps of Monte Carlo tree search\n\n\n\nAlphaGo won 4-1 vs. Lee Sedol, but the loss was informative.\nAlphaGo Zero is the next iteration of the algorithm that removes all human knowledge from the training process. It uses no human data - only self play, no human features - only takes raw board as input, a single neural network - combining policy and value networks into a single network, and simpler search - no randomized monte-carlo rollout - only uses NN evaluation.\n\n\n\n\n\n\nFigure 16: Alpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two\n\n\n\n\n\nExample 10 (Game of Thrones) A fan of HBO’s Game of Thrones saga who could not wait until the next chapter of the A Song of Ice and Fire book it is based on comes out, used a deep learning model to generate one. He used recurrent neural networks which were trained on the previous chapters. First, each of the 32,000 unique words in the text were enumerated and then the machine learned some plot points and was able to generate new patterns of text.\nIn August 2023, George R.R. Martin, author of the “A Song of Ice and Fire” series that inspired Game of Thrones, joined a lawsuit against OpenAI, the creator of ChatGPT. The lawsuit alleges that OpenAI used copyrighted material from the New York Times, including articles about Game of Thrones, to train its AI models without permission. While not directly about the show itself, Martin’s involvement draws a connection due to his authorship of the source material. Here is the reply ChatGPT 5 gives for the Can you tell (in 1 paragraph) about Copyright lawsuit related to Game of Thrones? query.\n\nA notable copyright-lawsuit related to the Game of Thrones universe involves its original author George R.R. Martin plus a group of 16 other prominent writers who filed a suit in September 2023 against OpenAI. They claim that OpenAI’s large-language model (the one behind ChatGPT) was trained on their copyrighted works without permission and now can generate unauthorized derivative material—such as a detailed outline of a prequel to Martin’s epic fantasy series called “A Dawn of Direwolves.” The suit argues this constitutes “systematic theft on a mass scale” and seeks an injunction preventing further unauthorized use of authors’ works, as well as damages.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#anything-as-a-vector-2vec-tokenization",
    "href": "00-intro.html#anything-as-a-vector-2vec-tokenization",
    "title": "The Modern AI Playbook",
    "section": "Anything as a Vector – 2vec: Tokenization",
    "text": "Anything as a Vector – 2vec: Tokenization\nAnother concept behind modern AI systems is the ability to represent any type of information as vectors—ordered lists of numbers that computers can efficiently manipulate. Whether we’re processing text, images, audio, or even chess board positions, the first step is always the same: convert the raw data into numerical vectors that capture the essential patterns and relationships. In natural language processing, this transformation begins with tokenization, where text is broken down into discrete units called tokens. A token might represent a word, part of a word, or even individual characters, depending on the tokenization strategy.\nFor example, the sentence “Hello world!” might be tokenized into three tokens: [“Hello”, “world”, “!”]. Each token is then mapped to a unique numerical identifier and eventually transformed into a high-dimensional vector that encodes semantic meaning, grammatical role, and contextual relationships with other tokens. For example, the sentence “The quick brown fox jumps over the lazy dog” might be tokenized into nine tokens: [“The”, “quick”, “brown”, “fox”, “jumps”, “over”, “the”, “lazy”, “dog”]. Each token is then mapped to a unique numerical identifier and eventually transformed into a high-dimensional vector that encodes semantic meaning, grammatical role, and contextual relationships with other tokens.\n\n\n\n\n\n\ngraph LR\n    A[\"Text:&lt;br/&gt;The quick brown fox jumps over the lazy dog\"] --&gt; B[\"Tokens:&lt;br/&gt;['The','quick','brown','fox','jumps','over','the','lazy','dog']\"]\n    B --&gt; C[\"Token IDs (example):&lt;br/&gt;[102, 451, 317, 890, 775, 233, 117, 642, 508]\"]\n    C --&gt; D[\"Embeddings:&lt;br/&gt;each ID -&gt; vector in R^d\"]\n    D --&gt; E[\"Model computes with vectors (attention, etc.)\"]\n\n\n\n\nFigure 17: Tokenization of “The quick brown fox jumps over the lazy dog” into tokens, token IDs, and embeddings.\n\n\n\n\n\nConsider a simple example using a vocabulary of just four words: [“cat”, “dog”, “runs”, “sleeps”]. We might assign these tokens the numerical IDs [1, 2, 3, 4] respectively. The sentence “cat runs” would become the token sequence [1, 3]. However, these raw IDs don’t capture semantic relationships—they treat “cat” and “dog” as completely unrelated despite both being animals. This is where vector embeddings become powerful. Instead of simple IDs, each token is represented by a vector like: cat = [0.2, 0.8, 0.1], dog = [0.3, 0.7, 0.2], runs = [0.9, 0.1, 0.8], sleeps = [0.1, 0.1, 0.9]. These learned vectors position similar concepts closer together in the multidimensional space—notice how “cat” and “dog” have similar first two values, while “runs” and “sleeps” (both actions) differ more dramatically. This vector representation enables AI models to perform mathematical operations that capture semantic relationships: the vectors for “cat” and “dog” are closer to each other than to “runs” or “sleeps”, reflecting their shared conceptual category as animals.\n\nExample 11 (Semantic Relations) One of the most intriguing aspects of vector representations is their ability to capture semantic relationships through simple arithmetic operations. In natural language processing, this is famously illustrated by analogies such as “king - man + woman ≈ queen,” where the difference between “king” and “man” encodes the concept of royalty, and adding “woman” shifts the meaning to “queen.” This property emerges because the learned vectors for words, phrases, or even entities are organized in such a way that similar relationships are reflected as consistent directions in the high-dimensional space. The same principle applies beyond language: for example, in sports analytics, we might find that the vector for “Ovechkin” (a star hockey player) plus the vector for “Capitals” (his team) minus the vector for “Gretzky” (another legendary player) yields a vector close to “Oilers” (Gretzky’s team), capturing the underlying relationships between players and their teams. \\[\n\\text{Ovechkin + Capitals - Gretzky = Oilers}\n\\]\nThis ability to perform analogical reasoning with vectors is not limited to words or names—it extends to images, audio, and even structured data like chess positions. In computer vision, for instance, the difference between the vector representations of two images might correspond to a specific transformation, such as changing the background or adding an object. In recommendation systems, the vector difference between a user’s preferences and an item’s features can help identify the best match. These semantic relations, encoded as vector arithmetic, enable AI systems to generalize, reason, and make creative associations across domains. The power of this approach lies in its universality: once information is embedded in a vector space, the same mathematical tools can be used to uncover patterns and relationships, regardless of the original data type.\n\nThe vectorization concept becomes particularly clear when we examine how chess positions can be represented numerically. A chess board contains 64 squares, each of which can be empty or occupied by one of 12 different piece types (6 pieces \\(\\times\\) 2 colors). We can represent any chess position as a vector by simply listing the contents of each square in order. For instance, we might use the encoding: empty=0, white pawn=1, white rook=2, …, white king=6, black pawn=7, black rook=8, …, black king=12. A chess position would then become a 64-dimensional vector like [8, 9, 10, 11, 12, 10, 9, 8, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, …] representing the starting position with black pieces on the back rank, black pawns on the second rank, and so forth. More sophisticated representations might include additional dimensions for castling rights, en passant possibilities, or whose turn it is to move, creating vectors of 70 or more dimensions. This numerical representation allows chess engines to use the same mathematical operations that work for language or images. The AI can learn that certain vector patterns (piece configurations) are more advantageous than others, and it can mathematically compute how different moves transform one position vector into another. Modern chess engines like AlphaZero process millions of these position vectors to evaluate potential moves, demonstrating how any complex domain can be reduced to vector operations that computers excel at manipulating.\nThe power of vector representations lies in their ability to capture complex relationships through simple mathematical operations. Similarity between objects can be measured using cosine similarity or Euclidean distance. Linear combinations of vectors can represent mixtures of concepts. This mathematical framework enables the development of powerful algorithms for classification, clustering, and generation tasks.\nWhile machines use numerical vectors to represent information, humans rely on distinct cognitive systems for processing different types of data. A revealing case study of human representational architecture emerged from informal experiments that Richard Feynman and John Tukey conducted as graduate students at Princeton in 1939 (Gleick 1992). The pair devised a playful challenge: could they accurately count time intervals while engaged in other cognitive tasks? To test the limits of their mental abilities under physical stress, they would race up and down staircases, elevating their heart rates, all while attempting to maintain accurate internal counts of seconds and steps. Their results revealed a fascinating asymmetry. When Feynman attempted to speak while counting, his time-keeping accuracy collapsed; yet he could read text without any degradation in counting performance. Tukey exhibited precisely the opposite pattern—he maintained accurate counts while reciting poetry aloud, but his performance suffered significantly when reading. This complementary breakdown pattern, discovered decades before formal cognitive psychology established the framework, inadvertently demonstrated what researchers now call working memory’s dual slave systems: the phonological loop handling auditory-verbal information and the visuo-spatial sketchpad processing visual-spatial data. Feynman’s counting strategy evidently relied on verbal-auditory representations (hence interference with speech but not vision), while Tukey employed visual-spatial mental representations (producing the inverse interference pattern). This historical example underscores a fundamental difference between biological and artificial intelligence: human cognition evolved specialized, domain-specific processing channels, whereas modern AI systems operate on domain-general vector representations that process text, images, and structured data through identical mathematical operations.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#how-did-those-machines-learn",
    "href": "00-intro.html#how-did-those-machines-learn",
    "title": "The Modern AI Playbook",
    "section": "How did those Machines Learn?",
    "text": "How did those Machines Learn?\nThe second key idea is to use loss function to combine information from multiple sources represented as vectors into a single number that measures how well model performs the task at hand, e.g. text generation or image classification. This is a key idea in modern AI.\nWhile vector representations provide the language, loss functions provide the learning signal that guides AI systems toward better performance. The loss function is particularly important because it connects probabilistic reasoning with optimization, creating a bridge between statistical theory and practical implementation. The combination of learned vector representations and loss functions has enabled remarkable advances across AI domains:\nNatural Language Processing: Modern language models like GPT and BERT learn contextual vector representations of words and sentences. The loss function ensures that the representations capture semantic relationships that are useful for tasks like translation, summarization, and question answering.\nComputer Vision: Convolutional neural networks learn hierarchical feature representations, from low-level edges and textures to high-level object parts and categories. The loss function ensures these representations are optimized for the specific task, whether it’s object detection, segmentation, or image generation.\nRecommendation Systems: User and item representations are learned simultaneously, with the loss function measuring how well the model predicts user preferences. This approach has revolutionized recommendation engines at companies like Netflix, Amazon, and Spotify.\nMulti-Modal AI: The latest frontier in artificial intelligence represents information from multiple modalities—text, images, audio, and video—in shared vector spaces. This enables models like GPT-4V, DALL-E, and CLIP to understand and generate content across different media types. For instance, a multi-modal model can analyze a photograph and generate a detailed textual description, or conversely, create images from text prompts. The loss function in these systems ensures that semantically related concepts across different modalities are positioned closely in the shared vector space—the vector for the word “cat” should be near the vector for an actual cat image. This breakthrough has enabled applications ranging from automated image captioning and visual question answering to sophisticated content creation tools that seamlessly blend text and imagery.\n\nLet us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence. (Turing 1950)\n\nModern large language models (LLMs) operationalize Turing’s vision. They begin with random parameters and a simple learning rule, and through exposure to vast text corpora they continually modify their own internal tables (weights) to compress, predict, and thereby internalize regularities of language and the world. The mechanism is straightforward:\n\nText is tokenized and mapped to vectors (embeddings). Similar tokens occupy nearby points in a high-dimensional space, as in early word-vector models (Mikolov et al. 2013).\nA transformer computes contextual representations via attention and feed-forward layers (Vaswani et al. 2023). Attention learns where to look; the feed-forward layers learn what to compute given that context.\nThe model predicts the next token. Errors are measured by a loss and used to update weights by gradient descent, subtly rewriting its own instruction tables on every batch.\n\nThis objective does not explicitly ask for grammar, facts, or reasoning. Yet these capabilities emerge because they help reduce prediction error. Syntax reduces uncertainty about what can come next; factual knowledge reduces uncertainty about named entities and relations; reasoning helps when surface cues are insufficient. Over time, internal vectors reorganize so that directions in representation space align with concepts (e.g., tense, number, sentiment) and relations (e.g., subject–verb agreement, coreference, part–whole). Earlier we saw how vector spaces can encode analogies; in LLMs, entire layers compose such directions so that rich concepts become linearly or near-linearly accessible.\nIntuitively, the model learns concepts by clustering and separating patterns that repeatedly co-occur in context. If tokens and phrases that describe, say, “photosynthesis” often appear with “chlorophyll,” “light,” and “glucose,” the model will place their vectors so these ideas are nearby and distinguishable from unrelated topics. The model learns relations by assigning attention to the right places and by computing transformations that map one concept to another. Certain attention heads specialize (e.g., long-distance agreement), while feed-forward pathways implement conditional rules that activate only in specific contexts. Together, these circuits encode who-did-what-to-whom, temporal order, causality hints, and definitions.\nFrom a systems perspective, LLM learning resembles compression with side information: the network restructures its internal space to represent frequent, useful regularities with short descriptions, while preserving the capacity to disambiguate rare cases. As scale grows (data, model size, compute), the representation becomes more detailed and compositional, allowing the model to recombine known pieces into new, coherent outputs.\nWhy does this produce seemingly abstract knowledge? Because the cheapest way to predict language at scale is to internalize the latent structure that generated it: human conventions of grammar, stable facts about the world, common-sense regularities, and task patterns (definitions, explanations, step-by-step solutions). The network’s continual self-modification—Turing’s pupil—pushes its internal tables toward representations that make these regularities linearly separable and compositionally usable during generation.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "The Modern AI Playbook",
    "section": "Generative AI",
    "text": "Generative AI\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include providing accessible mental health support and a sense of connection (therapy/companionship), especially in regions with limited access to human therapists where users find AI to be available 24/7 and non-judgmental. People are also creating timelines for tasks, planning daily habits, and managing personal projects (organizing my life), using AI as a study guide to explain complex topics and reinforce learning (enhanced learning), generating meal plans based on specific dietary needs and macro calculations (healthier living), planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time (creating travel itineraries), and drafting appeal letters for things like parking tickets (disputing fines).\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.\n\n\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. The program worked by identifying keywords in user statements and transforming them into questions or reflective responses. For example, if a user typed “I am sad,” ELIZA might respond with “Why do you think you are sad?” or “Tell me more about being sad.” While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.\nWhat surprised Weizenbaum was not just that ELIZA worked, but how readily people attributed human-like understanding to the program. Users began forming emotional attachments to ELIZA, sharing deeply personal information and believing the computer genuinely cared about their problems. Some even requested private sessions without Weizenbaum present. This phenomenon, now known as the ELIZA effect, describes the human tendency to unconsciously assume computer behaviors are analogous to human behaviors, even when we know better intellectually.\nThe ELIZA effect reveals something profound about human psychology: we are predisposed to anthropomorphize systems that exhibit even rudimentary conversational abilities. This has significant implications for modern AI systems. Today’s large language models like ChatGPT and Claude are vastly more sophisticated than ELIZA, yet they still operate through pattern matching and statistical prediction rather than genuine understanding. However, their responses are so fluent and contextually appropriate that the ELIZA effect is amplified dramatically. Users often attribute consciousness, emotions, and intentionality to these systems, leading to both beneficial therapeutic interactions and concerning over-reliance on AI for emotional support.\nUnderstanding the ELIZA effect is crucial as we navigate the current AI landscape. While AI can provide valuable assistance for mental health support, learning, and personal organization, we must remain aware that these systems are sophisticated pattern matchers rather than conscious entities. The therapeutic value may be real—many users do find comfort and insight through AI interactions—but it stems from the human capacity for self-reflection prompted by the conversation, not from genuine empathy or understanding on the machine’s part.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#ai-agents",
    "href": "00-intro.html#ai-agents",
    "title": "The Modern AI Playbook",
    "section": "AI Agents",
    "text": "AI Agents\nThe success of deep learning models has led to the development of software libraries that abstract away the complexity of implementing these components. Libraries like PyTorch and TensorFlow provide efficient implementations of both vector operations and loss functions, making it possible for practitioners to focus on the high-level design of their AI systems rather than the low-level implementation details. Modern deep learning models used for Large Language Models (LLMs) are developed using these libraries.\nHowever, a language model alone isn’t enough to solve complex tasks that require multiple steps and external tools. To solve complex tasks, you need AI agents—software systems that perform tasks, make decisions, and interact with other systems by using reasoning and tool use. Unlike traditional software, agents can dynamically adapt their behavior based on input, context, and goals. Cloud providers like Nebius and AWS provide “orchestration” services that allow practitioners to focus on the high-level design of their AI systems and agents rather than the low-level implementation details.\nFor example, Nebius AI Studio provides a production-grade platform specifically designed for deploying AI agents at enterprise scale. Unlike traditional cloud providers that focus on basic infrastructure, Nebius addresses the core challenges teams face when moving from prototype agents to production systems: cost optimization, observability, evaluation frameworks, and seamless integration with existing workflows. The platform offers access to over 30 open-source models including Llama, Mistral, DeepSeek, and Qwen through two pricing tiers—a fast tier with sub-2-second response times for live applications, and a base tier that cuts costs in half for batch workloads. With competitive pricing at $0.38 per million output tokens for Qwen2.5 72B while generating 70+ tokens per second, teams can achieve both performance and cost efficiency.\nThe platform’s strength lies in its production-ready agent orchestration capabilities. Through OpenAI-compatible APIs and integrations with frameworks like CrewAI, Google ADK, LangChain, and Agno, developers can build sophisticated multi-agent systems without vendor lock-in. For instance, a Financial Document Processing Agent for a multinational bank could leverage Nebius’s infrastructure to analyze millions of loan applications, contracts, and compliance documents across multiple languages. The system would use the fast tier for real-time customer interactions while processing bulk document analysis through the cost-efficient base tier. Integration with agent frameworks like CrewAI would orchestrate specialized agents—a document classifier, risk assessor, and compliance checker—working collaboratively. The platform’s built-in observability tools would monitor agent decision paths, track performance metrics, and implement human-in-the-loop approval workflows for high-stakes financial decisions. This architecture ensures regulatory compliance while maintaining the speed and accuracy required for enterprise financial operations, demonstrating how modern AI infrastructure enables sophisticated agent deployments that were previously accessible only to the largest technology companies.\nAs AI continues to evolve, these two pillars remain central to the field’s progress. New architectures and training methods build upon these foundations, but the fundamental principles of learning meaningful vector representations and optimizing them through appropriate loss functions continue to drive innovation in artificial intelligence.\n\n\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nGleick, James. 1992. Genius: The Life and Science of Richard Feynman. New York: Pantheon Books.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nKeynes, John Maynard. 1930. “Economic Possibilities for Our Grandchildren.” In Essays in Persuasion, 358–73. W. W. Norton & Company.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nick, Vadim Sokolov, and Jianeng Xu. 2023. “Quantum Bayesian Computation.” Applied Stochastic Models in Business and Industry 39 (6): 869–83.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nTuring, A. M. 1950. “Computing Machinery and Intelligence.” Mind; a Quarterly Review of Psychology and Philosophy 59 (236): 433–60.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nVeblen, Thorstein. 1899. The Theory of the Leisure Class: An Economic Study of Institutions. New York: Macmillan.\n\n\nWiener, Norbert. 1950. The Human Use of Human Beings: Cybernetics and Society. Boston: Houghton Mifflin.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "01-prob.html",
    "href": "01-prob.html",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "1.1 Odds as Probabilities\nProbability deals with randomness and provides a language to communicate uncertainty, which is usually associated with our lack of knowledge or information. For example in classical coin toss example, if we know the exact force that was applied to the coin, we can predict the outcome with certainty. However, practically it is never the case and we treat coin toss outcome as random.\nAssigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (observed data). Consider the coin-tossing example. Say event \\(A\\) represents a Head. Then, to empirically estimate \\(P(A)\\) we can repeat the two-toss experiment \\(N\\) times and count \\(n\\), the number of times \\(A\\) occurred. The plot below shows the proportion of heads after \\(N\\) trials.\nWe can see that as \\(N\\) grows, the curve converges to \\(0.5\\). This is the law of large numbers. When \\(N\\) is large, \\(n/N\\) will be close to \\(P(A)\\). The probability as a limit definition is natural and was proposed by von Mises. However, this definition is not operational. First, it requires an infinite number of trials to be conducted. We can use a relaxed definition due to Bernoulli, and define probability as simply the ratio of the number of heads to the total number of trials in a given experiment. This definition is operational and can be used to estimate the probability of an event. This definition requires the experiment to be repeated under identical conditions. If we are to repeat this experiment under different conditions, e.g. when an unbalanced coin is used, our estimate of \\(P(A)\\) will change as well.\nAn alternative operational definition of probability was proposed by Frank Ramsey (1926) and later refined by Bruno de Finetti (1937). Rather than relying on long-run frequencies, this approach defines probability through the lens of rational betting behavior. The key insight is that your probability assignment for an event should correspond to the odds at which you would be willing to bet on that event.\nSuppose you believe the probability of an event \\(A\\) is \\(p\\). According to Ramsey’s definition, this means you should be indifferent between paying \\(p \\cdot S\\) dollars to receive \\(S\\) dollars if event \\(A\\) occurs (and nothing otherwise) and accepting \\(p \\cdot S\\) dollars to pay someone \\(S\\) dollars if event \\(A\\) occurs. However, not all probability assignments lead to rational behavior. Suppose someone assigns probabilities to events in an incoherent way. In that case, a clever adversary could construct a series of bets, called a Dutch book, where the individual is guaranteed to lose money regardless of which events occur. The requirement that probabilities must be assigned in such a way that no Dutch book can be constructed against you is known as coherence.\nLet us consider a simple example. Suppose you assign probability \\(P(A) = 0.7\\) to event \\(A\\) occurring and probability \\(P(\\bar A) = 0.2\\) to event \\(A\\) not occurring, where \\(A^c\\) denotes the complement of \\(A\\). A Dutch book can be constructed as follows:\nYour total payment is \\(\\$0.90\\), but you will receive exactly \\(\\$1\\) regardless of whether \\(A\\) or \\(A^c\\) occurs, for a guaranteed loss of \\(\\$0.10\\). The incoherence arises because \\(P(A) + P(\\bar A) = 0.9 \\neq 1\\).\nMore generally, coherence implies that probabilities must satisfy the following basic properties:\nTo see why non-negativity must hold, suppose \\(P(A) = -0.1\\) for some event \\(A\\). According to the betting interpretation, you would receive \\(\\$0.10\\) and then pay \\(\\$1\\) if \\(A\\) occurs. If \\(A\\) does not occur, you keep the \\(\\$0.10\\) but receive nothing. However, this means you are offering to pay someone to take a bet against \\(A\\)—clearly irrational behavior.\nThe normalization requirement \\(P(\\Omega) = 1\\) ensures that you assign probability one to something that is certain to happen. If \\(P(\\Omega) &lt; 1\\), you would be willing to pay less than \\(\\$1\\) to receive \\(\\$1\\) with certainty, allowing an arbitrageur to make a riskless profit. Conversely, if \\(P(\\Omega) &gt; 1\\), you would pay more than \\(\\$1\\) for a certain payoff of \\(\\$1\\), guaranteeing a loss.\nThe additivity axiom ensures consistency across mutually exclusive events. If \\(A\\) and \\(B\\) cannot both occur, then betting on “\\(A\\) or \\(B\\)” should cost the same as placing separate bets on \\(A\\) and on \\(B\\). Violating this principle again opens the door to Dutch books.\nAnother axiom \\(P(A) + P(\\bar A) = 1\\) just follows from the normalization requirement and the additivity axiom.\nThese are precisely the axioms proposed by Andrey Kolmogorov (1933) in his foundational work on probability theory. Although, Kolmogorov’s axioms are agnostic to any definition of probability and are purely mathematical in nature, the fact that the can be derived from from the Dutch book argument shows their applicability to rational decision-making under uncertainty. Any violation of these axioms opens you to guaranteed losses through carefully constructed bets. However, Kolmogorov’s axiomatic approach helps use to derive results in more complex settings. For examples Kolmogorov’s framework is applicable in infinite sample spaces and continuous random variables.\nThe axioms provide a number of rules that probabilities must follow. There are several important corollaries that can help us assign probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:\nAll of these axioms follow simply from the principle of coherence and the avoidance of Dutch book arbitrage. This includes the Bayes rule itself (de Finetti, Shimony).\nBayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.\nBayes rule simply provides a disciplined probability accounting of how probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities he cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.\nThese corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.\nThe age of artificial intelligence (AI) has certainly proved that Bayes is a powerful tool. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics or the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.\nAnother way, sometimes more convenient, to talk about uncertainty and to express probabilities via odds, such as 9 to 2 or 3 to 1. We assign odds “on \\(A\\)” or “against \\(A\\)”. For example, when we say that the odds on a Chicago Bears’ Super Bowl win are 2 to 9, it means that if they are to play 11 times (9+2), they will win 2 times. If \\(A\\) is the win event, then odds on \\(A\\) \\[\nO(A) = \\dfrac{P(\\mbox{not A}) }{P(A)} = \\dfrac{1-P(A)}{P(A)}\n\\] Equivalently, probabilities can be determined from odds \\[\nP(A) = \\dfrac{1}{1+O(A)}\n\\] For example, if the odds are one \\(O(A) = 1\\), then for every $1 bet you will pay out $1. This event has probability \\(0.5\\).\nIf \\(O(A) = 2\\), then you are willing to offer \\(2:1\\). For a $1 bet you’ll payback $3. In terms of probability \\(P(A) = 1/3\\).\nOdds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#odds-as-probabilities",
    "href": "01-prob.html#odds-as-probabilities",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "Example 1.1 (Odds) One of the main sources of prediction markets is bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. Figure 1.1 shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hillary Clinton would win over Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.\n\n\n\n\n\n\nFigure 1.1: Presidential Odds 2016\n\n\n\nAhead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hillary Clinton. The best odds on Clinton are \\(1/3\\), this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the Market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides, otherwise they are risking to stay out of business.\n\n\nExample 1.2 (Kentucky Derby) The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is \\(1 \\frac{1}{4}\\) miles (2 kilometers) and is the first time the three-year old horses have raced the distance.\nThere was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have won many times in a row. The market is getting better at predicting who’s going to win. Here’s the data\n\n\n\nHorse Name\nYear\nOdds\n\n\n\n\nSpectacular Bid\n1979\n0.6/1\n\n\nFusaichi Pegasus\n2000\n2.3/1\n\n\nStreet Sense\n2007\n9/2\n\n\nBig Brown\n2008\n5/2\n\n\n\nRecently, favorites have had a lot more success\n\n\n\nHorse Name\nYear\nOdds\n\n\n\n\nCalifornia Chrome\n2014\n5/2\n\n\nAmerican Pharoah\n2015\n2/1\n\n\nNyquist\n2016\n3.3/1\n\n\nAlways Dreaming\n2017\n5.2/1\n\n\n\nThe most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only other horse that in 2001 has broken two minutes at odds 11.5/1.\n\n\nExample 1.3 (Exacta Betting and the Harville Formula) How can probability help you with betting on the race? There are many different types of bets, and probability can help you find fair odds. The Derby is a Grade 1 stakes race for three-year-old thoroughbred horses. Colts and geldings carry 126 pounds and fillies 121. The odds are set by pari-mutuel betting by the public. After all the wagers have been placed, the racetrack takes a fee (18%). After the winning horse passes the finishing line, the pool of money is redistributed to the winning tickets. Random betting therefore loses you 18%, so it’s important to learn some empirical facts to try and tilt the odds in your favor.\nFor example, you can place bets as follows:\n\nWin: “$2 win horse 1”\nStraight Exacta: “$2 exacta 1 with 2”\n\nExacta Box: “$2 exacta box 1 and 2” You win with either order: 2 bets = $4.\n\nConsider a hypothetical race where Sovereignty wins at 9/1 odds and Journalism comes second at 7/2 odds. For a $2 bet on Sovereignty to Win at 9/1, the payout would be \\(2 \\cdot 9 + 2 = \\$20\\) (the 9/1 win plus your initial $2 bet returned).\nLet’s figure out the fair value for an exacta bet given that you know the win odds. This is known as the Harville formula. The exacta is probably one of the most popular bets for many horseplayers, corresponding to predicting the first two horses in the correct order.\nThe Harville formula provides an answer. We use the rule of conditional probability. The probability for the straight exacta of horses \\(A\\) beating horse \\(B\\) is: \\[\nP(A \\text{ beats } B) = P(A \\text{ Wins}) \\cdot P(B \\text{ Second} \\mid A \\text{ Wins})\n\\]\nA reasonable assessment of \\(P(B \\text{ Second} \\mid A \\text{ Wins})\\) can be derived as follows. For horse \\(B\\) to come second, imagine taking horse \\(A\\) out of the betting pool and re-distributing the probability to \\(B\\). This gives: \\[\nP(B \\text{ Second} \\mid A \\text{ Wins}) = \\frac{P(B \\text{ Wins})}{1 - P(A \\text{ Wins})}\n\\]\nIn total, the fair price for the exacta is: \\[\nP(A \\text{ beats } B) = P(A \\text{ Wins}) \\cdot \\frac{P(B \\text{ Wins})}{1 - P(A \\text{ Wins})}\n\\]\nTherefore, we have: \\[\np_{12} = p_1 \\cdot \\frac{p_2}{1-p_1} \\text{ where } p_1 = \\frac{1}{1+O_1}, p_2 = \\frac{1}{1+O_2}\n\\]\nSolving for odds, we get the Harville formula: \\[\nO_{12} = O_1(1 + O_2) - 1\n\\]\nUsing our example with 9/1 and 7/2 odds: \\(O_{12} = 9 \\cdot (1 + 3.5) - 1 = 39.5/1\\).\nNotice that the actual payout is determined solely by the volume of money wagered on that combination. There’s no requirement it matches our probabilistic analysis. However, the Harville formula gives us an idea of fair value. Some bettors searching for value try to find significantly undervalued exacta bets relative to the Harville formula.\nThere are many other factors to consider: jockey performance, bloodlines, and post positions can all matter significantly in determining the actual race outcome.\n\n\nExample 1.4 (Boy-Girl Paradox) If a woman has two children and one is a girl, the chance that the other child is also female has to be \\(50-50\\), right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, and I’m still not sure I believe this, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are \\[\nBB \\; \\; BG \\; \\; GB \\; \\; GG\n\\] Conditional on the information that one is a girl means that you know we can’t have the \\(BB\\) scenario. Hence we are left with three possibilities \\[\nBG \\; \\; GB \\; \\; GG\n\\] In one of these is the other a girl. Hence \\(1/3\\).\nIt’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is \\(1/2\\) as there are two possibilities \\[\nGB \\; \\; GG\n\\] This leads to the probability of \\(1/2\\).\n\n\nExample 1.5 (Galton Paradox) You flip three fair coins. What is the \\(P(\\text{all} \\; \\text{alike})\\)?\nAssuming a fair coin (i.e. \\(p(H) = p(T) = 1/2\\)), a formal approach might consist of computing the probability for all heads or all tails, which is\n\\[\\begin{align*}\np(HHH) &\\equiv p(H \\text{ and } H \\text{ and } H) \\\\\n&= p(H)\\times p(H)\\times p(H) \\\\\n&= \\left(\\frac{1}{2}\\right)^3\n\\end{align*}\\] and, since we’re ultimately interested in the probability of either (mutually exclusive) case, \\[\\begin{align*}\nP(\\text{all alike}) &= P(HHH \\text{ or } TTT) \\\\\n&= P(HHH) + P(TTT) \\\\\n&= 2 \\times \\frac{1}{8}\n\\end{align*}\\]\nOne could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, \\[\np(\\text{all alike}) = 1/2\n\\] The fault lies somewhere within the terms “remaining/last” and their connotation. A faulty symmetry assumption is being made in that statement pertaining to the distribution of the “remaining/last” coin. Loosely put, you’re certain to ultimately be in the case where at least two are alike, as stated in the above argument, but within each case the probability of landing the “remaining/last” matching \\(H\\) or \\(T\\) is not \\(1/2\\), due to the variety of ways you can arrive at two matching coins.\nFor a real treatment of the subject, we highly recommend reading Galton’s essay at galton.org.\n\n\nExample 1.6 (Three Cards) Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that it’s red. What’s the probability the other side is red? \\(1/2\\)? No, it’s \\(2/3\\)! By a similar logic there are six initial possibilities \\[\nB_1 B_2 \\; \\; B_2 B_1 \\; \\; B R \\; \\; R B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] where \\(1\\) and \\(2\\) index the sides of the same colored cards.\nIf we now condition on the top side being red we see that there are still three possibilities left \\[\nR B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] Hence the probability is \\(2/3\\) and not the intuitive \\(1/2\\).\n\n\nExample 1.7 (NFL: New England Patriots Coin Toss) Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The NFL team New England Patriots won 19 out of 25 coin tosses in the 2014-15 season. What is the probability of this happening?\nLet \\(X\\) be a random variable equal to \\(1\\) if the Patriots win and \\(0\\) otherwise. It’s reasonable to assume \\(P(X = 1) = \\frac{1}{2}\\). The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is \\((1/2)^{25}\\). We can code a typical sequence as, \\[\n1,1,1,\\ldots,1,0,0,\\ldots,0.\n\\] There are \\(177,100\\) different sequences of 25 games where the Patriots win 19. There are \\(25! = 1\\cdot 2\\cdot \\ldots \\cdot 25\\) ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are \\(19!\\) ways to re-arrange the ones and \\(6!\\) ways to rearrange the sequence of zeroes. Thus, the total number of different winning sequences is\n\nfactorial(25)/(factorial(19)*factorial(25-19))\n\n## [1] 177100\n\n\nEach potential sequence has probability \\(0.5^{25}\\), thus \\[\nP\\left(\\text{Patriots win 19 out of 25 tosses}\\right) =  177,100 \\times 0.5^{25} = 0.005\n\\]\nOften, it is easier to communicate uncertainties in the form of odds. In terms of betting odds of \\(1:1\\) gives \\(P = \\frac{1}{2}\\), odds of \\(2:1\\) (I give \\(2\\) for each \\(1\\) you bet) is \\(P = \\frac{1}{3}\\).\nRemember, odds, \\(O(A)\\), is the ratio of the probability of happening over not happening, \\[\nO(A) = (1 - P(A))/P(A),\n\\] equivalently, \\[\nP(A) = \\frac{1}{1 + O(A)}.\n\\]\nThe odds of the Patriots winning sequence are then 1 to 199\n\n0.005/(1-0.005)\n\n## [1] 0.005\n\n\n\n\nExample 1.8 (Hitting Streak) Pete Rose of the Cincinnati Reds set a National League record of hitting safely in \\(44\\) consecutive games. How likely is such a long sequence of safe hits to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once in every one of those 44 games. Here are a couple of facts we know about him:\n\nRose was a \\(300\\) hitter, he hits safely 3 times out of 10 attempts\nEach at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.\n\nAssuming he comes to bat \\(4\\) times each game, what probability might reasonably be associated with that hitting streak? First we define notation. We use \\(A_i\\) to denote an event of hitting safely at game \\(i\\), then \\[\n\\begin{aligned}\n& P( \\mathrm{Rose \\; Hits \\; Safely \\; in \\;44 \\; consecutive \\; games} ) = \\\\\n& P ( A_1 \\; \\text{and} \\;  A_2  \\ldots \\text{and} \\;  A_{44} ) = P ( A_1 ) P ( A_2 ) \\ldots P ( A_{44} )\n\\end{aligned}\n\\] We now need to find \\(P(A_i)\\)s where \\(P (A_i ) = 1 - P ( \\text{not} \\; A_i )\\) \\[\\begin{align*}\nP ( A_1 ) & = 1 - P ( \\mathrm{ not} \\; A_1 ) \\\\\n& = 1 - P ( \\mathrm{ Rose \\; makes \\; 4 \\; outs } ) \\\\\n& = 1 - ( 0.7)^4 = 0.76\n\\end{align*}\\] For the winning streak, then we have \\((0.76)^{44} = 0.0000057\\), a very low probability. In terms of odds, there are three basic inferences\n\nThis means that the odds for a particular player as good as Pete Rose starting a hitting streak today are \\(175,470\\) to \\(1\\).\nThis doesn’t mean that the run of \\(44\\) won’t be beaten by some player at some time: the Law of Very Large Numbers\nJoe DiMaggio’s record is 56. He is a 325 hitter, thus we have \\((0.792)^{56} = 2.13 \\times 10^{-6}\\) or 455,962 to 1. It’s going to be hard to beat.\n\n\n\nExample 1.9 (Derek Jeter) Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and finer subset selection can change your average effects.\n\n\n\n\n1995\n\n1996\n\nCombined\n\n\n\n\n\nDerek Jeter\n12/48\n0.250\n183/582\n0.314\n195/630\n0.310\n\n\nDavid Justice\n104/411\n0.253\n45/140\n0.321\n149/551\n0.270\n\n\n\nThis situation is known as confounding. It occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if \\(A,B,C\\) are events it is possible to have the three inequalities \\[\\begin{align*}\n&P( A \\mid B \\text{ and } C ) &gt; P( A \\mid B \\text{ and } \\bar C )\\\\\n&P( A \\mid \\bar  B \\text{ and } C ) &gt; P( A \\mid \\bar  B \\text{ and } \\bar  C )\\\\\n&P( A \\mid C ) &lt; P( A \\mid \\bar C )\n\\end{align*}\\] The three inequalities can’t hold simultaneously when \\(P(B\\mid C) = P(B\\mid \\bar  C)\\).\n\n\nExample 1.10 (Birthday Problem) The birthday problem (Diaconis and and Mosteller 1989) is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.\nIn general, given \\(N\\) items (people) randomly distributed into \\(c\\) categories (birthdays), where the number of items is small compared to the number of categories \\(N \\ll c\\), the probability of no match is given by \\[\nP(\\text{no match}) \\approx \\exp\\left(-N^2/2c\\right).\n\\] Given \\(A_i\\) is the event that person \\(i\\) has a matching birthday with someone, we have \\[\nP(\\text{no match})  = \\prod_{i=1}^{N-1}(1-P(A_i)) = \\exp\\left(\\sum_{i=1}^{N-1}\\log (1-P(A_i))\\right).\n\\] Here \\(P(A_i) =\\dfrac{i}{c}\\) Then use the approximation \\(\\log(1-x) \\approx -x\\) for small \\(x\\) to get \\(P(\\text{no match})\\). \\[\n\\sum_{i=1}^{N-1}\\log (1-P(A_i)) \\approx -\\sum_{i=1}^{N-1}\\dfrac{i}{c} = -\\dfrac{N(N-1)}{2c}.\n\\]\nThe probability of at least two people sharing a birthday is then the complement of the probability above: \\[\nP(\\text{At least one shared birthday}) = 1 - P(\\text{no match}).\n\\] Solving for \\(P(\\text{match})=1/2\\), leads to a square root law \\(N=1.2\\sqrt{c}\\), if \\(c=365\\) then \\(N=23\\), and if \\(c=121\\) (near birthday match), then \\(N=13\\).\nThe unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#dutch-book-and-the-rules-of-probability",
    "href": "01-prob.html#dutch-book-and-the-rules-of-probability",
    "title": "1  Probability and Uncertainty",
    "section": "1.2 Dutch book and the rules of probability",
    "text": "1.2 Dutch book and the rules of probability\nIf probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.\nThe starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event” (p. 101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.\nFormally, for any event \\(A\\), the identity \\[\nP(A)  =\\frac{1}{1+\\text{odds}(A)}\\text{ or }\\text{odds}(A)=\\frac{1-P(A)}{P(A)},\n\\] where \\(\\bar A\\) is the complement of \\(A\\), links odds and probabilities. Throughout, we use \\(P\\) as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is \\(1/3\\). The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are fair if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.\nIn constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “could have a book made against him by a cunning bettor and would then stand to lose in any event” (Ramsey (1931), p. 22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional probability. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.\nDutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, \\(S\\). The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.\nTo derive the rules, consider the first axiom of probability: for any event \\(A\\), \\(0\\leq P(A) \\leq 1\\). Suppose that the odds imply probabilities \\(P(A)\\) for \\(A\\) occurring and \\(P(\\bar A)\\) for other outcomes, with associated payoffs of \\(S_{A}\\) and \\(S_{\\bar A}\\). Then, having bet \\(S_{A}\\) and \\(S_{\\bar A}\\), the gains if \\(A\\) or \\(\\bar A\\) occur, \\(G_{A}\\) and \\(G_{\\bar A}\\), respectively, are \\[\\begin{align*}\nG(A)   &  =S_{A}-P(A) S_{A}-P(\\bar A)  S_{\\bar A}\\\\\nG(\\bar A)   &  =S_{\\bar A}-P(A) S_{A}-P(\\bar A)  S_{\\bar A}.\n\\end{align*}\\] To see this, note that the bettor receives \\(S_{A}\\) and pays \\(P(A) S_{A}\\) for a bet on event \\(A\\). The bookie can always choose to place a zero stake on \\(\\bar A\\) occurring, which implies that \\(G(A) =S_{A}-P(A) S_{A}\\) and \\(G\\left(\\bar A\\right) =-P(A) S_{A}\\). Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus \\(G(A) G(\\bar A) \\leq 0\\). Substituting, \\(\\left( 1-P(A) \\right) P(A) \\geq0\\) or \\(0\\leq P(A) \\leq 1\\), which is the first axiom of probability. The second axiom, that the set of all possible outcomes has probability \\(1\\), is similarly straightforward to show.\nThe third axiom is that probabilities add, that is, for two disjoint events \\(A_{1}\\) and \\(A_{2}\\), \\(P(A) =P\\left( A_{1} \\text{ or } A_{2}\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\). Assuming stakes sizes of \\(S_{A}\\), \\(S_{A_{1}},\\) and \\(S_{A_{2}}\\) (and zero stakes on their complements) there are three possible outcomes. If neither \\(A_{1}\\) nor \\(A_{2}\\) occur, the gain is \\[\nG(\\bar A)  =-P(A)  S_{A} -P\\left(  A_{1}\\right)  S_{A_{1}}-P\\left( A_{2}\\right)  S_{A_{2}}.\n\\]\nIf \\(A_{1}\\) occurs, \\(A\\) also occurs, and the gain is \\[\nG\\left(  A_{1}\\right)  =\\left(  1-P(A) \\right)  S_{A}+\\left(  1-P\\left(  A_{1}\\right)  \\right) S_{A_{1}}-P\\left(  A_{2}\\right)  S_{A_{2}},\n\\] and finally if \\(A_{2}\\) occurs, \\(A\\) also occurs, and \\[\nG\\left(  A_{2}\\right)  =\\left(  1-P(A) \\right)  S_{A}-P\\left(  A_{1}\\right)  S_{A_{1}}+\\left( 1-P\\left(  A_{2}\\right)  \\right)  S_{A_{2}}.\n\\] Arranging these into a matrix equation, \\(G=PS\\): \\[\n\\left( \\begin{array}\n[c]{c}%\nG(\\bar A) \\\\\nG\\left(  A_{1}\\right) \\\\\nG\\left(  A_{2}\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P(A)  & -P\\left(  A_{1}\\right)  &\n-P\\left(  A_{2}\\right) \\\\\n1-P(A)  & 1-P\\left(  A_{1}\\right)  &\n-P\\left(  A_{2}\\right) \\\\\n1-P(A)  & -P\\left(  A_{1}\\right)  &\n1-P\\left(  A_{2}\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{A}\\\\\nS_{A_{1}}\\\\\nS_{A_{2}}%\n\\end{array}\n\\right).\n\\]\nThe absence of a Dutch book arbitrage implies that there is no set of stakes, \\(S_{A}\\), \\(S_{A_{1}}\\), and \\(S_{A_{2}}\\), such that the winnings in all three events are positive. If the matrix \\(P\\) is invertible, it is possible to find stakes with positive gains. To rule this out, the determinant of \\(P\\) must be zero, which implies that \\(0=-P(A) +P\\left(A_{1}\\right) +P\\left( A_{2}\\right)\\), or \\(P\\left(A\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\).\nThe fourth axiom is conditional probability. Consider an event \\(B\\), with \\(P\\left( B\\right) &gt;0\\), an event \\(A\\) that occurs conditional on \\(B\\), and the event that both \\(A\\) and \\(B\\) occur. The probabilities or prices of these bets are \\(P\\left( B\\right)\\), \\(P\\left( A \\mid B\\right)\\), and \\(P\\left( A \\text{ and } B\\right)\\). Consider bets with stakes \\(S_{B}\\), \\(S_{A \\mid B}\\) and \\(S_{A \\text{ and } B}\\), with the understanding that if \\(B\\) does not occur, the conditional bet on \\(A\\) is canceled. The payoffs to the events that \\(B\\) does not occur, \\(B\\) occurs but not \\(A\\), and \\(A\\) and \\(B\\) occur, are \\[\n\\left( \\begin{array}\n[c]{c}%\nG\\left(  \\bar B\\right) \\\\\nG\\left(  \\bar A \\text{ and } B\\right) \\\\\nG\\left(  A \\text{ and } B\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  & 0\\\\\n1-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  &\n-P\\left(  A \\mid B\\right) \\\\\n1-P\\left(  B\\right)  & 1-P\\left(  A \\text{ and } B\\right)  &\n1-P\\left(  A \\mid B\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{B}\\\\\nS_{A \\text{ and } B}\\\\\nS_{A \\mid B}%\n\\end{array}\n\\right).\n\\] Similar arguments imply the determinant must be zero, which implies that \\[\nP\\left(  A \\mid B\\right)  =\\frac{P\\left(  A \\text{ and } B\\right) }{P\\left(  B\\right)  },\n\\] which is the law of conditional probability, given \\(P(B)&gt;0\\), of course, otherwise the conditional probability is not defined, and the \\(P\\) matrix has determinant 0.\nPolson and Sokolov (2025) explores the concept of negative probabilities, which arise in fields like physics and quantum computing, and also appear in Bayesian modeling as mixing distributions for unobserved latent variables. They establish a conceptual link between these two different applications. The article uses Bartlett’s definition of negative probabilities involving extraordinary random variables and presents a version of Bayes’ rule that incorporates negative mixing weights.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#random-variables",
    "href": "01-prob.html#random-variables",
    "title": "1  Probability and Uncertainty",
    "section": "1.3 Random Variables",
    "text": "1.3 Random Variables\nA random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.\nFor example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.\nWhile events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.\nRandom variables are quantities that we are not certain about. A random variable that can take a finite or a countable number of values is called a discrete random variable (number of rainy days next week). Otherwise, it will be a continuous random variable (amount of rain tomorrow).\nDiscrete random variables are often constructed by assigning specific values to events such as \\(\\{X=x\\}\\) which corresponds to the outcomes where \\(X\\) equals a specific number \\(x\\). For example\n\nWill a user click-through on a Google ad? (0 or 1)\nWho will win the 2024 elections? (Trump=1, Biden=2, Independent=3)\n\nTo fix notation, we will use \\(\\prob{X=x}\\) to denote the probability that random variable \\(X\\) is equal to \\(x\\). A map from all possible values \\(x\\) of a discrete random variable \\(X\\) to probabilities is called a probability mass function \\(p(x)\\). We will interchangeably use \\(\\prob{X=x}\\) and \\(p(x)\\). An important property of the probability mass function is that (normalization Kolmogorov axiom) \\[\n\\sum_{x\\in S} p(x) = 1.\n\\] Here \\(S\\) denotes the set of all possible values of random variable \\(X\\).\nClearly, all probabilities have to be greater than or equal to zero, so that \\(p(x)\\ge 0\\).\nFor a continuous random variable, the probability distribution is represented by a probability density function (PDF) \\(p(x)\\), which indicates the likelihood of the variable taking a specific value. Then to calculate probability of a variable falling within a particular range \\(a \\leq X \\leq b\\) we need to integrate the PDF over the range: \\[\n\\prob{a \\leq X \\leq b} = \\int_a^b p(x) dx.\n\\] Often, we are interested in \\(F(x) = P(X\\le x)\\), this is the cumulative distribution function (CDF).\nThe Cumulative Distribution Function (CDF) for a discrete random variable is a function that provides the probability that the random variable is less than or equal to a particular value. The CDF is a monotonically increasing function (never decreases as \\(x\\) increases). In other words, if \\(a \\leq b\\), then \\(F_X(a) \\leq F_X(b)\\). The value of the CDF always lies between 0 and 1, inclusive.\n\nExample 1.11 (Discrete CDF) Suppose \\(X\\) is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of \\(X\\) is:\n\\[\nP(X = x) = \\frac{1}{6}\n\\] for \\(x = 1, 2, 3, 4, 5, 6\\)\nThe CDF of \\(X\\), \\(F(x)\\), is calculated as follows:\n\nFor \\(x &lt; 1\\), \\(F(x) = 0\\) (since it’s impossible to roll less than 1).\nFor \\(1 \\leq x &lt; 2\\), \\(F(x) = \\frac{1}{6}\\) (the probability of rolling a 1).\nFor \\(2 \\leq x &lt; 3\\), \\(F(x) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\) (the probability of rolling a 1 or 2).\nThis pattern continues, adding \\(\\frac{1}{6}\\) for each integer interval up to 6.\nFor \\(x \\geq 6\\), \\(F(x) = 1\\) (since it’s certain to roll a number 6 or less).\n\nGraphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.\n\nplot(ecdf(1:6), main=\"\")\n\n\n\n\nCDF of a discrete random variable\n\n\n\n\n\n\nContinuous Random Variables\nIf we want to build a probabilistic model of a stock price or return, we need to use a continuous random variable that can take an interval of values. Instead of a frequency function we will use a density function, \\(p(x)\\) to describe a continuous variable. Unlike the discrete case, \\(p(x)\\) is not the probability that the random variable takes value \\(x\\). Rather, we need to talk about the value being inside an interval. For example, the probability of \\(X\\) with density \\(p(x)\\) being inside any interval \\([a,b]\\), with \\(a&lt;b\\) is given by \\[\nP(a &lt; X &lt; b) = \\int_{a}^{b}p(x)dx.\n\\] The total probability is one as \\(\\int_{-\\infty}^\\infty p(x) dx=1\\). The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then \\[\n  p(x) = \\begin{cases} 1/10, ~~~20 \\le x \\le 30\\\\0, ~~~\\mbox{otherwise}\\end{cases}\n\\]\nUnder this model, the probability of temperature being between 25 and 27 degrees is \\[\nP(25 \\le x \\le 27) = \\int_{25}^{27} p(x)dx = (27-25)/10 = 0.2\n\\]\n\n\n\nUniform Distribution: Probability of temperature being between 25 and 27\n\n\nThe Cumulative Distribution Function for a random variable \\(X\\), denoted as \\(F_X(x)\\). For a continuous random variable, it is defined similarly to discrete RV CDF as \\[\nF(x) = P(X \\leq x)\n\\] It is a non-decreasing function and takes values in [0,1].\n\nExample 1.12 (Continuous CDF for Uniform Distribution) \\[\np(x) = \\begin{cases}\n1 & \\text{if } 0 \\leq x \\leq 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe CDF, \\(F(x)\\), is obtained by integrating the PDF:\n\nFor \\(x &lt; 0\\), \\(F(x) = 0\\).\nFor \\(0 \\leq x \\leq 1\\), \\(F(x) = \\int_0^x 1 \\, dt = x\\).\nFor \\(x &gt; 1\\), \\(F(x) = 1\\).\n\nSo, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as \\(x\\) goes from 0 to 1.\nGraphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as \\(x\\) increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature. Figure below shows the CDF of a uniform and normal random variable, respectively.\nplot(ecdf(runif(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\nplot(ecdf(rnorm(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\n\n\n\n\n\n\nCDF of a uniform random variable\n\n\n\n\n\n\n\nCDF of a normal random variable\n\n\n\n\n\n\n\n\nThe Inverse CDF Method\nThe inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, \\(F\\). Since \\(F\\left(  x\\right)\\) is uniformly distributed on \\(\\left[ 0,1\\right]\\), draw a uniform random variable and invert the CDF to get a draw from \\(F\\). Thus, to sample from \\(F\\), \\[\\begin{align*}\n&  \\text{Step 1}\\text{: Draw }U\\sim U\\left[  0,1\\right]  \\ \\\\\n&  \\text{Step 2}\\text{: }\\text{Set }X=F^{-1}\\left(  U\\right)  ,\n\\end{align*}\\] where \\(F^{-1}\\left(  U\\right)  =\\inf\\left\\{  x:F\\left(  x\\right)  =U\\right\\}\\).\nThis inversion method provides i.i.d. draws from \\(F\\) provided that \\(F^{-1}\\left(  U\\right)\\) can be exactly calculated. For example, the CDF of an exponential random variable with parameter \\(\\mu\\) is \\(F\\left(  x\\right) =1-\\exp\\left(  -\\mu x\\right)\\), which can easily be inverted. When \\(F^{-1}\\) cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, \\(F\\left(  x\\right)\\) can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from \\(F\\). With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.\n\n\nFunctional Transformations\nThe second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that \\(X\\sim F\\), admitting a density \\(f\\), and that \\(y=h\\left(  x\\right)\\) is an increasing continuous function. Thus, we can define \\(x=h^{-1}\\left(  y\\right)\\) as the inverse of the function \\(h\\). The distribution of \\(y\\) is given by \\[\nF_Y\\left(y\\right)  =\\text{P}\\left(  Y\\leq y\\right)  =\\int_{-\\infty}^{h^{-1}\\left(  y\\right)  }f\\left(  x\\right)  dx=F_X\\left(  X\\leq h^{-1}\\left(y\\right)  \\right).\n\\] Differentiating with respect to \\(y\\) gives the density via Leibnitz’s rule: \\[\nf_{Y}\\left(  y\\right)  =f\\left(  h^{-1}\\left(  y\\right)  \\right)  \\left\\vert\\frac{d}{dy}\\left(  h^{-1}\\left(  y\\right)  \\right)  \\right\\vert,\n\\] where we make explicit that the density is over the random variable \\(Y\\). This result is used widely. For example, if \\(X\\sim\\mathcal{N}\\left(  0,1\\right)\\), then \\(Y=\\mu+\\sigma X\\). Since \\(x=h^{-1}\\left(  y\\right)  =\\frac{y-\\mu}{\\sigma}\\), the distribution function is \\(F\\left(  \\frac{x-\\mu}{\\sigma}\\right)\\) and density \\[\nf_{Y}\\left(  y\\right)  =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{y-\\mu}{\\sigma}\\right)  ^{2}\\right).\n\\] Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if \\(Y\\sim\\mathcal{X}^{2}\\left(  \\nu\\right)\\) and \\(\\nu\\) is an integer, then \\(Y=\\sum_{i=1}^{\\nu}X_{i}^{2}\\) where each \\(X_{i}\\) is independent standard normal. Exponential random variables can be used to simulate \\(\\mathcal{X}^{2}\\), Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be simulated via sums of squared vectors of standard normal random variables.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#expectation-and-variance-reward-and-risk",
    "href": "01-prob.html#expectation-and-variance-reward-and-risk",
    "title": "1  Probability and Uncertainty",
    "section": "1.4 Expectation and Variance (Reward and Risk)",
    "text": "1.4 Expectation and Variance (Reward and Risk)\nAn expected value of a random variable, denoted by \\(\\E{X}\\) is a weighted average. Each possible value of a random variable is weighted by its probability. For example, Google Maps uses expected value when calculating travel times. We might compute two different routes by their expected travel time. Typically, a forecast or expected value is all that is required — these expected values can be updated in real time as we travel. Say I am interested in travel time from Washington National airport to Fairfax in Virginia. The histogram below shows the travel times observed for a work day evening and were obtained from Uber.\n\nExample 1.13 (Uber) Let’s look at the histogram of travel times from Fairfax, VA to Washington, DC\n\nd = read.csv(\"../data/dc_travel_time.csv\") \n# use evening travel times (column 18) and convert from seconds to minutes \nevening_tt = d[,18]/60; day_tt = d[,15]/60; \nevening_tt = evening_tt[!is.na(evening_tt)] # remove missing observations \nhist(evening_tt, freq = F,main=\"\", xlab=\"Travel Time [min]\", nclass=20, col=\"lightblue\")\n\n\n\n\nTravel times in the evening\n\n\n\n\nFrom this dataset, we can empirically estimate the probabilities of observing different values of travel times\n\nbins = hist(evening_tt, breaks = 3, plot = F) \nknitr::kable(data.frame(\"tt\" = bins$mids, \"Probability\" = bins$counts/length(evening_tt)),col.names = c(\"Travel Time\",\"Probability\"),digits=2)\n\n\n\n\nTravel Time\nProbability\n\n\n\n\n18\n0.05\n\n\n22\n0.77\n\n\n28\n0.18\n\n\n\n\n\nThere is a small chance (5%) I can get to Washington, DC in 18 minutes, which probably happens on a holiday and a non-trivial chance (18%) to travel for 28 minutes, possibly due to a sports game or bad weather. Most of the time (77%) our travel time is 22 minutes. However, when Uber shows you the travel time, it uses the expected value as a forecast rather than the full distribution. Specifically, you will be given an expected travel time of 23 minutes.\n\n0.05*18 + 0.77*22 + 0.18*28\n\n## [1] 23\n\n\nIt is a simple summary that takes into account travel accidents and other events that can affect travel time as best as it can.\n\nThe expected value \\(\\E{X}\\) of discrete random variable \\(X\\) which takes possible values \\(\\{x_1,\\ldots x_n\\}\\) is calculated using\n\\[\n\\E{X} =\\sum_{i=1}^{n}x_i\\prob{X = x_i}\n\\]\nFor example, in a binary scenario, if \\(X\\in \\{0,1\\}\\) and \\(P(X=1)=p\\), then \\(\\E{X} = 0\\times(1-p)+1\\times p = p\\). The expected value of a Bernoulli random variable is simply the probability of success. In many binary scenarios, a probabilistic forecast is sufficient.\nIf \\(X\\) is continuous with probability distribution \\(p(x)\\), then we have to calculate the expectation as an integral \\[\n\\E{X} = \\int xp(x)d x  \\text{ and } \\int p(x)dx = 1.\n\\] When you have a random variable \\(x\\) that has a support that is non-negative (that is, the variable has nonzero density/probability for only positive values), you can use the following property: \\[\n\\E{X} = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x,\n\\] where \\(F(x)\\) is the cumulative distribution function (CDF) of \\(X\\). The proof is as follows: \\[\n\\E{X} = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x = \\int_0^\\infty \\int_x^\\infty f(y) \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^\\infty \\int_0^y \\,\\mathrm{d}x f(y) \\,\\mathrm{d}y = \\int_0^\\infty y f(y) \\,\\mathrm{d}y,\n\\] where \\(f(x)\\) is the probability density function (PDF) of \\(X\\).\nThere are cases, when the expectation does not exist. For example for a Cauchy distribution, the integral diverges, the area under the curve is infinite. In such cases, we say the expectation is undefined.\nWe can also calculate the expectation in terms of the quantile function (inverse CDF). \\[\n\\E{X} = \\int_{0}^1 F^{-1}(p) \\,\\mathrm{d}p,\n\\] where \\(F^{-1}(p) = \\inf\\{y \\in \\mathbb{R} \\mid F(y)\\ge p\\}\\) is the inverse CDF of \\(X\\). The proof is as follows: \\[\n\\E{X} = \\int_{0}^1 F^{-1}(p) \\,\\mathrm{d}p = \\int_{0}^1 \\int_{-\\infty}^{F^{-1}(p)} f(x) \\,\\mathrm{d}x \\,\\mathrm{d}p = \\int_{-\\infty}^{\\infty} \\int_{0}^{F(x)} \\,\\mathrm{d}p f(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} x f(x) \\,\\mathrm{d}x.\n\\] We will use this result later when we discuss quantile neural networks.\n\nStandard Deviation and Covariance\nVariance measures the spread of a random variable around its expected value \\(\\mu = \\E{X}\\). For a discrete random variable \\(X\\) with possible values \\(\\{x_1,\\ldots x_n\\}\\), we have \\[\n\\Var{X} = \\E{(X-\\mu)^2} =  \\sum_{i=1}^n (x_i-\\mu)^2 \\prob{X=x_i}.\n\\] In the continuous case, we have \\[\n\\Var{X} = \\int_{-\\infty}^\\infty (x-\\mu) ^2 p(x)dx,\n\\] where \\(\\mu = \\mathbb{E}(X)=\\int_{-\\infty}^{\\infty}p_X(x)dx\\). The standard deviation is more convenient and is the square root of variance \\(\\sd{X} = \\sqrt{\\Var{X}}\\). Standard deviation has the desirable property that it is measured in the same units as the random variable \\(X\\) itself and is a more useful measure.\nSuppose that we have two random variables \\(X\\) and \\(Y\\). We need to measure whether they move together or in opposite directions. The covariance is defined by \\[\n\\Cov{X,Y} = \\E{\\left[ (X- \\E{X})(Y- \\E{Y})\\right]}.\n\\]\nWhen \\(X\\) and \\(Y\\) are discrete and we are given the joint probability distribution, we need to calculate \\[\n\\Cov{X,Y} = \\sum_{x,y}  ( x - \\E{X} )(y - \\E{Y})p(x,y).\n\\] Covariance is measured in units of \\(X\\times\\)units of \\(Y\\). This can be inconvenient and makes it hard to compare covariances of different pairs of variables. A more convenient metric is the correlation, which is defined by \\[\n\\Cor{X,Y}= \\frac{ \\Cov{X,Y} }{ \\sd{X} \\sd{Y} }.\n\\] Correlation, \\(\\Cor{X,Y}\\), is unitless and takes values between -1 and 1.\nIn the case of joint continuous distribution it is convenient to use the covariance matrix \\(\\Sigma\\) which is defined as \\[\n\\Sigma = \\begin{bmatrix}\n\\Var{X} & \\Cov{X,Y} \\\\\n\\Cov{X,Y} & \\Var{Y}\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) are independent, then \\(\\Cov{X,Y} = 0\\) and \\(\\Sigma\\) is diagonal. The correlation matrix is defined as \\[\n\\rho = \\begin{bmatrix}\n1 & \\Cor{X,Y} \\\\\n\\Cor{X,Y} & 1\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) have an exact linear relationship, then \\(\\Cor{X,Y} = 1\\) and \\(\\Cov{X,Y}\\) is the product of standard deviations. In matrix notation, the relation between the covariance matrix and correlation matrix is given by \\[\n\\rho = \\mathrm{diag}\\left(\\Sigma\\right)^{-1/2} \\Sigma\\mathrm{diag}\\left(\\Sigma\\right)^{-1/2},\n\\] where \\(\\Sigma\\) is a diagonal matrix with standard deviations on the diagonal.\n\n\nPortfolios: linear combinations\nCalculating means and standard deviations of combinations of random variables is a central tool in probability. It is known as the portfolio problem. Let \\(P\\) be your portfolio, which comprises a mix of two assets \\(X\\) and \\(Y\\), typically stocks and bonds, \\[\nP = aX + bY,\n\\] where \\(a\\) and \\(b\\) are the portfolio weights, typically \\(a+b=1\\), as we are allocating our total capital. Imagine that you have placed \\(a\\) dollars on the random outcome \\(X\\), and \\(b\\) dollars on \\(Y\\). The portfolio \\(P\\) measures your total weighted outcome.\nKey portfolio rules: The expected value and variance follow the relations \\[\\begin{align*}\n\\E{aX + bY} = &      a\\E{X}+b\\E{Y}\\\\\n\\Var{ aX + bY }  = & a^2 \\Var{X} + b^2 \\Var{Y} + 2 ab \\Cov{X,Y },\n\\end{align*}\\] with covariance defined by \\[\n\\Cov{X,Y} = \\E{ ( X- \\E{X} )(Y- \\E{Y})}.\n\\] Expectation and variance help us to understand the long-run behavior. When we make long-term decisions, we need to use the expectations to avoid biases.\nThe covariance is related to the correlation by \\(\\Cov{X,Y} = \\text{Corr}(X, Y) \\cdot \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)}\\).\n\nExample 1.14 (Tortoise and Hare) Tortoise and Hare are selling cars. Say \\(X\\) is the number of cars sold and probability distributions, means and variances are given by the following table\n\n\n\n\n\\(X\\)\n\n\n\nMean\nVariance\nsd\n\n\n\n\n\n0\n1\n2\n3\n\\(\\E{X}\\)\n\\(\\Var{X}\\)\n\\(\\sqrt{\\Var{X}}\\)\n\n\nTortoise\n0\n0.5\n0.5\n0\n1.5\n0.25\n0.5\n\n\nHare\n0.5\n0\n0\n0.5\n1.5\n2.25\n1.5\n\n\n\nLet’s calculate Tortoise’s expectations and variances \\[\\begin{align*}\n\\E{T} & = (1/2) (1) + (1/2)(2) = 1.5 \\\\\n\\Var{T} & = \\E{T^2} - \\E{T}^2 \\\\\n& =  (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25\n\\end{align*}\\]\nNow the Hare’s \\[\\begin{align*}\n\\E{H} & = (1/2)(0) + (1/2)(3) = 1.5 \\\\\n\\Var{H} & =  (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25\n\\end{align*}\\]\nWhat do these tell us about the long run behavior?\n\nTortoise and Hare have the same expected number of cars sold.\nTortoise is more predictable than Hare. He has a smaller variance.\n\nThe standard deviations \\(\\sqrt{\\Var{X}}\\) are \\(0.5\\) and \\(1.5\\), respectively. Given two equal means, you always want to pick the lower variance. If we are to invest in one of those, we prefer Tortoise.\nWhat about a portfolio of Tortoise and Hare? Suppose I want to evenly split my investment between Tortoise and Hare. What is the expected number of cars sold and the variance of the number of cars sold? \\[\n\\E{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{2} \\E{T} + \\frac{1}{2} \\E{H} = 1.5\n\\] For variance, we need to know \\(\\Cov{T,H}\\). Let’s take \\(\\Cov{T,H} = -1\\) and see what happens. \\[\n\\Var{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{4} \\Var{T} + \\frac{1}{4} \\Var{H} + \\frac{1}{2} \\Cov{T,H} = 0.0625 + 0.5625 -0.5 = 0.125\n\\]\nNotice that the portfolio variance (0.125) is lower than both individual variances (0.25 and 2.25). This demonstrates the power of diversification - by combining investments with negative covariance, we can reduce overall risk while maintaining the same expected return. The negative covariance indicates that when Tortoise performs well, Hare tends to perform poorly, and vice versa, creating a natural hedge.\nThis example illustrates a fundamental principle in finance and decision theory: diversification can reduce risk without sacrificing expected returns when assets are not perfectly positively correlated. The key insight is that variance depends not only on individual asset volatilities but also on their covariances, making portfolio construction a crucial consideration in risk management.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#limiting-behavior-of-averages",
    "href": "01-prob.html#limiting-behavior-of-averages",
    "title": "1  Probability and Uncertainty",
    "section": "1.5 Limiting Behavior of Averages",
    "text": "1.5 Limiting Behavior of Averages\nWhen we observe a stochastic process repeatedly, we naturally ask: what happens to averages as we collect more data? This question lies at the heart of statistical inference and forms the theoretical foundation for learning from experience. The answer is provided by the law of large numbers, one of the most fundamental results in probability theory.\n\nThe Weak Law of Large Numbers\nThe weak law of large numbers, in its simplest form, states that sample averages converge in probability to the expected value. For independent and identically distributed (i.i.d.) random variables \\(X_1, X_2, \\ldots\\) with finite mean \\(\\mu = \\E{X_i}\\), the sample average \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\\) satisfies:\n\\[\n\\lim_{n \\to \\infty} P\\left(\\left|\\bar{X}_n - \\mu\\right| &gt; \\epsilon\\right) = 0\n\\]\nfor any \\(\\epsilon &gt; 0\\). This form of convergence, known as convergence in probability, means that for large enough \\(n\\), the probability that the sample average deviates from the true mean by more than any fixed amount becomes arbitrarily small.\nThe proof of this result, when the variance \\(\\sigma^2 = \\Var{X_i}\\) exists and is finite, follows elegantly from Chebyshev’s inequality. Since \\(\\E{\\bar{X}_n} = \\mu\\) and \\(\\Var{\\bar{X}_n} = \\sigma^2/n\\), we have:\n\\[\nP\\left(\\left|\\bar{X}_n - \\mu\\right| &gt; \\epsilon\\right) \\leq \\frac{\\Var{\\bar{X}_n}}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\to 0\n\\]\nas \\(n \\to \\infty\\). This simple argument reveals why averages become more reliable as sample sizes grow: the variance of the sample mean shrinks at rate \\(1/n\\).\n\n\nKolmogorov’s Strong Law of Large Numbers\nWhile the weak law establishes convergence in probability, a stronger form of convergence is possible. The strong law of large numbers states that sample averages converge almost surely (with probability one) to the expected value. This is a fundamentally stronger statement: it means that for almost every realization of the sequence, the sample average actually approaches the true mean, not merely that the probability of large deviations vanishes.\nAndrey Kolmogorov formalized this result in his groundbreaking 1933 monograph Foundations of the Theory of Probability (Kolmogoroff 1933), establishing the conditions under which strong convergence holds. His result revolutionized probability theory by providing a rigorous measure-theoretic foundation for probabilistic reasoning.\nKolmogorov’s Strong Law states that if \\(X_1, X_2, \\ldots\\) are independent random variables with \\(\\E{|X_i|} &lt; \\infty\\), then:\n\\[\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\to \\mu \\quad \\text{almost surely}\n\\]\nwhere \\(\\mu = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^n \\E{X_i}\\), provided this limit exists. For i.i.d. random variables with \\(\\E{|X_1|} &lt; \\infty\\), this simplifies to \\(\\bar{X}_n \\to \\E{X_1}\\) almost surely.\nThe distinction between convergence in probability and almost sure convergence is subtle but crucial. Convergence in probability allows for infinitely many large deviations, as long as they become increasingly rare. Almost sure convergence is stronger: it requires that eventually, after some finite (but random) time, all deviations remain small forever.\nFormally, almost sure convergence means:\n\\[\nP\\left(\\lim_{n \\to \\infty} \\bar{X}_n = \\mu\\right) = 1\n\\]\nThis is equivalent to saying that the set of sequences for which the limit fails to equal \\(\\mu\\) has probability zero. For practical purposes, this means we can be confident that the specific sequence we observe will exhibit convergence—not merely that convergence is likely.\nThe independence assumption is central to Kolmogorov’s results, but it can be relaxed in various ways for different applications. For stochastic processes with dependent observations, the key question becomes: how much dependence can we tolerate while still obtaining convergence of averages?\nFor stationary processes—where the joint distribution of \\((X_t, X_{t+1}, \\ldots, X_{t+k})\\) does not depend on \\(t\\)—a law of large numbers holds under considerably weaker conditions than independence. If the process is ergodic (roughly, if it eventually “forgets” its initial conditions), then time averages converge to ensemble averages:\n\\[\n\\frac{1}{n}\\sum_{i=1}^n X_i \\to \\E{X_1} \\quad \\text{almost surely}\n\\]\nThis ergodic theorem extends Kolmogorov’s law of large numbers to dependent sequences and has profound implications for statistical inference from time series data. It justifies the common practice of estimating population means from a single long realization of a stochastic process.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#binomial-poisson-and-normal-distributions",
    "href": "01-prob.html#binomial-poisson-and-normal-distributions",
    "title": "1  Probability and Uncertainty",
    "section": "1.6 Binomial, Poisson, and Normal Distributions",
    "text": "1.6 Binomial, Poisson, and Normal Distributions\nHaving explored specific examples of random variables and their properties through the Tortoise and Hare scenario, we now turn to examining some of the most fundamental and widely-used probability distributions. These distributions form the building blocks of probability theory and statistics, appearing repeatedly across diverse fields from finance and engineering to biology and social sciences. Understanding their properties—including their probability mass or density functions, expected values, and variances—is essential for modeling real-world phenomena and making informed decisions under uncertainty. In this section, we’ll introduce several key distributions, starting with the simple yet powerful Bernoulli distribution and building toward more complex models that capture different types of random behavior.\n\nBernoulli Distribution\nThe formal model of a coin toss was described by Bernoulli. He modeled the notion of probability for a coin toss, now known as the Bernoulli distribution, where \\(X \\in \\{0,1\\}\\) and \\(P(X=1)=p, P(X=0) = 1-p\\). Laplace gave us the principle of insufficient reason: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete uniform distribution on the set of possible outcomes.\nA Bernoulli trial relates to an experiment with the following conditions\n\nThe result of each trial is either a success or failure.\nThe probability \\(p\\) of a success is the same for all trials.\nThe trials are assumed to be independent.\n\nThe Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by \\(\\text{Bernoulli}(p)\\), where \\(p\\) is the probability of success.\nThe probability mass function (PMF) of a Bernoulli distribution is defined as follows: \\[\nP(X = x) = \\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\] The expected value (mean) of a Bernoulli distributed random variable \\(X\\) is given by: \\[\\E{X} = p\n\\] Simply speaking, if you are to toss a coin many times, you expect \\(p\\) heads.\nThe variance of \\(X\\) is given by: \\[\n\\Var{X} = p(1-p)\n\\]\n\nExample 1.15 (Coin Toss) The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is \\(S = \\{H,T\\}\\), and \\(p(X = H) = p(X = T) = 1/2\\). On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads \\(X\\) out of two coin tosses is\n\n\n\n\\(x\\)\n\\(p(x)\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nGiven the probability mass function we can, for example, calculate the probability of at least one head as \\(\\prob{X \\geq 1} = \\prob{X =1} + \\prob{X =2} = p(1)+p(2) = 3/4\\).\n\nThe Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to \\(X\\), which is the number of successes. Its probability distribution is calculated via: \\[\n\\prob{X=x} = {n \\choose x} p^x(1-p)^{n-x}.\n\\] Here \\({n \\choose x}\\) is the combinatorial function, \\[\n{n \\choose x} = \\frac{n!}{x!(n-x)!},\n\\] Here compbinatorial function \\({n \\choose x}\\) counts the number of ways of getting \\(x\\) successes in \\(n\\) trials and \\(n!=n(n-1)(n-2)\\ldots 2 \\cdot 1\\) counts the number of permutations of \\(n\\) observations without replacement.\nPlot below shows the probability mass function of a Binomial distribution with \\(n = 20\\) and \\(p = 0.3\\).\n\nbarplot(dbinom(0:20, size = 20, prob = 0.3), names.arg = 0:20, col = \"blue\",\n xlab = \"Number of Heads\", ylab = \"Probability\")\n\n\n\n\nProbability Mass Function of a Binomial Distribution (n = 20, p = 0.3)\n\n\n\n\nThe table below shows the expected value and variance of a Binomial random variable. Those quantities can be calculated by plugging in the possible outcomes ans corresponding probabilities into the definitions of expected value and variance.\n\nMean and Variance of Binomial\n\n\nBinomial Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = n p\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = n p ( 1 - p )\\)\n\n\n\nFor large sample sizes \\(n\\), this distribution is approximately normal with mean \\(np\\) and variance of \\(np(1-p)\\).\nSuppose we are about to toss two coins. Let \\(X\\) denote the number of heads. Then the following table specifies the probability distribution \\(p(x)\\) for all possible values \\(x\\) of \\(X\\). This leads to the following table\n\nOutcomes of two coin flips\n\n\n\\(x\\)\n\\(\\prob{X=x}\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nThus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely the Binomial distribution.\nLet \\(X\\) be the number of heads in three flips. Each possible outcome (“realization”) of \\(X\\) is an event. Now consider the event of getting only two heads \\[\n\\{ X= 2\\} = \\{ HHT, HTH, THH \\} ,\n\\] The probability distribution of \\(X\\) is Binomial with parameters \\(n = 3, p= 1/2\\), where \\(n\\) denotes the sample size (a.k.a. number of trials) and \\(p\\) is the probability of heads; we have a fair coin. The notation is \\(X \\sim \\mathrm{Bin} \\left ( n = 3 , p = \\frac{1}{2} \\right )\\) where the sign \\(\\sim\\) is read as distributed as.\n\nOutcomes of three coin flips\n\n\nResult\n\\(X\\)\n\\(\\prob{X=x}\\)\n\n\n\n\nHHH\n3\n\\(p^3\\)\n\n\nHHT\n2\n\\(p^2 ( 1- p)\\)\n\n\nHTH\n2\n\\(p^2 ( 1 - p)\\)\n\n\nTHH\n2\n\\((1-p)p^2\\)\n\n\nHTT\n1\n\\(p( 1-p)^2\\)\n\n\nTHT\n1\n\\(p ( 1-p)^2\\)\n\n\nTTH\n1\n\\((1-p)^2 p\\)\n\n\nTTT\n0\n\\((1-p)^3\\)\n\n\n\n\n\nPoisson Distribution\nThe Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. You can think of Poisson distribution as a limiting case of the Binomial distribution when the number of trials \\(n\\) is large and the probability of success \\(p\\) is small, such that \\(np = \\lambda\\) remains constant. Think of a soccer game where the goal is the “successful” event of interest. The soccer team does not have a predefined number of attempts to score a goal. Rather they continiously try to score a goal until they do. The number of goals scored in a game is the number of events occurring in a fixed interval of time. The mean number of goals scored in a game is the mean rate of events occurring in a fixed interval of time.\nThere are many examples of Poisson distributed random variables. For example, the number of phone calls received by a call center per hour, the number of emails received per day, the number of customers arriving at a store per hour, the number of defects in a manufactured product, the number of accidents on a highway per month.\nA random variable \\(X\\) follows a Poisson distribution with parameter \\(\\lambda &gt; 0\\) if its probability mass function is given by: \\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\] for \\(k = 0, 1, 2, 3, \\ldots\\), where \\(\\lambda\\) is both the mean and variance of the distribution.\nPlot below shows the probability mass function of a Poisson distribution with \\(\\lambda = 1, 5, 10\\).\n\n# Create x values\nx &lt;- 0:20\n\n# Plot the first distribution\nplot(x, dpois(x, lambda = 1), type = \"b\", col = \"blue\", lwd = 2,\n     xlab = \"Number of Events\", ylab = \"Probability\", \n     main = \"Poisson Distribution for Different Lambda Values\",\n     ylim = c(0, 0.4))\n\n# Add the other distributions\nlines(x, dpois(x, lambda = 5), col = \"red\", lwd = 2, type = \"b\")\nlines(x, dpois(x, lambda = 10), col = \"green\", lwd = 2, type = \"b\")\n\n# Add legend\nlegend(\"topright\", legend = c(\"lmb = 1\", \"lmb = 5\", \"lmb = 10\"), \n       col = c(\"blue\", \"red\", \"green\"), lwd = 2)\n\n\n\n\nProbability Mass Function of a Poisson Distribution\n\n\n\n\nWe write \\(X \\sim \\text{Poisson}(\\lambda)\\) to denote that \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\).\n\nMean and Variance of Poisson\n\n\nPoisson Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = \\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = \\lambda\\)\n\n\n\n\nExample 1.16 (Customer Arrivals) Suppose customers arrive at a coffee shop at an average rate of 3 customers per hour. What is the probability that exactly 5 customers will arrive in the next hour?\nUsing the Poisson distribution with \\(\\lambda = 3\\): \\[\nP(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\times e^{-3}}{120} \\approx 0.101\n\\]\nSo there is approximately a 10.1% chance that exactly 5 customers will arrive in the next hour.\n\nThe Poisson distribution can be derived as a limiting case of the binomial distribution when \\(n\\) is large and \\(p\\) is small, such that \\(np = \\lambda\\) remains constant. This connection makes the Poisson distribution particularly useful for modeling rare events in large populations.\n\n\nNormal Distribution\nThe Normal distribution is a continuous probability distribution that is widely used in statistics and probability theory. It is also known as the Gaussian distribution or the bell curve. The Normal distribution is characterized by its symmetric bell-shaped curve and is defined by two parameters: the mean (\\(\\mu\\)) and the variance (\\(\\sigma^2\\)).\nThe probability density function (PDF) of the Normal distribution is given by: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\] for \\(-\\infty &lt; x &lt; \\infty\\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\nThe plot below shows the probability density function of a random variable \\(X \\sim N(0,1)\\) that follows a Normal distribution with mean 0 and standard deviation 1.\n\nx &lt;- seq(-4, 4, length.out = 100)\nplot(x, dnorm(x, mean = 0, sd = 1), type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"x\", ylab = \"f(x)\",\n     main = \"Normal Distribution for Mean 0 and Standard Deviation 1\")\n\n\n\n\nProbability Density Function of a Normal Distribution\n\n\n\n\nThe Normal distribution is symmetric around the mean, and the mean, median, and mode are all equal.\nThe Normal distribution is often used to model real-world phenomena such as measurement errors, heights, weights, and scores on standardized tests. It is also used in hypothesis testing and confidence interval construction.\n\nExample 1.17 (Heights of Adults) The heights of adult males in a certain population are normally distributed with a mean of 70 inches and a standard deviation of 3 inches.\n\nWhat is the probability that a randomly selected male is between 67 and 73 inches tall?\n\nUsing the Normal distribution with \\(\\mu = 70\\) and \\(\\sigma = 3\\):\n\n\nCode\n# Define parameters\nmu &lt;- 70\nsigma &lt;- 3\n\n# Create x values for the curve\nx &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\n\n# Plot the normal curve\nplot(x, y, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Height (inches)\", ylab = \"Density\",\n     main = \"Normal Distribution (mu = 70, sigma = 3)\")\n\n# Add vertical lines at 67 and 73\nabline(v = 67, col = \"red\", lty = 2, lwd = 2)\nabline(v = 73, col = \"red\", lty = 2, lwd = 2)\n\n# Highlight the area between 67 and 73\nx_area &lt;- seq(67, 73, length.out = 100)\ny_area &lt;- dnorm(x_area, mean = mu, sd = sigma)\npolygon(c(67, x_area, 73), c(0, y_area, 0), \n         col = rgb(0, 0, 1, alpha = 0.3), border = NA)\n\n# Add labels\ntext(67, -0.005, \"67\", col = \"red\", cex = 1.2)\ntext(73, -0.005, \"73\", col = \"red\", cex = 1.2)\ntext(70, 0.06, \"P(67 &lt; X &lt; 73)\", cex = 1.2, col = \"darkblue\")\n\n# Add grid for better readability\ngrid(col = \"gray\", lty = 3)\n\n\n\n\n\nNormal Distribution with Highlighted Area\n\n\n\n\nThe prrobability of a randomly selected male being between 67 and 73 inches tall is the area under the curve between 67 and 73 inches and is approximately 0.6827.\nNow let’s calculate this probability using R:\n\nprob_between &lt;- pnorm(73, mean = 70, sd = 3) - pnorm(67, mean = 70, sd = 3)\ncat(\"P(67 &lt; X &lt; 73) =\", round(prob_between, 4), \"\\n\")\n\n## P(67 &lt; X &lt; 73) = 0.68\n\n\nThe calculation shows that \\(P(67 &lt; X &lt; 73) = P(X \\leq 73) - P(X \\leq 67) \\approx 0.6827\\).\nThis result makes sense because 67 and 73 inches are exactly one standard deviation below and above the mean (70 ± 3), respectively. According to the empirical rule (68-95-99.7 rule), approximately 68% of values in a normal distribution fall within one standard deviation of the mean.\nSo approximately 68.27% of adult males are between 67 and 73 inches tall.\n\nWhat height corresponds to the 95th percentile?\n\nWe need to find \\(x\\) such that \\(P(X \\leq x) = 0.95\\). From standard normal tables, \\(\\Phi^{-1}(0.95) \\approx 1.645\\).\n\nx &lt;- qnorm(0.95, mean = 70, sd = 3)\ncat(\"Height corresponding to the 95th percentile:\", round(x, 2), \"inches\\n\")\n\n## Height corresponding to the 95th percentile: 75 inches\n\n\nTherefore: \\(x = \\mu + \\sigma \\cdot 1.645 = 70 + 3 \\cdot 1.645 = 74.935\\) inches.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#conditional-marginal-and-joint-distributions",
    "href": "01-prob.html#conditional-marginal-and-joint-distributions",
    "title": "1  Probability and Uncertainty",
    "section": "1.7 Conditional, Marginal and Joint Distributions",
    "text": "1.7 Conditional, Marginal and Joint Distributions\nSuppose that we have two random variables \\(X\\) and \\(Y\\), which can be related to each other. Knowing \\(X\\) would change your belief about \\(Y\\). For example, as a first pass, psychologists who study the phenomenon of happiness can be interested in understanding its relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.\nFormally, the joint distribution of two variables \\(X\\) and \\(Y\\) is a function given by \\[\np(x,y) = \\prob{X=x,Y=y}.\n\\] This maps all combinations of possible values of these two variables to a probability on the interval [0,1].\nThe conditional probability is a measure of the probability of a random variable \\(X\\), given that the value of another random variable was observed \\(Y = y\\). \\[\np(x\\mid y) = \\prob{X = x \\mid Y = y}.\n\\]\nThe marginal probability of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables \\(X\\) and \\(Y\\), the marginal probability \\(\\prob{X}\\) is the probability distribution of \\(X\\) when the values of \\(Y\\) are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of \\(Y\\). The converse is also true: the marginal distribution can be obtained for \\(Y\\) by summing over the separate values of \\(X\\).\nMarginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.\n\nExample 1.18 (Salary-Happiness) Let’s look at an example. Suppose that to model the relationship between two quantities, salary \\(Y\\) and happiness \\(X\\). After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.\n\n\n\nResults of the Gallup survey. Rows are Salary (\\(Y\\)) and columns are happiness (\\(X\\))\n\n\n\nX = 0 (low)\nX = 1 (medium)\nX = 2 (high)\n\n\n\n\nY = low (0)\n0.03\n0.13\n0.14\n\n\nY = medium (1)\n0.12\n0.11\n0.01\n\n\nY = high (2)\n0.07\n0.01\n0.09\n\n\nY = very high (3)\n0.02\n0.13\n0.14\n\n\n\n\n\nEach cell of the table is the joint probability, e.g. 14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.\nNow, if we want to answer the question what is the percent of high earners in the population. For that we need to calculate what is called a marginal probability \\(\\prob{y = 2}\\). We can calculate the proportion of high earners \\(\\prob{y = 2}\\) by summing up the entries in the third row of the table, which is 0.17 in our case.\n\n0.07 + 0.01 + 0.09\n\n## [1] 0.17\n\n\nFormally marginal probability over \\(y\\) is calculated by summing the joint probability over the other variable, \\(x\\), \\[\np(y) = \\sum_{x \\in S}p(x,y)\n\\] Where \\(S\\) is the set of all possible values of the random variable \\(X\\).\n\n\n\n\n\n\nAnother question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the conditional probability of \\(X\\) given that the value of variable \\(Y\\) is known. This is denoted by \\(\\prob{X=x\\mid Y=y}\\) or simply \\(p(x\\mid y)\\), where \\(\\mid\\) reads as “given” or “conditional upon”.\nThe conditional probability \\(p(x\\mid y)\\) also has interpretation as updating your probability over \\(X\\) after you have learned the new information about \\(Y\\). In this sense, probability is also the language of how you change opinions in light of new evidence. The proportion of happy people among high earners is given by the conditional probability \\(\\prob{X=2\\mid Y=2}\\) and can be calculated by dividing the proportion of those who are high earners and highly happy by the proportion of high earners \\[\n\\prob{X=2\\mid Y=2} = \\dfrac{\\prob{X=2,Y=2}}{\\prob{Y=2}} = \\dfrac{0.09}{0.17} = 0.5294118.\n\\]\nNow, if we compare it with the proportion of highly happy people \\(\\prob{X = 2} = 0.38\\), we see that on average you are more likely to be happy given your income is high.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#independence",
    "href": "01-prob.html#independence",
    "title": "1  Probability and Uncertainty",
    "section": "1.8 Independence",
    "text": "1.8 Independence\nHistorically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.\nUsing the notion of conditional probability, we can define independence of two variables. Two random variables \\(X\\) and \\(Y\\) are said to be independent if \\[\n\\prob{Y = y \\mid X = x} = \\prob{Y = y},\n\\] for all possible \\(x\\) and \\(y\\) values. That is, learning information \\(X=x\\) doesn’t affect our probabilistic assessment of \\(Y\\) for any value \\(y\\). This is known as the Prosecutor’s Fallacy as it arises when probability is used as evidence in a court of law. In the case of independence, \\(p(x \\mid y) = p(x)\\) and \\(p(y \\mid x) = p(y)\\). Specifically, the probability of innocence given the evidence is not the same as the probability of evidence given innocence. It is very important to ask the question “what exactly are we conditioning on?” Usually, the observed evidence or data. Probability, of course, given evidence was one of the first applications of Bayes. Central to personalized probability. Clearly this is a strong condition and rarely holds in practice.\nConditional probabilities are counter-intuitive. For example, one of the most important properties is typically \\(p( x \\mid y ) \\neq p( y\\mid x )\\).\nWe just derived an important relation that allows us to calculate conditional probability \\(p(x \\mid y)\\) when we know joint probability \\(p(x,y)\\) and marginal probability \\(p(y)\\). The total probability or evidence can be calculated as usual, via \\(p(y) = \\sum_{x}p(x,y)\\).\nWe will see that independence will lead to a different conclusion than the Bayes conditional probability decomposition: specifically, independence yields \\(p( x,y ) = p(x) p(y)\\) and Bayes says \\(p(x ,y) = p(x)p(y \\mid x)\\).\nWe need to specify a distribution on each of those variables. Two random variables \\(X\\) and \\(Y\\) are independent if \\[\n\\prob{Y = y \\mid X = x} = \\prob{Y = y},\n\\] for all possible \\(x\\) and \\(y\\) values. The joint distribution will be given by \\[\np(x,y) = p(x)p(y).\n\\] If \\(X\\) and \\(Y\\) are independent then the probability of event \\(X\\) and event \\(Y\\) happening 00 is the product of individual probabilities. From the conditional distribution formula it follows that \\[\np(x \\mid y) = \\dfrac{p(x,y)}{p(y)} = \\dfrac{p(x)p(y)}{p(y)} = p(x).\n\\] Another way to think of independence is to say that knowing the value of \\(Y\\) doesn’t tell us anything about possible values of \\(X\\). For example when tossing a coin twice, the probability of getting \\(H\\) in the second toss does not depend on the outcome of the first toss.\nThe expression of independence expresses the fact that knowing \\(X=x\\) tells you nothing about \\(Y\\). In the coin tossing example, if \\(X\\) is the outcome of the first toss and \\(Y\\) is the outcome of the second toss \\[\n\\prob{ X=H  \\mid  Y=T } = \\prob{X=H  \\mid  Y=H } = \\prob{X=H}.\n\\]\n\n\n\n\nDiaconis, Persi, and Frederick and Mosteller. 1989. “Methods for Studying Coincidences.” Journal of the American Statistical Association 84 (408): 853–61.\n\n\nFinetti, Bruno de. 1937. “Foresight: Its Logical Laws, Its Subjective Sources.” Annales de l’Institut Henri Poincaré 7 (1): 1–68.\n\n\nKolmogoroff, Andrei. 1933. Grundbegriffe Der Wahrscheinlichkeitsrechnung. Vol. 2. Ergebnisse Der Mathematik Und Ihrer Grenzgebiete. Berlin: Springer.\n\n\nKolmogorov, Andrey N. 1933. Grundbegriffe Der Wahrscheinlichkeitsrechnung. Berlin: Springer.\n\n\nPolson, Nick, and Vadim Sokolov. 2025. “Negative Probability.” Applied Stochastic Models in Business and Industry 41 (1): e2910.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” The Foundations of Mathematics and Other Logical Essays, 156–98.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "02-bayes.html",
    "href": "02-bayes.html",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "2.1 Law of Total Probability\nOne of the key questions in the theory of learning is: How do you update your beliefs in the presence of new information? Bayes rule provides the answer. Conditional probability can be interpreted as updating your probability of event \\(A\\) after you have learned the new information that \\(B\\) has occurred. In this sense probability is also the language of how you’ll change options in the light of new evidence. For example, we need to find the probability that a thrown die shows on its upper surface an odd number and we found out that the number shown is less than 4.\nProbability rules allow us to change our mind if the facts change. For example, suppose that we have evidence \\(E = \\{ E_1 , E_2 \\}\\) consists of two pieces of information and that we are interested in identifying a cause \\(C\\), \\(P(C\\mid E_1,E_2)\\). Bayes rule simply lets you calculate this conditional probability in a sequential fashion. First, conditioning on the information contained in \\(E_1\\), lets us calculate \\[\nP( C| E_1 ) = \\frac{ p(  E_1 \\mid C ) P( C) }{ P( E_1 ) }\n\\] Then, using the posterior probability \\(P( C| E_1 )\\) as the “new” prior for the next piece of information \\(E_2\\) lets us find \\[\nP( C| E_1 , E_2 ) = \\frac{ p(  E_2 \\mid E_1 , C ) P( C \\mid E_1 ) }{ P( E_2 \\mid E_1 ) }\n\\] Hence, we see that we need assessments of the two conditional probabilities \\(P( E_1 \\mid C )\\) and \\(P( E_2 \\mid E_1 , C )\\). In many situations, the latter will be simply \\(P( E_2 \\mid C )\\) and not involve \\(E_1\\). The events \\(( E_1, E_2 )\\) will be said to be conditionally independent given \\(C\\).\nThis concept generalizes to a sequence of events where \\(E = \\{ E_1,\\ldots E_n \\}\\). When learning from data we will use this property all the time. An illustrative example will be the Black Swan problem which we discuss later.\nBayes’ rule is a fundamental concept in probability theory and statistics. It describes how to update our beliefs about an event based on new evidence. We start with an initial belief about the probability of an event (called the prior probability). We then observe some conditional information (e.g. evidence). We use Bayes’ rule to update our initial belief based on the evidence, resulting in a new belief called the posterior probability. Remember, the formula is \\[\nP(A\\mid B) = \\dfrac{P(B\\mid A) P(A)}{P(B)}\n\\] where:\nThe ability to use Bayes rule sequentially is key in many applications, when we need to update our beliefs in the presence of new information. For example, Bayesian learning was used by mathematician Alan Turing in England at Bletchley Park to break the German Enigma code - a development that helped the Allies win the Second World War (Simpson 2010). Turing called his algorithm Banburismus, it is a process he invented which used sequential conditional probability to infer information about the likely settings of the Enigma machine.\nDennis Lindley argued that we should all be trained in Bayes rule and conditional probability can be simply viewed as disciplined probability accounting. Akin to how market odds change as evidence changes. One issue is human behavior and intuition is not well trained in how to use Bayes rule, akin to learning the Alphabet!\nThe Law of Total Probability is a fundamental rule relating marginal probabilities to conditional probabilities. It’s particularly useful when you’re dealing with a set of mutually exclusive and collectively exhaustive events.\nSuppose you have a set of events \\(B_1, B_2, ..., B_n\\) that are mutually exclusive (i.e., no two events can occur at the same time) and collectively exhaustive (i.e., at least one of the events must occur). The Law of Total Probability states that for any other event \\(A\\), the probability of \\(A\\) occurring can be calculated as the sum of the probabilities of \\(A\\) occurring given each \\(B_i\\), multiplied by the probability of each \\(B_i\\) occurring.\nMathematically, it is expressed as:\n\\[\nP(A) = \\sum_{i=1}^{n} P(A\\mid  B_i) P(B_i)\n\\]\nThis law is particularly useful in complex probability problems where direct calculation of probability is difficult. By breaking down the problem into conditional probabilities based on relevant events, it simplifies the calculation and helps to derive a solution.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#law-of-total-probability",
    "href": "02-bayes.html#law-of-total-probability",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "Example 2.2 (Total Probability) Let’s consider a simple example to illustrate this. Suppose you have two bags of balls. Bag 1 contains 3 red and 7 blue balls, while Bag 2 contains 6 red and 4 blue balls. You randomly choose one of the bags and then randomly draw a ball from that bag. What is the probability of drawing a red ball?\nHere, the events \\(B_1\\) and \\(B_2\\) can be choosing Bag 1 and Bag 2, respectively. You want to find drawing a red ball (event \\(A\\)).\nApplying the law:\n\n\\(P(A|B_1)\\) is the probability of drawing a red ball from Bag 1, which is \\(\\frac{3}{10}\\).\n\\(P(A|B_2)\\) is the probability of drawing a red ball from Bag 2, which is \\(\\frac{6}{10}\\).\nAssume the probability of choosing either bag is equal, so \\(P(B_1) = P(B_2) = \\frac{1}{2}\\).\n\nUsing the Law of Total Probability:\n\\[\nP(A) = P(A|B_1) \\times P(B_1) + P(A|B_2) \\times P(B_2)= \\frac{3}{10} \\times \\frac{1}{2} + \\frac{6}{10} \\times \\frac{1}{2} = \\frac{9}{20}\n\\]\nSo, the probability of drawing a red ball in this scenario is \\(\\frac{9}{20}\\).\n\n\n\nExample 2.3 (Craps) Craps is a fast-moving dice game with a complex betting layout. It’s highly volatile, but eventually your bankroll will drift towards zero. Let’s look at the pass line bet. The expectation \\(E(X)\\) governs the long run. When 7 or 11 comes up, you win. When 2, 3 or 12 comes up, this is known as “craps”, you lose. When 4, 5, 6, 8, 9 or 10 comes up, this number is called the “point”, the bettor continues to roll until a 7 (you lose) or the point comes up (you win).\nWe need to know the probability of winning. The pay-out, probability and expectation for a $1 bet\n\n\n\nWin\nProb\n\n\n\n\n1\n0.4929\n\n\n-1\n0.5071\n\n\n\nThis leads to an edge in favor of the house as \\[\nE(X) = 1 \\cdot 0.4929 + (- 1) \\cdot  0.5071 = -0.014\n\\] The house has a 1.4% edge.\nTo calculate the probability of winning: \\(P( \\text{Win} )\\) let’s use the law of total probability \\[\nP( \\text{Win} ) = \\sum_{ \\mathrm{Point} } P ( \\text{Win} \\mid \\mathrm{Point} ) P ( \\mathrm{Point} )\n\\] The set of \\(P( \\mathrm{Point} )\\) are given by\n\n\n\nValue\nProbability\nPercentage\n\n\n\n\n2\n1/36\n2.78%\n\n\n3\n2/36\n5.56%\n\n\n4\n3/36\n8.33%\n\n\n5\n4/36\n11.1%\n\n\n6\n5/36\n13.9%\n\n\n7\n6/36\n16.7%\n\n\n8\n5/36\n13.9%\n\n\n9\n4/36\n11.1%\n\n\n10\n3/36\n8.33%\n\n\n11\n2/36\n5.56%\n\n\n12\n1/36\n2.78%\n\n\n\nThe conditional probabilities \\(P( \\text{Win} \\mid \\mathrm{Point} )\\) are harder to calculate \\[\nP( \\text{Win} \\mid 7 \\; \\mathrm{or} \\; 11 ) = 1 \\; \\; \\mathrm{and} \\; \\; P( \\text{Win} \\mid 2 ,\n3 \\; \\mathrm{or} \\; 12 ) = 0\n\\] We still have to work out all the probabilities of winning given the point. Suppose the point is \\(4\\) \\[\nP( \\text{Win} \\mid 4 ) = P ( 4 \\; \\mathrm{before} \\; 7 ) = \\dfrac{P(4)}{P(7)+P(4)} = \\frac{3}{9} =\n\\frac{1}{3}\n\\] There are 6 ways of getting a 7, 3 ways of getting a 4 for a total of 9 possibilities. Now do all of them and sum them up. You get \\[\nP( \\text{Win}) = 0.4929\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#naive-bayes",
    "href": "02-bayes.html#naive-bayes",
    "title": "2  Bayes Rule",
    "section": "2.2 Naive Bayes",
    "text": "2.2 Naive Bayes\nUse of the Bayes rule allows us to build our first predictive model, called Naive Bayes classifier. Naive Bayes is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n\n\n\n\n\n\n\ngraph TB\n    C((Color)) --&gt; F((Fruit))\n    S((Size)) --&gt; F\n    Sh((Shape)) --&gt; F\n\n\n\n\n\n\nAlthough it’s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification. In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features). In summary, the advantages are:\n\nIt’s relatively simple to understand and build\nIt’s easily trained, even with a small dataset\nIt’s fast!\nIt’s not sensitive to irrelevant features\n\nThe main disadvantage is that it assumes every feature is independent, which isn’t always the case.\nLet’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below:\n\n\n\nFruit\nLong\nSweet\nYellow\nTotal\n\n\n\n\nBanana\n400\n350\n450\n500\n\n\nOrange\n0\n150\n300\n300\n\n\nOther\n100\n150\n50\n200\n\n\nTotal\n500\n650\n800\n1000\n\n\n\nFrom this data we can calculate marginal probabilities\n\n50% of the fruits are bananas\n30% are oranges\n20% are other fruits\n\nBased on our training set we can also say the following:\n\nFrom 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\nOut of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\nFrom the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.\n\nGiven the evidence \\(E\\) (\\(L\\) = Long, \\(S\\) = Sweet and \\(Y\\) = Yellow) we can calculate the probability of each class \\(C\\) (\\(B\\) = Banana, \\(O\\) = Orange or \\(F\\) = Other Fruit) using Bayes’ Theorem: \\[\\begin{align*}\nP(B \\mid E) = & \\frac{P(L \\mid B)P(S \\mid B)P(Y \\mid B)P(B)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.8\\times 0.7\\times 0.9\\times 0.5}{P(E)}=\\frac{0.252}{P(E)}\n\\end{align*}\\]\nOrange: \\[\nP(O\\mid E)=0.\n\\]\nOther Fruit: \\[\\begin{align*}\nP(F \\mid E) & = \\frac{P(L \\mid F)P(S \\mid F)P(Y \\mid F)P(F)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.5\\times 0.75\\times 0.25\\times 0.2}{P(E)}=\\frac{0.01875}{P(E)}\n\\end{align*}\\]\nIn this case, based on the higher score, we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.\nNotice, we did not have to calculate \\(P(E)\\) because it is a normalizing constant and it cancels out when we calculate the ratio\n\\[\n\\dfrac{P(B \\mid E)}{P(F \\mid E)} = 0.252/0.01875 = 13.44 &gt; 1.\n\\]\nNow that we’ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g. “spam” and “ham” in case of spam detection) as classes we can start to classify documents and text automatically.\n\nExample 2.4 (Spam Filtering) The original spam filtering algorithm was based on Naive Bayes. The “naive” aspect of Naive Bayes comes from the assumption that inputs (words in the case of text classification) are conditionally independent, given the class label. Naive Bayes treats each word independently, and the model doesn’t capture the sequential or structural information inherent in the language. It does not consider grammatical relationships or syntactic structures. The algorithm doesn’t understand the grammatical rules that dictate how words should be combined to form meaningful sentences. Further, it doesn’t understand the context in which words appear. For example, it may treat the word “bank” the same whether it refers to a financial institution or the side of a river bank. Despite its simplicity and the naive assumption, Naive Bayes often performs well in practice, especially in text classification tasks.\nWe start by collecting a dataset of emails labeled as “spam” or “not spam” (ham) and calculate the prior probabilities of spam (\\(P(\\text{spam})\\)) and not spam (\\(P(\\text{ham})\\)) based on the training dataset, by simply counting the proportions of each in the data.\nThen each email gets converted into a bag-of-words representation (ignoring word order and considering only word frequencies). Then, we create a vocabulary of unique words from the entire dataset \\(w_1,w_2,\\ldots,w_N\\) and calculate conditional probabilities \\[\nP(\\mathrm{word}_i  \\mid  \\text{spam}) = \\frac{\\text{Number of spam emails containing }\\mathrm{word}_i}{\\text{Total number of spam emails}}, ~ i=1,\\ldots,n\n\\] \\[\nP(\\mathrm{word}_i  \\mid  \\text{ham}) = \\frac{\\text{Number of ham emails containing }\\mathrm{word}_i}{\\text{Total number of ham emails}}, ~ i=1,\\ldots,n\n\\]\nNow, we are ready to use our model to classify new emails. We do it by calculating the posterior probability using Bayes’ theorem. Say an email has a set of \\(k\\) words \\(\\text{email} = \\{w_{e1},w_{e2},\\ldots, w_{ek}\\}\\), then \\[\nP(\\text{spam}  \\mid  \\text{email}) = \\frac{P(\\text{email}  \\mid  \\text{spam}) \\times P(\\text{spam})}{P(\\text{email})}\n\\] Here \\[\nP(\\text{email}  \\mid  \\text{spam}) = P( w_{e1}  \\mid  \\text{spam})P( w_{e2}  \\mid  \\text{spam})\\ldots P( w_{ek}  \\mid  \\text{spam})\n\\] We calculate \\(P(\\text{ham} \\mid \\text{email})\\) in a similar way.\nFinally, we classify the email as spam or ham based on the class with the highest posterior probability.\nSuppose you have a spam email with the word “discount” appearing. Using Naive Bayes, you’d calculate the probability that an email containing “discount” is spam \\(P(\\text{spam} \\mid \\text{discount})\\) and ham \\(P(\\text{ham} \\mid \\text{discount})\\), and then compare these probabilities to make a classification decision.\nWhile the naive assumption simplifies the model and makes it computationally efficient, it comes at the cost of a more nuanced understanding of language. More sophisticated models, such as transformers, have been developed to address these limitations by considering the sequential nature of language and capturing contextual relationships between words.\nIn summary, naive Bayes, due to its simplicity and the naive assumption of independence, is not capable of understanding the rules of grammar, the order of words, or the intricate context in which words are used. It is a basic algorithm suitable for certain tasks but may lack the complexity needed for tasks that require a deeper understanding of language structure and semantics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#real-world-bayes",
    "href": "02-bayes.html#real-world-bayes",
    "title": "2  Bayes Rule",
    "section": "2.3 Real World Bayes",
    "text": "2.3 Real World Bayes\n\nExample 2.5 (Google random clicker) Google provides a service where they ask visitors to your website to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories\n\nRandom Clicker (RC)\nTruthful Clicker (TC)\n\nThere are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also given the information that the expected fraction of random clickers is 0.3. After a trial period, you get the following survey results: 65% said Yes and 35% said No.\nThe question is: How many people who are truthful clickers answered yes \\(P(Y\\mid TC)\\)?\nWe are given \\(P(Y\\mid RC) = P(N\\mid RC) = 0.5\\), \\(P(RC)= 0.3\\) and \\(P(Y)\\) = 0.65\nThe total probability is \\[\nP(Y) = P(Y\\mid RC)P(RC) + P(Y\\mid TC)P(TC) = 0.65,\n\\] Thus \\[\nP(Y\\mid TC) = (P(Y) - P(Y\\mid RC)P(RC))/P(TC) = (0.65-0.5\\cdot 0.3)/0.7 = 0.71\n\\]\n\n\nExample 2.6 (USS Scorpion sank 5 June, 1968 in the middle of the Atlantic.) Experts placed bets on each casualty and how each would affect the sinking. Undersea soundings gave a prior on location. Bayes rule: \\(L\\) is location and \\(S\\) is scenario \\[\np (L \\mid S) = \\frac{ p(S \\mid L) p(L)}{p(S)}\n\\] The Navy spent \\(5\\) months looking and found nothing. Built a probability map: within \\(5\\) days, the submarine was found within \\(220\\) yards of the most likely probability!\nA similar story happened during the search of an Air France plane that flew from Rio to Paris.\n\n\nExample 2.7 (Wald and Airplane Safety) Many lives were saved by analysis of conditional probabilities performed by Abraham Wald during the Second World War. He was analyzing damages on the US planes that came back from bombing missions in Germany. Somebody suggested to analyze the distribution of the hits over different parts of the plane. The idea was to find a pattern in the damages and design a reinforcement strategy.\nAfter examining hundreds of damaged airplanes, researchers came up with the following table\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n53\n\n\nCockpit\n65\n\n\nFuel system\n96\n\n\nWings, fuselage, etc.\n434\n\n\n\nWe can convert those counts to probabilities\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n0.08\n\n\nCockpit\n0.1\n\n\nFuel system\n0.15\n\n\nWings, fuselage, etc.\n0.67\n\n\n\nWe can conclude that the most likely area to be damaged on the returned planes was the wings and fuselage. \\[\n\\prob{\\mbox{hit on wings or fuselage } \\mid \\mbox{returns safely}} = 0.67\n\\] Wald realized that analyzing damages only on survived planes is not the right approach. Instead, he suggested that it is essential to calculate the inverse probability \\[\n\\prob{\\mbox{returns safely} \\mid \\mbox{hit on wings or fuselage }} = ?\n\\] To calculate that, he interviewed many engineers and pilots, he performed a lot of field experiments. He analyzed likely attack angles. He studied the properties of a shrapnel cloud from a flak gun. He suggested to the army that they fire thousands of dummy bullets at a plane sitting on the tarmac. Wald constructed a ‘probability model’ carefully to reconstruct an estimate for the joint probabilities. The table below shows the results.\n\n\n\nHit\nReturned\nShot Down\n\n\n\n\nEngine\n53\n57\n\n\nCockpit\n65\n46\n\n\nFuel system\n96\n16\n\n\nWings, fuselage, etc.\n434\n33\n\n\n\nWhich allows us to estimate joint probabilities, for example \\[\n\\prob{\\mbox{outcome = returns safely} , \\mbox{hit  =  engine }} = 53/800 = 0.066\n\\] We also can calculate the conditional probabilities now \\[\n\\prob{\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  wings or fuselage  }} = \\dfrac{434}{434+33} = 0.9293362.\n\\] Should we reinforce wings or fuselage? Which part of the airplane needs to be reinforced? \\[\n\\prob{\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  engine  }} = \\dfrac{53}{53+57} = 0.48\n\\] Here is another illustration taken from the Economics literature. This insight led to George Akerlof winning the Nobel Prize for the concept of asymmetric information.\n\n\nExample 2.8 (Coin Jar) Large jar containing 1024 fair coins and one two-headed coin. You pick one at random and flip it \\(10\\) times and get all heads. What’s the probability that the coin is the two-headed coin? The probability of initially picking the two headed coin is 1/1025. There is a 1/1024 chance of getting \\(10\\) heads in a row from a fair coin. Therefore, it’s a \\(50/50\\) bet.\nLet’s do the formal Bayes rule math. Let \\(E\\) be the event that you get \\(10\\) Heads in a row, then\n\\[\nP \\left ( \\mathrm{two \\; headed}  \\mid  E \\right ) = \\frac{ P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left (  \\mathrm{ two \\; headed} \\right )}\n{P \\left ( E  \\mid  \\mathrm{ fair}  \\right )P \\left ( \\mathrm{ fair} \\right ) + P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left ( \\mathrm{ two \\; headed} \\right )}\n\\] Therefore, the posterior probability \\[\nP \\left (  \\mathrm{ two \\; headed}  \\mid  E \\right ) = \\frac{ 1 \\times \\frac{1}{1025} }{ \\frac{1}{1024} \\times \\frac{1024}{1025} + 1 \\times \\frac{1}{1025} } = 0.50\n\\] What’s the probability that the next toss is a head? Using the law of total probability gives\n\\[\\begin{align*}\n  P( H ) &= P( H  \\mid  \\mathrm{ two \\; headed} )P( \\mathrm{ two \\; headed}  \\mid E ) +  P( H  \\mid  \\mathrm{ fair} )P( \\mathrm{ fair}  \\mid E) \\\\\n  & = 1 \\times \\frac{1}{2} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{3}{4}\n\\end{align*}\\]\n\n\nExample 2.9 (Monty Hall Problem) Another example of a situation when calculating probabilities is counterintuitive. The Monty Hall problem was named after the host of the long-running TV show Let’s Make a Deal. The original solution was proposed by Marilyn vos Savant, who had a column with the correct answer that many Mathematicians thought was wrong!\nThe game set-up is as follows. A contestant is given the choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats. The game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat. He can’t open your door or show you a car\nYou have the choice of switching doors.\n\nThe question is, is it advantageous to switch? The answer is yes. The probability of winning if you switch is 2/3 and if you don’t switch is 1/3.\nConditional probabilities allow us to answer this question. Assume you pick door 2 (event \\(A\\)) at random, given that the host opened Door 3 and showed a goat (event B), we need to calculate \\(P(A\\mid B)\\). The prior probability that the car is behind Door 2 is \\(P(A) =  1/3\\) and \\(P(B\\mid A) = 1\\), if the car is behind Door 2, the host has no choice but to open Door 3. The Bayes rule then gives us \\[\nP(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\n\\] The overall probability of the host opening Door 3 \\[\nP(B) = (1/3 \\times 1/2) + (1/3 \\times 1) = 1/6 + 1/3 = 1/2.\n\\]\nThe posterior probability that the car is behind Door 2 after the host opens Door 3 is 2/3. It is to your advantage to switch doors.\n\n\nExample 2.10 (Prosecutors Fallacy) The Prosecutor’s Fallacy is a logical error that occurs when a prosecutor presents evidence or statistical data in a way that suggests a defendant’s guilt, even though the evidence is not as conclusive as it may seem. This fallacy arises from a misunderstanding or misrepresentation of conditional probabilities and not understanding that \\[\nP(E\\mid G) \\ne P(G\\mid E)\n\\]\nIt involves confusion between the probability of two events: the probability of the evidence \\(E\\), given the defendant’s guilt (which is what the prosecutor may be presenting), and the probability of the defendant’s guilt \\(G\\), given the evidence (which is what is often of more interest in a trial).\nHere’s a simplified example to illustrate the Prosecutor’s Fallacy. Suppose a crime has been committed, and DNA evidence is found at the crime scene. The prosecutor claims that the probability of finding this particular DNA at the scene, given the defendant’s innocence, is very low (making the evidence seem incriminating). However, the Prosecutor’s Fallacy occurs when the prosecutor incorrectly assumes that this low probability implies a low probability of the defendant’s innocence. In reality, the probability of the DNA being found at the crime scene (given the defendant’s innocence) might also be low if the DNA is relatively rare but not exclusive to the defendant.\nThe fallacy often arises from a failure to consider the base rate or prior probability of the event being investigated. To avoid the Prosecutor’s Fallacy, it’s crucial to carefully distinguish between the probability of the evidence given the hypothesis (guilt or innocence) and the probability of the hypothesis given the evidence.\nConsider a more concrete example of base rate fallacy. Say we have a witness who is 80% certain she saw a “checker” (\\(C\\)) taxi in the accident. We need to calculate \\(P(C\\mid E)\\). Assuming the base rate of 20% \\(P(C) = 0.2\\), we get \\[\nP(C\\mid E) = \\dfrac{P(E\\mid C)P(C)}{P(E)} = \\dfrac{0.8\\cdot 0.2}{0.8\\cdot 0.2 + 0.2\\cdot 0.8} = 0.5\n\\] The witness identification accuracy \\(P(C\\mid E) = 0.8\\) is called the sensitivity.\nEven with a highly accurate witness, the probability that the identified taxi is a Checker will be less than 80%, reflecting the impact of the base rate. Ignoring the base rate can lead to a significant overestimation of the probability of the identified event.\n\n\nExample 2.11 (Law Example)  \n\nSuppose you’re serving on a jury in the city of New York, with a population of roughly 10 million people. A man stands before you accused of murder, and you are asked to judge whether he is guilty \\(G\\) or not guilty \\(\\bar G\\). In his opening remarks, the prosecutor tells you that the defendant has been arrested on the strength of a single, overwhelming piece of evidence: that his DNA matched a sample of DNA taken from the scene of the crime. Let’s denote this evidence by the letter \\(D\\). To convince you of the strength of this evidence, the prosecutor calls a forensic scientist to the stand, who testifies that the probability that an innocent person’s DNA would match the sample found at the crime scene is only one in a million. The prosecution then rests its case. Would you vote to convict this man? If you answered “yes,” you might want to reconsider! You are charged with assessing \\(P(G \\mid D)\\) - that is, the probability that the defendant is guilty, given the information that his DNA matched the sample taken from the scene. Bayes’ rule tells us that \\[\nP(G\\mid D)= P(G)P(D\\mid G)/P(D), ~ P(D) = P(D \\mid G)P(G) + P(D \\mid \\bar G)P(\\bar G)\n\\] We know the following quantities:\n\nThe prior probability of guilt, \\(P(G)\\), is about one in 10 million. New York City has 10 million people, and one of them committed the crime.\nThe probability of a false match, \\(P(D \\mid \\bar G)\\), is one in a million, because the forensic scientist testified to this fact.\n\nTo use Bayes’ rule, let’s make one additional assumption: that the likelihood, \\(P(D\\mid  G)\\), is equal to 1. This means we’re assuming that, if the accused were guilty, there is a 100% chance of seeing a positive result from the DNA test. Let’s plug these numbers into Bayes’ rule and see what we get: \\[\nP(G\\mid D) = 0.09\n\\] The probability of guilt looks to be only 9%! This result seems shocking in light of the forensic scientist’s claim that \\(P(D \\mid \\bar G)\\) is so small: a “one in a million chance” of a positive match for an innocent person. Yet the prior probability of guilt is very low \\(P(G)\\) is a mere one in 10 million - and so even very strong evidence still only gets us up to \\(P(G | D) = 0.09\\).\nConflating \\(P(\\bar G \\mid  D)\\) with \\(P(D \\mid \\bar G)\\) is a serious error in probabilistic reasoning. These two numbers are typically very different from one another, because conditional probabilities aren’t symmetric. As we’ve said more than once, \\(P(\\text{practices hard} \\mid \\text{plays in NBA}) \\approx 1\\), while \\(P(\\text{plays in NBA} \\mid \\text{practices hard}) \\approx 0\\). An alternate way of thinking about this result is the following. Of the 10 million innocent people in New York, ten would have DNA matches merely by chance. The one guilty person would also have a DNA match. Hence there are 11 people with a DNA match, only one of whom is guilty, and so \\(P(G \\mid D) \\approx 1/11\\). Your intuition may mislead, but Bayes’ rule never does!\n\n\nExample 2.12 (Island Problem) There are \\(N+1\\) people on the island and one is a criminal. We have probability of a trait of a criminal equal to \\(p\\), which is \\(p = P(E\\mid I)\\), the probability of evidence, given innocence. Then we have a suspect who is matching the trait and we need to find probability of being guilty, given the evidence \\(P(G \\mid E)\\). It is easier to do the Bayes rule in the odds form. There are three components to the calculations: the prior odds of innocence, \\[\nO ( I ) = P (G) / P ( I ),\n\\] the Bayes factor, \\[\n\\frac{P(E\\mid G)}{P(E\\mid I)}.\n\\] and the posterior odds of innocence. \\[\n    O(I\\mid E) = \\dfrac{P(G\\mid E)}{P(I\\mid E)} = \\dfrac{1}{Np}.\n\\]\nCromwell’s rule states that the use of prior probability of 1 or 0 should be avoided except when it is known for certain that the probability is 1 or 0. It is named after Oliver Cromwell who wrote to the General Assembly of the Church of Scotland in 1650 “I beseech you, in the bowels of Christ, think it possible that you may be mistaken”. In other words, using the Bayes rule \\[\nP(G\\mid E) = \\dfrac{P(E\\mid G)}{P(E)}P(G),\n\\] if \\(P(G)\\) is zero, it does not matter what the evidence is. Symmetrically, probability of innocence is zero if the evidence is certain. In other words, if \\(P(E\\mid I) = 0\\), then \\(P(I\\mid E) = 0\\). This is a very strong statement. It is not always true, but it is a good rule of thumb, it is a good way to avoid the prosecutor’s fallacy.\n\n\nExample 2.13 (Nakamura’s Alleged Cheating) In our paper Maharaj, Polson, and Sokolov (2023), we provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex-world champion) and Hikaru Nakamura. Kramnik called into question Nakamura’s 45.5 out of 46 win streak in a 3+0 online blitz contest at chess.com. In this example we reproduce this paper and assess the weight of evidence using an a priori probabilistic assessment of Viswanathan Anand and the streak evidence of Kramnik. Our analysis shows that Nakamura has a 99.6 percent chance of not cheating given Anand’s prior assumptions.\nWe start by addressing the argument of Kramnik which is based on the fact that the probability of such a streak is very small. This falls into precisely the Prosecutor’s Fallacy. Let’s introduce the notations. We denote by \\(G\\) the event of being guilty and \\(I\\) the event of innocence. We use \\(E\\) to denote evidence. In our case the evidence is the streak of wins by Nakamura. Kramnik’s argument is that the probability of observing the streak is very low, thus we might have a case of cheating. This is the prosecutor’s fallacy \\[\nP(I \\mid E) \\neq P(E \\mid I).\n\\] Kramnik’s calculations neglect other relevant factors, such as the prior probability of cheating. The prosecutor’s fallacy can lead to an overestimation of the strength of the evidence and may result in an unjust conviction. In the cheating problem, at the top level of chess the prior probability of \\(P(G)\\) is small! According to a recent statement by Viswanathan Anand, the probability of cheating is \\(1/10000\\).\n\n\n\nAnand’s Prior\n\n\nGiven the prior ratio of cheaters to not cheaters is \\(1/N\\), meaning out of \\(N+1\\) players, there is one cheater, the Bayes calculations require two main terms. The first one is the prior odds of guilt: \\[\nO ( G ) = P (I) / P ( G ).\n\\] Here \\(P(I)\\) and \\(P(G)\\) are the prior probabilities of innocence and guilt respectively.\nThe second term is the Bayes factor, which is the ratio of the probability of the evidence under the guilt hypothesis to the probability of the evidence under the innocence hypothesis. The Bayes factor is given by \\[\n    L(E\\mid G) = \\frac{P(E\\mid I)}{P(E\\mid G)}.\n\\]\nThe product of the Bayes factor and the prior odds is the posterior odds of guilt, given the evidence. The posterior odds of guilt is given by \\[\n    O(G\\mid E) = O(G) \\times L(E\\mid G).\n\\]\nThe odds of guilt is \\[\n    O ( G )  = \\dfrac{N/(N+1)}{1/(N+1)} = N.\n\\]\nThe Bayes factor is given by \\[\n\\frac{P(E\\mid I)}{P(E\\mid G)} = \\dfrac{p}{1} = p.\n\\] Thus, the posterior odds of guilt are \\[\n    O(G\\mid E) = Np.\n\\] There are two numbers we need to estimate to calculate the odds of cheating given the evidence, namely the prior probability of cheating given via \\(N\\) and the probability of a streak \\(p = P(E\\mid I)\\).\nThere are multiple ways to calculate the probability of a streak. We can use the binomial distribution, the negative binomial distribution, or the Poisson distribution. The binomial distribution is the most natural choice. The probability of a streak of \\(k\\) wins in a row is given by \\[\n    P(E\\mid I) = \\binom{N}{k} q^k (1-q)^{N-k}.\n\\] Here \\(q\\) is the probability of winning a single game. Thus, for a streak of 45 wins in a row, we have \\(k = 45\\) and \\(N = 46\\). We encode the outcome of a game as \\(1\\) for a win and \\(0\\) for a loss or a draw. The probability of a win is \\(q = 0.8916\\) (Nakamura’s Estimate, he reported on his YouTube channel). The probability of a streak is then 0.029. The individual game win probability is calculated from the ELO rating difference between the players.\nThe ELO rating of Hikaru is 3300 and the average ELO rating of his opponents is 2950, according to Kramnik. The difference of 350 corresponds to the odds of winning of \\(wo = 10^{350/400} = 10^{0.875} = 7.2\\). The probability of winning a single game is \\(q = wo/(1+wo) = 0.8916\\).\nThen we use Anand’s prior of \\(N = 10000\\) to get the posterior odds of cheating given the evidence of a streak of 45 wins in a row. The posterior odds of being innocent are 285. The probability of cheating is then \\[\nP(G\\mid E) = 1/(1+O(G\\mid E)) = 0.003491.\n\\] Therefore the probability of innocence \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9965.\n\\]\nFor completeness, we perform sensitivity analysis and also get the odds of not cheating for \\(N = 500\\), which should be a high prior probability given the status of the player and the importance of the event. We get \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9445.\n\\]\nThere are several assumptions we made in this analysis.\n\nInstead of calculating game-by-game probability of winning, we used the average probability of winning of 0.8916, provided by Nakamura himself. This is a reasonable assumption given the fact that Nakamura is a much stronger player than his opponents. This assumption slightly shifts posterior odds in favor of not cheating. Due to Jensen’s inequality, we have \\(E(q^{50}) &gt; E(q)^{50}\\). Expected value of the probability of winning a single game is \\(E(q) = 0.8916\\) and the expected value of the probability of a streak of 50 wins is \\(E(q^{50})\\). We consider the difference between the two to be small. Further, there is some correlation between the games, which also shifts the posterior odds in favor of not cheating. For example, some players are on tilt. Given they lost the first game, they are more likely to lose the second game.\nThere are many ways to win 3+0 unlike in classical chess. For example, one can win on time. We argue that the probability of winning calculated from the ELO rating difference is underestimated.\n\nNext, we can use the Bayes analysis to solve an inverse problem and to find what prior you need to assume and how long of a sequence you need to observe to get 0.99 posterior? Small sample size, we have \\(p\\) close to 1. Figure 2.1 shows the combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\nIndeed, the results of the Bayesian analysis contradict the results of a traditional p-value based approach. A p-value is a measure used in frequentist statistical hypothesis testing. It represents the probability of obtaining the observed results, or results more extreme, assuming that the null hypothesis is true. The null hypothesis is a default position that Nakamura is not cheating and we compare the ELO-based expected win probability of \\(q=0.8916\\) to the observed one of \\(s=45/46=0.978\\). Under the null hypothesis, Nakamura should perform at the level predicted by \\(q\\).\n\nq = 0.8916\np = dbinom(45,46,q)\nN = 10000\nodds = p*N\nprint(1-1/(1+odds))\n\n## [1] 1\n\nprint(1/(1+odds))\n\n## [1] 0.0035\n\nprint(N*p/(N*p+1))\n\n## [1] 1\n\n\n\np = seq(from=0.006, to=0.07, length.out=500)\nN = seq(500,10000, by=250)\nplot(99/N,N,xlab=\"p\", ylab=\"N\", type='l', lwd=3, col=\"blue\")\n\n\n\n\n\n\n\nFigure 2.1: The combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\n\n\n\n\n\n\n\nDavid Hume discussed the problem similar to the Island problem in his “On Miracles” essay. Hume is making the following argument on miracles:\n\n“…no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavors to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior.”\n\nOne can view this as an application of the Island problem. Assuming the probability of a miracle \\(A\\) is \\(p( A) = p\\) and \\(p( not \\; A ) = 1 -p\\). Then Bayes rule gives \\[\np( A| a ) = \\frac{ p( a| A) p }{  p( a| A) p  +  p( a |  not \\; A) (1-p) }  \n\\] Prosecutor’s fallacy, \\(p( a| not \\; A)  \\neq 1 -  p( a| A)\\), in general.\nIn Hume’s assessment of miracles (has to be something not in the laws of nature) we have \\(p(A) = 10^{-6}\\). This assessment takes into account background information, \\(I\\). Rare to have a contradiction to the laws of nature. More informative to write \\(p( A | I )\\). Furthermore, we take \\(p( a| A) =0.99\\). The hardest bit is to assess \\(p(a | not \\; A )\\). The “frequency” of faked miracles and mankind’s propensity to be marvelous. We assess \\(p(a | not \\; A )  = 10^{-3}\\). This yields the chance of a miracle to be unlikely as \\[\np( A| a ) = \\frac{ 0.99 \\times 10^{-6}  }{  0.99 \\times 10^{-6}    +  10^{-3} (1- 10^{-6}) }   \\approx 10^{-3}.\n\\] Feynman considers the inverse problem: can we learn the laws of nature purely from empirical observation? Uses chess as an example. Is it a miracle that we have two bishops of the same color? No! according to Hume. We just didn’t know the laws of nature (a.k.a. model).\n\nExample 2.14 (Sally Clark Case: Independence or Bayes Rule?) To show that independence can lead to dramatically different results from Bayes conditional probabilities, consider the Sally Clark case. Sally Clark was accused and convicted of killing her two children who could have both died of SIDS. One explanation is that this was a random occurrence, the other one is that they both died of sudden infant death syndrome (SIDS). How can we use conditional probability to figure out a reasonable assessment of the probability that she murdered her children. First, some known probability assessments\n\nThe chance of a family of non-smokers having a SIDS death is \\(1\\) in \\(8,500\\).\nThe chance of a second SIDS death is \\(1\\) in \\(100\\).\nThe chance of a mother killing her two children is around \\(1\\) in \\(1,000,000\\).\n\nUnder Bayes \\[\\begin{align*}\n\\prob{\\mathrm{both} \\; \\; \\mathrm{SIDS}}   &  = \\prob{\\mathrm{first} \\; \\mathrm{SIDS}} \\prob{\\mathrm{Second} \\; \\;\\mathrm{SIDS} \\mid \\mathrm{first} \\; \\mathrm{SIDS}}\\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}.\n\\end{align*}\\]\nThe \\(1/100\\) comes from taking into account the genetic properties of SIDS. Independence, as implemented by the court, gets you to a probabilistic assessment of \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000).\n\\] This is a low probability. It is still not the answer to our question of context. We need a conditional probability, this will come to the Bayes rule.\nFirst, some general comment on the likelihood ratio calculation used to assess the weight of evidence in favor of guilty v.s. innocent evidence. Under Bayes we’ll find that there’s reasonable evidence that she’d be acquitted. We need the relative odds ratio. Let \\(I\\) denote the event that Sally Clark is innocent and \\(G\\) denotes guilty. Let \\(E\\) denote the evidence. In most cases, \\(E\\) contains a sequence \\(E_1, E_2, \\ldots\\) of ‘facts’ and we have to use the likelihood ratios in turn. Bayes rule then tells you to combine via multiplicative fashion. If likelihood ratio \\(&gt;1\\), odds of guilty. If likelihood ratio \\(&lt;1\\), more likelihood to be \\(I\\). By Bayes rule \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{p( E\\text{ and } I)}{p( E\\text{ and } G)}.\n\\] If we further decompose \\(p(E \\text{ and } I) = p(E\\mid I )p(I)\\) then we have to discuss the prior probability of innocence, namely \\(p(I)\\). Hence this is one subtle advantage of the above decomposition.\nThe underlying intuition that Bayes gives us in this example, is that of the two possible explanations of the data, both of which are unlikely, it is the relative likelihood comparison that should matter. Here is a case where the \\(p\\)-value would be non-sensible (\\(p(E\\mid I) \\neq p(I\\mid E)\\)). Effectively comparing two rare event probabilities from the two possible models or explanations.\nHence putting these two together gives the odds of guilt as \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{1/850,000}{1/1,000,000} = 1.15.\n\\] Solving for the posterior probability yields \\(46.5\\%\\) for probability of guilty given evidence. \\[\np( G\\mid E) = \\frac{1}{1 + O(G\\mid E)} = 0.465.\n\\] Basically a \\(50/50\\) bet. Not enough to definitively convict! But remember that our initial prior probability on guilt \\(p(G)\\) was \\(10^{-6}\\). So now there has been a dramatic increase to a posterior probability of \\(0.465\\). So it’s not as if Bayes rule thinks this is evidence in the suspect’s favor – but the magnitude is still not in the \\(0.999\\) range though, where most jurors would have to be to feel comfortable with a guilt verdict.\nIf you use the “wrong” model of independence (as the court did) you get \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = \\frac{1}{8500}\n  \\cdot\\frac{1}{8500} = \\frac{1}{73,000,000}.\n\\] With the independence assumption, you make the assessment \\[\n\\frac{p(I\\mid E)}{p(G\\mid E)} = \\frac{1}{73} \\; \\mathrm{ and} \\; p( G\\mid E) \\approx 0.99.\n\\] Given these probability assumptions, the suspect looks guilty with probability 99%.\nExperts also mis-interpret the evidence by saying: 1 in 73 million chance that it is someone else. This is clearly false and misleading to the jury and has led to appeals.\n\n\nExample 2.15 (O. J. Simpson Case: Dershowitz Fallacy)  \n\nThis example is based on I. J. Good’s, “When batterer turns murderer.” Nature, 15 June 1995, p. 541. Alan Dershowitz, on the O. J. Simpson defense team, stated on T.V. and in newspapers that only 1 in 2,500 of men who abuse their wives go on to murder them. He clearly wanted his audience to interpret this to mean that the evidence of abuse by Simpson would only suggest a 0.04% probability of his being guilty of murdering her. He used probability to argue that because so few husbands who batter their wives actually go on to murder their wives. Thus, O.J. is highly likely to be not guilty. This leaves out the most relevant conditioning information that we also know that Nicole Brown Simpson was actually murdered. Both authors believe the jury would be more interested in the probability that the husband is guilty of the murder of his wife given that he abused his wife and his wife was murdered. They both solve this problem by using Bayes’ theorem.\nIn this example, the notation \\(B\\) represents “woman battered by her husband, boyfriend, or lover”, \\(M\\) represents the event “woman murdered”, and \\(G\\) denotes “woman murdered by her batterer”. Our goal is to show that \\[\n% P(M,B \\mid M) \\neq P(M,B \\mid B).\nP(G \\mid M,B) \\neq P(G\\mid B).\n\\]\nIt is not hard to come to a wrong conclusion if you don’t take into account all the relevant conditional information. He intended this information to exonerate O.J. In 1992 the women population of the US was 125 million and 4936 women were murdered, thus \\[\nP(M) = 4936/125,000,000 = 0.00004 = 1/25,000.\n\\] At the same year about 3.5 million women were battered \\[\nP(B) = 3.5/125 = 0.028.\n\\] That same year 1432 women were murdered by their previous batterers, so the marginal probability of that event is \\(P(G) = 1432/125,000,000 = 0.00001 = 1/87,290\\), and the conditional probability, \\(P(G | B)\\) is 1432 divided by 3.5 million, or \\(1/2444\\). These are the numbers Dershowitz used to obtain his estimate that about 1 in 2500 battered women go on to be murdered by their batterers.\nWe need to calculate \\[\nP(G \\mid M,B) = P(M | G,B) P(G) / P(M).\n\\] We know \\(P(M | G,B) = 1\\) and \\(P(G) / P(M) = 0.00001/0.00004 = 0.29\\), or about 1 in 3.5.\nAlan Dershowitz provided the jury with an accurate but irrelevant probability. The fact the woman was murdered increases the probability that she was murdered by her batterer by a factor of 709 (0.29/(1/2444)). \\[\nP(G\\mid M,B)\\approx 709\\times P(G\\mid B).\n\\]\nThe argument used by Dershowitz relating to the Simpson case has been discussed by John Paulos in an op-ed article in the Philadelphia Inquirer (15 Oct. 1995, C7) and his book “Once Upon a Number”, by I.J. Good in an article in Nature (June 15,1995, p 541) and by Jon Merz and Jonathan Caulkins in an article in Chance Magazine, (Spring 1995, p 14).\n\nProbability measures the uncertainty of an event. But how do we measure probability? One school of thought takes probability as subjective, namely personal to the observer. de Finetti famously concluded that “Probability does not exist.” Measuring that is personal to the observer. It’s not like mass which is a property of an object. If two different observers have differing “news” then there is an opportunity for them to bet (exchange contracts). Thus leading to an assessment of probability. Ramsey (1926) takes this view.\nMuch of data science is then the art of building probability models to study phenomena. For many events most people will agree on their probabilities, for example \\(p(H) = 0.5\\) and \\(p(T) = 0.5\\). In the subjective view of probability we can measure or elicit a personal probability as a “willingness to play”. Namely, will you be willing to bet $1 so you can get $2 if the coin lands Tail and $0 if Head occurs? For more details, see Chapter 4.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#sec-Sensitivity",
    "href": "02-bayes.html#sec-Sensitivity",
    "title": "2  Bayes Rule",
    "section": "2.4 Sensitivity and Specificity",
    "text": "2.4 Sensitivity and Specificity\nConditional probabilities are used to define two fundamental metrics used for many probabilistic and statistical learning models, namely sensitivity and specificity.\nSensitivity and specificity are two key metrics used to evaluate the performance of diagnostic tests, classification models, or screening tools. These metrics help assess how well a test can correctly identify individuals with a condition (true positives) and those without the condition (true negatives). Let’s break down each term:\n\nSensitivity (true‐positive rate or recall) is the ability of a test \\(T\\) to correctly identify individuals who have a particular condition or disease (\\(D\\)), \\(P ( T=1 \\mid D=1 )\\), the probability of a positive test given that the individual has the disease. It is calculated as the ratio of true positives to the sum of true positives and false negatives. \\[\np(T=1\\mid D=1) = \\dfrac{p(T=1,D=1)}{p(D=1)}.\n\\] A high sensitivity indicates that the test is good at identifying individuals with the condition, minimizing false negatives.\nSpecificity (true‐negative rate) is the ability of a test to correctly identify individuals who do not have a particular condition or disease, \\(P (T=0 \\mid D=0 )\\). It is calculated as the ratio of true negatives to the sum of true negatives and false positives. \\[\np(T=0\\mid D=0) = \\dfrac{p(T=0,D=0)}{p(D=0)}\n\\] A high specificity indicates that the test is good at correctly excluding individuals without the condition, minimizing false positives.\n\nSensitivity and specificity are often trade-offs. Increasing sensitivity might decrease specificity, and vice versa. Thus, depending on the application, you might prefer sensitivity over specificity or vice versa, depending on the consequences of false positives and false negatives in a particular application.\nConsider a medical test designed to detect a certain disease. If the test has high sensitivity, it means that it is good at correctly identifying individuals with the disease. On the other hand, if the test has high specificity, it is good at correctly identifying individuals without the disease. The goal is often to strike a balance between sensitivity and specificity based on the specific needs and implications of the test results.\n\nSensitivity is often called the power of a procedure (a.k.a. test). There are two kinds of errors (type I and type II) as well as sensitivity and specificity are dual concepts.\n\n\n\n\n\n\nType I error (false positive rate)\n\n\n\nis the percentage of healthy people who tested positive, \\(P(T=1\\mid D=0)\\), it is the mistake of thinking something is true when it is not.\n\n\n\n\n\n\n\n\nType II error (or false negative rate)\n\n\n\nis the percentage of sick people who are tested negative, \\(P(T=0\\mid D=1)\\), it is the mistake of thinking something is not true when in fact it is true.\n\n\nWe would like to control both conditional probabilities with our test. Also if someone tests positive, how likely is it that they actually have the disease. There are two ‘errors’ one can make. Falsely diagnosing someone, or not correctly finding the disease.\nIn the stock market, one can think of type I error as not selling a losing stock quickly enough, and a type II error as failing to buy a growing stock, e.g. Amazon or Google.\n\n\n\n\n\n\n\n\n\n\\(p(T=1\\mid D=1)\\)\nSensitivity\nTrue Positive Rate\n\\(1-\\beta\\)\n\n\n\\(p(T=0\\mid D=0 )\\)\nSpecificity\nTrue Negative Rate\n\\(1-\\alpha\\)\n\n\n\\(p(T=1\\mid D=0)\\)\n1-Specificity\nFalse Positive Rate\n\\(\\alpha\\) (type I error)\n\n\n\\(p(T=0\\mid D =1)\\)\n1-Sensitivity\nFalse Negative Rate\n\\(\\beta\\) (type II error)\n\n\n\nOften it is convenient to write those four values in the form of a two-by-two matrix, called the confusion matrix:\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nwhere: TP: True Positive. FN: False Negative, FP: False Positive, TN: True Negative\nWe will extensively use the concepts of errors, specificity and sensitivity later in the book, when describing AB testing and predictive models. These examples illustrate why people can commonly miscalculate and mis-interpret probabilities. Those quantities can be calculated using the Bayes rule.\n\nExample 2.16 (Apple Watch Series 4 ECG and Bayes’ Theorem) The Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives) (Kim et al. 2024; Bumgarner et al. 2018).\n\n\n\n\n\n\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\nTotal\n\n\n\n\natrial fibrillation\n1960\n980\n2940\n\n\nno atrial fibrillation\n40\n97020\n97060\n\n\nTotal\n2000\n98000\n100000\n\n\n\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4? We use Bayes theorem to answer this question. \\[\np(\\text{atrial fibrillation}\\mid \\text{atrial fibrillation is identified }) = \\frac{0.01960}{ 0.02940} = 0.6667\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%.\nApple Watch’s positive predictive value is just 19.6 percent. That means in this group – which constitutes more than 90 percent of users of wearable devices like the Apple Watch – the app incorrectly diagnoses atrial fibrillation 79.4 percent of the time. (You can try the calculation yourself using this Bayesian calculator: enter 0.001 for prevalence, 0.98 for sensitivity, and 0.996 for specificity).\nThe electrocardiogram app becomes more reliable in older individuals: The positive predictive value is 76 percent among users between the ages of 60 and 64, 91 percent among those aged 70 to 74, and 96 percent for those older than 85.\nIn the case of medical diagnostics, the sensitivity is the ratio of people who have disease and tested positive to the total number of positive cases in the population \\[\np(T=1\\mid D=1) = \\dfrac{p(T=1,D=1)}{p(D=1)} = 0.019/0.002 = 0.95\n\\] The specificity is given by \\[\np(T=0\\mid D=0) = \\dfrac{p(T=0,D=0)}{p(D=0)} = 0.9702/0.98 = 0.99.\n\\] As we see the test is highly sensitive and specific. However, only 66% of those who are tested positive will have a disease. This is due to the fact that the number of sick people is much less than the number of healthy and presence of type I error.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "href": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "title": "2  Bayes Rule",
    "section": "2.5 Graphical Representation of Probability and Conditional Independence.",
    "text": "2.5 Graphical Representation of Probability and Conditional Independence.\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\np(x_1, x_2, \\ldots, x_n) = p(x_1)p(x_2 \\mid x_1)p(x_3 \\mid x_1, x_2) \\ldots p(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\] The expression on the right hand side can be simplified if some of the variables are conditionally independent. For example, if \\(x_3\\) is conditionally independent of \\(x_2\\), given \\(x_1\\), then we can write \\[\np(x_3 \\mid x_1, x_2) =p(x_3 \\mid x_1).\n\\]\nIn a high-dimensional case, when we have a joint distribution over a large number of random variables, we can often simplify the expression by using independence or conditional independence assumptions. Sometimes it is convenient to represent these assumptions in a graphical form. This is the idea behind the concept of a Bayesian network. Essentially, the graph is a compact representation of a set of independencies that hold in the distribution.\nLet’s consider an example of joint distribution with three random variables, we have the following joint distribution: \\[\np(a,b,c) = p(a\\mid b,c)p(b\\mid c)p(c)\n\\]\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network. Each node represents a random variable and the arrows represent the conditional dependencies between the variables. When two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\\[\np(b\\mid c,a) = p(b\\mid c),~ p(a,b,c) = p(a)p(c\\mid a)p(b\\mid c)\n\\]\n\n\n\n\nLine Structure\n\n\n\n\n\n\n\n\n\n\\[\np(a\\mid b,c) = p(a\\mid c), ~ p(a,b,c) = p(a\\mid c)p(b\\mid c)p(c)\n\\]\n\n\n\n\nLambda Structure\n\n\n\n\n\n\n\n\n\n\\[\np(a\\mid b) = p(a),~ p(a,b,c) = p(c\\mid a,b)p(a)p(b)\n\\]\n\n\n\n\nV-structure\n\n\n\nAlthough the graph shows us the conditional independence assumptions, we can also derive other independencies from the graph. An interesting question is whether they are connected through a third node. In the first case (a), we have \\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In case (b) the logic here is similar, \\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In the third case (c), \\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(p(a,b\\mid c)\\) and \\(p(a\\mid c)p(b\\mid c)\\).\n\nExample 2.17 (Bayes Home Diagnostics) Suppose that a house alarm system sends me a text notification when some motion inside my house is detected. It detects motion when I have a person inside (burglar) or during an earthquake. Say, from prior data we know that during an earthquake alarm is triggered in 10% of the cases. Once I receive a text message, I start driving back home. While driving I hear on the radio about a small earthquake in our area. Now we want to know \\(p(b \\mid a)\\) and \\(p(b \\mid a,r)\\). Here \\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  p(b,e,a,r) = p(r \\mid a,b,e)p(a \\mid b,e)p(b\\mid e)p(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\np(b,e,a,r) = p(r \\mid e)p(a \\mid b,e)p(b)p(e).\n\\] The \\(p(a \\mid b,e)\\) distribution is defined by\n\n\n\nTable 2.1: Conditional probability of alarm given burglary and earthquake\n\n\n\n\n\n\\(p(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1\n\n\n\n\n\n\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network.\n\n\n\n\n\n\n\n\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\nFigure 2.2: Bayesian network for alarm\n\n\n\n\n\nNow we can easily calculate \\(p(a=0 \\mid b,e)\\), from the property of a probability distribution \\(p(a=1 \\mid b,e) + p(a=0 \\mid b,e) = 1\\). In addition, we are given \\(p(r=1 \\mid e=1) = 0.5\\) and \\(p(r=1 \\mid e=0) = 0\\). Further, based on historic data we have \\(p(b) = 2\\cdot10^{-4}\\) and \\(p(e) = 10^{-2}\\). Note that causal relations allowed us to have a more compact representation of the joint probability distribution. The original naive representation requires specifying \\(2^4\\) parameters.\nTo answer our original question, calculate \\[\np(b \\mid a) = \\dfrac{p(a \\mid b)p(b)}{p(a)},~~p(a) = p(a=1 \\mid b=1)p(b=1) + p(a=1 \\mid b=0)p(b=0).\n\\] We have everything but \\(p(a \\mid b)\\). This is obtained by marginalizing \\(p(a=1 \\mid b,e)\\), to yield \\[\np(a \\mid b) = p(a \\mid b,e=1)p(e=1) + p(a \\mid b,e=0)p(e=0).\n\\] We can calculate \\[\np(a=1 \\mid b=1) = 1, ~p(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(p(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\).\nThis result is somewhat counterintuitive. We get such a low probability of burglary because its prior is very low compared to the prior probability of an earthquake. What will happen to the posterior if we live in an area with higher crime rates, say \\(p(b) = 10^{-3}\\). Figure 2.3 shows the relationship between the prior and posterior. \\[\np(b \\mid a) = \\dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}\n\\]\n\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nFigure 2.3: Relationship between the prior and posterior\n\n\n\n\n\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\np(b=1 \\mid a=1,r=1) =  \\dfrac{p(a,r  \\mid  b)p(b)}{p(a,r)}\n\\] and \\[\np(a,r  \\mid  b)p(b) = \\dfrac{\\sum_e p(b=1,e,a=1,r=1)}{\\sum_b\\sum_ep(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_ep(r=1 \\mid e)p(a=1 \\mid b=1,e)p(b=1)p(e)}{\\sum_b\\sum_ep(r=1 \\mid e)p(a=1 \\mid b,e)p(b)p(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact.\n\n\n\n\n\nBumgarner, John M., Chad T. Lambert, Ayman A. Hussein, Daniel J. Cantillon, Bryan Baranowski, Kathy Wolski, Bruce D. Lindsay, Oussama M. Wazni, and Khaldoun G. Tarakji. 2018. “Smartwatch Algorithm for Automated Detection of Atrial Fibrillation.” Journal of the American College of Cardiology 71 (21): 2381–88.\n\n\nKim, Young-Hoon, Jaehyung Shim, Hyoung-Seob Park, et al. 2024. “Diagnostic Accuracy of Single-Lead Handheld ECG Devices for Atrial Fibrillation Detection.” Journal of Cardiovascular Electrophysiology 35: 614–21.\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs Nakamura or Bayes Vs p-Value.” {{SSRN Scholarly Paper}}. Rochester, NY.\n\n\nSimpson, Edward. 2010. “Edward Simpson: Bayes at Bletchley Park.” Significance 7 (2): 76–80.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "03-bl.html",
    "href": "03-bl.html",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "3.1 Exchangeability and the Bayesian view of probability models\nStatistics makes use of parametric families of distributions and assumes that observed samples \\(y = (y_1,\\ldots,y_n)\\) are independent and identically distributed observations from a distribution with density function parametrized by \\(\\theta\\), the notation is \\(y\\mid \\theta \\sim p(y \\mid \\theta)\\). The functional form of \\(p(y \\mid \\theta)\\) is assumed to be known, but the value of \\(\\theta\\) is unknown. The goal of statistical inference is to estimate \\(\\theta\\) from the observed data \\(y_1,\\ldots,y_n\\). There are several tasks in statistical inference, including\nThe main tasks in statistical inference include estimation, hypothesis testing, and prediction. Estimation involves using the observed sample to infer the value of the unknown parameter \\(\\theta\\), either by providing a single best guess (denoted as \\(\\hat{\\theta}\\)) or by constructing an interval \\([a, b]\\) that is likely to contain the true value of \\(\\theta\\) with a specified probability. Hypothesis testing focuses on evaluating specific claims or hypotheses about the value of \\(\\theta\\); for instance, we might be interested in determining whether \\(\\theta\\) is equal to a particular value \\(\\theta_0\\). Prediction, on the other hand, is concerned with forecasting the value of a future observation \\(y_{n+1}\\) based on the data already observed, \\(y_1, \\ldots, y_n\\) via a model \\(p(y_{n+1} \\mid y_1, \\ldots, y_n)\\).\nIn this section we present a general framework for statistical inference, known as Bayesian inference, which is based on the use of probability distributions to represent uncertainty and make inferences about unknown parameters. We will use Bayes rule to update our beliefs about the parameters of a model based on new evidence or data. Bayesian inference provides a principled approach to statistical modeling and decision-making, and is widely used in various fields such as machine learning, econometrics, and engineering.\nIn the context of artificial intelligence and statistical modeling, Bayesian parameter learning is particularly relevant when dealing with models that have uncertain or unknown parameters. The goal is to update the probability distribution over the parameters of the model as new data becomes available. Suppose that you are interested in the values of k unknown quantities \\[\n\\theta = (\\theta_1, \\ldots, \\theta_k)\n\\]\nThe basic steps involved in Bayesian parameter learning include:\nThe key advantage of Bayesian parameter learning is its ability to incorporate prior knowledge and update beliefs based on observed data in a principled manner. It provides a framework for handling uncertainty and expressing the confidence or ambiguity associated with parameter estimates. However, it often requires computational methods, such as Markov Chain Monte Carlo (MCMC) or variational inference, to approximate or sample from the complex posterior distributions.\nBayes rule allows us to combine the prior distribution and the likelihood function \\[\n\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Marginal}}.\n\\]\nOften, Bayesian analysis can be performed without calculating the marginal likelihood, in this case we omit the total probability in the denominator on the right hand side and write Bayes rule as \\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}.\n\\]\nThe choice of prior distribution can significantly impact the ease of computation and the interpretation of the posterior distribution. Conjugate priors are a special type of prior distribution that, when combined with a specific likelihood function, result in a posterior distribution that belongs to the same family as the prior. This property simplifies the computation of the posterior distribution, and allows for analytical solutions.\nCommon examples of conjugate priors include:\nUsing conjugate priors simplifies the Bayesian analysis, especially in cases where analytical solutions are desirable. However, the choice of a conjugate prior is often a modeling assumption, and in some cases, non-conjugate priors may be more appropriate for capturing the true underlying uncertainty in the problem. The blind use of conjugate priors can lead to misleading results. We should never ignore the absence of evidence for use of a specific model.\nAt the basis of all statistical problems is a potential sample of data, \\(y=\\left( y_{1},\\ldots,y_{T}\\right)\\), and assumptions over the data generating process such as independence, a model or models, and parameters. How should one view the relationship between models, parameters, and samples of data? How should one define a model and parameters? These questions have fundamental implications for statistical inference and can be answered from different perspectives. We will discuss de Finetti’s representation theorem which provides a formal connection between data, models, and parameters.\nTo understand the issues, consider the simple example of an experiment consisting of tosses of a simple thumb tack in ideal “laboratory” conditions. The outcome of the experiment can be defined as a random variable \\(y_{i},\\) where \\(y_{i}=1\\) if the \\(i^{th}\\) toss was a heads (the tack lands on the spike portion) and \\(y_{i}=0\\) if the tack lands tails (on its flat portion). How do we model these random variables? The frequentist or objective approach assumes tosses are independent and identically distributed. In this setting, independence implies that \\[\nP\\left(  y_{2}=1,y_{1}=1\\right)  =P\\left(  y_{2}=1\\right)\nP\\left(  y_{1}=1\\right).\n\\]\nGiven this, are thumbtack tosses independent? Surprisingly, the answer is no. Or at least absolutely not under the current assumptions. Independence implies that \\[\nP\\left(  y_{2}=1 \\mid y_{1}=1\\right)  =P\\left(  y_{2}=1\\right),\n\\] which means that observing \\(y_{1}=1\\) does not affect the probability that \\(y_{2}=1\\). To see the implications of this simple fact, suppose that the results of 500 tosses were available. If the tosses were independent, then \\[\nP\\left(  y_{501}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=499\\right).\n\\] It is hard to imagine that anyone would believe this–nearly every observer would state that the second probability is near zero and the third probability is near 1 as the first 500 tosses contain a lot of information. Thus, the tosses are not independent.\nTo see the resolution of this apparent paradox, introduce a parameter, \\(\\theta\\), which is the probability that a thumb tack toss is heads. If \\(\\theta\\) were known, then it is true that, conditional on the value of this parameter, the tosses are independent and \\[\nP\\left(  y_{2}=1\\mid y_{1}=1,\\theta\\right)  =P\\left(y_{2}=1\\mid \\theta\\right)  =\\theta.\n\\] Thus, the traditional usage of independence, and independent sampling, requires that “true” parameter values are known. With unknown probabilities, statements about future tosses are heavily influenced by previous observations, clearly violating the independence assumption. Ironically, if the data was really independent, we would not need samples in the first place to estimate parameters because the probabilities would already be known! Given this, if you were now presented with a thumb tack from a box that was to be repeatedly tossed, do you think that the tosses are independent?\nThis example highlights the tenuous foundations, an odd circularity, and the internal inconsistency of the frequentist approach that proceeds under the assumption of a fixed “true” parameter. All frequentist procedures are founded on the assumption of known parameter values: sampling distributions of estimators are computed conditional on \\(\\theta\\); confidence intervals consist of calculations of the form: \\(P\\left( f\\left( y_{1}, \\ldots ,y_{T}\\right) \\in\\left( a,b\\right) |\\theta\\right)\\); and asymptotics also all rely on the assumption of known parameter values. None of these calculations are possible without assuming the known parameters.\nIn the frequentist approach, even though the parameter is completely unknown to the researcher, \\(\\theta\\) is not a random variable, does not have a distribution, and therefore inference is not governed by the rules of probability. Given this “fixed, but unknown” definition, it is impossible to discuss concepts like “parameter uncertainty.” This strongly violates our intuition, since things that are not known are typically thought of as random.\nThe Bayesian approach avoids this internal inconsistency by shedding the strong assumption of independence and assumption of a fixed but unknown parameter. Instead it assumes that \\(\\theta\\) is a random variable and describes the uncertainty about \\(\\theta\\) using a probability distribution, \\(p\\left( \\theta\\right)\\) (the prior). The joint distribution of the data is then \\[\np(y_{1}, \\ldots ,y_{T},\\theta)  = \\int p(y_{1}, \\ldots ,y_{T} \\mid \\theta)  p(\\theta)d\\theta = \\int\\prod_{t=1}^Tp(y_t\\mid \\theta)  p( \\theta)d\\theta.\n\\] Notice, that the right-hand-side does not depend on the order of the data, and the joint distribution of the data is the same for all potential orderings. This is a natural assumption about the symmetry of the data, and is called exchangeability. The Bayesian approach makes no assumptions about the order in which the data may arrive, and each observation has the same marginal distribution, \\(P\\left( y_{i}=1\\right) =P\\left(y_{j}=1\\right)\\) for any \\(i\\) and \\(j\\).\nThus, we replace the independence assumption with a weaker and more natural assumption of exchangeability: a collection of random variables, \\(y_{1}, \\ldots ,y_{T}\\), is exchangeable if the distribution of \\(y_{1}, \\ldots ,y_{T}\\) is the same as the distribution of any permutation \\(y_{\\pi_{1}}, \\ldots ,y_{\\pi_{T}}\\), where \\(\\pi=\\left( \\pi_{1}, \\ldots ,\\pi_{T}\\right)\\) is a permutation of the integers \\(1\\) to \\(T\\). Independent events are always exchangeable, but the converse is not true. Notice the differences between the assumptions in the Bayesian and frequentist approach: the Bayesian makes assumptions over potentially realized data, and there is no need to invent the construct of a fixed but unknown parameter, since exchangeability makes no reference to parameters.\nIn the case of the tack throwing experiment, exchangeability states that the ordering of heads and tails does not matter. Thus, if the experiment of 8 tosses generated 4 heads, it does not matter if the ordering was \\(\\left(1,0,1,0,1,0,1,0\\right)\\) or \\(\\left( 0,1,1,0,1,0,0,1\\right)\\). This is a natural assumption about the symmetry of the tack tosses, capturing the idea that the information in any toss or sequence of tosses is the same as any other–the idea of a truly random sample. It is important to note that exchangeability is a property that applies prior to viewing the data. After observation, data is no longer a random variable, but a realization of a random variable.\nBruno de Finetti introduced the notion of exchangeability, and then asked a simple question: “What do exchangeable sequences of random variables look like?” The answer to this question is given in the famous de Finetti’s theorem, which also defines models, parameters, and provides important linkages between frequentist and classical statistics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "href": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "de Finetti’s representation theorem\nde Finetti’s representation theorem provides the theoretical connection between data, models, and parameters. It is stated first in the simplest setting, where the observed data takes two values, either zero or one, and then extended below.\n\nTheorem 3.1 (de Finetti’s representation theorem) Let \\(\\left( y_{1},y_{2},\\ldots\\right)\\) be an infinite sequence of 0-1 exchangeable random variables with joint density \\(p\\left(y_{1}, \\ldots ,y_{T}\\right)\\). Then there exists a distribution function \\(P\\) such that \\[\np(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}\\theta^{y_{t}}(1-\\theta)^{1-y_{t}%\n}dP(\\theta)=\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\\right)  dP(\\theta)\n\\tag{3.1}\\] where \\[\nP(\\theta)=\\underset{T\\rightarrow\\infty}{\\lim}\\text{Prob}\\left[  \\frac{1}{T}\\sum_{t=1}^{T}y_{t}\\leq\\theta\\right]  \\text{ and }\\theta=\\underset {T\\rightarrow\\infty}{\\lim}\\frac{1}{T}\\sum_{t=1}^{T}y_{t}\\text{.}%\n\\] If the distribution function or measure admits a density with respect to Lebesgue measure, then \\(dP(\\theta)=p\\left( \\theta\\right) d\\theta\\).\n\nde Finetti’s representation theorem has profound implications for understanding models from a subjectivist perspective and in relating subjectivist to frequentist theories of inference. The theorem is interpreted as follows:\n\nUnder exchangeability, parameters exist, and one can act as if the \\(y_{t}\\)’s are drawn independently from a Bernoulli distribution with parameter \\(\\theta\\). That is, they are draws from the model \\(p\\left(y_{t} \\mid \\theta\\right) =\\theta^{y_{t}}(1-\\theta)^{1-y_{t}},\\) generating a likelihood function \\(p\\left( y \\mid \\theta\\right) =\\prod_{t=1}^{T}p\\left(y_{t} \\mid \\theta\\right)\\). Formally, the likelihood function is defined via the density \\(p\\left( y \\mid \\theta\\right)\\), viewed as a function of \\(\\theta\\) for a fixed sample \\(y=\\left( y_{1}, \\ldots ,y_{T}\\right)\\). More “likely” parameter values generate higher likelihood values, thus the name. The maximum likelihood estimate or MLE is \\[\n\\widehat{\\theta}=\\arg\\underset{\\theta\\in\\Theta}{\\max}\\text{ }p\\left(y \\mid \\theta\\right)  =\\arg\\underset{\\theta\\in\\Theta}{\\max}\\ln p\\left(y \\mid \\theta\\right),   \n\\] where \\(\\Theta\\) is the parameter space.\nParameters are random variables. The limit \\(\\theta=\\underset {T\\rightarrow\\infty}{\\lim}T^{-1}\\sum_{t=1}^{T}y_{t}\\) exists but is a random variable. This can be contrasted with the strong law of large numbers that requires independence and implies that \\(T^{-1}\\sum_{t=1}^{T}y_{t}\\) converges almost surely to a fixed value, \\(\\theta_{0}\\). From this, one can interpret a parameter as a limit of observables and justifies the frequentist interpretation of \\(\\theta\\) as a limiting frequency of 1’s.\nThe distribution \\(P\\left( \\theta\\right)\\) or density \\(p\\left(\\theta\\right)\\) can be interpreted as beliefs about the limiting frequency \\(\\theta\\) prior to viewing the data. After viewing the data, beliefs are updated via Bayes rule resulting in the posterior distribution, \\[\np\\left(  \\theta \\mid y\\right)  \\propto p\\left(  y \\mid \\theta\\right)  p(\\theta).\n\\] Since the likelihood function is fixed in this case, any distribution of observed data can be generated by varying the prior distribution.\n\nThe main implication of de Finetti’s theorem is a complete justification for Bayesian practice of treating the parameters as random variables and specifying a likelihood and parameter distribution. Stated differently, a “model” consists of both a likelihood and a prior distribution over the parameters. Thus, parameters as random variables and priors are a necessity for statistical inference, and not some extraneous component motivated by philosophical concerns.\nMore general versions of de Finetti’s theorem are available. A general version is as follows. If \\(\\left\\{ y_{t}\\right\\} _{t\\geq1}\\), \\(y_{t}\\in\\mathbb{R}\\), is a sequence of infinitely exchangeable random variables, then there exists a probability measure \\(P\\) on the space of all distribution functions, such that \\[\np(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}F\\left(  y_{t}\\right)\nP(dF)\n\\] with mixing measure \\[\nP\\left(  F\\right)  =\\underset{T\\rightarrow\\infty}{\\lim}P(F_{T}),\n\\] where \\(F_{T}\\) is the empirical distribution of the data. At this level of generality, the distribution function is infinite-dimensional. In practice, additional subjective assumptions are needed that usually restrict the distribution function to finite dimensional spaces, which implies that the distribution function is indexed by a parameter vector \\(\\theta\\): \\[\np(y_{1},\\ldots,y_{T})=\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\\right)\ndP\\left(  \\theta\\right).\n\\] To operationalize this result, the researcher needs to choose the likelihood function and the prior distribution of the parameters.\nAt first glance, de Finetti’s theorem may seem to suggest that there is a single model or likelihood function. This is not the case however, as models can be viewed in the same manner as parameters. Denoting a model specification by \\(\\mathcal{M}\\), then de Finetti’s theorem would imply that \\[\\begin{align*}\np(y_{1},\\ldots,y_{T})  &  =\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta ,\\mathcal{M}\\right)  p\\left(  \\theta \\mid \\mathcal{M}\\right)  p\\left(\\mathcal{M}\\right)  d\\theta d\\mathcal{M}\\\\\n&  =\\int p(y_{1},\\ldots,y_{T} \\mid \\mathcal{M})p\\left(  \\mathcal{M}\\right)\nd\\mathcal{M},\n\\end{align*}\\] in the case of a continuum of models. Thus, under the mild assumption of exchangeability, it is as if the \\(y_{t}\\)’s are generated from \\(p\\left( y_{t} \\mid \\theta,\\mathcal{M}\\right)\\), conditional on the random variables \\(\\theta\\) and \\(\\mathcal{M}\\), where \\(p\\left( \\theta \\mid \\mathcal{M}\\right)\\) are the beliefs over \\(\\theta\\) in model \\(\\mathcal{M}\\), and \\(p\\left(\\mathcal{M}_{j}\\right)\\) are the beliefs over model specifications.\nThe objective approach has been a prevailing one in scientific applications. However, it only applies to events that can be repeated under the same conditions a very large number of times. This is rarely the case in many important applied problems. For example, it is hard to repeat an economic event, such as a Federal Reserve meeting or the economic conditions in 2008 infinitely often. This implies that at best, the frequentist approach is limited to laboratory situations. Even in scientific applications, when we attempt to repeat an experiment multiple times, an objective approach is not guaranteed to work. For example, the failure rate of phase 3 clinical trials in oncology is 60% (Shen et al. (2021),Sun et al. (2022)). Prior to phase 3, the drug is usually tested on several hundred patients.\nSubjective probability is a more general definition of probability than the frequentist definition, as it can be used for all types of events, both repeatable and unrepeatable events. A subjectivist has no issues discussing the probability of a lection result, even though the underlying conditions has not been observed before. As Ramsey (1926) puts it, “the probability is simply the willingness to bet on an event with a counterparty”.\nThe event does not even has to be uncertain in nature. For example, the probability of me having coins in my pocket will depend on who is asked to make the assessment. I, knowing the contents of my pocket, will say the probability is 0. However, if you are asked to make the assessment, you will say the probability is 0.5, as you do not know the contents of my pocket. This is a classic example of subjectivist probability.\nSimilarly, consider the number of people currently in Antarctica. This number is fixed and deterministic at any given moment, yet different individuals will assign different probability distributions to this quantity based on their knowledge. A researcher who recently reviewed Antarctic population statistics might have a tight distribution centered around the correct value, while someone with no such knowledge might have a much wider distribution. A logistics coordinator for a polar research station would have precise information about personnel at their specific facility but uncertainty about other stations. Each of these represents valid subjective probabilities over the same underlying fixed quantity, illustrating how probability in the Bayesian sense quantifies personal uncertainty rather than intrinsic randomness.\nThe main difficulty in operationalizing subjective probability is the process of actually quantifying subjective beliefs into numeric probabilities. One practical approach is to elicit probabilities through a sequence of carefully designed bets.\nConsider eliciting someone’s probability distribution over the number of people in Antarctica. We could start by asking: “Would you accept a bet that pays $100 if the number is below 5,000, and you pay $50 if it’s above 5,000?” If they accept, this suggests they believe \\(P(\\text{population} &lt; 5000) &gt; 1/3\\). We then adjust the threshold and payoffs systematically. For instance, we might ask about betting on the population being below 2,000, or below 1,000, gradually narrowing down probability mass at different intervals.\nFor continuous quantities, we can elicit a full distribution through a sequence of binary bets about quantiles. By asking someone to specify values \\(q_{0.25}, q_{0.5}, q_{0.75}\\) such that they are indifferent between bets paying equal amounts if the true value falls below or above each threshold, we construct their 25th, 50th, and 75th percentiles. This process, known as probability elicitation, transforms abstract beliefs into concrete probability distributions by observing revealed preferences through betting behavior.\nThe betting framework provides two key advantages. First, it forces coherence: if someone states inconsistent probabilities (such as \\(P(A) + P(\\neg A) \\neq 1\\)), an adversary could construct a Dutch book—a set of bets that guarantees a loss regardless of the outcome. The threat of sure loss incentivizes rational probability assignments. Second, betting naturally handles non-repeatable events. We can elicit probabilities about tomorrow’s Supreme Court decision or next quarter’s GDP growth, neither of which has a frequentist interpretation.\nInstead of using repetitive experiments, subjective probabilities can be measured using betting odds, which have been used for centuries to gauge the uncertainty over an event. The probability attributed to winning a coin toss is revealed by the type of odds one would accept to bet. Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.\n\n\nPosterior Empirical CDF\nLet \\(m = \\{ f_\\theta ( y ) : y \\in \\mathcal{Y} \\}\\) be a model. When necessary we index the parameters in model \\(m\\), as \\(\\theta_m\\). Let \\(y = ( y_1 , \\ldots , y_n )\\) be a vector of signals. The conditional likelihood, under \\(m\\), is given by \\(f_\\theta(y) =  \\prod_{i=1}^n f_\\theta ( y_i )\\). We also allow for the possibility that the data is generated from a model \\(f\\) that does not belong to the family of models \\(f_\\theta\\).\nGiven a prior measure, \\(\\Pi ( d F )\\), over \\(\\mathcal{F}\\) the set of distributions, we can calculate the predictive density \\[\nf_n  ( y_{n+1} | y_1 , \\ldots , y_n ) = \\int f (y) \\Pi_n ( d F ) \\; \\text{where}\\; \\Pi_n ( d f ) = \\frac{ \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }{  \\int  \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }\n\\] Under the family, \\(f_\\theta\\), we can calculate the parameter posterior as \\[\np( \\theta | y ) =  \\frac{ \\prod_{i=1}^n f_\\theta ( y_i ) p(\\theta)  d \\theta }{  m(y) } \\;  \\text{where}\\; m(y) = \\int f_\\theta (y) p( \\theta ) d \\theta\n\\] Here \\(p(\\theta)\\) is a prior distribution over parameters and \\(m(y)\\) is the marginal distribution of the data implied by the model. There are many applications in Bayesian non-parametric statistics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sufficient-statistic-summary-statistic",
    "href": "03-bl.html#sufficient-statistic-summary-statistic",
    "title": "3  Bayesian Learning",
    "section": "3.2 Sufficient Statistic (Summary Statistic)",
    "text": "3.2 Sufficient Statistic (Summary Statistic)\nA statistic \\(S(y)\\) is sufficient for \\(\\theta\\), if the conditional distribution of \\(y\\) given \\(S(y)\\) is independent of \\(\\theta\\), namely \\[\np(y\\mid S(y),\\theta) = p(y\\mid S(y)).\n\\] Then, we can view \\(S(y)\\) as a dimension reducing map, as inference for \\(\\theta\\) can be solely determined by \\(S\\). This follows as \\(S(y)\\) is a deterministic map of \\(y\\), so with \\(P(S(y))&gt;0\\), we have \\[\np(y,S(y)\\mid \\theta) = p(y\\mid S(y),\\theta)p(S(y)\\mid \\theta) = p(y\\mid S(y))p(S(y)\\mid \\theta) \\propto p(S(y)\\mid\\theta)\n\\]\nIn Bayesian inference, we need to compute the posterior over unknown model parameters \\(\\theta\\), given data \\(y\\). The posterior density is denoted by \\(p(\\theta \\mid y)\\). Here \\(y = ( y_1 , \\ldots , y_n )\\) is high dimensional. A map from data to a real number or to a low-dimension vector \\[\nS = S(y) = ( S_1(y) , \\ldots , S_k(y) )\n\\] is called a statistic. Since statistic is a deterministic function of the data, then \\[\np(y\\mid \\theta) = p(y,S\\mid \\theta) = p(S\\mid \\theta)p(y\\mid S,\\theta).\n\\] If it happens that the likelihood is conditionally independent on \\(\\theta\\), given \\(S\\) then \\[\np(y\\mid \\theta) = p(S\\mid \\theta)p(y\\mid S).\n\\] In this case the statistic \\(S\\) is called the sufficient statistic for parameter \\(\\theta\\) given data \\(y\\). In other words all the information needed for estimating \\(\\theta\\) is given by \\(S\\).\nThere is a nice connection between the posterior mean and the sufficient statistics, especially minimal sufficient statistics in the exponential family. If there exists a sufficient statistic \\(S^*\\) for \\(\\theta\\), then Kolmogorov (1942) shows that for almost every \\(y\\), \\(p(\\theta\\mid y) = p(\\theta\\mid S^*(y))\\) , and further \\(S(y) = E_{p}(\\theta \\mid y) = E_{p}(\\theta \\mid S^*(y))\\) is a function of \\(S^*(y)\\). In the special case of an exponential family with minimal sufficient statistic \\(S^*\\) and parameter \\(\\theta\\), the posterior mean \\(S(y) = E_{p}(\\theta \\mid y)\\) is a one-to-one function of \\(S^*(y)\\), and thus is a minimal sufficient statistic.\nSummary Statistic: Let \\(S(y)\\) be a sufficient summary statistic in the Bayes sense (Kolmogorov (1942)), if for every prior \\(p\\) \\[\nf_B (y) :=   p_{\\theta \\mid y}(\\theta \\in B\\mid y) = p_{\\theta \\mid s(y)}(\\theta \\in B\\mid s(y)).\n\\] Then we need to use our pattern matching dataset \\((y^{(i)} , \\theta^{(i)})\\) which is simulated from the prior and forward model to “train” the set of functions \\(f_B (y)\\), where we pick the sets \\(B = ( - \\infty , q ]\\) for a quantile \\(q\\). Hence, we can then interpolate in between.\nEstimating the full sequence of functions is then done by interpolating for all Borel sets \\(B\\) and all new data points \\(y\\) using a NN architecture and conditional density NN estimation.\nThe notion of a summary statistic is prevalent in the ABC literature and is tightly related to the notion of a Bayesian sufficient statistic \\(S^*\\) for \\(\\theta\\), then (Kolmogorov 1942), for almost every \\(y\\), \\[\np(\\theta \\mid  Y=y) = p(\\theta \\mid S^*(Y) = S^*(y))\n\\] Furthermore, \\(S(y) = \\mathrm{E}\\left(\\theta \\mid Y = y\\right) = \\mathrm{E}_{p}\\left(\\theta \\mid S^*(Y) = S^*(y)\\right)\\) is a function of \\(S^*(y)\\). In the case of the exponential family, we have \\(S(Y) = \\mathrm{E}_{p}\\left(\\theta | Y \\right)\\) is a one-to-one function of \\(S^*(Y)\\), and thus is a minimal sufficient statistic.\nSufficient statistics are generally kept for parametric exponential families, where \\(S(\\cdot)\\) is given by the specification of the probabilistic model. However, many forward models have an implicit likelihood and no such structures. The generalization of sufficiency is a summary statistic (a.k.a. feature extraction/selection in a neural network). Hence, we make the assumption that there exists a set of features such that the dimensionality of the problem is reduced.\n\nExample 3.1 (Posterior Distribution for Coin Toss) What if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants? In this case, we need to estimate the probability of heads \\(\\theta\\) from the data. Suppose we have observed 10 flips \\[\n\\{H, T, H, H, H, T, H, T, H, H\\},\n\\] and only three of them were tails. What is the probability that the next flip will be tail? The frequency-based answer would be \\(3/10 = 0.3\\). However, the Bayes approach gives us more flexibility. Suppose we have a prior belief that the coin is fair, but we are not sure. We can model this belief by a prior distribution. Let’s discretize the variable \\(\\theta\\) and assign prior probabilities to each value of \\(\\theta\\) as follows\n\ntheta &lt;- seq(0, 1, by = 0.1)\nprior = c(0, 0.024, 0.077, 0.132, 0.173, 0.188, 0.173, 0.132, 0.077, 0.024, 0)\nbarplot(prior, names.arg = theta, xlab = \"theta\", ylab = \"prior\")\n\n\n\n\nPrior distribution\n\n\n\n\nWe put most of the mass to the fair assumption (\\(\\theta = 0.5\\)) and zero mass to the extreme values \\(\\theta = 0\\) and \\(\\theta = 1\\). Our mass is exponentially decaying as we move away from 0.5. This is a reasonable assumption, since we are not sure about the fairness of the coin. Now, we can use Bayes rule to update our prior belief. The posterior distribution is given by \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}.\n\\] The denominator is the marginal likelihood, which is given by \\[\np(y) = \\sum_{\\theta} p(y \\mid \\theta) p(\\theta).\n\\] The likelihood is given by the Binomial distribution \\[\np(y \\mid \\theta) \\propto \\theta^3 (1 - \\theta)^7.\n\\] Notice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are sufficient for the inference about \\(\\theta\\). The posterior distribution is given by\n\nlikelihood &lt;- function(theta, n, Y) {\n  theta^Y * (1 - theta)^(n - Y)\n}\nposterior &lt;- likelihood(theta, 10,3) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\n\nPosterior distribution\n\n\n\n\nIf you are to keep collecting more observations and say observe a sequence of 100 flips and 30 of them were heads, then the posterior distribution will be more concentrated around the value of \\(\\theta = 0.3\\).\n\nposterior &lt;- likelihood(theta, 100,30) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\")\n\n\n\n\nPosterior distribution for n=100\n\n\n\n\nThis demonstrates that for large sample sizes, the frequentist approach and the Bayesian approach agree.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sec-betabinomial",
    "href": "03-bl.html#sec-betabinomial",
    "title": "3  Bayesian Learning",
    "section": "3.3 Beta-Binomial Model",
    "text": "3.3 Beta-Binomial Model\nThe Beta-Binomial Bayesian model is a statistical model that is used when we are interested in learning about a proportion or probability of success, denoted by \\(p\\). This model is particularly useful when dealing with binary data such as conversions or clicks in A/B testing.\nIn the Beta-Binomial model, we assume that the probability of success \\(\\theta\\) in each of \\(n\\) Bernoulli trials is not fixed but randomly drawn from a Beta distribution. The Beta distribution is defined by two shape parameters, \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\).\nThe model combines the prior information about \\(\\theta\\) (represented by the Beta distribution) and the observed data \\(y = (y_1, \\ldots, y_n)\\), where \\(y_i \\sim Binomial(m,\\theta)\\), to update our beliefs about \\(\\theta\\). This is done using Bayes’ Rule, which in this context can be written as: \\[\np(\\theta \\mid y) = \\dfrac{p(y \\mid \\theta)p(\\theta)}{p(y)}\n\\] where \\(p(\\theta)\\) is the prior distribution (Beta), \\(p(y \\mid \\theta)\\) is the likelihood function (Binomial), and \\(p(\\theta\\mid y)\\) is the posterior distribution.\nThe Beta distribution is a family of continuous probability distributions defined on the interval [0,1] in terms of two positive parameters, denoted by alpha (\\(\\alpha\\)) and beta (\\(\\beta\\)), that appear as exponents of the variable and its complement to 1, respectively, and control the shape of the distribution. The Beta distribution is frequently used in Bayesian statistics, empirical Bayes methods, and classical statistics to model random variables with values falling inside a finite interval.\nThe probability density function (PDF) of the Beta distribution is given by: \\[\nBeta(x; \\alpha, \\beta) = \\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\] where \\(x \\in [0, 1]\\), \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), and \\(B(\\alpha, \\beta)\\) is the beta function. It is simply a normalizing constant \\[\nB\\left( \\alpha,\\beta\\right)  =\\int_{0}^{1}\\theta^{\\alpha-1}\\left(  1-\\theta\\right)\n^{\\beta-1}d\\theta .\n\\]\nThe mean and variance of the Beta distribution are given by: \\[\n\\begin{aligned}\n\\mu &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\sigma^2 &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{aligned}\n\\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance.\nThe Beta-Binomial model is one of the simplest Bayesian models and is widely used in various fields including epidemiology, intelligence testing, and marketing. It provides the tools we need to study the proportion of interest, \\(\\theta\\), in a variety of settings.\nThe nice property of the Beta-Binomial model is that the posterior \\(p(p\\mid Y)\\) is yet another Beta distribution. Beta is called a conjugate prior for the Binomial likelihood and is a very useful property.\nWhen \\(m=1\\) (observations follow the Bernoulli distribution), the posterior is given by \\[\np(\\theta\\mid Y) = Beta(Y+\\alpha, 1-Y+\\beta)\n\\] where \\(Y\\) is the number of successful outcomes \\[\nY = \\sum_{i=1}^n y_i,\n\\] where \\(y_i \\sim Bernoulli(\\theta)\\).\nHere the count of successful outcome \\(Y\\) acts as a sufficient statistic for the parameter \\(\\theta\\). This means that the posterior distribution depends on the data only through the sufficient statistic \\(Y\\). This is a very useful property and is a consequence of the conjugacy of the Beta prior and Binomial likelihood.\nIn the case of \\(n&gt;1\\) (observations follow the Binomial distribution), the posterior is given by \\[\np(\\theta\\mid Y) = Beta(Y+\\alpha, n-Y+\\beta)\n\\] where \\(n\\) is the number of observations and \\(Y\\) is the number of successful outcomes as before. \\[\nY = \\sum_{i=1}^n y_i,\n\\] where \\(y_i \\sim Binomial(n,\\theta)\\).\nThe posterior mean and variance are \\[\n\\mathbb{E}\\left[ \\theta\\mid Y\\right]  =\\frac{\\alpha_{n}}{\\alpha_{n}+\\beta_{n}} \\;\\text{ and }\\; \\Var{\n\\theta\\mid Y}  =\\frac{\\alpha_{n}\\beta_{n}}{\\left(  \\alpha_{n}+\\beta_{n}\\right)  ^{2}\\left(   \\alpha_{n}+\\beta_{n}+1\\right)  }\\text{,}\n\\] where \\(\\alpha_{n} = \\alpha + Y\\) and \\(\\beta_{n} = \\beta + n - Y\\).\n\nExample 3.2 (Black Swans) A related problem is the Black Swan inference problem. Suppose that after \\(n\\) trials where \\(n\\) is large you have only seen successes and that you assess the probability of the next trial being a success as \\((T+1)/(T+2)\\) that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. Taleb (2007) makes it sound as if the rules of probability are not rich enough to be able to handle Black Swan events. There is a related class of problems in finance known as Peso problems where countries decide to devalue their currencies and there is little prior evidence from recent history that such an event is going to happen.\nTo obtain such a probability assessment we use a Binomial/Beta conjugate Bayes updating model. The key point is that it can also explain that there is still a large probability of a Black Swan event to happen sometime in the future. An independence model has difficulty doing this.\nThe Bayes Learning Beta-Binomial model will have no problem. We model with \\(y_{t}=0\\) or \\(1\\), with probability \\(P\\left( y_{t}=1\\mid \\theta\\right) =\\theta\\). This is the classic Bernoulli “coin-flipping” model and is a component of more general specifications such as regime switching or outlier-type models.\nLet \\(Y = \\sum_{t=1}^{T}y_{t}\\) be the number of observed successful outcomes. The likelihood for a sequence of Bernoulli observations is then \\[\np\\left(  y\\mid \\theta\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t}\\mid \\theta\\right)\n=\\theta^{Y}\\left(  1-\\theta\\right)^{T-Y}.\n\\] The maximum likelihood estimator is the sample mean, \\(\\widehat{\\theta} = T^{-1}Y\\). This makes little sense when you just observe white swans. It predicts \\(\\hat{\\theta} = 1\\) and gets shocked when it sees a black swan (zero probability event). Bayes, on the other hand, allows for “learning”.\nBayes rule then tells us how to combine the likelihood and prior to obtain a posterior distribution, namely \\(\\theta \\mid Y=y\\). What do we believe about \\(\\theta\\) given a sequence of observations? Our predictor rule is then \\(P(y_{t+1} =1 \\mid Y=y ) = \\mathbb{E}(\\theta \\mid y)\\) and it is straightforward to show that the posterior distribution is again a Beta distribution with \\[\np\\left( \\theta\\mid y\\right)  \\sim Beta\\left(  \\alpha_{n},\\beta_{n}\\right)  \\; \\mathrm{ and} \\;  \\alpha_{n}=\\alpha+k , \\beta_{n}=\\beta+T-k.\n\\]\n\n\nExample 3.3 (Clinical Trials) Consider a problem of designing clinical trials in which \\(K\\) possible drugs \\(a\\in 1,\\dots,K\\) need to be tested. The outcome of the treatment with drug \\(a\\) is binary \\(y(a) \\in \\{0,1\\}\\). We use Bernoulli distribution with mean \\(f(a)\\) to model the outcome. Thus, the full probabilistic model is described by \\(w = f(1),\\dots,f(K)\\). Say we have observed a sample \\(D = \\{y_1,\\dots,y_n\\}\\). We would like to compute posterior distribution over \\(w\\). We start with a Beta prior \\[\np(w\\mid \\alpha,\\beta) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha,\\beta)    \n\\] Then the posterior distribution is given by \\[\np(w\\mid D) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha + n_{a,1},\\beta + n_{a,0})   \n\\]\nThis setup allows us to perform sequential design of experiments. The simplest version of it is called Thompson sampling. After observing \\(n\\) patients, we draw a single sample \\(\\tilde w\\) from the posterior and then maximize the resulting surrogate \\[\na_{n+1} = \\arg\\max_{a} f_{\\tilde w}(a), ~~~ \\tilde{w} \\sim p(w\\mid D)\n\\]\n\n\nExample 3.4 (Shrinkage and Baseball Batting Averages) The batter-pitcher match-up is a fundamental element of a baseball game. There are detailed baseball records that are examined regularly by fans and professionals. This data provides a good illustration of Bayesian hierarchical methods. There is a great deal of prior information concerning the overall ability of a player. However, we only see a small amount of data about a particular batter-pitcher match-up. Given the relatively small sample size, to determine our optimal estimator we build a hierarchical model taking into account the within pitcher variation.\nLet’s analyze the variability in Jeter’s \\(2006\\) season. Let \\(p_{i}\\) denote Jeter’s ability against pitcher \\(i\\) and assume that \\(p_{i}\\) varies across the population of pitchers according to a particular probability distribution \\((p_{i} \\mid \\alpha,\\beta)\\sim Be(\\alpha,\\beta)\\). To account for extra-binomial variation we use a hierarchical model for the observed number of hits \\(y_{i}\\) of the form \\[\n(y_{i} \\mid p_{i})\\sim Bin(T_{i},p_{i})\\;\\;\\mathrm{with}\\;\\;p_{i}\\sim\nBe(\\alpha,\\beta)\n\\] where \\(T_{i}\\) is the number of at-bats against pitcher \\(i\\). A priori we have a prior mean given by \\(E(p_{i})=\\alpha/(\\alpha+\\beta)=\\bar{p}\\). The extra heterogeneity leads to a prior variance \\(Var(p_{i})=\\bar{p}(1-\\bar{p})\\phi\\) where \\(\\phi=(\\alpha+\\beta+1)^{-1}\\). Hence \\(\\phi\\) measures how concentrated the beta distribution is around its mean, \\(\\phi=0\\) means highly concentrated and \\(\\phi=1\\) means widely dispersed. \nThis model assumes that each player \\(i\\) has a true ability \\(p_{i}\\) that is drawn from a common distribution. The model is hierarchical in the sense that the parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from the data. The model is also a shrinkage model in the sense that the estimates of \\(p_{i}\\) are shrunk towards the overall mean \\(\\bar{p}_{i}\\). In reality, we don’t know that each \\(p_i\\) exists. We also don’t know if it follows a Binomial distribution with the Beta prior. We are making a model assumption. However, the model is a good approximation to the data and is a good way to estimate the parameters.\nStern et al. (2007) estimates the parameter \\(\\hat{\\phi} = 0.002\\) for Derek Jeter, showing that his ability varies a bit but not very much across the population of pitchers. The effect of the shrinkage is not surprising. The extremes are shrunk the most with the highest degree of shrinkage occurring for the match-ups that have the smallest sample sizes. The amount of shrinkage is related to the large amount of prior information concerning Jeter’s overall batting average. Overall Jeter’s performance is extremely consistent across pitchers as seen from his estimates. Jeter had a season \\(.308\\) average. We see that his Bayes estimates vary from\\(.311\\) to\\(.327\\) and that he is very consistent. If all players had a similar record then the assumption of a constant batting average would make sense.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nR. Mendoza\n6\n5\n.833\n.322\n(.282, .394)\n\n\nH. Nomo\n20\n12\n.600\n.326\n(.289, .407)\n\n\nA.J.Burnett\n5\n3\n.600\n.320\n(.275, .381)\n\n\nE. Milton\n28\n14\n.500\n.324\n(.291, .397)\n\n\nD. Cone\n8\n4\n.500\n.320\n(.218, .381)\n\n\nR. Lopez\n45\n21\n.467\n.326\n(.291, .401)\n\n\nK. Escobar\n39\n16\n.410\n.322\n(.281, .386)\n\n\nJ. Wettland\n5\n2\n.400\n.318\n(.275, .375)\n\n\nT. Wakefield\n81\n26\n.321\n.318\n(.279, .364)\n\n\nP. Martinez\n83\n21\n.253\n.312\n(.254, .347)\n\n\nK. Benson\n8\n2\n.250\n.317\n(.264, .368)\n\n\nT. Hudson\n24\n6\n.250\n.315\n(.260, .362)\n\n\nJ. Smoltz\n5\n1\n.200\n.314\n(.253, .355)\n\n\nF. Garcia\n25\n5\n.200\n.314\n(.253, .355)\n\n\nB. Radke\n41\n8\n.195\n.311\n(.247, .347)\n\n\nD. Kolb\n5\n0\n.000\n.316\n(.258, .363)\n\n\nJ. Julio\n13\n0\n.000\n.312\n(.243, .350 )\n\n\nTotal\n6530\n2061\n.316\n\n\n\n\n\nSome major league managers believe strongly in the importance of such data (Tony La Russa, Three days in August). One interesting example is the following. On Aug 29, 2006, Kenny Lofton (career \\(.299\\) average, and current \\(.308\\) average for \\(2006\\) season) was facing the pitcher Milton (current record \\(1\\) for \\(19\\)). He was rested and replaced by a \\(.273\\) hitter. Is putting in a weaker player really a better bet? Was this just an over-reaction to bad luck in the Lofton-Milton match-up? Statistically, from Lofton’s record against Milton we have \\(P\\left( \\leq 1\\;\\mathrm{hit\\;in}\\ 19\\;\\mathrm{attempts} \\mid p=0.3\\right) =0.01\\) an unlikely \\(1\\)-in-\\(100\\) event. However, we have not taken into account the multiplicity of different batter-pitcher match-ups. We know that Lofton’s batting percentage will vary across different pitchers, it’s just a question of how much? A hierarchical analysis of Lofton’s variability gave a \\(\\phi=0.008\\) – four times larger than Jeter’s \\(\\phi=0.002\\). Lofton has batting estimates that vary from \\(.265\\) to \\(.340\\) with the lowest being against Milton. Hence, the optimal estimate for a pitch against Milton is \\(.265&lt;.275\\) and resting Lofton against Milton is justified by this analysis.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-model-for-count-data",
    "href": "03-bl.html#poisson-model-for-count-data",
    "title": "3  Bayesian Learning",
    "section": "3.4 Poisson Model for Count Data",
    "text": "3.4 Poisson Model for Count Data\nThe Poisson distribution is obtained as a result of the Binomial when \\(p\\) is small and \\(n\\) is large. In applications, the Poisson models count data. Suppose we want to model the arrival rate of users to one of our stores. Let \\(\\lambda = np\\), which is fixed and take the limit as \\(n \\rightarrow \\infty\\). There is a relationship between \\(p(x)\\) and \\(p(x+1)\\) given by \\[\n\\dfrac{p(x+1)}{p(x)}= \\dfrac{\\left(\\dfrac{n}{x+1}\\right)p^{x+1}(1-p)^{n-x-1}}{\\left(\\dfrac{n}{x}\\right)p^{x}(1-p)^{n-x}} \\approx \\dfrac{np}{x+1}\n\\] If we approximate \\(p(x+1)\\approx \\lambda p(x)/(x+1)\\) with \\(\\lambda=np\\), then we obtain the Poisson pdf given by \\(p(x) = p(0)\\lambda^x/x!\\). To ensure that \\(\\sum_{x=0}^\\infty p(x) = 1\\), we set \\[\nf(0) = \\dfrac{1}{\\sum_{x=0}^{\\infty}\\lambda^x/x!} = e^{-\\lambda}.\n\\] The above equality follows from the power series property of the exponent function \\[\ne^{\\lambda} = \\sum_{x=0}^{\\infty}\\dfrac{\\lambda^x}{x!}\n\\] The Poisson distribution counts the occurrence of events. Given a rate parameter, denoted by \\(\\lambda\\), we calculate probabilities as follows \\[\np( X = x ) = \\frac{ e^{-\\lambda} \\lambda^x }{x!} \\; \\mathrm{ where} \\; x=0,1,2,3, \\ldots\n\\] The mean and variance of the Poisson are given by:\n\n\n\nPoisson Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = \\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = \\lambda\\)\n\n\n\nHere \\(\\lambda\\) denotes the rate of occurrence of an event.\nConsider the problem of modeling soccer scores in the English Premier League (EPL) games. We use data from Betfair, a website, which posts odds on many football games. The goal is to calculate odds for the possible scores in a match. \\[\n0-0, \\; 1-0, \\; 0-1, \\; 1-1, \\; 2-0, \\ldots\n\\]\nAnother question we might ask, is what’s the odds of a team winning? This is given by \\(P\\left ( X&gt; Y \\right )\\). The odds of a draw are given by \\(P \\left ( X = Y \\right )\\). Here \\(X\\) is the number of goals scored by the home team and \\(Y\\) is the number of goals scored by the away team.\nProfessional sports bettors rely on sophisticated statistical models to predict the outcomes. Instead, we present a simple, but useful model for predicting outcomes of EPL games. We follow the methodology given in Spiegelhalter and Ng (2009).\nTo make the discussion more concrete, we will use the data from the English Premier League (EPL) games from the 2014/2015 season. and will model the game between Manchester United and Hull City.\nFirst, load the data and then model the number of goals scored using Poisson distribution for each team.\n\ndf = read.csv(\"../data/epl.csv\")\nknitr::kable(head(df[,c(\"home_team_name\",\"away_team_name\",\"home_score\",\"guest_score\")]))\n\n\n\n\nhome_team_name\naway_team_name\nhome_score\nguest_score\n\n\n\n\nArsenal\nLiverpool\n3\n4\n\n\nBournemouth\nManchester United\n1\n3\n\n\nBurnley\nSwansea\n0\n1\n\n\nChelsea\nWest Ham\n2\n1\n\n\nCrystal Palace\nWest Bromwich Albion\n0\n1\n\n\nEverton\nTottenham\n1\n1\n\n\n\n\n\nLet’s look at the empirical distribution across the number of goals scored by Manchester United\n\nteam_name=\"Manchester United\" \nteam_for  = c(df[df$home_team_name==team_name,\"home_score\"],df[df$away_team_name==team_name,\"guest_score\"]) \nn = length(team_for) \nfor_byscore = table(team_for)/n \nbarplot(for_byscore, col=\"coral\", main=\"\")\n\n\n\n\nHistogram of Goals Scored by MU\n\n\n\n\nHence the historical data fits closely to a Poisson distribution, the parameter \\(\\lambda\\) describes the average number of goals scored and we calculate it by calculating the sample mean, the maximum likelihood estimate. A Bayesian method where we assume that \\(\\lambda\\) has a Gamma prior is also available. This lets you incorporate outside information into the predictive model.\n\nlambda_for = mean(team_for) \nbarplot(rbind(dpois(0:4, lambda = lambda_for),for_byscore),beside = T, col=c(\"aquamarine3\",\"coral\"), xlab=\"Goals\", ylab=\"probability\", main=\"\") \nlegend(\"topright\", c(\"Poisson\",\"MU\"), pch=15, col=c(\"aquamarine3\", \"coral\"), bty=\"n\")\n\n\n\n\nHistogram vs Poisson Model Prediction of Goals Scored by MU\n\n\n\n\nNow we will use Poisson model and Monte Carlo simulations to predict possible outcomes of the MU vs Hull games. First we estimate the rate parameter for goals by MU lmb_mu and goals by Hull lmb_h. Each team played a home and away game with every other team, thus 38 total games was played by all teams. We calculate the average by dividing total number of goals scored by the number of games\n\nsumdf = df %&gt;% \n  group_by(home_team_name) %&gt;% \n  summarise(Goals_For_Home = sum(home_score)) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;% \n              summarise(Goals_For_Away = sum(guest_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(home_team_name) %&gt;% \n              summarise(Goals_Against_Home = sum(guest_score))\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;%\n              summarise(Goals_Against_Away = sum(home_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  rename(Team=home_team_name)\nknitr::kable(sumdf[sumdf$Team %in% c(\"Manchester United\", \"Hull\"),])\n\n\n\n\n\n\n\n\n\n\n\nTeam\nGoals_For_Home\nGoals_For_Away\nGoals_Against_Home\nGoals_Against_Away\n\n\n\n\nHull\n28\n9\n35\n45\n\n\nManchester United\n26\n28\n12\n17\n\n\n\n\n\n\nlmb_mu = (26+28)/38; print(lmb_mu)\n\n## [1] 1.4\n\nlmb_h = (28+9)/38; print(lmb_h)\n\n## [1] 0.97\n\n\nNow we simulate 100 games between the teams\n\nx = rpois(100,lmb_mu)\ny = rpois(100,lmb_h)\nsum(x&gt;y)\n\n## [1] 41\n\nsum(x==y)\n\n## [1] 25\n\nknitr::kable(table(x,y))\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n14\n10\n4\n2\n\n\n1\n9\n9\n10\n5\n\n\n2\n8\n9\n2\n3\n\n\n3\n5\n3\n1\n0\n\n\n4\n1\n2\n2\n0\n\n\n5\n0\n0\n0\n1\n\n\n\n\n\nFrom our simulation that 41 number of times MU wins and 25 there is a draw. The actual outcome was 0-0 (Hull at MU) and 0-1 (Mu at Hull). Thus our model gives a reasonable prediction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nhome_team_id\naway_team_id\nhome_team_name\naway_team_name\ndate_string\nhalf_time_score\nhome_score\nguest_score\n\n\n\n\n271\n271\n32\n214\nManchester United\nHull\n01/02/2017 20:00:00\n0 : 0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nhome_team_id\naway_team_id\nhome_team_name\naway_team_name\ndate_string\nhalf_time_score\nhome_score\nguest_score\n\n\n\n\n34\n34\n214\n32\nHull\nManchester United\n27/08/2016 17:30:00\n0 : 0\n0\n1\n\n\n\n\n\n\nThe model can be improved by calculating different averages for home and away games. For example, Hull does much better at home games compared to away games. Further, we can include the characteristics of the opponent team to account for interactions between attack strength (number of scored) and defense weakness of the opponent. Now we modify our value of expected goals for each of the teams by calculating \\[\n\\hat \\lambda = \\lambda \\times  \\text{Defense weakness}\n\\]\nLet’s model the MU at Hull game. The average away goals for MU \\(28/19 = 1.4736842\\) and the defense weakness of Hull is \\(36/19 = 1.8421053\\), thus the adjusted expected number of goals to be scored by MU is 2.7922438. Similarly, the adjusted number of goals Hull is expected to score is \\(28/19 \\times 17/19 = 1.3185596\\)\nAs a result of the simulation, we obtain\n\nset.seed(1)\nx &lt;- rpois(100, 28 / 19 * 35 / 19)\ny &lt;- rpois(100, 28 / 19 * 17 / 19)\nknitr::kable(table(x, y))\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n1\n3\n0\n0\n0\n0\n\n\n1\n3\n5\n6\n1\n1\n0\n\n\n2\n4\n16\n7\n3\n0\n0\n\n\n3\n6\n7\n2\n3\n0\n0\n\n\n4\n4\n7\n5\n2\n1\n0\n\n\n5\n2\n3\n1\n2\n0\n2\n\n\n6\n1\n0\n0\n1\n0\n0\n\n\n7\n1\n0\n0\n0\n0\n0\n\n\n\n\nimage(z = table(x, y), x = 0:7, y = 0:5, xlab = \"MU Score\", ylab = \"Hull Score\")\n\n\n\n\n\n\n\n\nNow we can calculate the number of times MU wins:\n\nsum(x &gt; y)\n\n## [1] 67\n\n\n\n\n\n\nA model is only as good as its predictions. Let’s see how well our model predicted the outcome of the MU vs Hull game. The actual outcome was 0-1 (MU at Hull). The model predicted 1-2 (16 games out of 100). In out simulation 0-1 was the cme (8 games out of 100). Man U wins 67 games out of 100, we should bet when odds ratio is below 67 to 100.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-gamma-learning-about-an-intensity",
    "href": "03-bl.html#poisson-gamma-learning-about-an-intensity",
    "title": "3  Bayesian Learning",
    "section": "3.5 Poisson-Gamma: Learning about an Intensity",
    "text": "3.5 Poisson-Gamma: Learning about an Intensity\nConsider a continuous-time stochastic process, \\(\\left\\{ N_{t}\\right\\} _{t\\geq0}\\), with \\(N_{0}=0\\), counting the number of events that have occurred up to time \\(t\\). The process is constant between event times, and jumps by one at event times: \\(\\Delta N_{t}=N_{t}-N_{t-}=1,\\) where \\(N_{t-}\\) is the limit from the left. The probability of an event over the next short time interval, \\(\\Delta t\\) is \\(\\lambda\\Delta t\\), and \\(N_{t}\\) is called a Poisson process because \\[\nP\\left[  N_{t}=k\\right]  =\\frac{e^{-\\lambda t}\\left(  \\lambda\nt\\right)  ^{k}}{k!}\\text{ for }k=1,\\ldots\n\\] which is the Poisson distribution, thus \\(N_{t}\\sim Poi\\left(\\lambda t\\right)\\). A more general version of the Poisson process is a Cox process, or doubly stochastic point process.\nHere, there is additional conditioning information in the form of state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\). The process now has two sources of randomness, one associated with the discontinuous jumps and another in the form of random state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\), that drive the intensity of the process. The intensity of the Cox process is \\(\\lambda_{t}=\\int_{0}^{t}\\lambda\\left( X_{s}\\right) ds\\), which is formally defined as \\[\nP\\left[  N_{t}-N_{s}=k \\mid \\left\\{  X_{u}\\right\\}  _{s\\leq u\\leq\nt}\\right]  =\\frac{\\left(  \\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)\n^{k}\\exp\\left(  -\\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)}{k!}, ~ k=0,1,\\ldots\n\\] Cox processes are very useful extensions to Poisson processes and are the basic building blocks of reduced form models of defaultable bonds.\nThe inference problem is to learn about \\(\\lambda\\) from a continuous-record of observation up to time \\(t\\). The likelihood function is given by \\[\np\\left(  N_{t}=k \\mid \\lambda\\right)  =\\frac{\\left(  \\lambda t\\right)  ^{k}%\n\\exp\\left(  -\\lambda t\\right)  }{k!},\n\\] and the MLE is \\(\\widehat{\\lambda}=N_{t}/t\\). The MLE has the unattractive property that prior to the first event \\(\\left\\{ t:N_{t}=0\\right\\}\\), the MLE is 0, despite the fact that the model explicitly assumes that events are possible. This problem often arises in credit risk contexts, where it would seem odd to assume that the probability of default is zero just because a default has not yet occurred.\nA natural prior for this model is the Gamma distribution, which has the following pdf \\[\np\\left(  \\lambda \\mid a,A\\right)  =\\frac{A^{a}}{\\Gamma(a)  }\\lambda^{a-1}\\exp\\left(  -A\\lambda\\right)  \\text{.}\n\\tag{3.2}\\] Like the beta distribution, a Gamma prior distribution allows for a variety of prior shapes and is parameterized by two hyperparameters. Combining the prior and likelihood, the posterior is also Gamma: \\[\np\\left(  \\lambda \\mid N_{t}\\right)  \\propto\\frac{\\left(  \\lambda\\right)\n^{N_{t}+a-1}\\exp\\left(  -\\lambda\\left(  t+A\\right)  \\right)  }{N_{t}!}%\n\\sim\\mathcal{G}\\left(  a_{t},A_{t}\\right)  ,\n\\] where \\(a_{t}=N_{t}+a\\) and \\(A_{t}=t+A\\). The expected intensity, based on information up to time \\(t\\), is \\[\n\\mathbb{E}\\left[  \\lambda \\mid N_{t}\\right]  =\\frac{a_{t}}{A_{t}}=\\frac{N_{t}%\n+a}{t+A}=w_{t}\\frac{N_{t}}{t}+\\left(  1-w_{t}\\right)  \\frac{a}{A},\n\\] where the second line expresses the posterior mean in shrinkage form as a weighted average of the MLE and the prior mean where \\(w_{t}=t/(t+A)\\). In large samples, \\(w_{t}\\rightarrow1\\) and \\(E\\left( \\lambda \\mid N_{t}\\right) \\approx N_{t}/t=\\widehat{\\lambda}\\).\nTo understand the updating mechanics, Figure 3.1 (right column) displays a simulated sample path, posterior means, and (5%,95%) posterior quantiles for various prior configurations. In this case, time is measured in years and the intensity used to simulate the data is \\(\\lambda=1\\), implying on average one event per year. The four prior configurations embody different beliefs. In the first case, in the middle left panel, \\(a=4\\) and \\(A=1\\), captures a high-activity prior, that posits that jumps occur, on average, four times per year, and there is substantial prior uncertainty over the arrival rate as the (5%,95%) prior quantiles are (1.75,6.7). In the second case, captures a prior that is centered over the true value with modest prior uncertainty. The third case captures a low-activity prior, with a prior mean of 0.2 jumps/year. The fourth case captures a dogmatic prior, that posits that jumps occur three times per year, with high confidence in these beliefs.\nThe priors were chosen to highlight different potential paths for Bayesian learning. The first thing to note from the priors is the discontinuity upward at event times, and the exponential decrease during periods of no events, both of which are generic properties of Bayesian learning in this model. If one thinks of the events as rare, this implies rapid revisions in beliefs at event times and a constant drop in estimates of the intensity in periods of no events. For the high-activity prior and the sample path observed, the posterior begins well above \\(\\lambda=1\\), and slowly decreases, getting close to \\(\\lambda=1\\) at the end of the sample. This can be somewhat contrasted with the low-activity prior, which has drastic revisions upward at jump times. In the dogmatic case, there is little updating at event times. The prior parameters control how rapidly beliefs change, with noticeable differences across the priors.\nset.seed(8) # Ovi\nt = 1:5\nlmb = 1\nN = rpois(5,t*lmb)\n\n# A: rate (beta), a: shape (alpha)\nplotgamma = function(a,A,N) {\n    x = seq(0,10,0.01)\n    plot(x,dgamma(x,a,A),type=\"l\",xlab=\"t\",ylab=\"Gamma(t)\")\n    at = N+a\n    At = t+A\n    mean = at/At\n    plot(N, type='l', col=\"orange\", ylim=c(0,5), xlab=\"t\", ylab=\"N(t)\", lwd=2)\n    lines(mean, col=\"blue\", lwd=3, lty=2)\n    lines(qgamma(0.05,at,At), col=\"grey\", lwd=2, lty=2)\n    lines(qgamma(0.95,at,At), col=\"grey\", lwd=2, lty=2)\n}\nplotgamma(a=4,A=1, N)\nplotgamma(a=1,A=1, N)\nplotgamma(a=1,A=5, N)\nplotgamma(a=30,A=10, N)\n\n\n\n\n\n\n\n\n\n\n\n(a) a = 4, A = 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) a = 1, A = 1\n\n\n\n\n\n\n\n\n\n\n\n(d) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) a = 1, A = 5\n\n\n\n\n\n\n\n\n\n\n\n(f) Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) a = 30, A = 10\n\n\n\n\n\n\n\n\n\n\n\n(h) Posterior\n\n\n\n\n\n\n\nFigure 3.1: Sensitivity of Gamma Prior for Poisson Process\n\n\n\nPoisson event models are often embedded as portion of more complicated model to capture rare events such as stock market crashes, volatility surges, currency revaluations, or defaults. In these cases, prior distributions are often important–even essential–since it is common to build models with events that could, but have not yet occurred. These events are often called ‘Peso’ events. For example, in the case of modeling corporate defaults a researcher wants to allow for a jump to default. This requires positing a prior distribution that places non-zero probability on an event occurring. Classical statistical methods have difficulties dealing with these situations since the MLE of the jump probability is zero, until the first event occurs.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-normal-model-for-continuous-data",
    "href": "03-bl.html#normal-normal-model-for-continuous-data",
    "title": "3  Bayesian Learning",
    "section": "3.6 Normal-Normal Model for Continuous Data",
    "text": "3.6 Normal-Normal Model for Continuous Data\nThe Normal or Gaussian distribution is central to probability and statistical inference. Suppose that we are trying to predict tomorrow’s return on the S&P500. There’s a number of questions that come to mind\n\nWhat is the random variable of interest?\nHow can we describe our uncertainty about tomorrow’s outcome?\nInstead of listing all possible values we’ll work with intervals instead. The probability of an interval is defined by the area under the probability density function.\n\nReturns are continuous (as opposed to discrete) random variables. Hence a normal distribution would be appropriate - but on what scale? We will see that on the log-scale a Normal distribution provides a good approximation.\nThe most widely used model for a continuous random variable is the normal distribution. Standard normal random variable \\(Z\\) has the following properties\nThe standard Normal has mean \\(0\\) and has a variance \\(1\\), and is written as \\[\nZ \\sim N(0,1)\n\\] Then, we have the probability statements of interest \\[\\begin{align*}  \nP(-1 &lt;Z&lt; 1) &=0.68\\\\\nP(-1.96 &lt;Z&lt; 1.96) &=0.95\\\\\n\\end{align*}\\]\nIn R, we can find probabilities\n\npnorm(1.96)\n\n## [1] 0.98\n\n\nand quantiles\n\nqnorm(0.9750)\n\n## [1] 2\n\n\nThe quantile function qnorm is the inverse of pnorm.\nA random variable that follows normal distribution with general mean and variance \\(X \\sim \\mbox{N}(\\mu, \\sigma^2)\\), has the following properties \\[\\begin{align*}\n  p(\\mu - 2.58 \\sigma &lt; X &lt; \\mu + 2.58 \\sigma) &=0.99 \\\\\n  p(\\mu - 1.96 \\sigma &lt; X &lt; \\mu + 1.96 \\sigma) &=0.95 \\, .\n\\end{align*}\\] The chance that \\(X\\) will be within \\(2.58 \\sigma\\) of its mean is \\(99\\%\\), and the chance that it will be within \\(2\\sigma\\) of its mean is about \\(95\\%\\).\nThe probability model is written \\(X \\sim N(\\mu,\\sigma^2)\\), where \\(\\mu\\) is the mean, \\(\\sigma^2\\) is the variance. This can be transformed to a standardized normal via \\[\nZ =\\frac{X-\\mu}{\\sigma} \\sim N(0,1).\n\\] For a normal distribution, we know that \\(X \\in [\\mu-1.96\\sigma,\\mu+1.96\\sigma]\\) with probability 95%. We can make similar claims for any other distribution using the Chebyshev’s empirical rule, which is valid for any population:\n\nAt least 75% probability lies within 2\\(\\sigma\\) of the mean \\(\\mu\\)\nAt least 89% lies within 3\\(\\sigma\\) of the mean \\(\\mu\\)\nAt least \\(100(1-1/m^2)\\)% lies within \\(m\\times \\sigma\\) of the mean \\(\\mu\\).\n\nThis also holds true for the Normal distribution. The percentages are \\(95\\)%, \\(99\\)% and \\(99.99\\)%.\n\nExample 3.5 (Google Stock 2019) Consider observations of daily log-returns of a Google stock for 2019 Daily log-return on day \\(t\\) is calculated by taking a logarithm of the ratio of price at close of day \\(t\\) and at close of day \\(t-1\\) \\[\n  y_t = \\log\\left(\\dfrac{P_t}{P_{t-1}}\\right)\n\\] For example on January 3 of 2017, the open price is 778.81 and close price was 786.140, then the log-return is \\(\\log(786.140/778.81) =  -0.0094\\). It was empirically observed that log-returns follow a Normal distribution. This observation is a basis for Black-Scholes model with is used to evaluate future returns of a stock.\n\np = read.csv(\"../data/GOOG2019.csv\")$Adj.Close; n = length(p) \nr = log(p[2:n]/p[1:(n-1)]) \nhist(r, breaks=30, col=\"lightblue\", main=\"\")\n\n\n\n\n\n\n\n\nObservations on the far right correspond to the days when positive news was released and on the far left correspond to bad news. Typically, those are days when the quarterly earnings reports are released.\nTo estimate the expected value \\(\\mu\\) (return) and standard deviation \\(\\sigma\\) (a measure of risk), we simply calculate their sample counterparts \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, ~\\mathrm{ and }~    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x} )^2\n\\] The empirical (or sample) values \\(\\bar x\\) and \\(s^2\\) are called sample mean and sample variance. Here simply vie them as our best guess about the mean and variance of the normal distribution model then our probabilistic model for next day’s return is then given by \\[\nR \\sim N(\\bar x, s^2).\n\\]\nSay we are interested in investing into Google and would like to calculated the expected return of our investment as well as risk associated with this investment We assume that behavior of the returns in the future will be the same as in 2019.\n\nn = length(r) \nrbar = sum(r)/n; print(rbar) \n\n## [1] 0.00098\n\ns2 = sum((r-rbar)^2)/(n-1); print(s2) \n\n## [1] 0.00023\n\nx = seq(-0.08,0.08, length.out = 200) \nhist(r, breaks=30, col=\"lightblue\", freq = F, main=\"\", xlab=\"\") \nlines(x,dnorm(x,rbar,sqrt(s2)), col=\"red\", lwd=3)\n\n\n\n\nHistogram (blue) and fitted normal curve (red) of for the Google stock daily return data.\n\n\n\n\nNow, assume, I invest all my portfolio into Google. I can predict my annual return to be \\(251 \\times 9.7981977\\times 10^{-4}\\) = 0.2459348 and risk (volatility) of my investment is \\(\\sqrt{s^2}\\) = 1.5198424% a year.\nI can predict the risk of loosing 3% or more in one day using my model is 1.93%.\n\npnorm(log(1-0.03), rbar, sqrt(s2))*100\n\n## [1] 1.9\n\n\n\nsp = read.csv(\"../data/SPMonthly.csv\")$Adj.Close; n = length(sp) \nspret = sp[602:n]/sp[601:(n-1)]-1 # Calculate  1977-1987 returns \n\n\nmean(spret) \n\n## [1] 0.012\n\nsd(spret)\n\n## [1] 0.043",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-mean",
    "href": "03-bl.html#normal-with-unknown-mean",
    "title": "3  Bayesian Learning",
    "section": "3.7 Normal With Unknown Mean",
    "text": "3.7 Normal With Unknown Mean\nLet \\(Y\\) be a random variable with a normal distribution, \\(Y \\sim N(\\mu, \\sigma^2)\\). The mean \\(\\mu\\) is unknown, but the variance \\(\\sigma^2\\) is known. The likelihood function is given by \\[\np(y \\mid \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)\n\\] The MLE of \\(\\mu\\) is \\(\\hat{\\mu} = \\bar{y}\\), the sample mean. Normal prior for the mean parameter \\(\\mu\\) is conjugate to the normal likelihood. \\[\n\\mu \\sim N(\\mu_0, \\sigma_0^2)\n\\] The posterior distribution is also normal. \\[\np(\\mu \\mid y) \\sim N(\\mu_n, \\sigma_n^2)\n\\]\nwhere \\[\n\\mu_n = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\bar{y}\n\\] and \\[\n\\sigma_n^2 = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\n\\] The posterior mean is a weighted average of the prior mean and the sample mean, with the weights being proportional to the precision of the prior and the likelihood. The posterior variance is smaller than the prior variance, and the sample size \\(n\\) appears in the denominator. The posterior mean is a shrinkage estimator of the sample mean, and the amount of shrinkage is controlled by the prior variance \\(\\sigma_0^2\\). A couple of observations \\[\n\\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ and } \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\rightarrow 1, \\text{ as } n \\rightarrow \\infty.\n\\] Further, \\[\n\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ as } n \\rightarrow \\infty.\n\\]\n\nExample 3.6 (Stylezed Example) Assuming the prior distribution \\(\\mu \\sim N(-1,1)\\), say we observed \\(y=2\\) and we want to update our beliefs about \\(\\mu\\). The likelihood function is \\(p(y \\mid \\mu) = N(\\mu,2)\\), and the posterior distribution is \\[\np(\\mu \\mid y) \\propto p(y \\mid \\mu) p(\\mu) = N(y\\mid \\mu,2) N(\\mu\\mid -1,1) = N(-0.4,0.9).\n\\]\n\nmu0 = -1\nsigma0 = 1\nybar = 2\nsigma = 2\nmu1 = (mu0/sigma0^2 + ybar/sigma^2)/(1/sigma0^2 + 1/sigma^2)\nsigma1 = sqrt(1/(1/sigma0^2 + 1/sigma^2))\nsprintf(\"Posterior mean: %f, Posterior variance: %f\", mu1, sigma1)\n\n## [1] \"Posterior mean: -0.400000, Posterior variance: 0.894427\"\n\n\nGraphically we can represent this as follows\n\n# The prior distribution \nmu = seq(-4,10,0.01)\ny = seq(-4,10,0.01)\n# Prior\nplot(mu,dnorm(mu,mu0,sigma0),type=\"l\",xlab=\"x\",ylab=\"p(x)\",lwd=3,col=\"blue\",ylim=c(0,0.5))\n# The likelihood function\nlines(y,dnorm(y,ybar,sigma),type=\"l\",lwd=3,col=\"red\")\n# The posterior distribution\nlines(y,dnorm(y,mu1,sigma1),type=\"l\",lwd=3,col=\"green\")\n# legend\nlegend(\"topright\", c(\"Prior\",\" Data (Likelihood)\",\"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\nFigure 3.2: Norm-Norm Updating\n\n\n\n\n\nNote, the posterior mean is in between those of prior and likelihood and posterior variance is lower than variance of both prior and likelihood, this is effect of combining information from data and prior!\n\nMore generally, when we observe \\(n\\) independent and identically distributed (i.i.d.) data points \\(y_1,\\ldots,y_n\\) from a normal distribution with known variance \\(\\sigma^2\\), the likelihood function is given by \\[\np(y \\mid \\mu) = N(\\bar y\\mid \\mu,\\sigma^2/n),~ \\text{where}~ \\bar y = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] Note, that average over the observed data \\(\\bar y = \\mathrm{Ave}(y_1,\\ldots,y_n)\\) is the sufficient statistics for the mean \\(\\mu\\). The prior distribution is given by \\[\np(\\mu) = N(\\mu\\mid \\mu_0,\\sigma_0^2)\n\\] The posterior distribution is given by \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0-\\mu_0^2}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}-\\bar{y}^2}{2\\sigma^2/n}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}}{2\\sigma^2/n}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we combine the terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{(-\\mu^2+2\\mu\\mu_0)\\sigma^2 +(-\\mu^2+2\\mu\\bar{y})n\\sigma_0^2}{2\\sigma_0^2\\sigma^2}}\\Bigg]. \\\\\n\\end{split}\n\\] Now re-arrange and combine \\(\\mu^2\\) and \\(\\mu\\) terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2(n\\sigma_0^2+\\sigma^2)+2\\mu(\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2) }{2\\sigma_0^2\\sigma^2}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\left(\\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\right) }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we add constants which do not depend upon \\(\\mu\\) to complete the square in the numerator: \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\bigg(\\mu - \\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\bigg)^2 }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Finally we get the posterior mean \\[\n\\mu_n = \\frac{\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2} = \\mu_0\\frac{\\sigma^2}{n\\sigma_0^2+\\sigma^2} + \\bar{y}\\frac{n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\n\\] and the posterior variance \\[\n\\sigma_n^2 = \\frac{\\sigma_0^2\\sigma^2}{n\\sigma_0^2+\\sigma^2}.\n\\]\n\nExample 3.7 (Chicago Bears 2014-2015 Season) The Chicago Bears are a professional American football team based in Chicago, Illinois. The Bears were a young team in 2014-2015, an were last in the their division. This season the Chicago Bears suffered back-to-back \\(50\\)-points defeats and lost to Patriots and Packers.\n\nPatriots-Bears \\(51-23\\)\nPackers-Bears \\(55-14\\)\n\nTheir next game was at home against the Minnesota Vikings. Current line against the Vikings was \\(-3.5\\) points. Slightly over a field goal. What’s the Bayes approach to learning the line? We use hierarchical data and Bayes learning to update our beliefs in light of new information. The current average win/lose this year can be modeled as a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We assume that \\(\\mu\\) is normally distributed with mean \\(\\mu_0\\) and standard deviation \\(\\tau\\). \\[\\begin{align*}\n\\bar{y} \\mid \\mu & \\sim N \\left ( \\mu , \\frac{\\sigma^2}{n} \\right ) \\sim N \\left ( \\mu , \\frac{18.34^2}{9} \\right )\\\\\n\\mu & \\sim N( 0 , \\tau^2 )\n\\end{align*}\\] Here \\(n =9\\) games so far. With \\(s = 18.34\\) points. We assume the pre-season prior mean \\(\\mu_0 = 0\\), standard deviation \\(\\tau = 4\\). Base on the observed data so-far: \\(\\bar{y} = -9.22\\).\nThe Bayes Shrinkage estimator is then \\[\n\\mathbb{E} \\left( \\mu \\mid \\tau, \\bar y  \\right) = \\frac{ \\tau^2 }{ \\tau^2 + \\frac{\\sigma^2}{n} }\\bar{y} .\n\\]\nThe shrinkage factor is \\(0.3\\)! That’s quite a bit of shrinkage. Why? Our updated estimator is \\[\n\\mathbb{E} \\left ( \\mu | \\bar{y} , \\tau \\right ) = - 2.75 &gt; -.3.5\n\\] where current line is \\(-3.5\\).\n\nBased on our hierarchical model this is an over-reaction. One point change on the line is about \\(3\\)% on a probability scale.\nAlternatively, calculate a market-based \\(\\tau\\) given line \\(=-3.5\\). \\[\n\\tau^2 = \\frac{\\sigma^2}{n} \\frac{1}{0.3^2} = 18.34^2 \\frac{1}{0.3^2} = 180.\n\\]\nThe market-based \\(\\tau\\) is \\(13.4\\) points.\n\n\nbears=c(-3,8,8,-21,-7,14,-13,-28,-41)\nprint(mean(bears))\n\n## [1] -9.2\n\nprint(sd(bears))\n\n## [1] 18\n\ntau=4\nsig2=sd(bears)*sd(bears)/9\nprint(tau^2/(sig2+tau^2))\n\n## [1] 0.3\n\nprint(0.29997*-9.22)\n\n## [1] -2.8\n\nprint(pnorm(-2.76/18))\n\n## [1] 0.44\n\n\nHome advantage is worth \\(3\\) points. The actual result of the game is Bears 21, Vikings 13.\n\nPosterior Predictive\nThe posterior predictive distribution is the distribution of a new observation \\(y_{n+1}\\) given the observed data \\(y_1,\\ldots,y_n\\). The posterior predictive distribution is given by\n\\[\\begin{align*}\np(y_{n+1} \\mid & y_1,\\ldots,y_n) = \\int p(y_{n+1} \\mid \\mu) p(\\mu \\mid y_1,\\ldots,y_n) d\\mu \\\\\n&  = \\int N(y_{n+1} \\mid \\mu, \\sigma^2) N(\\mu \\mid \\mu_n, \\sigma_n^2) d\\mu = N(y_{n+1} \\mid \\mu_n, \\sigma_n^2 + \\sigma^2).\n\\end{align*}\\] This follows from the general properties of the Gaussian distribution",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-variance",
    "href": "03-bl.html#normal-with-unknown-variance",
    "title": "3  Bayesian Learning",
    "section": "3.8 Normal With Unknown Variance",
    "text": "3.8 Normal With Unknown Variance\nConsider, another example, when mean \\(\\mu\\) is fixed and variance is a random variable which follows some distribution \\(\\sigma^2 \\sim p(\\sigma^2)\\). Given an observed sample \\(y\\), we can update the distribution over variance using the Bayes rule \\[\np(\\sigma^2 \\mid  y) = \\dfrac{p(y\\mid \\sigma^2 )p(\\sigma^2)}{p(y)}.\n\\] Now, the total probability in the denominator can be calculated as \\[\np(y) = \\int p(y\\mid \\sigma^2 )p(\\sigma^2) d\\sigma^2.\n\\]\nA conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. Thus, if \\[\n\\sigma^2 \\mid  \\alpha,\\beta \\sim IG(\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\sigma^{2(-\\alpha-1)}\\exp\\left(-\\dfrac{\\beta}{\\sigma^2}\\right)\n\\] and \\[\ny \\mid \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\n\\] Then the posterior distribution is another inverse Gamma \\(IG(\\alpha_{\\mathrm{posterior}},\\beta_{\\mathrm{posterior}})\\), with \\[\n\\alpha_{\\mathrm{posterior}} = \\alpha + 1/2, ~~\\beta_{\\mathrm{posterior}} = \\beta + \\dfrac{y-\\mu}{2}.\n\\]\nNow, the predictive distribution over \\(y\\) can be calculated by \\[\np(y_{new}\\mid y) = \\int p(y_{new},\\sigma^2\\mid y)p(\\sigma^2\\mid y)d\\sigma^2.\n\\] Which happens to be a \\(t\\)-distribution with \\(2\\alpha_{\\mathrm{posterior}}\\) degrees of freedom, mean \\(\\mu\\) and variance \\(\\alpha_{\\mathrm{posterior}}/\\beta_{\\mathrm{posterior}}\\).\n\nThe Normal-Gamma Model\nNow, consider the case when both mean and variance are unknow. To simplify the formulas, we will use precision \\(\\tau^{-2} = \\sigma^2\\) instead of variance \\(\\sigma^2\\). The normal-Gamma distribution is a conjugate prior for the normal distribution, when we do not know the precision and the mean. Given the observed data \\(Y  = \\{y_1,\\ldots,y_n\\}\\), we assume normal likelihood \\[\ny_i \\mid \\theta, \\tau^{-2} \\sim N(\\theta, 1/\\tau^{-2})\n\\]\nThe normal-gamma prior distribution is defined as \\[\n\\theta\\mid \\mu,\\tau^{-2},\\nu \\sim N(\\mu, 1/(\\tau^{-2} \\nu)), \\quad \\tau^{-2} \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta).\n\\] Thus, \\(1/\\tau^{-2}\\) has inverse-Gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\). Conditional on \\(\\tau^{-2}\\), the mean \\(\\theta\\) has normal distribution with mean \\(\\mu\\) and precision \\(\\nu\\tau^{-2}\\). Notice that in this model the mean \\(\\theta\\) and precision \\(\\tau^{-2}\\) are not independent. When the precision of observations \\(\\tau^{-2}\\) is low, we are also less certain about the mean. However, when \\(\\nu=0\\), we have an improper uniform distribution over \\(\\theta\\), that is independent of \\(\\tau^{-2}\\). There is no conjugate distribution for \\(\\theta,\\tau^{-2}\\) in which \\(\\theta\\) is independent of \\(\\tau^{-2}\\). Given the normal likelihood \\[\np(y\\mid \\theta, \\tau^{-2}) = \\left(\\dfrac{\\tau^{-2}}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\tau^{-2}}{2}\\sum_{i=1}^n(y_i-\\theta)^2\\right)\n\\] and the normal-gamma prior \\[\np(\\theta, \\tau^{-2} \\mid \\mu,\\nu,\\alpha,\\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\nu\\tau^{-2\\alpha-1}\\exp(-\\beta\\tau^{-2})\\left(\\dfrac{\\nu\\tau^{-2}}{2\\pi}\\right)^{1/2}\\exp\\left(-\\dfrac{\\nu\\tau^{-2}}{2}(\\theta-\\mu)^2\\right)\n\\] the posterior distribution is given by \\[\np(\\theta, \\tau^{-2}\\mid y) \\propto p(y\\mid \\theta, \\tau^{-2})p(\\theta, \\tau^{-2}).\n\\] The posterior distribution is a normal-Gamma distribution with parameters \\[\n\\begin{aligned}\n\\mu_n &= \\dfrac{\\nu\\mu + n\\bar{y}}{\\nu+n},\\\\\n\\nu_n &= \\nu+n,\\\\\n\\alpha_n &= \\alpha + \\dfrac{n}{2},\\\\\n\\beta_n &= \\beta + \\dfrac{1}{2}\\sum_{i=1}^n(y_i-\\bar{y})^2 + \\dfrac{n\\nu}{2(\\nu+n)}(\\bar{y}-\\mu)^2.\n\\end{aligned}\n\\] where \\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\) is the sample mean and \\(n\\) is the sample size. The posterior distribution is a normal-Gamma distribution with parameters \\(\\mu_n, \\nu_n, \\alpha_n, \\beta_n\\).\n\n\nCredible Intervals for Normal-Gamma Model Posterior Parameters\nThe precission posterior follows a Gamma distribution with parameters \\(\\alpha_n, \\beta_n\\), thus we can use quantiles of the Gamma distribution to calculate credible intervals. A symmetric \\(100(1-c)%\\) credible interval \\([g_{c/2},g_{1-c/2}]\\) is given by \\(c/2\\) and \\(1-c/2\\) quantiles of the gamma distrinution. To find credible intterval for the variance \\(v = 1/\\tau^{-2}\\), we simply use \\[\n[1/g_{1-c/2},1/g_{c/2}].\n\\] and for standard deviation \\(s = \\sqrt{v}\\) we use \\[\n[\\sqrt{1/g_{1-c/2}},\\sqrt{1/g_{c/2}}].\n\\] To find credible interval over the mean \\(\\theta\\), we need to integrate out the precision \\(\\tau^{-2}\\) from the posterior distribution. The marginal distribution of \\(\\theta\\) is a Student’s t-distribution with parameters center at \\(\\mu_n\\), variance \\(\\beta_n/(\\nu_n\\alpha_n)\\) and degrees of freedom \\(2\\alpha_n\\).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#multivariate-normal",
    "href": "03-bl.html#multivariate-normal",
    "title": "3  Bayesian Learning",
    "section": "3.9 Multivariate Normal",
    "text": "3.9 Multivariate Normal\nIn the multivariate case, the normal-normal model is \\[\n\\theta \\sim N(\\mu_0,\\Sigma_0), \\quad y \\mid \\theta \\sim N(\\theta,\\Sigma).\n\\] The posterior distribution is \\[\n\\theta \\mid y \\sim N(\\mu_1,\\Sigma_1),\n\\] where \\[\n\\Sigma_1 = (\\Sigma_0^{-1} + \\Sigma^{-1})^{-1}, \\quad \\mu_1 = \\Sigma_1(\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y).\n\\] The predictive distribution is \\[\ny_{new} \\mid y \\sim N(\\mu_1,\\Sigma_1 + \\Sigma).\n\\]\n\nExample 3.8 (Satya Nadella: CEO of Microsoft) In 2014, Satya Nadella became the CEO of Microsoft. The stock price of Microsoft has been on a steady rise since then. Suppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.\nSuppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&P500 index and tracks overall market performance. We are interested in estimating the mean returns of the positions in our portfolio. You believe that the returns are normally distributed and are related to each other. You have prior beliefs about these returns, which are also normally distributed. We will use what is called the empirical prior for the mean returns. This is a prior that is based on historical data. The empirical prior is a good choice when you have a lot of historical data and you believe that the future mean returns will be similar to the historical mean returns. We assume the prior for the mean returns is a bivariate normal distribution, let \\(\\mu_0 = (\\mu_{M}, \\mu_{S})\\) represent the prior mean returns for the stocks. The covariance matrix \\(\\Sigma_0\\) captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. We will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix. The prior covariance matrix is given by \\[\n\\Sigma_0 = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the historical returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the historical returns of MSFT and SPY. The prior mean is given by \\[\n\\mu_0 = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the historical returns of MSFT and SPY, respectively. The likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns \\(\\mu = [\\mu_A, \\mu_B]\\). The covariance matrix \\(\\Sigma\\) of the likelihood represents the uncertainty in your data. We will use the sample mean and covariance matrix of the observed returns as the likelihood mean and covariance matrix. The likelihood covariance matrix is given by \\[\n\\Sigma = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the observed returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by \\[\n\\mu = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the observed returns of MSFT and SPY, respectively. In a Bayesian framework, you update your beliefs (prior) about the mean returns using the observed data (likelihood). The posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution. The mean \\(\\mu_{\\text{post}}\\) and covariance \\(\\Sigma_{\\text{post}}\\) of the posterior are calculated using Bayesian updating formulas, which involve \\(\\mu_0\\), \\(\\Sigma_0\\), \\(\\mu\\), and \\(\\Sigma\\).\nWe use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014. Thus, our observed data includes July 2015 - Dec 2023 period. We assume the likelihood of observing this data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns. The covariance matrix \\(Sigma\\) of the likelihood represents the uncertainty in your data and is calculated from the overall observed returns data 2001-2023.\n\ngetSymbols(c(\"MSFT\", \"SPY\"), from = \"2001-01-01\", to = \"2023-12-31\")\n\n## [1] \"MSFT\" \"SPY\"\n\ns = 3666 # 2015-07-30\nprior = 1:s\nobs = s:nrow(MSFT) # post covid\n# obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23\na = as.numeric(dailyReturn(MSFT))\nc = as.numeric(dailyReturn(SPY))\n# Prior\nmu0 = c(mean(a[prior]), mean(c[prior]))\nSigma0 = cov(data.frame(a=a[prior],c=c[prior]))\n# Data\nmu = c(mean(a[obs]), mean(c[obs]))\nSigma = cov(data.frame(a=a,c=c))\n# Posterior\nSigmaPost = solve(solve(Sigma0) + solve(Sigma))\nmuPost = SigmaPost %*% (solve(Sigma0) %*% mu0 + solve(Sigma) %*% mu)\n# Plot\nplot(a[obs], c[obs], xlab=\"MSFT\", ylab=\"SPY\", xlim=c(-0.005,0.005), \n    ylim=c(-0.005,0.005), pch=16, cex=0.5)\nabline(v=0, h=0, col=\"grey\")\nabline(v=mu0[1], h=mu0[2], col=\"blue\",lwd=3) #prior\nabline(v=mu[1], h=mu[2], col=\"red\",lwd=3) #data\nabline(v=muPost[1], h=muPost[2], col=\"green\",lwd=3) #posterior\nlegend(\"bottomright\", c(\"Prior\", \"Likelihood\", \"Posterior\"), pch=15, \n    col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\nFigure 3.3: Bayesian Portfolio Updating\n\n\n\n\n\nWe can see the posterior mean for SPY is close to the prior mean, while the posterior mean for MSFT is further away. The performance of MSFT was significantly better past 2015 compared to SPY. The posterior mean (green) represents mean reversion value. We can think of it a expected mean return if the performance of MSFT starts reverting to its historical averages.\nThis model is particularly powerful because it can be extended to more dimensions (more stocks) and can include more complex relationships between the variables. It’s often used in finance, econometrics, and other fields where understanding the joint behavior of multiple normally-distributed variables is important.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#mixtures-of-conjugate-priors",
    "href": "03-bl.html#mixtures-of-conjugate-priors",
    "title": "3  Bayesian Learning",
    "section": "3.10 Mixtures of Conjugate Priors",
    "text": "3.10 Mixtures of Conjugate Priors\nThe mixture of conjugate priors is a powerful tool for modeling complex data. It allows us to combine multiple conjugate priors to create a more flexible model that can capture a wider range of data patterns. The mixture of conjugate priors is particularly useful when the data is generated from a mixture of distributions, where each component of the mixture is generated from a different distribution. \nIf \\(p_1(x),\\ldots,p_k(x)\\) are proper density functions and \\(\\pi_1,\\ldots,\\pi_k\\) are non-negative weights that sum to 1, then the mixture distribution is given by \\[\np(x) = \\sum_{i=1}^k \\pi_i p_i(x).\n\\] It is easy to show that \\(p(x)\\) is a proper density. Indeed, given domain \\(x\\in A\\subset \\mathbb{R}\\) we have \\[\n\\int_A p(x)dx = \\sum_{i=1}^k \\pi_i \\int_A p_i(x)dx  = \\sum_{i=1}^k \\pi_i = 1.\n\\]\nAssume our prior is a mixture of distributions, that is \\[\n\\theta \\sim p(\\theta) = \\sum_{k=1}^K \\pi_k p_k(\\theta).\n\\] Then the posterior is also a mixture of normal distributions, that is \\[\np(\\theta\\mid y) = p(y\\mid \\theta)\\sum_{k=1}^K \\pi_k p_k(\\theta)/C.\n\\] We introduce a normalizing constant for each component \\[\nC_k = \\int p(y\\mid \\theta)p_k(\\theta)d\\theta.\n\\] then \\[\np_k(\\theta\\mid y)  = p_k(\\theta)p(y\\mid \\theta)/C_k\n\\] is a proper distribution and our posterior is a mixture of these distributions \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\pi_k C_k p_k(\\theta\\mid y)/C.\n\\] Meaning that we need to require \\[\n\\dfrac{\\sum_{k=1}^K \\pi_k C_k}{C} = 1, \\quad \\text{or} \\quad C = \\sum_{k=1}^K \\pi_k C_k.\n\\] Then the posterior density is a mixture \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\hat \\pi_k p_k(\\theta \\mid y).\n\\]\nConsider an example of a mixture of two normal distributions. The prior distribution is a mixture of two normal distributions, that is \\[\n\\mu \\sim 0.5 N(0,1) + 0.5 N(5,1).\n\\] The likelihood is a normal distribution with mean \\(\\mu\\) and variance 1, that is \\[\ny \\mid \\mu \\sim N(\\mu,1).\n\\] The posterior distribution is a mixture of two normal distributions, that is \\[\np(\\mu \\mid y) \\propto \\phi(y\\mid \\mu,1) \\left(0.5 \\phi(\\mu\\mid 0,1) + 0.5 \\phi(\\mu\\mid 5,1)\\right),\n\\] where \\(\\phi(x\\mid \\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We can calculate it using property of a normal distribution \\[\n\\phi(x\\mid \\mu_1,\\sigma_1^2)\\phi(x\\mid \\mu_2,\\sigma_2^2) = \\phi(x\\mid \\mu_3,\\sigma_3^2)\\phi(\\mu_1-\\mu_2\\mid 0,\\sigma_1^2+\\sigma_2^2)\n\\] where \\[\n\\mu_3 = \\dfrac{\\mu_1/\\sigma_2^2 + \\mu_2/\\sigma_1^2}{1/\\sigma_1^2 + 1/\\sigma_2^2}, \\quad \\sigma_3^2 = \\dfrac{1}{1/\\sigma_1^2 + 1/\\sigma_2^2}.\n\\]\nGiven, we observed \\(y = 2\\), we can calculate the posterior distribution for \\(\\mu\\)\n\nmu0 = c(0,5)\nsigma02 = c(1,1)\npi = c(0.5,0.5)\ny = 2\nmu3 = (mu0/sigma02 + y) / (1/sigma02 + 1)\nsigma3 = 1/(1/sigma02 + 1)\nC = dnorm(y-mu0,0,1+sigma02)*pi\nw = C/sum(C)\n# To add a new line in sprintf, use \"\\n\" inside the format string.\nprint(\"Component parameters:\")\n\n## [1] \"Component parameters:\"\n\nsprintf(\"Mean = (%1.1f,%2.1f)  Var = (%1.1f,%1.1f)  weights = (%1.2f,%1.2f)\", \n        mu3[1], mu3[2], sigma3[1], sigma3[2], w[1], w[2])\n\n## [1] \"Mean = (1.0,3.5)  Var = (0.5,0.5)  weights = (0.65,0.35)\"",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exponential-gamma-model",
    "href": "03-bl.html#exponential-gamma-model",
    "title": "3  Bayesian Learning",
    "section": "3.11 Exponential-Gamma Model",
    "text": "3.11 Exponential-Gamma Model\nExponential distribution is a continuous distribution that is often used to model waiting times between events. For example, the time between two consecutive arrivals of a Poisson process is exponentially distributed. If the number of events in 1 unit of time has the Poisson distribution with rate parameter \\(\\lambda\\), then the time between events has the exponential distribution with mean \\(1/\\lambda\\). The probability density function (PDF) of an exponential distribution is defined as: \\[\nf(x;\\lambda) =  \\lambda e^{-\\lambda x}, ~ x \\geq 0\n\\] The exponential distribution is defined for \\(x \\geq 0\\), and \\(\\lambda\\) is the rate parameter, which is the inverse of the mean (or expected value) of the distribution. It must be greater than 0. The exponential distribution is a special case of the Gamma distribution with shape 1 and scale \\(1/\\lambda\\).\nThe mean and variance are give in terms of the rate parameter \\(\\lambda\\) as\n\n\n\nExponential Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = 1/\\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = 1/\\lambda^2\\)\n\n\n\nHere are some examples of when exponential model provides a good fit\n\nLifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.\nTime Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.\nRadioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution.\n\nIn these examples, the key assumption is that events happen independently and at a constant average rate, which makes the exponential distribution a suitable model.\nThe Exponential-Gamma model, often used in Bayesian statistics, is a model where the exponential distribution’s parameter is itself modeled as following a Gamma distribution. This approach is particularly useful in situations where there is uncertainty or variability in the rate parameter of the exponential distribution.\nThe Exponential-Gamma model assumes that the data follows an exponential distribution (likelihood). As mentioned earlier, the exponential distribution is suitable for modeling the time between events in processes where these events occur independently and at a constant rate. At the next level, the rate parameter \\(\\lambda\\) of the exponential distribution is assumed to follow a Gamma distribution. The Gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. \\[\\begin{align*}\n    \\lambda &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n    y_t\\mid \\lambda &\\sim \\text{Exponential}(\\lambda)\n\\end{align*}\\]\nThe probability density function of the Gamma distribution is given by Equation 3.2 and has two parameters, shape \\(\\alpha\\) and rate \\(\\beta\\). The posterior distribution of the rate parameter \\(\\lambda\\) is given by: \\[\np(\\lambda\\mid y_1, \\ldots, y_n) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\prod_{t=1}^n \\lambda e^{-\\lambda y_t} = \\lambda^{\\alpha + n - 1} e^{-(\\beta + \\sum_{t=1}^n y_t)\\lambda}\n\\] which is a Gamma distribution with shape parameter \\(\\alpha + n\\) and rate parameter \\(\\beta + \\sum_{t=1}^n y_t\\). The posterior mean and variance are given by: \\[\n\\mathbb{E}[\\lambda|y_1, \\ldots, y_n] = \\frac{\\alpha + n}{\\beta + \\sum_{t=1}^n y_t}, \\quad \\mathrm{Var}[\\lambda|y_1, \\ldots, y_n] = \\frac{\\alpha + n}{(\\beta + \\sum_{t=1}^n y_t)^2}.\n\\] Notice, that \\(\\sum y_t\\) is the sufficient statistic for inference about parameter \\(\\lambda\\)!\nThe Exponential-Gamma model finds practical application in a variety of fields where the underlying rate of events is not constant but subject to uncertainty or heterogeneity.\nIn reliability engineering, for example, the failure rate of components or systems may fluctuate due to manufacturing differences, operational environments, or aging effects. The Exponential-Gamma model allows engineers to estimate the time until failure while explicitly incorporating this uncertainty in the failure rate, leading to more robust predictions and maintenance schedules.\nIn medical research, the model is often used to analyze survival times of patients. Here, the rate at which mortality or disease progression occurs can vary significantly across individuals, influenced by factors such as age, genetic predispositions, or environmental exposures. By modeling the rate parameter as a random variable, the Exponential-Gamma approach captures this variability, providing a more realistic assessment of survival probabilities within a heterogeneous population.\nIn ecology, when studying the time between rare environmental events—such as extreme weather occurrences—the frequency of these events may change due to shifting climate conditions or other ecological factors. The Exponential-Gamma model accommodates this variability, enabling ecologists to better quantify the uncertainty in the timing of such rare events.\nAcross these domains, the Exponential-Gamma model offers a more flexible and nuanced alternative to the simple exponential model, as it explicitly accounts for the variability in the rate parameter rather than assuming it is fixed and known.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#summary-of-conjugate-priors-for-common-likelihoods",
    "href": "03-bl.html#summary-of-conjugate-priors-for-common-likelihoods",
    "title": "3  Bayesian Learning",
    "section": "3.12 Summary of Conjugate Priors for Common Likelihoods",
    "text": "3.12 Summary of Conjugate Priors for Common Likelihoods\nSummary table of random variables\n\n\n\nTable 3.1: Summary table of commonly used random variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\\(\\theta\\)\nPDF\nMean\nVariance\nSupport\n\n\n\n\nNormal\n\\(\\mu, \\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\\(x \\geq 0\\)\n\n\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\)\n\\(\\frac{\\alpha}{\\beta}\\)\n\\(\\frac{\\alpha}{\\beta^2}\\)\n\\(x \\geq 0\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(x \\in \\mathbb{N}\\)\n\n\nBinomial\n\\(n, p\\)\n\\(\\binom{n}{x}p^x(1-p)^{n-x}\\)\n\\(np\\)\n\\(np(1-p)\\)\n\\(x \\in \\{0, 1, \\ldots, n\\}\\)\n\n\nBernoulli\n\\(p\\)\n\\(p^x(1-p)^{1-x}\\)\n\\(p\\)\n\\(p(1-p)\\)\n\\(x \\in \\{0, 1\\}\\)\n\n\nMultinomial\n\\(n, \\boldsymbol{p}\\)\n\\(\\frac{n!}{x_1!x_2!\\cdots x_k!}p_1^{x_1}p_2^{x_2}\\cdots p_k^{x_k}\\)\n\\(np_i\\)\n\\(np_i(1-p_i)\\)\n\\(\\sum x_i = n, x_i \\in \\mathbb{R}^+\\)\n\n\nBeta\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)\n\\(\\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\\(x \\in [0, 1]\\)\n\n\nInverse Gamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{-\\alpha-1}e^{-\\frac{\\beta}{x}}\\)\n\\(\\frac{\\beta}{\\alpha-1}\\)\n\\(\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\)\n\\(x &gt; 0\\)\n\n\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\)\n\\(\\frac{\\alpha}{\\beta}\\)\n\\(\\frac{\\alpha}{\\beta^2}\\)\n\\(x \\geq 0\\)\n\n\n\n\n\n\nTable 3.2 summarizes the conjugate prior distributions for common likelihoods. Thus far, we’ve considered the Normal-Normal model with both known and unknown variance as well as Poisson-Gamma and Beta Binomial. The other pairs are left as an exercise. Given observed data \\(x = (x_1,\\ldots,x_n)\\) and \\(s = \\sum_{i=1}^nx_i\\), \\(\\bar x = s/n\\).\n\n\n\nTable 3.2: Conjugate prior table for common likelihoods\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nPrior\nPrior Parameters\nModel Parameters\nPosterior Parameters\n\n\n\n\nNormal (known \\(\\sigma^2\\))\nNormal\n\\(\\mu_0, \\sigma^2_0\\)\n\\(\\mu\\)\n\\(\\frac{n\\sigma^2_0 \\bar x + \\sigma^2 \\mu_0}{\\sigma^2 + n\\sigma^2_0},~\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\)\n\n\nNormal (known \\(\\mu\\))\nInverse Gamma\n\\(\\alpha,\\beta\\)\n\\(\\sigma^2\\)\n\\(\\alpha+n/2, \\beta + \\frac{1}{2}\\sum(x_i-\\mu)^2\\)\n\n\nBinomial (\\(m\\) trials)\nBeta\n\\(\\alpha, \\beta\\)\n\\(p\\)\n\\(\\alpha + s, \\beta + nm - s\\)\n\n\nPoisson\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(\\alpha + s, \\beta + n\\)\n\n\nExponential\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(\\alpha + n, \\beta + s\\)\n\n\nMultinomial\nDirichlet\n\\(\\alpha \\in \\mathbb{R}^k\\)\n\\(p \\in \\mathbb{R}^k\\)\n\\(\\alpha+s\\)\n\n\nNormal\nNormal-inverse gamma\n\\(\\mu_0, \\nu, \\alpha, \\beta\\)\n\\(\\mu, \\sigma\\)\n\\(\\frac{\\nu\\mu_0+n\\bar{x}}{\\nu+n} ,\\, \\nu+n,\\, \\alpha+\\frac{n}{2} ,\\,\\)  \\(\\beta + \\tfrac{1}{2} \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\frac{n\\nu}{\\nu+n}\\frac{(\\bar{x}-\\mu_0)^2}{2}\\)\n\n\n\n\n\n\nThese conjugate relationships simplify Bayesian calculations by ensuring that the posterior distributions are in the same family as the priors.\n\n\n\n\nKolmogorov, AN. 1942. “Definition of Center of Dispersion and Measure of Accuracy from a Finite Number of Observations (in Russian).” Izv. Akad. Nauk SSSR Ser. Mat. 6: 3–32.\n\n\nShen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad Patell, and Dhruv S Kazi. 2021. “Underperformance of Contemporary Phase III Oncology Trials and Strategies for Improvement.” Journal of the National Comprehensive Cancer Network 19 (9): 1072–78.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to Go!” Significance 6 (4): 151–53.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference about Batter-Pitcher Matchups in Baseball from Small Samples.” Statistical Thinking in Sports, 153–65.\n\n\nSun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. “Why 90% of Clinical Drug Development Fails and How to Improve It?” Acta Pharmaceutica Sinica B 12 (7): 3049–62.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The Impact of the Highly Improbable. Annotated edition. New York. N.Y: Random House.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "04-dec.html",
    "href": "04-dec.html",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "4.1 Expected Utility\nLife is about making decisions under uncertainty. We always prefer informed decisions. Statistical decision theory studies the process of finding a reasonable course of action when faced with statistical uncertainty–uncertainty that can in part be quantified from observed data. In most cases, the problem can be separated into two problems: a learning problem or parameter estimation problem and then a decision problem that uses the output of the learning problem. In finance, a classic example of this is finding optimal portfolios with a mean-variance utility/criterion function assuming the underlying means, variances and covariances are unknown based on a historical sample of data. In statistics, the classic problem is using decision theory to evaluate the relative merits of different parameter estimators and hypothesis tests.\nLet \\(P,Q\\) be two possible risky gambles or probability bets. An agent’s preferences can then be specified as an ordering on probability bets where we write \\(P\\) is preferred to \\(Q\\) as \\(P \\succeq Q\\) and indifference as \\(P \\sim Q\\). A compound or mixture bet is defined by the probability assignment \\(p P + (1 - p ) Q\\) for a prospect weight \\(0 \\leq p \\leq 1\\).\nRamsey-de Finetti-Savage show that if an agent’s preferences satisfy a number of plausible axioms – completeness, transitivity, continuity and independence – then they can be represented by the expectation of a utility function. The theory is a normative one and not necessarily descriptive. It suggests how a rational agent should formulate beliefs and preferences and not how they actually behave.\nThis representation of preferences in terms of expected utility \\(U(P)\\) of a risky gamble is then equivalent to \\[\nP \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q )\n\\] Therefore, the higher the value taken by the utility function the more the gamble is preferred. Specifically, the axioms lead to existence of expected utility and uniqueness of probability.\nThe two key facts then are uniqueness of probability and existence of expected utility. Formally,\nThis implies that \\(U\\) is additive and it is also unique up to affine transformation.\nProof: If \\(w\\) is not unique then \\(\\exists w_1\\) such that \\(w_1 P + (1 - w_1 ) Q \\sim R\\). Without loss of generality assume that \\(w_1 &lt; w\\) and so \\(0 &lt; w - w_1 &lt; 1 - w_1\\). However, we can write the bet \\(Q\\) as \\[\nQ = \\left ( \\frac{w-w_1}{1-w_1} \\right ) Q + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\] By transitivity, as \\(P \\succeq Q\\) we have \\[\n\\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q \\succeq Q\n\\] However, \\[\nw P + ( 1 - w) Q = w_1 P + (1 - w_1 ) \\left (  \\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\right )\n\\] implying by transitivity that \\[\nw P + (1 - w ) Q \\succeq w_1 P + (1 - w_1 ) Q\n\\] which is a contradiction.\nThis can be used together with the axioms to then prove the existence and uniqueness of a utility function.\nOther interesting extensions: how do people come to a consensus (DeGroot, 1974, Morris, 1994, 1996). Ramsey (1926) observation that if someone is willing to offer you a bet then that’s conditioning information for you. All probabilities are conditional probabilities.\nIf the bet outcome \\(P\\) is a monetary value, then the utility functions \\(P, P^2, \\sqrt{P}, \\ln P\\) are all monotonically increasing (the more the better). However, the utility function \\(P^2\\) is convex and the utility function \\(\\ln P\\) is concave. The concavity of the utility function implies that the agent is risk averse and the convexity implies that the agent is risk seeking.\nNow, consider a more general situation, when you have two gambles 1: get \\(P_1\\) for sure, 2: get \\(P_2 = P_1+k\\) and \\(P_3 = P_1-k\\) with probability 1/2. Then we will compare the utility of those gambles \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) \\text{ and } U(P_1).\n\\] If the utility function is linear then we should be indifferent between the two gambles. However, if the utility function is concave then we should prefer the sure thing. This is known as the certainty effect. \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) &lt; U(P_1).\n\\]\nThe usual situation can be described as follows. Let \\(\\Omega\\) be a finite set of possible outcomes with \\(\\Omega = \\{ \\omega_1 , \\ldots , \\omega_n \\}\\). Let \\(P_i\\) be the consequence that assigns one to outcome \\(\\omega_i\\) and zero otherwise and let \\(P = ( p_1 , \\ldots , p_n )\\) assign probability \\(p_i\\) to outcome \\(\\omega_i\\). Then we can write the expected utility, \\(U(P)\\), of the gamble \\(P\\) as \\[\nU(P) = \\sum_{i=1}^n p_i U( P_i ).\n\\] That is, the utility of \\(P\\) is the expected value of a random variable \\(W\\) (wealth) that takes the value \\(U(P_i)\\) if the outcome is \\(\\omega_i\\). Therefore, we can write \\(U(P) = \\mathbb{E}_P \\left ( U( W ) \\right)\\).\nThis leads us to the notion of risk aversion and a categorization of agents according to their risk tolerance: the agent is said to be\nHere we assume that these hold for all probabilities and random variables. Risk aversion is equivalent to the agent having concave utility and risk seeking convex.\nPower utility and log-utilities allow us to model constant relative risk aversion (CRRA). The main advantage is that the optimal rule is unaffected by wealth effects. The CRRA utility of wealth takes the form \\[\nU_\\gamma (W) = \\frac{ W^{1-\\gamma} -1 }{1-\\gamma}\n\\]\nThe special case \\(U(W) = \\log (W )\\) for \\(\\gamma = 1\\). This leads to a myopic Kelly criterion rule.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#expected-utility",
    "href": "04-dec.html#expected-utility",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "If \\(P \\succeq R \\succeq Q\\) and \\(w P + (1 - w ) Q \\sim R\\) then \\(w\\) is unique.\nThere exists an expected utility \\(U(\\cdot )\\) such that \\(P \\succeq Q ~ \\iff ~ U (P) \\geq U (Q)\\). Furthermore \\[\nU \\left (w P + (1 - w ) Q \\right ) = wU (P) +(1 - w ) U(Q)\n\\] for any \\(P, Q\\) and \\(0 \\leq w \\leq 1\\).\n\n\n\n\n\nTheorem 4.1 If \\(V\\) is any other function satisfying these results then \\(V\\) is an affine function of \\(U\\).\n\n\nProof. If \\(\\forall P , Q\\) we have \\(P \\sim Q\\), then define \\(u(P) \\equiv 0\\). Hence suppose that there exists \\(S \\succ T\\). Define \\(U(S) =1\\) and \\(U(T)=0\\). For any \\(P \\in \\mathcal{P}\\) there are five possibilities: \\(P \\succ T\\) or \\(P \\sim S\\) or \\(S \\succ P \\succ T\\) or \\(P \\sim T\\) or \\(T \\succ P\\).\nIn the first case define \\(1/U(P)\\) to be the unique \\(p\\) (see previous theorem) defined by \\(p P + ( 1 -p )T \\sim S\\). In the second case, define \\(U(P) =1\\). In the third, there exists a unique \\(q\\) with \\(q S + ( 1 -q )T \\sim P\\) and then define \\(U(P)=q\\). In the fourth case, define \\(U(P)=0\\) and finally when \\(T \\succ P\\) there exists a unique \\(r\\) with \\(r S + ( 1-r )P \\sim T\\) and then we define \\(U(P) = - r / (1 - r)\\).\nThen check that \\(U(P)\\) satisfies the conditions. See Savage (1954), Ramsey (1927) and de Finetti (1931)\n\n\n\n\nExample 4.1 (Saint Petersburg Paradox) The Saint Petersburg paradox is a concept in probability and decision theory that was first introduced by Daniel Bernoulli in 1738. It revolves around the idea of how individuals value risky propositions and how those valuations may not align with classical expected utility theory.\nThe paradox is named after the city of Saint Petersburg, where the problem was formulated. Here’s a simplified version of the paradox:\nImagine a gambling game where a fair coin is flipped repeatedly until it lands on heads. The payoff for the game is \\(2^N\\), where \\(N\\) is the number of tosses needed for the coin to land on heads. The expected value of this game, calculated by multiplying each possible payoff by its probability and summing the results, is infinite:\n\\[\nE(X) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot 4 + \\frac{1}{8} \\cdot 8 + \\ldots = \\infty\n\\]\nThis means that, in theory, a rational person should be willing to pay any finite amount to play this game, as the expected value is infinite. However, in reality, most people would be unwilling to pay a large amount to play such a game.\nThe paradox arises because traditional expected utility theory assumes that individuals make decisions based on maximizing their expected gain. Bernoulli argued that people do not maximize expected monetary value but rather expected utility, where utility is a subjective measure of satisfaction or happiness. He proposed that individuals exhibit diminishing marginal utility for wealth, meaning that the additional satisfaction gained from an extra unit of wealth decreases as total wealth increases.\nIn the case of the Saint Petersburg paradox, although the expected monetary value is infinite, the utility gained from each additional dollar diminishes rapidly, leading to a reluctance to pay large amounts to play the game.\nIn modern decision theory and economics, concepts like diminishing marginal utility and expected utility are fundamental in understanding how individuals make choices under uncertainty and risk. The Saint Petersburg paradox highlights the limitations of relying solely on expected monetary value in explaining human behavior in such situations.\nOne common approach is to consider aspects of potential players, such as their possible risk aversion, available funds, etc., through a utility function \\(U(x)\\). Applying a utility function in this situation means changing our focus to the quantity \\[\nE[U(X)] = \\sum^\\infty_{k=1} 2^{-k} U(2^k).   \n\\]\nSome examples of utility functions are,\n\n\\(U(x) = V_0 (1-x^{-\\alpha})\\), \\(\\alpha &gt; 0\\), which gives an expected utility of \\(V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\)\nLog utility, \\(U(x) = \\log(x)\\), with expected value \\(2 \\log(2)\\).\n\nNotice that after obtaining an expected utility value, you’ll have to find the corresponding reward/dollar amount.\nFor the log utility case, we need to find the certain dollar amount \\(x^*\\) that provides the same utility as playing the game. Setting \\(U(x^*) = 2\\log(2)\\), we solve: \\[\n\\log(x^*) = 2\\log(2) = \\log(2^2) = \\log(4)\n\\] which gives \\(x^* = 4\\). Therefore, under log utility, a rational player would be willing to pay at most $4 to play the Saint Petersburg game, despite its infinite expected monetary value. This is a dramatic reduction from infinity and demonstrates how risk aversion (captured by the concave log utility function) resolves the paradox.\nSimilarly, for the power utility \\(U(x) = V_0 (1-x^{-\\alpha})\\) with \\(\\alpha &gt; 0\\), we have an expected utility of \\(V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\). To find the certainty equivalent \\(x^*\\), we solve: \\[\nV_0 (1-(x^*)^{-\\alpha}) = V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\n\\] which simplifies to \\((x^*)^{-\\alpha} = \\frac{1}{2^{\\alpha+1}-1}\\), giving: \\[\nx^* = \\left(2^{\\alpha+1}-1\\right)^{1/\\alpha}\n\\] For example, with \\(\\alpha = 1\\), we get \\(x^* = (2^2-1)^1 = 3\\) dollars. As risk aversion increases (larger \\(\\alpha\\)), the certainty equivalent further decreases.\n\n\n\n\n\nRisk Averse if \\(\\mathbb{E}_P \\left ( U(W) \\right ) \\leq U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Neutral if \\(\\mathbb{E}_P \\left ( U(W) \\right ) =    U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Seeking if \\(\\mathbb{E}_P \\left ( U(W) \\right ) \\geq U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\n\n\n\nExample 4.2 (Risk Aversion) Consider two concave utility functions log-utility (Kelly) \\(U(W)=\\log(W)\\) and fractional Kelly \\(U(W)=\\dfrac{W^{1-\\gamma} - 1}{1-\\gamma}\\), when \\(\\gamma = 1\\), we get the log-utility. Both correspond to different risk aversion levels. The log-utility is a special case of the power utility function, which is defined as \\(U(W) = \\frac{W^{1-\\gamma} - 1}{1-\\gamma}\\) for \\(\\gamma &gt; 0\\). The fractional Kelly criterion is a specific case of the power utility function where \\(\\gamma = 2\\).\n\nw &lt;- seq(0.2, 2, length.out = 500)\ngamma=2\n # Plot this data frame\ndf &lt;- data.frame(w = w, \n                 logu = log(w), \n                 kelly = (w^(1-gamma) - 1) / (1-gamma))\n# 3. Plot use different y-scales for \nggplot(df, aes(x = w)) +\n  geom_line(aes(y = logu, color = \"log utility\"), size = 1.5) +\n  geom_line(aes(y = kelly, color = \"Kelly criterion\"), size = 1.5) +\n  geom_vline(xintercept = c(0.5, 1, 1.5), linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"log utility\" = \"blue\", \"Kelly criterion\" = \"red\")) +\n  labs(title = \"Convex Utility Functions\",\n       x = \"Wealth (w)\",\n       y = \"Utility\",\n       color = \"Utility Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nw1 &lt;- 0.5\nw2 &lt;- 1.5\np &lt;- 0.5\ngamme = 2\n# Utility function (example: log utility)\nutility1 &lt;- function(w) log(w)\nutility2 &lt;- function(w) (w^(1 - gamma) - 1) / (1 - gamma)\n\n# Calculate expected utility\nexpected_utility &lt;- p * utility1(w1) + p * utility1(w2)\nexpected_utility2 &lt;- p * utility2(w1) + p * utility2(w2)\n# Print expected utility\ncat(\"Expected utility for log utility:\", expected_utility, \"\\n\")\n\nExpected utility for log utility: -0.14 \n\ncat(\"Expected utility for Kelly criterion:\", expected_utility2, \"\\n\")\n\nExpected utility for Kelly criterion: -0.33 \n\n\n\nGeometrically, Jensen’s inequality explains the pattern: for a concave function the chord lies below the curve (risk aversion).\n\n\nExample 4.3 (Kelly Criterion) The Kelly criterion has been used effectively by many practitioners. Ed Thorp, in his book Beat the Dealer, pioneered its use in blackjack and later applied it to investing in financial markets. Since then, many market participants, such as Jim Simons, have stressed the importance of this money management approach. The criterion’s application extends to other domains: Phil Laak described its use for bet sizing in a game-theoretic approach to poker, and Bill Benter applied it to horse racing. Stewart Ethier provided a mathematical framework for multiple outcomes and analyzed a “play the winner” rule in roulette. Claude Shannon also developed a system to detect and exploit unintentionally biased roulette wheels, an endeavor chronicled in the book The Eudaemonic Pie.\nSuppose you have $1000 to invest. With probability \\(0.55\\) you will win whatever you wager and with probability \\(0.45\\) you lose whatever you wager. What’s the proportion of capital that leads to the fastest compounded growth rate?\nQuoting Kelly (1956), the exponential rate of growth, \\(G\\), of a gambler’s capital is \\[\nG = \\lim_{N\\to \\infty} \\frac{1}{N} \\log_2 \\frac{W_N}{W_0}\n\\] for initial capital \\(W_0\\) and capital after \\(N\\) bets \\(W_N\\).\nUnder the assumption that a gambler bets a fraction of his capital, \\(\\omega\\), each time, we use \\[\nW_N = (1+\\omega)^W (1-\\omega)^L W_0\n\\] where \\(W\\) and \\(L\\) are the number of wins and losses in \\(N\\) bets. We get \\[\nG = p \\log_2(1+\\omega)+ q \\log_2(1-\\omega)\n\\] in which the limit(s) of \\(\\frac{W}{N}\\) and \\(\\frac{L}{N}\\) are the probabilities \\(p\\) and \\(q\\), respectively.\nThis also comes about by considering the sequence of i.i.d. bets with \\[\np ( X_t = 1 ) = p \\; \\; \\text{ and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] We want to find an optimal allocation \\(\\omega^*\\) that maximizes the expected long-run growth rate: \\[\\begin{align*}\n\\max_\\omega \\mathbb{E} \\left ( \\ln ( 1 + \\omega W_T ) \\right )\n& = p \\ln ( 1 + \\omega ) + (1 -p) \\ln (1 - \\omega ) \\\\\n& \\leq p \\ln p + q \\ln q + \\ln 2 \\; \\text{ and} \\; \\omega^\\star = p - q\n\\end{align*}\\]\nThe solution is \\(w^* = 0.55 - 0.45 = 0.1\\).\nBoth approaches give the same optimization problem, which, when solved, give the optimal fraction rate \\(\\omega^* = p-q\\), thus, with \\(p=0.55\\), the optimal allocation is 10% of capital.\nWe can generalize the rule to the case of asymmetric payouts \\((a,b)\\). Then the expected utility function is \\[\np \\ln ( 1 + b \\omega ) + (1 -p) \\ln (1 - a \\omega )\n\\] The optimal solution is \\[\n\\omega^\\star = \\frac{bp - a q}{ab}\n\\]\nIf \\(a=b=1\\) this reduces to the pure Kelly criterion.\nA common case occurs when \\(a=1\\) and market odds \\(b=O\\). The rule becomes \\[\n\\omega^* = \\frac{p \\cdot O  -q }{O}.\n\\]\nLet’s consider another scenario. You have two possible market opportunities: one where it offers you \\(4/1\\) when you have personal odds of \\(3/1\\) and a second one when it offers you \\(12/1\\) while you think the odds are \\(9/1\\).\nIn expected return these two scenarios are identical both offering a 33% gain. In terms of maximizing long-run growth, however, they are not identical.\nTable 4.1 shows the Kelly criterion advises an allocation that is twice as much capital to the lower odds proposition: \\(1/16\\) weight versus \\(1/40\\).\n\n\n\nTable 4.1: Kelly rule\n\n\n\n\n\nMarket\nYou\n\\(\\Delta\\)\n\\(\\omega^\\star\\)\n\n\n\n\n\\(4/1\\)\n\\(3/1\\)\n\\(1/4\\)\n\\(1/16\\)\n\n\n\\(12/1\\)\n\\(9/1\\)\n\\(1/10\\)\n\\(1/40\\)\n\n\n\n\n\n\nThe optimal allocation \\(\\omega^\\star = ( p O - q ) / O\\) is \\[\n\\frac{ (1/4) \\times 4 - (3/4) }{4} = \\frac{1}{16} \\; \\text{ and} \\;\n\\frac{ (1/10) \\times 12 - (9/10) }{12} = \\frac{1}{40}.\n\\]\nNote, that although the expected return is the same, the risk is different. The first gamble has a higher variance than the second gamble.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#unintuitive-nature-of-decision-making",
    "href": "04-dec.html#unintuitive-nature-of-decision-making",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.2 Unintuitive Nature of Decision Making",
    "text": "4.2 Unintuitive Nature of Decision Making\n\nExample 4.4 (Ellsberg Paradox: Ambiguity Aversion) The Ellsberg paradox is a thought experiment that was first proposed by Daniel Ellsberg in 1961. It is a classic example of a situation where individuals exhibit ambiguity aversion, meaning that they prefer known risks over unknown risks. The paradox highlights the importance of considering ambiguity when making decisions under uncertainty.\nThere are two urns each containing 100 balls. It is known that urn A contains 50 red and 50 black, but urn B contains an unknown mix of red and black balls. The following bets are offered to a participant:\n\nBet 1A: get $1 if red is drawn from urn A, $0 otherwise\nBet 2A: get $1 if black is drawn from urn A, $0 otherwise\nBet 1B: get $1 if red is drawn from urn B, $0 otherwise\nBet 2B: get $1 if black is drawn from urn B, $0 otherwise\n\n\n\nExample 4.5 (Allais Paradox: Independence Axiom) The Allais paradox is a choice problem designed by Maurice Allais to show an inconsistency of actual observed choices with the predictions of expected utility theory. The paradox is that the choices made in the second problem seem irrational, although they can be explained by the fact that the independence axiom of expected utility theory is violated.\nWe run two experiments. In each experiment a participant has to make a choice between two gambles.\n\n\n\n\n\n\nExperiment 1\n\n\n\n\n\n\n\nGamble \\({\\cal G}_1\\)\n\nGamble \\({\\cal G}_2\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n1\n$5m\n0.89\n\n\n$0m\n0\n$0m\n0.01\n\n\n\n\n\n\n\n\nExperiment 2\n\n\n\n\n\n\n\nGamble \\({\\cal G}_3\\)\n\nGamble \\({\\cal G}_4\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25\n0\n$25m\n0.1\n\n\n$5\n0.11\n$5m\n0\n\n\n$0m\n0.89\n$0m\n0.9\n\n\n\n\n\n\nThe difference in expected gains is identical in two experiments\n\nE1 = 5*1 \nE2 = 25*0.1 + 5*0.89 + 0*0.01\nE3 = 5*0.11 + 0*0.89\nE4 = 25*0.1 + 0*0.9\nprint(c(E1-E2,E3-E4))\n\n -2 -2\n\n\nHowever, typically a person prefers \\({\\cal G}_1\\) to \\({\\cal G}_2\\) and \\({\\cal G}_4\\) to \\({\\cal G}_3\\), we can conclude that the expected utilities of the preferred are greater than the expected utilities of the second choices. The fact is that if \\({\\cal G}_1 \\geq {\\cal G}_2\\) then \\({\\cal G}_3 \\geq {\\cal G}_4\\) and vice-versa.\nAssuming the subjective probabilities \\(P = ( p_1 , p_2 , p_3)\\). The expected utility \\(E ( U | P )\\) is \\(u ( 0 ) = 0\\) and for the high prize set \\(u ( \\$ 25 \\; \\text{million} ) = 1\\), which leaves one free parameter \\(u = u(\\$ 5 \\; \\text{million})\\).\nHence to compare gambles with probabilities \\(P\\) and \\(Q\\) we look at the difference \\[\nE ( u | P ) - E ( u | Q ) = ( p_2 - q_2 ) u + ( p_3 - q_3 )\n\\]\nFor comparing \\({\\cal G}_1\\) and \\({\\cal G}_2\\) we get \\[\\begin{align*}\nE ( u | {\\cal G}_1 ) - E ( u | {\\cal G}_2 ) &= 0.11 u - 0.1 \\\\\nE ( u | {\\cal G}_3 ) - E ( u | {\\cal G}_4 ) &= 0.11 u - 0.1\n\\end{align*}\\] The order is the same, given your \\(u\\). If your utility satisfies \\(u &lt; 0.1/0.11 = 0.909\\) you take the “riskier” gamble.\n\n\nExample 4.6 (Winner’s Curse) One of the interesting facts about expectation is that when you are in a competitive auctioning game then you shouldn’t value things based on pure expected value. You should take into consideration the event that you win \\(W\\). Really you should be calculating \\(E(X\\mid W)\\) rather than \\(E(X)\\).\nThe winner’s curse: given that you win, you should feel regret: \\(E(X\\mid W) &lt; E(X)\\).\nA good example is claiming racehorse whose value is uncertain.\n\n\n\nValue\nOutcome\n\n\n\n\n0\nhorse never wins\n\n\n50,000\nhorse improves\n\n\n\nSimple expected value tells you \\[\nE(X) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 50,000 = \\$25,000.\n\\] In a $20,000 claiming race (you can buy the horse for this fixed fee ahead of time from the owner) it looks like a simple decision to claim the horse.\nIt’s not so simple! We need to calculate a conditional expectation. What’s \\(E( X\\mid W )\\), given you win event (\\(W\\))? This is the expected value of the horse given that you win that is relevant to assessing your bid. In most situations \\(E(X\\mid W) &lt; 20,000\\).\nAnother related feature of this problem is asymmetric information. The owner or trainer of the horse may know something that you don’t know. There’s a reason why they are entering the horse into a claiming race in the first place.\nWinner’s curse implies that immediately after you have won, you should feel a little regret, as the object is less valuable to you after you have won! Or put another way, in an auction nobody else in the room is willing to offer more than you at that time.\n\n\nExample 4.7 (The Hat Problem) There are \\(N\\) prisoners in a forward facing line. Each guy is wearing a blue or red hat. Everyone can see all the hats in front of him, but cannot see his own hat. The hats can be in any combination of red and blue, from all red to all blue and every combination in between. The first guy doesn’t know his own hat.\nA guard is going to walk down the line, starting in the back, and ask each prisoner what color hat they have on. They can only answer “blue” or “red.” If they answer incorrectly, or say anything else, they will be shot dead on the spot. If they answer correctly, they will be set free. Each prisoner can hear all of the other prisoners’ responses, as well as any gunshots that indicate an incorrect response. They can remember all of this information.\nThere is a rule that all can agree to follow such that the first guy makes a choice (“My hat is …”) and everyone after that, including the last guy, will get their color right with probability \\(1\\). You have a \\(100\\)% chance of saving all but the last prisoner, and a \\(50\\)% chance of saving that one. Here’s the strategy the prisoners have agreed on. The last prisoner counts the number of blue hats worn; if the number is even, the last prisoner yells “blue”, if odd, yells “red”. If the \\(99\\)th prisoner hears “blue”, but counts an odd number of blue hats, then his hat must be blue so that the total number of blue hats is even. If he counts an even number of blue hats, then his hat must be red. If the last prisoner yells red, then 99 knows that there are an odd number of blue hats. So 99 counts the number of blue hats he can see. Again, if they are even, his hat is blue, if odd, his hat is red. The 99th prisoner then yells out the color of his hat and is spared. The next prisoner now knows whether the remaining number of blue hats, including his own, is odd or even, by taking into account whether 99 had a blue hat or not. Then by counting the number of blue hats he sees, he knows the color of his hat. So he yells out the color of his hat and is spared. This saves all but the last prisoner, and there is a \\(50\\)% chance that his hat is the color he shouted out.\nOne hundred prisoners are too many to work with. Suppose there are two. The last person can save the guy in front of him by shouting out the color of his hat. OK, how about if there are three? The third prisoner can see 0,1, or 2 blue hats. There seem to be three possibilities but only two choices of things to say. But, two of the possibilities have something in common namely the number of blue hats is even. So if the last prisoner yells “blue” then he can tell 1 and 2 that he sees an even number of blue hats. Then the second prisoner, by looking ahead and counting the number of blue hats, knows his must be blue if he sees one blue hat, and red if he sees no blue hats. The last prisoner agrees to yell “red” if the number of blue hats seen is odd. Then if 2 sees a blue hat on 1, his must be red, and if 1 has a red hat, his must be blue. By shouting out the color of his hat, 1 also knows his hat color. Two “blues” or two “reds” in a row mean he wears blue, while one blue and one red means he wears red. OK. This looks like this always works, because there are always only two possibilities as far as the number of blue hats worn they are either even or odd. So, check as in the three-person case that using this strategy (“blue” for an even number of blue hats “red” for an odd number) tells 99 the color of his hat, and then each prisoner in turn can learn the color of his hat by taking into account the parity of the number of blue hats he can see, the parity of the number of blue hats 100 saw and the number of prisoners behind him wearing blue hats.\n\n\nExample 4.8 (Lemon’s Problem) The lemon problem is an interesting conditional probability puzzle and is a classic example of asymmetric information in economics. It was first proposed by George Akerlof in his 1970 paper “The Market for Lemons: Quality Uncertainty and the Market Mechanism.” The problem highlights the importance of information in markets and how it can lead to adverse selection, where the quality of goods or services is lower than expected.\nThe basic tenet of the lemons principle is that low-value cars force high-value cars out of the market because of the asymmetrical information available to the buyer and seller of a used car. This is primarily due to the fact that a seller does not know what the true value of a used car is and, therefore, is not willing to pay a premium on the chance that the car might be a lemon. Premium-car sellers are not willing to sell below the premium price so this results in only lemons being sold.\nSuppose that a dealer pays $20K for a car and wants to sell for $25K. Some cars on the market are Lemons. The dealer knows whether a car is a lemon. A lemon is only worth $5K. There is asymmetric information as the customer doesn’t know if the particular new car is a lemon. S/he estimates the probability of lemons on the road by using the observed frequency of lemons. We will consider two separate cases:\n\nLet’s first suppose only 10% of cars are lemons.\nWe’ll then see what happens if 50% are lemons.\n\nThe question is how does the market clear (i.e. at what price do car’s sell). Or put another way does the customer buy the car and if so what price is agreed on? This is very similar to winner’s curse: when computing an expected value what conditioning information should I be taking into account?\nIn the case where the customer thinks that \\(p=0.10\\) of the car’s are lemons, they are willing to pay \\[\nE (X)= \\frac{9}{10} \\cdot 25 + \\frac{1}{10} \\cdot 5 = \\$ 23 K\n\\] This is greater than the initial $20 that the dealer paid. The car then sells at $23K \\(&lt;\\) $25K.\nOf course, the dealer is disappointed that there are lemons on the road as he is not achieving the full value – missing $2000. Therefore, they should try and persuade the customer its not a lemon by offering a warranty for example.\nThe more interesting case is when \\(p=0.5\\). The customer now values the car at \\[\nE (X)  = \\frac{1}{2} \\cdot 25 + \\frac{1}{2} \\cdot 5 = \\$ 15K\n\\] This is lower than the $20K – the reservation price that the dealer would have for a good car. Now what type of car and at what price do they sell?\nThe key point in asymmetric information is that the customer must condition on the fact that if the dealer still wants to sell the car, the customer must update his probability of the type of the car. We already know that if the car is not a lemon, the dealer won’t sell under his initial cost of $20K. So at $15K he is only willing to sell a lemon. But then if the customer computes a conditional expectation \\(E( X \\mid \\mathrm{Lemon})\\) – conditioning on new information that the car is a lemon \\(L\\) we get the valuation \\[\nE ( X \\mid L ) = 1 \\cdot  5 = \\$ 5K\n\\] Therefore only lemons sell, at $ 5K, even if the dealer has a perfectly good car the customer is not willing to buy!\nAgain what should the dealer do? Try to raise the quality and decrease the frequency of lemons in the observable market. This type of modeling has all been used to understand credit markets and rationing in periods of loss of confidence.\n\n\nExample 4.9 (Envelope Paradox) The envelope paradox is a thought experiment or puzzle related to decision-making under uncertainty. It is also known as the “exchange paradox” or the “two-envelope paradox.” The paradox highlights the importance of carefully considering the information available when making decisions under uncertainty and the potential pitfalls of making assumptions about unknown quantities.\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent. The amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope. You know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch! Where’s the flaw in this argument?\nThis is an open-ended problem, but it will not be very confusing if we well understand both the frequentist and bayesian approaches. Actually, this is a very good example to show how these two approaches are different and to check if we understand them correctly. There many conditions in this problem, so we cannot argue everything in this example; instead, we are going to focus on some interesting cases. First, assume we’re risk-neutral (although, we can simply change “money” with “utility” in this paradox, so it doesn’t matter). We will compare frequentist/bayesian, open/not open, and discrete/continuous. The finite, or bounded space, case will not be considered here since they are not very interesting.\nIf I DO NOT look in my envelope, in this case, even from a frequentist viewpoint, we can find a fallacy in this naive expectation reasoning \\(E[trade] = 5X/4\\) . First, the right answer from a frequentist view is, loosely, as follows. If we switch the envelope, we can obtain \\(m\\) (when \\(X = m\\)) or lose \\(m\\) (when \\(X = 2m\\)) with the same probability \\(1/2\\). Thus, the value of a trade is zero, so that trading matters not for my expected wealth.\nInstead, naive reasoning is confusing the property of variables \\(x\\) and \\(m\\), \\(x\\) is a random variable and \\(m\\) is a fixed parameter which is constant (again, from a frequentist viewpoint). By trading, we can obtain \\(x\\) or lose \\(x/2\\) with the same probability. Here, the former \\(x=m\\) is different from the latter \\(X= 2m\\). Thus, \\(X \\frac{1}{2} - \\frac{X}{2} \\frac{1}{2} = \\frac{X}{4}\\) is the wrong expected value of trading. On the other hand, from a bayesian view, since we have no information, we are indifferent to either trading or not.\nThe second scenario is if I DO look in my envelope. As the Christensen & Utts (1992) article said, the classical view cannot provide a completely reasonable resolution to this case. It is just ignoring the information revealed. Also, the arbitrary decision rule introduced at the end of the paper or the extension of it commented by Ross (1996) are not the results of reasoning from a classical approach. However, the bayesian approach provides a systematic way of finding an optimal decision rule using the given information.\nWe can use the Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami. Such an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by \\[\nE(y\\mid x) =  p \\left ( y = \\frac{1}{2} x \\mid  x \\right ) \\cdot  \\frac{1}{2} x +\n  p \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E(y) &gt; x\\) becomes a decision rule.\nLet \\(g(m)\\) be the prior distribution of \\(m\\). Applying Bayes’ theorem, we have \\[\np(m = x \\mid X = x) = \\frac{p(X = x \\mid m = x) g(x)}{p(X = x)} = \\frac{g(x)}{g(x)+g(x/2)}.\n\\] Similarly, we have \\[\np(m = x/2 \\mid X = x) = \\frac{p(X = x \\mid m = x/2) g(x/2)}{p(X = x/2)} = \\frac{g(x/2)}{g(x)+g(x/2)}.\n\\] The Bayesian can now compute his expected winnings from the two actions. If he keeps the envelope he has, he wins \\(x\\) dollars. If he trades envelopes, he wins \\(x/2\\) if he currently has the envelope with \\(2m\\) dollars, i.e., if \\(m = x/2\\) and he wins \\(2\\)x if he currently has the envelope with \\(m\\) dollars, i.e., \\(m = x\\). His expected winnings from a trade are \\[\nE(W\\mid Trade) = E(Y\\mid X = x) = \\frac{g(x/2)}{g(x)+g(x/2)} \\frac{x}{2} + \\frac{g(x)}{g(x)+g(x/2)} 2x.\n\\] It is easily seen that when \\(g(x/2) = 2g(x)\\), \\(E(W\\mid Trade) = x\\). Therefore, if \\(g(x/2) &gt; 2g(x)\\) it is optimal to keep the envelope and if \\(g(x/2) &lt; 2g(x)\\) it is optimal to trade envelopes. For example, if your prior distribution on \\(m\\) is exponential \\(\\lambda\\), so that \\(g(m) = \\lambda e^{-\\lambda m}\\), then it is easily seen that it is optimal to keep your envelope if \\(x &gt; 2\\log(2)/\\lambda\\).\nThe intuitive value of the expected winnings when trading envelopes was shown to be \\(5x/4\\). This value can be obtained by assuming that \\(g(x)/[g(x) + g(x/2)] =\n1/2\\) for all \\(x\\). In particular, this implies that \\(g(x) = g(x/2)\\) for all x, i.e., \\(g(x)\\) is a constant function. In other words, the intuitive expected winnings assumes an improper “noninformative” uniform density on \\([0, \\infty)\\). It is of interest to note that the improper noninformative prior for this problem gives a truly noninformative (maximum entropy) posterior distribution.\nMost of the arguments in the Christensen & Utts (1992) paper are right, but there is one serious error in the article which is corrected in Bachman-Christensen-Utts (1996) and discussed in Brams & Kilgour (1995). The paper calculated the marginal density of \\(X\\) like below. \\[\\begin{align*}\np(X = x) &= p(m = x)g(x) + p(2m = x)g(x/2) \\\\\n&= \\frac{1}{2} g(x) + \\frac{1}{2} g(x/2)\n\\end{align*}\\] where \\(g(x)\\) is the prior distribution of \\(m\\). However, integrating \\(p(X = x)\\) with respect to \\(x\\) from \\(0\\) to \\(\\infty\\) gives \\(3/2\\) instead of \\(1\\). In fact, their calculation of \\(p(X = x)\\) can hold only when the prior distribution \\(g(x)\\) is discrete and \\(p(X = x)\\), \\(g(m)\\), \\(g(m/2)\\) represent the probabilities that \\(X = x\\), \\(m = m\\), \\(m = m/2\\), respectively.\nFor the correct calculation of the continuous \\(X\\) case, one needs to properly transform the distribution. That can be done by remembering to include the Jacobian term alongside the transformed PDF, or by working with the CDF of \\(X\\) instead. The latter forces one to properly consider the transform, and we proceed with that method.\nLet \\(G(x)\\) be the CDF of the prior distribution of \\(m\\) corresponding to \\(g(x)\\). \\[\\begin{align*}\np(x &lt; X \\leq x+dx) &= p(m = x)dG(x)+ p(2m = x)dG(x/2) \\\\\n&= \\frac{1}{2} \\left( dG(x)+ dG(x/2) \\right)\n\\end{align*}\\] where \\(g(x) = dG(x)/dx\\). Now, the PDF of \\(X\\) is \\[\\begin{align*}\nf_X(x) &= \\frac{d}{dx} p(x &lt; X \\leq x + dx) \\\\\n&= \\frac{1}{2} \\left(g(x) + \\frac{1}{2} g(x/2) \\right)\n\\end{align*}\\] We have an additional \\(1/2\\) in the last term due to the chain rule, or the Jacobian in the change-in-variable formula. Therefore, the expected amount of a trade is \\[\\begin{align*}\nE(Y\\mid X = x) &= \\frac{x}{2} p(2m = x\\mid X = x) + 2 x \\, p(m = x\\mid X = x) \\\\\n&= \\frac{x}{2} \\frac{g(x)}{g(x) + g(x/2)/2} + 2 x \\frac{g(x/2)/2}{g(x) + g(x/2)/2} \\\\\n&=  \\frac{\\frac{x}{2}g(x) + x g(x/2)}{g(x) + g(x/2)/2}\n\\end{align*}\\]\nThus, for the continuous case, trading is advantageous whenever \\(g(x/2) &lt; 4g(x)\\), instead of the decision rule for the discrete case \\(g(x/2) &lt; 2g(x)\\).\nNow, think about which prior will give you the same decision rule as the frequentist result. In the discrete case, \\(g(x)\\) such that \\(g(x/2) = 2g(x)\\), and in the continuous case \\(g(x)\\) such that \\(g(x/2) = 4g(x)\\). However, both do not look like useful, non-informative priors. Therefore, the frequentist approach does not always equal the Bayes approach with a non-informative prior. At the moment you start to treat \\(x\\) as a given number, and consider \\(p(m \\mid X = x)\\) (or \\(p(Y \\mid X = x)\\)), you are thinking in a bayesian way, and need to understand the implications and assumptions in that context.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#decision-trees",
    "href": "04-dec.html#decision-trees",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.3 Decision Trees",
    "text": "4.3 Decision Trees\nDecision trees can effectively model and visualize conditional probabilities. They provide a structured way to break down complex scenarios into smaller, more manageable steps, allowing for clear calculations and interpretations of conditional probabilities.\nEach node in a decision tree, including the root, represents an event or condition. The branches represent the possible outcomes of that condition. Along each branch, you’ll often see a probability. This is the chance of that outcome happening, given the condition at the node. As you move down the tree, you’re looking at more specific conditions and their probabilities. The leaves of the tree show the final probabilities of various outcomes, considering all the conditions along the path to that leaf. Thus, the probabilities of the leaves need to sum to 1.\n\nExample 4.10 (Medical Testing) A patient goes to see a doctor. The doctor performs a test which is 95% sensitive – that is 95 percent of people who are sick test positive and 99% specific – that is 99 percent of the healthy people test negative. The doctor also knows that only 1 percent of the people in the country are sick. Now the question is: if the patient tests positive, what are the chances the patient is sick? The intuitive answer is 99 percent, but the correct answer is 66 percent.\nFormally, we have two binary variables, \\(D=1\\) that indicates you have a disease and \\(T=1\\) that indicates that you test positive for it. The estimates we know already are given by \\(P(D) = 0.02\\), \\(P(T\\mid D) = 0.95\\), and \\(P(\\bar T \\mid \\bar D) = 0.99\\). Here we used shortcut notations, instead of writing \\(P(D=1)\\) we used \\(P(D)\\) and instead of \\(P(D=0)\\) we wrote \\(P(\\bar D)\\).\nSometimes it is more intuitive to describe probabilities using a tree rather than tables. The tree below shows the conditional distribution of \\(D\\) and \\(T\\).\n\n\n\n\n\n\n\n\ngraph LR\n    D --0.2--&gt; d1(D=1)\n    D --0.98--&gt; d0(D=0)\n    d1--0.95--&gt;t1(T=1)\n    d1 --0.05--&gt; t0(T=0)\n    d0--0.01--&gt;t10(T=1)\n    d0 --0.99--&gt; t00(T=0)\n\n\n\n\nFigure 4.1\n\n\n\n\n\nThe result is not as intuitive as in the NBA example. Let’s think about this intuitively. Rather than relying on Bayesian math to help us with this, let us consider another illustration. Imagine that the above story takes place in a small town, with \\(1,000\\) people. From the prior \\(P(D)=0.02\\), we know that 2 percent, or 20 people, are sick, and \\(980\\) are healthy. If we administer the test to everyone, the most probable result is that 19 of the 20 sick people test positive. Since the test has a 1 percent error rate, however, it is also probable that 9.8 of the healthy people test positive, we round it to 10.\nNow if the doctor sends everyone who tests positive to the national hospital, there will be 10 healthy and 19 sick patients. If you meet one, even though you are armed with the information that the patient tested positive, there is only a 66 percent chance this person is sick.\nLet’s extend the example and add the utility of the test and the utility of the treatment. Then the decision problem is to treat \\(a_T\\) or not to treat \\(a_N\\). The Q-function is the function of the state \\(S \\in \\{D_0,D_1\\}\\) and the action \\(A \\in \\{a_T,a_N\\}\\)\n\nUtility of the test and the treatment.\n\n\nA/S\n\\(a_T\\)\n\\(a_N\\)\n\n\n\n\n\\(D_0\\)\n90\n100\n\n\n\\(D_1\\)\n90\n0\n\n\n\nThen expected utility of the treatment is 90 and no treatment is 98. A huge difference. Given our prior knowledge, we should not treat everyone.\n\n0.02*90 + 0.98*90  # treat\n\n 90\n\n0.02*0 + (1-0.02)*100 # do not treat\n\n 98\n\n\nHowever, the expected utility will change when our probability of disease changes. Let’s say that we are in a country where the probability of disease is 0.1 or we performed a test and updated our prior probability of disease to some number \\(p\\). Then the expected utility of the treatment is \\(E\\left[U(a_T)\\right] = 90\\) and no treatment is \\[\nE\\left[U(a_N)\\right] = 0\\cdot p + 100 \\cdot (1-p) = 100(1-p)\n\\] When we are unsure about the value of \\(p\\) we may want to explore how the optimal decision changes as we vary \\(p\\)\n\np = seq(0,1,0.01)\nplot(p, 100*(1-p), type = \"l\", xlab = \"p\", ylab = \"E[U(a)]\")\nabline(h=90, col=\"red\")\nlegend(\"bottomleft\", legend = c(TeX(\"$E[U(a_N)]$\"), \n      TeX(\"$E[U(a_T)]$\")), col = c(\"black\", \"red\"), lty = 1, bty='n')\n\n\n\n\nExpected utility of the treatment and no treatment as a function of the prior probability of disease.\n\n\n\n\nIf our estimate is at the crossover point, then we should be indifferent between treatment and no treatment, if on the left of the crossover point, we should treat, and if on the right, we should not treat. The crossover point is. \\[\n100(1-p) = 90, ~p = 0.1\n\\]\nThe gap of \\(90-100(1-p)\\) is the expected gain from treatment.\nNow, let us calculate the value of test, e.g. the change in expected utility from the test. We will need to calculate the posterior probabilities\n\n# P(D | T = 0) = P(T = 0 | D) P(D) / P(T = 0)\npdt0 = 0.05*0.02/(0.05*0.02 + 0.99*0.98) \nprint(pdt0)\n\n 0.001\n\n# Expected utility given the test is negative \n# E[U(a_N | T=0)]\nUN0 = pdt0*0 + (1-pdt0)*100\nprint(UN0)\n\n 100\n\n# E[U(a_T | T=0)]\nUT0 = pdt0*90 + (1-pdt0)*90\nprint(UT0)\n\n 90\n\n\nGiven test is negative, our best action is not to treat. Our utility is 100. What if the test is positive?\n\n# P(D | T = 1) = P(T = 1 | D) P(D) / P(T = 1)\npdt = 0.95*0.02/(0.95*0.02 + 0.01*0.98)\nprint(pdt)\n\n 0.66\n\n# E[U(a_N | T=1)]\nUN1 = pdt*0 + (1-pdt)*100\nprint(UN1)\n\n 34\n\n# E[U(a_T | T=1)]\nUT1 = pdt*90 + (1-pdt)*90\nprint(UT1)\n\n 90\n\n\nThe best option is to treat now! Given the test our strategy is to treat if the test is positive and not treat if the test is negative. Let’s calculate the expected utility of this strategy.\n\n# P(T=1) = P(T=1 | D) P(D) + P(T=1 | D=0) P(D=0)\npt = 0.95*0.02 + 0.01*0.98\nprint(pt)\n\n 0.029\n\n# P(T=0) = P(T=0 | D) P(D) + P(T=0 | D=0) P(D=0)\npt0 = 0.05*0.02 + 0.99*0.98\nprint(pt0)\n\n 0.97\n\n# Expected utility of the strategy\npt*UT1 + pt0*UN0\n\n 100\n\n\nThe utility of our strategy of 100 is above the strategy prior to testing (98), this difference of 2 is called the value of information.\n\n\nExample 4.11 (Mudslide) I live in a house that is at risk of being damaged by a mudslide. I can build a wall to protect it. The wall costs $10,000. If there is a mudslide, the wall will protect the house with probability \\(0.95\\). If there is no mudslide, the wall will not cause any damage. The prior probability of a mudslide is \\(0.01\\). If there is a mudslide and the wall does not protect the house, the damage will cost $100,0000. Should I build the wall?\nLet’s formally solve this as follows:\n\nBuild a decision tree.\nThe tree will list the probabilities at each node. It will also list any costs there are you going down a particular branch.\nFinally, it will list the expected cost of going down each branch, so we can see which one has the better risk/reward characteristics.\n\n\n\n\n\n\n\n\n\ngraph LR\n    B--\"Build: $40, $40.5\"--&gt;Y\n    B--\"Don't Build: $0, $10\"--&gt;N\n    Y--\"Slide: $0, $90\"--&gt;yy[Y]\n    Y--\"No slide $0, $40\"--&gt;40\n    N--\"Slide: $1000, $1000\"--&gt;1000\n    N--\"No slide $0, $0\"--&gt;0\n    yy --\"Hold: $0, $40\"--&gt;401[40]\n    yy --\"Not Hold: $1000, $1040\"--&gt;1040\n\n\n\n\nFigure 4.2\n\n\n\n\n\nThe first dollar value is the cost of the edge, e.g. the cost of building the wall is $10,000. The second dollar value is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide, the expected cost is $15,000. If you build the wall and there is no mudslide, the expected cost is $10,000. The expected cost of building the wall is $10,050. The expected cost of not building the wall is $1,000. The expected cost of building the wall is greater than the expected cost of not building the wall, so you should not build the wall. The dollar value at the leaf nodes is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide and the wall does not hold, the expected cost is $110,000.\nThere’s also the possibility of a further test to see if the wall will hold. Let’s include the geological testing option. The test costs $3000 and has the following accuracies. \\[\nP( T  \\mid  \\mathrm{Slide} ) = 0.90 \\; \\; \\mathrm{and } \\; \\; P( \\mathrm{not~}T  \\mid\n\\mathrm{No \\; Slide} ) = 0.85\n\\] If you choose the test, then should you build the wall?\nLet’s use the Bayes rule. The initial prior probabilities are \\[\nP( Slide ) = 0.01  \\; \\; \\mathrm{and} \\; \\; P ( \\mathrm{No \\; Slide} ) = 0.99\n\\]\n\\[\\begin{align*}\nP( T) & = P( T  \\mid  \\mathrm{Slide} ) P( \\mathrm{Slide} ) +\nP( T  \\mid  \\mathrm{No \\;  Slide} ) P( \\mathrm{No \\; Slide} ) \\\\\nP(T)& = 0.90 \\times 0.01 + 0.15 \\times 0.99 = 0.1575\n\\end{align*}\\] We’ll use this to find our optimal course of action.\nThe posterior probability given a positive test is \\[\\begin{align*}\nP ( Slide  \\mid  T ) & = \\frac{ P ( T  \\mid  Slide ) P ( Slide )}{P(T)} \\\\\n& = \\frac{ 0.90 \\times 0.01}{ 0.1575} = 0.0571\n\\end{align*}\\]\nThe posterior probability given a negative test is \\[\\begin{align*}\nP \\left ( \\mathrm{Slide}  \\mid  \\mathrm{not~}T \\right ) & = \\frac{ P ( \\mathrm{not~}T  \\mid  \\mathrm{Slide} ) P ( \\mathrm{Slide} )}{P(\\mathrm{not~}T)} \\\\\n& = \\frac{0.1 \\times 0.01 }{0.8425} \\\\\n& =0.001187\n\\end{align*}\\]\nCompare this to the initial base rate of a \\(1\\)% chance of having a mud slide.\nGiven that you build the wall without testing, what is the probability that you’ll lose everything? With the given situation, there is one path (or sequence of events and decisions) that leads to losing everything:\n\nBuild without testing (given) Slide (\\(0.01\\))\nDoesn’t hold (\\(0.05\\)) \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{build} \\; \\mathrm{w/o} \\;\n\\mathrm{testing} ) = 0.01 \\times 0.05 = 0.0005\n\\]\n\nGiven that you choose the test, what is the probability that you’ll lose everything? There are two paths that lead to losing everything:\n\nThere are three things that have to happen to lose everything. Test +ve (\\(P=0.1575\\)), Build, Slide (\\(P= 0.0571\\)), Doesn’t Hold (\\(P=0.05\\))\nNow you lose everything if Test -ve (\\(P=0.8425\\)), Don’t Build, Slide given negative (\\(P=0.001187\\)).\n\nThe conditional probabilities for the first path \\[\nP ( \\mathrm{first} \\; \\mathrm{path} ) = 0.1575 \\times 0.0571 \\times 0.05\n= 0.00045\n\\]\nFor the second path \\[\nP ( \\mathrm{second} \\; \\mathrm{path} ) = 0.8425 \\times 0.001187 = 0.00101\n\\]\nHence putting it all together \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{testing} ) = 0.00045 + 0.00101 = 0.00146\n\\]\nPutting these three cases together we can build a risk/reward table\n\n\n\nChoice\nExpected Cost\nRisk\nP\n\n\n\n\nDon’t Build\n$1,000\n0.01\n1 in 100\n\n\nBuild w/o testing\n$10,050\n0.0005\n1 in 2000\n\n\nTest\n$4,693\n0.00146\n1 in 700\n\n\n\nThe expected cost with the test is \\(3000+10000\\times 0.1575+100000\\times 0.001187 = 4693\\)\nWhat do you choose?",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#nash-equilibrium",
    "href": "04-dec.html#nash-equilibrium",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.4 Nash Equilibrium",
    "text": "4.4 Nash Equilibrium\nWhen multiple decision makers interact with each other, meaning the decision of one player changes the state of the “world” and thus affects the decision of another player, then we need to consider the notion of equilibrium. It is a central concept in economics and game theory. The most widely used type of equilibrium is the Nash equilibrium, named after John Nash, who introduced it in his 1950 paper “Equilibrium Points in N-Person Games.” It was popularized by the 1994 film “A Beautiful Mind,” which depicted Nash’s life and work.\nIt is defined as a set of strategies where no player can improve their payoff by unilaterally changing their strategy, assuming others keep their strategies constant. In other words, a Nash equilibrium is a set of strategies where no player has an incentive to deviate from their current strategy, given the strategies of the other players.\nHere are a few examples of Nash equilibria:\n\nPrisoner’s Dilemma: Two prisoners must decide whether to cooperate with each other or defect. The Nash equilibrium is for both to defect, even though they would be better off if they both cooperated.\nPricing Strategies: Firms in a market choose prices to maximize profits, taking into account their competitors’ pricing decisions. The equilibrium is the set of prices where no firm can increase profits by changing its price unilaterally.\nTraffic Flow: Drivers choose routes to minimize travel time, based on their expectations of other drivers’ choices. The equilibrium is the pattern of traffic flow where no driver can reduce their travel time by choosing a different route.\n\n\nExample 4.12 (Marble Game) Here is a subtle marble game where players have to call out (or present) either red or blue with different payoffs according to how things match. Two players \\(A\\) and \\(B\\) have both a red and a blue marble. They present one marble to each other. The payoff table is as follows:\n\nIf both present red, \\(A\\) wins $3.\nIf both present blue, \\(A\\) wins $1.\nIf the colors do not match, \\(B\\) wins $2\n\nThe question is whether it is better to be \\(A\\) or \\(B\\) or does it matter? Moreover, what kind of strategy should you play? A lot depends on how much credit you give your opponent. A lot of empirical research was done on the tit-for-tat strategy, where you cooperate until your opponent defects. Then you match his last response.\nNash equilibrium will also allow us to study the concept of a randomized strategy (ie. picking a choice with a certain probability) which turns out to be optimal in many game theory problems.\nFirst, assume that the players have a \\(\\frac{1}{2}\\) probability of playing Red or Blue. Thus each player has the same expected payoff \\(E(A) = \\$1\\) \\[\\begin{align*}\n    E(A) &= \\frac{1}{4} \\cdot 3 + \\frac{1}{4} \\cdot 1 =1 \\\\\n    E(B) &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 =1\n\\end{align*}\\] We might go one step further and look at the risk (and measured by a standard deviation) and calculate the variances of each player’s payouts. \\[\\begin{align*}\n    Var (A) & = (1-1)^2 \\cdot \\frac{1}{4} +(3-1)^2 \\cdot \\frac{1}{4} + (0-1)^2 \\cdot \\frac{1}{2} = 1.5 \\\\\n    Var(B) & = 1^2 \\cdot \\frac{1}{2} + (2-1)^2 \\cdot \\frac{1}{2} = 1\n\\end{align*}\\] Therefore, under this scenario, if you are risk averse, player \\(B\\) position is favored.\nThe matrix of probabilities with equally likely choices is given by\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( red, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\nNow there is no reason to assume ahead of time that the players will decide to play \\(50/50\\). We will show that there’s a mixed strategy (randomized) that is a Nash equilibrium that is, both players won’t deviate from the strategy. We’ll prove that the following equilibrium happens:\n\n\\(A\\) plays Red with probability 1/2 and blue 1/2\n\\(B\\) plays Red with probability 1/4 and blue 3/4\n\nIn this case the expected payoff to playing Red equals that of playing Blue for each player. We can simply calculate: \\(A\\)’s expected payoff is 3/4 and \\(B\\)’s is $1 \\[\nE(A) = \\frac{1}{8} \\cdot 3 + \\frac{3}{8} \\cdot 1 = \\frac{3}{4}\n\\] Moreover, \\(E(B) =1\\), thus \\(E(B) &gt; E(A)\\). We see that \\(B\\) is the favored position. It is clear that if I know that you are going to play this strategy and vice-versa, neither of us will deviate from this strategy – hence the Nash equilibrium concept.\nNash equilibrium probabilities are: \\(p=P( A \\; red )= 1/2, p_1 = P( B \\; red ) = 1/4\\) with payout matrix\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( red, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\\(P( blue, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( blue, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\nWe have general payoff probabilities: \\(p=P( A \\; red ), p_1 = P( B \\; red )\\)\n\\[\\begin{align*}\n    f_A ( p , p_1 ) =& 3 p p_1 + ( 1 -p ) ( 1 - p_1 ) \\\\\n    f_B ( p , p_1 ) =& 2 \\{ p(1 - p_1) + ( 1 -p ) p_1 \\}\n\\end{align*}\\]\nTo find the equilibrium point \\[\\begin{align*}\n    ( \\partial / \\partial p ) f_A ( p , p_1 ) =& 3 p_1 - ( 1 - p_1 ) = 4 p_1 -1 \\; \\; \\mathrm{so} \\; \\; p_1= 1/4 \\\\\n    ( \\partial / \\partial p_1 ) f_B ( p , p_1 ) =& 2 ( 1 - 2p ) \\; \\; \\mathrm{so} \\; \\; p= 1/2\n\\end{align*}\\]\nMuch research has been directed to repeated games versus the one-shot game and is too large a topic to discuss further.\n\nEquilibrium analysis helps predict the likely outcomes of strategic interactions, even when individuals are acting in their own self-interest. Further, we can use it to understand how markets function and how firms make pricing and production decisions or to design mechanisms (e.g., auctions, voting systems) that incentivize desired behavior and achieve efficient outcomes.\nOne major drawback is that equilibrium analysis relies on assumptions about rationality and common knowledge of preferences and strategies, which may not always hold in real-world situations. Furthermore, some games may have multiple equilibria, making it difficult to predict which one will be reached. The problem of dynamic strategies, when individuals may learn and adjust their strategies as they gain experience, is hard.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#statistical-decisions-and-risk",
    "href": "04-dec.html#statistical-decisions-and-risk",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.5 Statistical Decisions and Risk",
    "text": "4.5 Statistical Decisions and Risk\nThe statistical decision making problem can be posed as follows. A decision maker (you) has to chose from a set of decisions or acts. The consequences of these decisions depend on an unknown state of the world. Let \\(d\\in\\mathcal{D}\\) denote the decision and \\(\\theta\\in\\Theta\\) the state of the world. As an example, think of \\(\\theta\\) as the unknown parameter and the decision as choosing a parameter estimation or hypothesis testing procedure. To provide information about the parameter, the decision maker obtains a sample \\(y\\in\\mathcal{Y}\\) that is generated from the likelihood function \\(p\\left(y|\\theta\\right)\\). The resulting decision depends on the observed data, is denoted as \\(d\\left(  y\\right)\\), and is commonly called the decision rule.\nTo make the decision, the decision maker uses a “loss” function as a quantitative metric to assesses the consequences or performance of different decisions. For each state of the world \\(\\theta\\), and decision \\(d\\), \\(\\mathcal{L}\\left(  \\theta,d\\right)\\) quantifies the “loss” made by choosing \\(d\\) when the state of the world is \\(\\theta.\\) Common loss functions include a quadratic loss, \\(\\mathcal{L}(\\theta,d)=(\\theta-d)^{2},\\) an absolute loss, \\(\\mathcal{L}(\\theta,d)=|\\theta-d|\\), and a \\(0-1\\) loss, \\[\n\\mathcal{L}(\\theta,d)=L_{0}1_{\\left[  \\theta\\in\\Theta_{0}\\right]  }+L_{1}1_{\\left[  \\theta\\in\\Theta_{1}\\right]  }.\n\\] For Bayesians, the utility function provides a natural loss function. Historically, decision theory was developed by classical statisticians, thus the development in terms of “objective” loss functions instead of “subjective” utility.\nClassical decision theory takes a frequentist approach, treating parameters as “fixed but unknown” and evaluating decisions based on their population properties. Intuitively, this thought experiment entails drawing a dataset \\(y\\) of given length and applying the same decision rule in a large number of repeated trials and averaging the resulting loss across those hypothetical samples. Formally, the classical risk function is defined as \\[\nR(\\theta,d)=\\int_{\\mathcal{Y}}\\mathcal{L}\\left[  \\theta,d(y)\\right]  p(y|\\theta )dy=\\mathbb{E}\\left[  \\mathcal{L}\\left[  \\theta,d(y)\\right]  |\\theta\\right]  .\n\\] Since the risk function integrates over the data, it does not depend on a given observed sample and is therefore an ex-ante or a-priori metric. In the case of quadratic loss, the risk function is the mean-squared error (MSE) and is \\[\\begin{align*}\nR(\\theta,d)  &  =\\int_{\\mathcal{Y}}\\left[  \\theta-d\\left(  y\\right)  \\right]\n^{2}p(y|\\theta)dy\\\\\n&  =\\mathbb{E}\\left[  \\left(  d\\left(  y\\right)  -E\\left[  d\\left(  y\\right)\n|\\theta\\right]  \\right)  ^{2}|\\theta\\right]  +\\mathbb{E}\\left[  \\left(\nE\\left[  d\\left(  y\\right)  |\\theta\\right]  -\\theta\\right)  ^{2}|\\theta\\right]\n\\\\\n&  =Var\\left(  d\\left(  y\\right)  |\\theta\\right)  +\\left[  bias\\left(\nd\\left(  y\\right)  -\\theta\\right)  \\right]  ^{2}%\n\\end{align*}\\] which can be interpreted as the bias of the decision/estimator plus the variance of the decision/estimator. Common frequentist estimators choose unbiased estimators so that the bias term is zero, which in most settings leads to unique estimators.\nThe goal of the decision maker is to minimize risk. Unfortunately, rarely is there a decision that minimizes risk uniformly for all parameter values. To see this, consider a simple example of \\(y\\sim N\\left(  \\theta,1\\right)\\), a quadratic loss, and two decision rules, \\(d_{1}\\left(  y\\right)  =0\\) or \\(d_{2}\\left(  y\\right)  =y\\). Then, \\(R\\left(  \\theta,d_{1}\\right)  =\\theta^{2}\\) and \\(R\\left(  \\theta,d_{2}\\right)  =1\\). If \\(\\left\\vert \\theta\\right\\vert &lt;1\\), then \\(R\\left(  \\theta,d_{1}\\right)  &lt;R\\left(  \\theta,d_{2}\\right)\\), with the ordering reversed for \\(\\left\\vert \\theta\\right\\vert &gt;1\\). Thus, neither rule uniformly dominates the other.\nOne way to deal with the lack of uniform domination is to use the minimax principle: first maximize risk as function of \\(\\theta\\), \\[\n\\theta^{\\ast}=\\underset{\\theta\\in\\Theta}{\\arg\\max}R(\\theta,d)\\text{,}%\n\\] and then minimize the resulting risk by choosing a decision:\n\\[\nd_{m}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\left[  R(\\theta^{\\ast },d)\\right]  \\text{.}%\n\\] The resulting decision is known as a minimax decision rule. The motivation for minimax is game theory, with the idea that the statistician chooses the best decision rule against the other player, mother nature, who chooses the worst parameter.\nThe Bayesian approach treats parameters as random and specifies both a likelihood and prior distribution, denoted here by \\(\\pi\\left(  \\theta\\right)\\). The Bayesian decision maker recognizes that both the data and parameters are random, and accounts for both sources of uncertainty when calculating risk. The Bayes risk is defined as\n\\[\\begin{align*}\nr(\\pi,d)  &  =\\int_{\\mathcal{\\Theta}}\\int_{\\mathcal{Y}}\\mathcal{L}\\left[  \\theta ,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  dyd\\theta\\\\\n&  =\\int_{\\mathcal{\\Theta}}R(\\theta,d)\\pi\\left(  \\theta\\right)  d\\theta =\\mathbb{E}_{\\pi}\\left[  R(\\theta,d)\\right]  ,\n\\end{align*}\\] and thus the Bayes risk is an average of the classical risk, with the expectation taken under the prior distribution. The Bayes decision rule minimizes expected risk:\n\\[\nd_{\\pi}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\text{ }r(\\pi,d)\\text{.}%\n\\] The classical risk of a Bayes decision rule is defined as \\(R\\left(\n\\theta,d_{\\pi}^{\\ast}\\right)\\), where \\(d_{\\pi}^{\\ast}\\) does not depend on \\(\\theta\\) or \\(y\\). Minimizing expected risk is consistent with maximizing posterior expected utility or, in this case, minimizing expected loss. Expected posterior risk is \\[\nr(\\pi,d)=\\int_{\\mathcal{Y}}\\left[  \\int_{\\mathcal{\\Theta}}\\mathcal{L}\\left[\n\\theta,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  d\\theta\\right]  dy,\n\\] where the term in the brackets is posterior expected loss. Minimizing posterior expected loss for every \\(y\\in\\mathcal{Y},\\) is clearly equivalent to minimizing posterior expected risk, provided it is possibility to interchange the order of integration.\nThe previous definitions did not explicitly state that the prior distribution was proper, that is, that \\(\\int_{\\mathcal{\\Theta}}\\pi\\left(  \\theta\\right)d\\theta=1\\). In some applications and for some parameters, researchers may use priors that do not integrate, \\(\\int_{\\Theta}\\pi\\left(  \\theta\\right)d\\theta=\\infty\\), commonly called improper priors. A generalized Bayes rule is one that minimizes \\(r(\\pi,d),\\) where \\(\\pi\\) is not necessarily a distribution, if such a rule exists. If \\(r(\\pi,d)&lt;\\infty\\), then the mechanics of this rule is clear, although its meaning is less clear.\n\n\n\n\nKelly, J. L. 1956. “A New Interpretation of Information Rate.” Bell System Technical Journal 35 (4): 917–26.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "05-ab.html",
    "href": "05-ab.html",
    "title": "5  A/B Testing",
    "section": "",
    "text": "5.1 Confidence Intervals\nThe Internet age opened the door to enormous data collection from images to videos as well as personalized information. The data collected are observational rather than data collected from designed experiments—where we can control the environment to find the effects of interventions.\nAny competitive business strives to improve its efficiency. Requirements to improve can be driven by changing market conditions, customer behavior, and their demands. We would like to test our ideas on how to improve things and pick the best course of action. AB testing provides a statistical framework for addressing such issues. It is the underpinning for testing new ideas and measuring the effects of strategy A versus strategy B. It is widely used for testing marketing campaigns, product designs, or even the effects of new drugs. It relies on a statistical procedure sometimes known as hypothesis testing, hence AB testing allows us to draw conclusions from controlled designed experiments. We would also like to use these methods on observational studies and thus bring up questions of causation. Instrumental variables provide ways of trying to tease out main effects. Propensity scores and matching are also popular techniques. We are often interested in whether the observed effect is due to noise or a true effect. We can use hypothesis testing to answer this question in a rigorous way. Think of a coin tossing experiment. If you tossed a coin twice and it came up heads both times, does it mean that the coin is biased? Common sense tells us that two tosses are not enough to make a definitive conclusion and we should toss this coin a few more times to gain confidence. Hypothesis testing is just that—the procedure that tells us if we have enough evidence to make a conclusion or if more data is required. It uses probability distributions to quantify uncertainty about experiment outcomes. Let’s do a more practical example.\nThere is a whole field on bandit problems—how to optimally sequentially allocate our resources as a trade-off between exploitation (gaining more information in an environment you understand to solely gain efficiency) or exploration (learning about new environments which might be less optimal than the current one).\nYou work as a quant for a trading firm and you have developed a new algorithm to trade stocks. You tested your algorithm on historical data and it outperformed the state-of-the-art algorithm used in your company. Now, the important question is whether your trading strategy can truly outperform the market or it just got lucky. We need to analyze the performance of the algorithm after it was created and decide whether we have truly discovered a dominant strategy. Sequential analysis—a natural framework for Bayesian methods—allows us to decide how long we need to wait before we have enough evidence that our algorithm has an edge.\nThe effect we try to measure is usually present in some statistics that we calculate from data, for example, sample mean, proportion, or difference in means.\nCentral Limit Theorem\nCLT states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution. The theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nThe first and simplest case of the CLT was published in 1738 by de Moivre and is called the De Moivre-Laplace theorem. According to this theorem the standard normal distribution arises as the limit of scaled and centered Binomial distributions, in the following sense. Let \\(x_1,\\ldots,x_n\\) be independent, identically distributed Rademacher random variables, that is, independent random variables with distribution \\[\nP(X_i = 1) = P(X_i = -1) = \\frac{1}{2}.\n\\] Then, the distribution of the sum of these random variables converges to the standard normal distribution as \\(n\\) tends to infinity. That is, for any \\(a&lt;b\\), we have \\[\n\\lim_{n\\to\\infty} P\\left(a \\le \\frac{X_1+\\cdots+X_n}{\\sqrt{n}} \\le b\\right) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx.\n\\] In this case, the sum \\(X_1+\\cdots+X_n\\) has mean \\(n\\mu\\) and variance \\(n\\sigma^2\\), so that the standardized sum \\((X_1+\\cdots+X_n - n\\mu)/\\sqrt{n\\sigma^2}\\) has mean \\(0\\) and variance \\(1\\). The theorem then states that the distribution of this standardized sum converges to the standard normal distribution as \\(n\\) tends to infinity.\nIn 1889 Francis Galton published a paper where he described what we now call the Galton Board. The Galton Board is a vertical board with interleaved rows of pins. Balls are dropped from the top, and bounce left and right as they hit the pins. Eventually, they are collected into one of several bins at the bottom. The distribution of balls in the bins approximates the normal distribution. Each pin is a physical realization of the binomial draw and each row is a summand. The location at the bottom is a sum of the binomial draws. The galton-ball.r script simulates the Galton board experiment. The script is available in the R folder of the book repository. Figure 5.1 shows the result of the simulation. The distribution of the balls in the bins approximates the normal distribution.\nThe fact that the distribution of the simulated means from the Pyx example can be described well by a normal bell curve, in fact has a theoretical justification. It is called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution.\nThere are a few conditions. The sampled observations must be independent. In practice, this means that the sampling should be random, and one observation should not influence another. Further, the sample size should be sufficiently large. While there is no strict rule for what constitutes ‘large enough,’ a common guideline is a sample size of 30 or more. However, if the population distribution is far from normal, a larger sample size may be required.\nWe can estimate the mean of this bell curve using \\(\\bar x\\) and the standard deviation (standard error) using \\(s/\\sqrt{n}\\).\nThe square-root nature of this relation is somewhat unfortunate. To double your certainty about the population mean, you need to quadruple the sample size.\nOne of the main applications of this result is the construction of confidence intervals. A confidence interval is a range of values that is likely to contain the true value of the population mean. It is a plausible range for the quantity we are trying to estimate. The confidence interval is calculated using the sample mean \\(\\bar x\\) and the standard error \\(s/\\sqrt{n}\\). The confidence interval is centered around the sample mean and has a width of \\(2 \\times s_{\\bar x}\\). The confidence interval is calculated as follows \\[\n\\bar x \\pm 1.96s_{\\bar x}, ~ s_{\\bar x} = \\frac{s}{\\sqrt{n}}.\n\\]\nThe theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nHere are a few conclusions we can make thus far\nComing back to the Patriots coin toss example, we know that they won 19 out of 25 tosses during the 2014-2015 season. In this example, our observations are values 0 (lost toss) and 1 (won toss) and the average over those 0-1 observations is called the proportion and is denoted by \\(\\hat p\\) instead of \\(\\bar x\\). When we deal with proportions, we can calculate the sample variance from its mean \\(\\hat p\\) as follows \\[\ns^2_{\\hat p} = \\frac{\\hat p(1-\\hat p)}{n}.\n\\] Thus, we know that given our observations and CLT, the true value of the probability of winning a toss is normally distributed. Our best guess at the mean \\(\\hat p\\) is \\(19/25 = 0.76\\) and variance \\(s^2 = 0.76(1-0.76)/25 = 0.0073\\) \\[\n\\hat p \\sim N(0.76, 0.0073).\n\\] Then a \\(95\\%\\) Confidence Interval is calculated by\n0.76 + c(-1.96,1.96)*sqrt(0.0073)\n\n 0.59 0.93\nSince 0.5 is outside the confidence interval, we say that we do not have enough evidence to say that the coin tosses were fair.\nThen we formulate a hypothesis that we are to test. Our status-quo assumption (there is no effect) is called the null hypothesis and is typically denoted by \\(H_0\\).\nTo translate the question from this experiment into language of hypothesis testing, we say that our null hypothesis is that proportion of yawning participants in control (\\(\\hat p_c\\)) and experimental group (\\(\\hat p_e\\)) is the same \\(H_0: \\hat p_c - \\hat p_e = 0\\), and the alternative hypothesis is \\(H_a: \\hat p_c &lt; \\hat p_e\\). The goal is to use the data to tell us if the hypothesis is correct or not.\nA key statistical fact behind the hypothesis testing is the Central Limit Theorem. It states that if we have a sample \\(\\{x_1,\\ldots,x_n\\}\\) with \\(n\\) observations from any distribution \\(x_i \\sim p(x)\\), then the average of the sample follows a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) \\[\n\\bar X = \\frac{1}{n}\\sum_{i=1}^{n}X_i \\sim N(\\mu, \\sigma^2/n)\n\\]\nLet us use a simple simulated data set to demonstrate the central limit theorem. We generate 100 outcomes of a Bernoulli trial with \\(p=0.3\\) and calculate the mean of this sample \\(\\hat p\\). We repeat it 2000 times and compare the empirical distribution of \\(\\hat p\\) with \\(N(0.3, 0.046)\\).\nset.seed(1)\na = replicate(2000,mean(rbinom(100,1,0.3)))\nplot(density(a), main=\"\")\nse = sqrt(0.3*(1-0.3)/100)  #  0.046\nx = seq(0,0.5,length.out = 300)\nlines(x,dnorm(x,mean = 0.3,sd = se), col=\"red\", lwd=3)\nThere are three ways to quantify uncertainty in hypothesis testing. The first approach relies on calculating confidence intervals, as we did for the yawn example. There are two complementary approaches. One is to calculate what is called a \\(p\\)-value, that is the probability of getting the result observed in the data, assuming null-hypothesis is true. If \\(p\\)-value is low, then we reject the null-hypothesis. For the yawn example, the conditional probability that the observed difference in proportions is greater than 0.044, given null hypothesis is true is given by\n\\[\np\\text{-value} = P(\\hat p_e - \\hat p_c \\ge 0.044 \\mid H_0),\n\\] which can be calculated using pnorm function\n1 - pnorm(0.044,0,sqrt(0.0177))     \n\n 0.37\nThe \\(p\\)-value of 0.37 means that there is a 37% chance to observe the difference to be greater than 0.044 assuming the null-hypothesis. It is quite high! We want the \\(p\\)-value to be low, only then we can claim that we have discovered a new fact, i.e. that yawning is contagious. In many applications we require this number to be at most 0.005. The smallest acceptable \\(p\\)-value is called the significance level and is typically denoted as \\(\\alpha\\). We can test the hypothesis at different levels of significance \\(\\alpha\\). Further we assume that the statistic we are analyzing follows the sampling distribution. The probability distribution of the statistics values is either Normal, or \\(t\\)-distribution for continuous variable.\nIn a nutshell a hypothesis is a statement about a population developed for the purpose of testing with data. To summarize the process of testing a significance of our discovery for proportions, we perform the hypothesis testing following the 5-step process.\nStep 1: Formulate the Null Hypothesis (\\(H_0\\)), which we assume to be true unless there is sufficient evidence to the contrary. Then, alternative Hypothesis (\\(H_1\\)): test against the null, e.g. \\(H_0: p_e - p_c = 0\\), and \\(H_a: p_e - p_c &gt; 0\\). If there is evidence that \\(H_0\\) is false, we accept \\(H_1\\).\nStep 2: Select the significance level \\(\\alpha\\). While \\(\\alpha = 0.05\\) (the 5% level) is the most commonly used, \\(\\alpha = 0.01\\) (the 1% level) is prevalent in medical and quality assurance examples.\nStep 3: Compute the Test Statistic (\\(Z\\) or \\(T\\))\nStep 4: Formulate the Decision Rule. For example, reject the Null hypothesis if \\(|Z| &gt; 1.96\\)\nStep 5: Make a Decision, Compute the p-value. p-value is the smallest significance level at which a null hypothesis can be rejected. If \\(p\\)-value \\(&lt;\\alpha\\), we have evidence that \\(H_1\\) is true, we accept \\(H_1\\) and claim we have a discovery. If \\(p\\)-value is \\(\\ge \\alpha\\), then we cannot reject the null-hypothesis.\nIn Steps 1-2 we formulate the hypothesis. In steps 3-5 we make a decision.\nIn the context of hypothesis testing, we come back to the type I and type II errors we already discussed. They can be used to describe two types of errors you can make when testing\nAnd the significance level is then \\[\nP(\\mbox{reject } H_0 \\mid H_0 \\; \\mbox{ true}) =\nP(\\mbox{type I error}).\n\\]\nHypothesis testing is often used in scientific reporting. For example, the discovery of Higgs Boson was announced as a result of hypothesis testing. Scientists used the five-sigma concept to test the Higgs-Boson hypothesis. This concept, however, is somewhat counter-intuitive. If the particle doesn’t exist, one in 3.5 million is the chance an experiment just like the one announced would nevertheless come up with a result appearing to confirm it does exist. In other words, one in 3.5 million is the likelihood of finding a false positive—a fluke produced by random statistical fluctuation that seems as definitive as the findings released by two teams of researchers at the CERN laboratory in Geneva. So we can talk about the significance level as \\(p\\)-value to be one-in-3.5-million and then the \\(Z\\)-score is five.\nThe test statistic (\\(T\\) or \\(Z\\)) quantifies uncertainty between the null-hypothesis value and the observed one and is equal to the number of standard deviations they are apart from each other. This value is called the \\(Z\\)-score, and is calculated as \\[\nZ = \\frac{ \\bar{x} - \\mu_0 }{s/\\sqrt{n}},\n\\] where \\(\\mu_0\\) is the mean assumed under null-hypothesis. The square root of the statistic’s variance \\(s/\\sqrt{n}\\) is called standard error and is denoted by \\(se(\\bar X)\\).\nLet’s calculate the \\(Z\\)-score for the yawning example. l-in \\(\\mu_0 = 0\\), \\(\\hat{p} = \\hat p_e - \\hat p_c = 0.044\\), \\(\\Var{\\hat p }=\\Var{\\hat p_e - \\hat p_c} = 0.0177\\), we get \\(Z\\) statistic to be 0.33. Thus, our observed difference is very close to 0.\nTo summarize the duality of confidence interval, \\(p\\)-value and \\(Z\\)-score, the following statements are equivalent:\nLet us proceed with another example.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#confidence-intervals",
    "href": "05-ab.html#confidence-intervals",
    "title": "5  A/B Testing",
    "section": "",
    "text": "Mean estimates are based on random samples and therefore random (uncertain) themselves\nWe need to account for this uncertainty!\nStandard Error measures the uncertainty of an estimate\nUsing properties of the Normal distribution, we can construct \\(95\\)% Confidence Intervals\nThis provides us with a plausible range for the quantity we are trying to estimate.\n\n\n\n\n\nExample 5.5 (Mythbusters) In 2006 the creators of Mythbusters TV show on Discovery channel wanted to test whether yawning is contagious in humans. They recruited 50 participants and each of those went through an interview. At the end of 34 randomly selected interviews the interviewer did yawn. Then participants were asked to wait in a next door room. Out of 34 participants from the experimental group, 10 did yawn (29.4%) and only 4 out of 16 (25%) in the control group did yawn. The difference in the proportion of those who did yawn was 4.4%. The show hosts Kari Byron, Tory Belleci and Scottie Chapman concluded that yawn is indeed contagious.\nThe question is what happens if we are to re-run this experiment several times with different groups of participants, will we see the same difference of 4.4%? The fact is that from one experiment to another calculated proportions of yawners in both groups will be different.\nIn our example, the proportion of yawners in the experimental group is \\(\\hat p_e = 0.294\\) and in the control group is \\(\\hat p_c = 0.25\\). Thus, \\[\n\\hat \\sigma^2_e = 0.294(1-0.294) = 0.208,~~~\\hat \\sigma^2_c = 0.25(1-0.25) = 0.188\n\\]\nWe can apply CLT and calculate the uncertainty about \\(\\hat p_{e}\\) and \\(\\hat p_{c}\\) \\[\n\\hat p_e\\sim N(0.294, 0.208/34),~~~ \\hat p_c\\sim N(0.25, 0.188/16).\n\\] Now, instead of comparing proportions (numbers), we can compare their distributions and thus quantify uncertainties. If we plot density functions of those two Normal variables, we can see that although means are different, there is a large overlap of the two density functions.\n\np = seq(0.0,0.6, length.out = 200)\nplot(p,dnorm(p,0.25, sqrt(0.188/16)), col=2, type='l', lwd=3, ylab=\"Density\", ylim = c(0,5))\nlines(p,dnorm(p,0.294, sqrt(0.208/34)), col=3, lwd=3, ylim = c(0, 6))\nlegend(\"topright\", c(\"control\", \"experiment\"), col=c(3,2), lwd=3, bty='n')\n\n\n\n\n\n\n\n\nThe amount of overlap is the measure of how certain we are that \\(p_e\\) and \\(p_c\\) are different. Large overlap means we are not very certain if proportions are truly different. For example, both \\(p_e\\) and \\(p_c\\) have a high probability of being between 0.2 and 0.4. We can use properties of normal distribution to say specifically what is the amount of this overlap by calculating the corresponding 95% confidence interval of the difference between the proportions. Note that the difference of two Normal random variables is another Normal \\[\n\\hat p_e - \\hat p_c \\sim N(0.294 - 0.25, 0.208/34 + 0.188/16) = N(0.044, 0.0177)\n\\] Now we can calculate 95% confidence interval for \\(\\hat p_e - \\hat p_c\\), again using properties of Normal\n\n0.044 + c(-1.96,1.96)*sqrt(0.0177)\n\n -0.22  0.30\n\n\nThe interval is wide and most importantly, it does contain 0. Thus, we cannot say for sure that the proportions are different. They might just appear to be different due to chance (sampling error). Meaning, that if we are to re-run the experiment we should expect the difference to be anywhere between -0.22 and 0.31 in 95% of the cases.\nThus, statistical analysis does not confirm the conclusion made by the show hosts and indicates that there is no evidence that the proportion of yawners is different between the control and experimental groups.\n\n\nExample 5.6 (Search algorithm) Let’s look at another example and test effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\nGoogle Search Algorithm\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1828\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nThe probability of success is estimated to be \\(\\hat{p}_1 = 1755/2500 = 0.702\\) for the current algorithm and \\(\\hat{p}_2 = 1828/2500 = 0.731\\) for the new algorithm. We can calculate the 95% confidence interval or 95% Bayesian credible region for both estimated proportions.\n\np1 = 1755/2500; p2 = 1828/2500\nAlgo1 = round(p1 + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500), 3)\nAlgo2 = round(p2 + c(-1.96,1.96)*sqrt(p2*(1-p2)/2500), 3)\nkable(rbind(Algo1, Algo2), col.names = c(\"Lower\", \"Upper\"), caption = \"95% Confidence Interval for the Proportions\")\n\n\n95% Confidence Interval for the Proportions\n\n\n\nLower\nUpper\n\n\n\n\nAlgo1\n0.68\n0.72\n\n\nAlgo2\n0.71\n0.75\n\n\n\n\n\nGiven that the intervals do slightly overlap, there is not enough evidence to say that algorithms are different, and the new Algo 2 is not necessarily more efficient.\nWe will get a slightly more precise estimation of uncertainty if we calculate confidence interval for the difference of the proportions. Since \\(p_1\\) and \\(p_2\\) both follow Normal distribution, their difference is also normally distributed \\[\np_1 - p_2 \\sim N(\\hat p_1 - \\hat p_2, s_1^2/n + s_2^2/n).\n\\] Applying this formula for the Google search algorithm experiment, we calculate the 95% confidence interval for the difference\n\ndiff = p1 - p2  + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500 + p2*(1-p2)/2500)\nprint(diff, digits = 5)\n\n -0.0541696 -0.0042304\n\n\nThe confidence interval for the difference does not contain 0, and thus we can say that we are confident that algorithms are different!\nMore generally, if the number of observations in two groups are different, say \\(n_1\\) and \\(n_2\\) then the \\[\ns_{ \\bar{X}_1 - \\bar{X}_2 } = \\sqrt{ \\frac{ s^2_{ \\bar{X}_1 }}{n_1} + \\frac{ s^2_{ \\bar{X}_2 }}{n_2} }\n\\] or for proportions, we compute \\[\ns_{ \\hat{p}_1 - \\hat{p}_2 } = \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)}{n_2} }.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType I Error: Rejecting a true \\(H_0\\).\nType II Error: Not rejecting a false \\(H_0\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatement\nCondition\n\n\n\n\n0 is inside the 95% confidence interval\n\\(p\\)-value is greater than 0.05\n\n\n\\(p\\)-value is greater than 0.05\n\\(Z\\)-statistic is less than 1.96\n\n\n\\(Z\\)-statistic is less than 1.96\n0 is inside the 95% confidence interval\n\n\n\n\n\nExample 5.7 (Coke vs Pepsi) The most famous hypothesis test in history in whether people can decide the difference between Coke and Pepsi. We run a double blind experiment, neither the experimenter or subject know the allocation. Pepsi claimed that more than half of Diet Coke drinkers said they preferred to drink Diet Pepsi. That is our null hypothesis. The data comes from a random sample of \\(100\\) drinkers. We find that \\(56\\) favor Pepsi.\nThis is a hypothesis test about the proportion of drinkers who prefer Pepsi \\[H_0 : p = \\frac{1}{2} \\; \\; \\mathrm{ and} \\; \\; H_1 : p &gt; \\frac{1}{2}\\] Let’s estimate our statistics form data: \\[\\hat{p} = X/n = 56/100 = 0.56\\]\nThis is my best estimate of the true \\(p\\). The standard error of my statistic \\[se(\\hat{p}) =  \\sqrt{\\hat{p}(1-\\hat{p})/n} = 0.0496 .\\]\nThe \\(95\\)% is then \\[0.56 \\pm 1.96 (0.0496) = 0.56 \\pm 0.098 = ( 0.463, 0.657 )\\] \\(p=0.5\\) lies inside the confidence interval. Pepsi was lying!\nThe \\(Z\\)-score now with \\(s_{ \\hat{p} }= \\sqrt{ p_0(1-p_0)/n} = 0.05\\) \\[Z = \\frac{ \\hat{p} - p_0 }{ s_{\\hat{p} }} = \\frac{ 0.56-0.5}{0.05} = 1.2 &lt; 1.64\\] Let’s take the usual \\(\\alpha = 0.05\\). Don’t reject \\(H_0\\) for a one-sided test at \\(5\\)% level. We need a larger \\(n\\) to come to a more definitive conclusion. We might come to a different conclusion with a larger sample size. One of the downsides of hypothesis testing is that it generates a yes/no answer without having any uncertainty associated with it.\n\nprop.test(56,100,alternative='greater', conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  56 out of 100, null probability 0.5\nX-squared = 1, df = 1, p-value = 0.1\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.47 1.00\nsample estimates:\n   p \n0.56 \n\n\n\n\nExample 5.8 (Avonex) Now we consider a few more examples of Hypothesis testing. We consider the dispute about Biogen’s Avonex. Biogen made the following assertion:\n“Avonex delivers the highest rate of satisfaction: 95% among patients” In response to that statement, the U.S. Food and Drug Administration (FDA) on October 30th, 2002 informed the biotech company Biogen to stop publishing misleading promotions for its multiple sclerosis drug Avonex. To clarify the issue, FDA did run an experiment. The FDA found that in a random sample of \\(75\\) patients surveyed, only 60% said they were satisfied with Avonex. The question is: Who is right?\nLet’s use hypothesis testing to get an answer. Following our five-step process to set up a Hypothesis Test:\n\nFormulate the Null Hypothesis: \\(H_0 : p = 0.95 = p_0\\). The alternative hypothesis: \\(H_1 : p &lt; 0.95\\). A 1-sided alternative.\nWe’ll use a small significance level, 1%.\nThe appropriate test statistic is \\[Z = \\frac{ \\hat{p} - p_0 }{ \\sqrt{ \\frac{ p_0 ( 1 - p_0 ) }{ n} } }\\] where \\(\\hat{p} = 0.60 , p_0 = 0.95\\) and \\(n=75\\).\nCompute the Test Statistic and make a decision \\(Z = \\frac{ 0.6 - 0.95 }{ \\sqrt{ \\frac{ 0.95 \\times 0.05  }{ 75} } } = - 14\\).\n\nTo complete our analysis, we need to determine the critical region and calculate the \\(p\\)-value for this test. For a one-sided test at the 1% significance level, the critical region is \\(Z &lt; -2.32\\). Since our observed test statistic of \\(Z = -14\\) falls well within this rejection region, we reject the null hypothesis.\nThe \\(p\\)-value of the test is \\(P(Z &lt; -14) = 0.0000\\), which is essentially zero. This extremely small \\(p\\)-value provides overwhelming statistical evidence that the FDA is correct and Biogen’s claim is false.\nThis procedure can is implemented in R in the prop.test function.\n\nprop.test(45,75,0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  45 out of 75, null probability 0.95\nX-squared = 186, df = 1, p-value &lt;0.0000000000000002\nalternative hypothesis: true p is not equal to 0.95\n95 percent confidence interval:\n 0.48 0.71\nsample estimates:\n  p \n0.6 \n\n\nThe p-value is less than \\(2.2 \\times 10^{-16}\\), which is essentially zero. This extremely small \\(p\\)-value provides overwhelming statistical evidence that the FDA is correct and Biogen’s claim is false.\n\n\nExample 5.9 (Pfizer) We consider another example that involves pharmaceutical company Pfizer. Pfizer introduced Viagra in early 1998. During \\(1998\\) of the \\(6\\) million Viagra users \\(77\\) died from coronary problems such as heart attacks. Pfizer claimed that this rate is no more than the general population.A clinical study found \\(11\\) out of \\(1,500,000\\) men who were not on Viagra died of coronary problems during the same length of time as the\\(77\\) Viagra users who died in \\(1998\\).The question is,Let’s calculate the significance Interval. A 95% confidence interval fora difference in proportions \\(p_1 - p_2\\) is \\[\n( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96\n\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  +\n    \\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\n\\]\nWe can do a confidence interval or a \\(Z\\)-score test.\nWith Viagra, \\(\\hat{p}_1 = 77/6000000 = 0.00001283\\) and without Viagra \\(\\hat{p}_2 = 11/1500000 = 0.00000733\\).\nWe need to test whether these are equal.\nWith a \\(95\\)% confidence interval for \\((p_1 - p_2)\\) you get an interval \\[\n( 0.00000549 , 0.0000055).\n\\]\nSince this confidence interval does not contain zero, we have evidence that the proportion of deaths from coronary problems is significantly higher among Viagra users compared to the general population. Despite the small proportions involved, the measurement is highly accurate due to the large sample sizes in both groups.\nFor hypothesis testing, we would use a one-sided test with a significance level of \\(\\alpha = 0.01\\) to test whether the proportion of deaths is higher in the Viagra group. The difference in proportions can be analyzed as follows:\n\nprop.test(x=c(11,77), n=c(1500000,6000000), alternative='greater',conf.level=.95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(11, 77) out of c(1500000, 6000000)\nX-squared = 3, df = 1, p-value = 0.9\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.00001  1.00000\nsample estimates:\n   prop 1    prop 2 \n0.0000073 0.0000128 \n\n\nThe p-value for the Null is \\(1-0.948 =0.052\\). This p-value is greater than 0.01, so we cannot reject the null hypothesis.\n\n\nExample 5.10 (Lord Rayleigh’s Argon Discovery) Lord Rayleigh won the Nobel Prize for discovery of Argon. This discovery occurred when he noticed a small discrepancy between two sets of measurements on nitrogen gas that he had extracted from the air and one he had made in the lab.\nFirst, he removed all oxygen from a sample of air. He measured the density of the remaining gas in a fixed volume at constant temperature and pressure.\nSecond, he prepared the same volume of pure nitrogen by the chemical decomposition of nitrous oxide (\\(N_2 O\\)) and nitric oxide \\(NO\\).\nHere’s the results\n\nair = c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, NA)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\nd = data.frame(\"Air\"=air,\"Chemical Decomposition\"=decomp)\nknitr::kable(d, booktabs = TRUE,caption = 'Lord Rayleigh Argon Discovery')\n\n\nLord Rayleigh Argon Discovery\n\n\nAir\nChemical.Decomposition\n\n\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\nNA\n2.3\n\n\n\n\n\n\nboxplot(d, col = c(\"red\", \"green\"))\n\n\n\n\nLord Rayleigh’s results\n\n\n\n\n\n\n\n\n\n\nmean\nsd\n\n\n\n\nAir\n2.3\n0.00014\n\n\nDecomposition\n2.3\n0.00138\n\n\n\n\nt.test(air,decomp,var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  air and decomp\nt = 20, df = 13, p-value = 0.00000000003\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.0095 0.0118\nsample estimates:\nmean of x mean of y \n      2.3       2.3 \n\n\nThe Z-score is 20. It is a 20-sigma event and we’ve found Argon!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#multiple-testing",
    "href": "05-ab.html#multiple-testing",
    "title": "5  A/B Testing",
    "section": "5.2 Multiple Testing",
    "text": "5.2 Multiple Testing\nConsider this simple multiple-testing scenario. If we want to test 1000 hypotheses and we test each hypothesis one-by-one, say the ground truth is that only 10% (100) of those hypotheses are true. Using \\(\\alpha=0.05\\) rule, we assume that out of 900 false hypotheses \\(0.05 \\cdot 900 = 45\\) will show up as positive (false positives). Now we run our one-by-one hypothesis tests and our procedure correctly identified 80 out of 100 true positives and incorrectly identified 45 false positives and 20 false negatives. Now, among 125 hypotheses identified as positives 45 in fact are not! Another way to look at it is to calculate the probability of at least one false positive \\(P(\\mbox{at least one false positive}) = 1 - (1-0.05)^{1000} = 1\\). We are almost guaranteed to see at least one false positive.\n\nplot(1:100,1 - (1-0.05)^{1:100}, type='l', ylab=\"False Positive Rate\", xlab=\"Number of Tests\", col=\"red\", lwd=2)\n\n\n\n\nProbability of At Least 1 False Positive\n\n\n\n\nOne way to deal with the problem is to lower the cut-off to \\(\\alpha/n\\). This approach is called the Bonferroni correction. For the case of 1000 hypotheses we set \\(\\alpha = 0.00005\\). However this conservative approach will lead to many false negatives. The probability of identifying at least one significant result is then \\(1 - (1-0.00005)^{1000} = 0.049\\)\n\nClassification of results for a testing procedure. T/F = True/False, D/N = Discovery/Non-discovery. We observe \\(m\\), \\(D\\) and \\(N\\).\n\n\n\n\\(H_0\\) Accepted\n\\(H_0\\) Rejected\nTotal\n\n\n\n\n\\(H_0\\) True\nTN\nFD\n\\(T_0\\)\n\n\n\\(H_0\\) False\nFN\nTD\n\\(T_1\\)\n\n\nTotal\n\\(N\\)\n\\(D\\)\n\\(m\\)\n\n\n\nA more practical approach is to use the False Discovery Rate \\[\n\\text{FDR} = \\E{\\frac{FD}{D}}\n\\] which is the proportion of false positives among all significant results. We aim to set a cutoff so that FDR \\(&lt; Q\\). The FDR approach allows us to increase the power while maintaining some principled bound on error.\nBenjamini and Hochberg developed a procedure based on FDR to perform multiple testing. Under their procedure, we put individual \\(p\\)-values in order from smallest to largest. Then we choose the largest \\(p_k\\) value that is smaller than \\((k/m)/Q\\) where \\(Q\\) is the false discovery rate you choose. Then all hypotheses with index \\(i&lt;k\\) are significant. Benjamini and Hochberg showed that under this procedure the FDR \\(&lt;Q\\).\nAs an example, García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. They found the following results:\n\nd = read.csv(\"../data/cancer-diet.csv\")\nknitr::kable(d, booktabs = TRUE,caption = 'Dietary Risk Factors of Cancer')\n\n\nDietary Risk Factors of Cancer\n\n\nLabel\np.value\nRank\nBH\n\n\n\n\nTotal calories\n0.00\n1\n0.01\n\n\nOlive oil\n0.01\n2\n0.02\n\n\nWhole milk\n0.04\n3\n0.03\n\n\nWhite meat\n0.04\n4\n0.04\n\n\nProteins\n0.04\n5\n0.05\n\n\nNuts\n0.06\n6\n0.06\n\n\nCereals and pasta\n0.07\n7\n0.07\n\n\nWhite fish\n0.20\n8\n0.08\n\n\nButter\n0.21\n9\n0.09\n\n\nVegetables\n0.22\n10\n0.10\n\n\nSkimmed milk\n0.22\n11\n0.11\n\n\nRed meat\n0.25\n12\n0.12\n\n\nFruit\n0.27\n13\n0.13\n\n\nEggs\n0.28\n14\n0.14\n\n\nBlue fish\n0.34\n15\n0.15\n\n\nLegumes\n0.34\n16\n0.16\n\n\nCarbohydrates\n0.38\n17\n0.17\n\n\nPotatoes\n0.57\n18\n0.18\n\n\nBread\n0.59\n19\n0.19\n\n\nFats\n0.70\n20\n0.20\n\n\nSweets\n0.76\n21\n0.21\n\n\nDairy products\n0.94\n22\n0.22\n\n\nSemi-skimmed milk\n0.94\n23\n0.23\n\n\nTotal meat\n0.98\n24\n0.24\n\n\nProcessed meat\n0.99\n25\n0.25\n\n\n\n\n\nIf we choose \\(Q = 0.25\\), then \\(k=5\\) (Proteins) is our cut-off rank. Thus we accept \\(H_0\\) for the first five tests. Note that traditional hypothesis testing procedure only controls for Type 1 error and FDR-based procedure controls for both error types.\n\n\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope, Pilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al. 2014. “Calorie Intake, Olive Oil Consumption and Mammographic Density Among Spanish Women.” International Journal of Cancer 134 (8): 1916–25.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html",
    "href": "06-hyp.html",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "6.1 Likelihood Principle\nThe hypothesis testing problem is as follows. Based on a sample of data, \\(y\\), generated from \\(p\\left( y \\mid \\theta\\right)\\) for \\(\\theta\\in\\Theta\\), the goal is to determine if \\(\\theta\\) lies in \\(\\Theta_{0}\\) or in \\(\\Theta_{1}\\), two disjoint subsets of \\(\\Theta\\). In general, the hypothesis testing problem involves an action: accepting or rejecting a hypothesis. The problem is described in terms of a null, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), which are defined as \\[\nH_{0}:\\theta\\in\\Theta_{0}\\;\\;\\mathrm{and}\\;\\;H_{1}%\n:\\theta\\in\\Theta_{1}\\text{.}%\n\\]\nDifferent types of regions generate different types of hypothesis tests. If the null hypothesis assumes that \\(\\Theta_{0}\\) is a single point, \\(\\Theta _{0}=\\theta_{0}\\), this is known as a simple or “sharp” null hypothesis. If the region consists of multiple points, the hypothesis is called composite; this occurs when the space is unconstrained or corresponds to an interval of the real line. In the case of a single parameter, typical one-sided tests are of the form \\(H_{0}:\\theta&lt;\\theta_{0}\\) and \\(H_{1}:\\theta&gt;\\theta_{0}\\).\nThere are two correct decisions and two possible types of errors. The correct decisions are accepting a null or an alternative that is true, whereas a Type I error incorrectly rejects a true null and a Type II error incorrectly accepts a false null.\nFormally, the probabilities of Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) errors are defined as: \\[\n\\alpha=P \\left[  \\text{reject }H_{0} \\mid H_{0}\\text{\nis true }\\right]  \\text{ and }\\beta=P \\left[  \\text{accept\n}H_{0} \\mid H_{1}\\text{ is true }\\right]  \\text{.}%\n\\]\nIt is useful to think of the decision to accept or reject as a decision rule, \\(d\\left( y\\right)\\). In many cases, the decision rules form a critical region \\(R\\), such that \\(d\\left( y\\right) =d_{1}\\) if \\(y\\in R\\). These regions often take the form of simple inequalities. Next, defining the decision to accept the null as \\(d\\left( y\\right) =d_{0}\\), and the decision to accept the alternative as \\(d_{1},\\) the error types are \\[\\begin{align*}\n\\alpha_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{1} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{0}\\text{ }(H_{0}\\text{ is true})\\\\\n\\beta_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{0} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{1}\\text{ }(H_{1}\\text{ is true})\\text{.}%\n\\end{align*}\\] where both types of errors explicitly depend on the decision and the true parameter value. Notice that both of these quantities are determined by the population properties of the data. In the case of a composite null hypothesis, the size of the test (the probability of making a type I error) is defined as \\[\n\\alpha = \\underset{\\theta\\in\\Theta_{0}}{\\sup}~\\alpha_{\\theta}\\left( d\\right)\n\\] and the power is defined as \\(1-\\beta_{\\theta}\\left( d\\right)\\). It is always possible to set either \\(\\alpha_{\\theta}\\left( d\\right)\\) or \\(\\beta_{\\theta }\\left( d\\right)\\) equal to zero, by finding a test that always rejects the alternative or null, respectively.\nThe total probability of making an error is \\(\\alpha_{\\theta}\\left(d\\right) +\\beta_{\\theta}\\left(d\\right)\\), and ideally one would seek to minimize the total error probability, absent additional information. The optimal action \\(d^*\\) minimizes the posterior expected loss; \\(d^* = d_0 = 0\\) if the posterior probability of hypothesis \\(H_0\\) exceeds 1/2, and \\(d^* = d_1=1\\) otherwise \\[\nd^* = 1\\left(  P \\left(  \\theta \\in \\Theta_0 \\mid y\\right) &lt; P \\left(  \\theta \\in \\Theta_1 \\mid y\\right)\\right)  = 1\\left(P \\left(  \\theta \\in \\Theta_0 \\mid y\\right)&lt;1/2\\right).\n\\] Simply speaking, the hypothesis with higher posterior probability is selected.\nThe easiest way to reduce the error probability is to gather more data, as the additional evidence should lead to more accurate decisions. In some cases, it is easy to characterize optimal tests, those that minimize the sum of the errors. Simple hypothesis tests of the form \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta=\\theta_{1}\\), are one such case admitting optimal tests. Defining \\(d^{\\ast}\\) as a test accepting \\(H_{0}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &gt;a_{1}f\\left( y \\mid \\theta_{1}\\right)\\) and \\(H_{1}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &lt;a_{1}f\\left( y \\mid \\theta _{1}\\right)\\), for some \\(a_{0}\\) and \\(a_{1}\\). Either \\(H_{0}\\) or \\(H_{1}\\) can be accepted if \\(a_{0}f\\left(y \\mid \\theta_{0}\\right) =a_{1}f\\left( y \\mid \\theta_{1}\\right)\\). Then, for any other test \\(d\\), it is not hard to show that \\[\na_{0}\\alpha\\left(  d^{\\ast}\\right)  +a_{1}\\beta\\left(  d^{\\ast}\\right)  \\leq\na_{0}\\alpha\\left(  d\\right)  +a_{1}\\beta\\left(  d\\right),\n\\] where \\(\\alpha_{d}=\\alpha_{d}\\left( \\theta\\right)\\) and \\(\\beta_{d}=\\beta_{d}\\left( \\theta\\right)\\). This result highlights the optimality of tests defining rejection regions in terms of the likelihood ratio statistic, \\(f\\left( y \\mid \\theta_{0}\\right)/f\\left( y \\mid \\theta_{1}\\right)\\). It turns out that the results are in fact stronger. In terms of decision theoretic properties, tests that define rejection regions based on likelihood ratios are not only admissible decisions, but form a minimal complete class, the strongest property possible.\nA decision rule \\(d\\) is admissible if there is no other decision rule \\(d'\\) that is uniformly better than \\(d\\), meaning \\(R(\\theta, d') \\leq R(\\theta, d)\\) for all \\(\\theta \\in \\Theta\\), with strict inequality for at least one \\(\\theta\\). In the context of hypothesis testing, this means that an admissible test cannot be improved upon simultaneously for both Type I and Type II error rates.\nA class \\(\\mathcal{C}\\) of decision rules is called complete if for every decision rule \\(d\\) not in \\(\\mathcal{C}\\), there exists a rule \\(d' \\in \\mathcal{C}\\) such that \\(d'\\) is better than or equivalent to \\(d\\). A complete class is minimal if no proper subset of it is also complete. The minimal complete class represents the smallest collection of decision rules that contains all the “best” procedures.\nFor simple hypothesis testing problems of the form \\(H_0: \\theta = \\theta_0\\) versus \\(H_1: \\theta = \\theta_1\\), the likelihood ratio tests form a minimal complete class. This remarkable result means that any reasonable test can be expressed as a likelihood ratio test for some choice of critical value. Furthermore, every likelihood ratio test is admissible, providing strong theoretical justification for their use.\nThe practical implication is profound: when facing a simple versus simple hypothesis testing problem, one need only consider tests of the form “reject \\(H_0\\) if \\(\\frac{f(y|\\theta_1)}{f(y|\\theta_0)} &gt; k\\)” for some threshold \\(k \\geq 0\\). Any other test is either equivalent to such a likelihood ratio test or is inadmissible (meaning there exists a likelihood ratio test that performs better).\nOne of the main problems in hypothesis testing is that there is often a tradeoff between the two goals of reducing type I and type II errors: decreasing \\(\\alpha\\) leads to an increase in \\(\\beta\\), and vice-versa. Because of this, it is common to fix \\(\\alpha_{\\theta}\\left( d\\right)\\), or \\(\\sup~\\alpha_{\\theta}\\left( d\\right)\\), and then find a test to minimize \\(\\beta_{d}\\left( \\theta\\right)\\). This leads to “most powerful” tests. There is an important result from decision theory: test procedures that use the same size level of \\(\\alpha\\) in problems with different sample sizes are inadmissible. This is commonly done where significance is indicated by a fixed size, say 5%. The implications of this will be clearer below in examples.\nGiven observed data \\(y\\) and likelihood function \\(l(\\theta) = p(y\\mid \\theta)\\), the likelihood principle states that all relevant experimental information is contained in the likelihood function for the observed \\(y\\). Furthermore, two likelihood functions contain the same information about \\(\\theta\\) if they are proportional to each other. For example, the widely used maximum-likelihood estimation does satisfy the likelihood principle. However, this principle is sometimes violated by non-Bayesian hypothesis testing procedures. The likelihood principle is a fundamental principle in statistical inference, and it is a key reason why Bayesian procedures are often preferred.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#likelihood-principle",
    "href": "06-hyp.html#likelihood-principle",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "Example 6.1 (Testing fairness) Suppose we are interested in testing \\(\\theta\\), the unknown probability of heads for a possibly biased coin. Suppose, \\[\nH_0 :~\\theta=1/2 \\quad\\text{v.s.} \\quad  H_1 :~\\theta&gt;1/2.\n\\] An experiment is conducted and 9 heads and 3 tails are observed. This information is not sufficient to fully specify the model \\(p(y\\mid \\theta)\\). There are two approaches.\nScenario 1: Number of flips, \\(n = 12\\) is predetermined. Then number of heads \\(Y \\mid \\theta\\) is binomial \\(B(n, \\theta)\\), with probability mass function \\[\np(y\\mid \\theta)= {n \\choose y} \\theta^{y}(1-\\theta)^{n-y} = 220 \\cdot \\theta^9(1-\\theta)^3\n\\] For a frequentist, the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{12} {12 \\choose y} (1/2)^y(1-1/2)^{12-y} = (1+12+66+220)/2^{12} =0.073,\n\\] and if you recall the classical testing, \\(H_0\\) is not rejected at level \\(\\alpha = 0.05\\).\nScenario 2: The number of tails (successes) \\(\\alpha = 3\\) is predetermined; that is, flipping continues until 3 tails are observed. Then, \\(Y\\), the number of heads (failures) observed until 3 tails appear, follows a Negative Binomial distribution \\(NB(3, 1- \\theta)\\), \\[\np(y\\mid \\theta)= {\\alpha+y-1 \\choose \\alpha-1} \\theta^{y}(1-\\theta)^{\\alpha} = {3+9-1 \\choose 3-1} \\theta^9(1-\\theta)^3 = 55\\cdot \\theta^9(1-\\theta)^3.\n\\] For a frequentist, large values of \\(Y\\) are critical and the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{\\infty} {3+y-1 \\choose 2} (1/2)^{y}(1/2)^{3} = 0.0327.\n\\] We used the following identity here \\[\n\\sum_{x=k}^{\\infty} {2+x \\choose 2}\\dfrac{1}{2^x} = \\dfrac{8+5k+k^2}{2^k}.\n\\] The hypothesis \\(H_0\\) is rejected, and this change in decision is not caused by observations.\nAccording to the Likelihood Principle, all relevant information is in the likelihood \\(l(\\theta) \\propto \\theta^9(1 - \\theta)^3\\), and Bayesians could not agree more!\nEdwards, Lindman, and Savage (1963, 193) note: The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#the-bayesian-approach",
    "href": "06-hyp.html#the-bayesian-approach",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.2 The Bayesian Approach",
    "text": "6.2 The Bayesian Approach\nFormally, the Bayesian approach to hypothesis testing is a special case of the model comparison results to be discussed later. The Bayesian approach just computes the posterior distribution of each hypothesis. By Bayes \\[\nP \\left(  H_{i} \\mid y\\right)  =\\frac{p\\left(  y \\mid H_{i}\\right)  P \\left(  H_{i}\\right)  }{p\\left(  y\\right)}    , ~\\text{for} ~ i=0,1\n\\] where \\(P \\left( H_{i}\\right)\\) is the prior probability of \\(H_{i}\\), \\[\np\\left( y \\mid H_{i}\\right) =\\int_{\\theta \\in \\Theta_i} p\\left( y \\mid \\theta\\right) p\\left( \\theta \\mid H_{i}\\right) d\\theta\n\\] is the marginal likelihood under \\(H_{i}\\), \\(p\\left( \\theta \\mid H_{i}\\right)\\) is the parameter prior under \\(H_{i}\\), and \\[\np\\left(  y\\right)  = \\sum_{i=0,1} p\\left(  y \\mid H_{i}\\right)  P \\left( H_{i}\\right).\n\\]\nIf the hypotheses are mutually exclusive, \\(P \\left( H_{0}\\right) =1-P \\left( H_{1}\\right)\\).\nThe posterior odds of the null to the alternative is \\[\n\\text{Odds}_{0,1}=\\frac{P \\left(  H_{0} \\mid y\\right)  }{P %\n\\left(  H_{1} \\mid y\\right)  }=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{p\\left(  y \\mid H_{1}\\right)  }\\frac{P \\left(  H_{0}\\right)  }{P \\left(  H_{1}\\right)  }\\text{.}%\n\\]\nThe odds ratio updates the prior odds, \\(P \\left( H_{0}\\right) /P \\left( H_{1}\\right)\\), using the Bayes Factor, \\[\n\\mathrm{BF}_{0,1}=\\dfrac{p\\left(y \\mid H_{0}\\right)}{p\\left( y \\mid H_{1}\\right)}.\n\\] With exhaustive competing hypotheses, \\(P \\left( H_{0} \\mid y\\right)\\) simplifies to \\[\nP \\left(  H_{0} \\mid y\\right)  =\\left(  1+\\left(  \\mathrm{BF}_{0,1}\\right)  ^{-1}\\frac{\\left(  1-P \\left(  H_{0}\\right)\n\\right)  }{P \\left(  H_{0}\\right)  }\\right)  ^{-1}\\text{,}%\n\\] and with equal prior probability, \\(P\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathrm{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Both Bayes factors and posterior probabilities can be used for comparing hypotheses. Jeffreys (1961) advocated using Bayes factors, and provided a scale for measuring the strength of evidence that was given earlier. Bayes factors merely indicate that the null hypothesis is more likely if \\(\\mathrm{BF}_{0,1}&gt;1\\), \\(p\\left( y \\mid H_{0}\\right) &gt;p\\left( y \\mid H_{1}\\right)\\). The Bayesian approach merely compares density ordinates of \\(p\\left( y \\mid H_{0}\\right)\\) and \\(p\\left( y \\mid H_{1}\\right)\\), which mechanically involves plugging in the observed data into the functional form of the marginal likelihood.\nFor a point null, \\(H_{0}:\\theta=\\theta_{0}\\), the parameter prior is \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{\\theta_{0}}\\left( \\theta\\right)\\) (a Dirac mass at \\(\\theta_{0}\\)), which implies that \\[\np\\left( y \\mid H_{0}\\right) =\\int p\\left( y \\mid \\theta_{0}\\right) p\\left( \\theta \\mid H_{0}\\right) d\\theta=p\\left( y \\mid \\theta_{0}\\right).\n\\] With a general alternative, \\(H_{1}:\\theta\\neq\\theta_{0}\\), the probability of the null is \\[\nP \\left(  \\theta=\\theta_{0} \\mid y\\right)  =\\frac{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  }{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  +\\left(  1-P\\left( H_{0}\\right)  \\right)  \\int_{\\Theta}p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta},\n\\] where \\(p\\left( \\theta \\mid H_{1}\\right)\\) is the parameter prior under the alternative. This formula will be used below.\nBayes factors and posterior null probabilities measure the relative weight of evidence of the hypotheses. Traditional hypothesis testing involves an additional decision or action: to accept or reject the null hypothesis. For Bayesians, this typically requires some statement of the utility/loss that codifies the benefits/costs of making a correct or incorrect decision. The simplest situation occurs if one assumes a zero loss of making a correct decision. The loss incurred when accepting the null (alternative) when the alternative is true (false) is \\(L\\left( d_{0} \\mid H_{1}\\right)\\) and \\(L\\left( d_{1} \\mid H_{0}\\right)\\), respectively.\nThe Bayesian will accept or reject based on the posterior expected loss. If the expected loss of accepting the null is less than the alternative, the rational decision maker will accept the null. The posterior loss of accepting the null is \\[\n\\mathbb{E}\\left[  \\mathcal{L}\\mid d_{0},y\\right]  =L\\left(  d_{0} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  ,\n\\] since the loss of making a correct decision, \\(L\\left( d_{0} \\mid H_{0}\\right)\\), is zero. Similarly, \\[\n\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{1},y\\right]  =L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{1} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{1} \\mid H_{0}\\right)  P \\left(  H_{0} \\mid y\\right)  .\n\\] Thus, the null is accepted if \\[\n\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{0},y\\right]  &lt;\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{1},y\\right]\n\\Longleftrightarrow L\\left(  d_{0} \\mid H_{1}\\right)  P \\left( H_{1} \\mid y\\right)  &lt;L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  ,\n\\] which further simplifies to \\[\n\\frac{L\\left(  d_{0} \\mid H_{1}\\right)  }{L\\left(  d_{1} \\mid H_{0}\\right)  }&lt;\\frac{P \\left(  H_{0} \\mid y\\right)  }{P \\left(  H_{1} \\mid y\\right)  }.\n\\] In the case of equal losses, this simplifies to accept the null if \\(P \\left( H_{1} \\mid y\\right) &lt;P \\left( H_{0} \\mid y\\right)\\). One advantage of Bayes procedures is that the resulting estimators and decisions are always admissible.\n\nExample 6.2 (Enigma machine: Code-breaking) Consider an alphabet of \\(26\\) letters. Let \\(x\\) and \\(y\\) be two codes of length \\(T\\). We will look to see how many letters match (\\(M\\)) and don’t match (\\(N\\)) in these sequences. Even though the codes are describing different sentences, when letters are the same, if the same code is being used then the sequence will have a match. To compute the Bayes factor we need the joint probabilities \\[\nP( x,y\\mid  H_0 ) \\; \\; \\mathrm{ and} \\; \\; P( x,y\\mid  H_1 ),\n\\] where under \\(H_0\\) they are different codes, in which case the joint probability is \\(( 1 / A )^{2T}\\). For \\(H_1\\) we first need to know the chance of the same letter matching. If \\(p_t\\) denotes the frequencies of the use of English letters, then we have this match probability \\(m = \\sum_{i} p_i^2\\) which is about \\(2/26\\). Hence for a particular set of letters \\[\nP( x_i , y_i \\mid H_1 ) = \\frac{m}{A} \\; \\mathrm{ if} \\; x_i =y_i \\; \\; \\mathrm{ and} \\; \\;  P( x_i , y_i \\mid H_1 ) = \\frac{1-m}{A(A-1)} \\; \\mathrm{ if} \\; x_i \\neq y_i.\n\\] Hence the log Bayes factor is \\[\\begin{align*}\n\\ln \\frac{P( x,y\\mid  H_1 )}{P( x,y\\mid  H_0 )} & = M \\ln \\frac{ m/A}{1/A^2} +N \\ln \\frac{ ( 1-m ) / A(A-1) }{ 1/ A^2} \\\\\n& = M \\ln mA  + N \\ln \\frac{ ( 1-m )A }{A-1 }\n\\end{align*}\\] The first term comes when you get a match and the increase in the Bayes factor is large, \\(3.1\\) (on a \\(\\log_{10}\\)-scale), otherwise you get a no-match and the Bayes factor decreases by \\(- 0.18\\).\nExample: \\(N=4\\), \\(M=47\\) out of \\(T=51\\), then gives evidence of 2.5 to 1 in favor of \\(H_1\\).\nHow long a sequence do you need to look at? Calculate the expected log odds. Turing and Good figured you needed sequences of about length \\(400\\). Can also look at doubles and triples.\n\n\nExample 6.3 (Signal Transmission) Suppose that the random variable \\(X\\) is transmitted over a noisy communication channel. Assume that the received signal is given by \\[\nY=X+W,\n\\] where \\(W\\sim N(0,\\sigma^2)\\) is independent of \\(X\\). Suppose that \\(X=1\\) with probability \\(p\\), and \\(X=-1\\) with probability \\(1-p\\). The goal is to decide between \\(X=1\\) and \\(X=-1\\) by observing the random variable \\(Y\\). We will assume symmetric loss and will accept the hypothesis with the higher posterior probability. This is also sometimes called the maximum a posteriori (MAP) test.\nWe assume that \\(H_0: ~ X = 1\\), thus \\(Y\\mid H_0 \\sim N(1,\\sigma^2)\\), and \\(Y\\mid H_1 \\sim N(-1,\\sigma^2)\\). The Bayes factor is simply the likelihood ratio \\[\n\\dfrac{p(y\\mid H_0)}{p(y \\mid H_1)} =  \\exp\\left( \\frac{2y}{\\sigma^2}\\right).\n\\] The prior odds are \\(p/(1-p)\\), thus the posterior odds are \\[\n\\exp\\left( \\frac{2y}{\\sigma^2}\\right)\\dfrac{p}{1-p}.\n\\] We choose \\(H_0\\) (true \\(X\\) is 1), if the posterior odds are greater than 1, i.e., \\[\ny &gt; \\frac{\\sigma^2}{2} \\log\\left( \\frac{1-p}{p}\\right) = c.\n\\]\nFurther, we can calculate the error probabilities of our test. \\[\np(d_1\\mid H_0) = P(Y&lt;c\\mid X=1) = \\Phi\\left( \\frac{c-1}{\\sigma}\\right),\n\\] and \\[\np(d_0\\mid H_1) = P(Y&gt;c\\mid X=-1) = 1- \\Phi\\left( \\frac{c+1}{\\sigma}\\right).\n\\] Let’s plot the total error rate as a function of \\(p\\) and assuming \\(\\sigma=0.2\\) \\[\nP_e = p(d_1\\mid H_0) (1-p) + p(d_0\\mid H_1) p\n\\]\n\nsigma &lt;- 0.2\np &lt;- seq(0.01,0.99,0.01)\nc &lt;- sigma^2/2*log((1-p)/p)\nPe &lt;- pnorm((c-1)/sigma)*(1-p) + (1-pnorm((c+1)/sigma))*p\nplot(p,Pe,type=\"l\",xlab=\"p\",ylab=\"Total Error Rate\")\n\n\n\n\n\n\n\n\n\n\nExample 6.4 (Hockey: Hypothesis Testing for Normal Mean) The general manager of Washington Capitals (an NHL hockey team) thinks that their star center player Evgeny Kuznetsov is underperforming and is thinking of trading him to a different team. He uses the number of goals per season as a metric of performance. He knows that historically, a top forward scores on average 30 goals per season with a standard deviation of 5, \\(\\theta \\sim N(30,25)\\). In the 2022-2023 season Kuznetsov scored 12 goals. For the number of goals \\(X\\mid \\theta\\) he uses normal likelihood \\(N(\\theta, 36)\\). Kuznetsov’s performance was not stable over the years, thus the high variance in the likelihood. Thus, the posterior is \\(N(23,15)\\).\n\nsigma2 = 36\nsigma02 = 25\nmu=30\ny=12\nk = sigma02 + sigma2\nmu1 = sigma2/k*mu + sigma02/k*y\nsigma21 = sigma2*sigma02/k\nmu1\n\n 23\n\nsigma21\n\n 15\n\n\nThe manager thinks that Kuznetsov simply had a bad year and his true performance is at least 24 goals per season \\(H_0: \\theta \\geq 24\\), \\(H_1: \\theta&lt;24\\). The posterior probability of the \\(H_0\\) hypothesis is\n\na = 1-pnorm(24,mu1,sqrt(sigma21))\na\n\n 0.36\n\n\nIt is less than 1/2, only 36%. Thus, we should reject the null hypothesis. The posterior odds in favor of the null hypothesis are\n\na/(1-a)\n\n 0.56\n\n\nIf underestimating (and trading) Kuznetsov is two times more costly than overestimating him (fans will be upset and team spirit might be affected), that is \\(L(d_1\\mid H_0) = 2L(d_0\\mid H_1)\\), then we should accept the null when posterior odds are greater than 1/2. This is the case here, 0.55 is greater than 1/2. The posterior odds are in favor of the null hypothesis. Thus, the manager should not trade Kuznetsov.\nKuznetsov was traded to Carolina Hurricanes towards the end of the 2023-2024 season.\nNotice, when we try to evaluate a newcomer to the league, we use the prior probability of \\(\\theta \\geq 24\\):\n\na = 1-pnorm(24,mu,sqrt(sigma02))\nprint(a)\n\n 0.88\n\na/(1-a)\n\n 7.7\n\n\nThus, the prior odds in favor of \\(H_0\\) are 7.7.\n\n\nExample 6.5 (Hypothesis Testing for Normal Mean: Two-Sided Test) In the case of two sided test, we are interested in testing\n\n\\(H_0: \\theta = \\theta_0\\), \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{\\theta_0}\\left( \\theta\\right)\\)\n\\(H_1: \\theta \\neq \\theta_0\\), \\(p\\left( \\theta \\mid H_{1}\\right) = N\\left( \\theta_0,\\sigma^{2}/n_0\\right)\\)\n\nWhere \\(n\\) is the sample size and \\(\\sigma^2\\) is the variance (known) of the population. Observed samples are \\(Y = (y_1, y_2, \\ldots, y_n)\\) with \\[\ny_i \\mid \\theta,\\sigma^2 \\sim N(\\theta, \\sigma^2).\n\\]\nThe Bayes factor can be calculated analytically \\[\nBF_{0,1} = \\frac{p(Y\\mid \\theta = \\theta_0, \\sigma^2 )}\n{\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid \\theta_0, n_0, \\sigma^2)\\, d \\theta}\n\\] \\[\n\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid \\theta_0, n_0, \\sigma^2)\\, d \\theta = \\frac{\\sqrt{n_0}\\exp\\left\\{-\\frac{n_0(\\theta_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}\\right\\}}{\\sqrt{2\\pi}\\sigma^2\\sqrt{\\frac{n_0+n}{\\sigma^2}}}\n\\] \\[\np(Y\\mid \\theta = \\theta_0, \\sigma^2 ) = \\frac{\\exp\\left\\{-\\frac{(\\bar y-\\theta_0)^2}{2 \\sigma ^2}\\right\\}}{\\sqrt{2 \\pi } \\sigma }\n\\] Thus, the Bayes factor is \\[\nBF_{0,1} = \\frac{\\sigma\\sqrt{\\frac{n_0+n}{\\sigma^2}}e^{-\\frac{(\\theta_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}}}{\\sqrt{n_0}}\n\\]\n\\[\nBF_{0,1} =\\left(\\frac{n + n_0}{n_0} \\right)^{1/2} \\exp\\left\\{-\\frac{1}{2} \\frac{n }{n + n_0} Z^2 \\right\\}\n\\]\n\\[\nZ =  \\frac{(\\bar{Y} - \\theta_0)}{\\sigma/\\sqrt{n}}\n\\]\nOne way to interpret the scaling factor \\(n_0\\) is to look at the standard effect size \\[\n\\delta = \\frac{\\theta - \\theta_0}{\\sigma}.\n\\] The prior of the standard effect size is \\[\n\\delta \\mid H_1 \\sim N(0, 1/n_0).\n\\] This allows us to think about a standardized effect independent of the units of the problem.\nLet’s consider now example of Argon discovery.\n\nair =    c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, 2.31028)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\n\nOur null hypothesis is that the mean of the difference equals to zero. We assume that measurements made in the lab have normal errors, this the normal likelihood. We empirically calculate the standard deviation of our likelihood. The Bayes factor is\n\ny = air - decomp\nn = length(y); m0 = 0\nsigma = sqrt(var(air) + var(decomp))\nn0 = 1\nZ = (mean(y) - m0)/(sigma/sqrt(n))\nBF = sqrt((n + n0)/n0)*exp(-0.5*n/(n + n0)*Z^2)\nBF\n\n 1.9e-91\n\n\nWe have extremely strong evidence in favor \\(H_1: \\theta \\ne 0\\) hypothesis. The posterior probability of the alternative hypothesis is numerically 1!\n\na = 1/(1+BF)\na\n\n 1\n\n\n\n\nExample 6.6 (Hypothesis Testing for Proportions) Let’s look at again at the effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1818\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nHere we assume binomial likelihood and use conjugate beta prior, for mathematical convenience. We are putting independent beta priors on the click-through rates of the two algorithms, \\(p_1\\sim Beta(\\alpha_1,\\beta_1)\\) and \\(p_2\\sim Beta(\\alpha_2,\\beta_2)\\). The posterior for \\(p_1\\) and \\(p_2\\) are independent Beta distributions \\[\np(p_1, p_1 \\mid y) \\propto p_1^{\\alpha_1 + 1755 - 1} (1-p_1)^{\\beta_1 + 745 - 1}\\times p_2^{\\alpha_2 + 1818 - 1} (1-p_2)^{\\beta_2 + 682 - 1}.\n\\]\nThe easiest way to explore this posterior is via Monte Carlo simulation of the posterior.\n\nset.seed(92) #Kuzy\ny1 &lt;- 1755; n1 &lt;- 2500; alpha1 &lt;- 1; beta1 &lt;- 1\ny2 &lt;- 1818; n2 &lt;- 2500; alpha2 &lt;- 1; beta2 &lt;- 1\nm = 10000\np1 &lt;- rbeta(m, y1 + alpha1, n1 - y1 + beta1)\np2 &lt;- rbeta(m, y2 + alpha2, n2 - y2 + beta2)\nrd &lt;- p2 - p1\nplot(density(rd), main=\"Posterior Difference in Click-Through Rates\", \n    xlab=\"p2 - p1\", ylab=\"Density\")\nq = quantile(rd, c(.05, .95))\nprint(q)\n\n    5%    95% \n0.0037 0.0465 \n\nabline(v=q,col=\"red\")",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#interval-estimation-credible-sets",
    "href": "06-hyp.html#interval-estimation-credible-sets",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.3 Interval Estimation: Credible Sets",
    "text": "6.3 Interval Estimation: Credible Sets\nThe interval estimators of model parameters are called credible sets. If we use the posterior measure to assess the credibility, the credible set is a set of parameter values that are consistent with the data and gives us is a natural way to measure the uncertainty of the parameter estimate.\nThose who are familiar with the concept of classical confidence intervals (CI’s) often make an error by stating that the probability that the CI interval \\([L, U ]\\) contains parameter \\(\\theta\\) is \\(1 - \\alpha\\). The right statement seems convoluted, one needs to generate data from such model many times and for each data set to exhibit the CI. Now, the proportion of CI’s covering the unknown parameter is “tends to” \\(1 - \\alpha\\). Bayesian interpretation of a credible set \\(C\\) is natural: The probability of a parameter belonging to the set \\(C\\) is \\(1 - \\alpha\\). A formal definition follows. Assume the set \\(C\\) is a subset of domain of the parameter \\(\\Theta\\). Then, \\(C\\) is credible set with credibility \\((1 - \\alpha)\\cdot 100\\%\\) if \\[\np(\\theta \\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] If the posterior is discrete, then the integral becomes sum (counting measure) and \\[\np(\\theta \\in C \\mid y) = \\sum_{\\theta_i\\in C}p(\\theta_i\\mid y) \\ge 1 - \\alpha.\n\\] This is the definition of a \\((1 - \\alpha)100\\%\\) credible set, and of course for a given posterior function such set is not unique.\nFor a given credibility level \\((1 - \\alpha)100\\%\\), the shortest credible set is of interest. To minimize size the sets should correspond to highest posterior probability (density) areas. Thus the acronym HPD.\n\nDefinition 6.1 (Highest Posterior Density (HPD) Credible Set) The \\((1 - \\alpha)100\\%\\) HPD credible set for parameter \\(\\theta\\) is a set \\(C \\subset \\Theta\\) of the form \\[\nC = \\{ \\theta \\in \\Theta : p(\\theta \\mid y) \\ge k(\\alpha) \\},\n\\] where \\(k(\\alpha)\\) is the largest value such that \\[\nP(\\theta\\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] Geometrically, if the posterior density is cut by a horizontal line at the height \\(k(\\alpha)\\), the set \\(C\\) is the projection on the \\(\\theta\\) axis of the region where the posterior density lies above the line.\n\n\n\n\n\n\n\n\n\n\n\nLemma 6.1 The HPD set \\(C\\) minimizes the size among all sets \\(D \\subset \\Theta\\) for which \\[\nP(\\theta \\in D) = 1 - \\alpha.\n\\]\n\n\nProof. The proof is essentially a special case of Neyman-Pearson lemma. If \\(I_C(\\theta) = 1(\\theta \\in C)\\) and \\(I_D(\\theta) = 1(\\theta \\in D)\\), then the key observation is \\[\n\\left(p(\\theta\\mid y) - k(\\alpha)\\right)(I_C(\\theta) - I_D(\\theta)) \\ge 0.\n\\] Indeed, for \\(\\theta\\)’s in \\(C\\cap D\\) and \\((C\\cup D)^c\\), the factor \\(I_C(\\theta)-I_D(\\theta) = 0\\). If \\(\\theta \\in C\\cap D^c\\), then \\(I_C(\\theta)-I_D(\\theta) = 1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\ge 0\\). If, on the other hand, \\(\\theta \\in D\\cap C^c\\), then \\(I_C(\\theta)-I_D(\\theta) = -1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\le 0\\). Thus, \\[\n\\int_{\\Theta}(p(\\theta\\mid y) - k(\\alpha))(I_C(\\theta) - I_D(\\theta))d\\theta \\ge 0.\n\\] The statement of the theorem now follows from the chain of inequalities, \\[\n\\int_{C}(p(\\theta\\mid y) - k(\\alpha))d\\theta \\ge \\int_{D}(p(\\theta\\mid y) - k(\\alpha))d\\theta\n\\] \\[\n(1-\\alpha) - k(\\alpha)\\text{size}(C) \\ge (1-\\alpha) - k(\\alpha)\\text{size}(D)\n\\] \\[\nsize(C) \\le size(D).\n\\] The size of a set is simply its total length if the parameter space \\(\\theta\\) is one dimensional, total area, if \\(\\theta\\) is two dimensional, and so on.\n\nNote, when the distribution \\(p(\\theta \\mid y)\\) is unimodal and symmetric using quantiles of the posterior distribution is a good way to obtain the HPD set.\nAn equal-tailed interval (also called a central interval) of confidence level\n\\[\nI_{\\alpha} = [q_{\\alpha/2}, q_{1-\\alpha/2}],\n\\] here \\(q\\)’s are the quantiles of the posterior distribution. This is an interval on whose both right and left side lies \\((1-\\alpha/2)100\\%\\) of the probability mass of the posterior distribution; hence the name equal-tailed interval.\nUsually, when a credible interval is mentioned without specifying which type of the credible interval it is, an equal-tailed interval is meant.\nHowever, unless the posterior distribution is unimodal and symmetric, there are point outsed of the equal-tailed credible interval having a higher posterior density than some points of the interval. If we want to choose the credible interval so that this not happen, we can do it by using the highest posterior density criterion for choosing it.\n\nExample 6.7 (Cauchy.) Assume that the observed samples\n\ny = c(2,-7,4,-6)\n\ncome from Cauchy distribution. The likelihood is \\[\np(y\\mid \\theta, \\gamma) = \\frac{1}{\\pi\\gamma} \\prod_{i=1}^{4} \\frac{1}{1+\\left(\\dfrac{y_i-\\theta}{\\gamma}\\right)^2}.\n\\] We assume unknown location parameter \\(\\theta\\) and scale parameter \\(\\gamma=1\\). For the flat prior \\(\\pi(\\theta) = 1\\), the posterior is proportional to the likelihood.\n\nlhood = function(theta) 1/prod(1+(y-theta)^2)\ntheta &lt;- seq(-10,10,0.1)\npost &lt;- sapply(theta,lhood)\npost = 10*post/sum(post)\nplot(theta,post,type=\"l\",xlab=expression(theta),ylab=\"Posterior Density\")\nabline(h=c(0.008475, 0.0159, 0.1, 0.2),col=\"red\")\n\n\n\n\n\n\n\n\nThe four horizontal lines correspond to four credible sets\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(C\\)\n\\(P(\\theta \\in C \\mid y)\\)\n\n\n\n\n0.008475\n[-8.498, 5.077]\n99%\n\n\n0.0159\n[-8.189, -3.022] \\(\\cup\\) [-0.615, 4.755]\n95%\n\n\n0.1\n[-7.328, -5.124] \\(\\cup\\) [1.591, 3.120]\n64.2%\n\n\n0.2\n[-6.893, -5.667]\n31.2%\n\n\n\nNotice that for \\(k = 0.0159\\) and \\(k = 0.1\\) the credible set is not a compact. This shows that two separate intervals “clash” for the ownership of \\(\\theta\\) and this is a useful information. This non-compactness can also point out that the prior is not agreeing with the data. There is no frequentist counterpart for the CI for \\(\\theta\\) in the above model.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#alternative-approaches",
    "href": "06-hyp.html#alternative-approaches",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.4 Alternative Approaches",
    "text": "6.4 Alternative Approaches\nThe two main alternatives to the Bayesian approach are significance testing using \\(p-\\)values, developed by Ronald Fisher, and the Neyman-Pearson approach.\n\nSignificance testing using p-values\nFisher’s approach posits a test statistic, \\(T\\left( y\\right)\\), based on the observed data. In Fisher’s mind, if the value of the statistic was highly unlikely to have occured under \\(H_{0}\\), then the \\(H_{0}\\) should be rejected. Formally, the \\(p-\\)value is defined as \\[\np=P \\left[  T\\left(  Y\\right)  &gt;T\\left(  y\\right)   \\mid H_{0}\\right]  ,\n\\] where \\(y\\) is the observed sample and \\(Y=\\left( Y_{1}, \\ldots ,Y_{T}\\right)\\) is a random sample generated from model \\(p\\left( Y \\mid H_{0}\\right)\\), that is, the null distribution of the test-statistic in repeated samples. Thus, the \\(p-\\)value is the probability that a data set would generate a more extreme statistic under the null hypothesis, and not the probability of the null, conditional on the data.\nThe testing procedure is simple. Fisher (1946, p. 80) argues that: If P (the p-value) is between* \\(0.1\\) and \\(0.9\\), there is certainly no reason to suspect the hypothesis tested. If it is below \\(0.02\\), it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not be astray if we draw a line at 0.05 and consider that higher values of \\(\\mathcal{X}^{2}\\) indicate a real discrepancy. Defining \\(\\alpha\\) as the significance level, the tests rejects \\(H_{0}\\) if \\(p&lt;\\alpha\\). Fisher advocated a fixed significance level of \\(5\\%\\), based largely that \\(5\\%\\) is roughly the tail area of a mean zero normal distribution more than two standard deviations from \\(0\\), indicating a statistically significant departure. In practice, testing with \\(p-\\)values involves identifying a critical value, \\(t_{\\alpha}\\), and rejecting the null if the observed statistic \\(t\\left( y\\right)\\) is more extreme than \\(t_{\\alpha}\\). For example, for a significance test of the sample mean, \\(t\\left( y\\right) =\\left( \\overline{y}-\\theta_{0}\\right) /se\\left( \\overline{y}\\right)\\), where \\(se\\left( \\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\); the \\(5\\%\\) critical value is 1.96; and Fisher would reject the null if \\(t\\left( y\\right) &gt;t_{\\alpha}\\).\nFisher interpreted the \\(p-value\\) as the weight or measure of evidence of the null hypothesis. The alternative hypothesis is noticeable in its absence in Fisher’s approach. Fisher largely rejected the consideration of alternatives, believing that researchers should weigh the evidence or draw conclusions about the observed data rather than making decisions such as accepting or rejecting hypotheses based on it.\nThere are a number of issues with Fisher’s approach. The first and most obvious criticism is that it is possible to reject the null, when the alternative hypothesis is less likely. This is an inherent problem in using population tail probabilities–essentially rare events. Just because a rare event has occurred does not mean the null is incorrect, unless there is a more likely alternative. This situation often arises in court cases, where a rare event like a murder has occurred. Decisions based on p-values generates a problem called prosecutor’s Fallacy, which is discussed below. Second, Fisher’s approach relies on population properties (the distribution of the statistic under the null) that would only be revealed in repeated samples or asymptotically. Thus, the testing procedure relies on data that is not yet seen, a violation of what is known as the likelihood principle. As noted by Jeffreys’ (1939, pp. 315-316): “What the use of P implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable data that have not occurred. This seems a remarkable procedure” \nThird, Fisher is agnostic regarding the source of the test statistics, providing no discussion of how the researcher decides to focus on one test statistic over another. In some simple models, the distribution of properly scaled sufficient statistics provides natural test statistics (e.g., the \\(t-\\)test). In more complicated models, Fisher is silent on the sources. In many cases, there are numerous test statistics (e.g., testing for normality), and test choice is clearly subjective. For example, in GMM tests, the choice of test moments is clearly a subjective choice. Finally, from a practical perspective, \\(p-\\)values have a serious deficiency: tests using \\(p\\)-values often appear to give the wrong answer, in the sense that they provide a highly misleading impression of the weight of evidence in many samples. A number of examples of this will be given below, but in all cases, Fisher’s approach tends to over-reject the null hypotheses.\n\n\nNeyman-Pearson\nThe motivation for the Neyman-Pearson (NP) approach was W.S. Gosset, the famous `Student’ who invented the \\(t-\\)test. In analyzing a hypothesis, Student argued that a hypothesis is not rejected unless an alternative is available that provides a more plausible explanation of the data, in which case. Mathematically, this suggests analyzing the likelihood ratio, \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{,}%\n\\] and rejecting the null in favor of the alternative when the likelihood ratio is small enough, \\(\\mathcal{LR}_{0,1}&lt;k\\). This procedures conforms in spirit with the Bayesian approach.\nThe main problem was one of finding a value of the cut off parameter \\(k.\\) From the discussion above, by varying \\(k\\), one varies the probabilities of type one and type two errors in the testing procedure. Originally, NP argued this tradeoff should be subjectively specified: “how the balance (between the type I and II errors) should be struck must be left to the investigator” (Neyman and Pearson (1933a, p. 296) and “we attempt to adjust the balance between the risks \\(P_{1}\\)\\(P_{2}\\) to meet the type of problem before us” (1933b, p. 497). This approach, however, was not “objective *, and they then advocated fixing \\(\\alpha\\), the probability of a type I error, in order to determine \\(k\\). This led to their famous lemma:\n\nLemma 6.2 (Neyman-Pearson Lemma) Consider the simple hypothesis test of \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta =\\theta_{1}\\) and suppose that the null is rejected if \\(\\mathcal{LR}_{0,1}&lt;k_{\\alpha}\\), where \\(k_{\\alpha}\\) is chosen to fix the probability of a type I error at \\(\\alpha:\\)% \\[\n\\alpha=P \\left[  y:\\mathcal{LR}_{0,1}&lt;k_{\\alpha} \\mid H_{0}\\right]  \\text{.}%\n\\] Then, this test is the most powerful test of size \\(\\alpha\\) in the sense that any other test with greater power, must have a higher size.\n\nIn the case of composite hypothesis tests, parameter estimation is required under the alternative, which can be done via maximum likelihood, leading to the likelihood ratio \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{\\underset\n{\\theta\\in\\Theta}{\\sup}p\\left(  y \\mid \\Theta\\right)  }=\\frac{p\\left( y \\mid H_{0}\\right)  }{p\\left(  y \\mid \\widehat{\\theta}\\right)  }\\text{,}%\n\\] where \\(\\widehat{\\theta}\\) is the MLE. Because of this, \\(0\\leq\\mathcal{LR}_{0,1}\\leq 1\\) for composite hypotheses. In multi-parameter cases, finding the distribution of the likelihood ratio is more difficult, requiring asymptotic approximations to calibrate \\(k_{\\alpha}.\\)\nAt first glance, the NP approach appears similar to the Bayesian approach, as it takes into account the likelihood ratio. However, like the \\(p-\\)value, the NP approach has a critical flaw. Neyman and Pearson fix the Type I error, and then minimizes the type II error. In many practical cases, \\(\\alpha\\) is set at \\(5\\%\\) and the resulting \\(\\beta\\) is often very small, close to 0. Why is this a reasonable procedure? Given the previous discussion, this is essentially a very strong prior over the relative benefits/costs of different types of errors. While these assumptions may be warranted in certain settings, it is difficult to a priori understand why this procedure would generically make sense. The next section highlights how the \\(p-\\)value and NP approaches can generate counterintuitive and even absurd results in standard settings.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#examples-and-paradoxes",
    "href": "06-hyp.html#examples-and-paradoxes",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.5 Examples and Paradoxes",
    "text": "6.5 Examples and Paradoxes\nThis section provides a number of paradoxes arising when using different hypothesis testing procedures. The common strands of the examples will be discussed at the end of the section.\n\nExample 6.8 (Neyman-Pearson tests) Consider testing \\(H_{0}:\\mu=\\mu_{0}\\) versus \\(H_{1}:\\mu=\\mu_{1}\\), \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,\\sigma^{2}\\right)\\) and \\(\\mu_{1}&gt;\\mu_{0}\\). For this simple test, the likelihood ratio is given by \\[\n\\mathcal{LR}_{0,1}=\\frac{\\exp\\left(  -\\frac{1}{2\\sigma^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{0}\\right)  ^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2\\sigma\n^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{1}\\right)  ^{2}\\right)  }=\\exp\\left(  -\\frac{T}{\\sigma^{2}%\n}\\left(  \\mu_{1}-\\mu_{0}\\right)  \\left(  \\overline{y}-\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)  \\right)  \\right)  \\text{.}%\n\\] Since \\(\\mathrm{BF}_{0,1}=\\mathcal{LR}_{0,1}\\), assuming equal prior probabilities and symmetric losses, the Bayesian accepts \\(H_{0}\\) if \\(\\mathrm{BF}_{0,1}&gt;1\\). Thus, the Bayes procedure rejects \\(H_{0}\\) if \\(\\overline{y}&gt;\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)\\) for any \\(T\\) and \\(\\sigma^{2}\\), with \\(\\mu_{0}\\),\\(\\mu_{1}\\), \\(T,\\)and \\(\\sigma^{2}\\) determining the strength of the rejction. If \\(\\mathrm{BF}_{0,1}=1\\), there is equal evidence for the two hypotheses.\nwedewk dejewoi m\nThe NP procedure proceeds by first setting \\(\\alpha=0.05,\\) and rejects when \\(\\mathcal{LR}_{0,1}\\) is large. This is equivalent to rejecting when \\(\\overline{y}\\) is large, generating an `optimal’ rejection region of the form \\(\\overline{y}&gt;c\\). The cutoff value \\(c\\) is calibrated via the size of the test, \\[\nP \\left[  reject\\text{ }H_{0} \\mid H_{0}\\right]\n=P \\left[  \\overline{y}&gt;c \\mid \\mu_{0}\\right]  =P \\left[\n\\frac{\\left(  \\overline{y}-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}}&gt;\\frac{\\left( c-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}} \\mid H_{0}\\right] .\n\\] The size equals \\(\\alpha\\) if \\(\\sqrt{T}\\left( c-\\mu_{0}\\right) /\\sigma =z_{\\alpha}\\). Thus, the NP test rejects if then if \\(\\overline{y}&gt;\\mu _{0}+\\sigma z_{\\alpha}/\\sqrt{T}\\). Notice that the test rejects regardless of the value of \\(\\mu_{1}\\), which is rather odd, since \\(\\mu_{1}\\) does not enter into the size of the test only the power. The probability of a type II error is \\[\n\\beta=P \\left[  \\text{accept }H_{0} \\mid H_{1}\\right]\n=P \\left[  \\overline{y}\\leq\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}}z_{\\alpha\n} \\mid H_{1}\\right]  =\\int_{-\\infty}^{\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}%\n}z_{\\alpha}}p\\left(  \\overline{y} \\mid \\mu_{1}\\right)  d\\overline{y}\\text{,}%\n\\] where \\(p\\left( \\overline{y} \\mid \\mu_{1}\\right) \\sim\\mathcal{N}\\left( \\mu _{1},\\sigma^{2}/T\\right)\\).\nThese tests can generate strikingly different conclusions. Consider a test of \\(H_{0}:\\mu=0\\) versus \\(H_{1}:\\mu=5\\), based on \\(T=100\\) observations drawn from \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,10^{2}\\right)\\) with \\(\\overline{y}=2\\). For NP, since \\(\\sigma/\\sqrt{T}=1\\), \\(\\overline{y}\\) is two standard errors away from \\(0\\), thus \\(H_{0}\\) is rejected at the 5% level (the same conclusion holds for \\(p-\\)values). Since \\(p(\\overline {y}=2 \\mid H_{0})=0.054\\) and \\(p(\\overline{y}=2 \\mid H_{1})=0.0044\\), the Bayes factor is \\(\\mathrm{BF}_{0,1}=12.18\\) and \\(P \\left( H_{0} \\mid y\\right) =92.41\\%\\). Thus, the Bayesian is quite sure the null is true, while Neyman-Pearson reject the null.\nThe paradox can be seen in two different ways. First, although \\(\\overline{y}\\) is actually closer to \\(\\mu_{0}\\) than \\(\\mu_{1}\\), the NP test rejects \\(H_{0}\\). This is counterintuitive and makes little sense. The problem is one of calibration. The classical approach develops a test such that 5% of the time, a correct null would be rejected. The power of the test is easy to compute and implies that \\(\\beta=0.0012\\). Thus, this testing procedure will virtually never accept the null if the alternative is correct. For Bayesian procedure, assuming the prior odds is \\(1\\) and \\(L_{0}=L_{1}\\), then \\(\\alpha=\\beta=0.0062\\). Notice that the overall probability of making an error is 1.24% in the Bayesian procedure compared to 5.12% in the classical procedure. It should seem clear that the Bayesian approach is more reasonably, absent a specific motivation for inflating \\(\\alpha\\). Second, suppose the null and alternative were reversed, testing \\(H_{0}:\\mu=\\mu_{1}\\) versus \\(H_{1}:\\mu=\\mu_{0}\\) In the previous example, the Bayes approach gives the same answer, while NP once again rejects the null hypothesis! Again, this result is counterintuitive and nonsensical, but is common when arbitrarily fixing \\(\\alpha\\), which essentially hardwires the test to over-reject the null.\n\n\nExample 6.9 (Lindley’s paradox) Consider the case of testing whether or not a coin is fair, based on observed coin flips, \\[\nH_{0}:\\theta=\\frac{1}{2}\\text{ versus }H_{1}:\\theta\n\\neq\\frac{1}{2}\\text{,}%\n\\] based on \\(T\\) observations from \\(y_{t}\\sim Ber\\left( \\theta\\right)\\). As an example, Table 6.1 provides 4 datasets of differing lengths. Prior to considering the formal hypothesis tests, form your own opinion on the strength of evidence regarding the hypothesis in each data set. It is common for individuals, when confronted with this data to conclude that the fourth sample provides the strongest of evidence for the null and the first sample the weakest.\n\n\n\nTable 6.1: Lindley’s paradox\n\n\n\n\n\n\n#1\n#2\n#3\n#4\n\n\n\n\n# Flips\n50\n100\n400\n10,000\n\n\n# Heads\n32\n60\n220\n5098\n\n\nPercentage of heads\n64\n60\n55\n50.98\n\n\n\n\n\n\nFisher’s solution to the problem posits an unbiased estimator, the sample mean, and computes the \\(t-\\)statistic, which is calculated under \\(H_{0}\\): \\[\nt\\left(  y\\right)  =\\frac{\\overline{y}-E\\left[  \\overline{y} \\mid \\theta\n_{0}\\right]  }{se\\left(  \\overline{y}\\right)  }=\\sqrt{T}\\left(  2\\widehat\n{\\theta}-1\\right)  \\text{,}%\n\\] where \\(se\\left(\\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\). The Bayesian solution requires marginal likelihood under the null and alternative, which are \\[\np\\left(  y \\mid \\theta_{0}=1/2\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\n_{0}\\right)  =\\left(  \\frac{1}{2}\\right)  ^{\\sum_{t=1}^{T}y_{t}}\\left( \\frac{1}{2}\\right)  ^{T-\\sum_{t=1}^{T}y_{t}}=\\left(  \\frac{1}{2}\\right)  ^{T},\n\\tag{6.1}\\] and, from Equation 6.1, \\(p\\left( y \\mid H_{1}\\right) =B\\left( \\alpha_{T},\\beta_{T}\\right) /B\\left(\\alpha,\\beta\\right)\\) assuming a beta prior distribution.\nTo compare the results, note first that in the datasets given above, \\(\\widehat{\\theta}\\) and \\(T\\) generate \\(t_{\\alpha}=1.96\\) in each case. Thus, for a significance level of \\(\\alpha=5\\%\\), the null is rejected for each sample size. Assuming a flat prior distribution, the Bayes factors are \\[\n\\mathrm{BF}_{0,1}=\\left\\{\n\\begin{array}\n[c]{l}%\n0.8178\\text{ for }N=50\\text{ }\\\\\n1.0952\\text{ for }N=100\\\\\n2.1673\\text{ for }N=400\\\\\n11.689\\text{ for }N=10000\n\\end{array}\n\\right.  ,\n\\] showing increasingly strong evidence in favor of \\(H_{0}\\). Assuming equal prior weight for the hypotheses, the posterior probabilities are 0.45, 0.523, 0.684, and 0.921, respectively. For the smallest samples, the Bayes factor implies roughly equal odds of the null and alternative. As the sample size increase, the weight of evidence favors the null, with a 92% probability for \\(N=10K\\).\nNext, consider testing \\(H_{0}:\\theta_{0}=0\\) vs. \\(H_{1}:\\theta_{0}\\neq0,\\) based on \\(T\\) observations from \\(y_{t}\\sim \\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), where \\(\\sigma^{2}\\) is known. This is the formal example used by Lindley to generate his paradox. Using \\(p-\\)values, the hypothesis is rejected if the \\(t-\\)statistic is greater than \\(t_{\\alpha}\\). To generate the paradox, consider datasets that are exactly \\(t_{\\alpha}\\) standard errors away from \\(\\overline{y}\\), that is, \\(\\overline {y}^{\\ast}=\\theta_{0}+\\sigma t_{\\alpha}/\\sqrt{n}\\), and a uniform prior over the interval \\(\\left( \\theta_{0}-I/2,\\theta_{0}+I/2\\right)\\). If \\(p_{0}\\) is the probability of the null, then, \\[\\begin{align*}\nP \\left(  \\theta=\\theta_{0} \\mid \\overline{y}^{\\ast}\\right)   &\n=\\frac{\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\n_{0}\\right)  ^{2}}{\\sigma^{2}}\\right)  p_{0}}{\\exp\\left(  -\\frac{1}{2}%\n\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta_{0}\\right)  ^{2}}{\\sigma^{2}%\n}\\right)  p_{0}+\\left(  1-p_{0}\\right)  \\int_{\\theta_{0}-I/2}^{\\theta_{0}%\n+I/2}\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\\right)\n^{2}}{\\sigma^{2}}\\right)  I^{-1}d\\theta}\\\\\n&  =\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\int_{\\theta_{0}-I/2}^{\\theta_{0}+I/2}\\exp\\left(  -\\frac{1}{2}\\left( \\frac{\\left(  \\overline{y}^{\\ast}-\\theta\\right)  }{\\sigma/\\sqrt{T}}\\right)^2\n\\right)  d\\theta}\\\\\n&  \\geq\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\sqrt{2\\pi\\sigma^{2}/T}}\\rightarrow1\\text{ as }T\\rightarrow\\infty\\text{.}%\n\\end{align*}\\] In large samples, the posterior probability of the null approaches 1, whereas Fisher always reject the null. It is important to note that this holds for any \\(t_{\\alpha}\\), thus even if the test were performed at the 1% level or lower, the posterior probability would eventually reject the null.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#prior-sensitivity",
    "href": "06-hyp.html#prior-sensitivity",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.6 Prior Sensitivity",
    "text": "6.6 Prior Sensitivity\nOne potential criticism of the previous examples is the choice of the prior distribution. How do we know that, somehow, the prior is not biased against rejecting the null generating the paradoxes? Under this interpretation, the problem is not with the \\(p-\\)value but rather with the Bayesian procedure. One elegant way of dealing with the criticism is search over priors and prior parameters that minimize the probability of the null hypothesis, thus biasing the Bayesian procedure against accepting the null hypothesis.\nTo see this, consider the case of testing \\(H_{0}:\\mu_{0}=0\\) vs. \\(H_{1}:\\mu_{0}\\neq0\\) with observations drawn from \\(y_{t} \\sim\\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), with \\(\\sigma\\) known. With equal prior null and alternative probability, the probability of the null is \\(p\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathrm{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Under the null, \\[\np\\left(  y \\mid H_{0}\\right)  =\\left(  \\frac{1}{2\\pi\\sigma^{2}}\\right)\n^{\\frac{T}{2}}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{\\left(  \\overline\n{y}-\\theta_{0}\\right)  }{\\sigma/\\sqrt{T}}\\right)  ^{2}\\right)  \\text{.}%\n\\] The criticism applies to the priors under the alternative. To analyze the sensitivity, consider four classes of priors under the alternative: (a) the class of normal priors, \\(p\\left( \\theta \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( a,A\\right)\\); (b) the class of all symmetric unimodal prior distributions; (c) the class of all symmetric prior distributions; and (d) the class of all proper prior distributions. These classes provide varying degrees of prior information, allowing a thorough examination of the strength of evidence.\nIn the first case, consider the standard conjugate prior distribution, \\(p\\left( \\mu \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A\\right)\\). Under the alternative, \\[\\begin{align*}\np\\left(  y \\mid H_{1}\\right)   &  =\\int p\\left(  y \\mid \\mu,H_{1}\\right)  p\\left(  \\mu \\mid H_{1}\\right)  d\\mu\\\\\n&  =\\int p\\left(  \\overline{y} \\mid \\mu,H_{1}\\right)  p\\left( \\mu \\mid H_{1}\\right)  d\\mu\\text{,}%\n\\end{align*}\\] using the fact that \\(\\overline{y}\\) is a sufficient statistic. Noting that \\(p\\left( \\overline{y} \\mid \\mu,H_{1}\\right) \\sim N\\left( \\mu ,\\sigma^{2}/T\\right)\\) and \\(p\\left( \\mu \\mid H_{1}\\right) \\sim N\\left( \\mu_{0},A\\right)\\), we can use the “substitute” instead of integrate trick to assert that \\[\n\\overline{y}=\\mu_{0}+\\sqrt{A}\\eta+\\sqrt{\\sigma^{2}/T}\\varepsilon\\text{,}%\n\\] where \\(\\eta\\) and \\(\\varepsilon\\) are standard normal. Then, \\(p\\left( \\overline{y} \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A+\\sigma^{2}/T\\right)\\). Thus, \\[\n\\mathrm{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }=\\frac{p\\left(  \\overline{y} \\mid H_{0}\\right)\n}{p\\left(  \\overline{y} \\mid H_{1}\\right)  }=\\frac{\\left(  \\sigma^{2}/T\\right)\n^{-\\frac{1}{2}}}{\\left(  \\sigma^{2}/T+A\\right)  ^{-\\frac{1}{2}}}\\frac\n{\\exp\\left(  -\\frac{1}{2}t^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2}\\frac\n{z^{2}\\sigma^{2}/T}{A+\\sigma^{2}/T}\\right)  }\\text{.} \\label{BF_normal}%\n\\] To operationalize the test, \\(A\\) must be selected. \\(A\\) is chosen to minimizing the posterior probabilities of the null, with \\(P_{norm}\\left( H_{0} \\mid y\\right)\\) being the resulting lower bound on the posterior probability of the null. For \\(z\\geq1\\), the lower bound on the posterior probability of the null is \\[\nP_{norm}\\left(  H_{0} \\mid y\\right)  =\\left[\n1+\\sqrt{e}\\exp\\left(  -.5t^{2}\\right)  \\right]  ^{-1},\n\\] which is derived in a reference cited in the notes. This choice provides a maximal bias of the Bayesian approach toward rejecting the null. It is important to note that this is not a reasonable prior, as it was intentionally constructed to bias the null toward rejection.\nFor the class of all proper prior distributions, it is also easy to derive the bound. From equation above, minimizing the posterior probability is equivalent to minimizing the Bayes factor, \\[\n\\mathrm{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{.}%\n\\] Since \\[\np\\left(  y \\mid H_{1}\\right)  =\\int p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta\\leq p\\left( y \\mid \\widehat{\\theta}_{MLE},H_{1}\\right)  \\text{,}%\n\\] where \\(\\widehat{\\theta}_{MLE}=\\arg\\underset{\\theta\\neq0}{\\max}p\\left( y \\mid \\theta\\right)\\). The maximum likelihood estimator, maximizes the probability of the alternative, and provides a lower bound on the Bayes factor, \\[\n\\underline{\\mathrm{BF}}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{\\underset{\\theta\\neq0}{\\sup}p\\left(  y \\mid \\theta\\right)  }\\text{.}%\n\\] In this case, the bound is particularly easy to calculate and is given by \\[\nP_{all}\\left(  H_{0} \\mid y\\right)  =\\left( 1+\\exp\\left(  -\\frac{t^{2}}{2}\\right)  \\right)  ^{-1}\\text{.}%\n\\] A reference cited in the notes provides the bounds for the second and third cases, generating \\(P_{s,u}\\left( H_{0} \\mid y\\right)\\) and \\(P_{s}\\left( H_{0} \\mid y\\right)\\), respectively. All of the bounds only depend on the \\(t-\\)statistic and constants.\nTable 6.2 reports the \\(t-\\)statistics and associated \\(p-\\)values, with the remaining columns provide the posterior probability bounds. For the normal prior and choosing the prior parameter \\(A\\) to minimize the probability of the null, the posterior probability of the null is much larger than the \\(p-\\)value, in every case. For the standard case of a \\(t-\\)statistic of 1.96, \\(P\\left( H_{0} \\mid y\\right)\\) is more than six times greater than the \\(p-\\)value. For \\(t=2.576\\), \\(P\\left( H_{0} \\mid y\\right)\\) is almost 13 times greater than the \\(p-\\)value. These probabilities fall slightly for more general priors. For example, for the class of all priors, a t-statistic of 1.96/2.576 generates a lower bound for the posterior probability of 0.128/0.035\\(,\\) more than 2/3 times the \\(p-\\)value.\n\n\n\nTable 6.2: Comparison of strength of evidence against the point null hypothesis. The numbers are reproduced from Berger (1986).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-stat\n\\(p\\)-value\n\\(P_{norm}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{s,u}\\left( H_{0} \\mid y\\right)\\)\n\\(P_{s}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{all}\\left(H_{0} \\mid y\\right)\\)\n\n\n\n\n1.645\n0.100\n0.412\n0.39\n0.34\n0.205\n\n\n1.960\n0.050\n0.321\n0.29\n0.227\n0.128\n\n\n2.576\n0.010\n0.133\n0.11\n0.068\n0.035\n\n\n3.291\n0.001\n0.0235\n0.018\n0.0088\n0.0044",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#model-elaboration-and-nested-model-testing",
    "href": "06-hyp.html#model-elaboration-and-nested-model-testing",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.7 Model Elaboration and Nested Model Testing",
    "text": "6.7 Model Elaboration and Nested Model Testing\nAn elaborated model in Bayesian statistics refers to a model that extends or generalizes a simpler, baseline (or “underlying”) model by introducing additional parameters or structure. The purpose of elaboration is to capture more complex features of the data, account for possible deviations from the assumptions of the simpler model, or to allow for greater flexibility in modeling.\nFormally, suppose we start with a baseline model \\(f(y \\mid \\theta)\\), where \\(\\theta\\) is a parameter of interest. An elaborated model introduces an additional parameter (or set of parameters) \\(\\lambda\\), resulting in a family of models \\(f(y \\mid \\theta, \\lambda)\\) indexed by \\(\\lambda \\in \\Lambda\\). The original model is recovered as a special case for some fixed value \\(\\lambda_0\\) (i.e., \\(f(y \\mid \\theta) = f(y \\mid \\theta, \\lambda_0)\\)). The set \\(\\Lambda\\) describes the ways in which the model can be elaborated.\nWhen we use elaborated models then we need to compare nested models (where the simpler model is a special case of the more complex one). In Bayesian analysis, inference in an elaborated model involves integrating over the additional parameters, reflecting uncertainty about both the original and the elaborating parameters.\nApplying the usual Bayesian paradigm (disciplined probability accounting) to the elaborated framework, we see that inference about \\(\\theta\\) is determined by \\[\np(\\theta \\mid y) = \\int_\\Lambda p(\\theta \\mid \\lambda, y) p(\\lambda \\mid y) d\\lambda\n\\] where \\[\n\\begin{aligned}\np(\\theta \\mid \\lambda, y) &\\propto p(y \\mid \\theta, \\lambda) p(\\theta \\mid \\lambda) \\\\\np(\\lambda \\mid y) &\\propto p(y \\mid \\lambda) p(\\lambda) \\\\\n\\text{where } p(y \\mid \\lambda) &= \\int p(y \\mid \\theta, \\lambda) p(\\theta \\mid \\lambda) d\\theta\n\\end{aligned}\n\\]\nFor consistency with the elaborated and underlying model, we take \\(p(\\theta \\mid \\lambda_0) = p(\\theta)\\). Since \\(\\lambda\\) labels the form of departure from the initial model \\(M_0: \\lambda = \\lambda_0\\), the form of \\(p(\\lambda)\\) should be chosen to reflect this departure.\nA classical example is the exponential power elaboration of the traditional normal family, allowing for robustness. Here \\(\\lambda \\in (0,3)\\) indexes the power and \\(\\lambda_0 = 2\\) is the Normal case.\nThe posterior mean is simply a weighted average with respect to \\(p(\\lambda \\mid y)\\): \\[\n\\E{\\theta \\mid y} = \\int \\E{\\theta \\mid \\lambda, y} p(\\lambda \\mid y) d\\lambda = \\E[\\lambda \\mid y]{\\E{\\theta \\mid \\lambda, y}}\n\\]\n\nThe Dickey-Savage Approach to Nested Models\nThe Dickey-Savage approach provides a principled Bayesian method for testing nested models—situations where a simpler model is a special case of a more complex one. This approach is particularly useful when we want to assess whether the data support the inclusion of additional parameters or structure in our model, or whether the simpler, baseline model suffices.\nIn the context of Bayesian hypothesis testing, the Dickey-Savage method allows us to compute the Bayes factor for comparing a nested (elaborated) model to its simpler counterpart using only the posterior and prior distributions of the parameter(s) that distinguish the two models. This not only streamlines the computation but also clarifies the relationship between the models and the evidence provided by the data.\n\n\n\n\n\n\nThe Bayes Folklore\n\n\n\nBasically, Dickey-Savage support the commonly used approach in Bayes to fint a model as big as an elefant. You fit a model as big as allowed by your computaitonal budget and statistical skills, we call it \\(M\\). The calculations for any other model, e.g. \\(m_0\\) can be calculted under \\(M\\).\n\n\nSuppose you are conducting a hypothesis test comparing two models, \\(M_0\\) and \\(M_1\\). The null hypothesis \\(H_0\\) is that the simpler model \\(M_0\\) is true, and the alternative hypothesis \\(H_1\\) is that the more complex model \\(M_1\\) is true.\nLet’s explore how this approach works and why it is both elegant and practical for model comparison in Bayesian analysis.\nSuppose that \\(M_0 \\subset M\\). Let \\((\\theta, \\psi)\\) have matching priors such that \\[\np(\\psi \\mid \\theta = 0, M) = p(\\psi \\mid M_0)\n\\] where \\(\\theta = 0\\) corresponds to \\(M_0\\). That is, \\(p(y \\mid \\theta = 0, \\psi, M) = p(y \\mid \\psi, M_0)\\).\nThen, we can calculate solely under model \\(M\\) the Bayes factor as follows: \\[\nBF = \\frac{p(\\theta = 0 \\mid y, M)}{p(\\theta = 0 \\mid M)} = \\frac{p(y \\mid M_0)}{p(y \\mid M)}\n\\]\nThis is a ratio of posterior ordinates, valid as long as models are nested. By definition of marginals: \\[\n\\begin{aligned}\np(y \\mid \\theta = 0, M) &= \\int p(y \\mid \\theta = 0, \\psi, M) p(\\psi \\mid \\theta = 0, M) d\\psi \\\\\n&= \\int p(y \\mid \\psi, M_0) p(\\psi \\mid M_0) d\\psi \\\\\n&= p(y \\mid M_0)\n\\end{aligned}\n\\]\nThis elegant result shows that Bayes factors for nested models can be computed entirely within the larger model framework.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#the-difference-between-p-values-and-bayesian-evidence",
    "href": "06-hyp.html#the-difference-between-p-values-and-bayesian-evidence",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.8 The difference between p-values and Bayesian evidence",
    "text": "6.8 The difference between p-values and Bayesian evidence\nSuppose that you routinely reject two-sided hypotheses at a fixed level of significance, \\(\\alpha = 0.05\\). Furthermore, suppose that half the experiments under the null are actually true: \\(p(H_0) = p(H_1) = \\frac{1}{2}\\).\nThe observed p-value is not a probability in any real sense. The observed t-value is a realization of a statistic that happens to be \\(N(0,1)\\) under the null hypothesis. Suppose that we observe \\(t = 1.96\\).\nThen the maximal evidence against the null hypothesis, which corresponds to \\(t = 0\\), will be achieved by evaluating the likelihood ratio at the observed t-ratio. We get \\[\n\\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\geq \\frac{p(y \\mid \\theta = \\theta_0)}{p(y \\mid \\theta = \\hat{\\theta})}\n\\]\nTechnically, \\(p(y \\mid H_1) = \\int p(y \\mid \\theta) p(\\theta \\mid H_1) d\\theta \\leq p(y \\mid \\hat{\\theta})\\).\nFor testing, this gives: \\[\n\\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\geq \\frac{\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\cdot 1.96^2}}{\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\cdot 0^2}} = 0.146\n\\]\nIn terms of probabilities, with \\(p(H_0) = p(H_1)\\), we have: \\[\np(H_0 \\mid y) = \\frac{1}{1 + \\frac{p(y \\mid H_1)}{p(y \\mid H_0)} \\frac{p(H_1)}{p(H_0)}} \\geq 0.128\n\\]\nHence, there’s still a 12.8% chance that the null is true! That’s very different from the p-value of 5%.\nMoreover, among experiments with p-values of 0.05, at least 28.8% will actually turn out to be true nulls (Sellke, Bayarri, and Berger 2001)! Put another way, the probability of rejecting a true null conditional on the observed \\(p = 0.05\\) is at least 30%. You are throwing away good null hypotheses and claiming you have found effects!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#jeffreys-decision-rule",
    "href": "06-hyp.html#jeffreys-decision-rule",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.9 Jeffreys’ Decision Rule",
    "text": "6.9 Jeffreys’ Decision Rule\nJeffreys (1998) provided a famous rule for hypothesis testing. Consider testing \\(H_0: \\beta = 0\\) versus \\(H_1: \\beta \\neq 0\\) with a t-statistic \\(\\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}}\\). Jeffreys proposed the rule: \\[\n\\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}} &gt; \\log\\left(\\frac{2n}{\\pi}\\right)\n\\]\nThis follows from a Bayes factor of \\(BF = 1\\), corresponding to a Cauchy \\(C^+(0, \\sigma)\\) prior. The critical relationship is: \\[\n\\sqrt{\\frac{2n}{\\pi}} \\exp\\left(-\\frac{1}{2} \\frac{\\hat{\\beta}^2}{s_{\\hat{\\beta}}^2}\\right) = 1\n\\]\nThis follows from the Dickey-Savage density ratio: \\[\nBF = \\frac{p(\\theta = 0 \\mid y)}{p(\\theta = 0)} = \\frac{\\sqrt{n/(2\\pi\\sigma)} e^{-\\frac{1}{2}t^2}}{1/(\\pi\\sigma)}\n\\]\nAs the Cauchy prior has density ordinate \\(p(\\theta = 0) = 1/(\\pi\\sigma)\\).\nWe have the following critical values, as opposed to the usual 1.96:\n\nJeffreys’ decision rule critical values Jeffreys (1998) (p. 379)\n\n\n\\(n\\)\n\\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}}\\)\n\n\n\n\n5\n1.16\n\n\n10\n1.85\n\n\n100\n4.15\n\n\n100,000\n11.06\n\n\n\nFor instance, when \\(n = 10\\), we have \\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}} = \\log(20/\\pi) = 1.85\\), and for \\(n = 100\\), we have \\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}} = \\log(200/\\pi) = 4.15\\).\nJeffreys then explains the consequences of this sample-size dependence: traditional fixed critical values like 1.96 do not properly account for the evidence provided by larger sample sizes.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#cromwells-rule",
    "href": "06-hyp.html#cromwells-rule",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.10 Cromwell’s Rule",
    "text": "6.10 Cromwell’s Rule\nThe discussion of hypothesis testing throughout this chapter reveals a fundamental tension between the desire for certainty and the reality of uncertainty in statistical inference. This tension is captured by Cromwell’s Rule, a principle that serves as a philosophical foundation for Bayesian hypothesis testing.\nWe can write Bayes rule for updating models as follows: \\[\np(M \\mid D) = \\frac{p(D \\mid M)}{p(D)} p(M)\n\\]\nThus, if \\(p(M) = 0\\), then \\(p(M \\mid D) = 0\\) for all \\(D\\).\nThis mathematical result has profound implications: if you assign zero prior probability to a hypothesis, no amount of evidence can ever change your mind. The posterior probability remains zero regardless of how strongly the data might support that hypothesis.\nThis principle is named after Oliver Cromwell’s famous plea to the Church of Scotland in 1650:\n\nI beseech you, in the bowels of Christ, think it possible you may be mistaken\n\nCromwell’s appeal for intellectual humility resonates deeply with the Bayesian approach to hypothesis testing. The rule suggests that we should never assign zero probability to hypotheses that could conceivably be true, as doing so makes us unable to learn from any amount of contradictory evidence.\nThe rule emphasizes the importance of careful prior specification. As we saw in the section on prior sensitivity, even when we try to bias our priors against the null hypothesis, the resulting posterior probabilities often remain substantially higher than corresponding p-values. Cromwell’s Rule reminds us that assigning zero probability to any reasonable hypothesis is not just mathematically problematic—it’s epistemologically unsound.\nCromwell’s Rule further aligns with the likelihood principle discussed earlier. Just as the likelihood principle states that all relevant experimental information is contained in the likelihood function, Cromwell’s Rule ensures that we remain open to learning from all possible evidence. By avoiding zero prior probabilities, we maintain the ability to update our beliefs based on observed data.\nThis principle serves as a philosophical foundation that unifies the various approaches to hypothesis testing discussed in this chapter, emphasizing the importance of intellectual humility and the willingness to learn from evidence in statistical inference.\n\n\n\n\nJeffreys, Harold. 1998. Theory of Probability. Third Edition, Third Edition. Oxford Classic Texts in the Physical Sciences. Oxford, New York: Oxford University Press.\n\n\nSellke, Thomas, M. J Bayarri, and James O Berger. 2001. “Calibration of \\(\\rho\\) Values for Testing Precise Null Hypotheses.” The American Statistician 55 (1): 62–71.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "07-sp.html",
    "href": "07-sp.html",
    "title": "7  Stochastic Processes",
    "section": "",
    "text": "Implications for Bayesian Learning\nYet another fundamental concept that is useful for probabilistic reasoning is a stochastic process. An instance of a process is a function \\(X\\colon \\Omega \\rightarrow S\\) from an index set \\(\\Omega\\) to a set of possible values \\(S\\), called the state space. The state space of a stochastic process is the set of all possible states that the process can be in. Each state in the state space represents a possible outcome or condition of the system being modeled. The process then is the distribution over the space of functions from \\(\\Omega\\) to \\(S\\). The term process is used because the function \\(X\\) is often thought of as a time-varying quantity, and the index set \\(\\Omega\\) is often interpreted as time. However, the index set can be any set, and the process can be a random function of any other variable. Both the index set and the state space can be discrete or continuous. For example, a discrete time index can represent days or rounds and a continuous time index is a point on a time line. The state space can be discrete (composed of distinct states, like the number of customers in a store) or continuous (such as the price of a stock). The state space can be one-dimensional (only one aspect of the system is modeled) or multi-dimensional (multiple aspects are modeled simultaneously).\nA stochastic process is a family of random variables that describes the evolution through time of some (physical) process. We denote this by \\(X = \\{X(t),~t\\in T\\}\\), with \\(t\\) representing time and \\(X(t) = \\omega\\) is the state of the process at time \\(t\\). We will get a realization (a.k.a. sample path). In the case when time is discrete, the realization is a sequence of observed \\(X = \\Omega = \\{\\omega_1,\\omega_2,\\ldots\\}\\). Common discrete time processes are Markov chains. Brownian motion is a central process in continuous time and state with almost surely continuous but nowhere differentiable paths. Poisson processes are commonly used to account for jumps in the process.\nHere are some widely used stochastic processes:\nIn contexts like agricultural field trials, the domain for analyzing yield is commonly referred to as the collection of plots. This term is broadly suitable for practical field purposes but is mathematically interpreted as the collection of planar Borel subsets across various growing seasons. In a basic clinical trial for a COVID-19 vaccine, like the AstraZeneca trial in 2021, the domain is typically referred to as the group of patients. This implies the inclusion of all eligible patients, regardless of whether they were actually recruited and observed in the trial. In research on speciation or sexual compatibility in fruit flies, the domain is defined as the set of male-female pairs, encompassing all potential pairs with the desired genetic traits. For a competition experiment, such as a chess or tennis tournament, the domain is described as the set of ordered pairs of participants, which includes all possible pairings, not just those who actually competed against each other at events like US Open in 2024.\nIn data analysis, both experimental and observational data can exhibit variability. This variability is often modeled using probability distributions. These distributions can either represent simple processes with independent elements (then we are back to i.i.d case) or more complex stochastic processes that display dependencies, whether they be serial, spatial, or of other types. Essentially, this modeling approach helps in understanding and predicting data behavior under various conditions. The early sections of Davison (2003) work offer an insightful primer on how to develop and apply these stochastic models across various fields. This introduction is particularly useful for grasping the fundamental concepts and practical applications of these models.\nThe law of large numbers provides theoretical justification for Bayesian learning. As we collect more data, the posterior distribution concentrates around the true parameter value, regardless of the prior. This phenomenon, known as posterior consistency, follows from the fact that the likelihood function—being a product of many terms—is dominated by the data for large samples.\nConsider estimating the mean \\(\\mu\\) of a normal distribution from i.i.d. observations \\(X_1, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\) with a prior \\(\\mu \\sim N(\\mu_0, \\tau_0^2)\\). The posterior mean is:\n\\[\n\\E{\\mu \\mid X_1, \\ldots, X_n} = \\frac{\\tau_0^{-2}\\mu_0 + n\\sigma^{-2}\\bar{X}_n}{\\tau_0^{-2} + n\\sigma^{-2}}\n\\]\nAs \\(n \\to \\infty\\), the data term \\(n\\sigma^{-2}\\bar{X}_n\\) dominates, and by the law of large numbers, \\(\\bar{X}_n \\to \\mu\\) almost surely. Thus:\n\\[\n\\E{\\mu \\mid X_1, \\ldots, X_n} \\to \\mu \\quad \\text{almost surely}\n\\]\nThe posterior concentrates at \\(\\mu\\), regardless of the prior \\(\\mu_0\\). The prior matters for small samples but becomes negligible for large samples—a reassuring property that ensures different researchers with different priors eventually reach consensus as evidence accumulates.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#the-lévy-itô-decomposition-in-finance",
    "href": "07-sp.html#the-lévy-itô-decomposition-in-finance",
    "title": "7  Stochastic Processes",
    "section": "7.1 The Lévy-Itô Decomposition in Finance",
    "text": "7.1 The Lévy-Itô Decomposition in Finance\nOne of the most profound results in the theory of stochastic processes is the Lévy-Itô decomposition theorem, which provides a universal framework for understanding how randomness evolves over time. The theorem states that any Lévy process (a stochastic process with stationary and independent increments) can be uniquely decomposed into three fundamental components:\n\nA deterministic drift term (linear trend)\nA continuous Gaussian component (Brownian motion)\nA pure jump component (compound Poisson process)\n\nMathematically, any Lévy process \\(X_t\\) can be written as: \\[\nX_t = \\mu t + \\sigma B_t + J_t\n\\] where \\(\\mu\\) is the drift coefficient, \\(B_t\\) is standard Brownian motion with volatility \\(\\sigma\\), and \\(J_t\\) represents the jump component that can be expressed as: \\[\nJ_t = \\sum_{i=1}^{N_t} Z_i\n\\] where \\(N_t\\) is a Poisson process counting the number of jumps up to time \\(t\\), and \\(Z_i\\) are the jump sizes.\nThis decomposition is remarkable because it tells us that no matter how complex a stochastic process might appear, if it has independent and stationary increments, it can always be broken down into these three intuitive building blocks: a predictable trend, continuous random fluctuations, and discrete jumps.\nThe Lévy-Itô decomposition provides a natural motivation for studying Brownian motion and Poisson processes as fundamental objects. Brownian motion captures the continuous, infinitesimal random perturbations that accumulate over time, while the Poisson process models rare, discrete events that cause sudden changes in the system state. Together with a deterministic drift, these components form a complete toolkit for modeling virtually any phenomenon with independent increments.\nThe practical importance of this decomposition cannot be overstated. In finance, asset returns exhibit both continuous price movements (modeled by Brownian motion) and sudden jumps due to news announcements or market shocks (modeled by Poisson processes). In telecommunications, network traffic consists of a steady baseline load (drift) plus continuous fluctuations (Brownian component) and sudden spikes from large file transfers (jumps). In insurance, claim amounts follow a baseline trend with continuous variation and occasional catastrophic events.\n\nExample 7.1 (Financial Asset Prices) Consider modeling the logarithm of a stock price. The Lévy-Itô decomposition suggests we should account for:\n\nDrift: The expected return on the asset, reflecting long-term growth trends in the economy\nBrownian component: Day-to-day price fluctuations driven by the continuous arrival of market information and trading activity\nJump component: Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks\n\nFor instance, during the 2008 financial crisis, stock prices exhibited massive downward jumps that could not be explained by a pure Brownian motion model. The Lehman Brothers bankruptcy on September 15, 2008 caused the S&P 500 to drop by 4.7% in a single day—an event that would have probability essentially zero under a Gaussian model but is naturally accommodated by the jump component in the Lévy-Itô framework.\nThe practical application of this decomposition led to the development of jump-diffusion models in quantitative finance, where option prices are calculated by accounting for both continuous price movements and discrete jumps. This approach provides more realistic pricing and risk assessment compared to the classical Black-Scholes model, which assumes only continuous price movements.\nThe Lévy-Itô decomposition thus provides both theoretical insight and practical tools. It explains why Brownian motion and Poisson processes are the fundamental building blocks for continuous-time stochastic modeling, and it gives practitioners a principled framework for decomposing complex random phenomena into interpretable components that can be estimated, simulated, and managed separately.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#newton-and-the-south-sea-bubble",
    "href": "07-sp.html#newton-and-the-south-sea-bubble",
    "title": "7  Stochastic Processes",
    "section": "7.2 Newton and the South Sea Bubble",
    "text": "7.2 Newton and the South Sea Bubble\nThe South Sea Bubble of 1720 stands as one of history’s most spectacular financial disasters, demonstrating that even the greatest scientific minds can fall victim to speculative mania. The South Sea Company, established in 1711 ostensibly to trade with South America, proposed an audacious scheme: it would assume England’s national debt in exchange for company shares and exclusive trading privileges. By early 1720, the company’s directors launched an unprecedented campaign of stock manipulation, spreading rumors of fabulous wealth, bribing politicians and royalty, and offering generous credit terms that allowed investors to purchase shares with only a small down payment. The stock price soared from £128 at the start of 1720 to over £1,000 by June—an eightfold increase in six months (Figure 7.1). Sir Isaac Newton, then Master of the Royal Mint, initially profited by selling his holdings in April 1720 for £7,000, but he could not resist re-entering the market after watching shares continue to climb. When the bubble burst in September, Newton lost £20,000—several years of his salary—leading him to famously remark, “I can calculate the movement of stars, but not the madness of men.” His experience reveals how market psychology and the fear of missing out can overwhelm even the most disciplined rational minds.\n\n\n\n\n\n\nFigure 7.1: South Sea Bubble\n\n\n\nIn the midst of this frenzy, Parliament passed the Bubble Act in June 1720, ironically just as South Sea stock reached its peak. While ostensibly designed to protect investors from fraudulent schemes, the Act was actually lobbied for by South Sea Company directors seeking to eliminate competition from rival ventures attracting investor capital. The Act required all joint-stock companies to obtain expensive royal charters and imposed severe penalties on unauthorized companies, effectively giving the South Sea Company a monopoly on investor enthusiasm. Paradoxically, by forcing the shutdown of smaller speculative ventures and drying up alternative investments, the Act may have hastened the bubble’s collapse by causing investors to question the sustainability of the broader market euphoria. The Act’s unintended consequences proved profound and long-lasting—it severely restricted the development of joint-stock companies in Britain for over a century until its repeal in 1825, arguably impeding industrialization by creating legal obstacles for large-scale ventures requiring significant capital. The episode offers enduring lessons about financial markets: price bubbles exhibit the characteristics of non-stationary stochastic processes with time-varying volatility and jump risk, leverage amplifies both gains and losses, regulatory interventions can create unintended consequences, and even rational agents can behave irrationally when caught in speculative manias—all phenomena that modern stochastic models attempt to capture.\nThe table below shows the prices from five historical bubbles, including the South Sea Bubble.\n\nd &lt;- read.csv(\"../data/sea_bubble.csv\",header=T)\nhead(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGregorian.Date\nBank.of.England\nRoyal.African.Company\nOld.East.India.Company\nSouth.Sea.Company\nMississippi.Company\n\n\n\n\n21-8-1719\n143\n12\n189\n113\n3400\n\n\n22-8-1719\n143\n12\n188\n113\n3400\n\n\n23-8-1719\n143\n12\n188\n113\n3450\n\n\n24-8-1719\n143\n12\n189\n113\nNA\n\n\n25-8-1719\n144\n12\n189\n114\nNA\n\n\n26-8-1719\n144\n12\n190\n114\n3600\n\n\n\n\n\n\nThe plot of the prices reveals the interconnected nature of early 18th-century financial manias and demonstrates the stochastic features that modern models attempt to capture. The South Sea Bubble exhibits the classic pattern of explosive growth followed by catastrophic collapse—a dramatic jump discontinuity in September 1720 that cannot be explained by continuous Brownian motion alone. Remarkably, the contagion spread across markets: the Bank of England, Royal African Company, and Old East India Company all show synchronized price movements during 1720, rising in sympathy with the South Sea speculation before experiencing their own sharp corrections. Most striking is the Mississippi Company plot, which tracks John Law’s concurrent bubble in France—it peaked slightly earlier than the South Sea Bubble and collapsed even more precipitously, suggesting that speculative manias can propagate across national borders. The synchronization across these four series illustrates volatility clustering and correlation jumps, phenomena that motivate the stochastic volatility models with correlated jumps discussed later in this chapter. These price paths exhibit all three components of the Lévy-Itô decomposition: drift during the accumulation phase, continuous Brownian fluctuations throughout, and sudden Poisson jumps at the moment of collapse.\n\n\n\n\n\nHistorical Bubbles",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#brownian-motion",
    "href": "07-sp.html#brownian-motion",
    "title": "7  Stochastic Processes",
    "section": "7.3 Brownian Motion",
    "text": "7.3 Brownian Motion\nBrownian Motion, named after botanist Robert Brown, is a fundamental concept in the theory of stochastic processes. It describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.\nA one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process \\(B(t)_{t\\ge 0}\\) with the following properties:\n\n\\(B(0) = 0\\) almost surely\n\\(B(t)\\) has stationary independent increments: \\(B(t) - B(s) \\sim N(0, t-s)\\) for \\(0 \\le s &lt; t\\)\n\\(B(t)\\) is a continuous function of \\(t\\)\nFor each time \\(t &gt; 0\\), the random variable \\(B(t)\\) is normally distributed with mean 0 and variance \\(t\\), i.e., \\(B(t) \\sim N(0, t)\\).\n\nFormally, Brownian motion is a stochastic process \\(B(t)\\) which is a family of real random variables indexed by the set of nonnegative real numbers \\(t\\).\nFigure 7.2 below shows three sample paths of Brownian Motion.\n\n# Brownian Motion\nset.seed(92)\nt = seq(0, 1, 0.001)\nplot(t, cumsum(rnorm(1001, 0, sqrt(0.001))), type=\"l\", xlab=\"t\", ylab=\"B(t)\", lwd=2, ylim=c(-1.2, 2))\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))), lwd=2, col=2)\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))),lwd=2, col=3)\n\n\n\n\n\n\n\nFigure 7.2: Brownian Motion\n\n\n\n\n\nThus, for any times \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\(B(t_2) - B(t_1)\\), \\(B(t_3) - B(t_2)\\), , \\(B(t_n) - B(t_{n-1})\\) are independent and the function \\(t \\mapsto B(t)\\) is continuous almost surely.\nSome properties of Brownian Motion are:\n\nScale Invariance: If \\(B(t)\\) is a Brownian motion, then for any \\(a &gt; 0\\), the process \\(aB(t/a^2)\\) is also a Brownian motion.\nTime Inversion: If \\(B(t)\\) is a Brownian motion, then \\(tB(1/t)\\) is also a Brownian motion for \\(t &gt; 0\\).\nFractal Nature: Brownian motion paths are nowhere differentiable but continuous everywhere, reflecting a fractal-like nature.\n\nHistorically, the most widely used models for stock market returns relied on the assumption that asset returns follow a normal or a lognormal distribution. The lognormal model for asset returns was challenged after the October 1987 crash of the American stock market. On October 19 (Black Monday) the Dow Jones index had fallen 508 points, or 23 percent. It was the worst single day in history for the US markets. The reason for the crash was rather simple: it was caused by the portfolio insurance product created by one of the financial firms. The idea of this insurance was to switch from equities to US Treasury bills as markets go down. Although the lognormal model does a good job of describing the historical data, the jump observed on that day had a probability close to zero, according to the lognormal model. The lognormal model underestimates the probability of a large change (thin tail). The widely used Black-Scholes model for asset pricing was relying on the lognormal model; it was incapable of correctly pricing in the possibility of such a large drop.\nThe normal assumption of asset returns was first proposed in 1900 in the PhD thesis of Louis Bachelier, who was a student of Henri Poincare. Bachelier was interested in developing statistical tools for pricing options (predicting asset returns) on the Paris stock exchange. Although Bachelier’s work laid the foundation for the modern theory of stochastic processes, he was never given credit by his contemporaries, including Einstein, Levy and Borel.\nIn 1905 Einstein published a paper which used the same statistical model as Bachelier to describe the 1827 discovery by botanist Robert Brown, who observed that pollen particles suspended in water followed irregular random trajectories. Thus, we call the stochastic process that describes these phenomena Brownian motion. Einstein’s advisor at the University of Zurich was Hermann Minkowski who was a friend and collaborator of Poincare. Thus, it is likely Einstein knew about the work of Bachelier, but he never mentioned it in his paper. This was not the first instance when Einstein did not give proper credit. Poincare published a paper Poincaré (1898) on relativity theory in 1898, seven years before Einstein. This paper was published in a philosophy journal and thus Poincare avoided using any mathematical formulas except for the famous \\(E=mc^2\\). Poincare discussed his results on relativity theory with Minkowski. Minkowski asked Einstein to read Poincare’s work Arnol’d (2006). However, Einstein never referenced the work of Poincare until 1945. One of the reviewers for the 1905 paper on relativity by Einstein was Poincare and he wrote a very positive review mentioning it as a breakthrough. When Minkowski asked Poincare why he did not claim his priority on the theory, Poincare replied that our mission is to support young scientists. More about why credit is mistakenly given to Einstein for relativity theory is discussed by Logunov Logunov (2004).\nEinstein was not the only one who ignored the work of Bachelier; Paul Levy did so as well. Paul Levy was considered a pioneer and authority on stochastic processes during Bachelier’s time, although Bruno de Finetti introduced a dual concept of infinite divisibility in 1929, before the works of Levy in the early 1930s on this topic. Levy never mentioned the work of the obscure and little known mathematician Bachelier. The first to give credit to Bachelier was Kolmogorov in his 1931 paper Kolmogoroff (1931) (Russian translation A. N. Kolmogorov (1938) and English translation Shiryayev (1992)). Later Leonard Jimmie Savage translated Bachelier’s work to English and showed it to Paul Samuelson. Samuelson extended the work of Bachelier by considering the log-returns rather than absolute numbers, popularized the work of Bachelier among economists and the translation of Bachelier’s thesis was finally published in English in 1964 Cootner (1967). Many economists who extended the work of Bachelier won Nobel prizes, including Eugene Fama known for work on the efficient markets hypothesis, Paul Samuelson, and Myron Scholes for the Black-Scholes model, as well as Robert Merton.\nAlthough it was originally developed to model financial markets by Louis Bachelier in 1900, Brownian Motion has found applications in many other fields: biology (movement of biomolecules within cells), environmental science (diffusion processes, like the spread of pollutants in air or water), and mathematics (stochastic calculus and differential equations).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#black-scholes-model-for-sports-betting",
    "href": "07-sp.html#black-scholes-model-for-sports-betting",
    "title": "7  Stochastic Processes",
    "section": "7.4 Black-Scholes Model for Sports Betting",
    "text": "7.4 Black-Scholes Model for Sports Betting\nSports betting involves wagering on the outcome of athletic events. Bettors’ assessments of these outcomes are aggregated in markets that provide key metrics like the point spread, which is the expected margin of victory, and moneyline odds, which imply the probability of a team winning. These market-based measures can be used to analyze the uncertainty, or volatility, inherent in a sports game.\nTo quantify the uncertainty in a game’s outcome, the score difference between two teams over time can be modeled as a stochastic process. Specifically, we use a Brownian motion model, first proposed by Stern (1994), to represent the evolution of a team’s lead. In this framework, the score difference at time \\(t\\), denoted as \\(X(t)\\), is assumed to follow a normal distribution with a mean (or “drift”) that grows over time and a variance that also increases with time.\nThis can be expressed mathematically as: \\[\nX(t) = \\mu t + \\sigma B(t) \\sim N(\\mu t, \\sigma^2 t)\n\\] where \\(\\mu\\) is the drift parameter, representing the favored team’s point advantage over the whole game (derived from the point spread), \\(\\sigma\\) is the volatility parameter, representing the standard deviation of the final outcome, and \\(t\\) is the time elapsed in the game, scaled from 0 to 1.\nThis model allows for the calculation of a team’s win probability at any point in the game and provides a formal way to measure the uncertainty of the final score.\nThe concept of deriving a game’s volatility from betting markets is directly analogous to the Black-Scholes model in finance. In finance, the Black-Scholes formula is used to price options. If the market price of an option is known, one can work backward to solve for the volatility of the underlying stock; this is called implied volatility. The model in sports betting does the same: it uses the market-set point spread (\\(\\mu\\)) and win probability (\\(p\\)) to solve for the game’s implied volatility (\\(\\sigma\\)).\nBoth models use a Brownian motion framework to describe how a variable changes over time. However, there is a key difference. The sports model uses a standard Brownian motion, where the score changes additively. In contrast, the Black-Scholes model uses a geometric Brownian motion, which assumes that a stock price changes by a certain percentage, not by a fixed amount.\nEssentially, this approach applies the financial concept of implied volatility to the sports world, creating a lens through which betting market data can be interpreted to measure the expected uncertainty of a game.\n\nImplied Volatility for Sports Games\nThe concept of implied volatility is central to understanding how market prices reflect uncertainty. In the context of sports betting, implied volatility represents the market’s assessment of the uncertainty in a game’s final outcome, derived from observable betting market data.\nGiven the point spread \\(\\mu\\) (which represents the expected margin of victory) and the win probability \\(p\\) (derived from moneyline odds), we can solve for the implied volatility \\(\\sigma\\) using the relationship:\n\\[\np = \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)\n\\]\nRearranging this equation, the implied volatility is given by:\n\\[\n\\sigma = \\frac{\\mu}{\\Phi^{-1}(p)}\n\\]\nwhere \\(\\Phi^{-1}\\) is the inverse of the standard normal cumulative distribution function (the quantile function).\nThis approach mirrors the methodology used in financial markets, where option prices are used to infer the market’s expectation of future stock price volatility. In sports betting, the “option price” is effectively the betting odds, and the “underlying asset” is the game outcome. Just as financial implied volatility reflects market sentiment about future price movements, sports implied volatility captures the market’s view of how uncertain or “volatile” a particular game is likely to be.\nFor example, a game between two closely matched teams might have high implied volatility, reflecting greater uncertainty in the outcome, while a game featuring a heavily favored team against a significant underdog would typically exhibit lower implied volatility, as the outcome is more predictable.\n\nExample 7.2 (Black-Scholes Model for Super Bowl) In order to define the implied volatility of a sports game we begin with a distributional model for the evolution of the outcome in a sports game which we develop from Stern (1994). The model specifies the distribution of the lead of team A over team B, \\(X(t)\\) for any \\(t\\) as a Brownian motion process. If \\(B(t)\\) denotes a standard Brownian motion with distributional property \\(B(t) \\sim N(0,t)\\) and we incorporate drift, \\(\\mu\\), and volatility, \\(\\sigma\\), terms, then the evolution of the outcome \\(X(t)\\) that is given by: \\[\nX(t)=\\mu t + \\sigma B(t) \\sim N( \\mu t , \\sigma^2 t).\n\\] This distribution of the game outcome is similar to the Black-Scholes model of the distribution of a stock price.\nThis specification results in several useful measures (or, this specification results in closed-form solutions for a number of measures of interest). The distribution of the final score follows a normal distribution, \\(X(1)\\sim N(\\mu, \\sigma^2)\\). We can calculate the probability of team A winning, denoted \\(p=\\mathbb{P}(X(1)&gt;0)\\), from the spread and probability distribution. Given the normality assumption, \\(X(1) \\sim N(\\mu, \\sigma^2)\\), we have \\[\np = \\mathbb{P}(X(1)&gt;0) = \\Phi \\left ( \\frac{\\mu}{\\sigma} \\right )\n\\] where \\(\\Phi\\) is the standard normal cdf. Table 7.1 uses \\(\\Phi\\) to convert team A’s advantage \\(\\mu\\) to a probability scale using the information ratio \\(\\mu/\\sigma\\).\n\n\n\nTable 7.1: Probability of Winning \\(p\\) versus the Sharpe Ratio \\(\\mu/\\sigma\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu/\\sigma\\)\n0\n0.25\n0.5\n0.75\n1\n1.25\n1.5\n2\n\n\n\n\n\\(p=\\Phi(\\mu/\\sigma)\\)\n0.5\n0.60\n0.69\n0.77\n0.84\n0.89\n0.93\n0.977\n\n\n\n\n\n\nIf teams are evenly matched and \\(\\mu/\\sigma =0\\) then \\(p=0.5\\). Table 7.1 provides a list of probabilities as a function of \\(\\mu/\\sigma\\). For example, if the point spread \\(\\mu=-4\\) and volatility is \\(\\sigma=10.6\\), then the team has a \\(\\mu/\\sigma = -4/10.6 = - 0.38\\) volatility point disadvantage. The probability of winning is \\(\\Phi(-0.38) = 0.353 &lt; 0.5\\). A common scenario is that team A has an edge equal to half a volatility, so that \\(\\mu/\\sigma =0.5\\) and then \\(p= 0.69\\).\nOf particular interest here are conditional probability assessments made as the game progresses. For example, suppose that the current lead at time \\(t\\) is \\(l\\) points and so \\(X(t) = l\\). The model can then be used to update your assessment of the distribution of the final score with the conditional distribution \\((X(1) | X(t)=l )\\). To see this, we can re-write the distribution of \\(X(1)\\) given \\(X(t)\\) by noting that \\(X(1) = X(t)+ X(1) - X(t)\\). Using the formula above and substituting \\(t\\) for \\(1\\) where appropriate and noting that \\(X(t) = l\\) by assumption, this simplifies to \\[\nX(1)= l + \\mu(1- t) + \\sigma (B(1) - B(t)).\n\\] Here \\(B(1) - B(t)  \\stackrel{D}{=} B(1-t)\\) which is independent of \\(X(t)\\) with distribution \\(N(0,1-t)\\). The mean and variance of \\(X(1)|X(t)=l\\) decay to zero as \\(t \\rightarrow 1\\) and the outcome becomes certain at the realised value of \\(X(1)\\). We leave open the possibility of a tied game and overtime to determine the outcome.\nTo determine this conditional distribution, we note that there are \\(1-t\\) time units left together with a drift \\(\\mu\\) and as shown above in this case the uncertainty can be modeled as \\(\\sigma^2(1-t)\\). Therefore, we can write the distribution of the final outcome after \\(t\\) periods with a current lead of \\(l\\) for team A as the conditional distribution: \\[\n( X(1) | X(t)=l) =  (X(1)-X(t)) + l   \\sim N( l + \\mu(1 - t) , \\sigma^2 (1 - t) )\n\\] From the conditional distribution \\((X(1) | X(t)=l) \\sim N(l+\\mu(1-t), \\sigma^2 (1-t))\\), we can calculate the conditional probability of winning as the game evolves. The probability of team A winning at time \\(t\\) given a current lead of \\(l\\) point is: \\[\np_t = P ( X(1) &gt; 0 | X(t) = l) = \\Phi \\left ( \\frac{ l + \\mu ( 1 - t)  }{ \\sigma \\sqrt{ ( 1-t) } } \\right )\n\\]\n\n\n\n\n\n\nFigure 7.3: Score Evolution on a Discretized Grid\n\n\n\nFigure 7.3 A and B illustrate our methodology with an example. Suppose we are analyzing data for a Superbowl game between teams A and B with team A favored. Figure A presents the information available at the beginning of the game from the perspective of the underdog team B. If the initial point spread—or the market’s expectation of the expected outcome—is \\(-4\\) and the volatility is \\(10.6\\) (assumed given for the moment; more on this below) then the probability that the underdog team wins is \\(p = \\Phi ( \\mu /\\sigma ) = \\Phi ( - 4/ 10.6) = 35.3\\)%. This result relies on our assumption of a normal outcome distribution on the outcome as previously explained. Another way of saying this is \\(\\mathbb{P}(X(1)&gt;0)=0.353\\) for an outcome distribution \\(X(1) \\sim N(-4, 10.6^2)\\). Figure A illustrates this with the shaded red area under the curve.\nFigure 7.3 B illustrates the information and potential outcomes at half-time. Here we show the evolution of the actual score until half time as the solid black line. From half-time onwards we simulate a set of possible Monte Carlo paths to the end of the game.\nSpecifically, we discretise the model with time interval \\(\\Delta =1/200\\) and simulate possible outcomes given the score at half time. The volatility plays a key role in turning the point spread into a probability of winning as the greater the volatility of the distribution of the outcome, \\(X(1)\\), the greater the range of outcomes projected in the Monte Carlo simulation. Essentially the volatility provides a scale which calibrates the advantage implied by a given point spread.\nWe can use this relationship to determine how volatility decays over the course of the game. The conditional distribution of the outcome given the score at time \\(t\\), is \\((X(1)|X(t)=l)\\) with a variance of \\(\\sigma^2(1-t)\\) and volatility of \\(\\sigma \\sqrt{1-t}\\). The volatility is a decreasing function of \\(t\\), illustrating that the volatility dissipates over the course of a game. For example, if there is an initial volatility of \\(\\sigma = 10.6\\), then at half-time when \\(t=\\frac{1}{2}\\), the volatility is \\(10.6 / \\sqrt{2} = 7.5\\) volatility points left. Table 7.2, below, illustrates this relationship for additional points over the game.\n\n\n\nTable 7.2: Volatility Decay over Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\n\\(\\sigma \\sqrt{1-t}\\)\n10.6\n9.18\n7.50\n5.3\n0\n\n\n\n\n\n\nTo provide insight into the final outcome given the current score, Table 7.1 and Table 7.2 can be combined to measure the current outcome, \\(l\\), in terms of standard deviations of the outcome.\nFor example, suppose that you have Team B, an underdog, so from their perspective \\(\\mu = -4\\) and at half-time team B has a lead of 15, \\(l= 15\\). Team B’s expected outcome as presented earlier is \\(l + \\mu (1-t)\\) or \\(15 - 4 \\times \\frac{1}{2} = 13\\). If initial volatility is \\(\\sigma = 10.60\\) then the remaining volatility at half-time is \\(10.6/\\sqrt{2} = 7.50\\) and team B’s expected outcome of \\(13\\) in terms of standard deviations is \\(13/7.5 = 1.73\\). Thus team B’s expected outcome is at the 99th percentile of the distribution, \\(\\Phi ( 1.73 ) = 0.96\\), implying a 96% chance of winning.\nImplied Volatility\nThe previous discussion assumed that the variance (or volatility) parameter \\(\\sigma\\) was a known constant. We return to this important quantity now. We are now in a position to define the implied volatility implicit in the two betting lines that are available. Given our model, we will use the money-line odds to provide a market assessment of the probability of winning, \\(p\\), and the point spread to assess the expected margin of victory, \\(\\mu\\). The money line odds are shown for each team A and B and provide information on the payoff from a bet on the team winning. This calculation will also typically require an adjustment for the bookmaker’s spread. With these we can infer the implied volatility, \\(\\sigma_{IV}\\), by solving \\[\n\\sigma_{IV}: \\; \\; \\;  \\; \\; p = \\Phi \\left ( \\frac{\\mu}{\\sigma_{IV}} \\right ) \\; \\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV} = \\frac{ \\mu }{ \\Phi^{-1} ( p ) } \\; .\n\\] Here \\(\\Phi^{-1}(p)\\) denotes the standard normal quantile function such that the area under the standard normal curve to the left of \\(\\Phi^{-1}(p)\\) is equal to \\(p\\). In our example we calculate this using the qnorm in R. Note that when \\(\\mu =0\\) and \\(p= \\frac{1}{2}\\) there’s no market information about the volatility as \\(\\mu / \\Phi^{-1} (p)\\) is undefined. This is the special case where the teams are seen as evenly matched- the expected outcome has a zero point spread and there is an equal probability that either team wins.\nTime Varying Implied Volatility\nUp to this point the volatility rate has been assumed constant through the course of the game, i.e., that the same value of \\(\\sigma\\) is relevant. The amount of volatility remaining in the game is not constant but the basic underlying parameters has been assumed constant. This need not be true and more importantly the betting markets may provide some information about the best estimate of the volatility parameter at a given point of time. This is important because time-varying volatility provides an interpretable quantity that can allow one to assess the value of a betting opportunity.\nWith the advent of online betting there is a virtually continuous traded contract available to assess implied expectations of the probability of team A winning at any time \\(t\\). The additional information available from the continuous contract allows for further update of the implied conditional volatility. We assume that the online betting market gives us a current assessment of \\(p_t\\), that is the current probability that team A will win. We will then solve for \\(\\sigma^2\\) and in turn define resulting time-varying volatility, as \\(\\sigma_{IV,t}\\), using the resulting equation to solve for \\(\\sigma_{IV,t}\\) with \\[\np_t = \\Phi \\left ( \\frac{ l + \\mu(1-t)  }{\\sigma_{IV,t} \\sqrt{1-t}} \\right )\n\\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV,t} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}}\n\\] We will use our methodology to find evidence of time-varying volatility in the SuperBowl XLVII probabilities.\nSuper Bowl XLVII: Ravens vs San Francisco 49ers\nSuper Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013 and featured the San Francisco 49ers against the Baltimore Ravens. Going into Super Bowl XLVII the San Francisco 49ers were favorites to win which was not surprising following their impressive season. It was a fairly bizarre Super Bowl with a \\(34\\) minute power outage affecting the game by ultimately an exciting finish with the Ravens causing an upset victory \\(34-31\\). We will build our model from the viewpoint of the Ravens. Hence \\(X(t)\\) will correspond to the Raven’s score minus the San Francisco 49ers. Table 7.3 provides the score at the end of each quarter.\n\n\n\nTable 7.3: SuperBowl XLVII by Quarter\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\nRavens\n0\n7\n21\n28\n34\n\n\n49ers\n0\n3\n6\n23\n31\n\n\n\\(X(t)\\)\n0\n4\n15\n5\n3\n\n\n\n\n\n\nTo determine the parameters of our model we first use the point spread which was set at the Ravens being a four point underdog, i.e. \\(\\mu=-4\\). This sets the mean of our outcome, \\(X(1)\\), as \\[\n\\mu = \\mathbb{E} \\left (X(1) \\right )=-4 .\n\\] In reality, it was an exciting game with the Ravens upsetting the 49ers by \\(34-31\\). Hence, the realised outcome is \\(X(1)= 34-31=3\\) with the point spread being beaten by \\(7\\) points or the equivalent of a touchdown.\n\n\n\n\n\n\nFigure 7.4: Superbowl XLVII: Ravens vs 49ers: TradeSports contracts traded and dynamic probability of the Ravens winning\n\n\n\nTo determine the markets’ assessment of the probability that the Ravens would win at the beginning of the game we use the money-line odds. These odds were quoted as San Francisco \\(-175\\) and Baltimore Ravens \\(+155\\). This implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155. We can convert both of these money-lines to implied probabilities of the each team winning, by the equations \\[\np_{SF} = \\frac{175}{100+175} = 0.686 \\; \\; \\text{ and} \\; \\; p_{Ravens} = \\frac{100}{100+155} = 0.392\n\\] The probability sum to one plus the market overround: \\[\np_{SF} + p_{Ravens} = 0.686+0.392 = 1.078\n\\] namely a \\(7.8\\)% edge for the bookmakers. Put differently, if bettors place money proportionally across both teams then the bookies vig will be \\[\n\\text{Vig} = \\dfrac{0.078}{0.078+1} = 0.072\n\\] This means that the bookmaker is expected to make a profit of 7.2% of the total amount staked, no matter what happens to the outcome of the game.\nTo account for this edge in our model, we use the mid-point of the spread to determine \\(p\\) implying that \\[\np = \\frac{1}{2} p_{Ravens} + \\frac{1}{2} (1 - p_{SF} ) = 0.353\n\\] From the Ravens perspective we have \\(p = \\mathbb{P}(X(1)&gt;0) =0.353\\).\nFigure 7.4 shows the evolution of the markets conditional probability of winning \\(p_t\\) for the Ravens. The data are from the online betting website TradeSports.com. Starting at \\(p=0.353\\) we see how dramatically the markets assessment of the Ravens winning can fluctuate. Given their commanding lead at half time, the probability has as high as \\(0.90\\). At the end of the four quarter when the 49ers nearly went into the lead with a touchdown, at one point the probability had dropped to \\(30\\)%.\nOur main question of interest is then: What implied volatility is consistent with market expectations?\nTo calculate the implied volatility of the Superbowl we substitute the pair \\((\\mu,p)\\) into our definition and solve for \\(\\sigma_{IV}\\). We obtain \\[\n\\sigma_{IV} = \\frac{\\mu}{\\Phi^{-1}(p)} = \\frac{-4}{-0.377}  = 10.60\n\\] where we have used \\(\\Phi^{-1} ( p) = qnorm(0.353) = -0.377\\). So on a volatility scale the \\(4\\) point advantage assessed for the 49ers is under a \\(\\frac{1}{2} \\sigma\\) favorite. From Table 2, this is consistent with a win probability of \\(p=\\Phi(\\frac{1}{2})=0.69\\). Another feature is that a \\(\\sigma=10.6\\) is historically low, as a typical volatility of an NFL game is \\(14\\) (see Stern, 1991). However, the more competitive the game one might expect a lower volatility. In reality, the outcome \\(X(1)=3\\) was within one standard deviation of the model, which had an expectation of \\(\\mu=-4\\) and volatility \\(\\sigma=10.6\\). Another question of interest is\nWhat’s the probability of the Ravens winning given their lead at half time?\n\nAt half-time the Ravens were leading \\(21\\) to \\(6\\). This gives us \\(X(\\frac{1}{2})=21-6=15\\). From the online betting market we also have traded contracts on TradeSports.com that yield a current probability of \\(p_{\\frac{1}{2}} = 0.90\\). \nAn alternative view is to assume that the market assesses time varying volatility and the prices fully reflect the underlying probability. Here we ask the question\nWhat’s the implied volatility for the second half of the game?\nWe now have an implied volatility \\[\n\\sigma_{IV,t=\\frac{1}{2}} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}} = \\frac{15-2}{ \\Phi^{-1}(0.9) / \\sqrt{2} } = 14\n\\] where qnorm(0.9)=1.28. Notice that \\(14&gt; 10.6\\), our assessment of the implied volatility at the beginning of the game.\nWhat’s a valid betting strategy?\nAn alternative approach is to assume that the initial moneyline and point spread set the volatility and this stays constant throughout the game. This market is much larger than the online market and this is a reasonable assumption unless there has been material information as the game progresses such as a key injury.\nHence the market was expected a more typical volatility in the second half. If a bettor believed that there was no reason that \\(\\sigma\\) had changed from the initial \\(10.6\\) then their assessment of the Ravens win probability, under this models, would have been \\(\\Phi \\left ( 13/ (10.6/\\sqrt{2}) \\right ) = 0.96\\) and the \\(0.90\\) market rate would have been thought of as a betting opportunity.\nThe Kelly criterion (Kelly,1956) yields the betting rate \\[\n\\omega = p - \\dfrac{q}{b} = 0.96 - \\frac{0.1}{1/9} = 0.06\n\\] that is, \\(6\\)% of capital. A more realistic strategy is to use the fractional Kelly criterion, which scales the bet by a risk-aversion parameter \\(\\gamma\\). For example, in this case if \\(\\gamma =3\\), we would bet \\(0.06/3=0.02\\), or \\(2\\)% of our capital on this betting opportunity.\nFinally, odds changes can be dramatic at the end of the fourth quarter, and this Super Bowl was no exception. With the score at \\(34\\)–\\(29\\) and only a few minutes remaining, the 49ers were at first-and-goal. A few minutes after this, the probability of the Ravens winning had dropped precipitously from over \\(90\\)% to \\(30\\)%, see Figure 7.4. On San Francisco’s final offensive play of the game, Kaepernick threw a pass on fourth down to Michael Crabtree, but Ravens cornerback Jimmy Smith appeared to hold the wide receiver during the incompletion, No call was given and the final result was a Ravens win.\n\n\nExample 7.3 (Yahoo Stock Price Simulation) Investing in volatile stocks can be very risky. The Internet stocks during the late 1990’s were notorious for their volatility. For example, the leading Internet stock Yahoo! started 1999 at $62,rose to $122, then fell back to $55 in August, only to end the year at $216. Even more remarkable is the fact that by January 2000, Yahoo! has risen more than 100-fold from its offering price of $1.32 on April 15, 1996. In comparison, theNasdaq 100, a benchmark market index, was up about 5-fold during the same period.\nStock prices fluctuate somewhat randomly. Maurice Kendall, in his seminal 1953 paper on the random walk nature of stock and commodity prices, observed that “The series looks like a wandering one, almost as if once a week the Demon of Chance drew a random number from a symmetrical population of fixed dispersion and added to it the current price to determine next week’s price (p. 87).” While a pure random walk model for Yahoo!’s stock price is in fact not reasonable since its price cannot fall below zero, an alternative model that appears to provide reasonable results assumes that the logarithms of price changes, or returns, follow a random walk. This alternative model is the basis for the results in this example.\nTo evaluate a stock investment, we take the initial price as \\(X_0\\) and then we need to determine what the stock price might be in year \\(T\\), namely \\(X_T\\). Our approach draws from the Black-Scholes Model for valuing stock options. Technically, the Black-Scholes Model assumes that \\(X_T\\) is determined by the solution to a stochastic differential equation. This leads to the Geometric Brownian Motion \\[\nX_T = X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T + \\sigma B_T  \\right),\n\\] where \\(B_T\\) is a standard Brownian motion; that is, \\(B_0 = 0\\), \\(B_t - B_s\\) is independent of \\(B_s\\), and its distribution depends only on \\(t-s\\) with \\(B_t \\sim N(0,t)\\). Hence, \\(B_t = \\sqrt{t}Z\\), where \\(Z \\sim N(0,1)\\).\nThen, the expected value is \\[\\begin{align*}\nE(X_T) = &X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma B_T))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z)) \\\\\n&= X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\frac{1}{2}\\sigma^2T \\right) = X_0\\exp\\left( \\mu T \\right).\n\\end{align*}\\] The \\(E(\\exp(\\sigma \\sqrt{T}Z)) = \\exp\\left( 1/2\\sigma^2T \\right)\\) is due to the moment property of the log-normal distribution. We can interpret \\(\\mu\\) as the expected rate of return \\[\n\\hat \\mu = \\frac{1}{T}\\log\\left( \\frac{X_T}{X_0} \\right).\n\\] This provides a way to estimate the expected rate of return from the expected value of the stock price at time \\(T\\), by plugging in the observed values of \\(X_0\\) and \\(X_T\\).\nThe variance is \\[\\begin{align*}\n\\text{Var}(X_T) = &X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma B_T))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\sigma^2T \\right) - X_0^2\\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right)\\\\\n& = X_0^2\\exp\\left( 2\\mu T \\right)\\left( \\exp\\left( \\sigma^2T \\right) - 1 \\right).\n\\end{align*}\\]\nThe important consequence of the model for predicting future prices is that \\(\\log(X_T/X_0)\\) has a normal distribution with mean \\((\\mu-\\frac{1}{2} \\sigma^2)T\\) and variance \\(\\sigma^2 T\\) which is equivalent to saying that the ratio \\(X_T/X_0\\) has a log-normal distribution. It is interesting that although the Black-Scholes result is a standard tool for valuing options in finance the log-normal predictive distribution that follows from its assumptions is not commonly studied. In order to forecast \\(X_T\\) we need to estimate the unknowns \\(\\mu\\) and \\(\\sigma\\) (recall \\(X_0\\) is known). The unknown parameters \\(\\mu\\) and \\(\\sigma\\) can be interpreted as the instantaneous expected rate of return and the volatility, respectively. The mean parameter \\(\\mu\\) is known as the expected rate of return because the expected value of \\(X_T\\) is \\(X_0e^{\\mu T}\\). There are a number of ways of estimating the unknown parameters. One approach is to use an equilibrium model for returns, such as the Capital Asset Pricing Model or CAPM. We will discuss this model later. Another approach is to use historical data to estimate the parameters. For example, the expected rate of return can be estimated as the average historical return. The volatility can be estimated as the standard deviation of historical returns. The Black-Scholes model is a continuous time model, but in practice we use discrete time data. The Black-Scholes model can be adapted to discrete time by replacing the continuous time Brownian motion with a discrete time random walk.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#poisson-process",
    "href": "07-sp.html#poisson-process",
    "title": "7  Stochastic Processes",
    "section": "7.5 Poisson Process",
    "text": "7.5 Poisson Process\nA Poisson process is a fundamental stochastic process for modeling the occurrence of random events over time or space. It describes situations where events happen independently at a constant average rate, such as customer arrivals at a store, calls arriving at a call center, or goals scored in a soccer match.\nFormally, a counting process \\(\\{N(t), t \\geq 0\\}\\) is a Poisson process with rate parameter \\(\\lambda &gt; 0\\) if it satisfies the following properties:\n\n\\(N(0) = 0\\) (the process starts at zero)\nThe process has independent increments: for any \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\(N(t_2) - N(t_1), N(t_3) - N(t_2), \\ldots, N(t_n) - N(t_{n-1})\\) are independent\nThe process has stationary increments: for any \\(s &lt; t\\), the distribution of \\(N(t) - N(s)\\) depends only on the length of the interval \\(t - s\\)\nFor any interval of length \\(t\\), the number of events follows a Poisson distribution: \\[\nP(N(t) = k) = \\frac{e^{-\\lambda t}(\\lambda t)^k}{k!}, \\quad k = 0, 1, 2, \\ldots\n\\]\n\nThe parameter \\(\\lambda\\) represents the rate at which events occur per unit time. The expected number of events in an interval of length \\(t\\) is \\(\\E{N(t)} = \\lambda t\\), and the variance is \\(\\Var{N(t)} = \\lambda t\\).\nFigure 7.5 below shows three sample paths of a Poisson process with rate \\(\\lambda = 5\\) events per unit time.\n\n\n\n\n\n\n\n\nFigure 7.5: Poisson Process Trajectories\n\n\n\n\n\nAn equivalent characterization of the Poisson process is through the inter-arrival times between consecutive events. If \\(T_1, T_2, \\ldots\\) denote the times between successive events, then these are independent and identically distributed exponential random variables with mean \\(1/\\lambda\\). This connection between the Poisson and exponential distributions is fundamental: the Poisson process counts events while the exponential distribution models the waiting time between events.\nThe Poisson process can be viewed from two complementary perspectives. From a continuous-time viewpoint, we track the evolution of the counting process \\(N(t)\\) as time progresses, asking questions about the probability of observing a certain number of events by time \\(t\\) or the distribution of event times. From a discrete count data perspective, we observe the number of events that occurred during a fixed time interval and use this to make inferences about the underlying rate parameter \\(\\lambda\\).\nChapter 3 introduced Poisson models in the context of count data and Bayesian inference. The Poisson distribution (discussed in the section on Poisson Model for Count Data) emerges naturally when we observe a Poisson process over a fixed time interval. For instance, when modeling the number of goals scored by a soccer team in a match, we implicitly assume that goals occur according to a Poisson process with some rate \\(\\lambda\\), and we observe the total count at the end of the match.\nThe Bayesian approach to learning about the rate parameter \\(\\lambda\\) (covered in the section on Poisson-Gamma: Learning about a Poisson Intensity in Chapter 3) becomes particularly powerful in the continuous-time setting. When we observe a Poisson process over time, we can update our beliefs about \\(\\lambda\\) as new events occur. The Gamma distribution serves as a conjugate prior for \\(\\lambda\\), meaning that if we start with a Gamma prior and observe events from a Poisson process, the posterior distribution remains in the Gamma family with updated parameters. This elegant updating mechanism allows us to refine our estimates of the event rate as we gather more data, balancing prior beliefs with observed evidence.\nThe connection between these perspectives is crucial for applications. In many real-world scenarios, we observe event counts over fixed intervals (discrete perspective) but need to make predictions about future events or the timing of the next event (continuous perspective). The Poisson process framework unifies these views, allowing us to seamlessly move between counting events and modeling their temporal dynamics.\n\nExample 7.4 (EPL Betting) Feng, Polson, and Xu (2016) employ a Skellam process (a difference of Poisson random variables) to model real-time betting odds for English Premier League (EPL) soccer games. Given a matrix of market odds on all possible score outcomes, we estimate the expected scoring rates for each team. The expected scoring rates then define the implied volatility of an EPL game. As events in the game evolve, they re-estimate the expected scoring rates and our implied volatility measure to provide a dynamic representation of the market’s expectation of the game outcome. They use real-time market odds data for a game between Everton and West Ham in the 2015-2016 season. We show how the implied volatility for the outcome evolves as goals, red cards, and corner kicks occur.\nGambling on soccer is a global industry with revenues of over $1 trillion a year (see “Football Betting - the Global Gambling Industry worth Billions,” BBC Sport). Betting on the result of a soccer match is a rapidly growing market, and online real-time odds exist (Betfair, Bet365, Ladbrokes). Market odds for all possible score outcomes (\\(0-0, 1-0, 0-1, 2-0, \\ldots\\)) as well as outright win, lose, and draw are available in real time. In this paper, we employ a two-parameter probability model based on a Skellam process and a non-linear objective function to extract the expected scoring rates for each team from the odds matrix. The expected scoring rates then define the implied volatility of the game.\nSkellam Process\nTo model the outcome of a soccer game between team A and team B, we let the difference in scores, \\(N(t) = N_A(t) - N_B(t)\\), where \\(N_A(t)\\) and \\(N_B(t)\\) are the team scores at time point \\(t\\). Negative values of \\(N(t)\\) indicate that team A is behind. We begin at \\(N(0) = 0\\) and end at time one with \\(N(1)\\) representing the final score difference. The probability \\(\\mathbb{P}(N(1) &gt; 0)\\) represents the ex-ante odds of team A winning. Half-time score betting, which is common in Europe, is available for the distribution of \\(N(\\frac{1}{2})\\).\nThen we find a probabilistic model for the distribution of \\(N(1)\\) given \\(N(t) = \\ell\\), where \\(\\ell\\) is the current lead. This model, together with the current market odds, can be used to infer the expected scoring rates of the two teams and then to define the implied volatility of the outcome of the match. We let \\(\\lambda^A\\) and \\(\\lambda^B\\) denote the expected scoring rates for the whole game. We allow for the possibility that the scoring abilities (and their market expectations) are time-varying, in which case we denote the expected scoring rates after time \\(t\\) by \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), respectively, instead of \\(\\lambda^A(1-t)\\) and \\(\\lambda^B(1-t)\\).\nThe Skellam distribution is defined as the difference between two independent Poisson variables given by:\n\\[\n\\begin{aligned}\nN_A(t) &= W_A(t) + W(t) \\\\\nN_B(t) &= W_B(t) + W(t)\n\\end{aligned}\n\\]\nwhere \\(W_A(t)\\), \\(W_B(t)\\), and \\(W(t)\\) are independent processes with:\n\\[\nW_A(t) \\sim \\text{Poisson}(\\lambda^A t), \\quad W_B(t) \\sim \\text{Poisson}(\\lambda^B t).\n\\]\nHere \\(W(t)\\) is a non-negative integer-valued process to induce a correlation between the numbers of goals scored. By modeling the score difference, \\(N(t)\\), we avoid having to specify the distribution of \\(W(t)\\) as the difference in goals scored is independent of \\(W(t)\\). Specifically, we have a Skellam distribution:\n\\[\nN(t) = N_A(t) - N_B(t) \\sim \\text{Skellam}(\\lambda^A t, \\lambda^B t).\n\\tag{7.1}\\]\nAt time \\(t\\), we have the conditional distributions:\n\\[\n\\begin{aligned}\nW_A(1) - W_A(t) &\\sim \\text{Poisson}(\\lambda^A(1-t)) \\\\\nW_B(1) - W_B(t) &\\sim \\text{Poisson}(\\lambda^B(1-t)).\n\\end{aligned}\n\\]\nNow letting \\(N^*(1-t)\\), the score difference of the sub-game which starts at time \\(t\\) and ends at time 1 and the duration is \\((1-t)\\). By construction, \\(N(1) = N(t) + N^*(1-t)\\). Since \\(N^*(1-t)\\) and \\(N(t)\\) are differences of two Poisson process on two disjoint time periods, by the property of Poisson process, \\(N^*(1-t)\\) and \\(N(t)\\) are independent. Hence, we can re-express equation (Equation 7.1) in terms of \\(N^*(1-t)\\), and deduce\n\\[\n%N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim Skellam(\\lambda^A (1-t),\\lambda^B (1-t) )\nN^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim \\text{Skellam}(\\lambda^A_t,\\lambda^B_t)\n\\]\nwhere \\(W^*_A(1-t) = W_A(1) - W_A(t)\\), \\(\\lambda^A = \\lambda^A_0\\) and \\(\\lambda^A_t=\\lambda^A(1-t)\\). A natural interpretation of the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), is that they reflect the “net” scoring ability of each team from time \\(t\\) to the end of the game. The term \\(W(t)\\) models a common strength due to external factors, such as weather. The “net” scoring abilities of the two teams are assumed to be independent of each other as well as the common strength factor. We can calculate the probability of any particular score difference, given by \\(\\mathbb{P}(N(1)=x|\\lambda^A,\\lambda^B)\\), at the end of the game where the \\(\\lambda\\)’s are estimated from the matrix of market odds. Team strength and “net” scoring ability can be influenced by various underlying factors, such as the offensive and defensive abilities of the two teams. The goal of our analysis is to only represent these parameters at every instant as a function of the market odds matrix for all scores.\nAnother quantity of interest is the conditional probability of winning as the game progresses. If the current lead at time \\(t\\) is \\(\\ell\\), and \\(N(t)=\\ell=N_A(t)-N_B(t)\\), the Poisson property implied that the final score difference \\((N(1)|N(t)=\\ell)\\) can be calculated by using the fact that \\(N(1)=N(t)+N^*(1-t)\\) and \\(N(t)\\) and \\(N^*(1-t)\\) are independent. Specifically, conditioning on \\(N(t)=\\ell\\), we have the identity\n\\[ N(1)=N(t)+N^*(1-t)=\\ell+\\text{Skellam}(\\lambda^A_t,\\lambda^B_t). \\]\nWe are now in a position to find the conditional distribution (\\(N(1)=x|N(t)=\\ell\\)) for every time point \\(t\\) of the game given the current score. Simply put, we have the time homogeneous condition\n\\[\n\\begin{aligned}\n\\mathbb{P}(N(1)=x|\\lambda^A_t,\\lambda^B_t,N(t)=\\ell) &= \\mathbb{P}(N(1)-N(t)=x-\\ell |\\lambda^A_t,\\lambda^B_t,N(t)=\\ell) \\\\\n&= \\mathbb{P}(N^* (1-t)=x-\\ell |\\lambda^A_t,\\lambda^B_t)\n\\end{aligned}\n\\]\nwhere \\(\\lambda^A_t\\), \\(\\lambda^B_t\\), \\(\\ell\\) are given by market expectations at time \\(t\\). See Feng et al. for details.\nMarket Calibration\nOur information set at time \\(t\\) includes the current lead \\(N(t) = \\ell\\) and the market odds for \\(\\{Win, Lose, Draw, Score\\}_t\\), where \\(Score_t = \\{ ( i - j ) : i, j = 0, 1, 2, ....\\}\\). These market odds can be used to calibrate a Skellam distribution which has only two parameters \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\). The best fitting Skellam model with parameters \\(\\{\\hat\\lambda^A_t,\\hat\\lambda^B_t\\}\\) will then provide a better estimate of the market’s information concerning the outcome of the game than any individual market (such as win odds) as they are subject to a “vig” and liquidity.\nSuppose that the fractional odds for all possible final score outcomes are given by a bookmaker. Fractional odds, commonly used in the UK, express the ratio of profit to stake. For example, odds of \\(3:1\\) (read as “three-to-one”) mean that for every $1 wagered, the bettor receives $3 in profit if the bet wins, plus the original $1 stake returned, for a total payout of $4. In this case, if the bookmaker offers \\(3:1\\) odds on a 2-1 final score, the bookmaker pays out three times the amount staked by the bettor if the outcome is indeed 2-1. This contrasts with American money-line odds, where positive numbers indicate the profit on a $100 stake (e.g., +300 means $300 profit on $100 wagered), and negative numbers indicate the stake needed to win $100.\nThe market implied probability makes the expected winning amount of a bet equal to 0. For fractional odds of \\(3:1\\), the implied probability is calculated as \\(p = \\frac{1}{1+3} = \\frac{1}{4} = 0.25\\) or 25%. We can verify this creates a fair bet: the expected winning amount is \\(\\mu = -1 \\times (1-1/4) + 3 \\times (1/4) = -0.75 + 0.75 = 0\\). We denote these odds as \\(odds(2,1) = 3\\). To convert all the available odds to implied probabilities, we use the identity\n\\[ \\mathbb{P}(N_A(1) = i, N_B(1) = j)=\\frac{1}{1+odds(i,j)}. \\]\nThe market odds matrix, \\(O\\), with elements \\(o_{ij}=odds(i-1,j-1)\\), \\(i,j=1,2,3...\\) provides all possible combinations of final scores. Odds on extreme outcomes are not offered by the bookmakers. Since the probabilities are tiny, we set them equal to 0. The sum of the possible probabilities is still larger than 1 (see Dixon and Coles (1997) and Dixon and Coles (1997)). This “excess” probability corresponds to a quantity known as the “market vig.” For example, if the sum of all the implied probabilities is 1.1, then the expected profit of the bookmaker is 10%. To account for this phenomenon, we scale the probabilities to sum to 1 before estimation.\nTo estimate the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), for the sub-game \\(N^*(1-t)\\), the odds from a bookmaker should be adjusted by \\(N_A(t)\\) and \\(N_B(t)\\). For example, if \\(N_A(0.5)=1\\), \\(N_B(0.5)=0\\) and \\(odds(2,1)=3\\) at half time, these observations actually says that the odds for the second half score being 1-1 is 3 (the outcomes for the whole game and the first half are 2-1 and 1-0 respectively, thus the outcome for the second half is 1-1). The adjusted \\({odds}^*\\) for \\(N^*(1-t)\\) is calculated using the original odds as well as the current scores and given by\n\\[\n{odds}^*(x,y)=odds(x+N_A(t),y+N_B(t)).\n\\]\nAt time \\(t\\) \\((0\\leq t\\leq 1)\\), we calculate the implied conditional probabilities of score differences using odds information\n\\[\n\\mathbb{P}(N(1)=k|N(t)=\\ell)=\\mathbb{P}(N^*(1-t)=k-\\ell)=\\frac{1}{c}\\sum_{i-j=k-\\ell}\\frac{1}{1+{odds}^*(i,j)}\n\\]\nwhere \\(c=\\sum_{i,j} \\frac{1}{1+{odds}^*(i,j)}\\) is a scale factor, \\(\\ell=N_A(t)-N_B(t)\\), \\(i,j\\geq 0\\) and \\(k=0,\\pm 1,\\pm 2\\ldots\\).\nExample: Everton vs West Ham (3/5/2016)\nTable below shows the implied Skellam probabilities.\n\n\n\nTable 7.4: Table: Original odds data from Ladbrokes before the game started.\n\n\n\n\n\nEverton  West Ham\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n11/1\n12/1\n28/1\n66/1\n200/1\n450/1\n\n\n1\n13/2\n6/1\n14/1\n40/1\n100/1\n350/1\n\n\n2\n7/1\n7/1\n14/1\n40/1\n125/1\n225/1\n\n\n3\n11/1\n11/1\n20/1\n50/1\n125/1\n275/1\n\n\n4\n22/1\n22/1\n40/1\n100/1\n250/1\n500/1\n\n\n5\n50/1\n50/1\n90/1\n150/1\n400/1\n\n\n\n6\n100/1\n100/1\n200/1\n250/1\n\n\n\n\n7\n250/1\n275/1\n375/1\n\n\n\n\n\n8\n325/1\n475/1\n\n\n\n\n\n\n\n\n\n\nTable 7.4 shows the raw data of odds right the game. We need to transform odds data into probabilities. For example, for the outcome 0-0, 11/1 is equivalent to a probability of 1/12. Then we can calculate the marginal probability of every score difference from -4 to 5. We neglect those extreme scores with small probabilities and rescale the sum of event probabilities to one.\n\n\n\nTable 7.5: Market implied probabilities for the score differences versus Skellam implied probabilities at different time points. The estimated parameters \\(\\hat\\lambda^A=2.33\\), \\(\\hat\\lambda^B=1.44\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore difference\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n\n\n\n\nMarket Prob. (%)\n1.70\n2.03\n4.88\n12.33\n21.93\n22.06\n16.58\n9.82\n4.72\n2.23\n\n\nSkellam Prob. (%)\n0.78\n2.50\n6.47\n13.02\n19.50\n21.08\n16.96\n10.61\n5.37\n2.27\n\n\n\n\n\n\nTable 7.5 shows the model implied probability for the outcome of score differences before the game, compared with the market implied probability. As we see, the Skellam model appears to have longer tails. Different from independent Poisson modeling in Dixon and Coles (1997), our model is more flexible with the correlation between two teams. However, the trade-off of flexibility is that we only know the probability of score difference instead of the exact scores.\n\n\n\n\n\n\nFigure 7.6: The betting market data for Everton and West Ham is from ladbrokes.com. Market implied probabilities (expressed as percentages) for three different results (Everton wins, West Ham wins and draw) are marked by three distinct colors, which vary dynamically as the game proceeds. The solid black line shows the evolution of the implied volatility. The dashed line shows significant events in the game, such as goals and red cards. Five goals in this game are 13’ Everton, 56’ Everton, 78’ West Ham, 81’ West Ham and 90’ West Ham.\n\n\n\nFigure 7.6 examines the behavior of the two teams and represent the market predictions on the final result. Notably, we see the probability change of win/draw/loss for important events during the game: goals scoring and a red card penalty. In such a dramatic game, the winning probability of Everton gets raised to 90% before the first goal of West Ham in 78th minutes. The first two goals scored by West Ham in the space of 3 minutes completely reverses the probability of winning. The probability of draw gets raised to 90% until we see the last-gasp goal of West Ham that decides the game.\nFigure 7.6 plots the path of implied volatility throughout the course of the game. Instead of a downward sloping line, we see changes in the implied volatility as critical moments occur in the game. The implied volatility path provides a visualization of the conditional variation of the market prediction for the score difference. For example, when Everton lost a player by a red card penalty at 34th minute, our estimates \\(\\hat\\lambda^A_t\\) and \\(\\hat\\lambda^B_t\\) change accordingly. There is a jump in implied volatility and our model captures the market expectation adjustment about the game prediction. The change in \\(\\hat\\lambda_A\\) and \\(\\hat\\lambda_B\\) are consistent with the findings of Vecer, Kopriva, and Ichiba (2009) where the scoring intensity of the penalized team drops while the scoring intensity of the opposing team increases. When a goal is scored in the 13th minute, we see the increase of \\(\\hat\\lambda^B_t\\) and the market expects that the underdog team is pressing to come back into the game, an effect that has been well-documented in the literature. Another important effect that we observe at the end of the game is that as goals are scored (in the 78th and 81st minutes), the markets expectation is that the implied volatility increases again as one might expect.\n\n\n\n\n\n\nFigure 7.7: Red line: the path of implied volatility throughout the game, i.e., \\(\\sigma_{t}^{red} = \\sqrt{\\hat\\lambda^A_t+\\hat\\lambda^B_t}\\). Blue lines: the path of implied volatility with constant \\(\\lambda^A+\\lambda^B\\), i.e., \\(\\sigma_{t}^{blue} = \\sqrt{(\\lambda^A+\\lambda^B)*(1-t)}\\). Here \\((\\lambda^A+\\lambda^B) = 1, 2, ..., 8\\).\n\n\n\n\n\n\nTable 7.6: The calibrated \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) divided by \\((1-t)\\) and the implied volatility during the game. \\(\\{\\lambda^A_t, \\lambda^B_t\\}\\) are expected goals scored for rest of the game. The less the remaining time, the less likely to score goals. Thus \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) decrease as \\(t\\) increases to 1. Diving them by \\((1-t)\\) produces an updated version of \\(\\hat\\lambda_{0}\\)’s for the whole game, which are in general time-varying (but not decreasing necessarily).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt\n0\n0.11\n0.22\n0.33\n0.44\n0.50\n0.61\n0.72\n0.83\n0.94\n1\n\n\n\n\n\\(\\hat\\lambda^A_t/(1-t)\\)\n2.33\n2.51\n2.53\n2.46\n1.89\n1.85\n2.12\n2.12\n2.61\n4.61\n0\n\n\n\\(\\hat\\lambda^B_t/(1-t)\\)\n1.44\n1.47\n1.59\n1.85\n2.17\n2.17\n2.56\n2.90\n3.67\n5.92\n0\n\n\n\\((\\hat\\lambda^A_t+\\hat\\lambda^B_t)/(1-t)\\)\n3.78\n3.98\n4.12\n4.31\n4.06\n4.02\n4.68\n5.03\n6.28\n10.52\n0\n\n\n\\(\\sigma_{IV,t}\\)\n1.94\n1.88\n1.79\n1.70\n1.50\n1.42\n1.35\n1.18\n1.02\n0.76\n0\n\n\n\n\n\n\nFigure 7.7 compares the updating implied volatility of the game with implied volatilities of fixed \\((\\lambda^A+\\lambda^B)\\). At the beginning of the game, the red line (updating implied volatility) is under the “(\\(\\lambda^A+\\lambda^B=4)\\)”-blue line; while at the end of the game, it’s above the “(\\(\\lambda^A+\\lambda^B=8)\\)”-blue line. As we expect, the value of \\((\\hat\\lambda^A_t + \\hat\\lambda^B_t)/(1-t)\\) in Table 7.6 increases throughout the game, implying that the game became more and more intense and the market continuously updates its belief in the odds.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#stochastic-volatility-financial-economics",
    "href": "07-sp.html#stochastic-volatility-financial-economics",
    "title": "7  Stochastic Processes",
    "section": "7.6 Stochastic Volatility: Financial Economics",
    "text": "7.6 Stochastic Volatility: Financial Economics\nFinancial markets exhibit time-varying volatility—periods of calm trading alternate with episodes of extreme price movements. The October 1987 crash dramatically illustrated the limitations of constant volatility models: the Dow Jones index fell 23% in a single day, an event that had probability essentially zero under the lognormal model assumed by the Black-Scholes framework. This observation motivated the development of stochastic volatility models that allow uncertainty itself to evolve randomly over time.\nRobert Merton, who was a student of Samuelson, proposed a major extension to the work of Bachelier by introducing jumps to the model. The additive jump term addresses the issues of asymmetry and heavy tails in the distribution. Merton’s Jump Stochastic volatility model has a discrete-time version for log-returns, \\(y_t\\), with jump times, \\(J_t\\), jump sizes, \\(Z_t\\), and spot stochastic volatility, \\(V_t\\), given by the dynamics \\[\\begin{align*}\n    y_{t} & \\equiv \\log \\left( S_{t}/S_{t-1}\\right) =\\mu + V_t \\varepsilon_{t}+J_{t}Z_{t} \\\\V_{t+1} & = \\alpha_v + \\beta_v V_t + \\sigma_v \\sqrt{V_t} \\varepsilon_{t}^v\n\\end{align*}\\] where \\(\\mathbb{P} \\left ( J_t =1 \\right ) = \\lambda\\), \\(S_t\\) denotes a stock or asset price and log-returns \\(y^t = (y_1,\\ldots,y_t)\\) are the log-returns. The errors \\((\\varepsilon_{t},\\varepsilon_{t}^v)\\) are possibly correlated bivariate normals. The investor must obtain optimal filters for \\((V_t,J_t,Z_t)\\), and learn the posterior densities of the parameters \\((\\mu, \\alpha_v, \\beta_v, \\sigma_v^2 , \\lambda )\\). These estimates will be conditional on the information available at each time.\n\nMotivation: Combining Brownian Motion and Jumps\nThe Lévy-Itô decomposition provides the theoretical foundation for modeling asset prices. Recall that any Lévy process can be decomposed into three fundamental components: deterministic drift, continuous Brownian fluctuations, and discrete jumps. In finance, this decomposition maps naturally to observed price dynamics:\n\nDrift (\\(\\mu t\\)): The expected return on the asset, reflecting long-term growth trends\nBrownian component (\\(\\sigma B_t\\)): Continuous price fluctuations driven by the steady arrival of market information\n\nJump component (\\(\\sum_{i=1}^{N_t} Z_i\\)): Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks\n\nTraditional models like Black-Scholes use only the first two components, assuming constant volatility \\(\\sigma\\). However, empirical evidence overwhelmingly shows that volatility itself is stochastic and exhibits its own patterns: it clusters (high volatility follows high volatility), it mean-reverts to long-run averages, and it can experience sudden jumps during crises.\nStochastic volatility models extend the Lévy-Itô framework by allowing the volatility parameter to follow its own stochastic process. The most general formulation combines:\n\nBrownian motion for continuous price and volatility fluctuations\nPoisson processes for rare but important jump events in both prices and volatility\nCorrelation structure to capture the leverage effect—the empirical observation that volatility tends to rise when prices fall\n\nThis integration of the two fundamental stochastic processes creates a flexible modeling framework capable of capturing the rich dynamics observed in financial markets.\n\nExample 7.5 (Financial Crashes and the Need for Jumps) Consider the distribution of daily S&P 500 returns. Under a Gaussian model with annualized volatility of 15%, a one-day drop of 5% should occur roughly once every 10,000 years. Yet such events occurred multiple times in recent decades: October 1987 (-20.5%), October 2008 (-9.0%), March 2020 (-12.0%). The empirical distribution exhibits heavy tails—extreme events occur far more frequently than predicted by the normal distribution.\nJump-diffusion models accommodate these events naturally. Instead of treating crashes as impossible outliers, they model them as rare but expected occurrences from the Poisson jump component. This provides more realistic risk assessment and option pricing, particularly for out-of-the-money puts that protect against market crashes.\n\n\n\nThe Stochastic Volatility Model\nThe basic stochastic volatility (SV) model extends the geometric Brownian motion of Black-Scholes by allowing volatility to evolve as a latent stochastic process. In continuous time, the log-price \\(\\log S_t\\) and its variance \\(v_t\\) jointly evolve as:\n\\[\\begin{align*}\nd\\log S_{t} &= \\mu dt + \\sqrt{v_t} dB_{t}^{s}  \\\\\nd\\log v_{t} &= \\kappa_{v}(\\theta_{v} - \\log v_t) dt + \\sigma_{v} dB_{t}^{v}\n\\end{align*}\\]\nwhere \\(B_{t}^{s}\\) and \\(B_{t}^{v}\\) are (potentially correlated) Brownian motions. The variance follows a mean-reverting process in logs with:\n\n\\(\\theta_v\\): Long-run average log-variance\n\\(\\kappa_v\\): Speed of mean reversion (how quickly volatility returns to its average)\n\\(\\sigma_v\\): Volatility of volatility (how much randomness in the volatility process)\n\nDiscretizing this model at daily or weekly intervals yields the discrete-time specification:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s}  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v} \\varepsilon_{t}^{v}\n\\end{align*}\\]\nwhere \\(y_t = \\log(S_t/S_{t-1})\\) are log-returns, \\(\\varepsilon_{t}^{s}, \\varepsilon_{t}^{v} \\sim N(0,1)\\) are standard normal innovations, and the parameters relate to the continuous-time specification via \\(\\alpha_v = \\kappa_v \\theta_v \\Delta\\) and \\(\\beta_v = 1 - \\kappa_v \\Delta\\) for time interval \\(\\Delta\\).\nThis model exhibits several desirable features:\n\nVolatility clustering: Since \\(\\log v_t\\) follows an AR(1), periods of high volatility tend to persist\nStationarity: The mean-reverting specification ensures volatility doesn’t explode or collapse to zero\nFlexibility: The correlation between \\(\\varepsilon_t^s\\) and \\(\\varepsilon_t^v\\) allows for leverage effects\n\nAn important empirical regularity in equity markets is that volatility tends to increase when prices fall—a phenomenon known as the leverage effect. While originally attributed to changing debt-to-equity ratios as stock prices move, it is now understood as a more general feature of risk dynamics.\nTo incorporate the leverage effect, we allow the innovations in returns and volatility to be correlated:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s}  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v}\\left[\\rho \\varepsilon_{t}^{s} + \\sqrt{1-\\rho^2} \\varepsilon_{t}^{v}\\right]\n\\end{align*}\\]\nwhere \\(\\rho &lt; 0\\) for equity returns. A negative return shock (\\(\\varepsilon_t^s &lt; 0\\)) directly increases log-volatility through the \\(\\rho\\) term, generating the observed inverse relationship between prices and volatility.\nWhile stochastic volatility captures the time-varying nature of market uncertainty, it still relies on continuous Brownian motion for price movements. To accommodate the extreme events and heavy tails observed in returns, we augment the model with jump components—invoking the full Lévy-Itô decomposition.\nThe stochastic volatility with jumps (SVJ) model extends the basic SV specification by adding a Poisson-driven jump process to returns:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s} + J_t Z_t  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v} \\varepsilon_{t}^{v}\n\\end{align*}\\]\nwhere:\n\n\\(J_t \\sim \\text{Bernoulli}(\\lambda)\\) indicates whether a jump occurs at time \\(t\\)\n\\(Z_t \\sim N(\\mu_Z, \\sigma_Z^2)\\) is the jump size when a jump occurs\n\\(\\lambda\\) is the jump intensity (probability of a jump per period)\n\nThe total variance of returns now decomposes into two sources:\n\\[\n\\text{Var}(y_t) = \\E{v_{t-1}} + \\lambda E[Z_t^2]\n\\]\nThe first term captures diffusive volatility from continuous fluctuations, while the second captures jump variance from discrete events. This allows the model to simultaneously fit the day-to-day variations (through \\(v_t\\)) and occasional crashes (through jumps).\nThe 2008 financial crisis revealed another important feature: volatility itself experiences sudden jumps. The VIX index (a measure of market volatility expectations) more than doubled in a matter of days during the Lehman Brothers collapse. To capture this, we extend the model to allow jumps in both returns and volatility.\nThe stochastic volatility with correlated jumps (SVCJ) model specifies:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s} + J_t Z_t  \\\\\nv_{t} &= \\alpha_{v} + \\beta_{v} v_{t-1} + \\sigma_{v}\\sqrt{v_{t-1}} \\varepsilon_{t}^{v} + J_t W_t\n\\end{align*}\\]\nwhere:\n\nThe same Bernoulli \\(J_t\\) triggers jumps in both returns and volatility (correlated jumps)\n\\(Z_t | W_t \\sim N(\\mu_Z + \\rho_J W_t, \\sigma_Z^2)\\) allows jump sizes to be correlated\n\\(W_t \\sim \\text{Exponential}(\\mu_W)\\) ensures volatility jumps are positive\n\nThe correlation parameter \\(\\rho_J &lt; 0\\) captures the empirical finding that large negative return jumps are typically accompanied by large positive volatility jumps. For example, during the March 2020 COVID-19 crash, the S&P 500 fell sharply while the VIX spiked to record levels.\nAn even more flexible specification, the stochastic volatility with independent jumps (SVIJ) model, allows jumps in returns and volatility to occur independently, governed by separate Poisson processes with intensities \\(\\lambda_Z\\) and \\(\\lambda_W\\). This provides maximum flexibility but requires more data to estimate reliably.\nTo understand the empirical importance of these model features, consider parameter estimates from S&P 500 daily returns (1980-1999):\n\n\n\nTable 7.7: Model features\n\n\n\n\n\nFeature\nSV\nSVJ\nSVCJ\nSVIJ\n\n\n\n\nStochastic volatility\n+\n+\n+\n+\n\n\nReturn jumps\n–\n+\n+\n+\n\n\nVolatility jumps\n–\n–\n+\n+\n\n\nIndependent jumps\n–\n–\n–\n+\n\n\n\n\n\n\nThe estimated average annualized volatility across models is remarkably stable (around 15%), closely matching the sample standard deviation of 16%. However, the decomposition of variance sources differs:\n\nSV model: All variation comes from the stochastic volatility component\nSVJ model: 85% from stochastic volatility, 15% from return jumps\nSVCJ model: 90% from stochastic volatility, 10% from return jumps\nSVIJ model: 92% from stochastic volatility, 8% from return jumps\n\nThe diminishing role of return jumps as we add volatility jumps reflects an important finding: much of what appears as “jumps in returns” in simpler models is actually driven by jumps in volatility. When volatility suddenly spikes, even Brownian motion can generate large price movements that might be misidentified as jumps.\nThe mean reversion parameter \\(\\kappa_v\\) also varies across specifications. In the SVCJ and SVIJ models, \\(\\kappa_v\\) roughly doubles compared to the SV model, indicating that volatility reverts more quickly when jumps account for sudden large moves. The volatility-of-volatility parameter \\(\\sigma_v\\) correspondingly falls, as jumps handle the extreme variations.\n\n\nBayesian Inference for Stochastic Volatility Models\nEstimating stochastic volatility models presents a significant challenge: the volatility \\(v_t\\) is never directly observed, appearing as a latent state variable. Classical maximum likelihood approaches require integrating out the entire volatility path, which is computationally intractable for nonlinear models with jumps.\nThe Bayesian approach via MCMC provides an elegant solution by treating the latent volatilities and jump indicators as parameters to be sampled alongside model parameters. The Clifford-Hammersley theorem structures the algorithm efficiently.\nFor the basic SV model with parameter vector \\(\\theta = (\\alpha_v, \\beta_v, \\sigma_v^2)\\) and latent volatilities \\(v = (v_1, \\ldots, v_T)\\), the joint posterior factors as:\n\\[\np(\\theta, v | y) \\propto p(y | v) p(v | \\theta) p(\\theta)\n\\]\nThe MCMC algorithm alternates between:\n\nParameter update: \\(p(\\theta | v, y)\\)\n\nGiven volatilities, returns are conditionally normal: \\(y_t | v_{t-1} \\sim N(\\mu, v_{t-1})\\)\nLog-volatilities follow AR(1): \\(\\log v_t | \\log v_{t-1} \\sim N(\\alpha_v + \\beta_v \\log v_{t-1}, \\sigma_v^2)\\)\nWith conjugate priors, conditional posteriors are standard (Normal-Inverse-Gamma)\n\nVolatility update: \\(p(v_t | v_{t-1}, v_{t+1}, \\theta, y)\\)\n\nThe conditional posterior for each \\(v_t\\) combines information from:\n\nThe likelihood \\(p(y_{t+1} | v_t)\\) (observed return depends on current volatility)\nThe state evolution \\(p(v_t | v_{t-1})\\) (Markov dynamics from previous period)\nThe forward evolution \\(p(v_{t+1} | v_t)\\) (Markov dynamics to next period)\n\nThis distribution is non-standard and requires Metropolis-Hastings sampling\n\n\nFor the jump-augmented models (SVJ, SVCJ, SVIJ), we additionally sample:\n\nJump indicators: \\(p(J_t | v, Z, \\theta, y)\\)\n\nEach \\(J_t \\in \\{0,1\\}\\) follows a Bernoulli posterior\nLarge observed returns increase the probability of \\(J_t = 1\\)\n\nJump sizes: \\(p(Z_t | J_t = 1, v, \\theta, y)\\)\n\nConditional on a jump occurring, the jump size has a Normal posterior\nThe posterior mean balances the jump prior and the size needed to explain the observed return\n\n\nThis modular structure allows us to build up from simpler models (SV) to more complex specifications (SVIJ) by adding components one at a time, reusing the same basic algorithmic building blocks.\nStochastic volatility models with jumps have become standard tools in quantitative finance for several applications:\nOption Pricing: The Black-Scholes model systematically misprices options, particularly out-of-the-money puts. The volatility smile—the observation that implied volatilities increase for strikes far from the current price—reflects the market’s recognition of jump risk and stochastic volatility. Jump-diffusion models with stochastic volatility can reproduce these patterns, providing more accurate prices and hedging strategies.\nRisk Management: Value-at-Risk (VaR) and Expected Shortfall calculations based on constant-volatility Gaussian models dramatically underestimate tail risks. By properly accounting for stochastic volatility and jumps, firms can better quantify their exposure to extreme market movements. During the 2008 crisis, many institutions discovered their VaR models had severely underestimated potential losses.\nPortfolio Allocation: The presence of stochastic volatility creates hedging demands even for long-horizon investors. An investor who correctly anticipates that volatility is mean-reverting will reduce equity exposure when volatility is high (because expected returns are temporarily compressed) and increase exposure when volatility is low. This generates countercyclical trading strategies.\nMarket Timing: The predictable component of volatility can be exploited for tactical asset allocation. Since volatility tends to mean-revert, unusually high volatility signals elevated future returns (as compensation for risk), making it an opportune time to increase risky asset exposure. Conversely, unusually low volatility may warrant defensive positioning.\nThe integration of Brownian motion and Poisson processes through stochastic volatility models exemplifies how the Lévy-Itô decomposition provides not just mathematical elegance, but practical power for understanding and managing financial risk in modern markets.\n\n\n\n\nA. N. Kolmogorov. 1938. “On the Analytic Methods of Probability Theory.” Rossíiskaya Akademiya Nauk, no. 5: 5–41.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of Poincaré.” Russian Mathematical Surveys 61 (1): 1.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market Prices. MIT press.\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol. 11. Cambridge university press.\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling Association Football Scores and Inefficiencies in the Football Betting Market.” Journal of the Royal Statistical Society Series C: Applied Statistics 46 (2): 265–80.\n\n\nFeng, Guanhao, Nicholas G. Polson, and Jianeng Xu. 2016. “The Market for English Premier League (EPL) Odds.” Journal of Quantitative Analysis in Sports 12 (4). https://arxiv.org/abs/1604.03614.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen Methoden in Der Wahrscheinlichkeitsrechnung.” Mathematische Annalen 104 (1): 415–58.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity Theory.” https://arxiv.org/abs/physics/0408077.\n\n\nMetropolis, Nicholas. 1987. “The Beginning of the Monte Carlo Method.” Los Alamos Science 15: 125–30.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92.\n\n\nMetropolis, Nicholas, and Stanislaw Ulam. 1949. “The Monte Carlo Method.” Journal of the American Statistical Association 44 (247): 335–41.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de métaphysique Et de Morale 6 (1): 1–13.\n\n\nPolson, Nicholas. 1996. “Convergence of Markov Chain Monte Carlo Algorithms (with Discussion).” Bayesian Statistics 5: 297–321.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability Theory.” In Selected Works of a. N. Kolmogorov: Volume II Probability Theory and Mathematical Statistics, edited by A. N. Shiryayev, 62–108. Dordrecht: Springer Netherlands.\n\n\nStern, Hal S. 1994. “A Brownian Motion Model for the Progress of Sports Scores.” Journal of the American Statistical Association 89 (427): 1128–34.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009. “Estimating the Effect of the Red Card in Soccer: When to Commit an Offense in Exchange for Preventing a Goal Opportunity.” Journal of Quantitative Analysis in Sports 5 (1).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html",
    "href": "08-gp.html",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "8.1 Making Predictions with Gaussian Processes\nA Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is a powerful tool for modelling and prediction in many fields and is particularly useful for regression and classification tasks in machine learning. A finite collection of \\(n\\) points drawn from a Gaussian Process is completely specified by its \\(n\\)-dimensional mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\). In what follows we assume the process is indexed by a real variable \\(x\\in\\mathbb{R}\\) and has real-valued outputs. The mean of the process (and a finite collection of points) is defined by function \\(m(x)\\) and covariance is defined by function \\(k(x, x')\\), where \\(x\\) and \\(x'\\) are points in the index space.\nThe mean function defines the average value of the function at point \\(x\\), and the covariance function, also known as the kernel, defines the extent to which the values of the function at two points \\(x\\) and \\(x'\\) are correlated. In other words, the kernel function is a measure of similarity between two input points. The covariance between two points is higher if they are similar, and lower if they are dissimilar. Thus a Gaussian Process is completely specified by its mean function and covariance function, and an instance of a one-dimensional GP is a function \\(f(x): \\mathbb{R} \\rightarrow \\mathbb{R}\\). The typical notation is \\[\nf(x) \\sim \\mathcal{GP}(m(x), k(x, x')).\n\\] The mean function \\(m(x) = \\mathbb{E}[f(x)]\\) represents the expected value of the function at point \\(x\\), and the covariance function \\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\) describes the amount of dependence between the values of the function at two different points in the input space.\nIn practice, the mean function is often less important than the covariance function. It is common to assume a zero mean, \\(m(x)=0\\), and concentrate on specifying an appropriate covariance kernel. The kernel function is often chosen to be a function of the distance between the two points \\(|x-x'|\\) or \\(\\|x-x'\\|_2\\) in higher dimensions. The most commonly used kernel function is the squared exponential kernel, which is a function of the squared distance between the two points. The squared exponential kernel is given by: \\[\nk(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right)\n\\] where \\(\\sigma^2\\) is the variance parameter and \\(l\\) is the length scale parameter. The variance parameter controls the vertical variation of the function (its amplitude), whereas the length-scale parameter governs horizontal variation (the number and width of “bumps”). The length scale parameter determines how far apart two points must be to be considered dissimilar. The larger the length scale, the smoother the function. The length scale parameter is also called the bandwidth parameter. In this case the covariance decays exponentially with the distance between the points. Observe, that \\(k(x,x) = \\sigma^2\\) and \\(k(x,x') \\rightarrow 0\\) as \\(|x-x'| \\rightarrow \\infty\\).\nWe can illustrate a GP with a simulated example. First generate a sequence of 100 input points (indices)\nand then define the mean function and the covariance function\nThe covariance function depends only on the distance between two points, not on their absolute values. The squared exponential kernel is infinitely differentiable, which means that the GP is a very smooth function. The squared exponential kernel is also called the radial basis function (RBF) kernel. The covariance matrix is then defined as\nand we can generate a sample from the GP using the mvrnorm function from the MASS package and plot a sample\nFigure 8.1 displays 100 values of a function \\(f(x)\\) drawn from a GP with zero mean and a squared-exponential kernel at inputs \\(x=(0,0.1,0.2,\\ldots,10)\\). The realisation is smooth, with most values lying between -2 and 2. Because each diagonal element of the covariance matrix equals \\(\\sigma^2=1\\), the marginal variance is one. By properties of the normal distribution, approximately 95 percent of the points of \\(Y\\) should therefore fall within 1.96 standard deviations of the mean. The mild oscillations arise because values with neighbouring indices are highly correlated.\nWe can generate a few more samples from the same GP and plot them together\nEach finite sample path differs from the next, yet all share a similar range, a comparable number of bumps, and overall smoothness. That’s what it means to have function realizations under a GP prior: \\(Y = f(x) \\sim \\mathcal{GP}(0, k(x, x'))\\)\nSuppose our observed data, consisting of inputs \\(X=(x_1,\\ldots,x_n)\\) and outputs \\(Y=(y_1,\\ldots,y_n)\\), are a realisation of a GP. We can then use the GP to predict the outputs at new inputs \\(x_* \\in \\mathbb{R}^q\\). The joint distribution of the observed data \\(Y\\) and the new data \\(y_*\\) is given by \\[\n\\begin{bmatrix} Y \\\\ y_* \\end{bmatrix} \\sim \\mathcal{N} \\left ( \\begin{bmatrix} \\mu \\\\ \\mu_* \\end{bmatrix}, \\begin{bmatrix} K & K_* \\\\ K_*^T & K_{**} \\end{bmatrix} \\right )\n\\] where \\(K = k(X, X)\\in \\mathbb{R}^{n\\times n}\\), \\(K_* = k(X, x_*)\\in \\mathbb{R}^{n\\times q}\\), \\(K_{**} = k(x_*, x_*) \\in \\mathbb{R}^{q\\times q}\\), \\(\\mu = \\mathbb{E}[Y]\\), and \\(\\mu_* = \\mathbb{E}[y_*]\\). The conditional distribution of \\(y_*\\) given \\(y\\) is then given by \\[\ny_* \\mid Y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}).\n\\] The mean of the conditional distribution is given by \\[\n\\mu_{\\mathrm{post}} = \\mu_* + K_*^TK^{-1} (Y - \\mu)\n\\tag{8.1}\\] and the covariance is given by \\[\n\\Sigma_{\\mathrm{post}} = K_{**} - K_*^T K^{-1} K_*.\n\\tag{8.2}\\]\nEquation 8.1 and Equation 8.2 are convenient properties of a multivariate normal distribution.\nNow, instead of using GP to fit a known function (\\(\\sin\\)), we will apply it to a real-world data set. We will use the motorcycle accident data set from the MASS package. The data set contains accelerator readings taken through time in a simulated experiment on the efficacy of crash helmets.\nIn summary, Gaussian Processes provide a robust and flexible framework for modeling and predicting in situations where uncertainty and correlation among data points play a critical role. Their versatility and powerful predictive capabilities make them a popular choice in various scientific and engineering disciplines. GPs are considered non-parametric, which means they can model functions of arbitrary complexity. Through the choice of the kernel function, GPs can model a wide range of correlations between the data points. The mean and covariance functions can incorporate prior knowledge about the behavior of the function being modeled. There are many areas of applications for GP. The main applications include predictive modeling, optimization, and uncertainty quantification. We will focus on the first two applications in the later sections. In predictive modeling we can use GPs to predict the value of a function at new points, taking into account the uncertainty of the prediction. GPs are particularly useful in spatial data analysis, where the correlation between data points is often related to their physical distance. Thus, GPs are quite often used for environmental modeling to analyze temperature or pollution levels over geographical areas.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html#making-predictions-with-gaussian-processes",
    "href": "08-gp.html#making-predictions-with-gaussian-processes",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "Example 8.1 (Gaussian Process for \\(\\sin\\) function) We can use the GP to make predictions about the output values at new inputs \\(x_*\\). We use \\(x\\) in the [0,\\(2\\pi\\)] range and \\(y\\) to be the \\(y = \\sin(x)\\). We start by simulating the observed \\(x\\)-\\(y\\) pairs.\n\nn = 8; eps=1e-6\nX = matrix(seq(0, 2*pi, length=n), ncol=1)\nY = sin(X)\nK = outer(X[,1], X[,1], sqexpcov) + diag(eps, n)\n\nThe additive term diag(eps, n) \\(=\\epsilon I\\) adds a diagonal matrix with the small \\(\\epsilon\\) on the diagonal. This addition shifts the spectrum of \\(K\\) slightly and prevents eigenvalues from being exactly zero, ensuring that inverting or solving linear systems involving \\(K\\) remains numerically stable. In machine learning, this term is called jitter. Now we implement a function that calculates the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\nNow we generate a new set of inputs \\(x_*\\) and calculate the covariance matrices \\(K_*\\) and \\(K_{**}\\).\n\nq = 100\nXX = matrix(seq(-0.5, 2*pi + 0.5, length=q), ncol=1)\nKX = outer(X[,1], XX[,1],sqexpcov)\nKXX = outer(XX[,1],XX[,1], sqexpcov) + diag(eps, q)\n\nNotice that we did not add \\(\\epsilon I\\) to \\(K_*\\) = KX matrix, but we add it to \\(K_{**}\\) = KXX to guarantee that the resulting posterior covariance matrix is non-singular (invertible). Now we can calculate the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\n\nSi = solve(K)\nmup = t(KX) %*% Si %*% Y # we assume mu is 0\nSigmap = KXX - t(KX) %*% Si %*% KX\n\nNow, we can generate a sample from the posterior distribution over \\(y_*\\), given \\(Y\\)\n\nYY = mvrnorm(100, mup, Sigmap)\n\nUsing our convenience function plot_gp we can plot the posterior distribution over \\(y_*\\), given \\(Y\\).\n\nplot_gp = function(mup, Sigmap, X, Y, XX, YY){\n  q1 = mup + qnorm(0.05, 0, sqrt(diag(Sigmap)))\n  q2 = mup + qnorm(0.95, 0, sqrt(diag(Sigmap)))\n  matplot(XX, t(YY), type=\"l\", col=\"gray\", lty=1, xlab=\"x\", ylab=\"y\")\n  points(X, Y, pch=20, cex=2)\n  lines(XX, sin(XX), col=\"blue\")\n  lines(XX, mup, lwd=2)\n  lines(XX, q1, lwd=2, lty=2, col=2)\n  lines(XX, q2, lwd=2, lty=2, col=2)\n}\n\n\nplot_gp(mup, Sigmap, X, Y, XX, YY)\n\n\n\n\n\n\n\nFigure 8.3: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\n\n\n\n\nExample 8.2 (Gaussian Process for Simulated Data using MLE) In the previous example we assumed that the \\(x\\)-\\(y\\) relations are modeled by a GP with \\(\\sigma^2 = 1\\) and \\(2l^2 = 1\\). However, we can use the observed data to estimate those two parameters. In the context of GP models, they are called hyperparameters. We will use Maximum Likelihood Estimation (MLE) procedure to estimate those hyperparameters. The likelihood of data that follows a multivariate normal distribution is given by \\[\np(Y \\mid X, \\sigma, l) = \\frac{1}{(2\\pi)^{n/2} |K|^{1/2}} \\exp \\left ( -\\frac{1}{2} Y^T K^{-1} Y \\right )\n\\] where \\(K = K(X,X)\\) is the covariance matrix. We assume the mean is zero, to simplify the formulas. The log-likelihood is given by \\[\n\\log p(Y \\mid X, \\sigma, l) = -\\frac{1}{2} \\log |K| - \\frac{1}{2} Y^T K^{-1} Y - \\frac{n}{2} \\log 2\\pi.\n\\]\nWe can implement a function that calculates the log-likelihood of the data given the hyperparameters \\(\\sigma\\) and \\(l\\) and use optim function to find the maximum of the log-likelihood function.\n\nloglik = function(par, X, Y) {\n  sigma = par[1]\n  l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  return(-(-0.5 * log(det(K)) - 0.5 * t(Y) %*% Si %*% Y - (n/2)* log(2*pi)))\n}\npar = optim(c(1,1), loglik, X=X, Y=Y)$par\nprint(par)\n\n 1.5 2.4\n\n\nThe optim function returns the hyperparameters that maximize the log-likelihood function. We can now use those hyperparameters to make predictions about the output values at new inputs \\(x_*\\).\n\nl = par[2]; sigma = par[1]\npredplot = function(X, Y, XX, YY, l, sigma) {\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  KX = outer(X[,1], XX[,1],sqexpcov,l,sigma)\n  KXX = outer(XX[,1],XX[,1], sqexpcov,l,sigma) + diag(eps, q)\n  Si = solve(K)\n  mup = t(KX) %*% Si %*% Y # we assume mu is 0\n  Sigmap = KXX - t(KX) %*% Si %*% KX\n  YY = mvrnorm(100, mup, Sigmap)\n  plot_gp(mup, Sigmap, X, Y, XX, YY)\n}\npredplot(X, Y, XX, YY, l, sigma)\n\n\n\n\n\n\n\n\nWe can see that our uncertainty is much narrower—the posterior distribution is considerably tighter. This is because we used the observed data to estimate the hyperparameters. We can also see that the posterior mean is closer to the true function \\(y = \\sin(x)\\). Although our initial guess of \\(\\sigma^2 = 1\\) and \\(2l^2 = 1\\) was not too far off, the model fits the data much better when we use the estimated hyperparameters.\nThe function optim we used above uses a derivative-based optimization algorithm and when derivative is not provided by the user, it uses a numerical approximation. Although we can use numerical methods to calculate the derivative of the log-likelihood function, it is faster and more accurate to use analytical derivatives, when possible. In the case of the GP’s log-likelihood, the derivative can be analytically calculated. To do it, we need a couple of facts from matrix calculus. If elements of matrix \\(K\\) are functions of some parameter \\(\\theta\\), then \\[\n\\frac{\\partial  Y^T K^{-1} Y}{\\partial \\theta} =  Y^T \\frac{\\partial K^{-1}}{\\partial \\theta} Y.\n\\] The derivative of the inverse matrix \\[\n\\frac{\\partial K^{-1}}{\\partial \\theta} = -K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1}.\n\\] and the log of the determinant of a matrix \\[\n\\frac{\\partial \\log |K|}{\\partial \\theta} = \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ),\n\\] we can calculate the derivative of the log-likelihood function with respect to \\(\\theta\\) \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2}\\frac{\\partial \\log |K|}{\\partial \\theta}  + \\frac{1}{2} Y^T \\frac{\\partial K^{-1}}{\\partial \\theta}  Y.\n\\] Putting it all together, we get \\[\n\\frac{\\partial \\log p(Y \\mid X,\\theta)}{\\partial \\theta} = -\\frac{1}{2} \\mathrm{tr} \\left ( K^{-1} \\frac{\\partial K}{\\partial \\theta} \\right ) + \\frac{1}{2} Y^T K^{-1} \\frac{\\partial K}{\\partial \\theta} K^{-1} Y.\n\\] In the case of squared exponential kernel, the elements of the covariance matrix \\(K\\) are given by \\[\nK_{ij} = k(x_i, x_j) = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ).\n\\] The derivative of the covariance matrix with respect to \\(\\sigma\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial \\sigma} = 2\\sigma \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right );~\\frac{\\partial K}{\\partial \\sigma} = \\dfrac{2}{\\sigma}K.\n\\] The derivative of the covariance matrix with respect to \\(l\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial l} = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ) \\frac{(x_i - x_j)^2}{l^3};~ \\frac{\\partial K}{\\partial l}  = \\frac{(x_i - x_j)^2}{l^3} K.\n\\] Now we can implement a function that calculates the derivative of the log-likelihood function with respect to \\(\\sigma\\) and \\(l\\).\n\n# Derivative of the log-likelihood function with respect to sigma\ndloglik_sigma = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK = 2*K/sigma\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Derivative of the log-likelihood function with respect to l\ndloglik_l = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov ,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK =   outer(X[,1],X[,1], function(x, x1) (x - x1)^2)/l^3 * K\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Gradient function that returns a vector of derivatives\ngnlg = function(par,X,Y) {\n  return(c(dloglik_sigma(par, X, Y), dloglik_l(par, X, Y)))\n}\n\nNow we can use the optim function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.\n\npar1 = optim(c(1,1), fn=loglik, gr=gnlg ,X=X, Y=Y,method=\"BFGS\")$par\nl = par1[2]; sigma = par1[1]\nprint(par1)\n\n 1.5 2.4\n\n\nThe result is the same compared to when we called optim without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimization algorithm will be much faster.\nFurthermore, instead of coding our own derivative functions, we can use an existing package, such as the laGP package, developed by Bobby Gramacy to estimate the hyperparameters. The laGP package uses the same optimization algorithm we used above, but it also provides better selection of the covariance functions and implements approximate GP inference algorithms for large scale problems, when \\(n\\) becomes large and inversion of the covariance matrix \\(K\\) is prohibitively expensive.\n\nlibrary(laGP)\ngp = newGP(X, Y, 1, 0, dK = TRUE)\nres = mleGP(gp, tmax=20)\nl.laGP = sqrt(res$d/2)\nprint(l.laGP)\n\n 2.4\n\n\nIn the newGP function defines a Gaussian process with square exponential covariance function and assumes \\(\\sigma^2 = 1\\), then mleGP function uses optimization algorithm to maximize the log-likelihood and returns the estimated hyperparameters d = \\(2l^2\\), we can see that the length scale is close to the one we estimated above. We will use the predplot convenience function to calculate the predictions and plot the data vs fit.\npredplot(X, Y, XX, YY, l, sigma)\npredplot(X, Y, XX, YY, l.laGP, 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) MLE Fit\n\n\n\n\n\n\n\n\n\n\n\n(b) laGP Fit\n\n\n\n\n\n\n\nFigure 8.4: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that there is visually no difference between the two fits. Thus, it seems irrelevant whether we keep sigma fixed \\(\\sigma=1\\) or estimate it using MLE. However, in other applications when uncertainty is larger, the choice of \\(\\sigma\\) is important when we use GP for regression and classification tasks. Even for our example, if we ask our model to extrapolate\nXX1 = matrix(seq(-4*pi, 6*pi + 0.5, length=q), ncol=1)\npredplot(X, Y, XX1, YY, l, sigma)\npredplot(X, Y, XX1, YY, l.laGP, 1)\n\n\n\n\n\n\nMLE Fit\n\n\n\n\n\n\n\nlaGP Fit\n\n\n\n\n\n\nExtrapolation: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that outside of the range of the observed data, the model with \\(\\sigma=1\\) is more confident in its predictions.\n\n\n\nExample 8.3 (Gaussian Process for Motorcycle Accident Data) We first estimate the length scale parameter \\(l\\) using the laGP package.\n\nlibrary(MASS)\nX = mcycle$times\nY = mcycle$accel\ngp = newGP(matrix(X), Y, 2, 1e-6, dK = TRUE);\nmleGP(gp, tmax=10);\n\nNow we plot the data and the fit using the estimated length scale parameter \\(l\\).\n\nXX = matrix(seq(2.4, 55, length = 499), ncol=1)\np = predGP(gp, XX)\nN = 499\nq1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nggplot() + \n  geom_point(aes(x=X,y=Y)) + \n  geom_line(aes(x=XX,y=q3)) + \n  geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n\n\n\n\nMotorcycle Accident Data. Black line is the mean of the posterior distribution over \\(y_*\\), given \\(Y\\). Blue lines are the 95% confidence interval.\n\n\n\n\nWe can see that our model is more confident for time values between 10 and 30. The confidence interval is wider for time values between 0 and 10 and between 30 and 60, and less confident at the end close to the 60 mark. For some reason the acceleration values were not measured evenly. If we look at the histogram of time values, we can see that there are more data points in the middle of the time range.\n\nhist(X)\n\n\n\n\nHistogram of time values\n\n\n\n\nThe \\(\\sqrt{n}\\) decay in variance of the posterior distribution is a property of the squared exponential kernel.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "09-rl.html",
    "href": "09-rl.html",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "9.1 Multi-Armed Bandits\n“Information is the resolution of uncertainty.” Claude Shannon, 1948\nThus far we have discussed making decisions under uncertainty (Chapter 4 and Chapter 5) and two modes of data collection: field experiments and observational studies. In a field experiment we control the data‐generation process before the study begins, whereas in an observational study we have no such control and must work with whatever data are produced.\nWhat if we can choose which data to collect while the experiment is running? This leads to sequential (or adaptive) experimental design, in which each new observation is selected on the basis of the data gathered so far, creating a feedback loop between data generation and decision-making. The idea is illustrated in the following diagram:\nThere are multiple applications where this general framework can be applied and multiple algorithms that implement this concept. In this section, we will consider the most widely used among them:\nOne of the first practical demonstrations of reinforcement learning (then called trial-and-error learning) was Claude Shannon’s 1950s mechanical mouse Theseus, which learned to find its way through a maze.\nWhen the number of alternatives is large and the testing budget limited, a different approach to A/B testing —- the multi-armed bandit (MAB) can be more sample-efficient. Although the mathematical analysis and design of the experiments is slightly more complicated, the idea is simple. Instead of testing all alternatives at the same time, we test them sequentially. We start with a small number of alternatives and collect data. Based on the data we decide which alternative to test next. This way we can test more alternatives with the same number of samples. Thus, MABs allow us to explore and exploit simultaneously. MABs require that the outcome of each experiment is available immediately or with small enough delay to make a decision on the next experiment (Scott 2015). On the other hand, MABs are more sample efficient and allow us to find an optimal alternative faster. Thus, in the case when time is of the essence and there is an opportunity cost associated with delaying the decision, MABs are a better choice.\nFormally, there are \\(K\\) alternatives (arms) and each arm \\(a\\) is associated with a reward distribution \\(v_a\\), the value of this arm. The goal is to find the arm with the highest expected reward and accumulate the highest total reward in doing so. The reward distribution is unknown and we can only observe the reward after we select an arm \\(a\\) but we assume that we know the distribution \\(f_a(y\\mid \\theta)\\), where \\(a\\) is the arm index, \\(y\\) is the reward, and \\(\\theta\\) is the set of unknown parameters to be learned. The value of arm \\(v_a(\\theta)\\) is known, given \\(\\theta\\). Here are a few examples:\nA variation of the second example would include controlling for background variables, meaning adding covariates that correspond to variables that are not under our control. For example, we might want to control for the time of the day or the user’s location. This would allow us to estimate the effect of the design variables while controlling for the background variables. Another variation is to replace the binary outcome variable with a continuous variable, such as the time spent on the website or the amount of money spent on the website or count variable. In this case we simply use linear regression or another appropriate generalized linear model. Although at any given time, we might have our best guess about the values of parameters \\(\\hat \\theta\\), acting in a greedy way and choosing the arm with the highest expected reward \\(\\hat a = \\arg\\max_a v_a(\\hat \\theta)\\), we might be missing out on a better alternative. The problem is that we are not sure about our estimates \\(\\hat \\theta\\) and we need to explore other alternatives. Thus, we need to balance exploration and exploitation. A widely used and oldest approach for managing the explore/exploit trade-off in a multi-armed bandit problem is Thompson sampling. The idea is to use Bayesian inference to estimate the posterior distribution of the parameters \\(\\theta\\) and then sample from this distribution to select the next arm to test. The algorithm is as follows.\nAs the algorithm progresses, the posterior distributions of the arms are updated based on observed data, and arms with higher estimated success probabilities are favored for selection. Over time, Thompson Sampling adapts its beliefs and preferences, leading to better exploration and exploitation of arms.\nIt’s important to note that the effectiveness of Thompson Sampling depends on the choice of prior distributions and the updating process. The algorithm’s Bayesian nature allows it to naturally incorporate uncertainty and make informed decisions in the presence of incomplete information. The initial priors \\((\\alpha_i, \\beta_i)\\) can be set based on any available information about the arms, or as uniform distributions if no prior knowledge exists. This is a basic implementation of Thompson sampling. Variations exist for handling continuous rewards, incorporating side information, and adapting to non-stationary environments.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#multi-armed-bandits",
    "href": "09-rl.html#multi-armed-bandits",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "In online advertisements, we have \\(K\\) possible ads to be shown and probability of user clicking is given by vector \\(\\theta = (\\theta_1,\\ldots,\\theta_K)\\) of success probabilities for \\(K\\) independent binomial models, with \\(v_a(\\theta) = \\theta_a\\). The goal is to find the ad with the highest click-through rate.\nIn website design, we have two design variables, the color of a button (red or blue) and its pixel size (27 or 40), we introduce two dummy variables \\(x_c\\) for color, \\(x_s\\) for size and the probability of user clicking is given by \\[\n\\mathrm{logit} P(\\text{click} \\mid \\theta) = \\theta_0 + \\theta_x x_c + \\theta_s x_s + \\theta_{cs}x_cx_s,\n\\] with \\(v_a(\\theta) = P(\\text{click} \\mid \\theta)\\).\n\n\n\nInitial Beliefs. For each arm \\(k\\) (\\(k = 1,\\ldots,K\\)), define a prior belief about its reward distribution using a beta distribution with parameters \\(\\alpha_i\\) and \\(\\beta_i\\). These parameters represent the prior knowledge about the number of successes (\\(\\alpha_k\\)) and failures (\\(\\beta_k\\)) experienced with arm \\(k\\).\n\nSampling Parameters. For each arm \\(k\\), sample a reward \\[\n\\hat \\theta_k \\sim Beta(\\alpha_k, \\beta_k)\n\\]\nfrom the beta distribution. This simulates drawing a potential reward from the unknown true distribution of arm \\(i\\). A suboptimal greedy alternative is to select expected value of the parameter \\(\\hat \\theta_k = E(\\theta_k) =\\alpha_k/(\\alpha_k + \\beta_k)\\).\n\nChoosing the Best Arm. Select the arm \\(k\\) with the highest sampled reward \\(\\hat \\theta_k\\): \\[\na_t = \\arg\\max_i \\hat \\theta_k.\n\\]\nUpdating Beliefs. After observing the actual reward \\(R_t\\) for the chosen arm \\(a_t\\), update the parameters for that arm: \\[\n\\alpha_i = \\alpha_i + R_t, \\quad \\beta_i = \\beta_i + (1 - R_t)\n\\] This update incorporates the new information gained from the actual reward into the belief about arm \\(a_t\\)’s distribution.\nRepeat. Go back to step 2 and continue sampling, choosing, and updating until you reach a stopping point.\n\n\n\n\nWhen to End Experiments\nStep 2 of the TS algorithm can be replaced by calculating probability of an arm \\(a\\) being the best \\(w_{at}\\) and then choose the arm by sampling from the discrete distribution \\(w_{1t},\\ldots,w_{Kt}\\). The probability of an arm \\(a\\) being the best is given by \\[\nw_{at} = P(a \\text{ is optimal } \\mid y^t) = \\int P(a \\text{ is optimal } \\mid \\theta) P(\\theta \\mid y^t) d\\theta,\n\\] where \\(y^t = (y_1,\\ldots,y_t)\\) is the history of observations up to time \\(t\\). We can calculate the probabilities \\(w_{at}\\) using Monte Carlo. We can sample \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculate the probability as \\[\nw_{at}\\approx \\dfrac{1}{G}\\sum_{g=1}^G I(a = \\arg\\max_i v_i(\\theta^{(g)})),\n\\] where \\(I(\\cdot)\\) is the indicator function. This is simply the proportion of times the arm was the best in the \\(G\\) samples.\nAlthough using a single draw from posterior \\(p(\\theta \\mid y^t)\\) (as in the original algorithm) is equivalent to sampling proportional to \\(w_{at}\\), the explicitly calculated \\(w_{at}\\) yields a useful statistic that can be used to decide on when to end the experiment.\nWe will use the regret statistic to decide when to stop. Regret is the difference in values between the truly optimal arm and the arm that is apparently optimal at time \\(t\\). Although we cannot know the regret (it is unobservable), we can compute samples from its posterior distribution. We simulate the posterior distribution of the regret by sampling \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculating the regret as \\[\nr^{(g)} =  (v_a^*(\\theta^{(g)}) - v_{a^*_t}(\\theta^{(g)})),\n\\] Here \\(a^*\\) is the arm deemed best across all Monte Carlo draws and the first term is the value of the best arm within draw \\(g\\). We choose \\(a^*_t\\) as \\[\na^*_t = \\arg\\max_a w_{at}.\n\\]\nOften, it is convenient to measure the regret on the percent scale and then we use \\[\nr^{(g)} \\leftarrow r^{(g)}/v_{a^*_t}(\\theta^{(g)})\n\\]\nWe can demonstrate using simulated data. The function below generates samples \\(\\theta^{(g)}\\)\n\nbandit = function(x,n,alpha = 1,beta = 1,ndraws = 5000) {\n  set.seed(17) # Kharlamov\n  K &lt;- length(x) # number of bandits\n  prob &lt;- matrix(nrow = ndraws,ncol = K)\n  no = n - x\n  for (a in 1:K) {# posterior draws for each arm\n    prob[, a] = rbeta(ndraws,x[a] + alpha,no[a] + beta)\n  }\n  prob\n}\n\nSay we have three arms with 20, 30, and 40 sessions that have generated 12, 20, and 30 conversions. We assume a uniform prior for each arm \\(\\theta_i \\sim Beta(1,1)\\) and generate 6 samples from the posterior of \\(\\theta \\mid y^t\\).\n\nx = c(12,20,30)\nn = c(20, 30,40)\nprob = bandit(x, n,ndraws=6)\n\n\n\n\n\n\\(\\theta_1\\)\n\\(\\theta_2\\)\n\\(\\theta_3\\)\n\n\n\n\n1\n0.60\n0.63\n0.58\n\n\n2\n0.62\n0.62\n0.74\n\n\n3\n0.69\n0.53\n0.67\n\n\n4\n0.49\n0.59\n0.73\n\n\n5\n0.61\n0.51\n0.69\n\n\n6\n0.47\n0.64\n0.69\n\n\n\n\n\nNow, we calculate the posterior probabilities \\(w_{at} = P(a \\text{ is optimal } \\mid y^t)\\) for each of the three arms\n\nwat = table(factor(max.col(prob),levels = 1:3))/6\n\n\n\n\n1\n2\n3\n\n\n\n\n0.17\n0.17\n0.67\n\n\n\n\n\nThus far, the third arm is the most likely to be optimal, with probability 67%. Now, we calculate the regret for each of the six draws from the posterior of \\(\\theta \\mid y^t\\).\n\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0.09\n0\n0.03\n0\n0\n0\n\n\n\n\n\nWe compute value row by row by subtracting the largest element of that row from the element in column 3 (because arm 3 has the highest chance of being the optimal arm). All rows but 1 and 3 are zero. In the first row, the value is (0.63-0.58)/0.58 because column 2 is 0.05 larger than column 3. If we keep going down each row we get a distribution of values that we could plot in a histogram. We can generate one for a larger number of draws (10000).\n\nprob = bandit(x, n,ndraws=10000)\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\n\nwat = table(factor(max.col(prob),levels = 1:3))/10000\n\n\n\n\n1\n2\n3\n\n\n\n\n0.08\n0.2\n0.72\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\nArm 3 has a 72% probability of being the best arm, so the value of switching away from arm 3 is zero in 72% of the cases. The 95th percentile of the value distribution is the potential value remaining (CvR) in the experiment, which in this case works out to be about 16%.\n\nquantile(regret,0.95)\n\n 95% \n0.17 \n\n\nYou interpret this number as “We’re still unsure about the CvR for arm 3, but whatever it is, one of the other arms might beat it by as much as 16%.”\nGoogle Analytics, for example, “ends the experiment when there’s at least a 95% probability that the value remaining in the experiment is less than 1% of the champion’s conversion rate. That’s a 1% improvement, not a one percentage point improvement. So if the best arm has a conversion rate of 4%, then we end the experiment if the value remaining in the experiment is less than .04 percentage points of CvR”.\n\n\nContextual Bandits\nTraditional multi-armed bandit models, like the binomial model, assume independent observations with fixed reward probabilities. This works well when rewards are consistent across different groups and times. However, for situations with diverse user bases or fluctuating activity patterns, such as international audiences or browsing behavior, this assumption can be misleading.\nFor instance, companies with a global web presence may experience temporal effects as markets in Asia, Europe, and the Americas become active at different times of the day. Additionally, user behavior can change based on the day of the week, with people engaging in different browsing patterns and purchase behaviors. For example, individuals may research expensive purchases during work hours but make actual purchases on weekends.\nConsider an experiment with two arms, A and B. Arm A performs slightly better during the weekdays when users browse but don’t make purchases, while Arm B excels during the weekends when users are more likely to make purchases. If there is a substantial amount of traffic, the binomial model might prematurely conclude that Arm A is the superior option before any weekend traffic is observed. This risk exists regardless of whether the experiment is conducted as a bandit or a traditional experiment. However, bandit experiments are more susceptible to this issue because they typically run for shorter durations.\nTo mitigate the risk of being misled by distinct sub-populations, two methods can be employed. If the specific sub-populations are known in advance or if there is a proxy for them, such as geographically induced temporal patterns, the binomial model can be adapted to a logistic regression model. This modification allows for a more nuanced understanding of the impact of different factors on arm performance, helping to account for variations in sub-population behavior and temporal effects. \\[\n\\mathrm{logit} P(\\text{click}_a \\mid \\theta, x) = \\theta_{0a} + \\beta^Tx,\n\\] where \\(x\\) describes the circumstances or context of the observation. The success probability for selecting arm \\(a\\) under the context \\(x\\) is represented as \\(P(\\text{click}_a \\mid \\theta, x)\\). Each arm \\(a\\) has its own specific coefficient denoted as \\(\\beta_{0a}\\) with one arm’s coefficient set to zero as a reference point. Additionally, there is another set of coefficients represented as \\(\\beta\\) that are associated with the contextual data and are learned as part of the model. The value function can then be \\[\nv_a(\\theta) = \\mathrm{logit}^{-1}(\\beta_{0a}).\n\\]\nIf we lack knowledge about the crucial contexts, one option is to make the assumption that contexts are generated randomly from a context distribution. This approach is often exemplified by the use of a hierarchical model like the beta-binomial model. \\[\\begin{align*}\n\\theta_{at} &\\sim Beta(\\alpha_a,\\beta_a)\\\\\n\\text{click}_a \\mid \\theta &\\sim Binomial(\\theta_{at}),\n\\end{align*}\\] where \\(\\theta = \\{\\alpha_a,\\beta_a ~:~ a = 1,\\ldots,K \\}\\), with value function \\(v_a(\\theta) = \\alpha_a/\\beta_a\\)\n\n\nSummary of MAB Experimentation\nThe design phase begins with defining your arms by identifying the different options you want to evaluate, such as different website layouts, pricing strategies, or marketing campaigns. Next, choose a bandit algorithm that balances exploration and exploitation in various ways. Popular choices include Epsilon-greedy, Thompson Sampling, and Upper Confidence Bound (UCB). Then set your parameters by configuring the algorithm parameters based on your priorities and expected uncertainty. For example, a higher exploration rate encourages trying new arms earlier. Finally, randomize allocation by assigning users to arms randomly, ensuring unbiased data collection.\nDuring the analysis phase, track rewards by defining and measuring the reward metric for each arm, such as clicks, conversions, or profit. Monitor performance by regularly analyzing the cumulative reward and arm selection probabilities to see which arms are performing well and how the allocation strategy is adapting. Use statistical tools like confidence intervals or Bayesian methods to compare performance between arms and assess the significance of findings. Make adaptive adjustments by modifying the experiment based on ongoing analysis. You might adjust algorithm parameters, stop arms with demonstrably poor performance, or introduce new arms.\nStart with a small pool of arms to avoid information overload by testing a manageable number of options initially. Set a clear stopping criterion by deciding when to end the experiment based on a predetermined budget, time limit, or desired level of confidence in the results. Consider ethical considerations by ensuring user privacy and informed consent if the experiment involves personal data or user experience changes. Interpret results in context by remembering that MAB results are specific to the tested conditions and might not generalize perfectly to other contexts.\nBy following these steps and utilizing available resources, you can design and analyze effective MAB experiments to optimize your decision-making in various scenarios. Remember to adapt your approach based on your specific goals and context to maximize the benefits of this powerful technique.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bellman-principle-of-optimality",
    "href": "09-rl.html#bellman-principle-of-optimality",
    "title": "9  Reinforcement Learning",
    "section": "9.2 Bellman Principle of Optimality",
    "text": "9.2 Bellman Principle of Optimality\n\n“An optimum policy has the property that whatever the initial state and initial decision are, the remaining decision sequence must be optimum for the state resulting from the first decision.” – Richard Bellman\n\n\nExample 9.1 (Secretary Problem) The Secretary Problem, also known as the marriage problem or sultan’s dowry problem, is a classic problem in decision theory and probability theory. The scenario involves making a decision on selecting the best option from a sequence of candidates or options. The problem is often framed as hiring a secretary, but it can be applied to various situations such as choosing a house, a spouse, or any other scenario where you sequentially evaluate options and must make a decision.\nIn this problem, you receive \\(T\\) offers and must either accept or reject the offer “on the spot”. You cannot return to a previous offer once you have moved on to the next one. Offers are in random order and can be ranked against those previously seen. The aim is to maximize the probability of choosing the offer with the greatest rank. There is an optimal \\(r\\) (\\(1 \\le r &lt; T\\)) to be determined such that we examine and reject the first \\(r\\) offers. Then of the remaining \\(T - r\\) offers we choose the first one that is best seen to date.\nA decision strategy involves setting a threshold such that the first candidate above this threshold is hired, and all candidates below the threshold are rejected. The optimal strategy, known as the 37% rule, suggests that one should reject the first \\(r=T/e\\) candidates and then select the first candidate who is better than all those seen so far.\nThe reasoning behind the 37% rule is based on the idea of balancing exploration and exploitation. By rejecting the first \\(T/e\\) candidates, you gain a sense of the quality of the candidates but avoid committing too early. After that point, you select the first candidate who is better than the best among the initial \\(r\\) candidates.\nIt’s important to note that the 37% rule provides a probabilistic guarantee of selecting the best candidate with a probability close to 1/e (approximately 37%) as \\(T\\) becomes large.\nTo solve the secretary problem, we will use the principle of optimality due to Richard Bellman. The principle states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. In other words, the policy is optimal from the first decision onwards.\nThe solution to the secretary problem can be found via dynamic programming. Given an agent with utility function \\(u(x,d)\\), with current state \\(x\\), and decision \\(d\\). The law of motion of \\(x_t\\) is given by \\(x_{t+1} = p(x_t,d_t)\\). Bellman principle of optimality states that the optimal policy is given by the following recursion \\[\nV(x_t) = \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\mathbb{E} \\left [ V(x_{t+1}) \\right ] \\right \\}\n\\] where \\(\\gamma\\) is the discount factor. The optimal policy is given by \\[\nd_t^* = \\arg \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\mathbb{E} \\left [ V(x_{t+1}) \\right ] \\right \\}.\n\\]\nNow, back to the secretary problem. Let \\(y^t = (y_1,\\ldots,y_t)\\) denote the history of observations up to time \\(t\\). State \\(x_t=1\\) if the \\(t\\)th candidate is the best seen so far and \\(x_t=0\\) otherwise. The decision \\(d_t=1\\) if the \\(t\\)th candidate is hired and \\(d_t=0\\) otherwise. The utility function is given by \\(u(x_t,d_t) = x_t d_t\\).  The Bellman equation is given by \\[\nP(\\text{best of T}\\mid x_t=1) = \\dfrac{P(\\text{best of T})}{P(x_t=1)} = \\dfrac{1/T}{1/t} = \\dfrac{t}{T}.\n\\] The \\(t\\)th offer is the best seen so far places no restriction on the relative ranks of the first \\(t-1\\) offers. Therefore, \\[\np(x_t=1,y^{t-1}) = p(x_t=1)p(y^{t-1})\n\\] by the independence assumption. Hence, we have \\[\np(x_t=1 \\mid y^{t-1}) = p(x_t=1) = \\dfrac{1}{t}.\n\\]\nLet \\(p^*(x_{t-1}=0)\\) be the probability under the optimal strategy. Now we have to select the best candidate, given we have seen \\(t-1\\) offers so far and the last one was not the best or worse. The probability satisfies the Bellman equation \\[\np^*(x_{t-1}=0) = \\frac{t-1}{t} p^*(x_{t}=0) + \\frac{1}{t}\\max\\left(t/T, p^*(x_{t}=0)\\right).\n\\] This leads to \\[\np^*(x_{t-1}=0) = \\frac{t-1}{T} \\sum_{\\tau=t-1}^{T-1}\\dfrac{1}{\\tau}.\n\\]\nRemember, the strategy is to reject the first \\(r\\) candidates and then select the first. The probability of selecting the best candidate is given by \\[\nP(\\text{success}) = \\dfrac{1}{T}\\sum_{a=r+1}^T \\dfrac{r}{a} \\approx  \\dfrac{1}{T}\\int_{r}^{T}\\dfrac{r}{a} = \\dfrac{r}{T} \\log \\left ( \\dfrac{T}{r} \\right ).\n\\] We optimize over \\(r\\) by setting the derivative \\[\n\\frac{\\log \\left(\\frac{T}{r}\\right)}{T}-\\frac{1}{T}\n\\] to zero, to find the optimal \\(r=T/e\\).\nIf we plug in \\(r=T/e\\) back to the probability of success, we get \\[\nP(\\text{success}) \\approx \\dfrac{1}{e} \\log \\left ( e \\right ) = \\dfrac{1}{e}.\n\\]\nMonte Carlo Simulations\nSimulations are a powerful tool for making decisions when we deal with a complex system, which is difficult or impossible to analyze mathematically. They are used in many fields, including finance, economics, and engineering. They can also be used to test hypotheses about how a system works and to generate data for statistical analysis.\nWe start by showing how the secretary problem can be analyzed using simulations rather than analytical derivations provided above.\n\nset.seed(17) # Kharlamov\nnmc = 1000\nn = 1000\nsz = 300\nrules = round(n*seq(0.002,0.8,length.out = sz))\nrules = unique(rules[rules&gt;0])\nsz = length(rules)\ncnt = rep(0,sz)\nquality = rep(0,sz)\nfor (i in 1:sz)\n{\n  for (j in 1:nmc){\n    x = sample(1:n,n)\n    screen = x[1:(rules[i]-1)]\n    best_screen = max(screen)\n    xchoice = x[(rules[i]):n]\n    better_candidates = which(xchoice &gt; best_screen)\n    if (length(better_candidates)==0)\n      choice = x[n]\n    else\n      choice = xchoice[min(better_candidates)]\n    cnt[i] = cnt[i] + (choice == max(x))\n    quality[i] = quality[i] + choice\n  }\n}\nd = data.frame(cnt=cnt, quality=quality,nmc=nmc, rules=rules)\n\n\nplot(d$rules, d$cnt/d$nmc, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screened\", \n     ylab=\"Probability of Picking the Best\")\n\n\n\n\n\n\n\nplot(d$rules, d$quality/1000, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screened\", \n     ylab=\"Average Quality of Candidate\")",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#markov-decision-processes",
    "href": "09-rl.html#markov-decision-processes",
    "title": "9  Reinforcement Learning",
    "section": "9.3 Markov Decision Processes",
    "text": "9.3 Markov Decision Processes\nMarkov decision process (MDP) is a discrete time stochastic control process which provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. Almost all dynamic programming and reinforcement learning problems are formulated using the formalism of MDPs. MDPs are useful in various fields, including robotics, economics, and artificial intelligence. In fact, the multi-armed bandit problem considered before is a special case of MDP with one state. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard’s 1960 book, Dynamic Programming and Markov Processes.\nA Markov Decision Process (MDP) is a mathematical framework used for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful in various fields, including robotics, economics, and artificial intelligence, particularly in reinforcement learning. An MDP is defined by:\n\nStates (\\(S\\)): A set of states representing different scenarios or configurations the system can be in. The key assumption here is the Markov property, which states that the future is independent of the past given the present. This means that the decision only depends on the current state, not on the sequence of events that preceded it.\nActions (\\(A\\)): A set of actions available in each state.\nTransition Probability (\\(P\\)): \\(P(s', r | s, a)\\) is the probability of transitioning to state \\(s'\\), receiving reward \\(r\\), given that action \\(a\\) is taken in state \\(s\\).\nReward (\\(R\\)): A reward function \\(R(s, a, s')\\) that gives the feedback signal immediately after transitioning from state \\(s\\) to state \\(s'\\), due to action \\(a\\).\nDiscount Factor (\\(\\gamma\\)): A factor between 0 and 1, which reduces the value of future rewards.\n\n\nMathematical Representation\nThe states \\(s_t\\) and rewards \\(R_t\\) in MDP are indexed by time \\(t\\). The state at time \\(t+1\\) is distributed according to the transition probability \\[\nP(s_{t+1}\\mid s_t,a_t).\n\\] The reward function is \\(R_s^a = E[R_{t+1} \\mid s, a]\\).\nThe Markov property of the state is that the transition probability depends only on the current state and action and not on the history of states and actions. \\[\nP(s_{t+1}\\mid s_t,a_t) = P(s_{t+1}\\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots,s_0,a_0).\n\\] In other words, the future only depends on the present and not on the past history. The state is a sufficient statistic for the future.\nIn the case when the number of states is finite, we can represent the transition probability as a matrix \\(P_{ss'}^a = P(s_{t+1} = s' \\mid s_t = s, a_t = a)\\), where \\(s,s' \\in S\\) and \\(a \\in A\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1 \\[\nP^a = \\begin{bmatrix}\nP_{11}^a & P_{12}^a & \\cdots & P_{1|S|}^a \\\\\nP_{21}^a & P_{22}^a & \\cdots & P_{2|S|}^a \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP_{|S|1}^a & P_{|S|2}^a & \\cdots & P_{|S||S|}^a\n\\end{bmatrix}\n\\] The reward function is also a matrix \\(R_s^a = E[R_{t+1} \\mid s_t = s, a_t = a]\\).\nMarkov Reward Process\nWe can consider a simpler example of Markov Process. This is a special case of MDP when there is no action and the transition probability is simply a matrix \\(P_{ss'} = P(s_{t+1} = s' \\mid s_t = s)\\), where \\(s,s' \\in S\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1.\n\nExample 9.2 (Student Example) The graph below represents possible states (nodes) and transitions (links). Each node has reward assigned to it which corresponds to the reward function \\(R(s)\\). The transition probabilities are shown on the links. The graph is a Markov Chain, a special case of MDP with no actions.\n\n\n\n\n\n\n\n\ngraph LR\n    fb(\"Facebook; R=1\") --0.9--&gt; fb\n    fb--0.1--&gt;c1(\"Class 1; R=-2\")\n    c1--0.5--&gt;fb\n    c1--0.5--&gt;c2(\"Class 2; R=-2\")\n    c2--0.8--&gt;c3(\"Class 3; R=-2\")\n    c3--0.6--&gt;p(\"Pass; R=10\")\n    p--1.0--&gt;s(\"Sleep; R=0\")\n    c3--0.4--&gt;pub(\"Pub; R=3\")\n    pub--0.2--&gt;f(\"Fail; R=-20\")\n    f--1.0--&gt;s\n    pub--0.3--&gt;c3\n    pub--0.2--&gt;c1\n    pub--0.3--&gt;c2\n    c2--0.2--&gt;s\n\n\n\n\nFigure 9.2\n\n\n\n\n\nIf we are to pick an initial state and sample a trajectory (path on the graph above) by picking a random action at each state, we will get a random walk on the graph. The reward for each state is shown in the graph. The discounted value of the trajectory is then \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1},\n\\] where \\(\\gamma\\) is the discount factor. The discount factor is a number between 0 and 1 that determines the present value of future rewards. A discount factor of 0 makes the agent myopic and only concerned about immediate rewards. A discount factor of 1 makes the agent strive for a long-term high reward. The discount factor is usually denoted by \\(\\gamma\\) and is a parameter of the MDP. The discount of less than 1 is used to avoid infinite returns in cyclic Markov chains and allows us to discount less certain future rewards. The value of \\(\\gamma\\) is usually close to 1, for example 0.9 or 0.99. The value of \\(\\gamma\\) can be interpreted as the probability of the agent surviving from one time step to the next.\nWe can calculate sample returns \\(G_t\\) for this Markov Chain. We first read in the reward matrix\n\n# Reward function\nR = read.csv(\"../data/student-reward.tab\", header = T, sep = \"\\t\", row.names=1)\nt(R) %&gt;% knitr::kable()\n\n\nRewards\n\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\nReward\n-1\n-2\n-2\n-2\n3\n10\n-20\n0\n\n\n\n\ngetR = function(s) R[s,]\n\nand the transition probability matrix and the reward matrix\n\n# Read transition probability matrix\np = read.csv(\"../data/student-mdp.tab\", header = T, sep = \"\\t\", row.names=1)\nkbp = knitr::kable(p)\ngsub(0, ' ', kbp) # replace 0 with blank\n\n\n\n\n\nFacebook\nClass.1\nClass.2\nClass.3\nPub\nPass\nFail\nSleep\n\n\n\n\nFacebook\n.9\n.1\n.\n.\n.\n.\n.\n.\n\n\nClass 1\n.5\n.\n.5\n.\n.\n.\n.\n.\n\n\nClass 2\n.\n.\n.\n.8\n.\n.\n.\n.2\n\n\nClass 3\n.\n.\n.\n.\n.4\n.6\n.\n.\n\n\nPub\n.\n.2\n.3\n.3\n.\n.\n.2\n.\n\n\nPass\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\nFail\n.\n.\n.\n.\n.\n.\n1.\n.\n\n\nSleep\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\n\n\n\nNow we check that all of the rows sum to 1\n\np = as.matrix(p)\nrowSums(p) %&gt;% t() %&gt;% knitr::kable()\n\n\nTransition probability matrix row sums\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\nGiven the transition probability matrix, we can sample possible trajectories. First, we define a tr(s,m) convenience function that generates a trajectory of length m starting from state s\n\nset.seed(17)\n# Sample column s' using probabilities in row s \njump = function(s) sample(rownames(p), 1, prob = p[s,])\n# Function to generate a trajectory\ntr = function(s,m) {\n  traj = c(s)\n  for (i in 1:m) {\n    traj = c(traj, jump(traj[i]))\n  }\n  return(traj)\n}\n\nNow, we generate 6 trajectories of length 5 starting from state “Pub”\n\ntraj = t(replicate(6,tr(\"Pub\",5)))\nknitr::kable(traj)\n\n\n\n\nPub\nClass 3\nPub\nClass 2\nClass 3\nPass\n\n\nPub\nClass 2\nClass 3\nPass\nSleep\nSleep\n\n\nPub\nClass 2\nClass 3\nPub\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nClass 3\nPass\nSleep\nSleep\nSleep\n\n\n\n\n\nNow we can calculate the discounted value \\(G_t\\) of each of the trajectories\n\ntrajR = apply(traj,1:2, getR)\ndisc = 0.5^(0:5)\ntrajR %*% disc %&gt;% t() %&gt;% knitr::kable()\n\n\nDiscounted value of each trajectory\n\n\n2.7\n2.8\n0\n-16\n-16\n4.5\n\n\n\n\n\nWe can calculate the discounted value for 1000 trajectories\n\n# Value function of a trajectory\nvalue = function(s,m, gamma=0.5) {\n  traj = tr(s,m)\n  disc = gamma^(0:m)\n  return(sum(sapply(traj,getR) * disc))\n}\nvpub = replicate(1000,value(\"Pub\",6))\nhist(vpub)\n\n\n\n\n\n\n\nmean(vpub)\n\n -1.2\n\n\nWe can see that the distribution of discounted rewards is bimodal and depends on whether you get to “Fail” state or not.\n\nThe value of a state is the expected discounted reward starting from that state \\[\nV(s) = E[G_t \\mid s_t = s].\n\\] It evaluates the long-term value of state \\(s\\) (the goodness of a state). It can be drastically different from the reward value associated with the state. In our student example, the reward for the “Pub” state is 3, but the value is -1.2.\nThe value of a state can be calculated recursively using the Bellman equation \\[\\begin{align*}\nV(s) &= E[G_t \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s] \\\\\n&= E[R_{t+1} + \\gamma V(s_{t+1}) \\mid s_t = s] \\\\\n&= \\sum_{s'} P(s' \\mid s) \\left[R(s) + \\gamma V(s')\\right].\n\\end{align*}\\]\n\nExample 9.3 (Game of Chess as an MDP) We can consider a simple example of a game of chess.\nIn chess, a state \\(s\\) represents the configuration of the chessboard at any given time. This includes the positions of all the pieces (pawns, knights, bishops, rooks, queen, and king) for both players (white and black). Each possible arrangement of these pieces on the chessboard is a unique state. The game starts in a standard initial state (the standard chess setup) and progresses through a series of states as moves are made. If the game is played to completion, it ends in a terminal state (checkmate, stalemate, or draw). Also if we reach a state that has been seen before multiple times in a row, we can stop the game and declare a draw. Thus, we need to remember the states we have seen before and essentially expand the state space to include the number of times we have seen the state. In a timed game, the game can also end when a player runs out of time.\nActions \\(a\\) in chess are the legal moves that can be made by the player whose turn it is to move. This includes moving pieces according to their allowed movements, capturing opponent pieces, and special moves like castling or en passant. The set of actions available changes with each state, depending on the position of the pieces on the board.\nIn chess, the transition probability is deterministic for the most part, meaning that the outcome of a specific action (move) is certain and leads to a predictable next state. For example, moving a knight from one position to another (assuming it’s a legal move) will always result in the same new configuration of the chessboard. However, in the context of playing against an opponent, there is uncertainty in predicting the opponent’s response, which can be seen as introducing a probabilistic element in the larger view of the game.\nDefining a reward function \\(R\\) in chess can be complex. In the simplest form, the reward could be associated with the game’s outcome: a win, loss, or draw. Wins could have positive rewards, losses negative, and draws could be neutral or have a small positive/negative value. Alternatively, more sophisticated reward functions can be designed to encourage certain strategies or positions, like controlling the center of the board, protecting the king, or capturing opponent pieces.\nChess is a game of perfect information, meaning all information about the game state is always available to both players. While the number of states in chess is finite, it is extremely large, making exhaustive state analysis (like traditional MDP methods) computationally impractical.\nIn practice, solving chess as an MDP, especially using traditional methods like value iteration or policy iteration, is not feasible due to the enormous state space. Modern approaches involve heuristic methods, machine learning, and deep learning techniques. For instance, advanced chess engines and AI systems like AlphaZero use deep neural networks and reinforcement learning to evaluate board positions and determine optimal moves, but they do not solve the MDP in the classical sense.\n\nThe goal in an MDP is to find a policy \\(a = \\pi(s)\\) (a function from states to actions) that maximizes the sum of discounted rewards: \\[\nV^\\pi(s) = E_{\\pi}[G_t \\mid S_t = s],\n\\] where \\[\nG_t = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t), s_{t+1})\n\\]\nFunction \\(V^\\pi(s)\\) is the value of state s under policy \\(\\pi\\). Similarly we can define the action-value function \\(Q^\\pi(s,a)\\) as the value of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\): \\[\nQ^\\pi(s,a) = E_{\\pi}[G_t \\mid S_t = s, A_t = a].\n\\]\nBellman Equations for MDP simply state that the value of a state is the sum of the immediate reward and the discounted value of the next state \\[\nV^\\pi(s) = E_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1})\\mid S_t = s] = \\sum_{a\\in A}\\pi(a\\mid s)\\left(R_s^a + \\gamma \\sum_{s'\\in S}P^a_{ss'}V^pi(s') \\right).\n\\] The action-value function satisfies the following Bellman equation \\[\nQ^\\pi(s,a) = E_{\\pi}[R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1})\\mid S_t = s, A_t = a].\n\\] The value function can be defined as an expectation over the action-value function \\[\nV^\\pi(s) = E_{\\pi}[Q^\\pi(s,a)\\mid S_t = s] = \\sum_{a\\in A}\\pi(a\\mid s)Q^\\pi(s,a).\n\\] In matrix form, we have \\[\nQ^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^\\pi(s') = R_s^s + \\gamma \\sum_{s'\\in S}P_{ss'}^a\\sum_{a'\\in A}\\pi(a'\\mid s')Q^\\pi(s',a').\n\\] Now we can define the Bellman equation in the matrix form \\[\nV^\\pi = R^\\pi + \\gamma P^\\pi V^\\pi.\n\\] The direct solution is then \\[\nV^\\pi = (I - \\gamma P^\\pi)^{-1}R^\\pi.\n\\] The optimal value function \\(V^*(s)\\) is the value function for the optimal policy \\(\\pi^*(s)\\) \\[\nV^*(s) = \\max_\\pi V^\\pi(s).\n\\] The optimal action-value function \\(Q^*(s,a)\\) is the action-value function for the optimal policy \\(\\pi^*(s)\\) \\[\nQ^*(s,a) = \\max_\\pi Q^\\pi(s,a).\n\\] The optimal policy \\(\\pi^*(s)\\) is the policy that maximizes the value function \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a).\n\\] The optimal value function satisfies the Bellman optimality equation \\[\nV^*(s) = \\max_a Q^*(s,a).\n\\] and vice-versa \\[\nQ^*(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^*(s').\n\\]\nThe Bellman optimality equation is non-linear and is typically solved iteratively using Value Iteration, Policy Iteration, or Q-learning. Q-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example.\n\n\nMDP Solvers\nThe underlying approach behind all MDP solvers is to iteratively apply the Bellman equations until convergence. The main difference between the solvers is how they update the value function. All of them use dynamic programming approach to find optimal policy. Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure. If a problem can be solved by combining optimal solutions to non-overlapping subproblems, the strategy is called divide and conquer instead. This is why dynamic programming is applicable to solving MDPs.\nFirst, we consider how to find the values of states under a given policy \\(\\pi\\). We can iteratively apply Bellman expectation backup. We update the values using the following update rule \\[\nV_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a)[R(s, a, s') + \\gamma V_k(s')].\n\\] We will introduce the short-cut notation \\[  \nP_{ss'}^a = P(s' \\mid s, a), \\quad R_s^a = \\sum_{s'} P(s' \\mid s, a)R(s, a, s').\n\\] Then in matrix form the update rule becomes \\[\nV_{k+1} = R^{\\pi} + \\gamma P^{\\pi} V_k.\n\\]\nThe policy iteration algorithm involves two main steps: policy evaluation and policy improvement, which are iteratively applied until convergence. We start with an arbitrary value function, often initialized to zero for all states. \\[\\begin{align*}\nV_0(s) &= 0 \\\\\nV_{k+1} = & R^{\\pi} + \\gamma P^{\\pi} V_k\\\\\n\\pi_{k+1} &= \\arg\\max_a R^a + \\gamma P^a V_{k+1} = \\arg\\max_a Q^\\pi(s,a)\n\\end{align*}\\] The last step is to simply choose the action that maximizes the expected return in each state. Although it can be slow in practice, the convergence is guaranteed because the value function is a contraction mapping. We typically stop the iterations when the maximum change in the value function is below a threshold.\nIt can be used for calculating the optimal policy. The Bellman optimality equation is a fundamental part of finding the best policy in MDPs. \\[\nV^*(s) = \\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is then \\[\n\\pi^*(s) = \\arg\\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is the one that maximizes the value function. The optimal value function is the value function for the optimal policy. The optimal value function satisfies the Bellman optimality equation. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation.\nGiven an optimal policy, we can subdivide it into two parts: the optimal first action \\(A^*\\) and the optimal policy from the next state \\(S'\\). The optimal value \\(V^*\\) can be found using one-step lookahead \\[\nV^*(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s')\n\\] This allows us to define another approach to solving MDPs, called value iteration. The value iteration algorithm starts with an arbitrary value function and iteratively applies the Bellman optimality backup. The algorithm updates the value function using the following update rule \\[\nV_{k+1}(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V_k(s').\n\\] In matrix form, the update rule becomes \\[\nV_{k+1} = \\max_a R^a + \\gamma P^a V_k.\n\\] The algorithm stops when the maximum change in the value function is below a threshold. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation \\[\n\\pi^*(s) = \\arg\\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s').\n\\]\nIn practice, exactly solving the Bellman Expectation Equation in the policy evaluation step can be computationally expensive for large state spaces. Approximate methods may be used. Policy Iteration is particularly effective when the optimal policy needs to be very precise, as in high-stakes decision-making environments.\n\nExample 9.4 (MDP for a Maze) We use a mazemdp archive by Sally Gao, Duncan Rule, Yi Hao to demonstrate the value and policy iterations. Both are applied to the problem of finding an optimal policy for a maze. The maze is represented as a grid, with each cell either being a wall or empty. Agent (decision maker) does not know the maze structure and needs to find the optimal path from the start to the goal state. The agent starts in the bottom right corner and needs to reach the top left corner (marked as red). The agent can move up, down, left, or right, but not diagonally (actions). Moving into a wall keeps the agent in the same cell. Reaching the goal state gives a reward of +1, and all other transitions give a reward of 0. The goal is to find the optimal policy that maximizes the expected return (sum of discounted rewards) for the agent. In other words, the agent needs to find the shortest path to the exit.\nFigures below show the snapshot from policy (top row) and value (bottom row) iterations.\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=12\\)\n\n\n\n\n\n\n\n\\(t=24\\)\n\n\n\n\n\n\nFigure 9.3: Policy Iteration Solver\n\n\n\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=15\\)\n\n\n\n\n\n\n\n\\(t=30\\)\n\n\n\n\n\n\nFigure 9.4: Value Iteration Solver\n\n\n\nThe policy iterations converged after 24 iterations.\n\n\nExample 9.5 (MDP for Forest Management) We can consider one of the classic examples of a Markov Decision Process (MDP). Imagine you need to calculate an optimal policy to manage a forest to prevent possible fires. The goal is to decide between two possible actions to either ‘Wait’ or ‘Cut’. They correspond to balancing between ecological preservation and economic gain, considering the random event of a fire. We can break down the elements of this model.\n\nStates: Represent the age of the forest. The states are denoted as \\(\\{0, 1,\\ldots, S-1\\}\\), where 0 is the youngest state (just after a fire or cutting) and \\(S-1\\) is the oldest state of the forest.\nActions: There are two actions available:\n\n‘Wait’ (Action 0): Do nothing and let the forest grow for another year.\n‘Cut’ (Action 1): Harvest the forest, which brings immediate economic benefit but resets its state to the youngest.\n\nProbabilities: There’s a probability ‘p’ each year that a fire occurs, regardless of the action taken. If a fire occurs, the forest returns to state 0.\nTransition Matrix (P): This matrix defines the probabilities of moving from one state to another, given a specific action.\n\nWe will use mdp_example_forest function from the MDPtoolbox package to generate the transition probability matrix and reward matrix for the Forest example.\n\nlibrary(MDPtoolbox)\n# Define the transition and reward matrices for the Forest example\nres = mdp_example_forest(S=4,r1=10,r2=1,p=0.01)\n\nThis function generates a transition probability \\(P\\) of size \\((|A| \\times |S| \\times |S|\\), there are three states by default \\(S = \\{0,1,2\\}\\) and two actions.\nres$P[,,1]\nres$P[,,2]\n\n\n\n     [,1] [,2] [,3] [,4]\n[1,] 0.01 0.99 0.00 0.00\n[2,] 0.01 0.00 0.99 0.00\n[3,] 0.01 0.00 0.00 0.99\n[4,] 0.01 0.00 0.00 0.99\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n\n\n\nAs well as the reward matrix \\(R\\) of size \\(|S| \\times |A|\\).\n\nres$R\n\n     R1 R2\n[1,]  0  0\n[2,]  0  1\n[3,]  0  1\n[4,] 10  1\n\n\n\nmdp_value_iteration(res$P, res$R, discount = 0.9, epsilon = 1e-6, max_iter = 10000)\n\n \"MDP Toolbox WARNING: max_iter is bounded by  5000\"\n \"MDP Toolbox: iterations stopped, epsilon-optimal policy found\"\n\n\n 13 21 30 40\n\n$policy\n[1] 1 1 1 1\n\n$iter\n[1] 5\n\n$time\nTime difference of 0.0011 secs\n\n$epsilon\n[1] 1e-06\n\n$discount\n[1] 0.9\n\nmdp_policy_iteration(res$P, res$R, discount=0.9)\n\n  4.7  5.2  5.2 92.1\n\n$policy\n[1] 1 2 2 1\n\n$iter\n[1] 1\n\n$time\nTime difference of 0.011 secs\n\n\n\nA more general form of a value function is the action-value function \\(Q^\\pi(s,a)\\), which represents the expected return when starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter. \\[\nQ^\\pi(s,a) = E_{\\pi}\\left[G_t \\mid s_t = s, a_t = a\\right].\n\\] We can derive both the value and optimal policy functions from the action-value function: \\[\nV^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) Q^\\pi(s,a)\n\\] \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a)\n\\]\n\n\nModel-Free Methods\nBoth policy and value iterations we’ve considered thus far assume that transition probabilities between states given actions are known. However, this is often not the case in many real-world problems. Model-free methods learn through trial and error, by interacting with the environment and observing the rewards. The first method we consider is Monte Carlo methods. Monte Carlo methods for Markov Decision Processes (MDPs) are a class of algorithms used for finding optimal policies when the model of the environment (i.e., the transition probabilities and rewards) is unknown or too complex to model explicitly. These methods rely on learning from experience, specifically from complete episodes of interaction with the environment. Here’s a detailed look at how Monte Carlo methods work in the context of MDPs:\n\nGenerate Episodes: An episode is a sequence of states, actions, and rewards, from the start state to a terminal state. \\[\nS_0, A_0, R_1, S_1, A_1, R_2, \\ldots, S_{T-1}, A_{T-1}, R_T \\sim \\pi.\n\\] In Monte Carlo methods, these episodes are generated through actual or simulated interaction with the environment, based on a certain policy.\nEstimate Value Functions: Unlike dynamic programming methods, which update value estimates based on other estimated values, Monte Carlo methods update estimates based on actual returns received over complete episodes. This involves averaging the returns received after visits to each state. We use empirical mean to estimate the expected value.\nPolicy Improvement: After a sufficient number of episodes have been generated and value functions estimated, the policy is improved based on these value function estimates.\n\nMonte Carlo methods require complete episodes to update value estimates. This makes them particularly suitable for episodic tasks, where interactions naturally break down into separate episodes with clear starting and ending points. MC methods require sufficient exploration of the state space. This can be achieved through various strategies, like \\(\\epsilon\\)-greedy policies, where there’s a small chance of taking a random action instead of the current best-known action. In this case, the policy is given by \\[\n\\pi(a \\mid s) = \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n\\frac{\\epsilon}{|A|} & \\text{otherwise}\n\\end{cases}\n\\] where \\(\\epsilon\\) is the probability of taking a random action and \\(|A|\\) is the number of actions. The \\(\\epsilon\\)-greedy policy is an example of an exploration-exploitation strategy, where the agent explores the environment by taking random actions (exploration) while also exploiting the current knowledge of the environment by taking the best-known action (exploitation). The value of \\(\\epsilon\\) is typically decayed over time, so that the agent explores more in the beginning and exploits more later on.\nMonte Carlo methods are model-free, meaning they do not require a model of the environment (transition probabilities and rewards). They are also effective in dealing with high variance in returns, which can be an issue in some environments. However, they can be inefficient due to high variance and the need for many episodes to achieve accurate value estimates. They also require careful handling of the exploration-exploitation trade-off. The two main approaches for Monte Carlo methods are first-visit and every-visit methods.\n\nFirst-Visit MC: In this approach, the return for a state is averaged over all first visits to that state in each episode.\nEvery-Visit Monte Carlo: Here, the return is averaged over every visit to the state, not just the first visit in each episode.\n\nMonte Carlo Policy Iteration involves alternating between policy evaluation (estimating the value function of the current policy using Monte Carlo methods) and policy improvement (improving the policy based on the current value function estimate). This process is repeated until the policy converges to the optimal policy.\nTo find the optimal policy, a balance between exploration and exploitation must be maintained. This is achieved through strategies like \\(\\epsilon\\)-greedy exploration. In Monte Carlo Control, the policy is often improved in a greedy manner based on the current value function estimate.\nRecall that an arithmetic average can be updated recursively \\[\n\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\left(x_n + \\sum_{i=1}^{n-1} x_i\\right) = \\frac{1}{n}\\left(x_n + (n-1)\\bar{x}_{n-1}\\right) = \\bar{x}_{n-1} + \\frac{1}{n}(x_n - \\bar{x}_{n-1}).\n\\] This is called a running average. We can use this recursion to update the value function \\(V(s)\\) incrementally, each time we visit state \\(s\\) at time \\(t\\). \\[\nV(s_t) = V(s_t) + \\frac{1}{N(s_t)}(G_t - V(s_t)),\n\\] where \\(N(s_t)\\) is the number of times we visited state \\(s_t\\) before time \\(t\\) and \\(G_t\\) is the return at time \\(t\\). This is called first-visit Monte Carlo method. Alternatively, we can use every-visit Monte Carlo method, where we update the value function each time we visit state \\(s\\).\nAlternatively, we can use a learning rate \\(\\alpha\\) \\[\nV_{n+1} = V_n + \\alpha(G_n - V_n).\n\\] This is called constant step size update. The learning rate is a hyperparameter that needs to be tuned. The constant step size update is more convenient because it does not require keeping track of the number of visits to each state. The constant step size update is also more robust to non-stationary problems.\nTemporal Difference Learning (TD Learning) Similar to MC, TD methods learn directly from raw experience without a model of the environment. However, unlike MC methods, TD methods update value estimates based on other learned estimates, without waiting for the end of an episode. This is called bootstrapping. TD methods combine the sampling efficiency of Monte Carlo methods with the low variance of dynamic programming methods. They are also model-free and can learn directly from raw experience. However, they are more complex than MC methods and require careful tuning of the learning rate.\nA simple TD method is TD(0), which updates value estimates based on the current reward and the estimated value of the next state. The update rule is \\[\nV(S_t) = V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)),\n\\] where \\(\\alpha\\) is the learning rate. The TD(0) method is also called one-step TD because it only looks one step ahead. The \\(R_{t+1} + \\gamma V(S_{t+1})\\) term is called the TD target and is a biased estimate of \\(V(S_t)\\). The difference \\(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error. The TD target is an estimate of the return \\(G_t\\) and the TD error is the difference between the TD target and the current estimate \\(V(S_t)\\). Although TD algorithms have lower variance than MC methods, they have higher bias. In practice TD methods are more efficient than MC methods.\n\n\nQ-Learning\nQ-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example. The algorithm can be summarized as follows: \\[\nQ(S_t,A_t) = Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a) - Q(S_t,A_t)),\n\\] where \\(\\alpha\\) is the learning rate. The algorithm can be summarized as follows:\n\nInitialize \\(Q(s,a)\\) arbitrarily\nRepeat for each episode:\n\nInitialize \\(S\\)\nRepeat for each step of the episode:\n\nChoose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy)\nTake action \\(A\\), observe \\(R\\), \\(S'\\)\n\\(Q(S,A) = Q(S,A) + \\alpha(R + \\gamma \\max_a Q(S',a) - Q(S,A))\\)\n\\(S = S'\\)\n\nUntil \\(S\\) is terminal\n\n\nThen we can simplify the update rule to \\[\nQ(S_t,A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a)).\n\\]\n\nExample 9.6 (Q-Learning and Deal or No Deal) Deal or No Deal is a popular TV show where a contestant is presented with a number of sealed boxes, each containing a prize. The contestant selects a box and then proceeds to open the remaining boxes one by one. After a certain number of boxes have been opened, the banker makes an offer to buy the contestant’s box. The contestant can either accept the offer and sell the box or reject the offer and continue opening boxes. The game continues until the contestant either accepts an offer or opens all the boxes. The goal is to maximize the expected value of the prize in the contestant’s box. The rule of thumb is to continue as long as there are two large prizes left. Continuation value is large. For example, with three prizes and two large ones, risk averse people will naively choose deal, when if they incorporated the continuation value they would choose no deal.\nLet \\(s\\) denote the current state of the system and \\(a\\) an action. The \\(Q\\)-value, \\(Q_t(s,a)\\), is the value of using action \\(a\\) today and then proceeding optimally in the future. We use \\(a=1\\) to mean no deal and \\(a=0\\) means deal. The Bellman equation for \\(Q\\)-values becomes \\[\nQ_{t} ( s , a) = u( s , a  ) + \\sum_{ s^\\star } P( s^\\star | s ,a ) \\max_{ a } Q_{t+1} ( s^\\star , a )\n\\] where \\(u(s,a)\\) is the immediate utility of taking action \\(a\\) in state \\(s\\). The value function and optimal action are given by \\[\nV(s) = \\max_a Q ( s , a ) \\; \\; \\text{and} \\; \\;  a^\\star = \\text{argmax}  Q ( s , a )\n\\]\nTransition Matrix: Consider the problem where you have three prizes left. Now \\(s\\) is the current state of three prizes. \\[\ns^\\star = \\{ \\text{all \\; sets \\; of \\; two \\; prizes} \\} \\; \\; \\text{and} \\; \\; P( s^\\star | s, a =1) = \\frac{1}{3}\n\\] where the transition matrix is uniform to the next state. There’s no continuation for \\(P( s^\\star | s, a =0)\\).\nUtility: The utility of the next state depends on the contestant’s value for money and the bidding function of the banker \\[\nu( B ( s^\\star ) ) = \\frac{ B ( s^\\star )^{1-\\gamma} -1 }{1 - \\gamma }\n\\] in power utility case.\nExpected value implies \\(B( s ) = \\bar{s}\\) where \\(s\\) are the remaining prizes.\nThe website uses the following criteria: with three prizes left: \\[\nB( s) = 0.305 \\times \\text{big} + 0.5 \\times \\text{small}\n\\] and with two prizes left \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small}   \n\\]\nThree prizes left: \\(s = \\{ 750 , 500 , 25 \\}\\).\nAssume the contestant is risk averse with log-utility \\(U(x) = \\ln x\\). Banker offers the expected value we get \\[\nu( B( s = \\{ 750 , 500 , 25 \\}) ) = \\ln ( 1275/3  ) = 6.052\n\\] and so \\(Q_t ( s , a= 0 ) = 6.052\\).\nIn the continuation problem, \\(s^\\star = \\{ s_1^\\star , s_2^\\star , s_3^\\star  \\}\\) where \\(s_1^\\star = \\{750,500 \\}\\) and \\(s_2^\\star = \\{ 750,25 \\}\\) and \\(s_3^\\star = \\{ 500,25 \\}\\).\nWe’ll have offers \\(625 , 387.5 , 137.5\\) under the expected value. As the banker offers expected value the optimal action at time \\(t+1\\) is to take the deal \\(a=0\\) with Q-values given by \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  \\ln (625) + \\ln (387.5) + \\ln (262.5) \\right ) = 5.989\n\\end{align*}\\] as immediate utility \\(u( s,a ) = 0\\). Hence as \\[\nQ_{t} ( s , a=1)=5.989 &lt;  6.052 = Q_{t} ( s , a=0)\n\\] the optimal action is \\(a^\\star = 0\\), deal. Continuation value is not large enough to overcome the generous (expected value) offered by the banker.\nSensitivity analysis: we perform it by assuming different Banker’s bidding function. If we use the function from the website (2 prizes): \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small},\n\\] Hence \\[\\begin{align*}\nB( s_1^\\star = \\{750,500 \\}) &  = 516.25 \\\\\nB( s_2^\\star = \\{ 750,25 \\}) & =  278.75 \\\\\nB( s_3^\\star = \\{ 500,25 \\}) &  = 190  \n\\end{align*}\\]\nThe optimal action with two prizes left for the contestant is \\[\\begin{align*}\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (750) + \\ln (500) \\right ) = 6.415 \\\\\n& &gt; 6.246 = Q_{t+1} ( s_1^\\star , a=0) = \\ln \\left ( 516.25 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (750) + \\ln (25) \\right ) = 4.9194 \\\\\n& &lt; 5.63 = Q_{t+1} ( s_1^\\star , a=0)  = \\ln \\left ( 278.75 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\ln (500) + \\ln (25) \\right ) = 4.716 \\\\\n& &lt; 5.247 = Q_{t+1} ( s_1^\\star , a=0)  = \\left ( 516.25 \\right ) \\\\\n\\end{align*}\\] Hence future optimal policy will be no deal under \\(s_1^\\star\\), and deal under \\(s_2^\\star , s_3^\\star\\).\nTherefore solving for \\(Q\\)-values at the previous step gives \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  6.415+ 5.63 + 5.247 \\right ) = 5.764\n\\end{align*}\\] with a monetary equivalent as \\(\\exp(5.764  ) = 318.62\\).\nWith three prizes we have \\[\\begin{align*}\nQ_{t} ( s , a=0) & = u( B( s = \\{ 750 , 500 , 25 \\}) ) \\\\\n& = \\ln \\left ( 0.305 \\times 750 + 0.5 \\times 25 \\right ) \\\\\n& = \\ln ( 241.25 ) = 5.48.\n\\end{align*}\\] The contestant is offered $ 241.\nNow we have \\(Q_{t} ( s , a=1)= 5.7079  &gt; 5.48 = Q_{t} ( s , a=0)\\) and the optimal action is \\(a^\\star = 1\\), no deal. The continuation value is large. The premium is $ 241 compared to $319, a 33% premium.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bayesian-optimization",
    "href": "09-rl.html#bayesian-optimization",
    "title": "9  Reinforcement Learning",
    "section": "9.4 Bayesian Optimization",
    "text": "9.4 Bayesian Optimization\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is particularly useful when the objective function is expensive to evaluate. Bayesian optimization uses a surrogate model to approximate the objective function and an acquisition function to decide where to sample next. The surrogate model is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. Bayesian optimization is a global optimization method, meaning it does not require derivatives and can find the global optimum of the objective function. It is also sample-efficient, meaning it can find the optimum with fewer samples than other methods. However, it can be slow in practice and is not suitable for high-dimensional problems.\nGiven a function \\(f(x)\\) that is not known analytically (it can represent, for example, output of a complex computer program), the goal is to optimize \\[\nx^* = \\arg\\min_x f(x).\n\\]\nThe Bayesian approach to this problem is the following:\n\nDefine a prior distribution over \\(f(x)\\)\nCalculate \\(f\\) at a few points \\(x_1, \\ldots, x_n\\)\nRepeat until convergence:\n\nUpdate the prior to get the posterior distribution over \\(f(x)\\)\nChoose the next point \\(x^+\\) to evaluate \\(f(x)\\)\nCalculate \\(f(x^+)\\)\n\nPick \\(x^*\\) that corresponds to the smallest value of \\(f(x)\\) among evaluated points\n\nThe prior distribution is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The GP model is defined by a mean function \\(m(x)\\) and a covariance function \\(k(x,x')\\). The mean function is typically set to zero. The covariance function is typically a squared exponential function \\[\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2l^2}\\right),\n\\] where \\(\\sigma_f^2\\) is the signal variance and \\(l\\) is the length scale. The covariance function defines the similarity between two points \\(x\\) and \\(x'\\). The covariance function is also called a kernel function. The kernel function is a measure of similarity between inputs \\(x\\) and \\(x'\\).\nNow we need to decide where to sample next. We can use the acquisition function to decide where to sample next. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. The expected improvement (EI) function is a popular acquisition function. Suppose \\[\nf^* = \\min y\n\\] is the minimum value of \\(f(x)\\) among evaluated points. At a given point \\(x\\) and function value \\(y = f(x)\\), the expected improvement function is defined as \\[\na(x) = \\mathbb{E}\\left[\\max(0, f^* - y)\\right],\n\\] The function that we calculate expectation of \\[\nu(x) = \\max(0, f^* - y)\n\\] is the utility function. Thus, the acquisition function is the expected value of the utility function.\nThe acquisition function is high when \\(y\\) is likely to be lower than \\(f^*\\), and low when \\(y\\) is likely to be higher than \\(f^*\\). Given the GP prior, we can calculate the acquisition function analytically. The posterior distribution of Normal \\(y \\sim N(\\mu,\\sigma^2)\\), then the acquisition function is \\[\\begin{align*}\na(x) &= \\mathbb{E}\\left[\\max(0, f^* - y)\\right] \\\\\n&= \\int_{-\\infty}^{\\infty} \\max(0, f^* - y) \\phi(y,\\mu,\\sigma^2) dy \\\\\n&= \\int_{-\\infty}^{f^*} (f^* - y) \\phi(y,\\mu,\\sigma^2) dy\n\\end{align*}\\] where \\(\\phi(y,\\mu,\\sigma^2)\\) is the probability density function of the normal distribution. A useful identity is \\[\n\\int y \\phi(y,\\mu,\\sigma^2) dy =\\frac{1}{2} \\mu ~  \\text{erf}\\left(\\frac{y-\\mu }{\\sqrt{2} \\sigma }\\right)-\\frac{\\sigma\n   e^{-\\frac{(y-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi }},\n\\] where \\(\\Phi(y,\\mu,\\sigma^2)\\) is the cumulative distribution function of the normal distribution. Thus, \\[\n\\int_{-\\infty}^{f^*} y \\phi(y,\\mu,\\sigma^2) dy = \\frac{1}{2} \\mu (1+\\text{erf}\\left(\\frac{f^*-\\mu }{\\sqrt{2} \\sigma\n   }\\right))-\\frac{\\sigma  e^{-\\frac{(f^*-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi}} = \\mu \\Phi(f^*,\\mu,\\sigma^2) + \\sigma^2 \\phi(f^*,\\mu,\\sigma^2).\n\\]\nwe can write the acquisition function as \\[\na(x) = \\dfrac{1}{2}\\left(\\sigma^2 \\phi(f^*,\\mu,\\sigma^2) + (f^*-\\mu)\\Phi(f^*,\\mu,\\sigma^2)\\right)\n\\]\nWe can implement it\n\nacq &lt;- function(xx,p, fstar) {\n  x = matrix(xx, ncol=1)\n  d = fstar - p$mean\n  s = sqrt(diag(p$Sigma))\n  return(s*dnorm(d) + d*pnorm(d))\n}\n\n\nExample 9.7 (Taxi Fleet Optimisation) We will use the taxi fleet simulator from Emukit project. For a given demand (the frequency of trip requests) and the number of taxis in the fleet, it simulates the taxi fleet operations and calculates the profit. The simulator is a black-box function, meaning it does not have an analytical form and can only be evaluated at specific points. The goal is to find the optimal number of taxis in the fleet that maximizes the profit. We will use Bayesian optimization to solve this problem.\n\n\n\nTaxi Simulator Visualization\n\n\nWe start with initial set of three designs \\(x = (10,30,90)\\), where \\(x\\) is the number of the taxis in the fleet and observe the corresponding profits profit=(3.1,3.6,6.6). When \\(x=10\\), the demand for taxis exceeds the supply and passengers need to wait for their rides, leading to missed profit opportunities. At another extreme when we have 90 taxis, the profit is slightly better. However, there are many empty taxis, which is not profitable. The optimal number of taxis must be somewhere in the middle. Finally, we try 30 taxis and observe that the profit is higher than both of our previous attempts. However, should we increase or decrease the number of taxis from here? We can use Bayesian optimization to answer this question. First we define a convenience function to plot the GP emulator.\n\nplotgp = function(x,y,XX,p) {\n  q1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  ggplot() + \n    geom_point(aes(x=x,y=y)) + \n    geom_line(aes(x=XX,y=q3), col=2, lwd=2) + \n    geom_ribbon(aes(x=XX, ymin=q1, ymax=q2), fill=\"blue\", alpha=0.2)\n}\n\nNow, we fit the GP emulator using our initial set of observed taxi-profit pairs.\n\nlibrary(laGP)\nlibrary(mvtnorm)\nx = matrix(c(10,90,30), ncol=1)\nxx = seq(1,100, length=500)\nXX &lt;- matrix(xx, ncol = ncol(x))\nprofit = -c(3.1,3.6,6.6)\ngp &lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\np &lt;- predGP(gp, XX)\nplotgp(x,profit,XX,p)\n\n\n\n\n\n\n\n\nInstead of maximizing the profit, we minimize the negative profit. We see that there is potentially a better value at around 50 taxis. We can use the acquisition function to decide where to sample next. We define two functions: nextsample that uses the acquisition function to decide where to sample next and updgp that updates the GP emulator with the new sample. Then we call those two functions twice. First time, EI suggests 44 and second time it suggests 42. We update the GP emulator with the new samples and plot the updated emulator. We see that the GP emulator is updated to reflect the new samples.\nnextsample = function(){\n  ei = acq(xx,p,min(profit))\n  plot(xx,ei, type='l', col=2, lwd=2)\n  xnext = as.integer(xx[which.max(ei)])\n  return(xnext)\n}\nupdgp = function(xnext,f){\n  profit &lt;&lt;- c(profit, f)\n  x &lt;&lt;- c(x, xnext)\n  updateGP(gp, matrix(xnext,ncol=1), f)\n  p &lt;&lt;- predGP(gp, XX)\n  plotgp(x,profit,XX,p)\n}\nnextsample(); #44\nupdgp(44, -8.4);\nnextsample(); # 57\nupdgp(57, -7.1);\nnextsample(); # 45\nupdgp(45, -8.5);\nnextsample(); # 100\nupdgp(100, -3.3);\n\n\n\n\n\n\n\n\n\n\n 44\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 57\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 100\n\n\n\n\n\n\n\n\n\n\nIf we run nextsample one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and declare victory.\n\nnextsample()\n\n\n\n\n\n\n\n\n 47",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#concluding-remarks",
    "href": "09-rl.html#concluding-remarks",
    "title": "9  Reinforcement Learning",
    "section": "9.5 Concluding Remarks",
    "text": "9.5 Concluding Remarks\nIn this chapter, we explored the foundational concepts of Markov Decision Processes (MDPs) and their central role in reinforcement learning. We saw how MDPs provide a flexible mathematical framework for modeling sequential decision-making under uncertainty, with applications ranging from robotics and operations research to online recommendation systems and resource allocation in business.\nThrough both analytical derivations and Monte Carlo simulations, we examined classic problems such as the secretary problem and taxi fleet optimization, illustrating how simulation and Bayesian optimization can be used to make effective decisions in complex, uncertain environments. The use of Gaussian Process (GP) emulators and acquisition functions like Expected Improvement (EI) demonstrates the power of combining probabilistic modeling with principled exploration strategies—a hallmark of modern reinforcement learning.\nAs you continue your study of reinforcement learning, remember that the real world rarely presents us with simple, fully known models. The techniques introduced here—modeling uncertainty, simulating outcomes, and iteratively improving decisions—are essential tools for tackling the challenges of real-world AI and data-driven decision making. Whether you are optimizing ad placements, managing supply chains, or designing intelligent agents, the principles of MDPs and Bayesian optimization will serve as a strong foundation for your work.\n\n\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the Online Service Economy.” Applied Stochastic Models in Business and Industry 31 (1): 37–45.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. N. Kolmogorov. 1938. “On the Analytic Methods of Probability\nTheory.” Rossíiskaya Akademiya Nauk, no. 5:\n5–41.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of\nPoincaré.” Russian Mathematical\nSurveys 61 (1): 1.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nBumgarner, John M., Chad T. Lambert, Ayman A. Hussein, Daniel J.\nCantillon, Bryan Baranowski, Kathy Wolski, Bruce D. Lindsay, Oussama M.\nWazni, and Khaldoun G. Tarakji. 2018. “Smartwatch Algorithm for\nAutomated Detection of Atrial Fibrillation.” Journal of the\nAmerican College of Cardiology 71 (21): 2381–88.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market\nPrices. MIT press.\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol.\n11. Cambridge university press.\n\n\nDiaconis, Persi, and Frederick and Mosteller. 1989. “Methods for\nStudying Coincidences.” Journal of the American\nStatistical Association 84 (408): 853–61.\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling\nAssociation Football Scores and Inefficiencies\nin the Football Betting Market.” Journal of the\nRoyal Statistical Society Series C: Applied Statistics 46 (2):\n265–80.\n\n\nFeng, Guanhao, Nicholas G. Polson, and Jianeng Xu. 2016. “The\nMarket for English Premier League\n(EPL) Odds.” Journal of\nQuantitative Analysis in Sports 12 (4). https://arxiv.org/abs/1604.03614.\n\n\nFinetti, Bruno de. 1937. “Foresight: Its Logical\nLaws, Its Subjective Sources.” Annales de l’Institut Henri\nPoincaré 7 (1): 1–68.\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope,\nPilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al.\n2014. “Calorie\nIntake, Olive Oil Consumption and Mammographic Density Among\nSpanish Women.” International Journal of\nCancer 134 (8): 1916–25.\n\n\nGleick, James. 1992. Genius: The Life and\nScience of Richard Feynman. New York:\nPantheon Books.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nJeffreys, Harold. 1998. Theory of Probability.\nThird Edition, Third Edition. Oxford Classic Texts in the\nPhysical Sciences. Oxford, New York: Oxford University\nPress.\n\n\nKelly, J. L. 1956. “A New Interpretation of\nInformation Rate.” Bell System Technical\nJournal 35 (4): 917–26.\n\n\nKeynes, John Maynard. 1930. “Economic Possibilities for Our\nGrandchildren.” In Essays in Persuasion, 358–73. W. W.\nNorton & Company.\n\n\nKim, Young-Hoon, Jaehyung Shim, Hyoung-Seob Park, et al. 2024. “Diagnostic Accuracy\nof Single-Lead Handheld ECG Devices for Atrial Fibrillation\nDetection.” Journal of Cardiovascular\nElectrophysiology 35: 614–21.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen\nMethoden in Der\nWahrscheinlichkeitsrechnung.” Mathematische\nAnnalen 104 (1): 415–58.\n\n\n———. 1933. Grundbegriffe Der\nWahrscheinlichkeitsrechnung. Vol. 2. Ergebnisse Der\nMathematik Und Ihrer Grenzgebiete. Berlin:\nSpringer.\n\n\nKolmogorov, AN. 1942. “Definition of Center of Dispersion and\nMeasure of Accuracy from a Finite Number of Observations (in\nRussian).” Izv. Akad. Nauk SSSR Ser. Mat.\n6: 3–32.\n\n\nKolmogorov, Andrey N. 1933. Grundbegriffe Der\nWahrscheinlichkeitsrechnung. Berlin: Springer.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity\nTheory.” https://arxiv.org/abs/physics/0408077.\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs\nNakamura or Bayes Vs p-Value.” {{SSRN\nScholarly Paper}}. Rochester, NY.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nMetropolis, Nicholas. 1987. “The Beginning of the\nMonte Carlo Method.” Los Alamos Science 15:\n125–30.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth,\nAugusta H. Teller, and Edward Teller. 1953. “Equation of\nState Calculations by Fast Computing\nMachines.” The Journal of Chemical Physics 21\n(6): 1087–92.\n\n\nMetropolis, Nicholas, and Stanislaw Ulam. 1949. “The Monte\nCarlo Method.” Journal of the American Statistical\nAssociation 44 (247): 335–41.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n“Efficient Estimation of Word\nRepresentations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de\nmétaphysique Et de Morale 6 (1): 1–13.\n\n\nPolson, Nicholas. 1996. “Convergence of Markov Chain\nMonte Carlo Algorithms (with Discussion).”\nBayesian Statistics 5: 297–321.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nick, and Vadim Sokolov. 2025. “Negative\nProbability.” Applied Stochastic Models in\nBusiness and Industry 41 (1): e2910.\n\n\nPolson, Nick, Vadim Sokolov, and Jianeng Xu. 2023. “Quantum\nBayesian Computation.” Applied Stochastic Models\nin Business and Industry 39 (6): 869–83.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” The\nFoundations of Mathematics and Other Logical Essays, 156–98.\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the\nOnline Service Economy.” Applied Stochastic Models in\nBusiness and Industry 31 (1): 37–45.\n\n\nSellke, Thomas, M. J Bayarri, and James O Berger. 2001.\n“Calibration of ρ Values\nfor Testing Precise Null Hypotheses.” The\nAmerican Statistician 55 (1): 62–71.\n\n\nShen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad\nPatell, and Dhruv S Kazi. 2021. “Underperformance of Contemporary\nPhase III Oncology Trials and Strategies for\nImprovement.” Journal of the National Comprehensive Cancer\nNetwork 19 (9): 1072–78.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability\nTheory.” In Selected Works of a. N.\nKolmogorov: Volume II Probability Theory and\nMathematical Statistics, edited by A. N. Shiryayev, 62–108.\nDordrecht: Springer Netherlands.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nSimpson, Edward. 2010. “Edward Simpson:\nBayes at Bletchley Park.”\nSignificance 7 (2): 76–80.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to\nGo!” Significance 6 (4): 151–53.\n\n\nStern, Hal S. 1994. “A Brownian Motion Model for the\nProgress of Sports Scores.” Journal\nof the American Statistical Association 89 (427): 1128–34.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference\nabout Batter-Pitcher Matchups in Baseball from Small Samples.”\nStatistical Thinking in Sports, 153–65.\n\n\nSun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. “Why 90%\nof Clinical Drug Development Fails and How to Improve It?”\nActa Pharmaceutica Sinica B 12 (7): 3049–62.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The\nImpact of the Highly Improbable. Annotated\nedition. New York. N.Y: Random House.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nTuring, A. M. 1950. “Computing Machinery and Intelligence.”\nMind; a Quarterly Review of Psychology and Philosophy 59 (236):\n433–60.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.\n“Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nVeblen, Thorstein. 1899. The Theory of the Leisure Class:\nAn Economic Study of Institutions. New York:\nMacmillan.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009.\n“Estimating the Effect of the Red Card\nin Soccer: When to Commit an\nOffense in Exchange for\nPreventing a Goal Opportunity.”\nJournal of Quantitative Analysis in Sports 5 (1).\n\n\nWiener, Norbert. 1950. The Human Use of Human Beings:\nCybernetics and Society. Boston: Houghton Mifflin.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "References"
    ]
  }
]