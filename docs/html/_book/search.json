[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv. https://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "13-logistic.html",
    "href": "13-logistic.html",
    "title": "1  Logistic Regression",
    "section": "",
    "text": "1.1 Model Fitting\nClassification is a type of predictive modeling where the goal is to predict a categorical variable based on a set of input variables. A categorical variable is a variable that can take on only a limited number of values, such as 0 or 1, or it can be a multi-class variable, meaning it can take on more than two values. For example, in medical diagnosis, we might want to predict whether a patient has a disease (1) or not (0) based on symptoms and test results. A particularly important application is in self-driving cars, where computer vision systems must classify objects in real-time from camera feeds - distinguishing between pedestrians, other vehicles, traffic signs, and road obstacles to make safe driving decisions.\nGiven observed data \\(\\{(x_i,y_i)\\}_{i=1}^n\\), where each \\(y_i\\) is either 0 or 1, we start by assuming a binomial likelihood function for the response variable, defined as follows: \\[\nP(y_i = 1\\mid p_i) = p_i^{y_i} (1-p_i)^{1-y_i},\n\\] where \\(p_i\\) is the funciton of the inputs \\(x_i\\) and coefficients \\(\\beta\\) that gives us the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate \\(p_i\\) is to use logistic function \\[\\begin{align*}\nf_{\\beta}(x_i) = & \\beta^Tx_i\\\\\np_i  = & \\sigma(f_{\\beta}(x_i)) =  \\frac{e^{f_{\\beta}(x_i)}}{1+e^{f_{\\beta}(x_i)}},\n\\end{align*}\\]\nwhere \\(\\beta\\) is a vector of parameters. The logistic function \\(\\sigma(\\cdot)\\) is a function that maps any real number to a number between zero and one.\nThe we fit the model using Binomial log-likelihood minimisation. It leads us to the maximum likelihood estimator for parameters \\(\\beta\\) (a.k.a cross-entropy estimator), defined as \\[\n\\hat \\beta = \\arg\\min_{\\beta}\\mathcal{L}(\\beta),\n\\] where \\[\n\\mathcal{L}(\\beta) =  -\\sum_{i=1}^n \\left[ y_i \\log p_i  + (1-y_i) \\log \\left ( 1-p_i \\right ) \\right].\n\\] Similar to the least squares estimator, the cross-entropy estimator optimisaiton problem is convex, it has a unique solution.\nIn the unconditional case, when we do not observe any inputs \\(x\\), the cross-entropy estimator is again, the sample mean. If we take the derivative of the above expression with respect to \\(\\beta_0\\) and set it to zero, we get \\[\n- \\frac{d}{d\\beta_0}\\sum_{i=1}^n \\left[ y_i \\log \\left ( \\beta_0 \\right ) + (1-y_i) \\log \\left ( 1-\\beta_0 \\right ) \\right] = -\\sum_{i=1}^n \\left[ \\frac{y_i}{\\beta_0} - \\frac{1-y_i}{1-\\beta_0} \\right] = 0\n\\] which gives us the solution \\[\n\\hat{\\beta}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] which is the sample mean.\nUnlike the least squares estimator or in unconditional case, the system of equations \\[\n\\nabla \\mathcal{L}(\\beta) = 0\n\\] is not linear and cannot be solved by inverting a matrix. However, there are efficient iterative numerical optimization algorithms that can be used to find the optimal solution. The most common one is the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm. It is a quasi-Newton method that’s particularly well-suited for optimizing the cross-entropy loss function in logistic regression.\nIn the case when we have more than two classes \\(y \\in \\{1,\\ldots,K\\}\\), we simply build \\(K\\) models \\(f_{\\beta_1}(x),\\ldots, f_{\\beta_K}(x)\\), one for each class and then use the softmax function to convert the output of each model into a number between zero and one. The softmax function is defined as follows \\[\n\\mathrm{softmax}\\left(f_{\\beta_j}(x)\\right) = \\frac{\\exp(f_{\\beta_j}(x))}{\\sum_{i=1}^K \\exp(f_{\\beta_i}(x))}.\n\\] The softmax function is a generalization of the logistic function to the case of more than two classes. It is often used as the activation function in the output layer of neural networks for multi-class classification problems. It converts the output of each model into a probability distribution over the classes, making it suitable for multi-class classification with probabilistic outputs.\nThe vecotr of non-scaled outputs \\((f_{\\beta_1}(x),\\ldots, f_{\\beta_K}(x))\\) is called the logits.\nThe logistic function has a nice statistical interpretation. It is the CDF of the logistic distribution, which is a symmetric distribution with mean 0 and variance \\(\\pi^2/3\\), thus \\(p_i\\) is simply a value of this CDF, evaluated at \\(\\beta^Tx_i\\).\nFurther, Logistic regression models the log-odds (logit) of the probability as a linear function of the predictors, which aligns with the maximum likelihood estimation framework and provides desirable statistical properties. Specifically, if we invert the logistic function, \\[\np_i  = \\sigma(\\beta^Tx_i) =  \\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}},\n\\] we get the log-odds \\[\n\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta^Tx_i.\n\\] Meaning that \\(\\beta^Tx_i\\) measures how probability of \\(y_i = 1\\) changes with respect to the change in \\(x_i\\) on the log-odds scale. It allows us to interpret the model coefficients as the log-odds ratios of the response variable.\nIn some disciplines, such as econometrics, physcology and natural sciences, a normal CDF is used instead of the logistic CDF. It is done for historical reasons and because Normal CDF has slightly differnt assumptions about the data, that might be more natural in some cases.\nIn the case of the normal CDF, the model is called probit, it stands for probability unit, and the link function is called probit link. The probit model is defined as \\[\n\\Phi^{-1}(p_i) =  \\beta^Tx_i.\n\\] where \\(\\Phi(\\cdot)\\) is the normal CDF.\nThe term probit was coined in the 1930’s by biologists studying the dosage-cure rate link. We can fit a probit model using glm function in R.\nset.seed(92) # Kuzy\nx = seq(-3,3,length.out = 100)\ny = pnorm(x+rnorm(100))&gt;0.5\nprobitModel = glm(y~x, family=binomial(link=\"probit\"))\nmc = as.double(coef(probitModel))\n# we want to predict outcome for x = -1\nxnew = -1\n(yt = mc[1] + mc[2]*xnew)\n\n## [1] -0.86\n\n(pnorm(yt))\n\n## [1] 0.19\n\n(pred = predict(probitModel, list(x = c(xnew)), type=\"response\"))\n\n##    1 \n## 0.19\nnd = dnorm(mc[1] + mc[2]*x)\nplot(x,nd, type='l', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npolygon(c(-3,x[x&lt; -1],-1),c(0,nd[x&lt; -1],0), col=\"blue\")\nOur prediction is the blue area which is equal to 0.195.\nplot(x,y, type='p', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npred_probit = predict(probitModel, list(x=x), type=\"response\")\nlines(x,pred_probit, type='l')\nOutside of specific field, i.e. behavioral economics, the logistic function is much more popular of a choice compared to probit model. Besides that fact that is more intuitive to work with logit transform, it also has several nice properties when we deal with multiple classes (more then 2). Also, it is computationally easier then working with normal distributions. The density function of the logit is very similar to the probit one.\nlogitModel  = glm(y~x, family=binomial(link=\"logit\"))\npred_logit = predict(logitModel, list(x = x), type=\"response\")\nplot(x,pred_probit, pch=20, col=\"red\", cex=0.9, ylab=\"y\")\nlines(x,pred_logit, type='p', pch=20, cex=0.5, col=\"blue\")\nlines(x,y, type='p', pch=21, cex=0.5, bg=\"lightblue\")\nlegend(\"bottomright\",pch=20, legend=c(\"Logit\", \"Probit\"), col=c(\"blue\",\"red\"),y.intersp = 2)\nNotice, that the predict function returns a numeric value between 0 and 1. However, if we want to make a decision (to bet or not to bet), we need to have a binary outcome. A simple methods to move between the predicted probability and binary value is to use thresholding. \\[\n\\hat y_i = \\begin{cases}\n1 & \\text{if } \\hat p_i &gt; \\alpha \\\\\n0 & \\text{if } \\hat p_i \\leq \\alpha\n\\end{cases}\n\\] where \\(\\alpha\\) is a threshold value. A typical choice is \\(\\alpha = 0.5\\).\nNow let’s calculate the number of correct predictions using threshold \\(\\alpha = 0.5\\). R has a convinient table function that can summarise the counts of the predicted and actual values in a table.\ntable(NBA$favwin, as.integer(predict(nbareg, type=\"response\")&gt;0.5), dnn=c(\"Actual\", \"Predicted\"))\n\n##       Predicted\n## Actual   1\n##      0 131\n##      1 422\nOur model gets 0.7631103 of the predictions correctly. This number is called accuracy of the model.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#model-fitting",
    "href": "13-logistic.html#model-fitting",
    "title": "1  Logistic Regression",
    "section": "",
    "text": "Example 1.1 (Example: NBA point spread) We will use the NBA point spread data to illustrate the logistic regression. The data is available in the NBAspread.csv file. The data contains the point spread for each game in the NBA from 2013 to 2014 season. The data also contains the outcome of the game, whether the favorite won or not. The point spread is the number of points by which the favorite is expected to win the game and is predicted by the bookmakers. We simply want to see how well the point spread predicts the outcome of the game.\nWe start by loading the data and visualizing it.\n\nNBA = read.csv(\"../data/NBAspread.csv\")\nn = nrow(NBA)\nhead(NBA)\n\n\n\n\n\nfavwin\nfavscr\nundscr\nspread\nfavhome\nfregion\nuregion\n\n\n\n\n1\n72\n61\n7.0\n0\n3\n4\n\n\n1\n82\n74\n7.0\n1\n3\n1\n\n\n1\n87\n57\n17.0\n1\n3\n3\n\n\n0\n69\n70\n9.0\n1\n3\n3\n\n\n0\n77\n79\n2.5\n0\n2\n3\n\n\n1\n91\n65\n9.0\n0\n3\n4\n\n\n\n\n\n\nhist(NBA$spread[NBA$favwin==1], col=5, main=\"\", xlab=\"spread\")\nhist(NBA$spread[NBA$favwin==0], add=TRUE, col=6)\nlegend(\"topright\", legend=c(\"favwin=1\", \"favwin=0\"), fill=c(5,6), bty=\"n\")\nboxplot(NBA$spread ~ NBA$favwin, col=c(6,5), horizontal=TRUE, ylab=\"favwin\", xlab=\"spread\")\n\n\n\n\n\n\n\n\n\n\nDoes the Vegas point spread predict whether the favorite wins or not? The histogram shows the distribution of point spreads for games where the favorite won (turquoise) versus games where the favorite lost (purple). The boxplot provides another view of this relationship. Let’s fit a logistic regression model to quantify this relationship:\n\nnbareg = glm(favwin~spread-1, family=binomial, data=NBA)\nnbareg %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nspread\n0.16\n0.01\n11\n0\n\n\n\n\ns = seq(0,30,length=100)\nfit = exp(s*nbareg$coef[1])/(1+exp(s*nbareg$coef[1]))\nplot(s, fit, typ=\"l\", col=4, lwd=2, ylim=c(0.5,1), xlab=\"spread\", ylab=\"P(favwin)\")\n\n\n\n\n\n\n\n\nThe \\(\\beta\\) measures how our log-odds change, For this model, we have \\(\\beta = 0.156\\), meaning that for every one point increase in the point spread, the log-odds of the favorite winning increases by 0.156.\nNot, we can use the model to predict the probability of the favorite winning for a new game with a point spread of 8 or 4.\n\npredict(nbareg, newdata = data.frame(spread = c(8,4)), type = \"response\")\n\n##    1    2 \n## 0.78 0.65\n\n\nThe code above simply “Plugs-in” the values for the new game into our logistic regression \\[\n{ P \\left ( \\mathrm{ favwin}  \\mid  \\mathrm{ spread} \\right ) = \\frac{ e^{ \\beta x } }{ 1 + e^{\\beta x} } }\n\\] We can calculate it manually as well.\n\nexp(0.156*8)/(1+exp(0.156*8))\n\n## [1] 0.78\n\nexp(0.156*4)/(1+exp(0.156*4))\n\n## [1] 0.65\n\n\nCheck that when \\(\\beta =0\\) we have \\(p= \\frac{1}{2}\\).\nGiven our new values spread\\(=8\\) or spread\\(=4\\), the win probabilities are \\(78\\)% and \\(65\\)%, respectively. Clearly, the bigger spread means a higher chance of winning.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#confusion-matrix",
    "href": "13-logistic.html#confusion-matrix",
    "title": "1  Logistic Regression",
    "section": "1.2 Confusion Matrix",
    "text": "1.2 Confusion Matrix\nWe will analyse the tennis data set to show what is the decision boundary for the logistic regression model. The decision boundary is the line that separates the two classes. It is defined as the line where the probability of the favorite winning is 0.5. Then we will use the confusion matrix to evaluate the performance of the model.\n\nExample 1.2 (Logistic Regression for Tennis Classification) Data science plays a major role in tennis, you can learn about recent AI tools developed by IBM from this This Yahoo Article.\nWe will analyze the Tennis Major Tournament Match Statistics Data Set from the UCI ML repository. The data set has one per each game from four major Tennis tournaments in 2013 (Australia Open, French Open, US Open, and Wimbledon).\nLet’s load the data and familiarize ourselves with it\n\nd = read.csv(\"./../data/tennis.csv\")\ndim(d)\n\n## [1] 943  44\n\n\nLet’s look at the few coluns of the randomly selected five rows of the data\n\nd[sample(1:943,size = 5),c(\"Player1\",\"Player2\",\"Round\",\"Result\",\"gender\",\"surf\")]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer1\nPlayer2\nRound\nResult\ngender\nsurf\n\n\n\n\n532\nFlorian Mayer\nJuan Monaco\n1\n1\nM\nHard\n\n\n816\nL.Kubot\nJ.Janowicz\n5\n0\nM\nGrass\n\n\n431\nSvetlana Kuznetsova\nEkaterina Makarova\n1\n1\nW\nClay\n\n\n568\nMarcos Baghdatis\nGo Soeda\n1\n1\nM\nHard\n\n\n216\nMandy Minella\nAnastasia Pavlyuchenkova\n2\n0\nW\nHard\n\n\n\n\n\n\nWe have data for 943 matches and for each match we have 44 columns, including names of the players, their gender, surface type and match statistics. Let’s look at the number of break points won by each player. We will plot BPW (break points won) by each player on the scatter plot and will colorize each dot according to the outcome\n\nn = dim(d)[1]\nplot(d$BPW.1+rnorm(n),d$BPW.2+rnorm(n), pch=21, col=d$Result+2, cex=0.6, bg=\"yellow\", lwd=0.8,\n     xlab=\"BPW by Player 1\", ylab=\"BPW by Player 2\")\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\n\n\n\n\n\n\n\nWe can clearly see that number of the break points won is a clear predictor of the match outcome. Which is obvious and follows from the rules, to win a match, a player must win break points. Now, we want to understand the impact of a winning a break point on the overall match outcome. We do it by building a logistic regression model\n\nwhich(is.na(d$BPW.1)) # there is one row with NA value for the BPW.1 value and we remove it\n\n## [1] 171\n\nd = d[-171,]; n = dim(d)[1]\nm = glm(Result ~ BPW.1 + BPW.2-1, data=d, family = \"binomial\" )\nm %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nBPW.1\n0.40\n0.03\n15\n0\n\n\nBPW.2\n-0.42\n0.03\n-15\n0\n\n\n\n\n\nThe predicted values are stored in the fitted.values field of the model object. Those are the probabilities of player 1 winning the match. We need to convert them to binary predictions using \\(0.5\\) as a threshold for our classification.\n\ntable(d$Result, as.integer(m$fitted.values&gt;0.5), dnn=c(\"Actual\", \"Predicted\"))\n\n##       Predicted\n## Actual   0   1\n##      0 416  61\n##      1  65 400\n\n\nThis table shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. The first row shows the number of matches where player 1 won and the model predicted that player 1 won. The second row shows the number of matches where player 1 lost and the model predicted that player 1 lost. Thus, our model got (416+416)/942 = 0.8832272% of the predictions correctly! The accuracy is the ratio of the number of correct predictions to the total number of predictions.\nThis table is called confusion matrix. It is a table that shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. Formally, it is defined as\n\nConfusion Matrix. TPR - True Positive Rate, FPR - False Positive Rate, TNR - True Negative Rate, FNR - False Negative Rate.\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR\nFNR\n\n\nActual: NO\nFPR\nTNR\n\n\n\nEssentially, the logistic regresion is trying to draw a line that separates the red observations from the green one. In out case, we have two predictors \\(x_1\\) = BPW.1 and \\(x_2\\) = BPW.2 and our model is \\[\n\\log\\left(\\dfrac{p}{1-p}\\right) = \\beta_1x_1 + \\beta_2 x_2,\n\\] where \\(p\\) is the probability of player 1 winning the match. We want to find the line along which the probability is 1/2, meaning that \\(p/(1-p) = 1\\) and log-odds \\(\\log(p/(1-p)) = 0\\), thus the equation for the line is \\(\\beta_1x_1 + \\beta_2 x_2 = 0\\) or \\[\nx_2 = \\dfrac{-\\beta_1}{\\beta_2}x_1\n\\]\nLet’s see the line found by the glm function\n\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\nx = seq(0,30,length.out = 200)\ny  =  -m$coefficients[1]*x/m$coefficients[2]\nlines(x,y, lwd=2, col=\"red\") \n\n\n\n\n\n\n\n\nThere are a couple of observations. First, effect of a break point on the game outcome is significant and symmetric, effect of loosing break point is the same as the effect of winning one. We also can interpret the effect of winning a break point in the following way. We will keep BPW.2 = 0 and will calculate what happens to the probability of winning when BPW.1 changes from 0 to 1. The odds ration for player 1 winning when BPW.1 = 0 is exp(0) which is 1, meaning that the probability that P1 wins is 1/2. Now when BPW.1 = 1, the odds ratio is 1.5\n\nexp(0.4019)\n\n## [1] 1.5\n\n\nWe can calculate probability of winning from the regression equation \\[\n\\dfrac{p}{1-p} = 1.5,~~~p = 1.5(1-p),~~~2.5p = 1.5,~~~p = 0.6\n\\] Thus probability of winning goes from 50% to 60%, we can use predict function to get this result\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n##   1 \n## 0.5\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(1), BPW.2 = c(0)), type=\"response\")\n\n##   1 \n## 0.6\n\n\nWhat happens to the chances of winning when P1 wins three more break points compared to the opponent\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n\n##   1 \n## 0.5\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(3), BPW.2 = c(0)), type=\"response\")\n\n##    1 \n## 0.77\n\n\nChances go up by 27%.\nTennis is arguably the sport in which mean and women are treated equally. Both man and women matches are shown during the prime-time on TV, they both have the same prize money. However, one of the comments you hear often is that Women’s matches are “less predictable”, meaning that an upset (when the favorite looses) is more likely to happen in a women’s match compared to man Matches. We can test thus statement by looking at the residuals. The large the residual the less accurate our prediction was.\n\noutlind = which(d$res&lt;2)\nboxplot(d$res[outlind] ~ d$gender[outlind], col=c(2,3), xlab=\"Gender\",ylab=\"Residual\")\n\n\n\n\n\n\n\n\nLet’s do a formal T-test on the residuals foe men’s and women’s matches\n\nmen = d %&gt;% filter(res&lt;2, gender==\"M\") %&gt;% pull(res)\nwomen = d %&gt;% filter(res&lt;2, gender==\"W\") %&gt;% pull(res)\nt.test(men, women, alternative = \"two.sided\") %&gt;% tidy() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-0.07\n1.2\n1.3\n-4.7\n0\n811\n-0.11\n-0.04\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\nThe difference of \\(0.07\\) between men and women and the static value of \\(-4.7\\) means that the crowd wisdom that Women’s matches are less predictable is correct. The difference is statistically significant!",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#roc-curve-and-confounding-variables",
    "href": "13-logistic.html#roc-curve-and-confounding-variables",
    "title": "1  Logistic Regression",
    "section": "1.3 ROC Curve and Confounding Variables",
    "text": "1.3 ROC Curve and Confounding Variables\nUsing default data set, we will illustrate the concept of ROC curve and confounding variables.\n\nExample 1.3 (Credit Card Default) We will use the Default data set from the ISLR package to illustrate the logistic regression. The data set contains information on credit card defaults. Credit risk assessment is a major application of logistic regression and is widely used in the financial industry. In fact, the banks with the best credit risk assessment models are the ones that are able to offer the best interest rates to their customers and are winning the market.\nWe will also use this example to illustrate the concept of ROC (Receiver Operating Characteristic) curve. That helps us to understand the trade-off between sensitivity and specificity by varying the threshold \\(\\alpha\\).\nFirst, load the data set. We have 10,000 observations.\n\nDefault = read.csv(\"../data/CreditISLR.csv\", stringsAsFactors = T)\nhead(Default)\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n730\n44362\n\n\nNo\nYes\n817\n12106\n\n\nNo\nNo\n1074\n31767\n\n\nNo\nNo\n529\n35704\n\n\nNo\nNo\n786\n38464\n\n\nNo\nYes\n920\n7492\n\n\n\n\n\n\nThere are three predictors in the data set: balance, income and student. The balance variable represents the average credit card balance in dollars for each individual. This is the amount of money that a person owes on their credit card, which is a key predictor of whether they are likely to default on their credit card payments.\nIn the context of credit risk assessment, balance is one of the most important variables because it directly measures the amount of credit being utilized. Credit card companies and banks use this information, along with other factors like income and student status, to assess the likelihood that a customer will default on their payments. The logistic regression model we’re building will use this balance information to predict the probability of default, helping financial institutions make informed decisions about credit limits, interest rates, and risk management.\n\nglm.fit=glm(default~balance,data=Default,family=binomial)\nglm.fit %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.65\n0.36\n-29\n0\n\n\nbalance\n0.01\n0.00\n25\n0\n\n\n\n\n\nWe use it now to predict default for a new individual with a balance of $1000.\n\npredict.glm(glm.fit,newdata = list(balance=1000))\n\n##    1 \n## -5.2\n\n-1.065e+01 + 5.499e-03*1000\n\n## [1] -5.2\n\n\nNotice, that by default the predict function returns the log-odds. We can convert it to the probability using the logistic function.\n\npredict.glm(glm.fit,newdata = list(balance=1000), type=\"response\")\n\n##      1 \n## 0.0058\n\nexp(-1.065e+01 + 5.499e-03*1000)/(1+exp(-1.065e+01 + 5.499e-03*1000))\n\n## [1] 0.0058\n\n\nNow, let’s plot the predicted probability of default for a range of balances between 1000 and 3000.\n\nx = list(balance=seq(1000,to = 3000,length.out = 100))\ny = predict.glm(glm.fit,newdata = x, type=\"response\")\nplot(x$balance,y, pch=20, col=\"red\", xlab = \"balance\", ylab=\"Default\")\nlines(Default$balance, as.integer(Default$default)-1, type='p',pch=20)\n\n\n\n\n\n\n\n\nWe can calculate the confusion matrix for the model to see how well it predicts the default.\n\ny = predict.glm(glm.fit,newdata = Default, type=\"response\")\nconfusion_matrix = table(Default$default, as.integer(y&gt;0.2), dnn=c(\"Actual\", \"Predicted\"))\nconfusion_matrix\n\n##       Predicted\n## Actual    0    1\n##    No  9404  263\n##    Yes  134  199\n\n\nNotice, that instead of using the default threshold of 0.5, we used 0.2. This is a common practice in the financial industry. The lower the threshold, the more likely the model is to predict a default. In other words, we only approve a loan to a person if model predicts a default with probability 0.2 or lower.\nInstead of using counts in the confusion matrix, we can use rates\n\n# Convert to rates by dividing by row totals\nconfusion_matrix[1,] = confusion_matrix[1,] / sum(confusion_matrix[1,])  # Actual: NO row\nconfusion_matrix[2,] = confusion_matrix[2,] / sum(confusion_matrix[2,])  # Actual: YES row\n\nconfusion_matrix %&gt;% kable(digits=3)\n\n\n\n\n\n0\n1\n\n\n\n\nNo\n0.97\n0.027\n\n\nYes\n0.40\n0.598\n\n\n\n\n\nUsing the rates (proportions) is a more typical way to present the confusion matrix.\n\nFor our predictions, we used the value of \\(\\alpha=0.2\\) as a cut-off. What if I use smaller or larger \\(\\alpha\\), e.g. \\(\\alpha=0\\)? We wil use ROC curve to unswer this question. ROC curve shows the relationship between sensitivity (true positive rate) and 1 - specificity (false positive rate) for different classification thresholds. Here, 1 - specificity the proportion of negative cases that are incorrectly classified as positive. Thus, ROC curve shows both types of errors for different values of \\(\\alpha\\).\nFirst, we define a function, that calculates the ROC curve.\n\nroc &lt;- function(p,y, ...){\n  y &lt;- factor(y)\n  n &lt;- length(p)\n  p &lt;- as.vector(p)\n  alpha = seq(0,1,length.out = 100)\n  Q &lt;- p &gt; matrix(rep(alpha,n),ncol=100,byrow=TRUE)\n  specificity &lt;- colMeans(!Q[y==levels(y)[1],])\n  sensitivity &lt;- colMeans(Q[y==levels(y)[2],])\n  return(list(specificity=specificity, sensitivity=sensitivity, alpha=alpha))\n}\n\nNow, let’s plot the ROC curve.\n\n## roc curve and fitted distributions\npred = predict.glm(glm.fit,newdata = Default, type=\"response\")\nres = roc(p=pred, y=Default$default)\nplot(1-res$specificity, res$sensitivity, type=\"l\", bty=\"n\", main=\"in-sample\")\nabline(a=0,b=1,lty=2,col=8)\ndef = Default$default\n# our 1/5 rule cutoff\npoints(x= 1-mean((pred&lt;.2)[def==\"No\"]), \n       y=mean((pred&gt;.2)[def==\"Yes\"]), \n       cex=1.5, pch=20, col='red') \n## a standard `max prob' (p=.5) rule\npoints(x= 1-mean((pred&lt;.5)[def==\"No\"]), \n       y=mean((pred&gt;.5)[def==\"Yes\"]), \n       cex=1.5, pch=20, col='blue') \nlegend(\"bottomright\",fill=c(\"red\",\"blue\"),\n       legend=c(\"alpha=1/5\",\"alpha=1/2\"),bty=\"n\",title=\"cutoff\")\n\n\n\n\n\n\n\n\nROC curve shows the trade-off between sensitivity and specificity for different values of \\(\\alpha\\). The closer the curve is to the top-left corner, the better the model is. The diagonal line represents the random classifier (flip a coin for each prediction). The curve above the diagonal line is better than random, the curve below the diagonal line is worse than random.\nNow, let’s look at other predictors. We will add income and student to the model.\n\nglm.fit=glm(default~balance+income+student,data=Default,family=binomial)\nglm.fit %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.87\n0.49\n-22.08\n0.00\n\n\nbalance\n0.01\n0.00\n24.74\n0.00\n\n\nincome\n0.00\n0.00\n0.37\n0.71\n\n\nstudentYes\n-0.65\n0.24\n-2.74\n0.01\n\n\n\n\n\nThe p-values indicate that student is significant and income is not. However, the coefficient for student is negative. Meaning that students have lower probability of defaulting! This is counterintuitive, as one would expect that students have higher probability of defaulting. This is because the student variable and balance are confounded. We can see this by plotting the balance vs student.\n\nboxplot(balance~student,data=Default, ylab = \"balance\", col = c(2,3))\n\n\n\n\n\n\n\n\nWe can see that students have higher balance on average. This is not surprising, as students are typically younger and have lower income and many have student loans.\nConfounding occurs when the effect of one variable on the outcome is mixed with the effect of another variable, making it difficult to separate their individual contributions. In our case, student and balance are confounded.\nLet’s adjust for balance. We will plot the predicted probability of default for a range of balances between 1000 and 2500 for students and non-students.\n\nx1 = data.frame(balance = seq(1000,2500,length.out = 100), student = as.factor(rep(\"Yes\",100)), income=rep(40,100))\nx2 = data.frame(balance = seq(1000,2500,length.out = 100), student = as.factor(rep(\"No\",100)), income=rep(40,100))\ny1 = predict.glm(glm.fit,newdata = x1, type=\"response\")\ny2 = predict.glm(glm.fit,newdata = x2, type=\"response\")\nplot(x1$balance,y1, type='l', col=\"red\", xlab=\"Balance\", ylab = \"P(Default)\")\nlines(x2$balance,y2, type='l', col=\"black\")\nlegend(\"topleft\",bty=\"n\", legend=c(\"Not Student\", \"Student\"), col=c(\"black\",\"red\"), lwd=2)\n\n\n\n\n\n\n\n\nWe can see that for a given balance, students are actually less likely to default than non-students. This is because students have higher balance on average.\nTo summarise, what we’ve learned in this example, thus far. ROC curves visualize the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate) across different classification thresholds. Curves closer to the top-left corner indicate better model performance, while the diagonal line represents random classification (AUC = 0.5). Curves above the diagonal are better than random, below are worse. ROC curves help choose optimal classification thresholds based on business requirements rather than default 0.5 cutoff.\nConfounding occurs when the effect of one variable on the outcome is mixed with the effect of another variable. In the Default dataset, student status and balance are confounded. Students have higher average balances due to student loans and lower income. Without controlling for balance, students appear more likely to default. After adjusting for balance, students are actually less likely to default. The solution is to include confounding variables in the model to isolate individual effects. Always consider variable relationships when interpreting coefficients.\n\nNow, a natural question is how to choose the cut-off value \\(\\alpha\\)? Assume a bank is using our logistic regression model to predict probability of a loan default and would issue a loan if \\(p(y=1) &lt; \\alpha\\). Here \\(\\alpha\\) is the level of risk bank is willing to take. If bank chooses \\(\\alpha=1\\) and gives loans to everyone it is likely to loose a lot of money from defaulted accounts. If it chooses \\(\\alpha = 0\\) it will not issue loan to anyone and wont make any money. In order to choose an appropriate \\(\\alpha\\), we need to know what are the risks. Assume, bank makes $0.25 on every $1 borrowed in interest in fees and loose the entire amount of $1 if account defaults. This leads to the following pay-off matrix\n\nPay-off matrix for a loan\n\n\n\npayer\ndefaulter\n\n\n\n\nloan\n0.25\n-1\n\n\nno loan\n-0.25\n0\n\n\n\nWe are making 25 cents on a dollar on a good loan and loose everything on a default!\nGiven this pay-off matrix, we can calculate the expected profit for the bank.\n\npred = predict.glm(glm.fit,newdata = Default, type=\"response\")\nres = roc(p=pred, y=Default$default)\nalpha = res$alpha\nspecificity = res$specificity # correctly detect default\nsensitivity = res$sensitivity # correctly detect non-default\n# We loose money when we make a mistake. If we issue a loan to a non-defaulting customer, we loose $0.25. If we dont issue a loan to a defaulting customer, we loose $1.\nprofit = 0*specificity - 1*(1-specificity) + 0.25*sensitivity - 0.25*(1-sensitivity)\nplot(alpha, profit, type='l', xlab = \"alpha\", ylab = \"profit\")\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\nLet’s find the values of \\(\\alpha\\) for which profit is positive.\n\nmin(alpha[profit&gt;0])\n\n## [1] 0.02\n\nmax(alpha[profit&gt;0])\n\n## [1] 0.25\n\n\nWe have to be prudent and choose \\(\\alpha\\) in this range. Now let’s find \\(\\alpha\\) that maximises the profit.\n\n# Find index of max value in profit\nalpha[which.max(profit)]\n\n## [1] 0.051\n\n\nThus, to maximise the profit, we only approve the loan if we are 95% sure that the customer will not default.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#multinomial-logistic-regression",
    "href": "13-logistic.html#multinomial-logistic-regression",
    "title": "1  Logistic Regression",
    "section": "1.4 Multinomial logistic regression",
    "text": "1.4 Multinomial logistic regression\nSoftmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: \\(y_i \\in \\{0,1\\}\\) . We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle \\(y_i \\in \\{1,\\ldots ,K\\}\\) where \\(K\\) is the number of classes. Our model took the form: \\[\nf(x\\mid \\beta)=\\dfrac{1}{1+\\exp(-\\beta^Tx)}~,\n\\] and the model parameters \\(\\beta\\) were trained to minimize the loss function (negative log-likelihood) \\[\nJ(\\beta) = -\\left[ \\sum_{i=1}^m y_i \\log f(x\\mid \\beta) + (1-y_i) \\log (1-f(x\\mid \\beta)) \\right]\n\\]\nGiven a test input \\(x\\), we want our model to estimate the probability that \\(p(y=k|x)\\) for each value of \\(k=1,\\ldots ,K\\) Thus, our model will output a \\(K\\)-dimensional vector (whose elements sum to 1) giving us our K estimated probabilities. Concretely, our model \\(f(x\\mid \\beta)\\) takes the form: \\[\n\\begin{aligned}\nf(x\\mid \\beta) =\n\\begin{bmatrix}\np(y = 1 | x; \\beta) \\\\\np(y = 2 | x; \\beta) \\\\\n\\vdots \\\\\np(y = K | x; \\beta)\n\\end{bmatrix}\n=\n\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }}\n\\begin{bmatrix}\n\\exp(\\beta_1^{T} x ) \\\\\n\\exp(\\beta_2^{T} x ) \\\\\n\\vdots \\\\\n\\exp(\\beta_k^T x ) \\\\\n\\end{bmatrix}\\end{aligned}\n\\]\nHere \\(\\beta_i \\in R^n, i=1,\\ldots,K\\) are the parameters of our model. Notice that the term \\(1/ \\sum_{j=1}^{K}{\\exp(\\beta_k^T x) }\\) normalizes the distribution, so that it sums to one.\nFor convenience, we will also write \\(\\beta\\) to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent \\(\\beta\\) as an \\(n\\)-by-\\(K\\) matrix obtained by concatenating \\(\\beta_1,\\beta_2,\\ldots ,\\beta_K\\) into columns, so that \\[\n\\beta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n\\beta_1 & \\beta_2 & \\cdots & \\beta_K \\\\\n| & | & | & |\n\\end{array}\\right].\n\\]\nWe now describe the cost function that we’ll use for softmax regression. In the equation below, \\(1\\) is the indicator function, so that \\(1\\)(a true statement)=1, and \\(1\\)(a false statement)=0. For example, 1(2+3 &gt; 4) evaluates to 1; whereas 1(1+1 == 5) evaluates to 0. Our cost function will be: \\[\n\\begin{aligned}\nJ(\\beta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_i = k\\right\\} \\log \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}\\right]\n\\end{aligned}\n\\]\nNotice that this generalizes the logistic regression cost function, which could also have been written: \\[\n\\begin{aligned}\nJ(\\beta) &= - \\left[ \\sum_{i=1}^m   (1-y_i) \\log (1-f(x\\mid \\beta)) + y_i \\log f(x\\mid \\beta) \\right] \\\\\n&= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y_i = k\\right\\} \\log p(y_i = k | x_i ; \\beta) \\right]\\end{aligned}\n\\] The softmax cost function is similar, except that we now sum over the \\(K\\) different possible values of the class label. Note also that in softmax regression, we have that \\[\np(y_i = k | x_i ; \\beta) = \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) }.\n\\]\nSoftmax regression has an unusual property that it has a redundant set of parameters. To explain what this means, suppose we take each of our parameter vectors \\(\\beta_j\\), and subtract some fixed vector \\(\\psi\\). Our model now estimates the class label probabilities as \\[\n\\begin{aligned}\np(y_i = k | x_i ; \\beta)\n&= \\frac{\\exp((\\beta_k-\\psi)^T x_i)}{\\sum_{j=1}^K \\exp( (\\beta_j-\\psi)^T x_i)}  \\\\\n&= \\frac{\\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i) \\exp(-\\psi^T x_i)} \\\\\n&= \\frac{\\exp(\\beta_k^T x_i)}{\\sum_{j=1}^K \\exp(\\beta_k^T x_i)}.\\end{aligned}\n\\] In other words, subtracting \\(\\psi\\) does not affect our model’ predictions at all! This shows that softmax regression’s parameters are redundant. More formally, we say that our softmax model is overparameterized, meaning that for any model we might fit to the data, there are multiple parameter settings that give rise to exactly the same model function \\(f(x \\mid \\beta)\\) mapping from inputs \\(x\\) to the predictions.\nFurther, if the cost function \\(J(\\beta)\\) is minimized by some setting of the parameters \\((\\beta_1,\\ldots,\\beta_K)\\), then it is also minimized by \\((\\beta_1-\\psi,\\ldots,\\beta_K-\\psi)\\) for any value of \\(\\psi\\). Thus, the minimizer of \\(J(\\beta)\\) is not unique. Interestingly, \\(J(\\beta)\\) is still convex, and thus gradient descent will not run into local optima problems. But the Hessian is singular/non-invertible, which causes a straightforward implementation of Newton’s method to run into numerical problems. We can just set \\(\\psi\\) to \\(\\beta_i\\) and remove \\(\\beta_i\\).\n\nExample 1.4 (LinkedIn Study) How to Become an Executive(Irwin 2016; Gan and Fritzler 2016)?\nLogistic regression was used to analyze the career paths of about \\(459,000\\) LinkedIn members who worked at a top 10 consultancy between 1990 and 2010 and became a VP, CXO, or partner at a company with at least 200 employees. About \\(64,000\\) members reached this milestone, \\(\\hat{p} = 0.1394\\), conditional on making it into the database. The goals of the analysis were the following\n\nLook at their profiles – educational background, gender, work experience, and career transitions.\nBuild a predictive model of the probability of becoming an executive\nProvide a tool for analysis of “what if” scenarios. For example, if you are to get a master’s degree, how your jobs perspectives change because of that.\n\nLet’s build a logistic regression model with \\(8\\) key features (a.k.a. covariates): \\[\n\\log\\left ( \\frac{p}{1-p} \\right ) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_8x_8\n\\]\n\n\\(p\\): Probability of “success” – reach VP/CXO/Partner seniority at a company with at least 200 employees.\nFeatures to predict the “success” probability: \\(x_i (i=1,2,\\ldots,8)\\).\n\n\n\nVariable\nParameters\n\n\n\n\n\\(x_1\\)\nMetro region: whether a member has worked in one of the top 10 largest cities in the U.S. or globally.\n\n\n\\(x_2\\)\nGender: Inferred from member names: ‘male’, or ‘female’\n\n\n\\(x_3\\)\nGraduate education type: whether a member has an MBA from a top U.S. program / a non-top program / a top non-U.S. program / another advanced degree\n\n\n\\(x_4\\)\nUndergraduate education type: whether a member has attended a school from the U.S. News national university rankings / a top 10 liberal arts college /a top 10 non-U.S. school\n\n\n\\(x_5\\)\nCompany count: # different companies in which a member has worked\n\n\n\\(x_6\\)\nFunction count: # different job functions in which a member has worked\n\n\n\\(x_7\\)\nIndustry sector count: # different industries in which a member has worked\n\n\n\\(x_8\\)\nYears of experience: # years of work experience, including years in consulting, for a member.\n\n\n\n\nThe following estimated \\(\\hat\\beta\\)s of features were obtained. With a sample size of 456,000 thy are measured rather accurately. Recall, given each location/education choice in the “Choice and Impact” is a unit change in the feature.\n\nLocation: Metro region: 0.28\nPersonal: Gender(Male): 0.31\nEducation: Graduate education type: 1.16, Undergraduate education type: 0.22\nWork Experience: Company count: 0.14, Function count: 0.26, Industry sector count: -0.22, Years of experience: 0.09\n\nHere are three main findings\n\nWorking across job functions, like marketing or finance, is good. Each additional job function provides a boost that, on average, is equal to three years of work experience. Switching industries has a slight negative impact. Learning curve? lost relationships?\nMBAs are worth the investment. But pedigree matters. Top five program equivalent to \\(13\\) years of work experience!!!\nLocation matters. For example, NYC helps.\n\nWe can also personalize the prediction for predict future possible future executives. For example, Person A (p=6%): Male in Tulsa, Oklahoma, Undergraduate degree, 1 job function for 3 companies in 3 industries, 15-year experience.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\nPerson C (p=63%): Female in New York City, Top undergraduate program, Top MBA program, 4 different job functions for 4 companies in 1 industry, 15-year experience.\nLet’s re-design Person B.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\n\nWork in one industry rather than two. Increase \\(3\\)%\nUndergrad from top \\(10\\) US program rather than top international school. \\(3\\)%\nWorked for \\(4\\) companies rather than \\(2\\). Another \\(4\\)%\nMove from London to NYC. \\(4\\)%\nFour job functions rather than two. \\(8\\)%. A \\(1.5\\)x effect.\nWorked for \\(10\\) more years. \\(15\\)%. A \\(2\\)X effect.\n\nChoices and Impact (Person B) are shown below",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#imbalanced-data",
    "href": "13-logistic.html#imbalanced-data",
    "title": "1  Logistic Regression",
    "section": "1.5 Imbalanced Data",
    "text": "1.5 Imbalanced Data\nOften, you have much more observations with a specific label, such a sample is called imbalanced. This is a common problem in real-world classification tasks where one class significantly outnumbers the other(s). For example, in fraud detection, legitimate transactions vastly outnumber fraudulent ones; in medical diagnosis, healthy patients often outnumber those with rare diseases; and in manufacturing, defective products are typically much rarer than non-defective ones.\nWhen dealing with imbalanced data, you should avoid using accuracy as a metric to choose a model. Consider a binary classification problem with 95% of samples labeled as class 1. A naive classifier that simply assigns label 1 to every input will achieve 95% accuracy, making it appear deceptively good while being completely useless for practical purposes.\nInstead, more appropriate evaluation metrics should be used. The Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various classification thresholds. The Area Under the Curve (AUC) provides a single scalar value that measures the model’s ability to distinguish between classes, regardless of the chosen threshold. An AUC of 0.5 indicates random guessing, while 1.0 represents perfect classification.\nThe F1 score combines precision and recall into a single score, providing a balanced measure that penalizes models that are either too conservative or too aggressive: \\[\nF1 = 2\\dfrac{\\mathrm{precision} \\times \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n\\] where precision measures the proportion of true positives among predicted positives, and recall measures the proportion of true positives that were correctly identified.\nThe precision-recall curve is particularly useful for imbalanced datasets, as it plots precision against recall at various thresholds, focusing on the performance of the positive class. Cohen’s Kappa measures agreement between predicted and actual classifications while accounting for agreement by chance, making it more robust to class imbalance than accuracy.\nTo address imbalanced data, several strategies can be employed. Data-level approaches include oversampling, where you synthetically generate more samples of the minority class using techniques like bootstrap sampling with replacement, SMOTE (Synthetic Minority Over-sampling Technique) which creates synthetic examples by interpolating between existing minority class samples, or generative models like GANs or variational autoencoders to create realistic synthetic data. Undersampling reduces the majority class samples, which is particularly effective when the dataset is large enough. Hybrid approaches combine both oversampling and undersampling techniques.\nAlgorithm-level approaches include cost-sensitive learning, where you assign different misclassification costs to different classes, ensemble methods using techniques like bagging or boosting that can naturally handle imbalanced data, and threshold adjustment to modify the classification threshold to optimize for specific metrics like F1-score.\nThe choice of approach depends on the specific problem, available data, and computational resources. It’s often beneficial to experiment with multiple techniques and evaluate their performance using appropriate metrics rather than relying solely on accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#kernel-trick",
    "href": "13-logistic.html#kernel-trick",
    "title": "1  Logistic Regression",
    "section": "1.6 Kernel Trick",
    "text": "1.6 Kernel Trick\nThe kernel trick is particularly valuable when dealing with complex, non-linear patterns in data that cannot be separated by simple linear boundaries. Many real-world classification problems exhibit such non-linear relationships, making the kernel trick an essential tool in machine learning.\nThe key insight is that whilemany problems appear intractable in their original feature space, they often become linearly separable when mapped to higher-dimensional spaces using appropriate kernel functions. This transformation allows us to apply powerful linear classification methods to solve complex non-linear problems efficiently.\nKernel trick is a method of using a linear classifier to solve a non-linear problem. The idea is to map the data into a higher dimensional space, where it becomes linearly separable. The kernel trick is to use a kernel function \\(K(x_i,x_j)\\) to calculate the inner product of two vectors in the higher dimensional space without explicitly calculating the mapping \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\). The kernel function is defined as \\(K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\\). The most popular kernel functions are polynomial kernel \\(K(x_i,x_j) = (x_i^Tx_j)^d\\) and Gaussian kernel \\(K(x_i,x_j) = \\exp(-\\gamma||x_i-x_j||^2)\\). The kernel trick is used in Support Vector Machines (SVM) and Gaussian Processes (GP).\n\n\nCode\ngencircledata = function(numSamples,radius,noise) {\n    d = matrix(0,ncol = 3, nrow = numSamples); # matrix to store our generated data\n    # Generate positive points inside the circle.\n    for (i in 1:(numSamples/2) ) {\n    r = runif(1,0, radius * 0.4);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(0,x,y)\n    }\n\n    # Generate negative points outside the circle.\n    for (i in (numSamples/2+1):numSamples ) {\n    r = runif(1,radius * 0.8, radius);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(1,x,y)\n    }\n    colnames(d) = c(\"label\", \"x1\", \"x2\")\n    return(d)\n}\n\n\n\nd = gencircledata(numSamples=200, radius=1, noise=0.001)\nplot(d[,2],d[,3], col=d[,1]+1, pch=19, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\nFigure 1.1\n\n\n\n\n\nThe data on the left in Figure Figure 1.1 is clearly not linearly separable. However, if we map it to a three-dimensional space using the transformation: \\[\n\\begin{aligned}\n\\phi: R^{2} & \\longrightarrow R^{3} \\\\\n\\left(x_{1}, x_{2}\\right) & \\longmapsto\\left(z_{1}, z_{2}, z_{3}\\right)=\\left(x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}\\right),\n\\end{aligned}\n\\] and attempt to linearly separate the transformed data, the decision boundaries become hyperplanes in \\(R^{3}\\), expressed as \\(\\omega^{T} z + b = 0\\). In terms of the original variables \\(x\\), these boundaries take the form: \\[\n\\omega_{1} x_{1}^{2} + \\omega_{2} \\sqrt{2} x_{1} x_{2} + \\omega_{3} x_{2}^{2} = 0,\n\\] which corresponds to the equation of an ellipse. This demonstrates that we can apply a linear algorithm to transformed data to achieve a non-linear decision boundary with minimal effort.\nNow, consider what the algorithm is actually doing. It relies solely on the Gram matrix \\(K\\) of the data. Once \\(K\\) is computed, the original data can be discarded: \\[\n\\begin{aligned}\nK & = \\left[\\begin{array}{ccc}\nx_{1}^{T} x_{1} & x_{1}^{T} x_{2} & \\cdots \\\\\nx_{2}^{T} x_{1} & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right]_{n \\times n} = X X^{T}, \\\\\n\\text{where} \\quad X & = \\left[\\begin{array}{c}\nx_{1}^{T} \\\\\n\\vdots \\\\\nx_{n}^{T}\n\\end{array}\\right]_{n \\times d}.\n\\end{aligned}\n\\] Here, \\(X\\), which contains all the data, is referred to as the design matrix.\nWhen we map the data using \\(\\phi\\), the Gram matrix becomes: \\[\nK = \\left[\\begin{array}{ccc}\n\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{1}\\right) & \\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) & \\cdots \\\\\n\\phi\\left(x_{2}\\right)^{T} \\phi\\left(x_{1}\\right) & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right].\n\\]\nLet us compute these inner products explicitly. For vectors \\(r\\) and \\(s\\) in \\(R^{3}\\) corresponding to \\(a\\) and \\(b\\), respectively: \\[\n\\begin{aligned}\n\\langle r, s \\rangle & = r_{1} s_{1} + r_{2} s_{2} + r_{3} s_{3} \\\\\n& = a_{1}^{2} b_{1}^{2} + 2 a_{1} a_{2} b_{1} b_{2} + a_{2}^{2} b_{2}^{2} \\\\\n& = \\langle a, b \\rangle^{2}.\n\\end{aligned}\n\\]\nThus, instead of explicitly mapping the data via \\(\\phi\\) and then computing the inner product, we can compute it directly in one step, leaving the mapping \\(\\phi\\) implicit. In fact, we do not even need to know \\(\\phi\\) explicitly; all we require is the ability to compute the modified inner product. This modified inner product is called a kernel, denoted \\(K(x, y)\\). The matrix \\(K\\), which contains the kernel values for all pairs of data points, is also referred to as the kernel matrix.\nSince the kernel itself is the primary object of interest, rather than the mapping \\(\\phi\\), we aim to characterize kernels without explicitly relying on \\(\\phi\\). Mercer’s Theorem provides the necessary framework for this characterization.\nLet’s implement it\n\nlibrary(\"scatterplot3d\")\nphi &lt;- function(x1, x2) {\n    z1 &lt;- x1^2\n    z2 &lt;- sqrt(2) * x1 * x2\n    z3 &lt;- x2^2\n    return(cbind(z1, z2, z3))\n}\n\n# Generate sample 2D data (you can replace this with your actual data)\n\n\n# Apply the transformation\ntransformed_data &lt;- phi(d[,2], d[,3])\nscatterplot3d(transformed_data, color = ifelse(d[,1] == 0, \"red\", \"blue\"), pch = 19,\n                xlab = \"z1 (x1^2)\", ylab = \"z2 (sqrt(2) * x1 * x2)\", zlab = \"z3 (x2^2)\",\n                main = \"3D Scatter Plot of Transformed Data\", angle=222, grid=FALSE, box=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an Executive.”\n\n\nIrwin, Neil. 2016. “How to Become a C.E.O.? The Quickest Path Is a Winding One.” The New York Times, September.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316. https://arxiv.org/abs/1604.07316.\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an\nExecutive.”\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nIrwin, Neil. 2016. “How to Become a\nC.E.O.? The Quickest Path\nIs a Winding One.” The New York\nTimes, September.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893. https://arxiv.org/abs/1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\nhttps://arxiv.org/abs/1712.01815.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549. https://arxiv.org/abs/1704.03549.",
    "crumbs": [
      "References"
    ]
  }
]