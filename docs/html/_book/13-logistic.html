<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>13&nbsp; Logistic Regression and Generalized Linear Models – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./14-tree.html" rel="next">
<link href="./12-regression.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="13&nbsp; Logistic Regression and Generalized Linear Models – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="13-logistic_files/figure-html/unnamed-chunk-2-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="13&nbsp; Logistic Regression and Generalized Linear Models – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="13-logistic_files/figure-html/unnamed-chunk-2-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./13-logistic.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#model-fitting" id="toc-model-fitting" class="nav-link active" data-scroll-target="#model-fitting"><span class="header-section-number">13.1</span> Model Fitting</a></li>
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix"><span class="header-section-number">13.2</span> Confusion Matrix</a></li>
  <li><a href="#roc-curve-and-confounding-variables" id="toc-roc-curve-and-confounding-variables" class="nav-link" data-scroll-target="#roc-curve-and-confounding-variables"><span class="header-section-number">13.3</span> ROC Curve and Confounding Variables</a></li>
  <li><a href="#imbalanced-data" id="toc-imbalanced-data" class="nav-link" data-scroll-target="#imbalanced-data"><span class="header-section-number">13.4</span> Imbalanced Data</a></li>
  <li><a href="#kernel-trick" id="toc-kernel-trick" class="nav-link" data-scroll-target="#kernel-trick"><span class="header-section-number">13.5</span> Kernel Trick</a></li>
  <li><a href="#sec-glm" id="toc-sec-glm" class="nav-link" data-scroll-target="#sec-glm"><span class="header-section-number">13.6</span> Generalized linear models</a>
  <ul class="collapse">
  <li><a href="#deviance-and-model-comparison" id="toc-deviance-and-model-comparison" class="nav-link" data-scroll-target="#deviance-and-model-comparison">Deviance and model comparison</a></li>
  </ul></li>
  <li><a href="#sec-bayes-logistic" id="toc-sec-bayes-logistic" class="nav-link" data-scroll-target="#sec-bayes-logistic"><span class="header-section-number">13.7</span> Bayesian Logistic Regression with The Polya-Gamma Distribution</a>
  <ul class="collapse">
  <li><a href="#the-data-augmentation-strategy" id="toc-the-data-augmentation-strategy" class="nav-link" data-scroll-target="#the-data-augmentation-strategy">The Data-Augmentation Strategy</a></li>
  </ul></li>
  <li><a href="#bayesian-analysis-of-horse-race-betting" id="toc-bayesian-analysis-of-horse-race-betting" class="nav-link" data-scroll-target="#bayesian-analysis-of-horse-race-betting"><span class="header-section-number">13.8</span> Bayesian Analysis of Horse Race Betting</a></li>
  <li><a href="#theoretical-guarantees" id="toc-theoretical-guarantees" class="nav-link" data-scroll-target="#theoretical-guarantees">Theoretical Guarantees</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./13-logistic.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-logistic" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Classification predicts categories rather than numbers. Does this patient have the disease or not? Is this email spam? Is that object in the camera feed a pedestrian, a vehicle, or a traffic sign? The output is discrete—often just 0 or 1 (binary classification), sometimes multiple classes. Self-driving cars classify objects in real-time from camera feeds; medical systems flag high-risk patients; fraud detection systems sort transactions into suspicious and legitimate.</p>
<p>Given observed data <span class="math inline">\((x_i,y_i)_{i=1}^n\)</span>, where each <span class="math inline">\(y_i\)</span> is either 0 or 1, we start by assuming a binomial likelihood function for the response variable, defined as follows: <span class="math display">\[
P(y_i = 1\mid p_i) = p_i^{y_i} (1-p_i)^{1-y_i},
\]</span> where <span class="math inline">\(p_i\)</span> is the function of the inputs <span class="math inline">\(x_i\)</span> and coefficients <span class="math inline">\(\beta\)</span> that gives us the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate <span class="math inline">\(p_i\)</span> is to use the logistic function <span class="math display">\[\begin{align*}
f_{\beta}(x_i) = &amp; \beta^Tx_i\\
p_i  = &amp; \sigma(f_{\beta}(x_i)) =  \frac{e^{f_{\beta}(x_i)}}{1+e^{f_{\beta}(x_i)}},
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a vector of parameters. The logistic function <span class="math inline">\(\sigma(\cdot)\)</span> maps any real number to the interval <span class="math inline">\((0, 1)\)</span>, interpreting the output as a probability.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-sigmoid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-sigmoid">graph LR
x[Linear Predictor z] --&gt; s[Sigmoid Function]
s --&gt; p[Probability P]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sigmoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.1: The Sigmoid Function
</figcaption>
</figure>
</div>
</div>
</div>
<!-- ![Sigmoid Function](fig/sigmoid.png) -->
<section id="model-fitting" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="model-fitting"><span class="header-section-number">13.1</span> Model Fitting</h2>
<p>Then we fit the model using binomial log-likelihood minimization. It leads us to the maximum likelihood estimator for parameters <span class="math inline">\(\beta\)</span> (a.k.a <em>cross-entropy estimator</em>), defined as <span class="math display">\[
\hat \beta = \arg\min_{\beta}\mathcal{L}(\beta),
\]</span> where <span class="math display">\[
\mathcal{L}(\beta) =  -\sum_{i=1}^n \left[ y_i \log p_i  + (1-y_i) \log \left ( 1-p_i \right ) \right].
\]</span> Similar to the least squares estimator, the cross-entropy estimator optimization problem is convex, so it has a unique solution.</p>
<p>In the unconditional case (an intercept-only model with no inputs <span class="math inline">\(x\)</span>), the cross-entropy estimator simplifies to the sample mean. If we take the derivative of the above expression with respect to <span class="math inline">\(\beta_0\)</span> and set it to zero, we get <span class="math display">\[
- \frac{d}{d\beta_0}\sum_{i=1}^n \left[ y_i \log \left ( \beta_0 \right ) + (1-y_i) \log \left ( 1-\beta_0 \right ) \right] = -\sum_{i=1}^n \left[ \frac{y_i}{\beta_0} - \frac{1-y_i}{1-\beta_0} \right] = 0
\]</span> which gives us the solution <span class="math display">\[
\hat{\beta}_0 = \frac{1}{n}\sum_{i=1}^n y_i.
\]</span> which is the sample mean.</p>
<p>Unlike the least squares estimator or the unconditional case, the system of equations <span class="math display">\[
\nabla \mathcal{L}(\beta) = 0
\]</span> is not linear and cannot be solved by inverting a matrix. However, there are efficient iterative numerical optimization algorithms that can be used to find the optimal solution. The most common one is the <em>BFGS</em> (Broyden-Fletcher-Goldfarb-Shanno) algorithm. It is a quasi-Newton method that’s particularly well-suited for optimizing the cross-entropy loss function in logistic regression.</p>
<p>When we have more than two classes <span class="math inline">\(y \in \{1,\ldots,K\}\)</span>, we build <span class="math inline">\(K-1\)</span> models <span class="math inline">\(f_{\beta_1}(x),\ldots, f_{\beta_{K-1}}(x)\)</span>, one for each of the first <span class="math inline">\(K-1\)</span> classes, while treating the <span class="math inline">\(K\)</span>-th class as the reference class with <span class="math inline">\(f_{\beta_K}(x) = 0\)</span>. We then use the softmax function to convert the outputs into probabilities:</p>
<p>For classes <span class="math inline">\(j = 1, \ldots, K-1\)</span>: <span class="math display">\[
P(y = j \mid x) = \frac{\exp(f_{\beta_j}(x))}{1 + \sum_{i=1}^{K-1} \exp(f_{\beta_i}(x))}
\]</span></p>
<p>For the reference class <span class="math inline">\(K\)</span>: <span class="math display">\[
P(y = K \mid x) = \frac{1}{1 + \sum_{i=1}^{K-1} \exp(f_{\beta_i}(x))}
\]</span></p>
<p>Some implementations of the logistic regression use <span class="math inline">\(K\)</span> models, one for each class, and then use the softmax function to convert the outputs into probabilities. This is equivalent to the above approach, but it is more computationally expensive.</p>
<p>The vector of non-scaled outputs <span class="math inline">\((f_{\beta_1}(x),\ldots, f_{\beta_{K-1}}(x))\)</span> is called the <em>logits</em>.</p>
<p>The softmax function is a generalization of the logistic function to the case of more than two classes. It is often used as the activation function in the output layer of neural networks for multi-class classification problems. It converts the output of each model into a probability distribution over the classes, making it suitable for multi-class classification with probabilistic outputs.</p>
<p>The logistic function has a nice statistical interpretation. It is the CDF of the logistic distribution, which is a symmetric distribution with mean 0 and variance <span class="math inline">\(\pi^2/3\)</span>, thus <span class="math inline">\(p_i\)</span> is simply a value of this CDF, evaluated at <span class="math inline">\(\beta^Tx_i\)</span>.</p>
<p>Further, Logistic regression models the log-odds (logit) of the probability as a linear function of the predictors, which aligns with the maximum likelihood estimation framework and provides desirable statistical properties. Specifically, if we invert the logistic function, <span class="math display">\[
p_i  = \sigma(\beta^Tx_i) =  \frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}},
\]</span> we get the log-odds <span class="math display">\[
\log\left(\frac{p_i}{1-p_i}\right) = \beta^Tx_i.
\]</span> Meaning that <span class="math inline">\(\beta^Tx_i\)</span> measures how probability of <span class="math inline">\(y_i = 1\)</span> changes with respect to the change in <span class="math inline">\(x_i\)</span> on the log-odds scale. It allows us to interpret the model coefficients as the log-odds ratios of the response variable.</p>
<p>In some disciplines, such as econometrics, psychology and natural sciences, a normal CDF is used instead of the logistic CDF. This is often done for historical reasons or because the normal CDF implies slightly different theoretical assumptions that may be more appropriate for specific datasets.</p>
<p>In the case of the normal CDF, the model is called <em>probit</em>, it stands for probability unit, and the link function is called <em>probit link</em>. The probit model is defined as <span class="math display">\[
\Phi^{-1}(p_i) =  \beta^Tx_i.
\]</span> where <span class="math inline">\(\Phi(\cdot)\)</span> is the normal CDF.</p>
<p>The term probit was coined in the 1930’s by biologists studying the dosage-cure rate link. We can fit a probit model using <code>glm</code> function in <code>R</code>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<pre><code>## -0.86
## 0.19
##    1 
## 0.19</code></pre>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>Our prediction is the blue area which is equal to 0.195.</p>
<p>Outside fields like behavioral economics, the logistic function is generally preferred over the probit model due to the interpretability of log-odds and its natural extension to multi-class problems. The PDF of the logistic distribution is very similar to the normal PDF.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<div id="exm-nba" class="theorem example">
<p><span class="theorem-title"><strong>Example 13.1 (Example: NBA point spread)</strong></span> We will use the NBA point spread data to illustrate the logistic regression. The data is available in the <code>NBAspread.csv</code> file. The data contains the point spread for each game in the NBA from 2013 to 2014 season. The data also contains the outcome of the game, whether the favorite won or not. The point spread is the number of points by which the favorite is expected to win the game and is predicted by the bookmakers. We simply want to see how well the point spread predicts the outcome of the game.</p>
<p>We start by loading the data and visualizing it.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">favwin</th>
<th style="text-align: right;">favscr</th>
<th style="text-align: right;">undscr</th>
<th style="text-align: right;">spread</th>
<th style="text-align: right;">favhome</th>
<th style="text-align: right;">fregion</th>
<th style="text-align: right;">uregion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">72</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">82</td>
<td style="text-align: right;">74</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">87</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">17.0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: right;">0</td>
<td style="text-align: right;">69</td>
<td style="text-align: right;">70</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">77</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">2.5</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Does the Vegas point spread predict whether the favorite wins or not? The histogram shows the distribution of point spreads for games where the favorite won (turquoise) versus games where the favorite lost (purple). The boxplot provides another view of this relationship. Let’s fit a logistic regression model to quantify this relationship:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">spread</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>The <span class="math inline">\(\beta\)</span> measures how our log-odds change. For this model, we have <span class="math inline">\(\beta = 0.156\)</span>, meaning that for every one point increase in the point spread, the log-odds of the favorite winning increases by 0.156.</p>
<p>Now, we can use the model to predict the probability of the favorite winning for a new game with a point spread of 8 or 4.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(nbareg, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">spread =</span> <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">4</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">##    1    2 </span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.78 0.65</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The code above simply “Plugs-in” the values for the new game into our logistic regression <span class="math display">\[
{ P \left ( \mathrm{ favwin}  \mid  \mathrm{ spread} \right ) = \frac{ e^{ \beta x } }{ 1 + e^{\beta x} } }
\]</span> We can calculate it manually as well.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">0.156</span> <span class="sc">*</span> <span class="dv">8</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="fl">0.156</span> <span class="sc">*</span> <span class="dv">8</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.78</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">0.156</span> <span class="sc">*</span> <span class="dv">4</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="fl">0.156</span> <span class="sc">*</span> <span class="dv">4</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.65</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Check that when <span class="math inline">\(\beta =0\)</span> we have <span class="math inline">\(p= \frac{1}{2}\)</span>.</p>
<p>Given our new values spread<span class="math inline">\(=8\)</span> or spread<span class="math inline">\(=4\)</span>, the win probabilities are <span class="math inline">\(78\)</span>% and <span class="math inline">\(65\)</span>%, respectively. Clearly, the bigger spread means a higher chance of winning.</p>
</div>
<p>Notice that the predict function returns a numeric value between 0 and 1. However, if we want to make a decision (to bet or not to bet), we need to have a binary outcome. A simple method to move between the predicted probability and binary value is to use thresholding. <span class="math display">\[
\hat y_i = \begin{cases}
1 &amp; \text{if } \hat p_i &gt; \alpha \\
0 &amp; \text{if } \hat p_i \leq \alpha
\end{cases}
\]</span> where <span class="math inline">\(\alpha\)</span> is a threshold value. A typical choice is <span class="math inline">\(\alpha = 0.5\)</span>.</p>
<p>Now let’s calculate the number of correct predictions using threshold <span class="math inline">\(\alpha = 0.5\)</span>. <code>R</code> has a convenient <code>table</code> function that can summarize the counts of the predicted and actual values in a table.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(NBA<span class="sc">$</span>favwin, <span class="fu">as.integer</span>(<span class="fu">predict</span>(nbareg, <span class="at">type =</span> <span class="st">"response"</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span>), <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"Actual"</span>, <span class="st">"Predicted"</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       Predicted</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Actual   1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="do">##      0 131</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      1 422</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Our model gets 0.7631103 of the predictions correctly. This number is called <em>accuracy</em> of the model.</p>
</section>
<section id="confusion-matrix" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">13.2</span> Confusion Matrix</h2>
<p>We will analyse the tennis data set to show what is the decision boundary for the logistic regression model. The decision boundary is the line that separates the two classes. It is defined as the line where the probability of the favorite winning is 0.5. Then we will use the confusion matrix to evaluate the performance of the model.</p>
<div id="exm-tennis" class="theorem example">
<p><span class="theorem-title"><strong>Example 13.2 (Logistic Regression for Tennis Classification)</strong></span> Data science plays a major role in tennis, you can learn about recent AI tools developed by IBM from this <a href="https://finance.yahoo.com/video/ibm-serving-ai-technology-tennis-150742376.html">Yahoo! Finance Article</a>.</p>
<p>We will analyze the <a href="https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics">Tennis Major Tournament Match Statistics Data Set</a> from the UCI ML repository. The data set has one per each game from four major Tennis tournaments in 2013 (Australia Open, French Open, US Open, and Wimbledon).</p>
<p>Let’s look at a few columns of the randomly selected five rows of the data</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 5%">
<col style="width: 26%">
<col style="width: 33%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Player1</th>
<th style="text-align: left;">Player2</th>
<th style="text-align: right;">Round</th>
<th style="text-align: right;">Result</th>
<th style="text-align: left;">gender</th>
<th style="text-align: left;">surf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">532</td>
<td style="text-align: left;">Florian Mayer</td>
<td style="text-align: left;">Juan Monaco</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">Hard</td>
</tr>
<tr class="even">
<td style="text-align: left;">816</td>
<td style="text-align: left;">L.Kubot</td>
<td style="text-align: left;">J.Janowicz</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">Grass</td>
</tr>
<tr class="odd">
<td style="text-align: left;">431</td>
<td style="text-align: left;">Svetlana Kuznetsova</td>
<td style="text-align: left;">Ekaterina Makarova</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">W</td>
<td style="text-align: left;">Clay</td>
</tr>
<tr class="even">
<td style="text-align: left;">568</td>
<td style="text-align: left;">Marcos Baghdatis</td>
<td style="text-align: left;">Go Soeda</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">Hard</td>
</tr>
<tr class="odd">
<td style="text-align: left;">216</td>
<td style="text-align: left;">Mandy Minella</td>
<td style="text-align: left;">Anastasia Pavlyuchenkova</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">W</td>
<td style="text-align: left;">Hard</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>We have data for 943 matches and for each match we have 44 columns, including names of the players, their gender, surface type and match statistics. Let’s look at the number of break points won by each player. We will plot BPW (break points won) by each player on the scatter plot and will colorize each dot according to the outcome</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>We can clearly see that the number of break points won is a clear predictor of the match outcome. This is obvious and follows from the rules; to win a match, a player must win break points. Now, we want to understand the impact of winning a break point on the overall match outcome. We do it by building a logistic regression model</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(<span class="fu">is.na</span>(d<span class="sc">$</span>BPW<span class="fl">.1</span>)) <span class="co"># there is one row with NA value for the BPW.1 value and we remove it</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 171</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> d[<span class="sc">-</span><span class="dv">171</span>, ]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">dim</span>(d)[<span class="dv">1</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">glm</span>(Result <span class="sc">~</span> BPW<span class="fl">.1</span> <span class="sc">+</span> BPW<span class="fl">.2</span> <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> d, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>m <span class="sc">%&gt;%</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tidy</span>() <span class="sc">%&gt;%</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BPW.1</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">BPW.2</td>
<td style="text-align: right;">-0.42</td>
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">-15</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The predicted values are stored in the <code>fitted.values</code> field of the model object. Those are the probabilities of player 1 winning the match. We need to convert them to binary predictions using <span class="math inline">\(0.5\)</span> as a threshold for our classification.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(d<span class="sc">$</span>Result, <span class="fu">as.integer</span>(m<span class="sc">$</span>fitted.values <span class="sc">&gt;</span> <span class="fl">0.5</span>), <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"Actual"</span>, <span class="st">"Predicted"</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       Predicted</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Actual   0   1</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="do">##      0 416  61</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      1  65 400</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>This table shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. The first row shows the number of matches where player 1 won and the model predicted that player 1 won. The second row shows the number of matches where player 1 lost and the model predicted that player 1 lost. Thus, our model got (400+416)/942 = 86.6242038% of the predictions correctly! The accuracy is the ratio of the number of correct predictions to the total number of predictions.</p>
<p>This table is called <em>confusion matrix</em>. It is a table that shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. Formally, it is defined as</p>
<table class="caption-top table">
<caption>Confusion Matrix. TPR - True Positive Rate, FPR - False Positive Rate, TNR - True Negative Rate, FNR - False Negative Rate.</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Predicted: YES</th>
<th style="text-align: center;">Predicted: NO</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual: YES</td>
<td style="text-align: center;">TPR</td>
<td style="text-align: center;">FNR</td>
</tr>
<tr class="even">
<td>Actual: NO</td>
<td style="text-align: center;">FPR</td>
<td style="text-align: center;">TNR</td>
</tr>
</tbody>
</table>
<p>Fundamentally, logistic regression attempts to construct a linear boundary that separates the two classes. In our case, we have two predictors <span class="math inline">\(x_1\)</span> = BPW.1 and <span class="math inline">\(x_2\)</span> = BPW.2 and our model is <span class="math display">\[
\log\left(\dfrac{p}{1-p}\right) = \beta_1x_1 + \beta_2 x_2,
\]</span> where <span class="math inline">\(p\)</span> is the probability of player 1 winning the match. We want to find the line along which the probability is 1/2, meaning that <span class="math inline">\(p/(1-p) = 1\)</span> and log-odds <span class="math inline">\(\log(p/(1-p)) = 0\)</span>, thus the equation for the line is <span class="math inline">\(\beta_1x_1 + \beta_2 x_2 = 0\)</span> or <span class="math display">\[
x_2 = \dfrac{-\beta_1}{\beta_2}x_1
\]</span></p>
<p>Let’s see the line found by the <code>glm</code> function</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>BPW<span class="fl">.1</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n), d<span class="sc">$</span>BPW<span class="fl">.2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n),</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">col =</span> d<span class="sc">$</span>Result <span class="sc">+</span> <span class="dv">2</span>, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">bg =</span> <span class="st">"yellow"</span>, <span class="at">lwd =</span> <span class="fl">0.8</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">"BPW by Player 1"</span>, <span class="at">ylab =</span> <span class="st">"BPW by Player 2"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">30</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="sc">-</span>m<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">*</span> x <span class="sc">/</span> m<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, y, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>There are a couple of observations. First, the effect of a break point on the game outcome is significant and symmetric; the effect of losing a break point is the same as the effect of winning one. We also can interpret the effect of winning a break point in the following way. We will keep BPW.2 = 0 and will calculate what happens to the probability of winning when BPW.1 changes from 0 to 1. The odds ratio for player 1 winning when BPW.1 = 0 is <code>exp(0)</code> which is 1, meaning that the probability that P1 wins is 1/2. Now when BPW.1 = 1, the odds ratio is</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">0.4019</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 1.5</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can calculate probability of winning from the regression equation <span class="math display">\[
\dfrac{p}{1-p} = 1.5,~~~p = 1.5(1-p),~~~2.5p = 1.5,~~~p = 0.6
\]</span> Thus probability of winning goes from 50% to 60%, we can use <code>predict</code> function to get this result</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.glm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">BPW.1 =</span> <span class="fu">c</span>(<span class="dv">0</span>), <span class="at">BPW.2 =</span> <span class="fu">c</span>(<span class="dv">0</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">##   1 </span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.5</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.glm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">BPW.1 =</span> <span class="fu">c</span>(<span class="dv">1</span>), <span class="at">BPW.2 =</span> <span class="fu">c</span>(<span class="dv">0</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="do">##   1 </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.6</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>What happens to the chances of winning when P1 wins three more break points compared to the opponent</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.glm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">BPW.1 =</span> <span class="fu">c</span>(<span class="dv">0</span>), <span class="at">BPW.2 =</span> <span class="fu">c</span>(<span class="dv">0</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="do">##   1 </span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.5</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">predict.glm</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">BPW.1 =</span> <span class="fu">c</span>(<span class="dv">3</span>), <span class="at">BPW.2 =</span> <span class="fu">c</span>(<span class="dv">0</span>)), <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="do">##    1 </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.77</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Chances go up by 27%.</p>
<p>Tennis is arguably the sport in which men and women are treated equally. Both men’s and women’s matches are shown during prime-time on TV, and they both have the same prize money. However, one of the comments you hear often is that women’s matches are “less predictable”, meaning that an upset (when the favorite loses) is more likely to happen in a women’s match compared to men’s matches. We can test this statement by looking at the residuals. The larger the residual the less accurate our prediction was.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>outlind <span class="ot">&lt;-</span> <span class="fu">which</span>(d<span class="sc">$</span>res <span class="sc">&lt;</span> <span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(d<span class="sc">$</span>res[outlind] <span class="sc">~</span> d<span class="sc">$</span>gender[outlind], <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>), <span class="at">xlab =</span> <span class="st">"Gender"</span>, <span class="at">ylab =</span> <span class="st">"Residual"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>Let’s do a formal t-test on the residuals for men’s and women’s matches</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>men <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(res <span class="sc">&lt;</span> <span class="dv">2</span>, gender <span class="sc">==</span> <span class="st">"M"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(res)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>women <span class="ot">&lt;-</span> d <span class="sc">%&gt;%</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(res <span class="sc">&lt;</span> <span class="dv">2</span>, gender <span class="sc">==</span> <span class="st">"W"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pull</span>(res)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(men, women, <span class="at">alternative =</span> <span class="st">"two.sided"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  Welch Two Sample t-test</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="do">## data:  men and women</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="do">## t = -5, df = 811, p-value = 0.000003</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="do">## alternative hypothesis: true difference in means is not equal to 0</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 95 percent confidence interval:</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  -0.105 -0.043</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sample estimates:</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="do">## mean of x mean of y </span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="do">##       1.2       1.3</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The difference of <span class="math inline">\(0.07\)</span> between men and women and the statistic value of <span class="math inline">\(-4.7\)</span> means that the crowd wisdom that women’s matches are less predictable is correct. The difference is statistically significant!</p>
</div>
</section>
<section id="roc-curve-and-confounding-variables" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="roc-curve-and-confounding-variables"><span class="header-section-number">13.3</span> ROC Curve and Confounding Variables</h2>
<p>Using default data set, we will illustrate the concept of ROC curve and confounding variables.</p>
<div id="exm-HorseRacing" class="theorem example">
<p><span class="theorem-title"><strong>Example 13.3 (Horse Race Betting)</strong></span> Horse race betting provides a rich application of logistic regression, where predicting whether the favorite wins illustrates key classification concepts. This example uses data from the Hong Kong Jockey Club, one of the world’s largest horse racing operations (see the Benter case study for comprehensive analysis).</p>
<p>The betting public’s implied probabilities, derived from pari-mutuel odds, exhibit the well-documented <em>favorite-longshot bias</em>: favorites tend to win more often than the odds suggest, while longshots win less often. We can build a logistic regression model to predict whether the favorite wins based on race characteristics.</p>
<p>First, we load and prepare the data. We identify the favorite in each race as the horse with the highest implied probability from the betting odds.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<pre><code>## Dataset: 5885 races from 1997-11-30 to 2005-08-28
## Favorite win rate: 29.9%</code></pre>
</div>
<p>Our outcome variable is whether the favorite won (<code>win = 1</code>) or not (<code>win = 0</code>). The favorite’s implied probability from public odds should be a strong predictor; if the market is perfectly calibrated, this probability would equal the true win rate.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-2.4</td>
<td style="text-align: right;">0.11</td>
<td style="text-align: right;">-22</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">public_prob</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">0.41</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The positive coefficient on <code>public_prob</code> confirms that higher implied probability predicts higher actual win probability. We can visualize this relationship:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/betting-prob-plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>The model prediction lies above the 45-degree line, confirming the favorite-longshot bias: favorites win more often than their odds imply.</p>
<p>Now we build the confusion matrix. In betting, the threshold choice has financial implications: being too aggressive (low threshold) means betting on too many “favorites” that lose, while being too conservative means missing profitable opportunities.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">0.66</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.50</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We use a threshold of 0.3 because the baseline win rate for favorites is around 30%. The ROC curve helps us understand the trade-off between correctly identifying winners (sensitivity) and avoiding false predictions on losers (specificity).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/betting-roc-plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>The ROC curve shows reasonable predictive power above the diagonal. The choice of threshold depends on the betting strategy: a lower threshold captures more true winners but also more false positives.</p>
<p>Now let’s examine multiple predictors. Hong Kong has two racetracks: Happy Valley (a tight urban track) and Sha Tin (a modern facility with longer straights). We might expect venue to affect favorite performance.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">-2.95</td>
<td style="text-align: right;">0.32</td>
<td style="text-align: right;">-9.37</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">public_prob</td>
<td style="text-align: right;">6.32</td>
<td style="text-align: right;">0.43</td>
<td style="text-align: right;">14.55</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">field_size</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">1.83</td>
<td style="text-align: right;">0.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">venue_ST</td>
<td style="text-align: right;">-0.07</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">-0.96</td>
<td style="text-align: right;">0.34</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The coefficient for <code>field_size</code> is negative, which seems intuitive; in larger fields, there’s more competition, so favorites may have harder time winning. But the coefficient for <code>venue_ST</code> (Sha Tin) is also negative. Does this mean favorites perform worse at Sha Tin?</p>
<p>Let’s check for confounding by examining field sizes at each venue:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/betting-confounding-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>Sha Tin races have systematically larger fields. This creates confounding: the apparent venue effect may actually be a field size effect. When we control for field size in the model, the venue effect represents the <em>residual</em> impact of venue beyond what’s explained by field size.</p>
<p>To see this more clearly, let’s compare predictions across venues for races with the same field size:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/betting-adjusted-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>After controlling for field size, the venue difference is smaller. The remaining gap may reflect track characteristics: Happy Valley’s tighter turns could favor front-running favorites who avoid traffic problems.</p>
<p>To summarize: ROC curves help visualize the trade-off between catching winners and avoiding false positives across different probability thresholds. In betting applications, this trade-off has direct financial consequences, making threshold selection a critical business decision. Confounding is also evident: Sha Tin races appear to disadvantage favorites, but this partly reflects larger field sizes rather than track characteristics. Including confounders in the model isolates the true venue effect. These principles from horse racing apply broadly to any classification problem where features are correlated.</p>
</div>
<p>Now, a natural question is how to choose the cut-off value <span class="math inline">\(\alpha\)</span>? In betting, we place a bet when <span class="math inline">\(p(\text{win}) &gt; \alpha\)</span>. Here <span class="math inline">\(\alpha\)</span> is our confidence threshold. If we choose <span class="math inline">\(\alpha = 0\)</span> and bet on everything, we’ll lose money on bad bets. If we choose <span class="math inline">\(\alpha = 1\)</span>, we never bet and make no money. In order to choose an appropriate <span class="math inline">\(\alpha\)</span>, we need to know the payoffs.</p>
<p>For betting on favorites at typical odds (say, 2.5:1 decimal odds), the pay-off matrix looks like:</p>
<table class="caption-top table">
<caption>Pay-off matrix for betting</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Win</th>
<th style="text-align: right;">Lose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bet</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">-1</td>
</tr>
<tr class="even">
<td>Don’t bet</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>If we bet $1 and win at 2.5:1 odds, we gain $1.50 profit. If we lose, we lose our $1 stake. If we don’t bet, we neither gain nor lose.</p>
<p>Given this pay-off matrix, we can calculate the expected profit across different thresholds using our horse racing data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/betting-payoff-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
<p>To identify the most effective strategy, we evaluate the expected profit across all possible thresholds <span class="math inline">\(\alpha\)</span>. The expected profit per dollar wagered is calculated as a weighted average of the gains from true positives and the losses from false positives: <span class="math display">\[E[\text{Profit} \mid \alpha] = P(\text{win}) \cdot \text{Sensitivity}(\alpha) \cdot 1.5 - P(\text{lose}) \cdot (1 - \text{Specificity}(\alpha)) \cdot 1.0\]</span> where <span class="math inline">\(P(\text{win})\)</span> is the base rate of winners in the training data. This calculation allows us to identify the <em>profitable threshold range</em> (where expected profit is positive), the <em>optimal threshold</em> (which maximizes profit), and the resulting <em>maximum expected profit</em>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<pre><code>## Profitable threshold range: 0.32 to 0.80
## Optimal threshold: 0.40
## Maximum expected profit: $0.028 per $1 wagered</code></pre>
</div>
<p>This analysis shows that the choice of threshold has direct financial consequences. Unlike academic classification problems where we might default to <span class="math inline">\(\alpha = 0.5\)</span>, real-world applications require understanding the asymmetric costs of different types of errors.</p>
<div id="exm-LinkedIn" class="theorem example">
<p><span class="theorem-title"><strong>Example 13.4 (LinkedIn Study)</strong></span> How to Become an Executive<span class="citation" data-cites="irwin2016how gan2016how">(<a href="references.html#ref-irwin2016how" role="doc-biblioref">Irwin 2016</a>; <a href="references.html#ref-gan2016how" role="doc-biblioref">Gan and Fritzler 2016</a>)</span>?</p>
<p>Logistic regression was used to analyze the career paths of about <span class="math inline">\(459,000\)</span> LinkedIn members who worked at a <a href="http://www.vault.com/company-rankings/consulting/best-consulting-firms-prestige?sRankID=77">top 10 consultancy</a> between 1990 and 2010 and became a VP, CXO, or partner at a company with at least 200 employees. About <span class="math inline">\(64,000\)</span> members reached this milestone, <span class="math inline">\(\hat{p} = 0.1394\)</span>, conditional on making it into the database. The goals of the analysis were the following</p>
<ol type="1">
<li>Look at their profiles – educational background, gender, work experience, and career transitions.</li>
<li>Build a predictive model of the probability of becoming an executive</li>
<li>Provide a tool for analysis of “what if” scenarios. For example, if you are to get a master’s degree, how your jobs perspectives change because of that.</li>
</ol>
<p>Let’s build a logistic regression model with <span class="math inline">\(8\)</span> key features (a.k.a. covariates): <span class="math display">\[
\log\left ( \frac{p}{1-p} \right ) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_8x_8
\]</span></p>
<p>Here <span class="math inline">\(p\)</span> is the probability of “success” – meaning the person reaches VP/CXO/Partner seniority at a company with at least 200 employees. The features to predict the “success” probability are <span class="math inline">\(x_i (i=1,2,\ldots,8)\)</span></p>
<ul>
<li><span class="math inline">\(x_1\)</span>: Metro region: whether a member has worked in one of the top 10 largest cities in the U.S. or globally.</li>
<li><span class="math inline">\(x_2\)</span>: Gender: Inferred from member names: ‘male’, or ‘female’</li>
<li><span class="math inline">\(x_3\)</span>: Graduate education type: whether a member has an MBA from a top U.S. program / a non-top program / a top non-U.S. program / another advanced degree</li>
<li><span class="math inline">\(x_4\)</span>: Undergraduate education type: whether a member has attended a school from the U.S. News national university rankings / a top 10 liberal arts college /a top 10 non-U.S. school<br>
</li>
<li><span class="math inline">\(x_5\)</span>: Company count: # different companies in which a member has worked<br>
</li>
<li><span class="math inline">\(x_6\)</span>: Function count: # different job functions in which a member has worked</li>
<li><span class="math inline">\(x_7\)</span>: Industry sector count: # different industries in which a member has worked<br>
</li>
<li><span class="math inline">\(x_8\)</span>: Years of experience: # years of work experience, including years in consulting, for a member.</li>
</ul>
<p>The following estimated <span class="math inline">\(\hat\beta\)</span>s of features were obtained. With a sample size of 456,000 they are measured rather accurately. Recall that in logistic regression, the coefficients represent the change in log-odds for a unit change in the feature.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Category</th>
<th>Feature</th>
<th>Coefficient (<span class="math inline">\(\hat{\beta}\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Location</td>
<td>Metro region</td>
<td>0.28</td>
</tr>
<tr class="even">
<td>Personal</td>
<td>Gender (Male)</td>
<td>0.31</td>
</tr>
<tr class="odd">
<td>Education</td>
<td>Graduate education type</td>
<td>1.16</td>
</tr>
<tr class="even">
<td>Education</td>
<td>Undergraduate education type</td>
<td>0.22</td>
</tr>
<tr class="odd">
<td>Work Experience</td>
<td>Company count</td>
<td>0.14</td>
</tr>
<tr class="even">
<td>Work Experience</td>
<td>Function count</td>
<td>0.26</td>
</tr>
<tr class="odd">
<td>Work Experience</td>
<td>Industry sector count</td>
<td>-0.22</td>
</tr>
<tr class="even">
<td>Work Experience</td>
<td>Years of experience</td>
<td>0.09</td>
</tr>
</tbody>
</table>
<p>Here are three main findings</p>
<ol type="1">
<li>Working across job functions, like marketing or finance, is good. Each additional job function provides a boost that, on average, is equal to three years of work experience. Switching industries has a slight negative impact. Learning curve? lost relationships?</li>
<li>MBAs are worth the investment. But pedigree matters. <em>Top five program equivalent to <span class="math inline">\(13\)</span> years of work experience!!!</em></li>
<li>Location matters. For example, NYC helps.</li>
</ol>
<p>We can also personalize the prediction for predict future possible future executives. For example, <span style="color: blue">Person A (p=6%)</span>: Male in Tulsa, Oklahoma, Undergraduate degree, 1 job function for 3 companies in 3 industries, 15-year experience.</p>
<p><span style="color: blue">Person B (p=15%)</span>: Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.</p>
<p><span style="color: blue">Person C (p=63%)</span>: Female in New York City, Top undergraduate program, Top MBA program, 4 different job functions for 4 companies in 1 industry, 15-year experience.</p>
<p>Let’s re-design Person B.</p>
<p><span style="color: blue">Person B (p=15%)</span>: Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.</p>
<ol type="1">
<li>Work in one industry rather than two. Increase <span class="math inline">\(3\)</span>%</li>
<li>Undergrad from top <span class="math inline">\(10\)</span> US program rather than top international school. <span class="math inline">\(3\)</span>%</li>
<li>Worked for <span class="math inline">\(4\)</span> companies rather than <span class="math inline">\(2\)</span>. Another <span class="math inline">\(4\)</span>%</li>
<li>Move from London to NYC. <span class="math inline">\(4\)</span>%</li>
<li>Four job functions rather than two. <span class="math inline">\(8\)</span>%. A <span class="math inline">\(1.5\)</span>x effect.</li>
<li>Worked for <span class="math inline">\(10\)</span> more years. <span class="math inline">\(15\)</span>%. A <span class="math inline">\(2\)</span>X effect.</li>
</ol>
<p>Choices and Impact (Person B) are shown below</p>
<p>Choices and Impact (Person B) are shown below in <a href="#fig-choice" class="quarto-xref">Figure&nbsp;<span>13.2</span></a>. The chart illustrates the marginal impact of each individual strategic choice (e.g., getting an MBA, moving to NYC) on the probability of becoming an executive, compared to Person B’s baseline of 15%. When all these positive choices are combined, the probability skyrockets to 81%.</p>
<div id="fig-choice" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-choice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/choice.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-choice-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.2: Choices and Impact (Person B)
</figcaption>
</figure>
</div>
</div>
</section>
<section id="imbalanced-data" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="imbalanced-data"><span class="header-section-number">13.4</span> Imbalanced Data</h2>
<p>Often, you have much more observations with a specific label, such a sample is called imbalanced. This is a common problem in real-world classification tasks where one class significantly outnumbers the other(s). For example, in fraud detection, legitimate transactions vastly outnumber fraudulent ones; in medical diagnosis, healthy patients often outnumber those with rare diseases; and in manufacturing, defective products are typically much rarer than non-defective ones.</p>
<p>When dealing with imbalanced data, you should avoid using accuracy as a metric to choose a model. Consider a binary classification problem with 95% of samples labeled as class 1. A naive classifier that simply assigns label 1 to every input will achieve 95% accuracy, making it appear deceptively good while being completely useless for practical purposes.</p>
<p>Instead, more appropriate evaluation metrics should be used. The Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various classification thresholds. The Area Under the Curve (AUC) provides a single scalar value that measures the model’s ability to distinguish between classes, regardless of the chosen threshold. An AUC of 0.5 indicates random guessing, while 1.0 represents perfect classification.</p>
<p>The F1 score combines precision and recall into a single score, providing a balanced measure that penalizes models that are either too conservative or too aggressive: <span class="math display">\[
F1 = 2\dfrac{\mathrm{precision} \times \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
\]</span> where precision measures the proportion of true positives among predicted positives, and recall measures the proportion of true positives that were correctly identified.</p>
<p>The precision-recall curve is particularly useful for imbalanced datasets, as it plots precision against recall at various thresholds, focusing on the performance of the positive class. Cohen’s Kappa measures agreement between predicted and actual classifications while accounting for agreement by chance, making it more robust to class imbalance than accuracy.</p>
<p>To address imbalanced data, several strategies can be employed. Data-level approaches include oversampling, where you synthetically generate more samples of the minority class using techniques like bootstrap sampling with replacement, SMOTE (Synthetic Minority Over-sampling Technique) which creates synthetic examples by interpolating between existing minority class samples, or generative models like GANs or variational autoencoders to create realistic synthetic data. Undersampling reduces the majority class samples, which is particularly effective when the dataset is large enough. Hybrid approaches combine both oversampling and undersampling techniques.</p>
<p>Algorithm-level approaches include cost-sensitive learning, where you assign different misclassification costs to different classes, ensemble methods using techniques like bagging or boosting that can naturally handle imbalanced data, and threshold adjustment to modify the classification threshold to optimize for specific metrics like F1-score.</p>
<p>The choice of approach depends on the specific problem, available data, and computational resources. It is often beneficial to experiment with multiple techniques and evaluate their performance using appropriate metrics rather than relying solely on accuracy.</p>
</section>
<section id="kernel-trick" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="kernel-trick"><span class="header-section-number">13.5</span> Kernel Trick</h2>
<p>While logistic regression is a powerful linear classifier, there are cases where the classes are not linearly separable in the original feature space. For example, one class might be encircled by another. In such situations, we need to extend our linear methods to handle non-linear decision boundaries.</p>
<p>The kernel trick is a method of using a linear classifier to solve a non-linear problem. The idea is to map the data into a higher dimensional space, where it becomes linearly separable. The kernel trick is to use a kernel function <span class="math inline">\(K(x_i,x_j)\)</span> to calculate the inner product of two vectors in the higher dimensional space without explicitly calculating the mapping <span class="math inline">\(\phi(x_i)\)</span> and <span class="math inline">\(\phi(x_j)\)</span>. The kernel function is defined as <span class="math inline">\(K(x_i,x_j) = \phi(x_i)^T\phi(x_j)\)</span>. The most popular kernel functions are polynomial kernel <span class="math inline">\(K(x_i,x_j) = (x_i^Tx_j)^d\)</span> and Gaussian kernel <span class="math inline">\(K(x_i,x_j) = \exp(-\gamma||x_i-x_j||^2)\)</span>. The kernel trick is used in Support Vector Machines (SVM) and Gaussian Processes (GP).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-circle-data" class="quarto-float quarto-figure quarto-figure-center anchored" width="448.32" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-circle-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="13-logistic_files/figure-html/fig-circle-data-1.png" id="fig-circle-data" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" width="448">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-circle-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.3
</figcaption>
</figure>
</div>
</div>
</div>
<p>The data in <a href="#fig-circle-data" class="quarto-xref">Figure&nbsp;<span>13.3</span></a> is not linearly separable in two dimensions; however, projecting it into a three-dimensional space via the following transformation allows for a linear separation: <span class="math display">\[
\begin{aligned}
\phi: R^{2} &amp; \longrightarrow R^{3} \\
\left(x_{1}, x_{2}\right) &amp; \longmapsto\left(z_{1}, z_{2}, z_{3}\right)=\left(x_{1}^{2}, \sqrt{2} x_{1} x_{2}, x_{2}^{2}\right),
\end{aligned}
\]</span> and attempt to linearly separate the transformed data, the decision boundaries become hyperplanes in <span class="math inline">\(R^{3}\)</span>, expressed as <span class="math inline">\(\omega^{T} z + b = 0\)</span>. In terms of the original variables <span class="math inline">\(x\)</span>, these boundaries take the form: <span class="math display">\[
\omega_{1} x_{1}^{2} + \omega_{2} \sqrt{2} x_{1} x_{2} + \omega_{3} x_{2}^{2} = 0,
\]</span> which corresponds to the equation of an ellipse. This demonstrates that we can apply a linear algorithm to transformed data to achieve a non-linear decision boundary with minimal effort.</p>
<p>Now, consider what the algorithm is actually doing. It relies solely on the Gram matrix <span class="math inline">\(K\)</span> of the data. Once <span class="math inline">\(K\)</span> is computed, the original data can be discarded: <span class="math display">\[
\begin{aligned}
K &amp; = \left[\begin{array}{ccc}
x_{1}^{T} x_{1} &amp; x_{1}^{T} x_{2} &amp; \cdots \\
x_{2}^{T} x_{1} &amp; \ddots &amp; \\
\vdots &amp; &amp;
\end{array}\right]_{n \times n} = X X^{T}, \\
\text{where} \quad X &amp; = \left[\begin{array}{c}
x_{1}^{T} \\
\vdots \\
x_{n}^{T}
\end{array}\right]_{n \times d}.
\end{aligned}
\]</span> Here, <span class="math inline">\(X\)</span>, which contains all the data, is referred to as the design matrix.</p>
<p>When we map the data using <span class="math inline">\(\phi\)</span>, the Gram matrix becomes: <span class="math display">\[
K = \left[\begin{array}{ccc}
\phi\left(x_{1}\right)^{T} \phi\left(x_{1}\right) &amp; \phi\left(x_{1}\right)^{T} \phi\left(x_{2}\right) &amp; \cdots \\
\phi\left(x_{2}\right)^{T} \phi\left(x_{1}\right) &amp; \ddots &amp; \\
\vdots &amp; &amp;
\end{array}\right].
\]</span></p>
<p>Let us compute these inner products explicitly. For vectors <span class="math inline">\(r\)</span> and <span class="math inline">\(s\)</span> in <span class="math inline">\(R^{3}\)</span> corresponding to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, respectively: <span class="math display">\[
\begin{aligned}
\langle r, s \rangle &amp; = r_{1} s_{1} + r_{2} s_{2} + r_{3} s_{3} \\
&amp; = a_{1}^{2} b_{1}^{2} + 2 a_{1} a_{2} b_{1} b_{2} + a_{2}^{2} b_{2}^{2} \\
&amp; = \langle a, b \rangle^{2}.
\end{aligned}
\]</span></p>
<p>Thus, instead of explicitly mapping the data via <span class="math inline">\(\phi\)</span> and then computing the inner product, we can compute it directly in one step, leaving the mapping <span class="math inline">\(\phi\)</span> implicit. In fact, we do not even need to know <span class="math inline">\(\phi\)</span> explicitly; all we require is the ability to compute the modified inner product. This modified inner product is called a kernel, denoted <span class="math inline">\(K(x, y)\)</span>. The matrix <span class="math inline">\(K\)</span>, which contains the kernel values for all pairs of data points, is also referred to as the kernel matrix.</p>
<p>Since the kernel itself is the primary object of interest, rather than the mapping <span class="math inline">\(\phi\)</span>, we aim to characterize kernels without explicitly relying on <span class="math inline">\(\phi\)</span>. Mercer’s Theorem provides the theoretical guarantee for this: it states that for any symmetric, positive-definite function <span class="math inline">\(K(x, y)\)</span>, there exists a mapping <span class="math inline">\(\phi\)</span> such that <span class="math inline">\(K(x, y) = \langle \phi(x), \phi(y) \rangle\)</span>. This ensures we can implicitly work in a high-dimensional space just by defining a valid kernel function.</p>
<p>Let’s implement it</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/scatterplot3d-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="448"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-glm" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="sec-glm"><span class="header-section-number">13.6</span> Generalized linear models</h2>
<p>Logistic regression is one member of a larger family of models called generalized linear models (GLMs). A GLM has three components:</p>
<ol type="1">
<li>a distributional assumption for <span class="math inline">\(Y\)</span> from the exponential family (e.g., Normal, Bernoulli, Poisson, Gamma),</li>
<li>a linear predictor <span class="math inline">\(\eta_i = x_i^\top \beta\)</span>,</li>
<li>a link function <span class="math inline">\(g(\cdot)\)</span> connecting the mean response to the linear predictor, <span class="math inline">\(g(\E{Y_i\mid x_i})=\eta_i\)</span>.</li>
</ol>
<p>This framework makes it easy to move between problem types (continuous, binary, counts, positive-valued responses) while keeping a common estimation story based on likelihood and its negative log (<a href="11-pattern.html" class="quarto-xref"><span>Chapter 11</span></a>).</p>
<p>The link function encodes how predictors move the mean response. Common choices include:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 36%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Link Function</th>
<th>Distribution</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>logit</td>
<td>Bernoulli/binomial</td>
<td>logistic regression for binary outcomes</td>
</tr>
<tr class="even">
<td>probit</td>
<td>Bernoulli/binomial</td>
<td>Normal-CDF alternative for binary data</td>
</tr>
<tr class="odd">
<td>log</td>
<td>Poisson, Gamma</td>
<td>count data and positive-valued responses</td>
</tr>
<tr class="even">
<td>inverse</td>
<td>Gamma</td>
<td>positive continuous data (alternative parameterization)</td>
</tr>
</tbody>
</table>
<p>For count outcomes <span class="math inline">\(Y_i \in \{0,1,2,\ldots\}\)</span>, a standard GLM is <em>Poisson regression</em>: <span class="math display">\[
Y_i \mid x_i \sim \mathrm{Poisson}(\lambda_i), \qquad \log \lambda_i = x_i^\top \beta.
\]</span> The log link ensures <span class="math inline">\(\lambda_i&gt;0\)</span>. Coefficients have a multiplicative interpretation: increasing a covariate by one unit changes the mean count by a factor of <span class="math inline">\(\exp(\beta_j)\)</span>, holding other predictors fixed.</p>
<p>For continuous responses that are strictly positive and right-skewed (e.g., waiting times, costs), a common GLM choice is <em>Gamma regression</em>. One parameterization is <span class="math display">\[
Y_i \mid x_i \sim \mathrm{Gamma}(\text{shape},\text{rate}), \qquad \E{Y_i\mid x_i}=\mu_i, \qquad \log \mu_i = x_i^\top \beta,
\]</span> where the log link again enforces positivity of the mean. Alternatives include an inverse link, depending on the modeling context.</p>
<section id="deviance-and-model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="deviance-and-model-comparison">Deviance and model comparison</h3>
<p>In GLMs, goodness-of-fit is often summarized by the deviance, a likelihood-based measure that quantifies how well a fitted model explains the data relative to a benchmark. Understanding deviance requires first understanding what we mean by a saturated model.</p>
<p>A <em>saturated model</em> is one that fits the data perfectly by using as many parameters as there are observations. For binary logistic regression with <span class="math inline">\(n\)</span> distinct covariate patterns, a saturated model assigns a separate probability parameter <span class="math inline">\(\hat{p}_i\)</span> to each observation, setting <span class="math inline">\(\hat{p}_i = y_i\)</span>. This achieves the maximum possible log-likelihood for the given data structure, though at the cost of using <span class="math inline">\(n\)</span> parameters and having no degrees of freedom left for testing or prediction. The saturated model serves as an upper bound on how well any model can fit the observed data.</p>
<p>Formally, the <em>deviance</em> of a fitted GLM is defined as <span class="math display">\[
D = 2\left[\ell(\text{saturated}) - \ell(\hat{\beta})\right],
\]</span> where <span class="math inline">\(\ell(\text{saturated})\)</span> is the log-likelihood of the saturated model and <span class="math inline">\(\ell(\hat{\beta})\)</span> is the log-likelihood of the fitted model with parameter estimates <span class="math inline">\(\hat{\beta}\)</span>. The factor of 2 appears for historical reasons related to asymptotic chi-squared distributions and makes the deviance directly comparable to likelihood ratio test statistics.</p>
<p>For binary logistic regression, this becomes <span class="math display">\[
D = 2\sum_{i=1}^n \left[y_i \log\left(\frac{y_i}{\hat{p}_i}\right) + (1-y_i)\log\left(\frac{1-y_i}{1-\hat{p}_i}\right)\right],
\]</span> where <span class="math inline">\(\hat{p}_i = \sigma(x_i^\top\hat{\beta})\)</span> are the fitted probabilities. When <span class="math inline">\(y_i \in \{0,1\}\)</span>, terms with <span class="math inline">\(y_i\log(y_i)\)</span> or <span class="math inline">\((1-y_i)\log(1-y_i)\)</span> equal zero by convention (<span class="math inline">\(0\log 0 = 0\)</span>), so each observation contributes either <span class="math inline">\(-2\log(\hat{p}_i)\)</span> when <span class="math inline">\(y_i=1\)</span> or <span class="math inline">\(-2\log(1-\hat{p}_i)\)</span> when <span class="math inline">\(y_i=0\)</span>.</p>
<p>Lower deviance indicates better fit: a deviance of zero means the model fits as well as the saturated model. In practice, deviance is most useful for comparing nested models rather than as an absolute measure. Suppose we have two nested models with <span class="math inline">\(M_1 \subset M_2\)</span>, where model <span class="math inline">\(M_1\)</span> has <span class="math inline">\(p_1\)</span> parameters and model <span class="math inline">\(M_2\)</span> has <span class="math inline">\(p_2 &gt; p_1\)</span> parameters. The difference in deviances <span class="math display">\[
\Delta D = D_1 - D_2 = 2\left[\ell(\hat{\beta}_2) - \ell(\hat{\beta}_1)\right]
\]</span> follows approximately a <span class="math inline">\(\chi^2_{p_2-p_1}\)</span> distribution under the null hypothesis that the simpler model <span class="math inline">\(M_1\)</span> is adequate. This provides a formal likelihood ratio test: if <span class="math inline">\(\Delta D\)</span> exceeds the critical value from the chi-squared distribution, we reject <span class="math inline">\(M_1\)</span> in favor of the more complex <span class="math inline">\(M_2\)</span>.</p>
<p>This framework generalizes beyond logistic regression to all GLMs. For Poisson regression with counts <span class="math inline">\(y_i\)</span> and fitted means <span class="math inline">\(\hat{\mu}_i\)</span>, the deviance takes the form <span class="math display">\[
D = 2\sum_{i=1}^n \left[y_i\log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i)\right].
\]</span> The specific formula changes with the exponential family distribution, but the interpretation remains consistent: deviance measures twice the log-likelihood gap between the fitted and saturated models, enabling principled comparison of nested specifications. This connects naturally to the model selection criteria in <a href="16-select.html" class="quarto-xref"><span>Chapter 16</span></a>, where penalized versions of deviance like AIC and BIC trade off fit against model complexity.</p>
</section>
</section>
<section id="sec-bayes-logistic" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="sec-bayes-logistic"><span class="header-section-number">13.7</span> Bayesian Logistic Regression with The Polya-Gamma Distribution</h2>
<p>To perform Bayesian inference for logistic regression, we often need to sample from the posterior distribution of the coefficients <span class="math inline">\(\beta\)</span>. Conceptually this follows the same updating pattern as in <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>: posterior <span class="math inline">\(\propto\)</span> likelihood <span class="math inline">\(\times\)</span> prior. It also connects to the likelihood-to-loss framing in <a href="11-pattern.html" class="quarto-xref"><span>Chapter 11</span></a>: the same likelihood that defines the posterior also defines an optimization objective through its negative log. However, the logistic likelihood does not have a conjugate prior, making direct sampling difficult. A powerful modern approach uses data augmentation with Polya-Gamma variables to make the sampling exact and efficient.</p>
<p>The Polya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions (essentially, a weighted sum of an infinite number of independent Gamma variables)<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. A random variable X follows a Polya-Gamma distribution with parameters <span class="math inline">\(b &gt; 0\)</span> and <span class="math inline">\(c \in \mathbb{R}\)</span> if:</p>
<p><span class="math display">\[
X \stackrel{d}{=} \frac{1}{2\pi^2} \sum_{k=1}^{\infty} \frac{g_k}{(k-1/2)^2 + c^2/(4\pi^2)},
\]</span> where <span class="math inline">\(g_k \sim \text{Ga}(b,1)\)</span> are independent gamma random variables, and <span class="math inline">\(\stackrel{d}{=}\)</span> indicates equality in distribution.</p>
<p>The Polya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:</p>
<ol type="1">
<li><p><strong>Laplace Transform</strong>: For <span class="math inline">\(\omega \sim \text{PG}(b,0)\)</span>, the Laplace transform is <span class="math inline">\(\E{\exp(-\omega t)} = \cosh^{-b}(\sqrt{t}/2)\)</span>.</p></li>
<li><p><strong>Exponential Tilting</strong>: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:</p></li>
</ol>
<p><span class="math display">\[
p(x|b,c) = \frac{\exp(-c^2x/2)p(x|b,0)}{\E{\exp(-c^2\omega/2)}},
\]</span></p>
<p>where the expectation is taken with respect to PG(b,0)</p>
<ol start="3" type="1">
<li><p><strong>Convolution Property</strong>: The family is closed under convolution for random variates with the same tilting parameter</p></li>
<li><p><strong>Known Moments</strong>: All finite moments are available in closed form, with the expectation given by:</p></li>
</ol>
<p><span class="math display">\[\E{\omega} = \frac{b}{2c}\tanh(c/2) = \frac{b}{2c}\frac{e^c-1}{1+e^c}.\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Computational Advantage
</div>
</div>
<div class="callout-body-container callout-body">
<p>The known moments and convolution properties make the Polya-Gamma distribution computationally tractable and theoretically well-behaved.</p>
</div>
</div>
<section id="the-data-augmentation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="the-data-augmentation-strategy">The Data-Augmentation Strategy</h3>
<p>The core of the Polya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The key theorem states:</p>
<div class="theorem">
<p><strong>Theorem 1</strong>: For <span class="math inline">\(b &gt; 0\)</span> and <span class="math inline">\(a \in \mathbb{R}\)</span>, the following integral identity holds:</p>
<p><span class="math display">\[\frac{(e^\psi)^a}{(1+e^\psi)^b} = 2^{-b}e^{\kappa\psi} \int_0^{\infty} e^{-\omega\psi^2/2} p(\omega) d\omega\]</span></p>
<p>where <span class="math inline">\(\kappa = a - b/2\)</span>, and <span class="math inline">\(p(\omega)\)</span> is the density of <span class="math inline">\(\omega \sim \text{PG}(b,0)\)</span><span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>Moreover, the conditional distribution <span class="math inline">\(p(\omega|\psi)\)</span> is also in the Polya-Gamma class: <span class="math inline">\((\omega|\psi) \sim \text{PG}(b,\psi)\)</span><span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
</div>
<p>This integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. For a dataset with observations <span class="math inline">\(y_i \mid \psi_i \sim \text{Binom}(n_i, 1/(1+e^{-\psi_i}))\)</span> where <span class="math inline">\(\psi_i = x_i^T\beta\)</span>, and a Gaussian prior <span class="math inline">\(\beta \sim N(b,B)\)</span>, the algorithm iterates:</p>
<ol type="1">
<li><strong>Sample auxiliary variables</strong>: <span class="math inline">\((\omega_i|\beta) \sim \text{PG}(n_i, x_i^T\beta)\)</span> for each observation</li>
<li><strong>Sample parameters</strong>: <span class="math inline">\((\beta|y,\omega) \sim N(m_\omega, V_\omega)\)</span> where:
<ul>
<li><span class="math inline">\(V_\omega = (X^T\Omega X + B^{-1})^{-1}\)</span></li>
<li><span class="math inline">\(m_\omega = V_\omega(X^T\kappa + B^{-1}b)\)</span></li>
<li><span class="math inline">\(\kappa = (y_1-n_1/2, \ldots, y_n-n_n/2)\)</span></li>
<li><span class="math inline">\(\Omega = \text{diag}(\omega_1, \ldots, \omega_n)\)</span></li>
</ul></li>
</ol>
<p>This approach requires only Gaussian draws for the main parameters and Polya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>We can visualize the hierarchical structure of this data augmentation strategy as follows:</p>
<div class="cell" data-eval="false" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>

</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/pg-mermaid.png" class="img-fluid figure-img"></p>
<figcaption>Hierarchical structure of Polya-Gamma Data Augmentation</figcaption>
</figure>
</div>
<p>The practical success of the Polya-Gamma method depends on efficient simulation of Polya-Gamma random variables<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye <span class="citation" data-cites="devroye1986nonuniform">(<a href="references.html#ref-devroye1986nonuniform" role="doc-biblioref">Devroye 1986</a>)</span>. For the fundamental PG(1,c) case, the sampler uses exponential and inverse-Gaussian draws as proposals, achieving an acceptance probability uniformly bounded below at 0.99919. This high acceptance rate requires no tuning for optimal performance. The acceptance criterion is evaluated using iterative partial sums, making the algorithm both efficient and robust.</p>
<p>For integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property. This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications.</p>
<p>The <code>BayesLogit</code> package provides efficient tools for sampling from the Polya-Gamma distribution. The current version (2.1) focuses on core functionality: sampling from the Polya-Gamma distribution through the <code>rpg()</code> function and its variants.</p>
<p>We demonstrate the <code>BayesLogit</code> using a simulated data example. We generate <span class="math inline">\(n = 100\)</span> observations with two predictors (plus an intercept term). The predictor matrix <span class="math inline">\(X\)</span> consists of an intercept column and two columns of independent standard normal random variables. The true regression coefficients are set to <span class="math inline">\(\beta = (-0.5,1.2,-0.8)\)</span>. The binary outcomes <span class="math inline">\(y_i\)</span> are generated from a binomial distribution where the success probability for each observation follows the logistic transformation <span class="math inline">\(p_i = 1/(1 + \exp(-x_i^T\beta))\)</span>. This setup allows us to verify that the Bayesian inference procedure can recover the true parameter values from the simulated data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="13-logistic_files/figure-html/bayesian-logit-implementation-1.png" class="img-fluid figure-img" width="448"></p>
<figcaption>Posterior distributions for Bayesian Logistic Regression with Polya-Gamma Data Augmentation. Red line shows true value, blue line shows posterior mean.</figcaption>
</figure>
</div>
</div>
</div>
<p>The package offers several sampling methods for <span class="math inline">\(\omega\)</span>.</p>
<ul>
<li><code>rpg()</code>: Main function that automatically selects the best method</li>
<li><code>rpg.devroye()</code>: Devroye-like method for integer <span class="math inline">\(h\)</span> values</li>
<li><code>rpg.gamma()</code>: Sum of gammas method (slower but works for all parameters)</li>
<li><code>rpg.sp()</code>: Saddlepoint approximation method</li>
</ul>
<p>In the example above we use the automatic function <code>rpg</code>.</p>
</section>
</section>
<section id="bayesian-analysis-of-horse-race-betting" class="level2 exm-betting" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="bayesian-analysis-of-horse-race-betting"><span class="header-section-number">13.8</span> Bayesian Analysis of Horse Race Betting</h2>
<p>We now apply the Polya-Gamma methodology to the horse racing data from <a href="#exm-HorseRacing" class="quarto-xref">Example&nbsp;<span>13.3</span></a>, where we predict whether the race favorite wins. The Bayesian approach provides posterior distributions for the coefficients, quantifying uncertainty in our predictions, which is essential when making betting decisions.</p>
<p>The implementation uses Polya-Gamma augmentation to efficiently sample from the posterior distribution. The full R code for this analysis is available in <code>case_studies/betting/benter.R</code>, which implements the Gibbs sampler and generates all results shown below.</p>
<div id="fig-bayesian-betting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayesian-betting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bayesian-betting-plot.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayesian-betting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.4: Posterior distributions for horse racing logistic regression coefficients. Red dashed lines show MLE estimates; the Bayesian posteriors capture uncertainty around these point estimates.
</figcaption>
</figure>
</div>
<p>The posterior distributions (<a href="#fig-bayesian-betting" class="quarto-xref">Figure&nbsp;<span>13.4</span></a>) reveal important insights for betting strategy. <a href="#tbl-bayesian-betting-summary" class="quarto-xref">Table&nbsp;<span>13.1</span></a> shows the posterior summaries compared to maximum likelihood estimates:</p>
<div id="tbl-bayesian-betting-summary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-bayesian-betting-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;13.1: Bayesian posterior summaries for horse racing logistic regression
</figcaption>
<div aria-describedby="tbl-bayesian-betting-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Posterior Mean</th>
<th>Posterior SD</th>
<th>95% CI Lower</th>
<th>95% CI Upper</th>
<th>MLE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intercept</td>
<td>-0.885</td>
<td>0.030</td>
<td>-0.945</td>
<td>-0.828</td>
<td>-0.885</td>
</tr>
<tr class="even">
<td>Public Prob (scaled)</td>
<td>0.429</td>
<td>0.029</td>
<td>0.372</td>
<td>0.486</td>
<td>0.430</td>
</tr>
<tr class="odd">
<td>Field Size (scaled)</td>
<td>0.046</td>
<td>0.029</td>
<td>-0.011</td>
<td>0.103</td>
<td>0.045</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The Bayesian analysis confirms our findings: the public probability is a strong positive predictor of favorite success (posterior mean = 0.429, 95% CI: [0.372, 0.486]). The field size effect is weakly positive but uncertain, with the 95% credible interval spanning from -0.011 to 0.103, and only a 5.4% posterior probability that this effect is negative. This uncertainty is valuable for betting applications: we can propagate the posterior distribution through the Kelly criterion to obtain a <em>distribution</em> of optimal bet sizes rather than a single point estimate, leading to more robust position sizing.</p>
</section>
<section id="theoretical-guarantees" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-guarantees">Theoretical Guarantees</h3>
<p>The Polya-Gamma Gibbs sampler enjoys strong theoretical properties that distinguish it from many other MCMC approaches <span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. Perhaps most importantly, <span class="citation" data-cites="choi2013polya">Choi and Hobert (<a href="references.html#ref-choi2013polya" role="doc-biblioref">2013</a>)</span> proved that the sampler is <em>uniformly ergodic</em>, a property that guarantees geometric convergence to the stationary distribution regardless of the starting point. This uniform ergodicity result ensures that central limit theorems hold for Monte Carlo averages computed from the sampler output, enabling valid inference and standard error calculations for posterior quantities of interest.</p>
<p>The property of requiring no tuning parameters represents a significant practical advantage. Traditional Metropolis-Hastings algorithms demand careful calibration of proposal distributions, a task that typically requires pilot runs, domain expertise, and iterative adjustment <span class="citation" data-cites="roberts2001optimal">(<a href="references.html#ref-roberts2001optimal" role="doc-biblioref">Roberts and Rosenthal 2001</a>)</span>. Poorly tuned proposals can lead to slow mixing or even apparent convergence to incorrect distributions, and diagnosing such failures often proves difficult. The Polya-Gamma sampler sidesteps these concerns entirely: each conditional distribution in the Gibbs sampling scheme is fully specified by the model, leaving no algorithmic parameters for the practitioner to adjust. This makes the method accessible to users without deep expertise in MCMC diagnostics and eliminates a common source of errors in applied Bayesian analysis.</p>
<p>Equally important is the property of exact sampling from the target posterior distribution. Many popular alternatives to MCMC, such as variational inference or Laplace approximations, sacrifice exactness for computational speed, producing samples from an approximating distribution rather than the true posterior. While such approximations work well in many settings, they can introduce systematic biases that are difficult to quantify, particularly in hierarchical models where the posterior may exhibit complex dependencies. The Polya-Gamma Gibbs sampler, by contrast, produces draws from the exact posterior distribution. Combined with the ergodicity guarantee, this means that averages of the sampled values converge to true posterior expectations as the number of iterations increases, providing asymptotically exact inference without the approximation errors inherent in alternative methods</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Important Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.</p>
</div>
</div>
<p>The Polya-Gamma methodology extends naturally to negative binomial regression through direct application of the same data-augmentation scheme. For multinomial logistic models, the approach can be extended through partial difference of random utility models<span class="citation" data-cites="windle2014sampling">(<a href="references.html#ref-windle2014sampling" role="doc-biblioref">Windle, Polson, and Scott 2014</a>)</span>. The framework also allows seamless incorporation of random effects structures in mixed effects models, and provides efficient inference for spatial count data models.</p>
<p>Recent developments have expanded the methodology’s applicability. <span class="citation" data-cites="zhang2018scalable">Zhang, Datta, and Banerjee (<a href="references.html#ref-zhang2018scalable" role="doc-biblioref">2018</a>)</span> applied it to Gaussian process classification that relies on scalable variational approaches using Polya-Gamma augmentation. <span class="citation" data-cites="ye2023calibration">Ye et al. (<a href="references.html#ref-ye2023calibration" role="doc-biblioref">2023</a>)</span> demonstrated integration with neural network architectures by adding a Gaussian process layer with Pólya-Gamma augmentation to deep neural retrieval models for improved calibration and uncertainty estimation. <span class="citation" data-cites="fruhwirth-schnatter2010stochastic">Frühwirth-Schnatter and Wagner (<a href="references.html#ref-fruhwirth-schnatter2010stochastic" role="doc-biblioref">2010</a>)</span> showed application to dynamic binary time series models through state-space formulations.</p>
<p>The Polya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency. Its introduction of the Polya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive.</p>
<p>As computational demands continue to grow and models become increasingly complex, the Polya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit <span class="citation" data-cites="tiao2019polyagamma">(<a href="references.html#ref-tiao2019polyagamma" role="doc-biblioref">Tiao 2019</a>)</span>. Ongoing research continues to extend the Polya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-choi2013polya" class="csl-entry" role="listitem">
Choi, Hee Min, and James P Hobert. 2013. <span>“Uniform Ergodicity of the <span>Polya-Gamma Gibbs</span> Sampler.”</span> <em>Electronic Journal of Statistics</em> 7: 2054–64.
</div>
<div id="ref-devroye1986nonuniform" class="csl-entry" role="listitem">
Devroye, Luc. 1986. <em>Non-Uniform Random Variate Generation</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-fruhwirth-schnatter2010stochastic" class="csl-entry" role="listitem">
Frühwirth-Schnatter, Sylvia, and Helga Wagner. 2010. <span>“Stochastic Model Specification Search for <span>Gaussian</span> and Partial Non-<span>Gaussian</span> State Space Models.”</span> <em>Journal of Econometrics</em> 154: 85–100.
</div>
<div id="ref-gan2016how" class="csl-entry" role="listitem">
Gan, Link, and Alan Fritzler. 2016. <span>“How to Become an Executive.”</span>
</div>
<div id="ref-irwin2016how" class="csl-entry" role="listitem">
Irwin, Neil. 2016. <span>“How to <span>Become</span> a <span>C</span>.<span>E</span>.<span>O</span>.? <span>The Quickest Path Is</span> a <span>Winding One</span>.”</span> <em>The New York Times</em>, September.
</div>
<div id="ref-polson2013bayesian" class="csl-entry" role="listitem">
Polson, Nicholas G., James G. Scott, and Jesse Windle. 2013. <span>“Bayesian <span>Inference</span> for <span>Logistic Models Using Pólya</span>–<span>Gamma Latent Variables</span>.”</span> <em>Journal of the American Statistical Association</em> 108 (504): 1339–49.
</div>
<div id="ref-roberts2001optimal" class="csl-entry" role="listitem">
Roberts, Gareth O., and Jeffrey S. Rosenthal. 2001. <span>“Optimal Scaling for Various <span>Metropolis-Hastings</span> Algorithms.”</span> <em>Statistical Science</em> 16 (4): 351–67.
</div>
<div id="ref-tiao2019polyagamma" class="csl-entry" role="listitem">
Tiao, Louis. 2019. <span>“Pólya-<span>Gamma Bayesian</span> Logistic Regression.”</span> Blog post.
</div>
<div id="ref-windle2014sampling" class="csl-entry" role="listitem">
Windle, Jesse, Nicholas G. Polson, and James G. Scott. 2014. <span>“Sampling <span>Polya-Gamma</span> Random Variates: <span>Alternate</span> and Approximate Techniques.”</span> arXiv. <a href="https://arxiv.org/abs/1405.0506">https://arxiv.org/abs/1405.0506</a>.
</div>
<div id="ref-ye2023calibration" class="csl-entry" role="listitem">
Ye, Tong, Shijing Si, Jianzong Wang, Ning Cheng, Zhitao Li, and Jing Xiao. 2023. <span>“On the Calibration and Uncertainty with <span>Pólya-Gamma</span> Augmentation for Dialog Retrieval Models.”</span> In <em>Proceedings of the <span>AAAI</span> Conference on Artificial Intelligence</em>. <a href="https://arxiv.org/abs/2303.08606">https://arxiv.org/abs/2303.08606</a>.
</div>
<div id="ref-zhang2018scalable" class="csl-entry" role="listitem">
Zhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. <span>“Scalable <span>Gaussian</span> Process Classification with <span>Pólya-Gamma</span> Data Augmentation.”</span> <em>arXiv Preprint arXiv:1802.06383</em>. <a href="https://arxiv.org/abs/1802.06383">https://arxiv.org/abs/1802.06383</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./12-regression.html" class="pagination-link" aria-label="Linear Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./14-tree.html" class="pagination-link" aria-label="Tree Models">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>