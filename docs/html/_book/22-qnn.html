<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>22&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./23-cnn.html" rel="next">
<link href="./21-sgd.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left[#2\\right]", 2, ""],
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./22-qnn.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#meu" id="toc-meu" class="nav-link active" data-scroll-target="#meu"><span class="header-section-number">22.1</span> MEU</a></li>
  <li><a href="#bayes-rule-for-quantiles" id="toc-bayes-rule-for-quantiles" class="nav-link" data-scroll-target="#bayes-rule-for-quantiles"><span class="header-section-number">22.2</span> Bayes Rule for Quantiles</a>
  <ul class="collapse">
  <li><a href="#maximum-expected-utility" id="toc-maximum-expected-utility" class="nav-link" data-scroll-target="#maximum-expected-utility">Maximum Expected Utility</a></li>
  </ul></li>
  <li><a href="#normal-normal-bayes-learning-wang-distortion" id="toc-normal-normal-bayes-learning-wang-distortion" class="nav-link" data-scroll-target="#normal-normal-bayes-learning-wang-distortion"><span class="header-section-number">22.3</span> Normal-Normal Bayes Learning: Wang Distortion</a>
  <ul class="collapse">
  <li><a href="#numerical-example" id="toc-numerical-example" class="nav-link" data-scroll-target="#numerical-example">Numerical Example</a></li>
  </ul></li>
  <li><a href="#portfolio-learning" id="toc-portfolio-learning" class="nav-link" data-scroll-target="#portfolio-learning"><span class="header-section-number">22.4</span> Portfolio Learning</a></li>
  <li><a href="#bayes-rule-for-quantiles-1" id="toc-bayes-rule-for-quantiles-1" class="nav-link" data-scroll-target="#bayes-rule-for-quantiles-1"><span class="header-section-number">22.5</span> Bayes Rule for Quantiles</a></li>
  <li><a href="#normal-normal-bayes-learning-wang-distortion-1" id="toc-normal-normal-bayes-learning-wang-distortion-1" class="nav-link" data-scroll-target="#normal-normal-bayes-learning-wang-distortion-1"><span class="header-section-number">22.6</span> Normal-Normal Bayes Learning: Wang Distortion</a>
  <ul class="collapse">
  <li><a href="#numerical-example-1" id="toc-numerical-example-1" class="nav-link" data-scroll-target="#numerical-example-1">Numerical Example</a></li>
  </ul></li>
  <li><a href="#portfolio-learning-1" id="toc-portfolio-learning-1" class="nav-link" data-scroll-target="#portfolio-learning-1"><span class="header-section-number">22.7</span> Portfolio Learning</a>
  <ul class="collapse">
  <li><a href="#empirical-example" id="toc-empirical-example" class="nav-link" data-scroll-target="#empirical-example">Empirical Example</a></li>
  </ul></li>
  <li><a href="#learning-quantiles" id="toc-learning-quantiles" class="nav-link" data-scroll-target="#learning-quantiles"><span class="header-section-number">22.8</span> Learning Quantiles</a>
  <ul class="collapse">
  <li><a href="#cosine-embedding-for-tau" id="toc-cosine-embedding-for-tau" class="nav-link" data-scroll-target="#cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></a></li>
  </ul></li>
  <li><a href="#synthetic-data" id="toc-synthetic-data" class="nav-link" data-scroll-target="#synthetic-data"><span class="header-section-number">22.9</span> Synthetic Data</a></li>
  <li><a href="#quantiles-as-deep-learners" id="toc-quantiles-as-deep-learners" class="nav-link" data-scroll-target="#quantiles-as-deep-learners"><span class="header-section-number">22.10</span> Quantiles as Deep Learners</a>
  <ul class="collapse">
  <li><a href="#quantile-reinforcement-learning" id="toc-quantile-reinforcement-learning" class="nav-link" data-scroll-target="#quantile-reinforcement-learning">Quantile Reinforcement Learning</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./22-qnn.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><strong>GBC</strong> Let <span class="math inline">\((X,Y) \sim P_{X,Y}\)</span> be input-output pairs and <span class="math inline">\(P_{X,Y}\)</span> a joint measure from which we can simulate a training dataset <span class="math inline">\((X_i, Y_i)_{i=1}^N \sim P_{X,Y}\)</span>. Standard prediction techniques for the conditional posterior mean <span class="math inline">\(\hat{X}(Y) = E(X|Y) = f(Y)\)</span> of the input given the output. To do this, consider the multivariate non-parametric regression <span class="math inline">\(X = f(Y) + \epsilon\)</span> and provide methods for estimating the conditional mean. Typically estimators, <span class="math inline">\(\hat{f}\)</span>, include KNN and Kernel methods. Recently, deep learners have been proposed and the theoretical properties of superpositions of affine functions (a.k.a. ridge functions) have been provided (see Montanni and Yang, Schmidt-Hieber, Polson and Rockova).</p>
<p>Generative methods take this approach one step further. Let <span class="math inline">\(Z \sim P_Z\)</span> be a base measure for a latent variable, <span class="math inline">\(Z\)</span>, typically a standard multivariate normal or vector of uniforms. The goal of generative methods is to characterize the posterior measure <span class="math inline">\(P_{X|Y}\)</span> from the training data <span class="math inline">\((X_i, Y_i)_{i=1}^N \sim P_{X,Y}\)</span> where <span class="math inline">\(N\)</span> is chosen to be suitably large. A deep learner is used to estimate <span class="math inline">\(\hat{f}\)</span> via the non-parametric regression <span class="math inline">\(X = f(Y, Z)\)</span>. In the case where <span class="math inline">\(Z\)</span> is uniform, this amounts to inverse cdf sampling, namely <span class="math inline">\(X = F_{X|Y}^{-1}(U)\)</span>.</p>
<p>In general, we characterize the posterior map for <em>any</em> output <span class="math inline">\(Y\)</span>. Simply evaluate the network at any <span class="math inline">\(Y\)</span> via the transport map <span class="math display">\[
X = H(S(Y), \psi(Z))
\]</span> from a new base draw, <span class="math inline">\(Z\)</span>. Here <span class="math inline">\(\psi\)</span> denotes the cosine embedding so that the architecture for the latent variable corresponds to a Fourier approximation with rates of convergence given by <span class="math inline">\(O(N^{-\frac{1}{2}})\)</span>, see Barron (1993). The deep learner is estimated via a quantile NN from the triples <span class="math inline">\((X_i, Y_i, Z_i)_{i=1}^N \sim P_{X,Y} \times P_Z\)</span>. The ensuing estimator <span class="math inline">\(\hat{H}_N\)</span> can be thought of as a transport map from the base distribution to the posterior as required.</p>
<p>Specifically, the idea of generative methods is straightforward. Let <span class="math inline">\(y\)</span> denote data and <span class="math inline">\(\theta\)</span> a vector of parameters including any hidden states (a.k.a. latent variables) <span class="math inline">\(z\)</span>. First, we generate a “look-up” table of “fake” data <span class="math inline">\(\{y^{(i)}, \theta^{(i)}\}_{i=1}^N\)</span>. By simulating a training dataset of outputs and parameters allows us to use deep learning to solve for the inverse map via a supervised learning problem. Generative methods have the advantage of being likelihood-free. For example, our model might be specified by a forward map <span class="math inline">\(y^{(i)} = f(\theta^{(i)})\)</span> rather than a traditional random draw from a likelihood function <span class="math inline">\(y^{(i)} \sim p(y^{(i)}|\theta^{(i)})\)</span>.</p>
<p>Generative methods have a number of advantages. First, they are density free. Hence they can be applied in a variety of contexts such as computer simulation, economics where traditional methods are computationally harder. Secondly, they naturally extend to decision problems. Third, they exploit the use of deep neural networks such as Quantile NNs. Hence they naturally provide good forecasting tools.</p>
<p>Posterior uncertainty is solved via the inverse non-parametric regression problem where we predict <span class="math inline">\(\theta^{(i)}\)</span> from <span class="math inline">\(y^{(i)}\)</span> and <span class="math inline">\(\tau^{(i)}\)</span> which is an independent base distribution, <span class="math inline">\(p(\tau)\)</span>. The base distribution is typically uniform or a very large dimensional Gaussian vector. Then we need to train a deep neural network, <span class="math inline">\(H\)</span>, on <span class="math display">\[
\theta^{(i)} = H(S(y^{(i)}), \tau^{(i)}).
\]</span> Here <span class="math inline">\(S(y)\)</span> is a statistic to perform dimension reduction with respect to the signal distribution. Specifying <span class="math inline">\(H\)</span> is the key to the efficiency of the approach. <span class="citation" data-cites="polson2024generative">Polson, Ruggeri, and Sokolov (<a href="references.html#ref-polson2024generative" role="doc-biblioref">2024</a>)</span> propose the use of quantile neural networks implemented with ReLU activation functions.</p>
<section id="meu" class="level2" data-number="22.1">
<h2 data-number="22.1" class="anchored" data-anchor-id="meu"><span class="header-section-number">22.1</span> MEU</h2>
<p>To extend our generative method to MEU problems, we assume that the utility function <span class="math inline">\(U\)</span> is given. Then we simply draw additional associated utilities <span class="math inline">\(U^{(i)}_d \defeq U(d,\theta^{(i)})\)</span> for a given decision <span class="math inline">\(d\)</span> and <span class="math inline">\(\theta^{(i)}\)</span> draw from above. Then we append the utilities to our training dataset including the baseline distribution <span class="math inline">\(\tau^{(i)}\)</span> to yield a new training dataset <span class="math display">\[
\{U_d^{(i)}, y^{(i)}, \theta^{(i)}, \tau^{(i)}\}_{i=1}^N.
\]</span> Now we construct a non-parametric estimator of the form <span class="math display">\[
U_d^{(i)} = H(S(y^{(i)}), \theta^{(i)}, \tau^{(i)}, d),
\]</span> Given that the posterior quantiles of the distributional utility, denoted by <span class="math inline">\(F^{-1}_{U|d,y}(\tau)\)</span> are represented as a quantile neural network, we then use a key identity which shows how to represent any expectation as a marginal over quantiles, namely <span class="math display">\[
E_{\theta|y}[U(d, \theta)] = \int_0^1 F^{-1}_{U|d,y}(\tau) d\tau
\]</span> The optimal decision function simply maximizes the expected utility <span class="math display">\[
d^\star(y) \defeq \arg \max_d E_{\theta|y}[U(d, \theta)]
\]</span></p>
<p>To fix notation and to allow for deterministic updates (a.k.a. measures rather than probabilities). Let <span class="math inline">\(\mathcal{Y}\)</span> denote a locally compact metric space of signals, denoted by <span class="math inline">\(y\)</span>, and <span class="math inline">\(\mathcal{B}(\mathcal{Y})\)</span> the Borel <span class="math inline">\(\sigma\)</span>-algebra of <span class="math inline">\(\mathcal{Y}\)</span>. Let <span class="math inline">\(\lambda\)</span> be a measure on the measurable space of signals <span class="math inline">\((\mathcal{Y}, \mathcal{B}(\mathcal{Y}))\)</span>. Let <span class="math inline">\(P(dy|\theta)\)</span> denote the conditional distribution of signals given the parameters. Let <span class="math inline">\(\Theta\)</span> denote a locally compact metric space of admissible parameters (a.k.a. hidden states and latent variables <span class="math inline">\(z \in \mathcal{Z}\)</span>) and <span class="math inline">\(\mathcal{B}(\Theta)\)</span> the Borel <span class="math inline">\(\sigma\)</span>-algebra of <span class="math inline">\(\Theta\)</span>. Let <span class="math inline">\(\mu\)</span> be a measure on the measurable space of parameters <span class="math inline">\((\Theta, \mathcal{B}(\Theta))\)</span>. Let <span class="math inline">\(\Pi(d\theta|y)\)</span> denote the conditional distribution of the parameters given the observed signal <span class="math inline">\(y\)</span> (a.k.a., the posterior distribution). In many cases, <span class="math inline">\(\Pi\)</span> is absolutely continuous with density <span class="math inline">\(\pi\)</span> such that <span class="math display">\[
\Pi(d\theta|y) = \pi(\theta|y) \mu(d\theta).
\]</span> Moreover, we will write <span class="math inline">\(\Pi(d\theta) = \pi(\theta) \mu(d\theta)\)</span> for prior density <span class="math inline">\(\pi\)</span> when available. In the case of likelihood-free models, the output is simply specified by a map (a.k.a. forward equation) <span class="math display">\[
y = f(\theta)
\]</span> When a likelihood <span class="math inline">\(p(y|\theta)\)</span> is available w.r.t. the measure <span class="math inline">\(\lambda\)</span>, we write <span class="math display">\[
P(dy|\theta) = p(y|\theta) \lambda(dy).
\]</span> There are a number of advantages of such an approach, primarily the fact that they are density free. They use simulation methods and deep neural networks to invert the prior to posterior map. We build on this framework and show how to incorporate utilities into the generative procedure.</p>
<p><strong>Noise Outsourcing Theorem</strong> If <span class="math inline">\((Y, \Theta)\)</span> are random variables in a Borel space <span class="math inline">\((\mathcal{Y}, \Theta)\)</span> then there exists an r.v. <span class="math inline">\(\tau \sim U(0,1)\)</span> which is independent of <span class="math inline">\(Y\)</span> and a function <span class="math inline">\(H: [0,1] \times \mathcal{Y} \rightarrow \Theta\)</span> such that <span class="math display">\[
(Y, \Theta) \stackrel{a.s.}{=} (Y, H(Y, \tau))
\]</span> Hence the existence of <span class="math inline">\(H\)</span> follows from the noise outsourcing theorem <span class="citation" data-cites="kallenberg1997foundations">Kallenberg (<a href="references.html#ref-kallenberg1997foundations" role="doc-biblioref">1997</a>)</span>. Moreover, if there is a statistic <span class="math inline">\(S(Y)\)</span> with <span class="math inline">\(Y\)</span> independent of <span class="math inline">\(\Theta | S(Y)\)</span>, then <span class="math display">\[
\Theta\mid Y \stackrel{a.s.}{=} H(S(Y), \tau).
\]</span> The role of <span class="math inline">\(S(Y)\)</span> is equivalent to the ABC literature. It performs dimension reduction in <span class="math inline">\(n\)</span>, the dimensionality of the signal. Our approach then is to use deep neural network first to calculate the inverse probability map (a.k.a posterior) <span class="math inline">\(\theta \stackrel{D}{=} F^{-1}_{\theta|y}(\tau)\)</span> where <span class="math inline">\(\tau\)</span> is a vector of uniforms. In the multi-parameter case, we use an RNN or autoregressive structure where we model a vector via a sequence <span class="math inline">\((F^{-1}_{\theta_1}(\tau_1), F^{-1}_{\theta_2|\theta_1}(\tau_2), \ldots)\)</span>. A remarkable result due to <span class="citation" data-cites="brillinger2012generalized">Brillinger (<a href="references.html#ref-brillinger2012generalized" role="doc-biblioref">2012</a>)</span> shows that we can learn <span class="math inline">\(S\)</span> independent of <span class="math inline">\(H\)</span> simply via OLS.</p>
<p>As a default choice of network architecture, we will use a ReLU network for the posterior quantile map. The first layer of the network is given by the utility function and hence this is what makes the method different from learning the posterior and then directly using naive Monte Carlo to estimate expected utility. This would be inefficient as quite often the utility function places high weight on region of low-posterior probability representing tail risk.</p>
</section>
<section id="bayes-rule-for-quantiles" class="level2" data-number="22.2">
<h2 data-number="22.2" class="anchored" data-anchor-id="bayes-rule-for-quantiles"><span class="header-section-number">22.2</span> Bayes Rule for Quantiles</h2>
<p><span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> shows that quantile methods are direct alternatives to density computations. Specifically, given <span class="math inline">\(F_{\theta|y} (u)\)</span>, a non-decreasing and continuous from right function, we define</p>
<p><span class="math display">\[Q_{\theta| y} (u) \defeq  F^{-1}_{\theta|y}  ( u ) = \inf \left ( \theta : F_{\theta|y} (\theta) \geq u \right )\]</span></p>
<p>which is non-decreasing, continuous from left.</p>
<p><span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> shows the important probabilistic property of quantiles</p>
<p><span class="math display">\[\theta \stackrel{P}{=} Q_\theta ( F_\theta (\theta ) )\]</span></p>
<p>Hence, we can increase the efficiency by ordering the samples of <span class="math inline">\(\theta\)</span> and the baseline distribution and use monotonicity of the inverse CDF map.</p>
<p>Let <span class="math inline">\(g(y)\)</span> be non-decreasing and continuous from left with <span class="math inline">\(g^{-1} (z ) = \sup \left ( y : g(y ) \leq z \right )\)</span>. Then, the transformed quantile has a compositional nature, namely</p>
<p><span class="math display">\[Q_{ g(Y) } ( u ) = g ( Q (u ))\]</span></p>
<p>Hence, quantiles act as superposition (a.k.a. deep Learner).</p>
<p>This is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation</p>
<p><span class="math display">\[Q_{ \theta | Y=y } ( u ) = Q_\theta ( s )  \; \; \text{where}\; \;   s = Q_{ F(\theta) |  Y=y } ( u)\]</span></p>
<p>To compute <span class="math inline">\(s\)</span>, by definition <span class="math display">\[
u = F_{ F(\theta ) | Y=y} ( s  ) = P( F (\theta ) \leq s | Y=y ) = P( \theta \leq Q_\theta (s ) | Y=y )  = F_{ \theta | Y=y } ( Q_\theta ( s ) )
\]</span></p>
<section id="maximum-expected-utility" class="level3">
<h3 class="anchored" data-anchor-id="maximum-expected-utility">Maximum Expected Utility</h3>
<p>Decision problems are characterized by a utility function <span class="math inline">\(U( \theta , d )\)</span> defined over parameters, <span class="math inline">\(\theta\)</span>, and decisions, <span class="math inline">\(d \in \mathcal{D}\)</span>. We will find it useful to define the family of utility random variables indexed by decisions defined by</p>
<p><span class="math display">\[U_d \defeq U( \theta , d ) \; \; \text{where}\; \; \theta \sim \Pi ( d  \theta )\]</span></p>
<p>Optimal Bayesian decisions are then defined by the solution to the prior expected utility <span class="math display">\[
U(d) = E_{\theta}(U(d,\theta)) = \int U(d,\theta)p(\theta)d\theta
\]</span></p>
<p><span class="math display">\[
d^\star = \arg \max_d U(d)
\]</span></p>
<p>When information in the form of signals <span class="math inline">\(y\)</span> is available, we need to calculate the posterior distribution <span class="math inline">\(p( \theta | y ) = f(y | \theta ) p( \theta )/ p(y)\)</span>. Then we have to solve for the optimal <em>a posterior</em> decision rule <span class="math inline">\(d^\star (y)\)</span> defined by <span class="math display">\[
d^\star(y)  = \arg \max_d  \; \int U( \theta , d ) p( \theta | y ) d \theta
\]</span> where expectations are now taken w.r.t. <span class="math inline">\(p( \theta \mid y)\)</span> the posterior distribution.</p>
</section>
</section>
<section id="normal-normal-bayes-learning-wang-distortion" class="level2" data-number="22.3">
<h2 data-number="22.3" class="anchored" data-anchor-id="normal-normal-bayes-learning-wang-distortion"><span class="header-section-number">22.3</span> Normal-Normal Bayes Learning: Wang Distortion</h2>
<p>For the purpose of illustration, we consider the normal-normal learning model. We will develop the necessary quantile theory to show how to calculate posteriors and expected utility without resorting to densities. Also, we show a relationship with Wang’s risk distortion measure as the deep learning that needs to be learned.</p>
<p>Specifically, we observe the data <span class="math inline">\(y = ( y_1,\ldots,y_n)\)</span> from the following model</p>
<p><span class="math display">\[
y_1 , \ldots , y_n  \mid \theta \sim N(\theta, \sigma^2)
\]</span></p>
<p><span class="math display">\[
\theta \sim N(\mu,\alpha^2)
\]</span></p>
<p>Hence, the summary (sufficient) statistic is <span class="math inline">\(S(y) = \bar y = \frac{1}{n} \sum_{i=1}^n y_i\)</span>.</p>
<p>Given observed samples <span class="math inline">\(y = (y_1,\ldots,y_n)\)</span>, the posterior is then <span class="math inline">\(\theta \mid y \sim N(\mu_*, \sigma_*^2)\)</span> with</p>
<p><span class="math display">\[
\mu_* = (\sigma^2 \mu + \alpha^2s) / t, \quad \sigma^2_* = \alpha^2 \sigma^2 / t
\]</span></p>
<p>where</p>
<p><span class="math display">\[
t =  \sigma^2 + n\alpha^2 \; \; \text{and}\; \; s(y) = \sum_{i=1}^{n}y_i
\]</span></p>
<p>The posterior and prior CDFs are then related via the <span class="math display">\[
1-\Phi(\theta, \mu_*,\sigma_*) = g(1 - \Phi(\theta, \mu, \alpha^2))
\]</span> where <span class="math inline">\(\Phi\)</span> is the normal distribution function. Here the Wang distortion function defined by <span class="math display">\[
g(p) = \Phi\left(\lambda_1 \Phi^{-1}(p) + \lambda\right)
\]</span> where</p>
<p><span class="math display">\[\lambda_1 = \dfrac{\alpha}{\sigma_*} \; \; \text{and}\; \; \lambda = \alpha\lambda_1(s-n\mu)/t\]</span></p>
<p>The proof is relatively simple and is as follows</p>
<p><span class="math display">\[\begin{align*}
    g(1 - \Phi(\theta, \mu, \alpha^2)) &amp; = g(\Phi(-\theta, \mu, \alpha^2)) = g\left(\Phi\left(-\dfrac{\theta - \mu}{\alpha}\right)\right)\\
    &amp; = \Phi\left(\lambda_1 \left(-\dfrac{\theta - \mu}{\alpha}\right) + \lambda\right) =  1 - \Phi\left(\dfrac{\theta - (\mu+ \alpha\lambda/\lambda_1)}{\alpha/\lambda_1}\right)
\end{align*}\]</span></p>
<p>Thus, the corresponding posterior updated parameters are</p>
<p><span class="math display">\[\sigma_* = \alpha/\lambda_1, \quad \lambda_1 = \dfrac{\alpha}{\sigma_*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mu_* = \mu+ \alpha\lambda/\lambda_1, \quad \lambda = \dfrac{\lambda_1(\mu_* - \mu)}{\alpha} = \alpha\lambda_1(s-n\mu)/t\]</span></p>
<p>We now provide an empirical example.</p>
<section id="numerical-example" class="level3">
<h3 class="anchored" data-anchor-id="numerical-example">Numerical Example</h3>
<p>Consider the normal-normal model with Prior <span class="math inline">\(\theta \sim N(0,5)\)</span> and likelihood <span class="math inline">\(y \sim N(3,10)\)</span>. We generate <span class="math inline">\(n=100\)</span> samples from the likelihood and calculate the posterior distribution.</p>
<div id="fig-wang" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Model for simulated data
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang2.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Distortion Function <span class="math inline">\(g\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang1.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) 1 - <span class="math inline">\(\Phi\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22.1: Density for prior, likelihood and posterior, distortion function and 1 - <span class="math inline">\(\Phi\)</span> for the prior and posterior of the normal-normal model.
</figcaption>
</figure>
</div>
<p>The posterior distribution calculated from the sample is then <span class="math inline">\(\theta \mid y \sim N(3.28, 0.98)\)</span>.</p>
<p><a href="#fig-wang" class="quarto-xref">Figure&nbsp;<span>22.2</span></a> shows the Wang distortion function for the normal-normal model. The left panel shows the model for the simulated data, while the middle panel shows the distortion function, the right panel shows the 1 - <span class="math inline">\(\Phi\)</span> for the prior and posterior of the normal-normal model.</p>
</section>
</section>
<section id="portfolio-learning" class="level2" data-number="22.4">
<h2 data-number="22.4" class="anchored" data-anchor-id="portfolio-learning"><span class="header-section-number">22.4</span> Portfolio Learning</h2>
<p>Consider power utility and log-normal returns (without leverage). For <span class="math inline">\(\omega \in (0,1)\)</span></p>
<p><span class="math display">\[U(W) = -e^{-\gamma W}, ~ W\mid \omega \sim \mathcal{N}( (1-\omega) r_f + \omega R,\sigma^2)\]</span></p>
<p>Let <span class="math inline">\(W = (1-\omega)r_f + \omega R\)</span>, with <span class="math inline">\(R \sim N(\mu,\sigma^2)\)</span>, Here, <span class="math inline">\(U^{-1}\)</span> exists and <span class="math inline">\(r_f\)</span> is the risk-free rate, <span class="math inline">\(\mu\)</span> is the mean return and <span class="math inline">\(\tau^2\)</span> is the variance of the return. Then the expected utility is</p>
<p><span class="math display">\[U(\omega) = E(-e^{\gamma W}) = \exp\left\{\gamma E(W) + \frac{1}{2}\omega^2Var(W)\right\}\]</span></p>
<p>We have closed-form utility in this case, since it is the moment-generating function of the log-normal. Within the Gen-AI framework, it is easy to add learning or uncertainty on top of <span class="math inline">\(\sigma^2\)</span> and have a joint posterior distribution <span class="math inline">\(p(\mu, \sigma^2 \mid R)\)</span>.</p>
<p>Thus, the closed form solution is</p>
<p><span class="math display">\[U(\omega) = \exp\left\{\gamma \left\{(1-\omega)r_f + \omega\mu\right\}\right\} \exp \left \{ \dfrac{1}{2}\gamma^2\omega^2\sigma^2 \right \}\]</span></p>
<p>The optimal Kelly-Brieman-Thorpe-Merton rule is given by</p>
<p><span class="math display">\[\omega^* = (\mu - r_f)/(\sigma^2\gamma)\]</span></p>
<p>Now we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of <span class="math inline">\(U\)</span></p>
<p><span class="math display">\[E(U(W)) = \int_{0}^{1}F_{U(W)}^{-1}(\tau)d\tau\]</span></p>
<p>Hence, if we can approximate the inverse of the CDF of <span class="math inline">\(U(W)\)</span> with a quantile NN, we can approximate the expected utility and optimize over <span class="math inline">\(\omega\)</span>.</p>
<p>The stochastic utility is modeled with a deep neural network, and we write</p>
</section>
<section id="bayes-rule-for-quantiles-1" class="level2" data-number="22.5">
<h2 data-number="22.5" class="anchored" data-anchor-id="bayes-rule-for-quantiles-1"><span class="header-section-number">22.5</span> Bayes Rule for Quantiles</h2>
<p><span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> shows that quantile methods are direct alternatives to density computations. Specifically, given <span class="math inline">\(F_{\theta|y} (u)\)</span>, a non-decreasing and continuous from right function, we define</p>
<p><span class="math display">\[Q_{\theta| y} (u) \defeq  F^{-1}_{\theta|y}  ( u ) = \inf \left ( \theta : F_{\theta|y} (\theta) \geq u \right )\]</span></p>
<p>which is non-decreasing, continuous from left. <span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> shows the important probabilistic property of quantiles <span class="math display">\[
\theta \stackrel{P}{=} Q_\theta ( F_\theta (\theta ) )
\]</span> Hence, we can increase the efficiency by ordering the samples of <span class="math inline">\(\theta\)</span> and the baseline distribution and use monotonicity of the inverse CDF map.</p>
<p>Let <span class="math inline">\(g(y)\)</span> be non-decreasing and continuous from left with <span class="math inline">\(g^{-1} (z ) = \sup \left ( y : g(y ) \leq z \right )\)</span>. Then, the transformed quantile has a compositional nature, namely <span class="math display">\[
Q_{ g(Y) } ( u ) = g ( Q (u ))
\]</span> Hence, quantiles act as superposition (a.k.a. deep Learner).</p>
<p>This is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation <span class="math display">\[
Q_{ \theta | Y=y } ( u ) = Q_\theta ( s )  \; \;
\text{where}\; \;   s = Q_{ F(\theta) |  Y=y } ( u)
\]</span> To compute <span class="math inline">\(s\)</span>, by definition <span class="math display">\[
u = F_{ F(\theta ) | Y=y} ( s  ) = P( F (\theta ) \leq s | Y=y )
= P( \theta \leq Q_\theta (s ) | Y=y )  = F_{ \theta | Y=y } ( Q_\theta ( s ) )
\]</span></p>
<p>Decision problems are characterized by a utility function <span class="math inline">\(U( \theta , d )\)</span> defined over parameters, <span class="math inline">\(\theta\)</span>, and decisions, <span class="math inline">\(d \in \mathcal{D}\)</span>. We will find it useful to define the family of utility random variables indexed by decisions defined by <span class="math display">\[
U_d \defeq U( \theta , d ) \; \; \text{where}\; \; \theta \sim \Pi ( d  \theta )
\]</span> Optimal Bayesian decisions <span class="citation" data-cites="degroot2005optimal">DeGroot (<a href="references.html#ref-degroot2005optimal" role="doc-biblioref">2005</a>)</span> are then defined by the solution to the prior expected utility <span class="math display">\[
U(d) = E_{\theta}(U(d,\theta)) = \int U(d,\theta)p(\theta)d\theta,
\]</span> <span class="math display">\[
d^\star = \arg \max_d U(d)
\]</span> When information in the form of signals <span class="math inline">\(y\)</span> is available, we need to calculate the posterior distribution <span class="math inline">\(p( \theta | y ) = f(y | \theta ) p( \theta )/ p(y)\)</span>. Then we have to solve for the optimal decision rule <span class="math inline">\(d^\star (y)\)</span> defined by <span class="math display">\[
d^\star(y)  = \arg \max_d  \; \int U( \theta , d ) p( \theta | y ) d \theta
\]</span> where expectations are now taken w.r.t. <span class="math inline">\(p( \theta | y)\)</span> the posterior distribution.</p>
</section>
<section id="normal-normal-bayes-learning-wang-distortion-1" class="level2" data-number="22.6">
<h2 data-number="22.6" class="anchored" data-anchor-id="normal-normal-bayes-learning-wang-distortion-1"><span class="header-section-number">22.6</span> Normal-Normal Bayes Learning: Wang Distortion</h2>
<p>For the purpose of illustration, we consider the normal-normal learning model. We will develop the necessary quantile theory to show how to calculate posteriors and expected utility without resorting to densities. Also, we show a relationship with Wang’s risk distortion measure as the deep learning that needs to be learned.</p>
<p>Specifically, we observe the data <span class="math inline">\(y = (y_1,\ldots,y_n)\)</span> from the following model</p>
<p><span class="math display">\[y_1 , \ldots , y_n  \mid \theta \sim N(\theta, \sigma^2)\]</span> <span class="math display">\[\theta \sim N(\mu,\alpha^2)\]</span></p>
<p>Hence, the summary (sufficient) statistic is <span class="math inline">\(S(y) = \bar y = \frac{1}{n} \sum_{i=1}^n y_i\)</span>.</p>
<p>Given observed samples <span class="math inline">\(y = (y_1,\ldots,y_n)\)</span>, the posterior is then <span class="math inline">\(\theta \mid y \sim N(\mu_*, \sigma_*^2)\)</span> with</p>
<p><span class="math display">\[\mu_* = (\sigma^2 \mu + \alpha^2s) / t, \quad \sigma^2_* = \alpha^2 \sigma^2 / t\]</span></p>
<p>where</p>
<p><span class="math display">\[t =  \sigma^2 + n\alpha^2 \; \; \text{and}\; \; s(y) = \sum_{i=1}^{n}y_i\]</span></p>
<p>The posterior and prior CDFs are then related via the</p>
<p><span class="math display">\[1-\Phi(\theta, \mu_*,\sigma_*) = g(1 - \Phi(\theta, \mu, \alpha^2))\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the normal distribution function. Here the Wang distortion function defined by</p>
<p><span class="math display">\[g(p) = \Phi\left(\lambda_1 \Phi^{-1}(p) + \lambda\right)\]</span></p>
<p>where</p>
<p><span class="math display">\[\lambda_1 = \dfrac{\alpha}{\sigma_*} \; \; \text{and}\; \;
\lambda = \alpha\lambda_1(s-n\mu)/t\]</span></p>
<p>The proof is relatively simple and is as follows</p>
<p><span class="math display">\[g(1 - \Phi(\theta, \mu, \alpha^2)) = g(\Phi(-\theta, \mu, \alpha^2)) = g\left(\Phi\left(-\dfrac{\theta - \mu}{\alpha}\right)\right)\]</span></p>
<p><span class="math display">\[= \Phi\left(\lambda_1 \left(-\dfrac{\theta - \mu}{\alpha}\right) + \lambda\right) =  1 - \Phi\left(\dfrac{\theta - (\mu+ \alpha\lambda/\lambda_1)}{\alpha/\lambda_1}\right)\]</span></p>
<p>Thus, the corresponding posterior updated parameters are</p>
<p><span class="math display">\[\sigma_* = \alpha/\lambda_1, \quad \lambda_1 = \dfrac{\alpha}{\sigma_*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mu_* = \mu+ \alpha\lambda/\lambda_1, \quad \lambda = \dfrac{\lambda_1(\mu_* - \mu)}{\alpha} = \alpha\lambda_1(s-n\mu)/t\]</span></p>
<p>We now provide an empirical example.</p>
<section id="numerical-example-1" class="level3">
<h3 class="anchored" data-anchor-id="numerical-example-1">Numerical Example</h3>
<p>Consider the normal-normal model with Prior <span class="math inline">\(\theta \sim N(0,5)\)</span> and likelihood <span class="math inline">\(y \sim N(3,10)\)</span>. We generate <span class="math inline">\(n=100\)</span> samples from the likelihood and calculate the posterior distribution.</p>
<div id="fig-wang" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Model for simulated data
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang2.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Distortion Function <span class="math inline">\(g\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang1.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) 1 - <span class="math inline">\(\Phi\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22.2: Density for prior, likelihood and posterior, distortion function and 1 - <span class="math inline">\(\Phi\)</span> for the prior and posterior of the normal-normal model.
</figcaption>
</figure>
</div>
<p>The posterior distribution calculated from the sample is then <span class="math inline">\(\theta \mid y \sim N(3.28, 0.98)\)</span>.</p>
<p><a href="#fig-wang" class="quarto-xref">Figure&nbsp;<span>22.2</span></a> shows the Wang distortion function for the normal-normal model. The left panel shows the model for the simulated data, while the middle panel shows the distortion function, the right panel shows the 1 - <span class="math inline">\(\Phi\)</span> for the prior and posterior of the normal-normal model.</p>
</section>
</section>
<section id="portfolio-learning-1" class="level2" data-number="22.7">
<h2 data-number="22.7" class="anchored" data-anchor-id="portfolio-learning-1"><span class="header-section-number">22.7</span> Portfolio Learning</h2>
<p>Consider power utility and log-normal returns (without leverage). For <span class="math inline">\(\omega \in (0,1)\)</span></p>
<p><span class="math display">\[U(W) = -e^{-\gamma W}, ~ W\mid \omega \sim \mathcal{N}( (1-\omega) r_f + \omega R,\sigma^2)\]</span></p>
<p>Let <span class="math inline">\(W = (1-\omega)r_f + \omega R\)</span>, with <span class="math inline">\(R \sim N(\mu,\sigma^2)\)</span>, Here, <span class="math inline">\(U^{-1}\)</span> exists and <span class="math inline">\(r_f\)</span> is the risk-free rate, <span class="math inline">\(\mu\)</span> is the mean return and <span class="math inline">\(\tau^2\)</span> is the variance of the return. Then the expected utility is</p>
<p><span class="math display">\[U(\omega) = E(-e^{\gamma W}) = \exp\left\{\gamma E(W) + \frac{1}{2}\omega^2Var(W)\right\}\]</span></p>
<p>We have closed-form utility in this case, since it is the moment-generating function of the log-normal. Within the Gen-AI framework, it is easy to add learning or uncertainty on top of <span class="math inline">\(\sigma^2\)</span> and have a joint posterior distribution <span class="math inline">\(p(\mu, \sigma^2 \mid R)\)</span>.</p>
<p>Thus, the closed form solution is</p>
<p><span class="math display">\[U(\omega) = \exp\left\{\gamma \left\{(1-\omega)r_f + \omega\mu\right\}\right\} \exp \left \{ \dfrac{1}{2}\gamma^2\omega^2\sigma^2 \right \}\]</span></p>
<p>The optimal Kelly-Brieman-Thorpe-Merton rule is given by</p>
<p><span class="math display">\[\omega^* = (\mu - r_f)/(\sigma^2\gamma)\]</span></p>
<p>Now we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of <span class="math inline">\(U\)</span></p>
<p><span class="math display">\[E(U(W)) = \int_{0}^{1}F_{U(W)}^{-1}(\tau)d\tau\]</span></p>
<p>Hence, if we can approximate the inverse of the CDF of <span class="math inline">\(U(W)\)</span> with a quantile NN, we can approximate the expected utility and optimize over <span class="math inline">\(\omega\)</span>.</p>
<p>The stochastic utility is modeled with a deep neural network, and we write</p>
<p><span class="math display">\[Z = U(W) \approx F, ~ W  = U^{-1}(F)\]</span></p>
<p>We can do optimization by doing the grid for <span class="math inline">\(\omega\)</span>.</p>
<section id="empirical-example" class="level3">
<h3 class="anchored" data-anchor-id="empirical-example">Empirical Example</h3>
<p>Consider <span class="math inline">\(\omega \in (0,1)\)</span>, <span class="math inline">\(r_f = 0.05\)</span>, <span class="math inline">\(\mu=0.1\)</span>, <span class="math inline">\(\sigma=0.25\)</span>, <span class="math inline">\(\gamma = 2\)</span>. We have the closed-form fractional Kelly criterion solution</p>
<p><span class="math display">\[\omega^* = \frac{1}{\gamma}   \frac{ \mu - r_f}{ \sigma^2} = \frac{1}{2} \frac{ 0.1 - 0.05 }{ 0.25^2 } = 0.40\]</span></p>
<p>We can simulate the expected utility and compare with the closed-form solution.</p>
<div id="fig-portfolio" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-portfolio" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-portfolio-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-portfolio-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/portfolio-tau-z.png" class="img-fluid figure-img" data-ref-parent="fig-portfolio">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-portfolio-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Portfolio tau z
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-portfolio" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-portfolio-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-portfolio-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/portfolio.png" class="img-fluid figure-img" data-ref-parent="fig-portfolio">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-portfolio-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Portfolio
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22.3: Line at 0.4 optimum
</figcaption>
</figure>
</div>
<p>Add code.</p>
</section>
</section>
<section id="learning-quantiles" class="level2" data-number="22.8">
<h2 data-number="22.8" class="anchored" data-anchor-id="learning-quantiles"><span class="header-section-number">22.8</span> Learning Quantiles</h2>
<p>The 1-Wasserstein distance is the <span class="math inline">\(\ell_1\)</span> metric on the inverse distribution function. It is also known as earth mover’s distance and can be calculated using order statistics <span class="citation" data-cites="levina2001earth">(<a href="references.html#ref-levina2001earth" role="doc-biblioref">Levina and Bickel 2001</a>)</span>. For quantile functions <span class="math inline">\(F^{-1}_U\)</span> and <span class="math inline">\(F^{-1}_V\)</span> the 1-Wasserstein distance is given by</p>
<p><span class="math display">\[
W_1(F^{-1}_U, F^{-1}_V) = \int_0^1 |F^{-1}_U(\tau) - F^{-1}_V(\tau)| d\tau
\]</span></p>
<p>It can be shown that Wasserstein GAN networks outperform vanilla GAN due to the improved quantile metric. <span class="math inline">\(q = F^{-1}_U(\tau)\)</span> minimizes the expected quantile loss</p>
<p><span class="math display">\[
E_U[\rho_{\tau}(u-q)]
\]</span></p>
<p>Quantile regression can be shown to minimize the 1-Wasserstein metric. A related loss is the quantile divergence,</p>
<p><span class="math display">\[
q(U,V) = \int_0^1 \int_{F^{-1}_U(q)}^{F^{-1}_V(q)} (F_U(\tau)-q) dq d\tau
\]</span></p>
<p>The quantile regression likelihood function is an asymmetric function that penalizes overestimation errors with weight <span class="math inline">\(\tau\)</span> and underestimation errors with weight <span class="math inline">\(1-\tau\)</span>. For a given input-output pair <span class="math inline">\((x, y)\)</span>, and the quantile function <span class="math inline">\(f(x, \theta)\)</span>, parametrized by <span class="math inline">\(\theta\)</span>, the quantile loss is <span class="math inline">\(\rho_{\tau}(u) = u(\tau - I(u &lt; 0))\)</span>, where <span class="math inline">\(u = y - f(x)\)</span>. From the implementation point of view, a more convenient form of this function is</p>
<p><span class="math display">\[
\rho_{\tau}(u) = \max(u\tau, u(\tau-1))
\]</span></p>
<p>Given a training data <span class="math inline">\(\{x_i, y_i\}_{i=1}^N\)</span>, and given quantile <span class="math inline">\(\tau\)</span>, the loss is</p>
<p><span class="math display">\[
L_{\tau}(\theta) = \sum_{i=1}^N \rho_{\tau}(y_i - f(\tau, x_i, \theta))
\]</span></p>
<p>Further, we empirically found that adding a mean-squared loss to this objective function improves the predictive power of the model, thus the loss function we use is</p>
<p><span class="math display">\[
\alpha L_{\tau}(\theta) + \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i, \theta))^2
\]</span></p>
<p>One approach to learn the quantile function is to use a set of quantiles <span class="math inline">\(0 &lt; \tau_1 &lt; \tau_2 &lt; \ldots &lt; \tau_K &lt; 1\)</span> and then learn <span class="math inline">\(K\)</span> quantile functions simultaneously by minimizing</p>
<p><span class="math display">\[
L(\theta, \tau_1, \ldots, \tau_K) = \frac{1}{N K} \sum_{i=1}^N \sum_{k=1}^K \rho_{\tau_k}(y_i - f_{\tau_k}(x_i, \theta_k))
\]</span></p>
<p>The corresponding optimization problem of minimizing <span class="math inline">\(L(\theta)\)</span> can be augmented by adding a non-crossing constraint</p>
<p><span class="math display">\[
f_{\tau_i}(x, \theta_i) &lt; f_{\tau_j}(x, \theta_j), \quad \forall x, \; i &lt; j
\]</span></p>
<p>The non-crossing constraint has been considered by several authors, including <span class="citation" data-cites="chernozhukov2010quantile cannon2018noncrossing">(<a href="references.html#ref-chernozhukov2010quantile" role="doc-biblioref">Chernozhukov, Fernández-Val, and Galichon 2010</a>; <a href="references.html#ref-cannon2018noncrossing" role="doc-biblioref">Cannon 2018</a>)</span>.</p>
<section id="cosine-embedding-for-tau" class="level3">
<h3 class="anchored" data-anchor-id="cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></h3>
<p>To learn an inverse CDF (quantile function) <span class="math inline">\(F^{-1}(\tau, y) = f_\theta(\tau, y)\)</span> we will use a kernel embedding trick and augment the predictor space. We then represent the quantile function as a function of superposition for two other functions:</p>
<p><span class="math display">\[
F^{-1}(\tau, y) = f_\theta(\tau, y) = g(\psi(y) \circ \phi(\tau))
\]</span></p>
<p>where <span class="math inline">\(\circ\)</span> is the element-wise multiplication operator. Both functions <span class="math inline">\(g\)</span> and <span class="math inline">\(\psi\)</span> are feed-forward neural networks. <span class="math inline">\(\phi\)</span> is a cosine embedding. To avoid over-fitting, we use a sufficiently large training dataset, see <span class="citation" data-cites="dabney2018implicit">(<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">Dabney et al. 2018</a>)</span> in a reinforcement learning context.</p>
<p>Let <span class="math inline">\(g\)</span> and <span class="math inline">\(\psi\)</span> be feed-forward neural networks and <span class="math inline">\(\phi\)</span> a cosine embedding given by</p>
<p><span class="math display">\[
\phi_j(\tau) = \mathrm{ReLU}\left(\sum_{i=0}^{n-1} \cos(\pi i \tau) w_{ij} + b_j\right)
\]</span></p>
<p>We now illustrate our approach with a simple synthetic dataset.</p>
</section>
</section>
<section id="synthetic-data" class="level2" data-number="22.9">
<h2 data-number="22.9" class="anchored" data-anchor-id="synthetic-data"><span class="header-section-number">22.9</span> Synthetic Data</h2>
<p>Consider a synthetic data generated from the model</p>
<p><span class="math display">\[
x \sim U(-1, 1) \\
y \sim N(\sin(\pi x)/(\pi x), \exp(1-x)/10)
\]</span></p>
<p>The true quantile function is given by</p>
<p><span class="math display">\[
f_{\tau}(x) = \sin(\pi x)/(\pi x) + \Phi^{-1}(\tau) \sqrt{\exp(1-x)/10}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/synthetic.svg" class="img-fluid figure-img"></p>
<figcaption>We trained both implicit and explicit networks on the synthetic data set. The explicit network was trained for three fixed quantiles (0.05, 0.5, 0.95). We see no empirical difference between the two.</figcaption>
</figure>
</div>
<p>We train two quantile networks, one implicit and one explicit. The explicit network is trained for three fixed quantiles <span class="math inline">\((0.05, 0.5, 0.95)\)</span>. The figure above shows fits by both of the networks; we see no empirical difference between the two.</p>
</section>
<section id="quantiles-as-deep-learners" class="level2" data-number="22.10">
<h2 data-number="22.10" class="anchored" data-anchor-id="quantiles-as-deep-learners"><span class="header-section-number">22.10</span> Quantiles as Deep Learners</h2>
<p>One can show that quantile models are direct alternatives to other Bayes computations. Specifically, given <span class="math inline">\(F(y)\)</span>, a non-decreasing and continuous from right function, we define</p>
<p><span class="math display">\[
Q_{\theta|y}(u) \defeq F^{-1}_{\theta|y}(u) = \inf\{y : F_{\theta|y}(y) \geq u\}
\]</span></p>
<p>which is non-decreasing, continuous from left. Now let <span class="math inline">\(g(y)\)</span> be a non-decreasing and continuous from left with</p>
<p><span class="math display">\[
g^{-1}(z) = \sup\{y : g(y) \leq z\}
\]</span></p>
<p>Then, the transformed quantile has a compositional nature, namely</p>
<p><span class="math display">\[
Q_{g(Y)}(u) = g(Q(u))
\]</span></p>
<p>Hence, quantiles act as superposition (a.k.a. deep learner).</p>
<p>This is best illustrated in the Bayes learning model. We have the following result updating prior to posterior quantiles known as the conditional quantile representation:</p>
<p><span class="math display">\[
Q_{\theta | Y=y}(u) = Q_Y(s)
\]</span> where <span class="math inline">\(s = Q_{F(\theta) | Y=y}(u)\)</span></p>
<p>To compute <span class="math inline">\(s\)</span> we use</p>
<p><span class="math display">\[
u = F_{F(\theta) | Y=y}(s) = P(F(\theta) \leq s | Y=y) = P(\theta \leq Q_\theta(s) | Y=y) = F_{\theta | Y=y}(Q_\theta(s))
\]</span></p>
<p><span class="citation" data-cites="parzen2004quantile">(<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">Parzen 2004</a>)</span> also shows the following probabilistic property of quantiles:</p>
<p><span class="math display">\[
\theta = Q_\theta(F_\theta(\theta))
\]</span></p>
<p>Hence, we can increase the efficiency by ordering the samples of <span class="math inline">\(\theta\)</span> and the baseline distribution as the mapping being the inverse CDF is monotonic.</p>
<section id="quantile-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="quantile-reinforcement-learning">Quantile Reinforcement Learning</h3>
<p><span class="citation" data-cites="dabney2017distributional">(<a href="references.html#ref-dabney2017distributional" role="doc-biblioref">Dabney et al. 2017</a>)</span> use quantile neural networks for decision-making and apply quantile neural networks to the problem of reinforcement learning. Specifically, they rely on the fact that expectations are quantile integrals. The key identity in this context is the Lorenz curve:</p>
<p><span class="math display">\[
E(Y) = \int_{-\infty}^{\infty} y dF(y) = \int_0^1 F^{-1}(u) du
\]</span></p>
<p>Then, distributional reinforcement learning algorithm finds</p>
<p><span class="math display">\[
\pi(x) = \arg\max_a E_{Z \sim z(x, a)}(Z)
\]</span></p>
<p>Then a Q-Learning algorithm can be applied, since the quantile projection keeps contraction property of Bellman operator. Similar approaches that rely on the dual Expected Utility were proposed by <span class="citation" data-cites="yaari1987dual">(<a href="references.html#ref-yaari1987dual" role="doc-biblioref">Yaari 1987</a>)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-brillinger2012generalized" class="csl-entry" role="listitem">
Brillinger, David R. 2012. <span>“A <span>Generalized Linear Model With</span> <span>‘<span>Gaussian</span>’</span> <span>Regressor Variables</span>.”</span> In <em>Selected <span>Works</span> of <span>David Brillinger</span></em>, edited by Peter Guttorp and David Brillinger, 589–606. Selected <span>Works</span> in <span>Probability</span> and <span>Statistics</span>. New York, NY: Springer.
</div>
<div id="ref-cannon2018noncrossing" class="csl-entry" role="listitem">
Cannon, Alex J. 2018. <span>“Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes.”</span> <em>Stochastic Environmental Research and Risk Assessment</em> 32 (11): 3207–25.
</div>
<div id="ref-chernozhukov2010quantile" class="csl-entry" role="listitem">
Chernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010. <span>“Quantile and <span>Probability Curves Without Crossing</span>.”</span> <em>Econometrica</em> 78 (3): 1093–1125. <a href="https://www.jstor.org/stable/40664520">https://www.jstor.org/stable/40664520</a>.
</div>
<div id="ref-dabney2018implicit" class="csl-entry" role="listitem">
Dabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018. <span>“Implicit <span>Quantile Networks</span> for <span>Distributional Reinforcement Learning</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1806.06923">https://arxiv.org/abs/1806.06923</a>.
</div>
<div id="ref-dabney2017distributional" class="csl-entry" role="listitem">
Dabney, Will, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2017. <span>“Distributional <span>Reinforcement Learning</span> with <span>Quantile Regression</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1710.10044">https://arxiv.org/abs/1710.10044</a>.
</div>
<div id="ref-degroot2005optimal" class="csl-entry" role="listitem">
DeGroot, Morris H. 2005. <em>Optimal Statistical Decisions</em>. Wiley classics library ed. Wiley Classics Library. Hoboken, NJ: Wiley-Interscience.
</div>
<div id="ref-kallenberg1997foundations" class="csl-entry" role="listitem">
Kallenberg, Olav. 1997. <em>Foundations of <span>Modern Probability</span></em>. 2nd ed. edition. Springer.
</div>
<div id="ref-levina2001earth" class="csl-entry" role="listitem">
Levina, Elizaveta, and Peter Bickel. 2001. <span>“The Earth Mover’s Distance Is the Mallows Distance: <span>Some</span> Insights from Statistics.”</span> In <em>Proceedings Eighth <span>IEEE</span> International Conference on Computer Vision. <span>ICCV</span> 2001</em>, 2:251–56. IEEE.
</div>
<div id="ref-parzen2004quantile" class="csl-entry" role="listitem">
Parzen, Emanuel. 2004. <span>“Quantile <span>Probability</span> and <span>Statistical Data Modeling</span>.”</span> <em>Statistical Science</em> 19 (4): 652–62. <a href="https://www.jstor.org/stable/4144436">https://www.jstor.org/stable/4144436</a>.
</div>
<div id="ref-polson2024generative" class="csl-entry" role="listitem">
Polson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024. <span>“Generative <span>Bayesian Computation</span> for <span>Maximum Expected Utility</span>.”</span> <em>Entropy</em> 26 (12): 1076.
</div>
<div id="ref-yaari1987dual" class="csl-entry" role="listitem">
Yaari, Menahem E. 1987. <span>“The <span>Dual Theory</span> of <span>Choice</span> Under <span>Risk</span>.”</span> <em>Econometrica</em> 55 (1): 95–115. <a href="https://www.jstor.org/stable/1911158">https://www.jstor.org/stable/1911158</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./21-sgd.html" class="pagination-link" aria-label="Gradient Descent">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./23-cnn.html" class="pagination-link" aria-label="Convolutional Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>