<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Theory of AI: From MLE to Bayesian Regularization – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./00-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar docked quarto-light"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[2][]{\operatorname{E}_{#1}\left[#2\right]}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\DeclareMathOperator*{\minf}{minimize \quad}
\newcommand{\mininlineeq}[4]{\begin{equation}\label{#4}\mbox{minimize}_{#1}\quad#2\qquad\mbox{subject to }#3\end{equation}}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17-theoryai.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#normal-means-problem" id="toc-normal-means-problem" class="nav-link active" data-scroll-target="#normal-means-problem"><span class="header-section-number">1.1</span> Normal Means Problem</a></li>
  <li><a href="#maximum-aposteriori-estimation-map-and-regularization" id="toc-maximum-aposteriori-estimation-map-and-regularization" class="nav-link" data-scroll-target="#maximum-aposteriori-estimation-map-and-regularization"><span class="header-section-number">1.2</span> Maximum Aposteriori Estimation (MAP) and Regularization</a></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">1.3</span> Ridge Regression</a>
  <ul class="collapse">
  <li><a href="#kernel-view-of-ridge-regression" id="toc-kernel-view-of-ridge-regression" class="nav-link" data-scroll-target="#kernel-view-of-ridge-regression"><span class="header-section-number">1.3.1</span> Kernel View of Ridge Regression</a></li>
  </ul></li>
  <li><a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression"><span class="header-section-number">1.4</span> Lasso Regression</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17-theoryai.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-theoryai" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>As we have seen in the previous chapters, the development of learning from data algorithms has been driven by two fundamental paradigms: the classical frequentist approach centered around maximum likelihood estimation (MLE) and the Bayesian approach grounded in decision theory. This chapter explores how these seemingly distinct methodologies converge in the modern theory of AI, particularly through the lens of regularization and model selection.</p>
<p>Maximum likelihood estimation represents the cornerstone of classical statistical inference. Given observed data <span class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span> and a parametric model <span class="math inline">\(f_{\theta}(x)\)</span>, the MLE principle seeks to find the parameter values that maximize the likelihood function: <span class="math display">\[
\hat{\theta}_{MLE} = \arg\max_{\theta} \mathcal{L}(\theta; \mathcal{D}) = \arg\max_{\theta} \prod_{i=1}^n p(y_i | x_i, \theta)
\]</span></p>
<p>This approach has several appealing properties: it provides consistent estimators under mild conditions, achieves the Cramér-Rao lower bound asymptotically, and offers a principled framework for parameter estimation. However, MLE has well-documented limitations, particularly in high-dimensional settings MLE can lead to overfitting, poor generalization, and numerical instability. Furthermore, as was shown by Stein’s paradox, MLE can be inadmissible. Meaning, there are other estimators that have lower risk than the MLE. We will start this chapter with the normal means problem and show how MLE can be inadmissible.</p>
<section id="normal-means-problem" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="normal-means-problem"><span class="header-section-number">1.1</span> Normal Means Problem</h2>
<p>Consider the vector of means case where <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. We have <span id="eq-normal-mean"><span class="math display">\[
y_i \mid \theta_i \sim N(\theta_i, \sigma^2), ~ i=1,\ldots,p &gt; 2
\tag{1.1}\]</span></span></p>
<p>The goal is to estimate the vector of means <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span> and we can achieve this by borrowing strength across the observations. This is also a proxy for non-parametric regression, where <span class="math inline">\(\theta_i = f(x_i)\)</span>. Also typically <span class="math inline">\(y_i\)</span> is a mean of <span class="math inline">\(n\)</span> observations, i.e.&nbsp;<span class="math inline">\(y_i = \frac{1}{n} \sum_{j=1}^n x_{ij}\)</span>. Much has been written on the properties of the Bayes risk as a function of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. Much work has also been done on the asymptotic properties of the Bayes risk as <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> grow to infinity.</p>
<p>The goal is to estimate the vector <span class="math inline">\(\theta\)</span> using a squared loss <span class="math display">\[
\mathcal{L}(\theta, \hat{\theta}) = \sum_{i=1}^p (\theta_i - \hat{\theta}_i)^2,
\]</span> where <span class="math inline">\(\hat \theta\)</span> is the vector of estimates. Now, we will compare the MLE estimate and what is called the James-Stein estimate. A principled way to evaluate the performance of an estimator is to average its loss over the data, this metric is called the risk. The MLE estimate <span class="math inline">\(\hat \theta_{i} = y_i\)</span> has a constant risk <span class="math inline">\(p\)</span> <span class="math display">\[
R(\theta,\hat \theta ) = \E[y]{\mathcal{L}\left(\theta, \hat \theta\right) } = p.
\]</span> Here expectation is over the data given by distribution <a href="#eq-normal-mean" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>. The estimate is map (rule) from the data to the parameter space <span class="math inline">\(\hat \theta = \hat \theta(y)\)</span>.</p>
<p>Bayesian inference offers a fundamentally different perspective by incorporating prior knowledge and quantifying uncertainty through probability distributions. The Bayesian approach begins with a prior distribution <span class="math inline">\(p(\theta)\)</span> over the parameter space and updates this belief using Bayes’ rule: <span class="math display">\[
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}
\]</span></p>
<p>The <strong>Bayes estimator</strong> is the value <span class="math inline">\(\hat \theta^{B}\)</span> that minimizes the Bayes risk, the expected loss: <span class="math display">\[
\hat \theta^{B} = \arg\min_{\hat \theta(y)} R(\pi, \hat \theta(y))
\]</span> Here <span class="math inline">\(\pi\)</span> is the prior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(R(\pi, \hat \theta(y))\)</span> is the <strong>Bayes risk</strong> defined as: <span id="eq-bayes-risk"><span class="math display">\[
R(\pi, \hat{\theta}(y)) = \mathbb{E}_{\theta \sim \pi} \left[ \mathbb{E}_{y\mid \theta} \left[ \mathcal{L}(\theta, \hat{\theta}(y)) \right] \right].
\tag{1.2}\]</span></span> For squared error loss, this yields the posterior mean <span class="math inline">\(\E{\theta \mid y}\)</span>, while for absolute error loss, it gives the posterior median.</p>
<p>For the normal means problem with squared error loss, this becomes: <span class="math display">\[
R(\pi, \hat{\theta}(y)) = \int \left( \int (\theta - \hat{\theta}(y))^2 p(y|\theta) dy \right) \pi(\theta) d\theta
\]</span></p>
<p>The Bayes risk quantifies the expected performance of an estimator, taking into account both the uncertainty in the data and the prior uncertainty about the parameter. It serves as a benchmark for comparing different estimators: an estimator with lower Bayes risk is preferred under the chosen prior and loss function. In particular, the Bayes estimator achieves the minimum possible Bayes risk for the given prior and loss.</p>
<p>In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.</p>
<p>In each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.</p>
<p>The James-Stein estimator is a shrinkage estimator that shrinks the MLE towards the prior mean. The prior mean is typically the sample mean of the data. The James-Stein estimator is given by <span class="math display">\[
\hat \theta_{i}^{JS} = (1 - \lambda) \hat \theta_{i}^{MLE} + \lambda \bar y,
\]</span> where <span class="math inline">\(\lambda\)</span> is a shrinkage parameter and <span class="math inline">\(\bar y\)</span> is the sample mean of the data. The shrinkage parameter is typically chosen to minimize the risk of the estimator.</p>
<p>Following <span class="citation" data-cites="efron1975data">Efron and Morris (<a href="references.html#ref-efron1975data" role="doc-biblioref">1975</a>)</span>, we can view the James-Stein estimator through the lens of empirical Bayes methodology. Efron and Morris demonstrate that Stein’s seemingly paradoxical result has a natural interpretation when viewed as an empirical Bayes procedure that estimates the prior distribution from the data itself.</p>
<p>Consider the hierarchical model: <span class="math display">\[
\begin{aligned}
y_i | \theta_i &amp;\sim N(\theta_i, \sigma^2) \\
\theta_i | \mu, \tau^2 &amp;\sim N(\mu, \tau^2)
\end{aligned}
\]</span></p>
<p>The marginal distribution of <span class="math inline">\(y_i\)</span> is then <span class="math inline">\(y_i \sim N(\mu, \sigma^2 + \tau^2)\)</span>. In the empirical Bayes approach, we estimate the hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span> from the marginal likelihood:</p>
<p><span class="math display">\[
m(y | \mu, \tau^2) = \prod_{i=1}^p \frac{1}{\sqrt{2\pi(\sigma^2 + \tau^2)}} \exp\left(-\frac{(y_i - \mu)^2}{2(\sigma^2 + \tau^2)}\right)
\]</span></p>
<p>The maximum marginal likelihood estimators are: <span class="math display">\[
\hat{\mu} = \bar{y} = \frac{1}{p}\sum_{i=1}^p y_i
\]</span> <span class="math display">\[
\hat{\tau}^2 = \max\left(0, \frac{1}{p}\sum_{i=1}^p (y_i - \bar{y})^2 - \sigma^2\right)
\]</span></p>
<p>The empirical Bayes estimator then becomes: <span class="math display">\[
\hat{\theta}_i^{EB} = \frac{\hat{\tau}^2}{\sigma^2 + \hat{\tau}^2} y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \hat{\mu}
\]</span></p>
<p>This can be rewritten as: <span class="math display">\[
\hat{\theta}_i^{EB} = \left(1 - \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2}\right) y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \bar{y}
\]</span></p>
<p>When <span class="math inline">\(\mu = 0\)</span> and using the estimate <span class="math inline">\(\hat{\tau}^2 = \max(0, \|y\|^2/p - \sigma^2)\)</span>, this reduces to a form closely related to the James-Stein estimator: <span class="math display">\[
\hat{\theta}_i^{JS} = \left(1 - \frac{(p-2)\sigma^2}{\|y\|^2}\right) y_i
\]</span></p>
<p>Efron and Morris show that the empirical Bayes interpretation provides insight into why the James-Stein estimator dominates the MLE. The key insight is that the MLE implicitly assumes an improper flat prior <span class="math inline">\(\pi(\theta) \propto 1\)</span>, which leads to poor risk properties in high dimensions.</p>
<p>The risk of the MLE is constant: <span class="math display">\[
R(\theta, \hat{\theta}^{MLE}) = \mathbb{E}[\|\hat{\theta}^{MLE} - \theta\|^2] = p\sigma^2
\]</span></p>
<p>In contrast, the James-Stein estimator has risk: <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) = p\sigma^2 - (p-2)\sigma^2 \mathbb{E}\left[\frac{1}{\|\theta + \epsilon\|^2/\sigma^2}\right]
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim N(0, \sigma^2 I)\)</span>. Since the second term is always positive, we have: <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) &lt; R(\theta, \hat{\theta}^{MLE}) \quad \forall \theta \in \mathbb{R}^p, \quad p \geq 3
\]</span></p>
<p>This uniform domination demonstrates the <strong>inadmissibility</strong> of the MLE under squared error loss for <span class="math inline">\(p \geq 3\)</span>.</p>
<p>The James-Stein estimator is not the only shrinkage estimator that dominates the MLE. Other shrinkage estimators, such as the ridge regression estimator, also have lower risk than the MLE. The key insight is that shrinkage estimators can leverage prior information to improve estimation accuracy, especially in high-dimensional settings.</p>
<p>Note, that we used the empirical Bayes version of the definition of risk. Full Bayes approach incorporates both the data and the prior distribution of the parameter as in <a href="#eq-bayes-risk" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>.</p>
<p>In the normal means problem, the Bayes risk can be explicitly calculated due to the conjugacy of the normal prior and likelihood, and it illustrates how incorporating prior information (via shrinkage) can lead to estimators with lower overall risk compared to the MLE, especially in high-dimensional settings.</p>
<p>From a historical perspective, James-Stein (a.k.a <span class="math inline">\(L^2\)</span>-regularisation, <span class="citation" data-cites="stein1964inadmissibility">Stein (<a href="references.html#ref-stein1964inadmissibility" role="doc-biblioref">1964</a>)</span>) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with <span class="math inline">\(L^2\)</span>-regularisation. Consider the sparse <span class="math inline">\(r\)</span>-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).</p>
<p>Let the true parameter value be <span class="math inline">\(\theta_p = \left ( \sqrt{d/p} , \ldots , \sqrt{d/p} , 0 , \ldots , 0 \right )\)</span>. James-Stein is equivalent to the model <span class="math display">\[
y_i = \theta_i + \epsilon_i \; \mathrm{ and} \; \theta_i \sim \mathcal{N} \left ( 0 , \tau^2 \right )
\]</span></p>
<p>A better approach to address sparsity is to use a local shrinkage estimator, such as the horseshoe prior. The horseshoe prior is particularly effective for sparse signals, as it allows for strong shrinkage of noise while preserving signals.</p>
<p>The horseshoe prior is defined as: <span class="math display">\[
\theta_i \sim N(0, \sigma^2 \tau^2 \lambda_i^2), \quad \lambda_i \sim C^+(0, 1), \quad \tau \sim C^+(0, 1)
\]</span></p>
<p>Here, <span class="math inline">\(\lambda_i\)</span> is a local shrinkage parameter, and <span class="math inline">\(\tau\)</span> is a global shrinkage parameter. The half-Cauchy distribution <span class="math inline">\(C^+\)</span> ensures heavy tails, allowing for adaptive shrinkage. We will discuss the horseshoe prior in more detail later in this section.</p>
<div id="exm-baseball" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Example: James-Stein for Baseball Batting Averages)</strong></span> We reproduce the baseball batting average example from <span class="citation" data-cites="efron1977steins">Efron and Morris (<a href="references.html#ref-efron1977steins" role="doc-biblioref">1977</a>)</span>. Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/EfronMorrisBB.txt"</span>, <span class="at">sep =</span> <span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> <span class="fu">select</span>(LastName,AtBats,BattingAverage,SeasonAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can eatimate overall mean and variance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(baseball<span class="sc">$</span>BattingAverage)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> <span class="fu">var</span>(baseball<span class="sc">$</span>BattingAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As well as the posterior mean for each player (James-Stein estimator)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">&lt;-</span> baseball <span class="sc">%&gt;%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">JS =</span> (sigma2_hat <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> mu_hat <span class="sc">+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>      ((BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats) <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> BattingAverage</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(baseball)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">LastName</th>
<th style="text-align: right;">AtBats</th>
<th style="text-align: right;">BattingAverage</th>
<th style="text-align: right;">SeasonAverage</th>
<th style="text-align: right;">JS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Clemente</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr class="even">
<td style="text-align: left;">Robinson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Howard</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.31</td>
</tr>
<tr class="even">
<td style="text-align: left;">Johnstone</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Berry</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spencer</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kessinger</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.28</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvarado</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Santo</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Swaboda</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Petrocelli</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rodriguez</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scott</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Unser</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Williams</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Campaneris</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Munson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvis</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.22</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Plot below shows the observed averages vs.&nbsp;James-Stein estimate</p>
<div class="cell" data-fold="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(baseball, <span class="fu">aes</span>(<span class="at">x =</span> BattingAverage, <span class="at">y =</span> JS)) <span class="sc">+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Observed Batting Average"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"James-Stein Estimate"</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Empirical Bayes Shrinkage of Batting Averages"</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Calculate mean squared error (MSE) for observed and James-Stein estimates</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>mse_observed <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>BattingAverage <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mse_js <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>JS <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (Observed): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_observed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (Observed): 0.004584</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (James-Stein): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_js))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (James-Stein): 0.001031</code></pre>
</div>
</div>
<p>We can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="fu">nrow</span>(baseball)), <span class="dv">3</span>, <span class="fu">nrow</span>(baseball))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>SeasonAverage, baseball<span class="sc">$</span>JS),    <span class="dv">3</span>, <span class="fu">nrow</span>(baseball), <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(a, b, <span class="at">pch=</span><span class="st">" "</span>, <span class="at">ylab=</span><span class="st">"predicted average"</span>, <span class="at">xaxt=</span><span class="st">"n"</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">3.1</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.13</span>, <span class="fl">0.42</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">matlines</span>(a, b)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">rep</span>(<span class="fl">0.7</span>, <span class="fu">nrow</span>(baseball)), baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>LastName, <span class="at">cex=</span><span class="fl">0.6</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span>, <span class="fl">0.14</span>, <span class="st">"First 45</span><span class="sc">\n</span><span class="st">at bats"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">2</span>, <span class="fl">0.14</span>, <span class="st">"Average</span><span class="sc">\n</span><span class="st">of remainder"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">3</span>, <span class="fl">0.14</span>, <span class="st">"J-S</span><span class="sc">\n</span><span class="st">estimator"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now if we look at the season dynamics for Clemente</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&amp;y=1970</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/clemente.csv"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>H)<span class="sc">/</span><span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot x,y startind from index 2</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">-</span>ind],y[<span class="sc">-</span>ind], <span class="at">type=</span><span class="st">'o'</span>, <span class="at">ylab=</span><span class="st">"Betting Average"</span>, <span class="at">xlab=</span><span class="st">"Number at Bats"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add horizontal line for season average 145/412 and add text above line `Seaosn Average`</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span> <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"Season Average"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ted williams record is .406 in in 1941, so you know the first data points are noise</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>JS[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"JS"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>JS[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"After 45 Bets"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>The motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments. They have an ability to balance signal detection and noise suppression in high-dimensional settings. Unlike flat uniform priors, shrinkage priors adaptively shrink small signals towards zero while preserving large signals. This behavior is crucial for sparse estimation problems, where most parameters are expected to be zero or near-zero. The James-Stein procedure is an example of <strong>global shrinkage</strong>, when the overall sparsity level across all parameters is controlled, ensuring that the majority of parameters are shrunk towards zero. Later in this section we will discuss <strong>local shrinkage</strong> priors, such as the horseshoe prior, which allow individual parameters to escape shrinkage if they represent significant signals.</p>
<p>In summary, flat uniform priors (MLE) fail to provide adequate regularization in high-dimensional settings, leading to poor risk properties and overfitting. By incorporating probabilistic arguments and hierarchical structures, shrinkage priors offer a principled approach to regularization that aligns with Bayesian decision theory and modern statistical practice.</p>
</section>
<section id="maximum-aposteriori-estimation-map-and-regularization" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="maximum-aposteriori-estimation-map-and-regularization"><span class="header-section-number">1.2</span> Maximum Aposteriori Estimation (MAP) and Regularization</h2>
<p>In the previous sections, we have seen how the Bayesian approach provides a principled framework for parameter estimation through the use of prior distributions and the minimization of Bayes risk. However, in many practical scenarios, we may not have access to a full Bayesian model or the computational resources to perform Bayesian inference. This is where the concept of MAP or regularization comes into play. It also sometimes called a poor man’s Bayesian approach.</p>
<p>Given input-output pairs <span class="math inline">\((x_i,y_i)\)</span>, MAP learns the funvtion <span class="math inline">\(f\)</span> that maps inputs <span class="math inline">\(x_i\)</span> to outputs <span class="math inline">\(y_i\)</span> by minimizing <span class="math display">\[
\sum_{i=1}^N  \mathcal{L}(y_i,f(x_i)) + \lambda \phi(f) \rightarrow \text{minimize}_{f}.
\]</span> The first term is the <strong>loss function</strong> that measures the difference between the predicted output <span class="math inline">\(f(x_i)\)</span> and the true output <span class="math inline">\(y_i\)</span>. The second term is a <strong>regularization term</strong> that penalizes complex functions <span class="math inline">\(f\)</span> to prevent overfitting. The parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between fitting the data well and keeping the function simple. In the case when <span class="math inline">\(f\)</span> is a parametric model, then we simply replace <span class="math inline">\(f\)</span> with the parameters <span class="math inline">\(\theta\)</span> of the model, and the regularization term becomes a penalty on the parameters.</p>
<p>The loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when <span class="math inline">\(y\)</span> is numeric and <span class="math inline">\(y_i \mid x_i \sim N(f(x_i),\sigma^2)\)</span>, we get the squared loss <span class="math inline">\(\mathcal{L}(y,f(x)) = (y-f(x))^2\)</span>. When <span class="math inline">\(y_i\in \{0,1\}\)</span> is binary, we use the logistic loss <span class="math inline">\(\mathcal{L}(y,f(x)) = \log(1+\exp(-yf(x)))\)</span>.</p>
<p>The penalty term <span class="math inline">\(\lambda \phi(f)\)</span> discourages complex functions <span class="math inline">\(f\)</span>. Then, we can think of regularization as a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.</p>
<div id="exm-traffic" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Traffic)</strong></span> Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. <a href="#fig-speed-profile" class="quarto-xref">Figure&nbsp;<span>1.1</span></a> shows speed measured data, averaged over five minute intervals on one of the weekdays.</p>
<div id="fig-speed-profile" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig//svg/day_295.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Speed profile over 24 hour period on I-55, on October 22, 2013
</figcaption>
</figure>
</div>
<p>The statistical model is <span class="math display">\[
y_t = f_t + \epsilon_t, ~ \epsilon_t \sim N(0,\sigma^2), ~ t=1,\ldots,n,
\]</span> where <span class="math inline">\(y_t\)</span> is the speed measurement at time <span class="math inline">\(t\)</span>, <span class="math inline">\(f_t\)</span> is the true underlying speed at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\epsilon_t\)</span> is the measurement noise. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes. A naive MLE approach woule be to estimate the speed profile <span class="math inline">\(f = (f_1, \ldots, f_n)\)</span> by minimizing the squared loss <span class="math display">\[
\hat f = \arg\min_{f} \sum_{t=1}^{n} (y_t - f_t)^2.
\]</span> However, the minima of this loss function is 0 and corresponds to the case when <span class="math inline">\(\hat f_t = y_t\)</span> for all <span class="math inline">\(t\)</span>. We have learned nothing about the speed profile, and the estimate is simply the noisy observation <span class="math inline">\(y_t\)</span>. In this case, we have no way to distinguish between the true speed profile and the noise.</p>
<p>However, we can use regularization and bring some prior knowledge about traffic speed profiles to improve the estimate of the speed profile and to remove the noise.</p>
<p>Specifically, we will use a <strong>trend filtering</strong> approach. Under this approach, we assume that the speed profile <span class="math inline">\(f\)</span> is a piece-wise linear function of time, and we want to find a function that captures the underlying trend while ignoring the noise. The regularization term <span class="math inline">\(\phi(f)\)</span> is then the second difference of the speed profile, <span class="math display">\[
\lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|
\]</span> which penalizes the “kinks” in the speed profile. The value of this penalty is zero, when <span class="math inline">\(f_{t-1}, f_t, f_{t+1}\)</span> lie on a straight line, and it increases when the speed profile has a kink. The parameter <span class="math inline">\(\lambda\)</span> is a regularization parameter that controls the strength of the penalty.</p>
<p>Trend filtering penalized function is then <span class="math display">\[
(1/2) \sum_{t=1}^{n}(y_t - f_t)^2 + \lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|,
\]</span> which is a variation of a well-know Hodrick-Prescott filter.</p>
<p>This approach requires us to choose the regularization parameter <span class="math inline">\(\lambda\)</span>. A small value of <span class="math inline">\(\lambda\)</span> will lead to a function that fits the data well, but may not capture the underlying trend. A large value of <span class="math inline">\(\lambda\)</span> will lead to a function that captures the underlying trend, but may not fit the data well. The optimal value of <span class="math inline">\(\lambda\)</span> can be chosen using cross-validation or other model selection techniques. The left panel of <span class="quarto-unresolved-ref">?fig-traffic</span> shows the trend filtering for different values of <span class="math inline">\(\lambda \in \{5,50,500\}\)</span>. The right panel shows the optimal value of <span class="math inline">\(\lambda\)</span> chosen by cross-validation (by visual inspection).</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/traffic_l1.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filter for different penalty</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<dl>
<dt><img src="./fig//svg/day_295_tf.svg" class="img-fluid" alt="Trend filtering with optimal penalty"></dt>
<dd>
<p>Trend Filtering for Traffic Speed Data</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<p>There is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model <span class="math inline">\(f\)</span>. Given the likelihood <span class="math inline">\(L(y_i,f(x_i))\)</span>, the posterior is given by Bayes’ rule: <span class="math display">\[
p(f \mid y_i, x_i) = \frac{\prod_{i=1}^n L(y_i,f(x_i)) p(f)}{p(y_i \mid x_i)}.
\]</span> If we take the negative log of this posterior, we get: <span class="math display">\[
-\log p(f \mid y_i, x_i) = - \sum_{i=1}^n \log L(y_i,f(x_i)) - \log p(f) + \log p(y_i \mid x_i).
\]</span> Since loss is the negative log-likelihood <span class="math inline">\(-\log L(y_i,f(x_i))  = \mathcal{L}(y_i,f(x_i))\)</span>, the posterior maximization is equivalent to minimizing the following regularized loss function: <span class="math display">\[
\sum_{i=1}^n \mathcal{L}(y_i,f(x_i)) + \log p(f).
\]</span> The last term <span class="math inline">\(\log p(y_i \mid x_i)\)</span> does not depend on <span class="math inline">\(f\)</span> and can be ignored in the optimization problem. Thus, the equivalence is given by: <span class="math display">\[
\lambda \phi(f) = -\log p(f),
\]</span> where <span class="math inline">\(\phi(f)\)</span> is the penalty term that corresponds to the prior distribution of <span class="math inline">\(f\)</span>. Below we will consider several choices for the prior distribution of <span class="math inline">\(f\)</span> and the corresponding penalty term <span class="math inline">\(\phi(f)\)</span> commonly used in practice.</p>
</section>
<section id="ridge-regression" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">1.3</span> Ridge Regression</h2>
<p>The ridge regression uses a gaussian prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to a squared penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta \sim N(0, \sigma^2 I),
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix. The prior distribution of <span class="math inline">\(\beta\)</span> is a multivariate normal distribution with mean 0 and covariance <span class="math inline">\(\sigma^2 I\)</span>. The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{2\sigma^2} \|\beta||_2^2 + \text{const},
\]</span> where <span class="math inline">\(\|\beta||_2^2 = \sum_{j=1}^p \beta_j^2\)</span> is the squared 2-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{2\sigma^2} \|\beta||_2^2.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2 + \lambda ||\beta||_2^2,
\]</span> where <span class="math inline">\(\lambda = 1/\sigma^2\)</span> is the regularization parameter that controls the strength of the prior. The solution to this optimization problem is given by: <span class="math display">\[
\hat{\beta}_{\text{ridge}} = ( X^T X + \lambda I )^{-1} X^T y.
\]</span> The regularization parameter <span class="math inline">\(\lambda\)</span> is related to the variance of the prior distribution. When <span class="math inline">\(\lambda=0\)</span>, the function <span class="math inline">\(f\)</span> is the maximum likelihood estimate of the parameters. When <span class="math inline">\(\lambda\)</span> is large, the function <span class="math inline">\(f\)</span> is the prior mean of the parameters. When <span class="math inline">\(\lambda\)</span> is infinite, the function <span class="math inline">\(f\)</span> is the prior mode of the parameters.</p>
<p>Notice, that the OLS estimate (invented by Gauss) is a special case of ridge regression when <span class="math inline">\(\lambda = 0\)</span>: <span class="math display">\[
\hat{\beta}_{\text{OLS}} = ( X^T X )^{-1} X^T y.
\]</span></p>
<p>The original motivation for ridge regularisation was to address the problem of numerical instability in the OLS solution when the design matrix <span class="math inline">\(X\)</span> is ill-conditioned, i.e.&nbsp;when <span class="math inline">\(X^T X\)</span> is close to singular. In this case, the OLS solution can be very sensitive to small perturbations in the data, leading to large variations in the estimated coefficients <span class="math inline">\(\hat{\beta}\)</span>. This is particularly problematic when the number of features <span class="math inline">\(p\)</span> is large, as the condition number of <span class="math inline">\(X^T X\)</span> can grow rapidly with <span class="math inline">\(p\)</span>. The ridge regression solution stabilizes the OLS solution by adding a small positive constant <span class="math inline">\(\lambda\)</span> to the diagonal of the <span class="math inline">\(X^T X\)</span> matrix, which improves the condition number and makes the solution more robust to noise in the data. The additional term <span class="math inline">\(\lambda I\)</span> simply shifts the eigenvalues of <span class="math inline">\(X^T X\)</span> away from zero, thus improving the numerical stability of the inversion.</p>
<p>Another way to think and write the objective function of Ridge as the following constrained optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2 \quad \text{subject to} \quad ||\beta||_2^2 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the size of the coefficients <span class="math inline">\(\beta\)</span>. This formulation emphasizes the idea that ridge regression is a form of regularization that constrains the size of the coefficients, preventing them from growing too large and leading to overfitting. The constraint <span class="math inline">\(||\beta||_2^2 \leq t\)</span> can be interpreted as a budget on the size of the coefficients, where larger values of <span class="math inline">\(t\)</span> allow for larger coefficients and more complex models.</p>
<p>Constraint on the model parameters (and the original Ridge estimator) was proposed by <span class="citation" data-cites="tikhonov1943stability">Tikhonov et al. (<a href="references.html#ref-tikhonov1943stability" role="doc-biblioref">1943</a>)</span> for solving inverse problems to “discover” physical laws from observations. The norm of the <span class="math inline">\(\beta\)</span> vector would usually represent amount of energy required. Many processes in nature are energy minimizing!</p>
<p>Again, the tuning parameter <span class="math inline">\(\lambda\)</span> controls trade-off between how well model fits the data and how small <span class="math inline">\(\beta\)</span>s are. Different values of <span class="math inline">\(\lambda\)</span> will lead to different models. We select <span class="math inline">\(\lambda\)</span> using cross validation.</p>
<div id="exm-simulated-bias" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Shrinkage)</strong></span> Consider a simulated data with <span class="math inline">\(n=50\)</span>, <span class="math inline">\(p=30\)</span>, and <span class="math inline">\(\sigma^2=1\)</span>. The true model is linear with <span class="math inline">\(10\)</span> large coefficients between <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Our approximators <span class="math inline">\(\hat f_{\beta}\)</span> is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss <span class="math inline">\(1/n||y -\hat y||_2^2\)</span> and variance can be empirically calculated as <span class="math inline">\(1/n\sum  (\bar{\hat{y}} - \hat y_i)\)</span></p>
<p>Bias squared <span class="math inline">\(\mathrm{Bias}(\hat{y})^2=0.006\)</span> and variance <span class="math inline">\(\Var{\hat{y}} =0.627\)</span>. Thus, the prediction error = <span class="math inline">\(1 + 0.006 + 0.627 = 1.633\)</span></p>
<p>We’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_beta.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>True model coefficients</figcaption>
</figure>
</div>
<p>Now we see the accuracy of the model as a function of <span class="math inline">\(\lambda\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_mse.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Prediction error as a function of <span class="math inline">\(\lambda\)</span></figcaption>
</figure>
</div>
<p>Ridge Regression At best: Bias squared <span class="math inline">\(=0.077\)</span> and variance <span class="math inline">\(=0.402\)</span>.</p>
<p>Prediction error = <span class="math inline">\(1 + 0.077 + 0.403 = 1.48\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_bias_variance.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Ridge</figcaption>
</figure>
</div>
</div>
<section id="kernel-view-of-ridge-regression" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="kernel-view-of-ridge-regression"><span class="header-section-number">1.3.1</span> Kernel View of Ridge Regression</h3>
<p>Another interesting view stems from what is called the push-through matrix identity: <span class="math display">\[
(aI + UV)^{-1}U = U(aI + VU)^{-1}
\]</span> for <span class="math inline">\(a\)</span>, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> such that the products are well-defined and the inverses exist. We can obtain this from <span class="math inline">\(U(aI + VU) = (aI + UV)U\)</span>, followed by multiplication by <span class="math inline">\((aI + UV)^{-1}\)</span> on the left and the right. Applying the identity above to the ridge regression solution with <span class="math inline">\(a = \lambda\)</span>, <span class="math inline">\(U = X^T\)</span>, and <span class="math inline">\(V = X\)</span>, we obtain an alternative form for the ridge solution: <span class="math display">\[
\hat{\beta} = X^T (XX^T + \lambda I)^{-1} Y.
\]</span> This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as <span class="math display">\[
X\hat{\beta} = XX^T (XX^T + \lambda I)^{-1} Y.
\]</span> What does this remind you of? This is precisely <span class="math inline">\(K(K + \lambda I)^{-1}Y\)</span> where <span class="math inline">\(K = XX^T\)</span>, which, recall, is the fit from RKHS regression with a linear kernel <span class="math inline">\(k(x, z) = x^T z\)</span>. Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.) <span class="math display">\[
(aI + UV)^{-1}U = U(aI + VU)^{-1}
\]</span> for <span class="math inline">\(a\)</span>, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> such that the products are well-defined and the inverses exist. We can obtain this from <span class="math inline">\(U(aI + VU) = (aI + UV)U\)</span>, followed by multiplication by <span class="math inline">\((aI + UV)^{-1}\)</span> on the left and the right. Applying the identity above to the ridge regression solution with <span class="math inline">\(a = \lambda\)</span>, <span class="math inline">\(U = X^T\)</span>, and <span class="math inline">\(V = X\)</span>, we obtain an alternative form for the ridge solution: <span class="math display">\[
\hat{\beta} = X^T (XX^T + \lambda I)^{-1} Y.
\]</span> This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as <span class="math display">\[
X\hat{\beta} = XX^T (XX^T + \lambda I)^{-1} Y.
\]</span> What does this remind you of? This is precisely <span class="math inline">\(K(K + \lambda I)^{-1}Y\)</span> where <span class="math inline">\(K = XX^T\)</span>, which, recall, is the fit from RKHS regression with a linear kernel <span class="math inline">\(k(x, z) = x^T z\)</span>. Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)</p>
</section>
</section>
<section id="lasso-regression" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="lasso-regression"><span class="header-section-number">1.4</span> Lasso Regression</h2>
<p>The Lasso (Least Absolute Shrinkage and Selection Operator) regression uses a Laplace prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to an <span class="math inline">\(\ell_1\)</span> penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta_j \sim \text{Laplace}(0, b) \quad \text{independently for } j = 1, \ldots, p,
\]</span> where <span class="math inline">\(b &gt; 0\)</span> is the scale parameter. The Laplace distribution has the probability density function: <span class="math display">\[
p(\beta_j \mid b) = \frac{1}{2b}\exp\left(-\frac{|\beta_j|}{b}\right).
\]</span> The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{b} \|\beta\|_1 + \text{const},
\]</span> where <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^p |\beta_j|\)</span> is the <span class="math inline">\(\ell_1\)</span>-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{b} \|\beta\|_1.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 + \lambda \|\beta\|_1,
\]</span> where <span class="math inline">\(\lambda = 2\sigma^2/b\)</span> is the regularization parameter that controls the strength of the prior. Unlike ridge regression, the Lasso optimization problem does not have a closed-form solution due to the non-differentiable nature of the <span class="math inline">\(\ell_1\)</span> penalty. However, efficient algorithms such as coordinate descent and proximal gradient methods can be used to solve it.</p>
<p>The key distinguishing feature of Lasso is its ability to perform automatic variable selection. The <span class="math inline">\(\ell_1\)</span> penalty encourages sparsity in the coefficient vector <span class="math inline">\(\hat{\beta}\)</span>, meaning that many coefficients will be exactly zero. This property makes Lasso particularly useful for high-dimensional problems where feature selection is important.</p>
<p>When <span class="math inline">\(\lambda=0\)</span>, the Lasso reduces to the ordinary least squares (OLS) estimate. As <span class="math inline">\(\lambda\)</span> increases, more coefficients are driven to exactly zero, resulting in a sparser model. When <span class="math inline">\(\lambda\)</span> is very large, all coefficients become zero.</p>
<p>The geometric intuition behind Lasso’s sparsity-inducing property comes from the constraint formulation. We can write the Lasso problem as: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_1 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the sparsity of the solution. The constraint region <span class="math inline">\(\|\beta\|_1 \leq t\)</span> forms a diamond (in 2D) or rhombus-shaped region with sharp corners at the coordinate axes. The optimal solution often occurs at these corners, where some coefficients are exactly zero.</p>
<p>From a Bayesian perspective, the Lasso estimator corresponds to the maximum a posteriori (MAP) estimate under independent Laplace priors on the coefficients. We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior: <span class="math display">\[
\log p(\beta \mid y, b) \propto -\|y-X\beta\|_2^2 - \frac{2\sigma^2}{b}\|\beta\|_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span>, the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
<p>One of the most popular algorithms for solving the Lasso problem is coordinate descent. The algorithm iteratively updates each coefficient while holding all others fixed. For the <span class="math inline">\(j\)</span>-th coefficient, the update rule is: <span class="math display">\[
\hat{\beta}_j \leftarrow \text{soft}\left(\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - \sum_{k \neq j} x_{ik}\hat{\beta}_k), \frac{\lambda}{n}\right),
\]</span> where the soft-thresholding operator is defined as: <span class="math display">\[
\text{soft}(z, \gamma) = \text{sign}(z)(|z| - \gamma)_+ = \begin{cases}
z - \gamma &amp; \text{if } z &gt; \gamma \\
0 &amp; \text{if } |z| \leq \gamma \\
z + \gamma &amp; \text{if } z &lt; -\gamma
\end{cases}
\]</span></p>
<div id="exm-lasso-sparsity" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Sparsity and Variable Selection)</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simulated data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># number of observations</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">20</span>   <span class="co"># number of predictors</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># noise level</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create design matrix with some correlation structure</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Add some correlation between predictors</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>p) {</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  X[, i] <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> X[, i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">sqrt</span>(<span class="fl">0.75</span>) <span class="sc">*</span> X[, i]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># True coefficients - sparse signal</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>sparse_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(beta_true <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate response</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> sigma <span class="sc">*</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit LASSO path using glmnet</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot coefficient paths</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"LASSO Coefficient Paths"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation to select optimal lambda</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cross-validation curve</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv_lasso)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"Cross-Validation for LASSO"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-8-2.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients at optimal lambda</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lambda_min <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda.min</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>lambda_1se <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>coef_min <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_min)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>coef_1se <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_1se)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare estimates with true values</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">True =</span> <span class="fu">c</span>(<span class="dv">0</span>, beta_true),  <span class="co"># Include intercept</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">LASSO_min =</span> <span class="fu">as.vector</span>(coef_min),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">LASSO_1se =</span> <span class="fu">as.vector</span>(coef_1se)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(comparison) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print comparison table</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">round</span>(comparison, <span class="dv">3</span>), <span class="at">caption =</span> <span class="st">"Comparison of True and Estimated Coefficients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Comparison of True and Estimated Coefficients</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">True</th>
<th style="text-align: right;">LASSO_min</th>
<th style="text-align: right;">LASSO_1se</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Intercept</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.12</td>
<td style="text-align: right;">-0.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">X1</td>
<td style="text-align: right;">3.0</td>
<td style="text-align: right;">2.92</td>
<td style="text-align: right;">2.67</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X2</td>
<td style="text-align: right;">-2.0</td>
<td style="text-align: right;">-2.11</td>
<td style="text-align: right;">-1.79</td>
</tr>
<tr class="even">
<td style="text-align: left;">X3</td>
<td style="text-align: right;">1.5</td>
<td style="text-align: right;">1.66</td>
<td style="text-align: right;">1.42</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X4</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X5</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.01</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X6</td>
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">1.97</td>
<td style="text-align: right;">1.83</td>
</tr>
<tr class="even">
<td style="text-align: left;">X7</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.06</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X8</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.10</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X9</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.04</td>
<td style="text-align: right;">-0.03</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X10</td>
<td style="text-align: right;">-1.0</td>
<td style="text-align: right;">-1.02</td>
<td style="text-align: right;">-0.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">X11</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X12</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X13</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.11</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X14</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X15</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X16</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X17</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X18</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.03</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">X19</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">X20</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.13</td>
<td style="text-align: right;">0.00</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization of coefficient estimates</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Melt data for plotting</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">melt</span>(comparison, <span class="at">id.vars =</span> <span class="cn">NULL</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plot_data<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">rownames</span>(comparison), <span class="dv">3</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plot_data<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">factor</span>(plot_data<span class="sc">$</span>Variable, <span class="at">levels =</span> <span class="fu">rownames</span>(comparison))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> Variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">position =</span> <span class="st">"dodge"</span>) <span class="sc">+</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Coefficient Estimates Comparison"</span>, </span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Coefficient Value"</span>, </span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">fill =</span> <span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">type =</span> <span class="st">"qual"</span>, <span class="at">palette =</span> <span class="st">"Set2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-8-3.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction errors</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>pred_min <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_min)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>pred_1se <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_1se)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>mse_min <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_min)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>mse_1se <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_1se)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.min):"</span>, <span class="fu">round</span>(mse_min, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Mean Squared Error (lambda.min): 0.68</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.1se):"</span>, <span class="fu">round</span>(mse_1se, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Mean Squared Error (lambda.1se): 0.85</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable selection performance</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>selected_min <span class="ot">&lt;-</span> <span class="fu">which</span>(coef_min[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">!=</span> <span class="dv">0</span>)  <span class="co"># Exclude intercept</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>selected_1se <span class="ot">&lt;-</span> <span class="fu">which</span>(coef_1se[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">!=</span> <span class="dv">0</span>)  <span class="co"># Exclude intercept</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">True non-zero coefficients:"</span>, sparse_indices, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## True non-zero coefficients: 1 2 3 6 10</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Selected by LASSO (lambda.min):"</span>, selected_min, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Selected by LASSO (lambda.min): 1 2 3 5 6 7 8 9 10 11 12 13 15 18 19 20</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Selected by LASSO (lambda.1se):"</span>, selected_1se, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Selected by LASSO (lambda.1se): 1 2 3 5 6 9 10</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate selection metrics</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>true_positives_min <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">intersect</span>(sparse_indices, selected_min))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>false_positives_min <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">setdiff</span>(selected_min, sparse_indices))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>false_negatives_min <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">setdiff</span>(sparse_indices, selected_min))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>precision_min <span class="ot">&lt;-</span> true_positives_min <span class="sc">/</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">length</span>(selected_min))</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>recall_min <span class="ot">&lt;-</span> true_positives_min <span class="sc">/</span> <span class="fu">length</span>(sparse_indices)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Selection Performance (lambda.min):</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Selection Performance (lambda.min):</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Precision:"</span>, <span class="fu">round</span>(precision_min, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Precision: 0.31</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Recall:"</span>, <span class="fu">round</span>(recall_min, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Recall: 1</code></pre>
</div>
</div>
<p>The coefficient paths plot shows how LASSO coefficients shrink toward zero as the regularization parameter lambda increases. The colored lines represent different predictors, demonstrating LASSO’s variable selection property. The cross-validation plot reveals the bias-variance tradeoff: - lambda.min gives the minimum CV error (best predictive performance) - lambda.1se provides a more parsimonious model (within 1 SE of minimum) Performance metrics show LASSO’s effectiveness: - High precision and recall indicate successful variable selection - Lower MSE compared to unregularized models demonstrates regularization benefits - The comparison table reveals LASSO correctly identifies most true signals while setting noise variables to zero The coefficient estimates visualization clearly shows: - True coefficients (ground truth) - LASSO estimates at different lambda values - How regularization affects both signal preservation and noise suppression Key insights: 1. LASSO successfully performs automatic variable selection 2. The method balances model complexity with predictive accuracy 3. Cross-validation provides principled selection of regularization strength 4. Results demonstrate the practical value of sparsity-inducing priors ::</p>
<section id="scale-mixture-representation" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="scale-mixture-representation"><span class="header-section-number">1.4.1</span> Scale Mixture Representation</h3>
<p>The Laplace distribution can be represented as a scale mixture of Normal distributions <span class="citation" data-cites="andrews1974scale">(<a href="references.html#ref-andrews1974scale" role="doc-biblioref">Andrews and Mallows 1974</a>)</span>: <span class="math display">\[
\begin{aligned}
\beta_j \mid \sigma^2,\tau_j &amp;\sim N(0,\tau_j^2\sigma^2)\\
\tau_j^2 \mid \alpha &amp;\sim \text{Exp}(\alpha^2/2)\\
\sigma^2 &amp;\sim \pi(\sigma^2).
\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau_j\)</span>: <span class="math display">\[
p(\beta_j\mid \sigma^2,\alpha) = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi \tau_j\sigma^2}}\exp\left(-\frac{\beta_j^2}{2\sigma^2\tau_j^2}\right)\frac{\alpha^2}{2}\exp\left(-\frac{\alpha^2\tau_j^2}{2}\right)d\tau_j = \frac{\alpha}{2\sigma}\exp\left(-\frac{\alpha|\beta_j|}{\sigma}\right).
\]</span> This representation allows for efficient Gibbs sampling algorithms that can automatically tune the regularization parameter through the hierarchical Bayesian framework.</p>
</section>
<section id="penalty-and-regularisation" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="penalty-and-regularisation"><span class="header-section-number">1.5</span> Penalty and Regularisation</h2>
<p>The problem of finding a good model boils down to finding <span class="math inline">\(\phi\)</span> that minimize some form of Bayes risk for the problem at hand.</p>
<p>There are a number of commonly used penalty functions (a.k.a. log prior density). For example, the $ l^2$-norm corresponds to s normal prior. The resulting Bayes rule will take the form of a shrinkage estimator, a weighted combination between data and prior beliefs about the parameter. An $ l^1 $-norm will induce a sparse solution in the estimator and can be used an a variable selection operator. The $ l_0 $-norm directly induces a subset selection procedure.</p>
<p>The amount of regularisation <span class="math inline">\(\lambda\)</span> gauges the trade-off between the compromise between the observed data and the initial prior beliefs.</p>
<p>There are two main approaches to finding a good model:</p>
<ol type="1">
<li>Full Bayes: This approach places a prior distribution on the parameters and computes the full posterior distribution.</li>
<li>Regularization Methods: These approaches add penalty terms to the objective function to control model complexity.</li>
</ol>
<p>Now, let’s look at those two approaches in more detail.</p>
<p>The <strong>full Bayes</strong> approach is to place a prior distribution on the parameters and compute the full posterior distribution using the Bayes rule: <span class="math display">\[
p( \theta | y ) = \frac{ f( y | \theta ) p( \theta ) }{ m(y) },
\]</span> here <span class="math display">\[
m(y) = \int f( y| \theta ) p( \theta ) d \theta
\]</span> Here <span class="math inline">\(m(y)\)</span> is the marginal beliefs about the data. This can also be used to choose the amount of regularisation via the type II maximum likelihood estimator (MMLE) defined by <span class="math display">\[
\hat{\tau} = \arg \max \log m( y | \tau )
\]</span> where again $ m( y | ) = f( y | ) p( | ) $.</p>
<p>For example, in the normal-normal model, with <span class="math inline">\(\mu=0\)</span>, we can integrate out the high dimensional <span class="math inline">\(\theta\)</span> and find <span class="math inline">\(m(y | \tau)\)</span> in closed form as <span class="math inline">\(y_i \sim N(0, \sigma^2 + \tau^2)\)</span> <span class="math display">\[
m( y | \tau ) = ( 2 \pi)^{-n/2} ( \sigma^2 + \tau^2 )^{- n/2}  \exp \left ( - \frac{ \sum y_i^2 }{ 2 ( \sigma^2 + \tau^2) }
\]</span> The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean <span class="math inline">\(\bar y\)</span> and in this approach <span class="math display">\[
\theta \sim N(\mu,\tau^2).
\]</span> The original JS is <span class="math inline">\(\mu=0\)</span>. To estimate the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> you can do full Bayes or empirical Bayes that shrinks to overall grand mean <span class="math inline">\(\bar y\)</span>, which serves as the estimate of the original prior mean <span class="math inline">\(\mu\)</span>. It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior <span class="citation" data-cites="diaconis1983quantifying">Diaconis and Ylvisaker (<a href="references.html#ref-diaconis1983quantifying" role="doc-biblioref">1983</a>)</span> with marginal MLE (MMLE). The MMLE is the product <span class="math display">\[
\int_{\theta_i}\prod_{i=1}^k p(\bar y_i \mid \theta_i)p(\theta_i \mid \mu, \tau^2).
\]</span></p>
<p>Common examples include ridge regression (L2 penalty), lasso regression (L1 penalty), and elastic net (combination of L1 and L2 penalties).</p>
<p>Rather than having to perform high dimensional integration with the likes of MCMC etc, a common approach is to use a maximum a posteriori (MAP) estimator defined by <span class="math display">\[
\hat{\theta} = \arg \max \log p ( \theta | y )
\]</span> This can directly lead to sparsity as in the case of <span class="math inline">\(\ell_1\)</span>-norm optimisation.</p>
</section>
<section id="ell_2-shrinkage." class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="ell_2-shrinkage."><span class="header-section-number">1.6</span> <span class="math inline">\(\ell_2\)</span> Shrinkage.</h2>
<p>The original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean <span class="math inline">\(\bar y\)</span> and in this approach <span class="math display">\[
\theta \sim N(\mu,\tau^2).
\]</span> The original JS uses <span class="math inline">\(\mu=0\)</span>. To estimate the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> you can do full Bayes or empirical Bayes that shrinks to overall grand mean <span class="math inline">\(\bar y\)</span>, which serves as the estimate of the original prior mean <span class="math inline">\(\mu\)</span>. It seems paradoxical that you estimate proper prior from the data. However, this is not the case. You simply use mixture prior <span class="citation" data-cites="diaconis1983quantifying">Diaconis and Ylvisaker (<a href="references.html#ref-diaconis1983quantifying" role="doc-biblioref">1983</a>)</span> with marginal MLE (MMLE). The MMLE is the product <span class="math display">\[
\int_{\theta_i}\prod_{i=1}^k p(\bar y_i \mid \theta_i)p(\theta_i \mid \mu, \tau^2).
\]</span></p>
<section id="sparse-r-spike-problem" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="sparse-r-spike-problem"><span class="header-section-number">1.6.1</span> Sparse <span class="math inline">\(r\)</span>-spike problem</h3>
<p>For the sparse <span class="math inline">\(r\)</span>-spike problem we require a different rule. For a sparse signal, however, <span class="math inline">\(\hat \theta_{JS}\)</span> performs poorly when the true parameter is an <span class="math inline">\(r\)</span>-spike where <span class="math inline">\(\theta_r\)</span> has <span class="math inline">\(r\)</span> coordinates at <span class="math inline">\(\sqrt{p/r}\)</span> and the rest set at zero with norm <span class="math inline">\({\Vert \theta_r \Vert}^2 =p\)</span>.</p>
<p>The classical risk satisfies <span class="math inline">\(R \left ( \hat \theta_{JS} , \theta_r \right ) \geq p/2\)</span> where the simple thresholding rule <span class="math inline">\(\sqrt{2 \ln p}\)</span> performs with risk <span class="math inline">\(\sqrt{\ln p}\)</span> in the <span class="math inline">\(r\)</span>-spike sparse case even though it is inadmissible in MSE for a non-sparse signal. Then is due to the fact that for <span class="math inline">\(\theta_p\)</span> we have <span class="math display">\[
\frac{p \Vert \theta \Vert^2}{p + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_p \right ) \leq
2 + \frac{p \Vert \theta \Vert^2}{ d + \Vert \theta \Vert^2}.
\]</span> This implies that <span class="math inline">\(R \left ( \hat{\theta}^{JS} , \theta_p \right ) \geq (p/2)\)</span>. Hence, simple thresholding rule beats James-Stein this with a risk given by <span class="math inline">\(\sqrt{\log p }\)</span>. This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
<p>A Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate <span class="math display">\[
\sup_{ \theta \in l_0[p_n] } \;
\mathbb{E}_{ y | \theta } \|\hat y_{hs} - \theta \|^2 \asymp
p_n \log \left ( n / p_n \right ).
\]</span> The “worst’’ <span class="math inline">\(\theta\)</span> is obtained at the maximum difference between <span class="math inline">\(\left| \hat \theta_{HS} - y \right|\)</span> where <span class="math inline">\(\hat \theta_{HS} = \mathbb{E}(\theta|y)\)</span> can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).</p>
<p>One such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span>.</p>
</section>
<section id="efron-example" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="efron-example"><span class="header-section-number">1.6.2</span> Efron Example</h3>
<p>Efron provide an example which shows the importance of specifying priors in high dimensions. The key idea behind James-Stein shrinkage is that one when one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.</p>
<p>Stein’s phenomenon where <span class="math inline">\(y_i | \theta_i \sim N(\theta_i, 1)\)</span> and <span class="math inline">\(\theta_i \sim N(0, \tau^2)\)</span> where <span class="math inline">\(\tau \rightarrow \infty\)</span> illustrates this point well. This leads to the improper “non-informative” uniform prior. The corresponding generalized Bayes rule is the vector of means—which we know is inadmissible. so no regularisation leads to an estimator with poor risk property.</p>
<p>Let <span class="math inline">\(\|y\| = \sum_{i=1}^p y_i^2\)</span>. Then, we can make the following probabilistic statements from the model, <span class="math display">\[
P\left( \| y \| &gt; \| \theta \| \right) &gt; \frac{1}{2}
\]</span> Now for the posterior, this inequallty is reversed under a flat Lebesgue measure, <span class="math display">\[
P\left( \| \theta \| &gt; \| y \| \; | \; y \right) &gt; \frac{1}{2}
\]</span> which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.</p>
<p>The shrinkage rule (a.k.a. normal prior) where <span class="math inline">\(\tau^2\)</span> is “estimated” from the data avoids this conflict. More precisely, we have <span class="math display">\[
\hat{\theta}(y) = \left( 1 - \frac{k-2}{\|y\|^2} \right) y \quad \text{and} \quad E\left( \| \hat{\theta} - \theta \| \right) &lt; k, \; \forall \theta.
\]</span> Hence, when <span class="math inline">\(\|y\|^2\)</span> is small the shrinkage factor is more extreme. For example, if <span class="math inline">\(k=10\)</span>, <span class="math inline">\(\|y\|^2=12\)</span>, then <span class="math inline">\(\hat{\theta} = (1/3) y\)</span>. Now we have the more intuitive result that <span class="math inline">\(P\left(\|\theta\| &gt; \|y\| \; | \; y\right) &lt; \frac{1}{2}\)</span>.</p>
<p>This shows that careful specification of default priors matter in high dimensions is necessary.</p>
</section>
</section>
<section id="ell_1-sparsity" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="ell_1-sparsity"><span class="header-section-number">1.7</span> <span class="math inline">\(\ell_1\)</span> Sparsity</h2>
</section>
<section id="ell_0-subset-selection" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="ell_0-subset-selection"><span class="header-section-number">1.8</span> <span class="math inline">\(\ell_0\)</span> Subset Selection</h2>
<p>The canonical problem is estimaiton of the normal means problem. Here we have <span class="math inline">\(y_i = \theta_i + e_i,~i=1,\ldots,p\)</span> and <span class="math inline">\(e_i \sim N(0, \sigma^2)\)</span>. The goal is to estimate the vector of means <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. This is also a proxy for non-parametric regression, where <span class="math inline">\(\theta_i = f(x_i)\)</span>. Aslo typically <span class="math inline">\(y_i\)</span> is a mean of <span class="math inline">\(n\)</span> observations, i.e.&nbsp;<span class="math inline">\(y_i = \frac{1}{n} \sum_{j=1}^n x_{ij}\)</span>. ## James-Stein Estimator The classic James-Stein shrinkage rule, <span class="math inline">\(\hat y_{js}\)</span>, uniformly dominates the traditional sample mean estimator, <span class="math inline">\(\hat{\theta}\)</span>, for all values of the true parameter <span class="math inline">\(\theta\)</span>. In classical MSE risk terms: <span class="math display">\[
R(\hat y_{js}, \theta) \defeq E_{y|\theta} {\Vert \hat y_{js} - \theta \Vert}^2 &lt; p
    = E_{y|\theta} {\Vert y - \theta \Vert}^2, \;\;\; \forall \theta
\]</span> For a sparse signal, however, <span class="math inline">\(\hat y_{js}\)</span> performs poorly when the true parameter is an <span class="math inline">\(r\)</span>-spike where <span class="math inline">\(\theta_r\)</span> has <span class="math inline">\(r\)</span> coordinates at <span class="math inline">\(\sqrt{p/r}\)</span> and the rest set at zero with norm <span class="math inline">\({\Vert \theta_r \Vert}^2 =p\)</span>.</p>
<p>The classical risk satisfies <span class="math inline">\(R \left ( \hat y_{js} , \theta_r \right ) \geq p/2\)</span> where the simple thresholding rule <span class="math inline">\(\sqrt{2 \ln p}\)</span> performs with risk <span class="math inline">\(\sqrt{\ln p}\)</span> in the <span class="math inline">\(r\)</span>-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.</p>
</section>
<section id="r-spike-problem" class="level2" data-number="1.9">
<h2 data-number="1.9" class="anchored" data-anchor-id="r-spike-problem"><span class="header-section-number">1.9</span> R-spike Problem</h2>
<p>From a historical perspective, James-Stein (a.k.a <span class="math inline">\(L^2\)</span>-regularisation)<span class="citation" data-cites="stein1964inadmissibility">(<a href="references.html#ref-stein1964inadmissibility" role="doc-biblioref">Stein 1964</a>)</span> is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with <span class="math inline">\(L^2\)</span>-regularisation. Consider the sparse <span class="math inline">\(r\)</span>-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).</p>
<p>Let the true parameter value be <span class="math inline">\(\theta_p = \left ( \sqrt{d/p} , \ldots , \sqrt{d/p} , 0 , \ldots , 0 \right )\)</span>. James-Stein is equivalent to the model <span class="math display">\[
y_i = \theta_i + \epsilon_i \; \mathrm{ and} \; \theta_i \sim \mathcal{N} \left ( 0 , \tau^2 \right )
\]</span> This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage <span class="math inline">\(\hat{\tau}\)</span> is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of <span class="math inline">\(p(\tau^2|y)\)</span> is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective <span class="math inline">\(E \Vert \hat{\theta}^{JS} - \theta \Vert \leq p , \forall \theta\)</span> showing the inadmissibility of the MLE. At origin the risk is <span class="math inline">\(2\)</span>, <strong>but!</strong> <span class="math display">\[
\frac{p \Vert \theta \Vert^2}{p + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_p \right ) \leq
2 + \frac{p \Vert \theta \Vert^2}{ d + \Vert \theta \Vert^2}.
\]</span> This implies that <span class="math inline">\(R \left ( \hat{\theta}^{JS} , \theta_p \right ) \geq (p/2)\)</span>. Hence, simple thresholding rule beats James-Stein this with a risk given by <span class="math inline">\(\sqrt{\log p }\)</span>. This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
<p>The horseshoe estimator, which we will discuss in more detail later, <span class="math inline">\(\hat y_{hs}\)</span>, was proposed by <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span> to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate <span class="math display">\[
\sup_{ \theta \in l_0[p_n] } \;
\mathbb{E}_{ y | \theta } \|\hat y_{hs} - \theta \|^2 \asymp
p_n \log \left ( n / p_n \right ).
\]</span> The “worst’’ <span class="math inline">\(\theta\)</span> is obtained at the maximum difference between <span class="math inline">\(\left|\hat{y}_{hs} - y\right|\)</span> where <span class="math inline">\(\hat{y}_{hs} = \mathbb{E}(\theta|y)\)</span> can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).</p>
</section>
<section id="ell_2-shrinkage" class="level2" data-number="1.10">
<h2 data-number="1.10" class="anchored" data-anchor-id="ell_2-shrinkage"><span class="header-section-number">1.10</span> <span class="math inline">\(\ell_2\)</span> Shrinkage</h2>
<div id="exm-stein" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Stein’s Paradox)</strong></span> Stein’s paradox, as explained <span class="citation" data-cites="efron1977steins">Efron and Morris (<a href="references.html#ref-efron1977steins" role="doc-biblioref">1977</a>)</span>, is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.</p>
<p>In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.</p>
<p>In each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.</p>
<p>Suppose that we have <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(y_{1},\ldots,y_{n}\)</span> from a <span class="math inline">\(N\left(  \theta,\sigma^{2}\right)\)</span> distribution. The maximum likelihood estimator is <span class="math inline">\(\widehat{\theta}=\bar{y}\)</span>, the sample mean. The Bayes estimator is the posterior mean, <span class="math display">\[
\widehat{\theta}=\mathbb{E}\left[  \theta\mid y\right]  =\frac{\sigma^{2}}{\sigma^{2}+n}% \bar{y}.
\]</span> The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE <span class="math display">\[
\widehat{\theta}=\frac{\sigma^{2}}{\sigma^{2}+n}\bar{y}+\frac{n}{\sigma^{2}+n}\widehat{\theta}.
\]</span> This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE <span class="math display">\[
\widehat{\theta}=\frac{\sigma^{2}}{\sigma^{2}+n}\bar{y}+\frac{n}{\sigma^{2}+n}\widehat{\theta}.
\]</span> This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.</p>
<p>The original JS estimator shranks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and MOrris and Lindley showed that you want o shrink to overall mean <span class="math inline">\(\bar y\)</span> and in this approach <span class="math display">\[
\theta \sim N(\mu,\tau^2).
\]</span> The original JS is <span class="math inline">\(\mu=0\)</span>. To estimate the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> you can do full Bayes or empirical Bayes that shrinks to overall grand mean <span class="math inline">\(\bar y\)</span>, whcih serves as the estimate of the original prior mean <span class="math inline">\(\mu\)</span>. It seems paradoxical that you estimate propr from the data. However, this is not the case. You simply use mixture prior <span class="citation" data-cites="diaconis1983quantifying">Diaconis and Ylvisaker (<a href="references.html#ref-diaconis1983quantifying" role="doc-biblioref">1983</a>)</span> with marginal MLE (MMLE). The MMLE is the product <span class="math display">\[
\int_{\theta_i}\prod_{i=1}^k p(\bar y_i \mid \theta_i)p(\theta_i \mid \mu, \tau^2).
\]</span></p>
<p>We reproduce the baseball bartting average example from <span class="citation" data-cites="efron1977steins">Efron and Morris (<a href="references.html#ref-efron1977steins" role="doc-biblioref">1977</a>)</span>. Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/EfronMorrisBB.txt"</span>, <span class="at">sep =</span> <span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> <span class="fu">select</span>(LastName,AtBats,BattingAverage,SeasonAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can eatimate overall mean and variance</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(baseball<span class="sc">$</span>BattingAverage)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> <span class="fu">var</span>(baseball<span class="sc">$</span>BattingAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As well as the osterior mean for each player (James-Stein estimator)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">&lt;-</span> baseball <span class="sc">%&gt;%</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">JS =</span> (sigma2_hat <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> mu_hat <span class="sc">+</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>      ((BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats) <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> BattingAverage</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(baseball)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">LastName</th>
<th style="text-align: right;">AtBats</th>
<th style="text-align: right;">BattingAverage</th>
<th style="text-align: right;">SeasonAverage</th>
<th style="text-align: right;">JS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Clemente</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr class="even">
<td style="text-align: left;">Robinson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Howard</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.31</td>
</tr>
<tr class="even">
<td style="text-align: left;">Johnstone</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Berry</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spencer</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kessinger</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.28</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvarado</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Santo</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Swaboda</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Petrocelli</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rodriguez</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scott</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Unser</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Williams</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Campaneris</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Munson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvis</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.22</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Plot below shows the observed averages vs.&nbsp;James-Stein estimate</p>
<div class="cell" data-fold="true">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(baseball, <span class="fu">aes</span>(<span class="at">x =</span> BattingAverage, <span class="at">y =</span> JS)) <span class="sc">+</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Observed Batting Average"</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"James-Stein Estimate"</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Empirical Bayes Shrinkage of Batting Averages (2016)"</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Calculate mean squared error (MSE) for observed and James-Stein estimates</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>mse_observed <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>BattingAverage <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>mse_js <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>JS <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (Observed): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_observed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (Observed): 0.004584</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (James-Stein): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_js))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (James-Stein): 0.001031</code></pre>
</div>
</div>
<p>We can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="fu">nrow</span>(baseball)), <span class="dv">3</span>, <span class="fu">nrow</span>(baseball))</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>SeasonAverage, baseball<span class="sc">$</span>JS),    <span class="dv">3</span>, <span class="fu">nrow</span>(baseball), <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(a, b, <span class="at">pch=</span><span class="st">" "</span>, <span class="at">ylab=</span><span class="st">"predicted average"</span>, <span class="at">xaxt=</span><span class="st">"n"</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">3.1</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.13</span>, <span class="fl">0.42</span>))</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="fu">matlines</span>(a, b)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">rep</span>(<span class="fl">0.7</span>, <span class="fu">nrow</span>(baseball)), baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>LastName, <span class="at">cex=</span><span class="fl">0.6</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span>, <span class="fl">0.14</span>, <span class="st">"First 45</span><span class="sc">\n</span><span class="st">at bats"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">2</span>, <span class="fl">0.14</span>, <span class="st">"Average</span><span class="sc">\n</span><span class="st">of remainder"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">3</span>, <span class="fl">0.14</span>, <span class="st">"J-S</span><span class="sc">\n</span><span class="st">estimator"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now if we look at the season dynamics for Clemente</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&amp;y=1970</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/clemente.csv"</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>H)<span class="sc">/</span><span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot x,y startind from index 2</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">-</span>ind],y[<span class="sc">-</span>ind], <span class="at">type=</span><span class="st">'o'</span>, <span class="at">ylab=</span><span class="st">"Betting Average"</span>, <span class="at">xlab=</span><span class="st">"Number at Bats"</span>)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add horizontal line for season average 145/412 and add text above line `Seaosn Average`</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span> <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"Season Average"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ted williams record is .406 in in 1941, so you know the first data points are noise</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>JS[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"JS"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>JS[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"After 45 Bets"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>The motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments.</p>
<div id="exm-stein" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Efron Example)</strong></span> Efron shows the importance of priors in high dimensions when one can “borrow strength” (a.k.a. regularisation) across components.</p>
<p>Stein’s phenomenon where <span class="math inline">\(y_i | \theta_i \sim N(\theta_i, 1)\)</span> and <span class="math inline">\(\theta_i \sim N(0, \tau^2)\)</span> illustrates this point well. From the model,</p>
<p><span class="math display">\[
P\left( \| y \| &gt; \| \theta \| \right) &gt; \frac{1}{2}
\]</span></p>
<p>Under a flat Lebesgue measure, this inequality is reversed in the posterior, namely</p>
<p><span class="math display">\[
P\left( \| \theta \| &gt; \| y \| \; | \; y \right) &gt; \frac{1}{2}
\]</span></p>
<p>In conflict with the classical statement. However, if we use Stein’s rule (posterior where <span class="math inline">\(\tau^2\)</span> is estimated via empirical Bayes) we have</p>
<p><span class="math display">\[
\hat{\theta}(y) = \left( 1 - \frac{k-2}{\|y\|^2} \right) y \quad \text{and} \quad E\left( \| \hat{\theta} - \theta \| \right) &lt; k, \; \forall \theta.
\]</span></p>
<p>Hence, when <span class="math inline">\(\|y\|^2\)</span> is small the shrinkage factor is more extreme.</p>
<p>For example, if <span class="math inline">\(k=10\)</span>, <span class="math inline">\(\|y\|^2=12\)</span>, then <span class="math inline">\(\hat{\theta} = (1/3) y\)</span>. Now we have the more intuitive result:</p>
<p><span class="math display">\[
P\left( \| \theta \| &gt; \| y \| \; | \; y \right) &lt; \frac{1}{2}
\]</span></p>
<p>Showing that default priors matter in high dimensions.</p>
</div>
</section>
<section id="ell_1-sparsity-1" class="level2" data-number="1.11">
<h2 data-number="1.11" class="anchored" data-anchor-id="ell_1-sparsity-1"><span class="header-section-number">1.11</span> <span class="math inline">\(\ell_1\)</span> Sparsity</h2>
</section>
<section id="ell_0-subset-selection-1" class="level2" data-number="1.12">
<h2 data-number="1.12" class="anchored" data-anchor-id="ell_0-subset-selection-1"><span class="header-section-number">1.12</span> <span class="math inline">\(\ell_0\)</span> Subset Selection</h2>
</section>
<section id="bayesain-model-selection-via-regularisation" class="level2" data-number="1.13">
<h2 data-number="1.13" class="anchored" data-anchor-id="bayesain-model-selection-via-regularisation"><span class="header-section-number">1.13</span> Bayesain Model Selection via Regularisation</h2>
<p>From Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian <span class="math inline">\(\ell_0\)</span> regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.</p>
<p>In Bayesian approach, regularization requires the specification of a loss, denoted by <span class="math inline">\(\mathcal{L}\left(\beta\right)\)</span> and a penalty function, denoted by <span class="math inline">\(\phi_{\lambda}(\beta)\)</span>, where <span class="math inline">\(\lambda\)</span> is a global regularization parameter. From a Bayesian perspective, <span class="math inline">\(\mathcal{L}\left(\beta\right)\)</span> and <span class="math inline">\(\phi_{\lambda}(\beta)\)</span> correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form <span class="math display">\[
\underset{\beta \in R^p}{\mathrm{minimize}\quad}
\mathcal{L}\left(\beta\right) + \phi_{\lambda}(\beta) \; .
\]</span> Taking a probabilistic approach leads to a Bayesian hierarchical model <span class="math display">\[
p(y \mid \beta) \propto \exp\{-\mathcal{L}(\beta)\} \; , \quad p(\beta) \propto \exp\{ -\phi_{\lambda}(\beta) \} \ .
\]</span> The solution to the minimization problem estimated by regularization corresponds to the posterior mode, <span class="math inline">\(\hat{\beta} = \mathrm{ arg \; max}_\beta \; p( \beta|y)\)</span>, where <span class="math inline">\(p(\beta|y)\)</span> denotes the posterior distribution. Consider a normal mean problem with <span class="math display">\[
\label{eqn:linreg}
y = \theta+ e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \theta \le \infty \ .
\]</span> What prior <span class="math inline">\(p(\theta)\)</span> should we place on <span class="math inline">\(\theta\)</span> to be able to separate the “signal” <span class="math inline">\(\theta\)</span> from “noise” <span class="math inline">\(e\)</span>, when we know that there is a good chance that <span class="math inline">\(\theta\)</span> is sparse (i.e.&nbsp;equal to zero). In the multivariate case we have <span class="math inline">\(y_i = \theta_i + e_i\)</span> and sparseness is measured by the number of zeros in <span class="math inline">\(\theta = (\theta_1\ldots,\theta_p)\)</span>. The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where <span class="math display">\[
p(\theta_i \mid b) = 0.5b\exp(-|\theta|/b).
\]</span> We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior <span class="math display">\[
\log p(\theta \mid y, b) \propto ||y-\theta||_2^2 + \dfrac{2\sigma^2}{b}||\theta||_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span> the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to the small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
</section>
<section id="shrinkage-ell_2-norm" class="level2" data-number="1.14">
<h2 data-number="1.14" class="anchored" data-anchor-id="shrinkage-ell_2-norm"><span class="header-section-number">1.14</span> Shrinkage (<span class="math inline">\(\ell_2\)</span> Norm)</h2>
<p>We can estimate the risk bounds of <span class="math inline">\(\ell_2\)</span> Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. <span class="math display">\[
R(\theta,\hat \theta) = E_{y|\theta} \left [ \Vert \hat \theta - \theta \Vert^2 \right ] = \Vert \hat \theta - \theta \Vert^2 + E_{y|\theta} \left [ \Vert \hat \theta - \mathbb{E}(\hat \theta) \Vert^2 \right ]
\]</span></p>
<p>In a case of multiple parameters, the Stein bound is <span class="math display">\[
R(\theta,\hat \theta_{JS}) &lt; R(\theta,\hat \theta_{MLE}) \;\;\; \forall \theta \in \mathbb{R}^p, \;\;\; p \geq 3.
\]</span> In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with <span class="math inline">\(p=100\)</span> and <span class="math inline">\(n=100\)</span>, the risk of the MLE is <span class="math inline">\(R(\theta,\hat \theta_{MLE}) = 100\)</span> while the risk of the JS estimator is <span class="math inline">\(R(\theta,\hat \theta_{JS}) = 1.5\)</span>. The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.</p>
<p>JS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is <span class="math display">\[
\hat \theta_{ridge} = \left (  X^T X + \lambda I \right )^{-1} X^T y
\]</span> where <span class="math inline">\(\lambda\)</span> is the regularization parameter.</p>
</section>
<section id="sparsity-ell_1-norm" class="level2" data-number="1.15">
<h2 data-number="1.15" class="anchored" data-anchor-id="sparsity-ell_1-norm"><span class="header-section-number">1.15</span> Sparsity (<span class="math inline">\(\ell_1\)</span> Norm)</h2>
<p>High-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model <span class="math display">\[
y_i \mid \theta_i \sim N(\theta_i,1),~ 1\le i\le p, ~ \theta = (\theta_1,\ldots,\theta_p),
\]</span> where parameter <span class="math inline">\(\theta\)</span> lies in the ball <span class="math display">\[
||\theta||_{\ell_0} = \{\theta : \text{number of  }\theta_i \ne 0 \le p_n\}.
\]</span></p>
<p>Even threshholding can beat MLE, when the signal is sparse. The thresholding estimator is <span class="math display">\[
\hat \theta_{thr} = \left \{ \begin{array}{ll} \hat \theta_i &amp; \mbox{if} \; \hat \theta_i &gt; \sqrt{2 \ln p} \\ 0 &amp; \mbox{otherwise} \end{array} \right .
\]</span></p>
<p>Sparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model <span class="math inline">\(( y_i | \theta_i ) \sim N( \theta_i,1)\)</span>. We wish to provide an estimator <span class="math inline">\(\hat y_{hs}\)</span> for the vector of normal means <span class="math inline">\(\theta = ( \theta_1, \ldots , \theta_p )\)</span>. Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when <span class="math inline">\(p_n\)</span>, denoting the number of non-zero parameter values, and for <span class="math inline">\(\theta \in l_0 [ p_n]\)</span>, which denotes the set <span class="math inline">\(\# ( \theta_i \neq 0 ) \leq p_n\)</span> where <span class="math inline">\(p_n = o(n)\)</span> where <span class="math inline">\(p_n \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs <span class="math inline">\((x_1,y_1),\ldots, (x_n,y_n)\)</span>.</p>
<p>The model is then used to predict the output <span class="math inline">\(y\)</span> for new inputs <span class="math inline">\(x\)</span>. The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.</p>
<!-- 

There are many ways to build a predictive rule $f(x)$ that estimates the conditional mean of the output y, given input x. Here are some of the most common approaches:

**1. Linear Regression:**

- This is a simple and widely used method that assumes a linear relationship between the input and output variables. The model is represented as:

$$
y = F(x) = \beta_0 + \beta_1x + \epsilon
$$

where $\beta_0$ and $\beta_1$ are the intercept and slope coefficients, respectively, and $\epsilon$ is the error term. The coefficients are estimated by minimizing the squared error between the predicted and actual values of $$y$.

**Advantages:**

- Simple to interpret and implement.
- Efficient for large datasets.

**Disadvantages:**

- Assumes a linear relationship between the input and output variables, which might not be true for all datasets.
- Sensitive to outliers.

**2. Polynomial Regression:**

- This is an extension of linear regression that allows for non-linear relationships between the input and output variables. The model is represented as:

$$
y = F(x) = \beta_0 + \beta_1x + \beta_2x^2 + ... + \beta_k x_k + \epsilon
$$

where k is the degree of the polynomial. The coefficients are estimated by minimizing the squared error between the predicted and actual values of y.

**Advantages:**

- More flexible than linear regression and can capture non-linear relationships.

**Disadvantages:**

- Can be prone to overfitting, especially for high-degree polynomials.
- More complex to interpret than linear regression.

**3. Support Vector Regression (SVR):**

- This is a non-linear regression method that uses kernel functions to map the input data to a higher-dimensional space. The model is represented as:

$$
y = F(x) = \sum \alpha_i K(x, x_i) + b
$$

where $\alpha_i$ are the Lagrange multipliers, $K(x, x_i)$ is the kernel function, and b is the bias term. The coefficients $\alpha_i$ and b are estimated by minimizing a loss function that penalizes both large errors and model complexity.

**Advantages:**

- Can capture non-linear relationships without overfitting.
- Robust to outliers.

**Disadvantages:**

- Can be computationally expensive for large datasets.
- Not as easy to interpret as linear regression.

**4. Random Forest Regression:**

- This is an ensemble method that combines the predictions of multiple decision trees. Each decision tree is built on a random subset of the data and makes predictions based on the input features. The final prediction is the average of the predictions from all trees.

**Advantages:**

- Can capture complex relationships between the input and output variables.
- Robust to outliers.

**Disadvantages:**

- Can be computationally expensive to train.
- Not as easy to interpret as individual decision trees.

**5. Neural Networks:**

- These are powerful models that can capture complex relationships between the input and output variables. They consist of multiple layers of interconnected nodes, which learn to process information and make predictions.

**Advantages:**

- Can capture complex relationships that other methods might miss.
- Highly flexible and can be applied to a wide range of problems.

**Disadvantages:**

- Can be prone to overfitting if not properly trained.
- Difficult to interpret and understand how they make predictions.

**Choosing the best method depends on several factors**:

- The size and nature of your dataset.
- The complexity of the relationship between the input and output variables.
- The desired level of interpretability.
- The available computational resources.

It is important to experiment with different methods and compare their performance on your specific dataset before choosing the best model for your task. -->
</section>
<section id="lasso" class="level2" data-number="1.16">
<h2 data-number="1.16" class="anchored" data-anchor-id="lasso"><span class="header-section-number">1.16</span> LASSO</h2>
<p>The Laplace distribution can be represented as scale mixture of Normal distribution<span class="citation" data-cites="andrews1974scale">(<a href="references.html#ref-andrews1974scale" role="doc-biblioref">Andrews and Mallows 1974</a>)</span> <span class="math display">\[
\begin{aligned}
\theta_i \mid \sigma^2,\tau \sim &amp;N(0,\tau^2\sigma^2)\\
\tau^2  \mid \alpha \sim &amp;\exp (\alpha^2/2)\\
\sigma^2 \sim &amp; \pi(\sigma^2).\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau\)</span> <span class="math display">\[
p(\theta_i\mid \sigma^2,\alpha) =  \int_{0}^{\infty} \dfrac{1}{\sqrt{2\pi \tau}}\exp\left(-\dfrac{\theta_i^2}{2\sigma^2\tau}\right)\dfrac{\alpha^2}{2}\exp\left(-\dfrac{\alpha^2\tau}{2}\right)d\tau = \dfrac{\alpha}{2\sigma}\exp(-\alpha/\sigma|\theta_i|).
\]</span> Thus it is a Laplace distribution with location 0 and scale <span class="math inline">\(\alpha/\sigma\)</span>. Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from <span class="math inline">\(\theta \mid a,y\)</span> and <span class="math inline">\(b\mid \theta,y\)</span> to estimate joint distribution over <span class="math inline">\((\hat \theta, \hat b)\)</span>. Thus, we so not need to apply cross-validation to find optimal value of <span class="math inline">\(b\)</span>, the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.</p>
<p>When prior is Normal <span class="math inline">\(\theta_i \sim N(0,\sigma_{\theta}^2)\)</span>, the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional <span class="math inline">\(\lambda\propto 1/\sigma_{\theta}^2\)</span>.</p>
</section>
<section id="subset-selection-ell_0-norm" class="level2" data-number="1.17">
<h2 data-number="1.17" class="anchored" data-anchor-id="subset-selection-ell_0-norm"><span class="header-section-number">1.17</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</h2>
<p>Skike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)</p>
</section>
<section id="bridge-ell_alpha" class="level2" data-number="1.18">
<h2 data-number="1.18" class="anchored" data-anchor-id="bridge-ell_alpha"><span class="header-section-number">1.18</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</h2>
<p>This is a non-convex penalty when <span class="math inline">\(0&lt;\alpha&lt;1\)</span>. It is an NP-hard problem. When <span class="math inline">\(\alpha=1\)</span> or <span class="math inline">\(\alpha=2\)</span> we have optimisation problems that are “solvable” for large scale cases. However, when <span class="math inline">\(0\le \alpha&lt;1\)</span> the current optimisation algorithms won’t work.</p>
<p>The real killer is that you can use data to estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> (let the data speak for itself) <span class="citation" data-cites="box1992bayesian">Box and Tiao (<a href="references.html#ref-box1992bayesian" role="doc-biblioref">1992</a>)</span>.</p>
<p>Bayesian analogue of the bridge estimator in regression is <span class="math display">\[
y = X\beta + \epsilon
\]</span></p>
<p>for some unknown vector <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)'\)</span>. Given choices of <span class="math inline">\(\alpha \in (0,1]\)</span> and <span class="math inline">\(\nu \in \mathbb{R}^+\)</span>, the bridge estimator <span class="math inline">\(\hat{\beta}\)</span> is the minimizer of</p>
<p><span id="eq-qy"><span class="math display">\[
Q_y(\beta) = \frac{1}{2} \|y - X\beta\|^2 + \nu \sum_{j=1}^p |\beta_j|^\alpha.
\tag{1.3}\]</span></span></p>
<p>This bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the <span class="math inline">\(\ell_1\)</span> (or lasso) penalty at the other. An early reference to this class of models can be found in <span class="citation" data-cites="frank1993statistical">Frank and Friedman (<a href="references.html#ref-frank1993statistical" role="doc-biblioref">1993</a>)</span>, with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (<span class="citation" data-cites="huang2008asymptotic">Huang, Horowitz, and Ma (<a href="references.html#ref-huang2008asymptotic" role="doc-biblioref">2008</a>)</span>, <span class="citation" data-cites="mazumder2011sparsenet">Mazumder, and and Hastie (<a href="references.html#ref-mazumder2011sparsenet" role="doc-biblioref">2011</a>)</span>).</p>
<p>Bridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat</p>
<p><span class="math display">\[
p(\beta \mid y) \propto \exp\{-Q_y(\beta)\}
\]</span></p>
<p>as a posterior distribution having the minimizer of <a href="#eq-qy" class="quarto-xref">Equation&nbsp;<span>1.3</span></a> as its global mode. This posterior arises in assuming a Gaussian likelihood for <span class="math inline">\(y\)</span>, along with a prior for <span class="math inline">\(\beta\)</span> that decomposes as a product of independent exponential-power priors (<span class="citation" data-cites="box1992bayesian">Box and Tiao (<a href="references.html#ref-box1992bayesian" role="doc-biblioref">1992</a>)</span>):</p>
<p><span class="math display">\[
p(\beta \mid \alpha, \nu) \propto \prod_{j=1}^p \exp\left(-\left|\frac{\beta_j}{\tau}\right|^\alpha\right), \quad \tau = \nu^{-1/\alpha}. \tag{2}
\]</span></p>
<p>Rather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for <span class="math inline">\(\beta\)</span> as its stationary distribution.</p>
<section id="spike-and-slab-prior" class="level3" data-number="1.18.1">
<h3 data-number="1.18.1" class="anchored" data-anchor-id="spike-and-slab-prior"><span class="header-section-number">1.18.1</span> Spike-and-Slab Prior</h3>
<p>Our Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem <span class="math display">\[
y = \beta_1x_1+\ldots+\beta_px_p + e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \beta_i \le \infty \ .
\]</span> We would like to solve the problem of variable selections, i.e.&nbsp;identify which input variables <span class="math inline">\(x_i\)</span> to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.</p>
<p>To perform a model selection, we would like to specify a prior distribution <span class="math inline">\(p\left(\beta\right)\)</span>, which imposes a sparsity assumption on <span class="math inline">\(\beta\)</span>, where only a small portion of all <span class="math inline">\(\beta_i\)</span>’s are non-zero. In other words, <span class="math inline">\(\|\beta\|_0 = k \ll p\)</span>, where <span class="math inline">\(\|\beta\|_0 \defeq \#\{i : \beta_i\neq0\}\)</span>, the cardinality of the support of <span class="math inline">\(\beta\)</span>, also known as the <span class="math inline">\(\ell_0\)</span> (pseudo)norm of <span class="math inline">\(\beta\)</span>. A multivariate Gaussian prior (<span class="math inline">\(l_2\)</span> norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for <span class="math inline">\(\beta\)</span> can be constructed to impose sparsity include the double exponential (lasso).</p>
<p>Under spike-and-slab, each <span class="math inline">\(\beta_i\)</span> exchangeably follows a mixture prior consisting of <span class="math inline">\(\delta_0\)</span>, a point mass at <span class="math inline">\(0\)</span>, and a Gaussian distribution centered at zero. Hence we write,</p>
<p><span class="math display">\[
\label{eqn:ss}
\beta_i | \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N\left(0, \sigma_\beta^2\right) \ .
\]</span> Here <span class="math inline">\(\theta\in \left(0, 1\right)\)</span> controls the overall sparsity in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_\beta^2\)</span> accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.</p>
<p>A useful re-parameterization, the parameters <span class="math inline">\(\beta\)</span> is given by two independent random variable vectors <span class="math inline">\(\gamma = \left(\gamma_1, \ldots, \gamma_p\right)'\)</span> and <span class="math inline">\(\alpha = \left(\alpha_1, \ldots, \alpha_p\right)'\)</span> such that <span class="math inline">\(\beta_i  =  \gamma_i\alpha_i\)</span>, with probabilistic structure <span class="math display">\[
\label{eq:bg}
\begin{array}{rcl}
\gamma_i\mid\theta &amp; \sim &amp; \text{Bernoulli}(\theta) \ ;
\\
\alpha_i \mid \sigma_\beta^2 &amp;\sim &amp; N\left(0, \sigma_\beta^2\right) \ .
\\
\end{array}
\]</span> Since <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\alpha_i\)</span> are independent, the joint prior density becomes <span class="math display">\[
p\left(\gamma_i, \alpha_i \mid \theta, \sigma_\beta^2\right) =
\theta^{\gamma_i}\left(1-\theta\right)^{1-\gamma_i}\frac{1}{\sqrt{2\pi}\sigma_\beta}\exp\left\{-\frac{\alpha_i^2}{2\sigma_\beta^2}\right\}
\ , \ \ \ \text{for } 1\leq i\leq p \ .
\]</span> The indicator <span class="math inline">\(\gamma_i\in \{0, 1\}\)</span> can be viewed as a dummy variable to indicate whether <span class="math inline">\(\beta_i\)</span> is included in the model.</p>
<p>Let <span class="math inline">\(S = \{i: \gamma_i = 1\} \subseteq \{1, \ldots, p\}\)</span> be the “active set" of <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\|\gamma\|_0 = \sum\limits_{i = 1}^p\gamma_i\)</span> be its cardinality. The joint prior on the vector <span class="math inline">\(\{\gamma, \alpha\}\)</span> then factorizes as <span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right) &amp; = &amp; \prod\limits_{i = 1}^p p\left(\alpha_i, \gamma_i \mid \theta, \sigma_\beta^2\right) \\
&amp; = &amp;
\theta^{\|\gamma\|_0}
\left(1-\theta\right)^{p - \|\gamma\|_0}
\left(2\pi\sigma_\beta^2\right)^{-\frac p2}\exp\left\{-\frac1{2\sigma_\beta^2}\sum\limits_{i = 1}^p\alpha_i^2\right\} \ .
\end{array}
\]</span></p>
<p>Let <span class="math inline">\(X_\gamma \defeq \left[X_i\right]_{i \in S}\)</span> be the set of “active explanatory variables" and <span class="math inline">\(\alpha_\gamma \defeq \left(\alpha_i\right)'_{i \in S}\)</span> be their corresponding coefficients. We can write <span class="math inline">\(X\beta = X_\gamma \alpha_\gamma\)</span>. The likelihood can be expressed in terms of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\alpha\)</span> as <span class="math display">\[
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)
=
\left(2\pi\sigma_e^2\right)^{-\frac n2}
\exp\left\{
-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
\right\} \ .
\]</span></p>
<p>Under this re-parameterization by <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span>, the posterior is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y\right) &amp; \propto &amp;
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right)
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)\\
&amp; \propto &amp;
\exp\left\{-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
-\frac1{2\sigma_\beta^2}\left\|\alpha\right\|_2^2
-\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0
\right\} \ .
\end{array}
\]</span> Our goal then is to find the regularized maximum a posterior (MAP) estimator <span class="math display">\[
\arg\max\limits_{\gamma, \alpha}p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y \right) \ .
\]</span> By construction, the <span class="math inline">\(\gamma\)</span> <span class="math inline">\(\in\left\{0, 1\right\}^p\)</span> will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span> the regularized least squares objective function</p>
<p><span id="eq-obj:map"><span class="math display">\[
\min\limits_{\gamma, \alpha}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2
+ 2\sigma_e^2\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0 \ .
\tag{1.4}\]</span></span> This objective possesses several interesting properties:</p>
<ol type="1">
<li><p>The first term is essentially the least squares loss function.</p></li>
<li><p>The second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) <span class="math inline">\(\sigma_\beta^2/\sigma_e^2\)</span>. Smaller SNR will be more likely to shrink the estimates towards <span class="math inline">\(0\)</span>. If <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want <span class="math inline">\(\sigma_\beta \to \infty\)</span> or to be “sufficiently large" in order to avoid imposing harsh shrinkage to non-zero signals.</p></li>
<li><p>If we further assume that <span class="math inline">\(\theta &lt; \frac12\)</span>, meaning that the coefficients are known to be sparse <em>a priori</em>, then <span class="math inline">\(\log\left(\left(1-\theta\right) / \theta\right) &gt; 0\)</span>, and the third term can be seen as an <span class="math inline">\(\ell_0\)</span> regularization.</p></li>
</ol>
<p>Therefore, our Bayesian objective inference is connected to <span class="math inline">\(\ell_0\)</span>-regularized least squares, which we summarize in the following proposition.</p>
<p>(Spike-and-slab MAP &amp; <span class="math inline">\(\ell_0\)</span> regularization)</p>
<p>For some <span class="math inline">\(\lambda &gt; 0\)</span>, assuming <span class="math inline">\(\theta &lt; \frac12\)</span>, <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the Bayesian MAP estimate defined by <a href="#eq-obj:map" class="quarto-xref">Equation&nbsp;<span>1.4</span></a> is equivalent to the <span class="math inline">\(\ell_0\)</span> regularized least squares objective, for some <span class="math inline">\(\lambda &gt; 0\)</span>, <span id="eq-obj:l0"><span class="math display">\[
\min\limits_{\beta}
\frac12\left\|y - X\beta\right\|_2^2
+ \lambda
\left\|\beta\right\|_0 \ .
\tag{1.5}\]</span></span></p>
<p>First, assuming that <span class="math display">\[
\theta &lt; \frac12, \ \ \  \sigma_\beta^2 \gg \sigma_e^2, \ \ \  \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2 \to 0 \ ,
\]</span> gives us an objective function of the form <span id="eq-obj:vs"><span class="math display">\[
\min\limits_{\gamma, \alpha}
\frac12 \left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \lambda
\left\|\gamma\right\|_0,  \ \ \ \  \text{where } \lambda \defeq \sigma_e^2\log\left(\left(1-\theta\right) / \theta\right) &gt; 0 \ .
\tag{1.6}\]</span></span></p>
<p>Equation <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> can be seen as a variable selection version of equation <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a>. The interesting fact is that <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a> and <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> are equivalent. To show this, we need only to check that the optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a> corresponds to a feasible solution to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> and vice versa. This is explained as follows.</p>
<p>On the one hand, assuming <span class="math inline">\(\hat\beta\)</span> is an optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a>, then we can correspondingly define <span class="math inline">\(\hat\gamma_i \defeq I\left\{\hat\beta_i \neq 0\right\}\)</span>, <span class="math inline">\(\hat\alpha_i \defeq \hat\beta_i\)</span>, such that <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is feasible to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> and gives the same objective value as <span class="math inline">\(\hat\beta\)</span> gives <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a>.</p>
<p>On the other hand, assuming <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is optimal to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a>, implies that we must have all of the elements in <span class="math inline">\(\hat\alpha_\gamma\)</span> should be non-zero, otherwise a new <span class="math inline">\(\tilde\gamma_i \defeq I\left\{\hat\alpha_i \neq 0\right\}\)</span> will give a lower objective value of <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a>. As a result, if we define <span class="math inline">\(\hat\beta_i \defeq \hat\gamma_i\hat\alpha_i\)</span>, <span class="math inline">\(\hat\beta\)</span> will be feasible to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.5</span></a> and gives the same objective value as <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> gives <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.6</span></a>.</p>
</section>
</section>
<section id="horseshoe-prior" class="level2" data-number="1.19">
<h2 data-number="1.19" class="anchored" data-anchor-id="horseshoe-prior"><span class="header-section-number">1.19</span> Horseshoe Prior</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//horseshoe.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>The sparse normal means problem is concerned with inference for the parameter vector <span class="math inline">\(\theta = ( \theta_1 , \ldots , \theta_p )\)</span> where we observe data <span class="math inline">\(y_i = \theta_i + \epsilon_i\)</span> where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by <span class="math inline">\(\pi_{HS}\)</span>, (a.k.a. log-prior, <span class="math inline">\(- \log p_{HS}\)</span>)? Lasso simply uses an <span class="math inline">\(L^1\)</span>-norm, <span class="math inline">\(\sum_{i=1}^K | \theta_i |\)</span>, as opposed to the horseshoe prior which (essentially) uses the penalty <span class="math display">\[
\pi_{HS} ( \theta_i | \tau ) = - \log p_{HS} ( \theta_i | \tau ) = - \log \log \left ( 1 + \frac{2 \tau^2}{\theta_i^2} \right ) .
\]</span> The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in <strong>both</strong> the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.</p>
<p>The horseshoe <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span> is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.</p>
<p>We introduce the horseshoe in the context of the normal means model, which is given by <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>. The horseshoe prior is given by <span class="math display">\[\begin{align*}
\beta_i &amp;\sim \mathcal{N}(0, \sigma^2 \tau^2 \lambda_i^2)\\
\lambda_i &amp;\sim C^+(0, 1),
\end{align*}\]</span> where <span class="math inline">\(C^+\)</span> denotes the half-Cauchy distribution. Optionally, hyperpriors on <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\sigma\)</span> may be specified, as is described further in the next two sections.</p>
<p>To illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for <span class="math inline">\(\beta_i\)</span> as a function of <span class="math inline">\(y_i\)</span> for three different values of <span class="math inline">\(\tau\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(horseshoe)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>tau.values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.005</span>, <span class="fl">0.05</span>, <span class="fl">0.5</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>y.values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">tau =</span> <span class="fu">rep</span>(tau.values, <span class="at">each =</span> <span class="fu">length</span>(y.values)),</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">rep</span>(y.values, <span class="dv">3</span>),</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">post.mean =</span> <span class="fu">c</span>(<span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">1</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">2</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">3</span>], <span class="at">Sigma2=</span><span class="dv">1</span>)) )</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> post.mean, <span class="at">group =</span> tau, <span class="at">color =</span> <span class="fu">factor</span>(tau))) <span class="sc">+</span> </span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette=</span><span class="st">"Dark2"</span>) <span class="sc">+</span> </span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">colour =</span> <span class="st">"grey"</span>) <span class="sc">+</span> </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"Tau"</span>) <span class="sc">+</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Horseshoe posterior mean for three values of tau"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Smaller values of <span class="math inline">\(\tau\)</span> lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to <span class="math inline">\(\sqrt{2\sigma^2\log(1/\tau)}\)</span> are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of <span class="math inline">\(\tau\)</span> is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.</p>
</section>
<section id="the-normal-means-problem" class="level2" data-number="1.20">
<h2 data-number="1.20" class="anchored" data-anchor-id="the-normal-means-problem"><span class="header-section-number">1.20</span> The normal means problem</h2>
<p>The normal means model is: <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>First, we will be computing the posterior mean only, with known variance <span class="math inline">\(\sigma^2\)</span> The function <code>HS.post.mean</code> computes the posterior mean of <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span>. It does not require MCMC and is suitable when only an estimate of the vector <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span> is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for <span class="math inline">\(\sigma^2\)</span> is available, please see below for the function <code>HS.normal.means</code>.</p>
<p>The function <code>HS.post.mean</code> requires the observed outcomes, a value for <span class="math inline">\(\tau\)</span> and a value for <span class="math inline">\(\sigma\)</span>. Ideally, <span class="math inline">\(\tau\)</span> should be equal to the proportion of nonzero <span class="math inline">\(\beta_i\)</span>’s. Typically, this proportion is unknown, in which case it is recommended to use the function <code>HS.MMLE</code> to find the marginal maximum likelihood estimator for <span class="math inline">\(\tau\)</span>.</p>
<p>As an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 <span class="math inline">\(\beta_i\)</span>’s are equal to five and the remaining <span class="math inline">\(\beta_i\)</span>’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">index =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>                 truth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">5</span>, <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">40</span>)),</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>                 y <span class="ot">&lt;-</span> truth <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">50</span>) <span class="co">#observations</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We estimate <span class="math inline">\(\tau\)</span> using the MMLE, using the known variance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>(tau.est <span class="ot">&lt;-</span> <span class="fu">HS.MMLE</span>(df<span class="sc">$</span>y, <span class="at">Sigma2 =</span> <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.96</code></pre>
</div>
</div>
<p>We then use this estimate of <span class="math inline">\(\tau\)</span> to find the posterior mean, and add it to the plot in red.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>post.mean <span class="ot">&lt;-</span> <span class="fu">HS.post.mean</span>(df<span class="sc">$</span>y, tau.est, <span class="dv">1</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean <span class="ot">&lt;-</span> post.mean</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>If the posterior variance is of interest, the function <code>HS.post.var</code> can be used. It takes the same arguments as <code>HS.post.mean</code>.</p>
<section id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2" class="level3" data-number="1.20.1">
<h3 data-number="1.20.1" class="anchored" data-anchor-id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2"><span class="header-section-number">1.20.1</span> Posterior mean, credible intervals and variable selection, possibly unknown <span class="math inline">\(\sigma^2\)</span></h3>
<p>The function <code>HS.normal.means</code> is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (<span class="math inline">\(\beta_i\)</span>’s, <span class="math inline">\(\tau\)</span>, <span class="math inline">\(\sigma\)</span>), the posterior median for the <span class="math inline">\(\beta_i\)</span>’s, and credible intervals for the <span class="math inline">\(\beta_i\)</span>’s.</p>
<p>The key choices to make are:</p>
<ul>
<li>How to handle <span class="math inline">\(\tau\)</span>. The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to <span class="math inline">\([1/n, 1]\)</span>). See the manual for other options.</li>
<li>How to handle <span class="math inline">\(\sigma\)</span>. The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.</li>
</ul>
<p>Other options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).</p>
<p>Let’s continue the example from the previous section. We first create a ‘horseshoe object’.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>hs.object <span class="ot">&lt;-</span> <span class="fu">HS.normal.means</span>(df<span class="sc">$</span>y, <span class="at">method.tau =</span> <span class="st">"truncatedCauchy"</span>, <span class="at">method.sigma =</span> <span class="st">"Jeffreys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We extract the posterior mean of the <span class="math inline">\(\beta_i\)</span>’s and plot them in red.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean.full <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>BetaHat</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We plot the marginal credible intervals (and remove the observations from the plot for clarity).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>lower.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>LeftCI</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>upper.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>RightCI</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI), <span class="at">width =</span> .<span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Red = estimates with 95% credible intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Finally, we perform variable selection using <code>HS.var.select</code>. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.CI <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)), </span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)),</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as <span class="math inline">\(c_iy_i\)</span> where <span class="math inline">\(y_i\)</span> is the observation and <span class="math inline">\(c_i\)</span> some number between 0 and 1. A variable is selected if <span class="math inline">\(c_i \geq c\)</span> for some user-selected threshold <span class="math inline">\(c\)</span> (default is <span class="math inline">\(c = 0.5\)</span>). In the example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.thres <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"threshold"</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)), </span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)),</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="polya-gamma" class="level2" data-number="1.21">
<h2 data-number="1.21" class="anchored" data-anchor-id="polya-gamma"><span class="header-section-number">1.21</span> Polya-Gamma</h2>
<p>Bayesian inference for logistic regression has long been recognized as a computationally challenging problem due to the analytically inconvenient form of the binomial likelihood function<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. While the probit model enjoys simple latent-variable methods for posterior sampling, the logistic model has historically required more complex approaches involving multiple layers of auxiliary variables or approximations<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The breakthrough work of Polson, Scott, and Windle (2013) introduced a revolutionary data-augmentation strategy using a novel class of distributions called Pólya-Gamma distributions, which enables simple and exact Gibbs sampling for Bayesian logistic regression<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>This methodology represents a significant advancement in Bayesian computation, providing a direct analog to the Albert and Chib (1993) method for probit regression while maintaining both exactness and simplicity<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The approach has proven particularly valuable for complex hierarchical models where traditional Metropolis-Hastings samplers are difficult to tune and implement effectively<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Innovation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Pólya-Gamma methodology provides exact Gibbs sampling for Bayesian logistic regression, eliminating the need for complex Metropolis-Hastings tuning while maintaining theoretical guarantees.</p>
</div>
</div>
</section>
<section id="the-pólya-gamma-distribution" class="level2" data-number="1.22">
<h2 data-number="1.22" class="anchored" data-anchor-id="the-pólya-gamma-distribution"><span class="header-section-number">1.22</span> The Pólya-Gamma Distribution</h2>
<p>The Pólya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. A random variable X follows a Pólya-Gamma distribution with parameters b &gt; 0 and c ∈ ℝ if:</p>
<p><span class="math display">\[X \stackrel{d}{=} \frac{1}{2\pi^2} \sum_{k=1}^{\infty} \frac{g_k}{(k-1/2)^2 + c^2/(4\pi^2)}\]</span></p>
<p>where <span class="math inline">\(g_k \sim \text{Ga}(b,1)\)</span> are independent gamma random variables, and <span class="math inline">\(\stackrel{d}{=}\)</span> indicates equality in distribution<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>The Pólya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:</p>
<ol type="1">
<li><p><strong>Laplace Transform</strong>: For <span class="math inline">\(\omega \sim \text{PG}(b,0)\)</span>, the Laplace transform is <span class="math inline">\(E\{\exp(-\omega t)\} = \cosh^{-b}(\sqrt{t}/2)\)</span><span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span></p></li>
<li><p><strong>Exponential Tilting</strong>: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:</p></li>
</ol>
<p><span class="math display">\[p(x|b,c) = \frac{\exp(-c^2x/2)p(x|b,0)}{E[\exp(-c^2\omega/2)]}\]</span></p>
<p>where the expectation is taken with respect to PG(b,0)<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span></p>
<ol start="3" type="1">
<li><p><strong>Convolution Property</strong>: The family is closed under convolution for random variates with the same tilting parameter<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span></p></li>
<li><p><strong>Known Moments</strong>: All finite moments are available in closed form, with the expectation given by:</p></li>
</ol>
<p><span class="math display">\[E(\omega) = \frac{b}{2c}\tanh(c/2) = \frac{b}{2c}\frac{e^c-1}{1+e^c}\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Computational Advantage
</div>
</div>
<div class="callout-body-container callout-body">
<p>The known moments and convolution properties make the Pólya-Gamma distribution computationally tractable and theoretically well-behaved.</p>
</div>
</div>
<section id="the-data-augmentation-strategy" class="level3" data-number="1.22.1">
<h3 data-number="1.22.1" class="anchored" data-anchor-id="the-data-augmentation-strategy"><span class="header-section-number">1.22.1</span> The Data-Augmentation Strategy</h3>
<p>The core of the Pólya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The key theorem states:</p>
<div class="theorem">
<p><strong>Theorem 1</strong>: For b &gt; 0 and a ∈ ℝ, the following integral identity holds:</p>
<p><span class="math display">\[\frac{(e^\psi)^a}{(1+e^\psi)^b} = 2^{-b}e^{\kappa\psi} \int_0^{\infty} e^{-\omega\psi^2/2} p(\omega) d\omega\]</span></p>
<p>where <span class="math inline">\(\kappa = a - b/2\)</span>, and <span class="math inline">\(p(\omega)\)</span> is the density of <span class="math inline">\(\omega \sim \text{PG}(b,0)\)</span><span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>Moreover, the conditional distribution <span class="math inline">\(p(\omega|\psi)\)</span> is also in the Pólya-Gamma class: <span class="math inline">\((\omega|\psi) \sim \text{PG}(b,\psi)\)</span><span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
</div>
</section>
<section id="gibbs-sampling-algorithm" class="level3" data-number="1.22.2">
<h3 data-number="1.22.2" class="anchored" data-anchor-id="gibbs-sampling-algorithm"><span class="header-section-number">1.22.2</span> Gibbs Sampling Algorithm</h3>
<p>This integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. For a dataset with observations <span class="math inline">\(y_i \sim \text{Binom}(n_i, 1/(1+e^{-\psi_i}))\)</span> where <span class="math inline">\(\psi_i = x_i^T\beta\)</span>, and a Gaussian prior <span class="math inline">\(\beta \sim N(b,B)\)</span>, the algorithm iterates:</p>
<ol type="1">
<li><strong>Sample auxiliary variables</strong>: <span class="math inline">\((\omega_i|\beta) \sim \text{PG}(n_i, x_i^T\beta)\)</span> for each observation</li>
<li><strong>Sample parameters</strong>: <span class="math inline">\((\beta|y,\omega) \sim N(m_\omega, V_\omega)\)</span> where:
<ul>
<li><span class="math inline">\(V_\omega = (X^T\Omega X + B^{-1})^{-1}\)</span></li>
<li><span class="math inline">\(m_\omega = V_\omega(X^T\kappa + B^{-1}b)\)</span></li>
<li><span class="math inline">\(\kappa = (y_1-n_1/2, \ldots, y_n-n_n/2)\)</span></li>
<li><span class="math inline">\(\Omega = \text{diag}(\omega_1, \ldots, \omega_n)\)</span></li>
</ul></li>
</ol>
<p>This approach requires only Gaussian draws for the main parameters and Pólya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
</section>
<section id="the-pg1z-sampler" class="level3" data-number="1.22.3">
<h3 data-number="1.22.3" class="anchored" data-anchor-id="the-pg1z-sampler"><span class="header-section-number">1.22.3</span> The PG(1,z) Sampler</h3>
<p>The practical success of the Pólya-Gamma method depends on efficient simulation of Pólya-Gamma random variables<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (1986)<span class="citation" data-cites="devroye1986nonuniform">(<a href="references.html#ref-devroye1986nonuniform" role="doc-biblioref">Devroye 1986</a>)</span>. For the fundamental PG(1,c) case, the sampler:</p>
<ul>
<li>Uses exponential and inverse-Gaussian draws as proposals</li>
<li>Achieves acceptance probability uniformly bounded below at 0.99919</li>
<li>Requires no tuning for optimal performance</li>
<li>Evaluates acceptance using iterative partial sums</li>
</ul>
</section>
<section id="general-pgbz-sampling" class="level3" data-number="1.22.4">
<h3 data-number="1.22.4" class="anchored" data-anchor-id="general-pgbz-sampling"><span class="header-section-number">1.22.4</span> General PG(b,z) Sampling</h3>
<p>For integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
</section>
</section>
<section id="implementation-with-bayeslogit-package" class="level2" data-number="1.23">
<h2 data-number="1.23" class="anchored" data-anchor-id="implementation-with-bayeslogit-package"><span class="header-section-number">1.23</span> Implementation with BayesLogit Package</h2>
<section id="package-overview" class="level3" data-number="1.23.1">
<h3 data-number="1.23.1" class="anchored" data-anchor-id="package-overview"><span class="header-section-number">1.23.1</span> Package Overview</h3>
<p>The <code>BayesLogit</code> package provides efficient tools for sampling from the Pólya-Gamma distribution<span class="citation" data-cites="windle2023bayeslogit">(<a href="references.html#ref-windle2023bayeslogit" role="doc-biblioref">Windle 2023</a>)</span>. The current version (2.1) focuses on core functionality: sampling from the Pólya-Gamma distribution through the <code>rpg()</code> function and its variants<span class="citation" data-cites="windle2023bayeslogit">(<a href="references.html#ref-windle2023bayeslogit" role="doc-biblioref">Windle 2023</a>)</span>.</p>
</section>
<section id="core-functions" class="level3" data-number="1.23.2">
<h3 data-number="1.23.2" class="anchored" data-anchor-id="core-functions"><span class="header-section-number">1.23.2</span> Core Functions</h3>
<p>The package offers several sampling methods:</p>
<ul>
<li><code>rpg()</code>: Main function that automatically selects the best method</li>
<li><code>rpg.devroye()</code>: Devroye-like method for integer h values</li>
<li><code>rpg.gamma()</code>: Sum of gammas method (slower but works for all parameters)</li>
<li><code>rpg.sp()</code>: Saddlepoint approximation method</li>
</ul>
</section>
<section id="installation-and-basic-usage" class="level3" data-number="1.23.3">
<h3 data-number="1.23.3" class="anchored" data-anchor-id="installation-and-basic-usage"><span class="header-section-number">1.23.3</span> Installation and Basic Usage</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install from CRAN</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">"BayesLogit"</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BayesLogit)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic usage examples</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample from PG(1, 0)</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>samples1 <span class="ot">&lt;-</span> <span class="fu">rpg</span>(<span class="dv">1000</span>, <span class="at">h=</span><span class="dv">1</span>, <span class="at">z=</span><span class="dv">0</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample with tilting parameter</span></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>samples2 <span class="ot">&lt;-</span> <span class="fu">rpg</span>(<span class="dv">1000</span>, <span class="at">h=</span><span class="dv">1</span>, <span class="at">z=</span><span class="fl">2.5</span>)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple shape parameters</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>h_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>z_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>samples3 <span class="ot">&lt;-</span> <span class="fu">rpg</span>(<span class="dv">100</span>, <span class="at">h=</span>h_values, <span class="at">z=</span>z_values)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="implementing-bayesian-logistic-regression" class="level3" data-number="1.23.4">
<h3 data-number="1.23.4" class="anchored" data-anchor-id="implementing-bayesian-logistic-regression"><span class="header-section-number">1.23.4</span> Implementing Bayesian Logistic Regression</h3>
<p>Here’s a complete implementation of Bayesian logistic regression using the Pólya-Gamma methodology:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bayesian Logistic Regression with Pólya-Gamma Data Augmentation</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>bayesian_logit_pg <span class="ot">&lt;-</span> <span class="cf">function</span>(y, X, <span class="at">n_iter=</span><span class="dv">5000</span>, <span class="at">burn_in=</span><span class="dv">1000</span>) {</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prior specification (weakly informative)</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>  beta_prior_mean <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>  beta_prior_prec <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fl">0.01</span>, p)  <span class="co"># Precision matrix</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Storage for samples</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>  beta_samples <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n_iter, p)</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>  omega_samples <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n_iter, n)</span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize</span></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Sample omega (auxiliary variables)</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    psi <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>    omega <span class="ot">&lt;-</span> <span class="fu">rpg</span>(n, <span class="at">h=</span><span class="dv">1</span>, <span class="at">z=</span>psi)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Sample beta (regression coefficients)</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior precision and mean</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    V_omega <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> <span class="fu">diag</span>(omega) <span class="sc">%*%</span> X <span class="sc">+</span> beta_prior_prec)</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>    kappa <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fl">0.5</span></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    m_omega <span class="ot">&lt;-</span> V_omega <span class="sc">%*%</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> kappa <span class="sc">+</span> beta_prior_prec <span class="sc">%*%</span> beta_prior_mean)</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample from multivariate normal</span></span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    beta <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, m_omega, V_omega)</span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store samples</span></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    beta_samples[iter, ] <span class="ot">&lt;-</span> beta</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>    omega_samples[iter, ] <span class="ot">&lt;-</span> omega</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Return samples after burn-in</span></span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">beta =</span> beta_samples[(burn_in<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n_iter, ],</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">omega =</span> omega_samples[(burn_in<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n_iter, ],</span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_samples =</span> n_iter <span class="sc">-</span> burn_in</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage with simulated data</span></span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span><span class="dv">2</span>), n, <span class="dv">2</span>))  <span class="co"># Intercept + 2 predictors</span></span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">1.2</span>, <span class="sc">-</span><span class="fl">0.8</span>)</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a>logits <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true</span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>logits))</span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, probs)</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">bayesian_logit_pg</span>(y, X, <span class="at">n_iter=</span><span class="dv">3000</span>, <span class="at">burn_in=</span><span class="dv">500</span>)</span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior summaries</span></span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a>posterior_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(results<span class="sc">$</span>beta)</span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>posterior_sds <span class="ot">&lt;-</span> <span class="fu">apply</span>(results<span class="sc">$</span>beta, <span class="dv">2</span>, sd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Computational Advantages</strong></p>
<p>Extensive benchmarking studies demonstrate the superior performance of the Pólya-Gamma method across various scenarios<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>:</p>
<ol type="1">
<li><strong>Simple logistic models</strong>: Competitive with well-tuned Metropolis-Hastings samplers</li>
<li><strong>Hierarchical models</strong>: Significantly outperforms alternative methods</li>
<li><strong>Mixed models</strong>: Provides substantial efficiency gains over traditional approaches</li>
<li><strong>Spatial models</strong>: Shows dramatic improvements for Gaussian process spatial models</li>
</ol>
<p><strong>Theoretical Guarantees</strong></p>
<p>The Pólya-Gamma Gibbs sampler enjoys strong theoretical properties<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>:</p>
<ul>
<li><strong>Uniform ergodicity</strong>: Proven by Choi and Hobert (2013), guaranteeing convergence and central limit theorems for Monte Carlo averages<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span></li>
<li><strong>No tuning required</strong>: Unlike Metropolis-Hastings methods, the sampler requires no manual tuning</li>
<li><strong>Exact sampling</strong>: Produces draws from the correct posterior distribution without approximation</li>
</ul>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The theoretical guarantees hold under standard regularity conditions, and the method requires proper prior specification for optimal performance.</p>
</div>
</div>
<p><strong>Beyond Binary Logistic Regression</strong></p>
<p>The Pólya-Gamma methodology extends naturally to various related models<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>:</p>
<ol type="1">
<li>Negative binomial regression: Direct application using the same data-augmentation scheme</li>
<li>Multinomial logistic models: Extended through partial difference of random utility models<span class="citation" data-cites="windle2014sampling">(<a href="references.html#ref-windle2014sampling" role="doc-biblioref">Windle, Polson, and Scott 2014</a>)</span></li>
<li>Mixed effects models: Seamless incorporation of random effects structures</li>
<li>Spatial models: Efficient inference for spatial count data models</li>
</ol>
</section>
<section id="modern-applications" class="level3" data-number="1.23.5">
<h3 data-number="1.23.5" class="anchored" data-anchor-id="modern-applications"><span class="header-section-number">1.23.5</span> Modern Applications</h3>
<p>Recent developments have expanded the methodology’s applicability[<span class="citation" data-cites="windle2014sampling">Windle, Polson, and Scott (<a href="references.html#ref-windle2014sampling" role="doc-biblioref">2014</a>)</span>]<span class="citation" data-cites="zhang2018scalable">(<a href="references.html#ref-zhang2018scalable" role="doc-biblioref">Zhang, Datta, and Banerjee 2018</a>)</span>:</p>
<ul>
<li>Gaussian process classification: Scalable variational approaches using Pólya-Gamma augmentation</li>
<li>Deep learning: Integration with neural network architectures for Bayesian deep learning</li>
<li>State-space models: Application to dynamic binary time series models</li>
</ul>
<p>The Pólya-Gamma methodology represents a fundamental advancement in Bayesian computation for logistic models, combining theoretical elegance with practical efficiency<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>. Its introduction of the Pólya-Gamma distribution class and the associated data-augmentation strategy has enabled routine application of Bayesian methods to complex hierarchical models that were previously computationally prohibitive<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>The <code>BayesLogit</code> package provides researchers and practitioners with efficient, well-tested implementations of these methods<span class="citation" data-cites="windle2023bayeslogit">(<a href="references.html#ref-windle2023bayeslogit" role="doc-biblioref">Windle 2023</a>)</span>. The combination of exact inference, computational efficiency, and theoretical guarantees makes the Pólya-Gamma approach the method of choice for Bayesian logistic regression in most practical applications<span class="citation" data-cites="polson2013bayesian">(<a href="references.html#ref-polson2013bayesian" role="doc-biblioref">Polson, Scott, and Windle 2013</a>)</span>.</p>
<p>As computational demands continue to grow and models become increasingly complex, the Pólya-Gamma methodology’s advantages become even more pronounced, establishing it as an essential tool in the modern Bayesian statistician’s toolkit (<span class="citation" data-cites="tiao2019polyagamma">Tiao (<a href="references.html#ref-tiao2019polyagamma" role="doc-biblioref">2019</a>)</span>). Ongoing research continues to extend the Pólya-Gamma methodology to new domains, including high-dimensional settings, nonparametric models, and integration with modern machine learning frameworks.</p>


</section>
</section>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-andrews1974scale" class="csl-entry" role="listitem">
Andrews, D. F., and C. L. Mallows. 1974. <span>“Scale <span>Mixtures</span> of <span>Normal Distributions</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 36 (1): 99–102. <a href="https://www.jstor.org/stable/2984774">https://www.jstor.org/stable/2984774</a>.
</div>
<div id="ref-box1992bayesian" class="csl-entry" role="listitem">
Box, George E. P., and George C. Tiao. 1992. <em>Bayesian <span>Inference</span> in <span>Statistical Analysis</span></em>. New York: Wiley-Interscience.
</div>
<div id="ref-carvalho2010horseshoe" class="csl-entry" role="listitem">
Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. <span>“The Horseshoe Estimator for Sparse Signals.”</span> <em>Biometrika</em>, asq017.
</div>
<div id="ref-devroye1986nonuniform" class="csl-entry" role="listitem">
Devroye, Luc. 1986. <em>Non-Uniform Random Variate Generation</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-diaconis1983quantifying" class="csl-entry" role="listitem">
Diaconis, P., and D. Ylvisaker. 1983. <span>“Quantifying <span>Prior Opinion</span>.”</span>
</div>
<div id="ref-efron1975data" class="csl-entry" role="listitem">
Efron, Bradley, and Carl Morris. 1975. <span>“Data <span>Analysis Using Stein</span>’s <span>Estimator</span> and Its <span>Generalizations</span>.”</span> <em>Journal of the American Statistical Association</em> 70 (350): 311–19.
</div>
<div id="ref-efron1977steins" class="csl-entry" role="listitem">
———. 1977. <span>“Stein’s Paradox in Statistics.”</span> <em>Scientific American</em> 236 (5): 119–27.
</div>
<div id="ref-frank1993statistical" class="csl-entry" role="listitem">
Frank, Ildiko E., and Jerome H. Friedman. 1993. <span>“A <span>Statistical View</span> of <span>Some Chemometrics Regression Tools</span>.”</span> <em>Technometrics</em> 35 (2): 109–35. <a href="https://www.jstor.org/stable/1269656">https://www.jstor.org/stable/1269656</a>.
</div>
<div id="ref-huang2008asymptotic" class="csl-entry" role="listitem">
Huang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. <span>“Asymptotic Properties of Bridge Estimators in Sparse High-Dimensional Regression Models.”</span> <em>The Annals of Statistics</em> 36 (2): 587–613.
</div>
<div id="ref-mazumder2011sparsenet" class="csl-entry" role="listitem">
Mazumder, Rahul, Friedman, and Trevor and Hastie. 2011. <span>“<a href="https://www.ncbi.nlm.nih.gov/pubmed/25580042"><span>SparseNet</span>: <span>Coordinate Descent With Nonconvex Penalties</span></a>.”</span> <em>Journal of the American Statistical Association</em> 106 (495): 1125–38.
</div>
<div id="ref-polson2013bayesian" class="csl-entry" role="listitem">
Polson, Nicholas G., James G. Scott, and Jesse Windle. 2013. <span>“Bayesian <span>Inference</span> for <span class="nocase">Logistic Models Using P<span class="nocase">ó</span>lya</span>–<span>Gamma Latent Variables</span>.”</span> <em>Journal of the American Statistical Association</em> 108 (504): 1339–49.
</div>
<div id="ref-stein1964inadmissibility" class="csl-entry" role="listitem">
Stein, Charles. 1964. <span>“Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 16 (1): 155–60.
</div>
<div id="ref-tiao2019polyagamma" class="csl-entry" role="listitem">
Tiao, Louis. 2019. <span>“P<span>ó</span>lya-<span>Gamma Bayesian</span> Logistic Regression.”</span> Blog post.
</div>
<div id="ref-tikhonov1943stability" class="csl-entry" role="listitem">
Tikhonov, Andrey Nikolayevich et al. 1943. <span>“On the Stability of Inverse Problems.”</span> In <em>Dokl. Akad. Nauk Sssr</em>, 39:195–98.
</div>
<div id="ref-windle2023bayeslogit" class="csl-entry" role="listitem">
Windle, Jesse. 2023. <span>“<span>BayesLogit</span>: <span>Bayesian</span> Logistic Regression.”</span> R package version 2.1.
</div>
<div id="ref-windle2014sampling" class="csl-entry" role="listitem">
Windle, Jesse, Nicholas G. Polson, and James G. Scott. 2014. <span>“Sampling <span>Polya-Gamma</span> Random Variates: Alternate and Approximate Techniques.”</span> arXiv. <a href="https://arxiv.org/abs/1405.0506">https://arxiv.org/abs/1405.0506</a>.
</div>
<div id="ref-zhang2018scalable" class="csl-entry" role="listitem">
Zhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. <span>“Scalable <span>Gaussian</span> Process Classification with <span class="nocase">P<span class="nocase">ó</span>lya-Gamma</span> Data Augmentation.”</span> <em>arXiv Preprint arXiv:1802.06383</em>. <a href="https://arxiv.org/abs/1802.06383">https://arxiv.org/abs/1802.06383</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-intro.html" class="pagination-link" aria-label="Principles of Data Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Principles of Data Science</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>