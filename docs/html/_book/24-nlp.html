<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>24&nbsp; Natural Language Processing – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./25-llm.html" rel="next">
<link href="./23-cnn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<!-- Open Graph meta tags for social media sharing -->
<meta property="og:title" content="24&nbsp; Natural Language Processing – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/cover_gpt4o.png">
<meta property="og:type" content="book">
<meta property="og:url" content="">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">

<!-- Twitter Card meta tags -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="24&nbsp; Natural Language Processing – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/cover_gpt4o.png">

<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left[#2\\right]", 2, ""],
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-nlp.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#converting-words-to-numbers-embeddings" id="toc-converting-words-to-numbers-embeddings" class="nav-link active" data-scroll-target="#converting-words-to-numbers-embeddings"><span class="header-section-number">24.1</span> Converting Words to Numbers (Embeddings)</a>
  <ul class="collapse">
  <li><a href="#the-math-of-twenty-questions" id="toc-the-math-of-twenty-questions" class="nav-link" data-scroll-target="#the-math-of-twenty-questions">The Math of Twenty Questions</a></li>
  </ul></li>
  <li><a href="#word2vec-and-distributional-semantics" id="toc-word2vec-and-distributional-semantics" class="nav-link" data-scroll-target="#word2vec-and-distributional-semantics"><span class="header-section-number">24.2</span> Word2Vec and Distributional Semantics</a></li>
  <li><a href="#word2vec-for-war-and-peace" id="toc-word2vec-for-war-and-peace" class="nav-link" data-scroll-target="#word2vec-for-war-and-peace"><span class="header-section-number">24.3</span> Word2Vec for War and Peace</a></li>
  <li><a href="#the-skip-gram-model" id="toc-the-skip-gram-model" class="nav-link" data-scroll-target="#the-skip-gram-model">The Skip-Gram Model</a></li>
  <li><a href="#the-continuous-bag-of-words-cbow-model" id="toc-the-continuous-bag-of-words-cbow-model" class="nav-link" data-scroll-target="#the-continuous-bag-of-words-cbow-model">The Continuous Bag of Words (CBOW) Model</a></li>
  <li><a href="#pretraining-word2vec" id="toc-pretraining-word2vec" class="nav-link" data-scroll-target="#pretraining-word2vec">Pretraining Word2Vec</a></li>
  <li><a href="#computational-efficiency-through-negative-sampling" id="toc-computational-efficiency-through-negative-sampling" class="nav-link" data-scroll-target="#computational-efficiency-through-negative-sampling"><span class="header-section-number">24.4</span> Computational Efficiency Through Negative Sampling</a></li>
  <li><a href="#global-vectors-and-matrix-factorization" id="toc-global-vectors-and-matrix-factorization" class="nav-link" data-scroll-target="#global-vectors-and-matrix-factorization"><span class="header-section-number">24.5</span> Global Vectors and Matrix Factorization</a></li>
  <li><a href="#beyond-words-subword-and-character-models-tokenization" id="toc-beyond-words-subword-and-character-models-tokenization" class="nav-link" data-scroll-target="#beyond-words-subword-and-character-models-tokenization"><span class="header-section-number">24.6</span> Beyond Words: Subword and Character Models (Tokenization)</a></li>
  <li><a href="#attention-mechanisms-and-contextual-representations" id="toc-attention-mechanisms-and-contextual-representations" class="nav-link" data-scroll-target="#attention-mechanisms-and-contextual-representations"><span class="header-section-number">24.7</span> Attention Mechanisms and Contextual Representations</a></li>
  <li><a href="#kernel-smoothing-as-attention" id="toc-kernel-smoothing-as-attention" class="nav-link" data-scroll-target="#kernel-smoothing-as-attention"><span class="header-section-number">24.8</span> Kernel Smoothing as Attention</a></li>
  <li><a href="#attention-over-a-sequence-of-words" id="toc-attention-over-a-sequence-of-words" class="nav-link" data-scroll-target="#attention-over-a-sequence-of-words">Attention over a sequence of Words</a></li>
  <li><a href="#cross-attention-for-translation" id="toc-cross-attention-for-translation" class="nav-link" data-scroll-target="#cross-attention-for-translation"><span class="header-section-number">24.9</span> Cross-attention for Translation</a></li>
  <li><a href="#multi-head-attention-parallel-perspectives" id="toc-multi-head-attention-parallel-perspectives" class="nav-link" data-scroll-target="#multi-head-attention-parallel-perspectives">Multi-Head Attention: Parallel Perspectives</a></li>
  <li><a href="#positional-information-in-attention" id="toc-positional-information-in-attention" class="nav-link" data-scroll-target="#positional-information-in-attention">Positional Information in Attention</a></li>
  <li><a href="#computational-efficiency-and-practical-considerations" id="toc-computational-efficiency-and-practical-considerations" class="nav-link" data-scroll-target="#computational-efficiency-and-practical-considerations">Computational Efficiency and Practical Considerations</a></li>
  <li><a href="#from-attention-to-modern-language-models" id="toc-from-attention-to-modern-language-models" class="nav-link" data-scroll-target="#from-attention-to-modern-language-models">From Attention to Modern Language Models</a></li>
  <li><a href="#transformer-architecture" id="toc-transformer-architecture" class="nav-link" data-scroll-target="#transformer-architecture"><span class="header-section-number">24.10</span> Transformer Architecture</a>
  <ul class="collapse">
  <li><a href="#computational-complexity-and-scalability" id="toc-computational-complexity-and-scalability" class="nav-link" data-scroll-target="#computational-complexity-and-scalability">Computational Complexity and Scalability</a></li>
  </ul></li>
  <li><a href="#pretraining-at-scale-bert-and-beyond" id="toc-pretraining-at-scale-bert-and-beyond" class="nav-link" data-scroll-target="#pretraining-at-scale-bert-and-beyond"><span class="header-section-number">24.11</span> Pretraining at Scale: BERT and Beyond</a>
  <ul class="collapse">
  <li><a href="#bert-architecture-and-training-details" id="toc-bert-architecture-and-training-details" class="nav-link" data-scroll-target="#bert-architecture-and-training-details">BERT Architecture and Training Details</a></li>
  <li><a href="#data-preparation-for-bert-pretraining" id="toc-data-preparation-for-bert-pretraining" class="nav-link" data-scroll-target="#data-preparation-for-bert-pretraining">Data Preparation for BERT Pretraining</a></li>
  </ul></li>
  <li><a href="#transfer-learning-and-downstream-applications" id="toc-transfer-learning-and-downstream-applications" class="nav-link" data-scroll-target="#transfer-learning-and-downstream-applications"><span class="header-section-number">24.12</span> Transfer Learning and Downstream Applications</a></li>
  <li><a href="#model-compression-and-efficiency" id="toc-model-compression-and-efficiency" class="nav-link" data-scroll-target="#model-compression-and-efficiency"><span class="header-section-number">24.13</span> Model Compression and Efficiency</a></li>
  <li><a href="#theoretical-perspectives-and-future-directions" id="toc-theoretical-perspectives-and-future-directions" class="nav-link" data-scroll-target="#theoretical-perspectives-and-future-directions"><span class="header-section-number">24.14</span> Theoretical Perspectives and Future Directions</a></li>
  <li><a href="#natural-language-processing-applications" id="toc-natural-language-processing-applications" class="nav-link" data-scroll-target="#natural-language-processing-applications"><span class="header-section-number">24.15</span> Natural Language Processing: Applications</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-from-opinions-to-insights" id="toc-sentiment-analysis-from-opinions-to-insights" class="nav-link" data-scroll-target="#sentiment-analysis-from-opinions-to-insights">Sentiment Analysis: From Opinions to Insights</a></li>
  <li><a href="#natural-language-inference-reasoning-about-meaning" id="toc-natural-language-inference-reasoning-about-meaning" class="nav-link" data-scroll-target="#natural-language-inference-reasoning-about-meaning">Natural Language Inference: Reasoning About Meaning</a></li>
  <li><a href="#token-level-applications-precision-at-the-word-level" id="toc-token-level-applications-precision-at-the-word-level" class="nav-link" data-scroll-target="#token-level-applications-precision-at-the-word-level">Token-Level Applications: Precision at the Word Level</a></li>
  <li><a href="#fine-tuning-pretrained-models" id="toc-fine-tuning-pretrained-models" class="nav-link" data-scroll-target="#fine-tuning-pretrained-models">Fine-Tuning Pretrained Models</a></li>
  <li><a href="#challenges-and-future-directions" id="toc-challenges-and-future-directions" class="nav-link" data-scroll-target="#challenges-and-future-directions">Challenges and Future Directions</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">24.16</span> Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-nlp.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The ability to understand and generate human language has long been considered a hallmark of intelligence. When Alan Turing proposed his famous test in 1950, he chose natural conversation as the ultimate benchmark for machine intelligence. Yet for decades, this goal remained frustratingly elusive. Early attempts at machine translation in the 1950s, which simply replaced words using bilingual dictionaries, produced nonsensical results that highlighted the profound complexity of human language. The phrase “The spirit is willing, but the flesh is weak” allegedly translated to Russian and back as “The vodka is good, but the meat is rotten”—a cautionary tale about the subtleties of meaning that transcend mere word substitution.</p>
<p>This chapter traces the remarkable journey from those early failures to today’s language models that can engage in nuanced dialogue, translate between languages with near-human accuracy, and even generate creative text. At the heart of this transformation lies a fundamental shift in how we represent language computationally: from discrete symbols manipulated by hand-crafted rules to continuous vector spaces learned from vast corpora of text.</p>
<section id="converting-words-to-numbers-embeddings" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="converting-words-to-numbers-embeddings"><span class="header-section-number">24.1</span> Converting Words to Numbers (Embeddings)</h2>
<p>Language presents unique challenges for mathematical modeling. Unlike images, which naturally exist as arrays of continuous pixel values, or audio signals, which are continuous waveforms, text consists of discrete symbols with no inherent geometric structure. The word “cat” is not inherently closer to “dog” than to “quantum”—at least not in any obvious mathematical sense. Yet humans effortlessly recognize that cats and dogs share semantic properties that neither shares with abstract physics concepts.</p>
<p>A naive way to represent words is through one-hot encoding, where each word in a vocabulary is assigned a unique vector with a single non-zero entry. For example, in a vocabulary of size <span class="math inline">\(|V|\)</span>, the word “cat” might be represented as <span class="math display">\[
v_{\text{cat}} = \overbrace{[0, 0, \ldots, 1, \ldots, 0]}^{|V|}
\]</span> with the 1 in the <span class="math inline">\(i\)</span>-th position corresponding to “cat”. However, this approach fails to capture any notion of semantic similarity: the cosine similarity between any two distinct one-hot vectors is zero, erasing all information about how words relate to each other.</p>
<p>This type of representation makes even the seemingly simple task of determining whether two sentences have similar meanings challenging. The sentences “The cat sat on the mat” and “A feline rested on the rug” express nearly identical ideas despite sharing no words except “the” and “on.” Conversely, “The bank is closed” could refer to a financial institution or a river’s edge—the same words encoding entirely different meanings. These examples illustrate why early symbolic approaches to natural language processing, based on logical rules and hand-crafted features, struggled to capture the fluid, contextual nature of meaning.</p>
<p>The breakthrough came from reconceptualizing the representation problem. Instead of treating words as atomic symbols, what if we could embed them in a continuous vector space where geometric relationships encode semantic relationships? This idea, simple in retrospect, revolutionized the field. In such a space, we might find that <span class="math inline">\(v_{\text{cat}} - v_{\text{dog}}\)</span> has similar direction to <span class="math inline">\(v_{\text{car}} - v_{\text{bicycle}}\)</span>, capturing the analogical relationship “cat is to dog as car is to bicycle” through vector arithmetic.</p>
<p>To formalize this intuition, we seek a mapping <span class="math inline">\(\phi: \mathcal{V} \rightarrow \mathbb{R}^d\)</span> from a vocabulary <span class="math inline">\(\mathcal{V}\)</span> of discrete tokens to <span class="math inline">\(d\)</span>-dimensional vectors. The challenge lies in learning this mapping such that the resulting geometry reflects semantic relationships. The naive approach of one-hot encoding, where each word is represented by a vector with a single non-zero entry, fails catastrophically: in an <span class="math inline">\(N\)</span>-word vocabulary, this produces <span class="math inline">\(N\)</span>-dimensional vectors where every pair of distinct words has cosine similarity zero, erasing all notion of semantic relatedness.</p>
<section id="the-math-of-twenty-questions" class="level3">
<h3 class="anchored" data-anchor-id="the-math-of-twenty-questions">The Math of Twenty Questions</h3>
<p>My (Vadim’s) daughter and I play a game of Twenty Questions during road trips. The rules are simple: one person thinks of something, and the other person has to guess what it is by asking yes-or-no questions. The person who is guessing can ask up to twenty questions, and then they have to make a guess. If they guess correctly, they win; if not, the other person wins. The game is fun, but it’s also a great way to illustrate how AI systems can learn to represent words and phrases as numbers. The trick is to come up with an optimal set of yes-or-no questions that will allow you to distinguish between all the words or phrases you might want to represent. Surpisingly, most of the words can be identified with a small set of universal questions asked in the same order every time. Usually person who has a better set of questions wins.For example, you might ask:</p>
<ul>
<li>Is it an animal? (Yes)</li>
<li>Is this a domestic animal? (No)</li>
<li>Is it larger than a human? (Yes)</li>
<li>Does it have a long tail? (No)</li>
<li>Is it a predator? (Yes)</li>
<li>Can move on two feet? (Yes)</li>
<li>Is it a bear? (Yes)</li>
</ul>
<p>Thus, if we use a 20-dimenstional 0-1 vector to represent the word “bear,” the portion of this vector corresponding to these questions would look like this:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Animal</th>
<th style="text-align: center;">Domestic</th>
<th style="text-align: center;">Larger than human</th>
<th style="text-align: center;">Long tail</th>
<th style="text-align: center;">Predator</th>
<th style="text-align: center;">Can move on two feet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bear</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>This is called a word vector. Specifically, it’s a “binary” or 0/1 vector: 1 means yes, 0 means no. Different words, would produce different answers to the same questions, so they would have different word vectors. If we stack all these vectors in a matrix, where each row is a word and each column is a question, we get something like this:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Animal</th>
<th style="text-align: center;">Domestic</th>
<th style="text-align: center;">Larger than human</th>
<th style="text-align: center;">Long tail</th>
<th style="text-align: center;">Predator</th>
<th style="text-align: center;">Can move on two feet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bear</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td>Dog</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td>Cat</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>The binary nature of these vectors forces us to choose 1 or 0 for the “Larger than human” question for “Dog”. However, there are some dog breeds that are larger than humans, so this binary representation is not very useful in this case. We can do better by allowing the answers to be numbers between 0 and 1, rather than just 0 or 1. This way, we can represent the fact that some dogs are larger than humans, but most are not. For example, we might answer the question “Is it larger than a human?” with a 0.1 for a dog, and a 0.8 for a bear, some types of bears can be smaller than humans, for example, black bears that live in North America, but most bears are larger than humans.</p>
<p>Using this approach, the vectors now become</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 20%">
<col style="width: 12%">
<col style="width: 10%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Animal</th>
<th style="text-align: center;">Domestic</th>
<th style="text-align: center;">Larger than human</th>
<th style="text-align: center;">Long tail</th>
<th style="text-align: center;">Predator</th>
<th style="text-align: center;">Can move on two feet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bear</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr class="even">
<td>Dog</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td>Cat</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>AI systems, also use non-binary scoring rules to judge a win or loss. For example, if the answer is “bear,” then the score might be 100 points for a correct guess, 90 points for a close guess like “Binturong” or “wolverine,” and 50 points for a distant guess like “eagle.” This way of keeping score matches the real-world design requirements of most NLP systems. For example, if you translate JFK saying “Ich bin ein Berliner” as “I am a German,” you’re wrong, but a lot closer than if you translate it as “I am a cronut.”</p>
<p>The process of converting words into numbers is called “embedding.” The resulting vectors are called “word embeddings.” The only part that is left is how to design an algorithms that can find a good set of questions to ask. Usually real-life word embeddings have hundreds of questions, not just twenty. The process of finding these questions is called “training” the model. The goal is to find a set of questions that will allow the model to distinguish between all the words in the vocabulary, and to do so in a way that captures their meanings. However, the algorithms do not have a notion of meaning in the same way that humans do. Instead, they learn by counting word co-location statistics—that is, which words tend to appear with which other words in real sentences written by humans.</p>
<p>These co-occurrence statistics serve as surprisingly effective proxies for meaning. For example, consider the question: “Among all sentences containing ‘fries’ and ‘ketchup,’ how frequently does the word ‘bun’ also appear?” This is the kind of query a machine can easily formulate and answer, since it relies on counting rather than true understanding.</p>
<p>While such a specific question may be too limited if you can only ask a few hundred, the underlying idea—using word co-occurrence patterns—is powerful. Word embedding algorithms are built on this principle: they systematically explore which words tend to appear together, and through optimization, learn the most informative “questions” to ask. By repeatedly applying these learned probes, the algorithms construct a vector for each word or phrase, capturing its relationships based on co-occurrence statistics. These word vectors are then organized into a matrix for further use.</p>
<p>There are many ways to train word embeddings, but the most common one is to use a neural network. The neural network learns to ask questions that are most useful for distinguishing between different words. It does this by looking at a large number of examples of words and their meanings, and then adjusting the weights of the questions based on how well they perform. One of the first and most popular algorithms for this is called Word2Vec, which was introduced by <span class="citation" data-cites="mikolov2013efficient">Mikolov et al. (<a href="references.html#ref-mikolov2013efficient" role="doc-biblioref">2013</a>)</span> at Google in 2013.</p>
</section>
</section>
<section id="word2vec-and-distributional-semantics" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="word2vec-and-distributional-semantics"><span class="header-section-number">24.2</span> Word2Vec and Distributional Semantics</h2>
<p>The theoretical foundation for learning meaningful word representations comes from the distributional hypothesis, articulated by linguist J.R. Firth in 1957: “You shall know a word by the company it keeps.” This principle suggests that words appearing in similar contexts tend to have similar meanings. If “coffee” and “tea” both frequently appear near words like “drink,” “hot,” “cup,” and “morning,” we can infer their semantic similarity.</p>
<p>The word2vec framework, introduced by <span class="citation" data-cites="mikolov2013efficient">Mikolov et al. (<a href="references.html#ref-mikolov2013efficient" role="doc-biblioref">2013</a>)</span>, operationalized this insight through a beautifully simple probabilistic model. The skip-gram variant posits that a word can be used to predict its surrounding context words. Given a corpus of text represented as a sequence of words <span class="math inline">\(w_1, w_2, \ldots, w_T\)</span>, the model maximizes the likelihood:</p>
<p><span class="math display">\[\mathcal{L} = \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} \mid w_t)\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the contextr window size. The conditional probability is parameterized using two sets of embeddings: <span class="math inline">\(\mathbf{v}_w\)</span> for words as centers and <span class="math inline">\(\mathbf{u}_w\)</span> for words as context:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^T \mathbf{v}_c)}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^T \mathbf{v}_c)}\]</span></p>
<p>This formulation reveals deep connections to the theoretical frameworks discussed in previous chapters. The dot product <span class="math inline">\(\mathbf{u}_o^T \mathbf{v}_c\)</span> acts as a compatibility score between center and context words, while the softmax normalization ensures a valid probability distribution. From the perspective of ridge functions, we can view this as learning representations where the function <span class="math inline">\(f(w_c, w_o) = \mathbf{u}_o^T \mathbf{v}_c\)</span> captures the log-odds of co-occurrence.</p>
</section>
<div id="ex-word2vec">
<section id="word2vec-for-war-and-peace" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="word2vec-for-war-and-peace"><span class="header-section-number">24.3</span> Word2Vec for War and Peace</h2>
<p>This analysis showcases the application of Word2Vec on Leo Tolstoy’s “War and Peace,” demonstrating how neural network-based techniques can learn vector representations of words by analyzing their contextual relationships within the text. The model employs the skip-gram algorithm with negative sampling (explained later), a widely used configuration for Word2Vec. We use vector dimensions range of 100, providing a balance between capturing semantic detail and computational efficiency. The context window size is set to 5, and the model is trained over several iterations to ensure convergence.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define training data, read from ../data/warpeace.txt</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'../data/warpeace.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    Text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [s.split() <span class="cf">for</span> s <span class="kw">in</span> Text.split(<span class="st">'. '</span>) ] <span class="co"># split the text into sentences</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove spaces and punctuation</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [[word.strip(<span class="st">'.,!?;:()[]'</span>) <span class="cf">for</span> word <span class="kw">in</span> sentence <span class="cf">if</span> word.strip(<span class="st">'.,!?;:()[]'</span>)] <span class="cf">for</span> sentence <span class="kw">in</span> sentences]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove empty sentences</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [sentence <span class="cf">for</span> sentence <span class="kw">in</span> sentences <span class="cf">if</span> sentence]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Word2Vec(sentences, min_count<span class="op">=</span><span class="dv">1</span>,seed<span class="op">=</span><span class="dv">42</span>,workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># fit a 2d PCA model to the vectors, only use the words from ../data/embedding_words.txt</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'../data/embedding_words.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> <span class="bu">file</span>.read().splitlines()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> model.wv[words]  <span class="co"># get the vectors for the words</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># reduce the dimensionality of the vectors to 2D using PCA</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># create a scatter plot of the projection</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>pyplot.scatter(result[:, <span class="dv">0</span>], result[:, <span class="dv">1</span>])</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># words = model.wv.index_to_key</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    pyplot.annotate(word, xy<span class="op">=</span>(result[i, <span class="dv">0</span>], result[i, <span class="dv">1</span>]))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-nlp_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The results reveal meaningful semantic relationships within the text and highlights several key insights into the thematic structure of Tolstoy’s War and Peace. First, the clustering of military-related terms (<code>soldier</code>, <code>regiment</code>, <code>battle</code>) underscores Word2Vec’s ability to group semantically related words based on their co-occurrence patterns. Additionally, the grouping of words such as “ballroom,” “court,” and “marriage” reflects the novel’s emphasis on aristocratic society and its social hierarchy. The use of PCA for dimensionality reduction effectively preserves meaningful relationships while reducing the original high-dimensional Word2Vec space (100 dimensions) to a two-dimensional representation. Overall, this visualization demonstrates how Word2Vec captures distinct thematic domains, including military, social, and goverment. Notice that a more abstract concept like <code>peace</code> is surrounded by words from both goverment (<code>history</code>, <code>power</code>, <code>war</code>) and social domains (<code>family</code>, <code>friendship</code>, <code>marriage</code>,<code>court</code>, <code>ballroom</code>, <code>society</code>), indicating its central role in the narrative but is distant from the military domain, which is more focused on the war aspect of the story.</p>
<p>These insights have practical applications in literary analysis, theme extraction, character relationship mapping, historical text understanding, and similarity-based text search and recommendation systems.</p>
</section>
</div>
<section id="the-skip-gram-model" class="level3">
<h3 class="anchored" data-anchor-id="the-skip-gram-model">The Skip-Gram Model</h3>
<p>The skip-gram model operates on a simple yet powerful principle: given a center word, predict the surrounding context words within a fixed window. This approach assumes that words appearing in similar contexts tend to have similar meanings, directly implementing the distributional hypothesis.</p>
<p>Consider the sentence “The man loves his son” with “loves” as the center word and a context window of size 2. The skip-gram model aims to maximize the probability of generating the context words “the,” “man,” “his,” and “son” given the center word “loves.” This relationship can be visualized as follows:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[loves] --&gt; B[the]
    A --&gt; C[man]
    A --&gt; D[his]
    A --&gt; E[son]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    
    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class A centerWord
    class B,C,D,E contextWord
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The mathematical foundation assumes conditional independence among context words given the center word, allowing the joint probability to factorize:</p>
<p><span class="math display">\[P(\text{"the"}, \text{"man"}, \text{"his"}, \text{"son"} \mid \text{"loves"}) = P(\text{"the"} \mid \text{"loves"}) \cdot P(\text{"man"} \mid \text{"loves"}) \cdot P(\text{"his"} \mid \text{"loves"}) \cdot P(\text{"son"} \mid \text{"loves"})\]</span></p>
<p>This independence assumption, while not strictly true in natural language, proves crucial for computational tractability. Each conditional probability is modeled using the softmax function over the entire vocabulary:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^T \mathbf{v}_c)}{\sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^T \mathbf{v}_c)}\]</span></p>
<p>The skip-gram objective seeks to maximize the likelihood of observing all context words across the entire corpus. For a text sequence of length <span class="math inline">\(T\)</span> with words <span class="math inline">\(w^{(1)}, w^{(2)}, \ldots, w^{(T)}\)</span>, the objective becomes:</p>
<p><span class="math display">\[\mathcal{L}_{\text{skip-gram}} = \frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w^{(t+j)} \mid w^{(t)})\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the context window size. The normalization by <span class="math inline">\(T\)</span> ensures that the objective remains bounded as corpus size grows.</p>
<p>The gradient with respect to the center word embedding reveals the learning dynamics:</p>
<p><span class="math display">\[\frac{\partial \log P(w_o \mid w_c)}{\partial \mathbf{v}_c} = \mathbf{u}_o - \sum_{i \in \mathcal{V}} P(w_i \mid w_c) \mathbf{u}_i\]</span></p>
<p>This elegant form shows that the gradient pushes the center word embedding toward the observed context word (<span class="math inline">\(\mathbf{u}_o\)</span>) while pulling it away from all other words, weighted by their predicted probabilities. This creates a natural contrast between positive and negative examples, even in the original formulation without explicit negative sampling.</p>
<p>The skip-gram architecture assigns two vector representations to each word: <span class="math inline">\(\mathbf{v}_w\)</span> when the word serves as a center word and <span class="math inline">\(\mathbf{u}_w\)</span> when it appears in context. This asymmetry allows the model to capture different aspects of word usage. After training, the center word vectors <span class="math inline">\(\mathbf{v}_w\)</span> are typically used as the final word embeddings, though some implementations average or concatenate both representations.</p>
</section>
<section id="the-continuous-bag-of-words-cbow-model" class="level3">
<h3 class="anchored" data-anchor-id="the-continuous-bag-of-words-cbow-model">The Continuous Bag of Words (CBOW) Model</h3>
<p>While skip-gram predicts context words from a center word, the Continuous Bag of Words (CBOW) model reverses this relationship: it predicts a center word based on its surrounding context. For the same text sequence “the”, “man”, “loves”, “his”, “son” with “loves” as the center word, CBOW models the conditional probability:</p>
<p><span class="math display">\[P(\text{"loves"} \mid \text{"the"}, \text{"man"}, \text{"his"}, \text{"son"})\]</span></p>
<p>The CBOW architecture can be visualized as multiple context words converging to predict a single center word:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    B[the] --&gt; A[loves]
    C[man] --&gt; A
    D[his] --&gt; A
    E[son] --&gt; A
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    
    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class A centerWord
    class B,C,D,E contextWord
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The key difference from skip-gram lies in how CBOW handles multiple context words. Rather than treating each context word independently, CBOW averages their embeddings. For a center word <span class="math inline">\(w_c\)</span> with context words <span class="math inline">\(w_{o_1}, \ldots, w_{o_{2m}}\)</span>, the conditional probability is:</p>
<p><span class="math display">\[P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\exp\left(\mathbf{u}_c^T \bar{\mathbf{v}}_o\right)}{\sum_{i \in \mathcal{V}} \exp\left(\mathbf{u}_i^T \bar{\mathbf{v}}_o\right)}\]</span></p>
<p>where <span class="math inline">\(\bar{\mathbf{v}}_o = \frac{1}{2m}\sum_{j=1}^{2m} \mathbf{v}_{o_j}\)</span> is the average of context word vectors. Note that in CBOW, <span class="math inline">\(\mathbf{u}_i\)</span> represents word <span class="math inline">\(i\)</span> as a center word and <span class="math inline">\(\mathbf{v}_i\)</span> represents it as a context word—the opposite of skip-gram’s convention.</p>
<p>The CBOW objective maximizes the likelihood of generating all center words given their contexts:</p>
<p><span class="math display">\[\mathcal{L}_{\text{CBOW}} = \sum_{t=1}^T \log P(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})\]</span></p>
<p>The gradient with respect to context word vectors reveals how CBOW learns:</p>
<p><span class="math display">\[\frac{\partial \log P(w_c \mid \mathcal{W}_o)}{\partial \mathbf{v}_{o_i}} = \frac{1}{2m}\left(\mathbf{u}_c - \sum_{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \mathbf{u}_j\right)\]</span></p>
<p>This gradient is scaled by <span class="math inline">\(\frac{1}{2m}\)</span>, effectively distributing the learning signal across all context words. CBOW tends to train faster than skip-gram because it predicts one word per context window rather than multiple words, but skip-gram often produces better representations for rare words since it generates more training examples per sentence.</p>
</section>
<section id="pretraining-word2vec" class="level3">
<h3 class="anchored" data-anchor-id="pretraining-word2vec">Pretraining Word2Vec</h3>
<p>Training word2vec models requires careful attention to implementation details that significantly impact the quality of learned representations. The training process begins with data preprocessing on large text corpora. Using the Penn Tree Bank dataset as an example—a carefully annotated corpus of Wall Street Journal articles containing about 1 million words—we implement several crucial preprocessing steps.</p>
<p>First, we build a vocabulary by counting word frequencies and retaining only words that appear at least a minimum number of times (typically 5-10). This thresholding serves two purposes: it reduces the vocabulary size from potentially millions to tens of thousands of words, and it prevents the model from wasting capacity on rare words that appear too infrequently to learn meaningful representations. Words below the threshold are replaced with a special <code>&lt;unk&gt;</code> token.</p>
<p>The training procedure uses stochastic gradient descent with a carefully designed learning rate schedule. The initial learning rate (typically 0.025 for skip-gram and 0.05 for CBOW) is linearly decreased to a minimum value (usually 0.0001) as training progresses:</p>
<p><span class="math display">\[\alpha_t = \alpha_0 \left(1 - \frac{\text{words\_processed}}{\text{total\_words} \times \text{epochs}}\right)\]</span></p>
<p>This schedule ensures larger updates early in training when representations are poor, transitioning to fine-tuning as the model converges.</p>
<p>The implementation uses several optimizations for efficiency. Word vectors are typically initialized randomly from a uniform distribution over <span class="math inline">\([-0.5/d, 0.5/d]\)</span> where <span class="math inline">\(d\)</span> is the embedding dimension (commonly 100-300). During training, we maintain two embedding matrices: one for center words and one for context words. After training, these can be combined (usually by averaging) or just the center word embeddings can be used.</p>
<p>For minibatch processing, training examples are grouped to enable efficient matrix operations. Given a batch of center-context pairs, the forward pass computes scores using matrix multiplication, applies the loss function (either full softmax, hierarchical softmax, or negative sampling), and backpropagates gradients. A typical training configuration might process batches of 512 word pairs, iterating 5-15 times over a corpus.</p>
<p>The quality of learned embeddings can be evaluated through word similarity and analogy tasks. For similarity, we compute cosine distances between word vectors and verify that semantically similar words have high cosine similarity. For analogies, we test whether vector arithmetic captures semantic relationships: the famous “king - man + woman <span class="math inline">\(\approx\)</span> queen” example demonstrates that vector differences encode meaningful semantic transformations.</p>
<p>Training word2vec on real data requires several practical considerations. Using the Penn Tree Bank dataset—a carefully annotated corpus of Wall Street Journal articles—we must first tokenize the text and build a vocabulary. Typically, we keep only words appearing at least 10 times, replacing rare words with a special <code>&lt;unk&gt;</code> token. This thresholding reduces vocabulary size and helps the model focus on learning good representations for common words.</p>
<p>A crucial but often overlooked aspect is subsampling of frequent words. Words like “the,” “a,” and “is” appear so frequently that they provide little information about the semantic content of their neighbors. The probability of discarding a word <span class="math inline">\(w\)</span> during training is:</p>
<p><span class="math display">\[P(\text{discard } w) = 1 - \sqrt{\frac{t}{f(w)}}\]</span></p>
<p>where <span class="math inline">\(f(w)\)</span> is the frequency of word <span class="math inline">\(w\)</span> and <span class="math inline">\(t\)</span> is a threshold (typically <span class="math inline">\(10^{-5}\)</span>). This formula ensures that very frequent words are aggressively subsampled while preserving most occurrences of informative words.</p>
<p>For training, we extract examples by sliding a window over the text. For each center word, we collect its context words within a window of size <span class="math inline">\(m\)</span>. Importantly, the actual window size is sampled uniformly from <span class="math inline">\([1, m]\)</span> for each center word, which helps the model learn representations that are robust to varying context sizes. Consider the sentence “the cat sat on the mat” with maximum window size 2. For the center word “sat,” we might sample a window size of 1, giving context words [“cat”, “on”], or a window size of 2, giving context words [“the”, “cat”, “on”, “the”].</p>
</section>
<section id="computational-efficiency-through-negative-sampling" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="computational-efficiency-through-negative-sampling"><span class="header-section-number">24.4</span> Computational Efficiency Through Negative Sampling</h2>
<p>The elegance of word2vec’s formulation belies a serious computational challenge. Computing the normalization term in the softmax requires summing over the entire vocabulary—potentially millions of terms—for every gradient update. With large corpora containing billions of words, this quickly becomes intractable.</p>
<p>Negative sampling transforms the problem from multi-class classification to binary classification. Instead of predicting which word from the entire vocabulary appears in the context, we ask a simpler question: given a word pair, is it a real center-context pair from the corpus or a randomly generated negative example? The objective becomes:</p>
<p><span class="math display">\[\mathcal{L}_{\text{NS}} = \log \sigma(\mathbf{u}_o^T \mathbf{v}_c) + \sum_{k=1}^K \mathbb{E}_{w_k \sim P_n} \left[\log \sigma(-\mathbf{u}_{w_k}^T \mathbf{v}_c)\right]\]</span></p>
<p>where <span class="math inline">\(\sigma(x) = 1/(1 + e^{-x})\)</span> is the sigmoid function, and <span class="math inline">\(P_n\)</span> is a noise distribution over words. The clever insight is that by carefully choosing the noise distribution—typically <span class="math inline">\(P_n(w) \propto f(w)^{3/4}\)</span> where <span class="math inline">\(f(w)\)</span> is word frequency—we can approximate the original objective while reducing computation from <span class="math inline">\(O(|\mathcal{V}|)\)</span> to <span class="math inline">\(O(K)\)</span>, where <span class="math inline">\(K \ll |\mathcal{V}|\)</span> is a small number of negative samples (typically 5-20).</p>
<p>An alternative solution is hierarchical softmax, which replaces the flat softmax over the vocabulary with a binary tree where each leaf represents a word. The probability of a word is then the product of binary decisions along the path from root to leaf:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \prod_{j=1}^{L(w_o)-1} \sigma\left([\![n(w_o, j+1) = \text{leftChild}(n(w_o, j))]\!] \cdot \mathbf{u}_{n(w_o,j)}^T \mathbf{v}_c\right)\]</span></p>
<p>where <span class="math inline">\(L(w_o)\)</span> is the length of the path to word <span class="math inline">\(w_o\)</span>, <span class="math inline">\(n(w_o, j)\)</span> is the <span class="math inline">\(j\)</span>-th node on this path, and <span class="math inline">\([\![\cdot]\!]\)</span> is the indicator function that returns 1 or -1. This reduces computational complexity from <span class="math inline">\(O(|\mathcal{V}|)\)</span> to <span class="math inline">\(O(\log |\mathcal{V}|)\)</span>, though negative sampling typically performs better in practice.</p>
</section>
<section id="global-vectors-and-matrix-factorization" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="global-vectors-and-matrix-factorization"><span class="header-section-number">24.5</span> Global Vectors and Matrix Factorization</h2>
<p>While word2vec learns from local context windows, GloVe (Global Vectors) leverages global co-occurrence statistics. The key observation is that the ratio of co-occurrence probabilities can encode semantic relationships. Consider the words “ice” and “steam” in relation to “solid” and “gas”: <span class="math inline">\(P(\text{solid} \mid \text{ice}) / P(\text{solid} \mid \text{steam})\)</span> is large (around 8.9), while <span class="math inline">\(P(\text{gas} \mid \text{ice}) / P(\text{gas} \mid \text{steam})\)</span> is small (around 0.085), and <span class="math inline">\(P(\text{water} \mid \text{ice}) / P(\text{water} \mid \text{steam})\)</span> is close to 1 (around 1.36).</p>
<p>These ratios capture the semantic relationships: ice is solid, steam is gas, and both relate to water. GloVe learns embeddings that preserve these ratios through the objective:</p>
<p><span class="math display">\[\mathcal{L}_{\text{GloVe}} = \sum_{i,j} h(X_{ij}) \left(\mathbf{v}_i^T \mathbf{u}_j + b_i + c_j - \log X_{ij}\right)^2\]</span></p>
<p>where <span class="math inline">\(X_{ij}\)</span> counts co-occurrences, <span class="math inline">\(h(\cdot)\)</span> is a weighting function that prevents very common or very rare pairs from dominating, and <span class="math inline">\(b_i, c_j\)</span> are bias terms. A typical choice is <span class="math inline">\(h(x) = (x/x_{\max})^{\alpha}\)</span> if <span class="math inline">\(x &lt; x_{\max}\)</span>, else 1, with <span class="math inline">\(\alpha = 0.75\)</span> and <span class="math inline">\(x_{\max} = 100\)</span>.</p>
<p>This formulation reveals GloVe as a weighted matrix factorization problem. We seek low-rank factors <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> such that <span class="math inline">\(\mathbf{V}^T\mathbf{U} \approx \log \mathbf{X}\)</span>, where the approximation is weighted by the function $h()`. This connection to classical linear algebra provides theoretical insights: the optimal embeddings lie in the subspace spanned by the top singular vectors of an appropriately transformed co-occurrence matrix.</p>
</section>
<section id="beyond-words-subword-and-character-models-tokenization" class="level2" data-number="24.6">
<h2 data-number="24.6" class="anchored" data-anchor-id="beyond-words-subword-and-character-models-tokenization"><span class="header-section-number">24.6</span> Beyond Words: Subword and Character Models (Tokenization)</h2>
<p>A fundamental limitation of word-level embeddings is their inability to handle out-of-vocabulary words or capture morphological relationships. The word “unhappiness” shares obvious morphological connections with “happy,” “unhappy,” and “happiness,” but word2vec treats these as completely independent tokens.</p>
<p>FastText addresses this limitation by representing words as bags of character n-grams. For the word “where” with n-grams of length 3 to 6, we extract: “&lt;wh”, “whe”, “her”, “ere”, “re&gt;”, and longer n-grams up to the full word. The word embedding is then the sum of its n-gram embeddings:</p>
<p><span class="math display">\[\mathbf{v}_{\text{where}} = \sum_{g \in \mathcal{G}_{\text{where}}} \mathbf{z}_g\]</span></p>
<p>This approach naturally handles out-of-vocabulary words by breaking them into known n-grams and provides better representations for rare words by sharing parameters across morphologically related words.</p>
<p>An even more flexible approach is Byte Pair Encoding (BPE), which learns a vocabulary of subword units directly from the data. Starting with individual characters, BPE iteratively merges the most frequent pair of adjacent units until reaching a desired vocabulary size. For example, given a corpus with word frequencies {“fast”: 4, “faster”: 3, “tall”: 5, “taller”: 4}, BPE might learn merges like “t” + “a” <span class="math inline">\(\rightarrow\)</span> “ta”, then “ta” + “l” <span class="math inline">\(\rightarrow\)</span> “tal”, and so on. This data-driven approach balances vocabulary size with representation power, enabling models to handle arbitrary text while maintaining reasonable computational requirements.</p>
<p>The choice of tokenization strategy has profound implications for model performance and behavior. Modern LLMs typically use vocabularies of 50,000 to 100,000 tokens, carefully balanced to represent different languages while maintaining computational efficiency. A vocabulary that’s too small forces the model to represent complex concepts with many tokens, making it harder to capture meaning efficiently.</p>
<p>Consider how the sentence “The Verbasizer helped Bowie create unexpected word combinations” might be tokenized by a BPE-based system. Rather than treating each word as a single unit, the tokenizer might split it into subword pieces like [“The”, ” Ver”, “bas”, “izer”, ” helped”, ” Bow”, “ie”, ” create”, ” unexpected”, ” word”, ” combinations”]. This fragmentation allows the model to handle rare words and proper names by breaking them into more common subcomponents.</p>
<p>This tokenization process explains some behavioral patterns in modern language models. Names from fictional universes, specialized terminology, and code snippets can be tokenized in ways that fragment their semantic meaning, potentially affecting model performance on these inputs. Understanding these tokenization effects is crucial for interpreting model behavior and designing effective prompting strategies.</p>
</section>
<section id="attention-mechanisms-and-contextual-representations" class="level2" data-number="24.7">
<h2 data-number="24.7" class="anchored" data-anchor-id="attention-mechanisms-and-contextual-representations"><span class="header-section-number">24.7</span> Attention Mechanisms and Contextual Representations</h2>
<p>Static word embeddings suffer from a fundamental limitation: they assign a single vector to each word, ignoring context. The word “bank” receives the same representation whether it appears in “river bank” or “investment bank.” This conflation of multiple senses into a single vector creates an information bottleneck that limits performance on downstream tasks.</p>
<p>The evolution of word embeddings culminated in the development of contextual representations, which dynamically adjust based on surrounding context. The context problem was solved by the introduction of attention mechanisms, particularly in the transformer architecture, which allows models to capture complex dependencies and relationships between words. The idea is to mimic the human ability to selectively focus attention. When reading this sentence, your visual system processes every word simultaneously, yet your conscious attention flows sequentially from word to word. You can redirect this attention at will—perhaps noticing a particular phrase or jumping back to reread a complex clause. This dynamic allocation of cognitive resources allows us to extract meaning from information-rich environments without being overwhelmed by irrelevant details.</p>
<p>Machine learning systems faced analogous challenges in processing sequential information. Early neural networks, particularly recurrent architectures, processed sequences step by step, maintaining a hidden state that theoretically encoded all previous information. However, this sequential bottleneck created fundamental limitations: information from early time steps could vanish as sequences grew longer, and the inherently sequential nature prevented parallel computation that could leverage modern hardware effectively.</p>
<p>The attention <span class="citation" data-cites="bahdanau2014neural">Vaswani et al. (<a href="references.html#ref-vaswani2023attention" role="doc-biblioref">2023</a>)</span> mechanism revolutionized this landscape by allowing models to directly access and combine information from any part of the input sequence. Rather than compressing an entire sequence into a fixed-size vector, attention mechanisms create dynamic representations that adaptively weight different parts of the input based on their relevance to the current computation. This breakthrough enabled the development of the Transformer architecture, which forms the foundation of modern language models from BERT to GPT.</p>
<p>At its core, attention implements an information retrieval system inspired by database operations. Consider how you might search through a library: you formulate a <strong>query</strong> (what you’re looking for), examine the <strong>keys</strong> (catalog entries or book titles), and retrieve the associated <strong>values</strong> (the actual books or their contents). Attention mechanisms formalize this intuition through three learned representations:</p>
<ul>
<li><strong>Queries</strong> (<span class="math inline">\(\mathbf{Q}\)</span>): What information are we seeking?</li>
<li><strong>Keys</strong> (<span class="math inline">\(\mathbf{K}\)</span>): What information is available at each position?</li>
<li><strong>Values</strong> (<span class="math inline">\(\mathbf{V}\)</span>): What information should we retrieve?</li>
</ul>
<p>Given a set of keys and values, attention computes a weighted sum of the values based on their relevance to the query. Mathematically, this can be expressed as: <span class="math display">\[
\textrm{Attention}(\mathbf{q}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha_i(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i,
\]</span></p>
<p>where <span class="math inline">\(\alpha_i\)</span> are scalar attention weights computed from the queries and keys. The operation itself is typically referred to as <strong>attention pooling</strong>. The name “attention” derives from the fact that the operation pays particular attention to the terms for which the weight <span class="math inline">\(\alpha_i\)</span> is significant (i.e., large).</p>
<p>It is typical in AI applications to assume that the attention weights <span class="math inline">\(\alpha_i\)</span> are nonnegative and form a convex combination, i.e., <span class="math inline">\(\sum_{i=1}^n \alpha_i = 1\)</span>. A common strategy for ensuring that the weights sum up to 1 is to normalize them via: <span class="math display">\[
\alpha_i = \frac{\exp(\alpha_i)}{\sum_{j=1}^n \exp(\alpha_j)}
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    %% Keys column
    k1["k₁"] 
    k2["k₂"]
    km["kₘ"]
    
    %% Attention weights (functions)
    alpha1["α(q, k₁)"]
    alpha2["α(q, k₂)"]
    alpham["α(q, kₘ)"]
    
    %% Query
    q["q"]
    
    %% Values column
    v1["v₁"]
    v2["v₂"]
    vm["vₘ"]
    
    %% Output
    output[" Sum "]
    
    %% Connections from keys to attention weights
    k1 --&gt; alpha1
    k2 --&gt; alpha2
    km --&gt; alpham
    
    %% Query to attention weights
    q --&gt; alpha1
    q --&gt; alpha2
    q --&gt; alpham
    
    %% Attention weights to values (dashed lines)
    alpha1 -.-&gt; v1
    alpha2 -.-&gt; v2
    alpham -.-&gt; vm
    
    %% Values to output
    v1 --&gt; output
    v2 --&gt; output
    vm --&gt; output
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<div id="ex-kernel-smoothing">
<section id="kernel-smoothing-as-attention" class="level2" data-number="24.8">
<h2 data-number="24.8" class="anchored" data-anchor-id="kernel-smoothing-as-attention"><span class="header-section-number">24.8</span> Kernel Smoothing as Attention</h2>
<p>The concept of attention pooling can actually be traced back to classical kernel methods like Nadaraya-Watson regression, where similarity kernels determine how much weight to give to different data points. Given a sequence of observations <span class="math inline">\(y_1, \ldots, y_n\)</span>, and each observation has a two-dimenstional index <span class="math inline">\(x^{(i)} \in \mathbb{R}^2\)</span>, such as in spacial process. Then the question is what would be the value of <span class="math inline">\(y\)</span> and a previously unobserved location <span class="math inline">\(x_{new} = (x_1,x_2)\)</span>. Let’s simulate some data and see what we can do.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some data</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> x1) <span class="op">+</span> x2<span class="op">*</span>x2<span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">100</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(x1, x2, c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar()<span class="op">;</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-nlp_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The kernel smoothing method then uses <span class="math inline">\(q = (x_1,x_2)\)</span> as the query and observed <span class="math inline">\(x_i\)</span>s as the keys. The weights are then computed as <span class="math inline">\(\alpha_i = K(q, x_i)\)</span> where <span class="math inline">\(K\)</span> is a kernel function, that measures, how close <span class="math inline">\(q\)</span> is to <span class="math inline">\(x_i\)</span>. Then, our prediction would be <span class="math display">\[
\hat{y} = \sum_{i=1}^n \alpha_i y_i.
\]</span></p>
<p>The weights <span class="math inline">\(\alpha_i\)</span> assign importance to <span class="math inline">\(y_i\)</span> based on how close <span class="math inline">\(q = (x_1, x_2)\)</span> is to <span class="math inline">\(x^{(i)}\)</span>. Common kernel choices and their formulas are summarized below:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Kernel</th>
<th style="text-align: center;">Formula for <span class="math inline">\(K(q, x^{(i)})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Gaussian</td>
<td style="text-align: center;"><span class="math inline">\(\exp\left(-\frac{\|q - x^{(i)}\|^2}{2h^2}\right)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Boxcar</td>
<td style="text-align: center;"><span class="math inline">\(\mathbb{I}\left(\|q - x^{(i)}\| &lt; h\right)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Constant</td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Epanechikov</td>
<td style="text-align: center;"><span class="math inline">\(\max\left(1 - \frac{\|q - x^{(i)}\|}{h},\ 0\right)\)</span></td>
</tr>
</tbody>
</table>
<p>Here, <span class="math inline">\(h\)</span> is a bandwidth parameter controlling the width of the kernel, and <span class="math inline">\(\mathbb{I}\)</span> is the indicator function. Code for the kernels is given below.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Define some kernels for attention pooling</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian(x):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> boxcar(x):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">abs</span>(x) <span class="op">&lt;</span> <span class="fl">1.0</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> constant(x):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">+</span> <span class="dv">0</span> <span class="op">*</span> x  <span class="co"># noqa: E741</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epanechikov(x):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">abs</span>(x), np.zeros_like(x))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, sharey<span class="op">=</span><span class="va">True</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> (gaussian, boxcar, constant, epanechikov)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> (<span class="st">'Gaussian'</span>, <span class="st">'Boxcar'</span>, <span class="st">'Constant'</span>, <span class="st">'Epanechikov'</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>, <span class="fl">0.1</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> kernel, name, ax <span class="kw">in</span> <span class="bu">zip</span>(kernels, names, axes):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    ax.plot(x, kernel(x))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(name)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-nlp_files/figure-html/unnamed-chunk-6-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<p>Each of these kernels represents a different way of weighting information based on similarity or distance. In neural networks, this translates to learning how to attend to different parts of the input sequence, but with the flexibility to learn much more complex patterns than these simple mathematical functions.</p>
<p>Now, lets plot the true value of the function and the kernel smoothed value.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Kernel Smoothing</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sys.path.append(<span class="st">'code'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nlp_chapter <span class="im">import</span> gaussian_kernel_smoothing</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian(x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate grid data for x1 and x2 (test data)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now compute the grid of x1 and x2 values</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>X1, X2 <span class="op">=</span> np.meshgrid(x1, x2)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate training data</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>x1_train <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>x2_train <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> x1_train) <span class="op">+</span> x2_train<span class="op">*</span>x2_train <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">100</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the function with the existing data</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gaussian_kernel_smoothing(X1, X2, x1_train, x2_train, y_train, bandwidth<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot comparison between true function and kernel smoothing prediction</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># True function (without noise for cleaner visualization)</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.sin(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> X1) <span class="op">+</span> X2<span class="op">*</span>X2</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: True function</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> axes[<span class="dv">0</span>].contourf(X1, X2, y_true, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(x1_train, x2_train, c<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'True Function'</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im1, ax<span class="op">=</span>axes[<span class="dv">0</span>])<span class="op">;</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Kernel smoothing prediction</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>im2 <span class="op">=</span> axes[<span class="dv">1</span>].contourf(X1, X2, y_pred, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(x1_train, x2_train, c<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'x'</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Gaussian Kernel Smoothing'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im2, ax<span class="op">=</span>axes[<span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 3: Difference (error)</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> y_true <span class="op">-</span> y_pred</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>im3 <span class="op">=</span> axes[<span class="dv">2</span>].contourf(X1, X2, diff, levels<span class="op">=</span><span class="dv">20</span>, cmap<span class="op">=</span><span class="st">'RdBu_r'</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].scatter(x1_train, x2_train, c<span class="op">=</span><span class="st">'black'</span>, marker<span class="op">=</span><span class="st">'x'</span>, s<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Error (True - Predicted)'</span>)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im3, ax<span class="op">=</span>axes[<span class="dv">2</span>])<span class="op">;</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="24-nlp_files/figure-html/smoothing-1.png" class="img-fluid figure-img" width="1440"></p>
<figcaption>Kernel Smoothing using Gaussian Kernel</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that a relatively simmple conept of calculating a prediction by averaging the values of the training data, weighted by the kernel function, can be used to estimate the value of the function at a new point rather well. Clearly, model is not performing well at the regions hwere we do not have training data, but it is still able to capture the general shape of the function.</p>
</section>
</div>
<section id="attention-over-a-sequence-of-words" class="level3">
<h3 class="anchored" data-anchor-id="attention-over-a-sequence-of-words">Attention over a sequence of Words</h3>
<p>This elegant mechanism liberates the model from the tyranny of sequential processing. It can look at an entire sequence at once, effortlessly handling text of any length, drawing connections between distant words in a text, and processing information in parallel, making it incredibly efficient to train.</p>
<p>There are two variations of attention that are particularly important:</p>
<ol type="1">
<li>Cross-Attention: allows a model to weigh the importance of words in an input sequence (like an encoder’s output) when generating a new output sequence (in the decoder).</li>
<li>Self-Attention: allows a model to weigh the importance of different words within a single input sequence</li>
</ol>
<p>In self-attention, queries, keys, and values all come from the same sequence. This allows each position in the sequence to attend to all other positions, creating rich representations that capture the complex web of relationships within a single piece of text. When you read a sentence like “The trophy would not fit in the brown suitcase because it was too big,” self-attention helps the model understand that “it” most likely refers to the trophy, not the suitcase, by considering the relationships between all the words simultaneously.</p>
<p>Let’s now consider a sequence of words (or tokens), where each word is represented as a vector (embedding) <span class="math display">\[
H = [h_1, h_2, \ldots, h_n] \in \mathbb{R}^{n \times d}.
\]</span> Here <span class="math inline">\(n\)</span> is the lenght of our sequence to be analyzed. It can be a sentence, a paragraph, a document. In case of self-attention, the trio of <span class="math inline">\(Q, K, V\)</span> are all projections of the same input sequence <span class="math inline">\(H\)</span></p>
<p><span class="math display">\[
Q = H W_Q, \quad K = H W_K, \quad V = H W_V.
\]</span> The projections are done by multiplying the input sequence <span class="math inline">\(H\)</span> by a learnable parameter matrix <span class="math inline">\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}\)</span>. Thus the resulting <span class="math inline">\(Q, K, V\)</span> are of dimension <span class="math inline">\(n \times d_k\)</span>. Now, we can calculate the attention weights as: <span class="math display">\[
\alpha_{ij} = \frac{q_i^T k_j}{\sqrt{d_k}}, \quad i,j = 1, \ldots, n.
\]</span> where <span class="math inline">\(q_i\)</span> is the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(Q\)</span>, <span class="math inline">\(k_j\)</span> is the <span class="math inline">\(j\)</span>-th row of <span class="math inline">\(K\)</span>, and <span class="math inline">\(d_k\)</span> is the dimension of the key vectors. The attention weights are then normalized by the softmax function: <span class="math display">\[
\alpha_{ij} = \frac{\exp(\alpha_{ij})}{\sum_{j=1}^n \exp(\alpha_{ij})}.
\]</span> This softmax normalization ensures that attention weights sum to one, creating a proper probability distribution over the input positions. The scaling factor <span class="math inline">\(1/\sqrt{d_k}\)</span> prevents the dot products from becoming too large, which would push the softmax into regions with extremely small gradients.</p>
<p>The final output is then computed as: <span class="math display">\[
o_i = \sum_{j=1}^n \alpha_{ij} v_j.
\]</span> where <span class="math inline">\(o_i\)</span> is the <span class="math inline">\(i\)</span>-th row of the output.</p>
<p>Notice, that instead of using Gaussian kernel, as we did before, we simply use the dot product. This makes computation much faster and more efficient, while still being able to capture the similarity between the words. Matrix-matrix product operation is implemented in all major deep learning libraries. It is differentiable, ensuring that its gradient remains stable, which is crucial for model training. However, the attention mechanism we discussed is not the sole approach available. For example, one could create a non-differentiable attention model that utilizes reinforcement learning techniques for training, as explored by Mnih et al.&nbsp;(2014). Such models are inherently more complex to train. As a result, most contemporary attention research adheres to the differentiable framework depicted in Fig. 11.1.1. Our discussion will therefore concentrate on these differentiable mechanisms.</p>
<p>The dot product naturally measures the alignment or “similarity” between two vectors. A larger dot product implies the vectors are pointing in a similar direction. When combined with the scaling factor and the softmax function, this simple calculation has proven to be a highly effective way to determine which tokens are most relevant to each other for language processing tasks. It provides a good signal for “unnormalized alignment” before being converted into a probability distribution. In fact, the dot product is a special case of a kernel function. For normalized vectors, the dot product is equivalent to the cosine similarity. <span class="math display">\[
Q_i \cdot K_j = \cos(\theta_{ij})
\]</span> where <span class="math inline">\(\theta_{ij}\)</span> is the angle between the <span class="math inline">\(i\)</span>-th and <span class="math inline">\(j\)</span>-th vectors. It is proportional to <span class="math display">\[
\cos(\theta_{ij}) \propto -\|Q_i - K_j\|^2
\]</span> up to constants. In fact, if <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span> are normalized and <span class="math inline">\(\sigma\)</span> is tuned, the Gaussian kernel becomes almost equivalent to a softmax over dot products. For the specific tasks that attention methods excel at, more complex kernels like the Gaussian kernel haven’t demonstrated a consistent or significant enough improvement in performance to justify their increased computational overhead.</p>
<p>Now instead of calculating norms, we calculate the dot product and instead of having a bandwidth parameter <em>that needs to be tuned</em>, we have projection matrices <span class="math inline">\(W_Q, W_K, W_V\)</span>, that are learned during training.</p>
<div id="ex-attention-weights">
<p>For example, let’s consider the following sentence:</p>
<blockquote class="blockquote">
<p>“The cat sat on the mat”</p>
</blockquote>
<p>A trained attention model will produce a matrix of attention weights, where each row corresponds to a word in the sentence, and each column corresponds to a word in the sentence. The attention weights are then used to compute the output embeddings</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/attention_matrix_svg.svg" class="img-fluid figure-img"></p>
<figcaption>Attention weights for the sentence “The cat sat on the mat”</figcaption>
</figure>
</div>
<p>In examining key attention patterns and linguistic insights, we observe several notable relationships. Firstly, the determiner-noun relationship is exemplified by the word “The” showing the strongest attention to its corresponding noun “cat,” with an attention weight of 0.70. This highlights the model’s capability to capture syntactic dependencies between determiners and their head nouns.</p>
<p>Next, we consider subject-predicate dependencies. The verb “sat” demonstrates a strong attention to its subject “cat,” with a weight of 0.60, indicating that the model effectively learns subject-verb relationships, which are crucial for semantic understanding. Conversely, the attention from “cat” to “sat” is weaker, with a weight of 0.20, illustrating asymmetric attention patterns.</p>
<p>In terms of prepositional phrase structure, the preposition “on” shows a strong attention to its object “mat,” with a weight of 0.50, effectively capturing the prepositional phrase structure. The object “mat” reciprocates with moderate attention to its governing preposition “on,” with a weight of 0.25.</p>
<p>Finally, self-attention patterns reveal that diagonal elements exhibit varying self-attention weights. Notably, “mat” and “cat” display higher self-attention weights of 0.50 and 0.40, respectively, suggesting their importance as content words in contrast to function words.</p>
</div>
<p>Cross-attention is a variant of attention where the queries come from one sequence, and the keys and values come from another sequence. Then the weight <span class="math inline">\(\alpha_{ij}\)</span> is calculated as: <span class="math display">\[
\alpha_{ij} = \frac{q_i^T k_j}{\sqrt{d_k}}, \quad i,j = 1, \ldots, n,
\]</span> and measures how important is the <span class="math inline">\(j\)</span>-th word in the second sequence for the <span class="math inline">\(i\)</span>-th word in the first sequence. The attention weights are then normalized by the softmax function: <span class="math display">\[
\]</span> _{ij} = . $$</p>
</section>
<div id="ex-cross-attention">
<section id="cross-attention-for-translation" class="level2" data-number="24.9">
<h2 data-number="24.9" class="anchored" data-anchor-id="cross-attention-for-translation"><span class="header-section-number">24.9</span> Cross-attention for Translation</h2>
<p>In the context of sequence-to-sequence models, one of the key challenges is effectively aligning words and phrases between the source and target languages. This alignment is crucial for ensuring that the translated output maintains the intended meaning and grammatical structure of the original text. Cross-attention mechanisms address this problem by allowing the model to focus on relevant parts of the source sequence when generating each word in the target sequence. This selective focus helps in capturing the nuances of translation, such as word order differences and syntactic variations, which are common in multilingual contexts.</p>
<p>The sequence-to-sequence is probably the most commonly used application of language models. Example include machine translation, text summarization, and text generation from a prompt, question-answering, and chatbots.</p>
<p>Consider a problem of translating ``The cat sleeps’’ into French. The encoder will produce a sequence of embeddings for the source sentence, and the decoder will produce a sequence of embeddings for the target sentence. The cross-attention mechanism will use English embeddings as keys and values, and French embeddings as queries. The attention weights will be calculated as shown in <a href="#fig-cross-attention-matrix" class="quarto-xref">Figure&nbsp;<span>24.1</span></a>.</p>
<div id="fig-cross-attention-matrix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-attention-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/cross_attention_matrix.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-attention-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24.1: Cross-attention matrix for the translation of “The cat sleeps” into French
</figcaption>
</figure>
</div>
<p>The rows are queries, and are represented by the French decoder tokens [“Le”, “chat”, “dort”]. Columns, which serve as Keys and Values, are represented by the English encoder tokens [“The”, “cat”, “sleeps”, “<eos>”]. The attention matrix is non-square, with dimensions of 3×4, corresponding to the decoder length and encoder length, respectively.</eos></p>
<p>Key Attention Patterns:</p>
<ol type="1">
<li>Word-by-Word Alignment:
<ul>
<li>The French word “Le” aligns with the English word “The” with an attention weight of 0.85, indicating that determiners align across languages.</li>
<li>The French word “chat” aligns with the English word “cat” with an attention weight of 0.80, demonstrating a direct semantic translation.</li>
<li>The French word “dort” aligns with the English word “sleeps” with an attention weight of 0.75, showing that verb concepts align.</li>
</ul></li>
<li>Cross-Linguistic Dependencies:
<ul>
<li>Each French word primarily focuses on its English equivalent, with minimal attention given to unrelated source words. This pattern reflects the model’s ability to learn translation alignments.</li>
</ul></li>
</ol>
</section>
</div>
<section id="multi-head-attention-parallel-perspectives" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention-parallel-perspectives">Multi-Head Attention: Parallel Perspectives</h3>
<p>While single-head attention captures one type of relationship between positions, real language exhibits multiple types of dependencies simultaneously. Consider the sentence “The student who studied hard passed the exam.” We might want to simultaneously track: - Syntactic relationships (subject-verb agreement between “student” and “passed”) - Semantic relationships (causal connection between “studied hard” and “passed”)<br>
- Coreference relationships (resolution of “who” to “student”)</p>
<p>Multi-head attention addresses this by computing multiple attention functions in parallel, each potentially specializing in different types of relationships:</p>
<p><span class="math display">\[\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O\]</span></p>
<p>where each head is computed as:</p>
<p><span class="math display">\[\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_Q^i, \mathbf{K}\mathbf{W}_K^i, \mathbf{V}\mathbf{W}_V^i)\]</span></p>
<p>The projection matrices <span class="math inline">\(\mathbf{W}_Q^i, \mathbf{W}_K^i, \mathbf{W}_V^i\)</span> are typically chosen so that <span class="math inline">\(d_k = d_v = d_{\text{model}}/h\)</span>, keeping the total computational cost comparable to single-head attention with full dimensionality.</p>
<p>The final linear projection <span class="math inline">\(\mathbf{W}_O\)</span> allows the model to learn how to combine information from different heads. This design enables the model to attend to different representation subspaces simultaneously, capturing multiple types of relationships that might be difficult for a single attention head to model.</p>
</section>
<section id="positional-information-in-attention" class="level3">
<h3 class="anchored" data-anchor-id="positional-information-in-attention">Positional Information in Attention</h3>
<p>Unlike recurrent networks that process sequences step by step, attention mechanisms are permutation-invariant—they produce the same output regardless of input order. While this property enables parallelization, it discards crucial positional information essential for language understanding.</p>
<p>The Transformer architecture addresses this through <strong>positional encodings</strong> that are added to the input embeddings before attention computation. The original formulation uses sinusoidal encodings:</p>
<p><span class="math display">\[\text{PE}(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)\]</span> <span class="math display">\[\text{PE}(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)\]</span></p>
<p>where <span class="math inline">\(\text{pos}\)</span> is the position and <span class="math inline">\(i\)</span> is the dimension. This encoding has several desirable properties: - Different frequencies allow the model to distinguish positions at different scales - The sinusoidal structure enables the model to extrapolate to longer sequences than seen during training - The encoding is deterministic and doesn’t require additional parameters</p>
<p>Alternative approaches include learned positional embeddings or relative position encodings that explicitly model the distance between positions rather than their absolute locations.</p>
</section>
<section id="computational-efficiency-and-practical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="computational-efficiency-and-practical-considerations">Computational Efficiency and Practical Considerations</h3>
<p>The quadratic complexity of self-attention has motivated numerous efficiency improvements. <strong>Sparse attention</strong> patterns restrict which positions can attend to each other, reducing complexity while maintaining representational power. For example, local attention only allows positions to attend within a fixed window, while strided attention creates dilated patterns that maintain long-range connectivity.</p>
<p><strong>Linear attention</strong> approximates the full attention matrix using low-rank decompositions or kernel approximations, reducing complexity to <span class="math inline">\(O(nd)\)</span>. While these methods sacrifice some representational capacity, they enable processing of much longer sequences.</p>
<p><strong>Gradient accumulation</strong> and <strong>mixed precision training</strong> have become standard practices for training large attention-based models. The former allows simulation of larger batch sizes on limited hardware, while the latter uses 16-bit floating point for most operations while maintaining 32-bit precision for critical computations.</p>
</section>
<section id="from-attention-to-modern-language-models" class="level3">
<h3 class="anchored" data-anchor-id="from-attention-to-modern-language-models">From Attention to Modern Language Models</h3>
<p>The attention mechanism’s success stems from its ability to create flexible, context-dependent representations while maintaining computational efficiency through parallelization. This foundation enabled the development of increasingly sophisticated architectures:</p>
<ul>
<li><strong>BERT</strong> uses bidirectional self-attention encoders for contextual understanding</li>
<li><strong>GPT</strong> employs causal self-attention decoders for autoregressive generation<br>
</li>
<li><strong>T5</strong> combines encoder-decoder architectures for sequence-to-sequence tasks</li>
</ul>
<p>Each of these models builds on the query-key-value framework, demonstrating its fundamental importance to modern NLP. The attention mechanism’s interpretability—we can visualize attention weights to understand what the model focuses on—provides valuable insights into model behavior and has influenced model design and debugging practices.</p>
<p>The evolution from fixed word embeddings to dynamic, contextual representations through attention mechanisms represents one of the most significant advances in computational linguistics. By enabling models to adaptively focus on relevant information, attention has unlocked capabilities that seemed impossible just a decade ago,</p>
</section>
<section id="transformer-architecture" class="level2" data-number="24.10">
<h2 data-number="24.10" class="anchored" data-anchor-id="transformer-architecture"><span class="header-section-number">24.10</span> Transformer Architecture</h2>
<p>The Transformer represents a fundamental shift in how we approach neural network architectures for sequential data. Introduced in the seminal paper “Attention is All You Need” by <span class="citation" data-cites="vaswani2023attention">Vaswani et al. (<a href="references.html#ref-vaswani2023attention" role="doc-biblioref">2023</a>)</span> in 2017, the Transformer has since become the backbone of virtually every state-of-the-art language model, from OpenAI’s GPT series to Google’s Gemini and Meta’s Llama. Beyond text generation, Transformers have proven remarkably versatile, finding applications in audio generation, image recognition, protein structure prediction, and even game playing.</p>
<p>At its core, text-generative Transformer models operate on a deceptively simple principle: <strong>next-word prediction</strong>. Given a text prompt from the user, the model’s fundamental task is to determine the most probable next word that should follow the input. This seemingly straightforward objective, when scaled to billions of parameters and trained on vast text corpora, gives rise to emergent capabilities that often surprise even their creators.</p>
<p>The first step in the Transformers is the <strong>self-attention mechanism</strong>, which allows them to process entire sequences simultaneously rather than sequentially. This parallel processing capability not only enables more efficient training on modern hardware but also allows the model to capture long-range dependencies more effectively than previous architectures like recurrent neural networks.</p>
<p>While modern language models like GPT-4 contain hundreds of billions of parameters, the fundamental architectural principles can be understood through smaller models like GPT-2. The GPT-2 small model, with its 124 million parameters, shares the same core components and principles found in today’s most powerful models, making it an ideal vehicle for understanding the underlying mechanics.</p>
<p>The figure below shows the schematic representation of a tranformer architecture.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Input text] --&gt; T[Toeknizer]
    T --&gt; B[Embedding &lt;br&gt; Layer]
    B --&gt; P[Positional &lt;br&gt; Encoding]
    P --&gt; C1[Self-Attention]
    P --&gt; C2[Self-Attention]
    P --&gt; C3[Self-Attention]
    C1 --&gt; M1[Multi-Layer&lt;br&gt;Perceptron]
    C2 --&gt; M2[Multi-Layer&lt;br&gt;Perceptron]
    C3 --&gt; M3[Multi-Layer&lt;br&gt;Perceptron]
    M1 --&gt; D[Output&lt;br&gt;Probabilities]
    M2 --&gt; D
    M3 --&gt; D
    D --&gt; E[Generated&lt;br&gt;Text]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Transformer consists of three fundamental components that work in concert to transform raw text into meaningful predictions. The first step involves converting text to numbers (<code>Toknizer</code>, <code>Embedding Layer</code>, <code>Positional Encoding</code>). The second step involves the <code>Self-Attention</code> mechanism. The third step involves the <code>Multi-Layer Perceptron</code> (MLP). The final step involves the <code>Output Probabilities</code>. We have covered the first two steps in the previous section. Now we will cover the third step, the <code>Multi-Layer Perceptron</code>. Following the attention mechanism, each token’s representation passes through a position-wise feedforward network. The MLP serves a different purpose than attention: while attention routes information between tokens, the MLP transforms each token’s representation independently.</p>
<p>The MLP consists of two linear transformations with a GELU activation function:</p>
<p><span class="math display">\[\text{MLP}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2\]</span></p>
<p>The first transformation expands the dimensionality, allowing the model to use a higher-dimensional space for computation before projecting back to the original size. This expansion provides the representational capacity needed for complex transformations while maintaining consistent dimensions across layers.</p>
<p>After processing through all Transformer blocks, the final step converts the rich contextual representations back into predictions over the vocabulary. This involves two key operations. First, The final layer representations are projected into vocabulary space using a linear transformation:</p>
<p><span class="math display">\[\text{logits} = \text{final\_representations} \cdot W_{\text{output}} + b_{\text{output}}\]</span></p>
<p>This produces a vector of <span class="math inline">\(|V|\)</span> values (one for each token in the vocabulary) called logits, representing the model’s raw preferences for each possible next token.</p>
<p>The logits are converted into a probability distribution using the softmax function:</p>
<p><span class="math display">\[P(\text{token}_i) = \frac{\exp(\text{logit}_i)}{\sum_{j=1}^{|\mathcal{V}|} \exp(\text{logit}_j)}\]</span></p>
<p>This creates a valid probability distribution where all values sum to 1, allowing us to sample the next token based on the model’s learned preferences.</p>
<p>While the three main components form the core of the Transformer, several additional mechanisms enhance stability and performance:</p>
<p><strong>Layer Normalization</strong></p>
<p>Applied twice in each Transformer block (before attention and before the MLP), layer normalization stabilizes training by normalizing activations across the feature dimension:</p>
<p><span class="math display">\[\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \odot \gamma + \beta\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the mean and standard deviation computed across the feature dimension, and <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learned scaling and shifting parameters.</p>
<p><strong>Residual Connections</strong></p>
<p>Residual connections create shortcuts that allow gradients to flow directly through the network during training:</p>
<p><span class="math display">\[\text{output} = \text{LayerNorm}(x + \text{Sublayer}(x))\]</span></p>
<p>These connections are crucial for training deep networks, preventing the vanishing gradient problem that would otherwise make it difficult to update parameters in early layers.</p>
<p><strong>Dropout</strong></p>
<p>During training, dropout randomly zeros out a fraction of neuron activations, preventing overfitting and encouraging the model to learn more robust representations. Dropout is typically disabled during inference.</p>
<section id="computational-complexity-and-scalability" class="level3">
<h3 class="anchored" data-anchor-id="computational-complexity-and-scalability">Computational Complexity and Scalability</h3>
<p>The Transformer architecture’s quadratic complexity with respect to sequence length (<span class="math inline">\(O(n^2)\)</span> for self-attention) has motivated numerous efficiency improvements. However, this complexity enables the model’s remarkable ability to capture long-range dependencies that simpler architectures cannot handle effectively.</p>
<p>Modern implementations leverage optimizations like: - <strong>Gradient checkpointing</strong> to trade computation for memory - <strong>Mixed precision training</strong> using 16-bit floating point - <strong>Efficient attention implementations</strong> that minimize memory usage</p>
<p>The Transformer’s success stems from its elegant balance of expressivity and computational efficiency. By enabling parallel processing while maintaining the ability to model complex dependencies, it has become the foundation for the current generation of large language models that are reshaping our interaction with artificial intelligence.</p>
<p>The complete Transformer architecture consists of two main components working in harmony. The encoder stack processes input sequences through multiple layers of multi-head self-attention and position-wise feed-forward networks, enhanced with residual connections and layer normalization that help stabilize training. The decoder stack uses masked multi-head self-attention to prevent the model from “cheating” by looking at future tokens during training.</p>
<p>This architecture enabled the development of three distinct families of models, each optimized for different types of tasks. Encoder-only models like BERT excel at understanding tasks such as classification, question answering, and sentiment analysis. They can see the entire input at once, making them particularly good at tasks where understanding context from both directions matters.</p>
<p>Decoder-only models like GPT are particularly good at generation tasks, producing coherent, contextually appropriate text. These models are trained to predict the next token given all the previous tokens in a sequence, which might seem simple but turns out to be incredibly powerful for natural text generation.</p>
<p>Encoder-decoder models like T5 bridge both worlds, excelling at sequence-to-sequence tasks like translation and summarization. The text-to-text approach treats all tasks as text generation problems. Need to translate from English to French? The model learns to generate French text given English input. Want to answer a question? The model generates an answer given a question and context.</p>
</section>
</section>
<section id="pretraining-at-scale-bert-and-beyond" class="level2" data-number="24.11">
<h2 data-number="24.11" class="anchored" data-anchor-id="pretraining-at-scale-bert-and-beyond"><span class="header-section-number">24.11</span> Pretraining at Scale: BERT and Beyond</h2>
<p>BERT, which stands for Bidirectional Encoder Representations from Transformers, is a groundbreaking model in the field of natural language processing (NLP). Developed by Google, BERT is designed to pretrain deep bidirectional representations of words by jointly conditioning on both left and right context in all layers. This approach allows BERT to capture the context of a word based on its surrounding words, making it highly effective for understanding the nuances of language.</p>
<p>The model’s architecture consists of multiple layers of bidirectional transformers, which allow it to learn complex patterns and relationships in text. BERT’s pretraining involves two main tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, some percentage of the input tokens are masked at random, and the model learns to predict these masked tokens. NSP involves predicting whether a given pair of sentences are consecutive in the original text.</p>
<p>BERT’s ability to be fine-tuned for specific tasks with relatively small amounts of data has made it a versatile and powerful tool in the NLP toolkit, leading to significant improvements in performance across a wide range of applications.</p>
<p>The availability of powerful architectures raised a crucial question: how can we best leverage unlabeled text to learn general-purpose representations? BERT (Bidirectional Encoder Representations from Transformers) introduced a pretraining framework that has become the foundation for modern NLP.</p>
<p>BERT’s key innovation was masked language modeling (MLM), where 15% of tokens are selected for prediction. For each selected token, 80% are replaced with [MASK], 10% with random tokens, and 10% left unchanged. This prevents the model from simply learning to copy tokens when they’re not masked. The loss function only considers predictions for masked positions:</p>
<p><span class="math display">\[\mathcal{L}_{\text{MLM}} = -\sum_{m \in \mathcal{M}} \log P(x_m \mid \mathbf{x}_{\backslash \mathcal{M}})\]</span></p>
<p>BERT combines MLM with next sentence prediction (NSP), which trains the model to understand relationships between sentence pairs. Training examples contain 50% consecutive sentences and 50% randomly paired sentences. The input representation concatenates both sentences with special tokens and segment embeddings to distinguish between them.</p>
<p>The scale of BERT pretraining represents a quantum leap from earlier approaches. The original BERT models were trained on a combination of BookCorpus (800 million words from over 11,000 books) and English Wikipedia (2,500 million words). This massive dataset enables the model to see diverse writing styles, topics, and linguistic phenomena. The preprocessing pipeline removes duplicate paragraphs, filters very short or very long sentences, and maintains document boundaries to ensure coherent sentence pairs for NSP.</p>
<section id="bert-architecture-and-training-details" class="level3">
<h3 class="anchored" data-anchor-id="bert-architecture-and-training-details">BERT Architecture and Training Details</h3>
<p>To understand BERT’s architectural significance, it’s helpful to compare it with its predecessors ELMo and GPT, which represent different approaches to contextual representation learning:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph col1 [" "]
        subgraph ELMo ["ELMo"]
            direction TB
            E1["BiLSTM&lt;br/&gt;Layer 1"]
            E2["BiLSTM&lt;br/&gt;Layer 2"]
            E3["Task-specific&lt;br/&gt;Architecture"]
            E4["Output"]
            
            E1 --&gt; E2
            E2 --&gt; E3
            E3 --&gt; E4
        end
        
        style E1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style E2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style E3 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style E4 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    subgraph col2 [" "]
        subgraph GPT ["GPT"]
            direction TB
            G1["Transformer&lt;br/&gt;Layer 1"]
            G2["Transformer&lt;br/&gt;Layer 2"]
            G3["Transformer&lt;br/&gt;Layer 3"]
            G4["Linear&lt;br/&gt;Output"]
            G5["Output"]
            
            G1 --&gt; G2
            G2 --&gt; G3
            G3 --&gt; G4
            G4 --&gt; G5
        end
        
        style G1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style G5 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    subgraph col3 [" "]
        subgraph BERT ["BERT"]
            direction TB
            B1["Transformer&lt;br/&gt;Encoder 1"]
            B2["Transformer&lt;br/&gt;Encoder 2"]
            B3["Transformer&lt;br/&gt;Encoder 3"]
            B4["Linear&lt;br/&gt;Output"]
            B5["Output"]
            
            B1 --&gt; B2
            B2 --&gt; B3
            B3 --&gt; B4
            B4 --&gt; B5
        end
        
        style B1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style B5 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    style col1 fill:none,stroke:none
    style col2 fill:none,stroke:none
    style col3 fill:none,stroke:none
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>ELMo uses bidirectional LSTMs with task-specific architectures, requiring custom model design for each application. GPT employs a unidirectional Transformer decoder that processes text left-to-right, making it task-agnostic but unable to see future context. BERT combines the best of both: bidirectional context understanding through Transformer encoders with minimal task-specific modifications (just an output layer). BERT comes in two main configurations that balance model capacity with computational requirements:</p>
<table class="caption-top table">
<caption>Bert configurations and parameters</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 20%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Transformer Layers</th>
<th>Hidden Dimensions</th>
<th>Attention Heads</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-Base</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>110M</td>
</tr>
<tr class="even">
<td>BERT-Large</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>340M</td>
</tr>
</tbody>
</table>
<p>Both models use a vocabulary of 30,000 WordPiece tokens, learned using a data-driven tokenization algorithm similar to BPE. The maximum sequence length is 512 tokens, though most pretraining uses sequences of 128 tokens to improve efficiency, with only the final 10% of training using full-length sequences.</p>
<p>The pretraining procedure involves several techniques to stabilize and accelerate training:</p>
<ol type="1">
<li>Warm-up Learning Rate: The learning rate increases linearly for the first 10,000 steps to <span class="math inline">\(10^{-4}\)</span>, then decreases linearly. This warm-up prevents large gradients early in training when the model is randomly initialized.</li>
<li>Gradient Accumulation: To simulate larger batch sizes on limited hardware, gradients are accumulated over multiple forward passes before updating weights. BERT uses an effective batch size of 256 sequences.</li>
<li>Mixed Precision Training: Using 16-bit floating point for most computations while maintaining 32-bit master weights speeds up training significantly on modern GPUs.</li>
</ol>
</section>
<section id="data-preparation-for-bert-pretraining" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-for-bert-pretraining">Data Preparation for BERT Pretraining</h3>
<p>The data preparation pipeline for BERT is surprisingly complex. Starting with raw text, the process involves:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Raw Text] --&gt; B[Document Segmentation]
    B --&gt; C[Sentence Segmentation]
    C --&gt; D[WordPiece Tokenization]
    D --&gt; E[Creating Training Examples]
    E --&gt; F[Creating TFRecord Files]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>In document segmentation stage, text is split into documents, maintaining natural boundaries. For books, this means chapter boundaries; for Wikipedia, article boundaries. The next stage is sentence segmentation, where each document is split into sentences using heuristic rules. Typically, this involves identifying periods followed by whitespace and capital letters, while accounting for exceptions such as abbreviations.</p>
<p>Following sentence segmentation, the text undergoes WordPiece tokenization. This process uses a learned WordPiece vocabulary to break text into subword units, ensuring that unknown words can be represented as sequences of known subwords. Special handling is applied to mark the beginning of words.</p>
<p>Once tokenized, the data is organized into training examples. For each example, a target sequence length is sampled from a geometric distribution to introduce variability. Sentences are packed together until the target length is reached. For the next sentence prediction (NSP) task, half of the examples use the actual next segment, while the other half use a randomly selected segment. Masked language modeling (MLM) is applied by masking 15% of tokens, following the 80/10/10 strategy: 80% of the time the token is replaced with [MASK], 10% with a random token, and 10% left unchanged. Special tokens such as [CLS] at the beginning and [SEP] between segments are added, and segment embeddings are created (0 for the first segment, 1 for the second). Sequences are padded to a fixed length using [PAD] tokens.</p>
<p>Finally, the prepared examples are serialized into TFRecord files for efficient input/output during training. Examples are grouped by sequence length to minimize the amount of padding required.</p>
<p>The democratization of pretraining through libraries like Hugging Face Transformers has made it possible for smaller organizations to leverage these powerful techniques, either by fine-tuning existing models or pretraining specialized models for their domains.</p>
</section>
</section>
<section id="transfer-learning-and-downstream-applications" class="level2" data-number="24.12">
<h2 data-number="24.12" class="anchored" data-anchor-id="transfer-learning-and-downstream-applications"><span class="header-section-number">24.12</span> Transfer Learning and Downstream Applications</h2>
<p>The power of pretrained models lies in their transferability. For sentiment analysis, we add a linear layer on top of the [CLS] token representation and fine-tune on labeled data. Popular datasets include IMDb movie reviews (50K examples) and Stanford Sentiment Treebank (11,855 sentences). Fine-tuning typically requires only 2-4 epochs, demonstrating the effectiveness of transfer learning.</p>
<p>Natural language inference (NLI) determines logical relationships between premise and hypothesis sentences. The Stanford Natural Language Inference corpus contains 570,000 sentence pairs labeled as entailment, contradiction, or neutral. For BERT-based NLI, we concatenate premise and hypothesis with [SEP] tokens and classify using the [CLS] representation.</p>
<p>Token-level tasks like named entity recognition classify each token independently. Common datasets include CoNLL-2003 (English and German entities) and OntoNotes 5.0 (18 entity types). The BIO tagging scheme marks entity boundaries: B-PER for beginning of person names, I-PER for inside, and O for outside any entity.</p>
<p>Question answering presents unique challenges. The SQuAD dataset contains 100,000+ questions where answers are text spans from Wikipedia articles. BERT approaches this by predicting start and end positions independently, with the final answer span selected to maximize the product of start and end probabilities subject to length constraints.</p>
</section>
<section id="model-compression-and-efficiency" class="level2" data-number="24.13">
<h2 data-number="24.13" class="anchored" data-anchor-id="model-compression-and-efficiency"><span class="header-section-number">24.13</span> Model Compression and Efficiency</h2>
<p>While large pretrained models achieve impressive performance, their computational requirements limit deployment. Knowledge distillation trains a small “student” model to mimic a large “teacher” model through a combined loss:</p>
<p><span class="math display">\[\mathcal{L}_{\text{distill}} = \alpha \mathcal{L}_{\text{task}} + (1-\alpha) \text{KL}(p_{\text{teacher}} \| p_{\text{student}})\]</span></p>
<p>DistilBERT achieves 97% of BERT’s performance with 40% fewer parameters and 60% faster inference. Quantization reduces numerical precision from 32-bit to 8-bit or even lower, while pruning removes connections below a magnitude threshold. These techniques can reduce model size by an order of magnitude with minimal performance degradation.</p>
</section>
<section id="theoretical-perspectives-and-future-directions" class="level2" data-number="24.14">
<h2 data-number="24.14" class="anchored" data-anchor-id="theoretical-perspectives-and-future-directions"><span class="header-section-number">24.14</span> Theoretical Perspectives and Future Directions</h2>
<p>The success of language models connects to several theoretical frameworks. Transformers are universal approximators for sequence-to-sequence functions—given sufficient capacity, they can approximate any continuous function to arbitrary precision. The self-attention mechanism provides an inductive bias well-suited to capturing long-range dependencies.</p>
<p>Despite having hundreds of millions of parameters, these models generalize remarkably well. This connects to implicit regularization in overparameterized models, where gradient descent dynamics bias toward solutions with good generalization properties. Language models automatically learn hierarchical features: early layers capture syntax and morphology, middle layers semantic relationships, and later layers task-specific abstractions.</p>
<p>Yet significant challenges remain. Models struggle with compositional generalization—understanding “red car” and “blue house” doesn’t guarantee understanding “red house” if that combination is rare in training. Sample efficiency remains poor compared to human learning. A child masters basic grammar from thousands of examples; BERT sees billions. This gap suggests fundamental differences in learning mechanisms.</p>
<p>Interpretability poses ongoing challenges. While attention visualizations provide some insights, we lack principled methods for understanding distributed representations across hundreds of layers and attention heads. Future directions include multimodal understanding (integrating text with vision and speech), more efficient architectures that maintain performance while reducing computational requirements, and developing theoretical frameworks to predict and understand model behavior.</p>
</section>
<section id="natural-language-processing-applications" class="level2" data-number="24.15">
<h2 data-number="24.15" class="anchored" data-anchor-id="natural-language-processing-applications"><span class="header-section-number">24.15</span> Natural Language Processing: Applications</h2>
<p>Having established the theoretical foundations and pretraining methodologies for natural language understanding, we now turn to the practical application of these techniques to solve real-world problems. The power of pretrained representations lies not merely in their mathematical elegance, but in their ability to transfer learned linguistic knowledge to diverse downstream tasks with minimal architectural modifications.</p>
<p>The landscape of NLP applications can be broadly categorized into two fundamental types based on their input structure and prediction granularity. <strong>Sequence-level tasks</strong> operate on entire text sequences, producing a single output per input sequence or sequence pair. These include sentiment classification, where we determine the emotional polarity of a review, and natural language inference, where we assess logical relationships between premise-hypothesis pairs. <strong>Token-level tasks</strong> make predictions for individual tokens within sequences, such as named entity recognition that identifies person names, locations, and organizations, or question answering that pinpoints answer spans within passages.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph "Pretrained Representations" ["Pretrained Text Representations"]
        P1[Word2Vec/GloVe] 
        P2[BERT/RoBERTa]
        P3[GPT/T5]
    end
    
    subgraph "Deep Learning Architectures" ["Model Architectures"]
        A1[MLPs]
        A2[CNNs] 
        A3[RNNs]
        A4[Transformers]
        A5[Attention Mechanisms]
    end
    
    subgraph "Sequence-Level Tasks" ["Sequence-Level Applications"]
        S1[Sentiment Analysis]
        S2[Text Classification]
        S3[Natural Language Inference]
        S4[Semantic Similarity]
    end
    
    subgraph "Token-Level Tasks" ["Token-Level Applications"]
        T1[Named Entity Recognition]
        T2[Part-of-Speech Tagging]
        T3[Question Answering]
        T4[Text Summarization]
    end
    
    P1 --&gt; A1
    P1 --&gt; A2
    P1 --&gt; A3
    P2 --&gt; A4
    P2 --&gt; A5
    P3 --&gt; A4
    
    A1 --&gt; S1
    A1 --&gt; S3
    A2 --&gt; S1
    A2 --&gt; S2
    A3 --&gt; S1
    A3 --&gt; S3
    A4 --&gt; S2
    A4 --&gt; S3
    A4 --&gt; T1
    A4 --&gt; T3
    A5 --&gt; S3
    A5 --&gt; T3
    
    style P1 fill:#e8f4f8
    style P2 fill:#e8f4f8
    style P3 fill:#e8f4f8
    style A1 fill:#f0f8e8
    style A2 fill:#f0f8e8
    style A3 fill:#f0f8e8
    style A4 fill:#f0f8e8
    style A5 fill:#f0f8e8
    style S1 fill:#fdf2e8
    style S2 fill:#fdf2e8
    style S3 fill:#fdf2e8
    style S4 fill:#fdf2e8
    style T1 fill:#f8e8f4
    style T2 fill:#f8e8f4
    style T3 fill:#f8e8f4
    style T4 fill:#f8e8f4
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This architectural flexibility represents one of the most significant advantages of the representation learning paradigm. Unlike earlier rule-based or feature-engineering approaches that required domain-specific expertise for each task, modern NLP systems can leverage the same pretrained representations across vastly different applications. A BERT model pretrained on general text can be fine-tuned for medical document classification, legal contract analysis, or social media sentiment detection with only task-specific output layers.</p>
<section id="sentiment-analysis-from-opinions-to-insights" class="level3">
<h3 class="anchored" data-anchor-id="sentiment-analysis-from-opinions-to-insights">Sentiment Analysis: From Opinions to Insights</h3>
<p>Sentiment analysis exemplifies the practical value of learned text representations. Consider the challenge of processing customer reviews for a major e-commerce platform. Traditional approaches might rely on hand-crafted lexicons of positive and negative words, but such methods fail to capture context-dependent sentiment. The phrase “not bad” expresses mild approval despite containing the negative word “bad,” while “insanely good” uses typically negative intensity (“insanely”) to convey strong positive sentiment.</p>
<p>Modern sentiment analysis systems leverage pretrained embeddings that capture these nuanced semantic relationships. A convolutional neural network architecture can effectively identify local sentiment-bearing phrases through learned filters, while recurrent networks model the sequential dependencies that determine how sentiment evolves throughout a text. The Stanford Sentiment Treebank, with its fine-grained annotations ranging from very negative to very positive, provides a testing ground where contemporary models achieve accuracy levels approaching human inter-annotator agreement.</p>
</section>
<section id="natural-language-inference-reasoning-about-meaning" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-inference-reasoning-about-meaning">Natural Language Inference: Reasoning About Meaning</h3>
<p>Natural language inference represents a more sophisticated challenge that requires understanding logical relationships between text pairs. Given a premise “The company’s profits increased by 15% this quarter” and a hypothesis “The business is performing well,” a model must determine whether the hypothesis follows from the premise (entailment), contradicts it, or remains neutral.</p>
<p>The Stanford Natural Language Inference corpus provides over 570,000 such premise-hypothesis pairs, enabling large-scale training of inference models. Successful approaches often employ attention mechanisms to align relevant portions of premises with hypotheses, identifying which words and phrases support or contradict the proposed logical relationship. The decomposable attention model, for instance, computes element-wise attention between premise and hypothesis tokens, allowing fine-grained comparison of semantic content.</p>
</section>
<section id="token-level-applications-precision-at-the-word-level" class="level3">
<h3 class="anchored" data-anchor-id="token-level-applications-precision-at-the-word-level">Token-Level Applications: Precision at the Word Level</h3>
<p>Token-level tasks require models to make predictions for individual words or subwords within sequences. Named entity recognition illustrates this paradigm: given the sentence “Apple Inc.&nbsp;was founded by Steve Jobs in Cupertino,” a model must identify “Apple Inc.” as an organization, “Steve Jobs” as a person, and “Cupertino” as a location.</p>
<p>The BIO (Begin-Inside-Outside) tagging scheme provides a standard framework for such tasks. The word “Apple” receives the tag “B-ORG” (beginning of organization), “Inc.” gets “I-ORG” (inside organization), “Steve” becomes “B-PER” (beginning of person), and so forth. This encoding allows models to handle multi-word entities while maintaining the token-level prediction structure.</p>
<p>Question answering presents another compelling token-level application. In extractive question answering, models must identify spans within passages that answer given questions. The SQuAD dataset poses questions about Wikipedia articles, requiring models to pinpoint exact text spans as answers. For example, given the passage about Apple Inc.&nbsp;and the question “Who founded Apple?”, the model should identify “Steve Jobs” as the answer span.</p>
</section>
<section id="fine-tuning-pretrained-models" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-pretrained-models">Fine-Tuning Pretrained Models</h3>
<p>The emergence of large-scale pretrained models like BERT has revolutionized the application landscape. Rather than training task-specific models from scratch, practitioners can fine-tune pretrained representations on downstream tasks with remarkable efficiency. This approach typically requires only 2-4 epochs of training on task-specific data, compared to the hundreds of epochs needed for training from random initialization.</p>
<p>The fine-tuning process involves freezing most pretrained parameters while allowing task-specific layers to adapt. For sequence classification, this might involve adding a single linear layer on top of BERT’s [CLS] token representation. For token-level tasks like named entity recognition, each token’s representation feeds through the same classification layer to produce per-token predictions.</p>
<p>This transfer learning paradigm has democratized access to state-of-the-art NLP capabilities. Organizations without massive computational resources can leverage pretrained models fine-tuned on their specific domains, achieving performance that would have required substantial research and development investments just a few years ago.</p>
</section>
<section id="challenges-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-future-directions">Challenges and Future Directions</h3>
<p>Despite remarkable progress, significant challenges remain in NLP applications. Models often exhibit brittleness when faced with adversarial examples or distribution shifts between training and deployment data. A sentiment classifier trained on movie reviews might fail on product reviews due to domain-specific language patterns. Similarly, named entity recognition systems trained on news articles may struggle with social media text due to informal language and unconventional capitalization.</p>
<p>Computational efficiency presents another ongoing concern. While large pretrained models achieve impressive performance, their size and inference requirements limit deployment in resource-constrained environments. Knowledge distillation and model pruning techniques offer promising approaches for creating smaller, faster models that retain much of the original performance.</p>
<p>The interpretability challenge looms large as models become more complex. Understanding why a model makes specific predictions becomes crucial for high-stakes applications like medical diagnosis or legal document analysis. Attention visualizations provide some insight, but we lack comprehensive frameworks for understanding decision-making processes in large neural networks.</p>
<p>Future directions point toward multimodal understanding that integrates text with visual and auditory information, few-shot learning that adapts quickly to new domains with minimal examples, and more robust architectures that maintain performance across diverse linguistic contexts. The field continues evolving rapidly, driven by the interplay between theoretical advances in representation learning and practical demands from real-world applications.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="24.16">
<h2 data-number="24.16" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">24.16</span> Conclusion</h2>
<p>The journey from symbolic manipulation to neural language understanding represents one of the great success stories of modern artificial intelligence. By reconceptualizing language as geometry in high-dimensional spaces, leveraging self-supervision at scale, and developing powerful architectural innovations like transformers, the field has achieved capabilities that seemed like science fiction just a decade ago.</p>
<p>The mathematical frameworks developed—from distributional semantics to attention mechanisms—provide not just engineering tools but lenses through which to examine fundamental questions about meaning and understanding. As these systems become more capable and widely deployed, understanding their theoretical foundations, practical limitations, and societal implications becomes ever more critical.</p>
<p>The techniques discussed in this chapter—word embeddings, contextual representations, pretraining, and fine-tuning—form the foundation of modern NLP systems. Yet despite impressive engineering achievements, we’ve only begun to scratch the surface of true language understanding. The rapid progress offers both tremendous opportunities and sobering responsibilities. The most exciting chapters in this story are yet to be written.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural <span>Machine Translation</span> by <span>Jointly Learning</span> to <span>Align</span> and <span>Translate</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient <span>Estimation</span> of <span>Word Representations</span> in <span>Vector Space</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-vaswani2023attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. <span>“Attention <span>Is All You Need</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./23-cnn.html" class="pagination-link" aria-label="Convolutional Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./25-llm.html" class="pagination-link" aria-label="Large Language Models: A Revolution in AI">
        <span class="nav-page-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>