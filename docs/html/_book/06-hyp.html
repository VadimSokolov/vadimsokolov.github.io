<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Bayesian Hypothesis Testing – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-sp.html" rel="next">
<link href="./05-ab.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="6&nbsp; Bayesian Hypothesis Testing – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="06-hyp_files/figure-html/unnamed-chunk-1-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="6&nbsp; Bayesian Hypothesis Testing – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="06-hyp_files/figure-html/unnamed-chunk-1-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./06-hyp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#likelihood-principle" id="toc-likelihood-principle" class="nav-link active" data-scroll-target="#likelihood-principle"><span class="header-section-number">6.1</span> Likelihood Principle</a></li>
  <li><a href="#the-bayesian-approach" id="toc-the-bayesian-approach" class="nav-link" data-scroll-target="#the-bayesian-approach"><span class="header-section-number">6.2</span> The Bayesian Approach</a></li>
  <li><a href="#interval-estimation-credible-sets" id="toc-interval-estimation-credible-sets" class="nav-link" data-scroll-target="#interval-estimation-credible-sets"><span class="header-section-number">6.3</span> Interval Estimation: Credible Sets</a></li>
  <li><a href="#alternative-approaches" id="toc-alternative-approaches" class="nav-link" data-scroll-target="#alternative-approaches"><span class="header-section-number">6.4</span> Alternative Approaches</a>
  <ul class="collapse">
  <li><a href="#significance-testing-using-p-values" id="toc-significance-testing-using-p-values" class="nav-link" data-scroll-target="#significance-testing-using-p-values">Significance testing using p-values</a></li>
  <li><a href="#bayes-vs-p-value" id="toc-bayes-vs-p-value" class="nav-link" data-scroll-target="#bayes-vs-p-value">Bayes vs P-value</a></li>
  <li><a href="#neyman-pearson" id="toc-neyman-pearson" class="nav-link" data-scroll-target="#neyman-pearson">Neyman-Pearson</a></li>
  </ul></li>
  <li><a href="#sequential-analysis" id="toc-sequential-analysis" class="nav-link" data-scroll-target="#sequential-analysis"><span class="header-section-number">6.5</span> Sequential Analysis</a>
  <ul class="collapse">
  <li><a href="#the-bayesian-framework-for-sequential-testing" id="toc-the-bayesian-framework-for-sequential-testing" class="nav-link" data-scroll-target="#the-bayesian-framework-for-sequential-testing">The Bayesian Framework for Sequential Testing</a></li>
  <li><a href="#walds-sequential-probability-ratio-test" id="toc-walds-sequential-probability-ratio-test" class="nav-link" data-scroll-target="#walds-sequential-probability-ratio-test">Wald’s Sequential Probability Ratio Test</a></li>
  <li><a href="#applications-in-clinical-trials" id="toc-applications-in-clinical-trials" class="nav-link" data-scroll-target="#applications-in-clinical-trials">Applications in Clinical Trials</a></li>
  <li><a href="#sequential-analysis-for-rare-diseases" id="toc-sequential-analysis-for-rare-diseases" class="nav-link" data-scroll-target="#sequential-analysis-for-rare-diseases">Sequential Analysis for Rare Diseases</a></li>
  <li><a href="#sec-mab-experiments" id="toc-sec-mab-experiments" class="nav-link" data-scroll-target="#sec-mab-experiments">Multi-Armed Bandit Experiments</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations">Practical Considerations</a></li>
  </ul></li>
  <li><a href="#examples-and-paradoxes" id="toc-examples-and-paradoxes" class="nav-link" data-scroll-target="#examples-and-paradoxes"><span class="header-section-number">6.6</span> Examples and Paradoxes</a></li>
  <li><a href="#prior-sensitivity" id="toc-prior-sensitivity" class="nav-link" data-scroll-target="#prior-sensitivity"><span class="header-section-number">6.7</span> Prior Sensitivity</a></li>
  <li><a href="#the-difference-between-p-values-and-bayesian-evidence" id="toc-the-difference-between-p-values-and-bayesian-evidence" class="nav-link" data-scroll-target="#the-difference-between-p-values-and-bayesian-evidence"><span class="header-section-number">6.8</span> The difference between p-values and Bayesian evidence</a></li>
  <li><a href="#jeffreys-decision-rule" id="toc-jeffreys-decision-rule" class="nav-link" data-scroll-target="#jeffreys-decision-rule"><span class="header-section-number">6.9</span> Jeffreys’ Decision Rule</a></li>
  <li><a href="#cromwells-rule" id="toc-cromwells-rule" class="nav-link" data-scroll-target="#cromwells-rule"><span class="header-section-number">6.10</span> Cromwell’s Rule</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./06-hyp.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-hypothesis" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The hypothesis testing problem is as follows. Based on a sample of data, <span class="math inline">\(y\)</span>, generated from <span class="math inline">\(p\left( y \mid \theta\right)\)</span> for <span class="math inline">\(\theta\in\Theta\)</span>, the goal is to determine if <span class="math inline">\(\theta\)</span> lies in <span class="math inline">\(\Theta_{0}\)</span> or in <span class="math inline">\(\Theta_{1}\)</span>, two disjoint subsets of <span class="math inline">\(\Theta\)</span>. In general, the hypothesis testing problem involves an action: accepting or rejecting a hypothesis. The problem is described in terms of a null, <span class="math inline">\(H_{0}\)</span>, and alternative hypothesis, <span class="math inline">\(H_{1}\)</span>, which are defined as <span class="math display">\[
H_{0}:\theta\in\Theta_{0}\;\;\mathrm{and}\;\;H_{1}%
:\theta\in\Theta_{1}\text{.}%
\]</span></p>
<p>We now move from the <em>how</em> of A/B testing to the <em>why</em> of statistical inference. This chapter dives deeper into the foundational differences between frequentist and Bayesian testing, introducing concepts like admissibility, complete classes, and the Likelihood Principle.</p>
<p>As a scope note, we will be explicit about notation: in this chapter <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> refer to Type I and Type II error probabilities in the classical sense, defined through repeated-sampling performance of a decision rule. When we discuss Bayesian evidence, we instead emphasize posterior probabilities and Bayes factors, and when we talk about posterior uncertainty we use credible intervals and posterior summaries without reusing <span class="math inline">\(\alpha\)</span> as a credibility-level parameter.</p>
<p>Different types of regions generate different types of hypothesis tests. If the null hypothesis assumes that <span class="math inline">\(\Theta_{0}\)</span> is a single point, <span class="math inline">\(\Theta _{0}=\theta_{0}\)</span>, this is known as a simple or “sharp” null hypothesis. If the region consists of multiple points, the hypothesis is called composite; this occurs when the space is unconstrained or corresponds to an interval of the real line. In the case of a single parameter, typical one-sided tests are of the form <span class="math inline">\(H_{0}:\theta&lt;\theta_{0}\)</span> and <span class="math inline">\(H_{1}:\theta&gt;\theta_{0}\)</span>.</p>
<p>There are two correct decisions and two possible types of errors. The correct decisions are accepting a null or an alternative that is true, whereas a Type I error incorrectly rejects a true null and a Type II error incorrectly accepts a false null.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(\theta\in\Theta_{0}\)</span></th>
<th><span class="math inline">\(\theta\in\Theta_{1}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accept <span class="math inline">\(H_{0}\)</span></td>
<td>Correct decision</td>
<td>Type II error</td>
</tr>
<tr class="even">
<td>Accept <span class="math inline">\(H_{1}\)</span></td>
<td>Type I error</td>
<td>Correct decision</td>
</tr>
</tbody>
</table>
<p>Formally, the probabilities of Type I (<span class="math inline">\(\alpha\)</span>) and Type II (<span class="math inline">\(\beta\)</span>) errors are defined as: <span class="math display">\[
\alpha=P \left[  \text{reject }H_{0} \mid H_{0}\text{
is true }\right]  \text{ and }\beta=P \left[  \text{accept
}H_{0} \mid H_{1}\text{ is true }\right]  \text{.}%
\]</span></p>
<p>It is useful to think of the decision to accept or reject as a decision rule, <span class="math inline">\(d\left( y\right)\)</span>. In many cases, the decision rules form a critical region <span class="math inline">\(R\)</span>, such that <span class="math inline">\(d\left( y\right) =d_{1}\)</span> if <span class="math inline">\(y\in R\)</span>. These regions often take the form of simple inequalities. Next, defining the decision to accept the null as <span class="math inline">\(d\left( y\right) =d_{0}\)</span>, and the decision to accept the alternative as <span class="math inline">\(d_{1},\)</span> the error types are <span class="math display">\[\begin{align*}
\alpha_{\theta}\left(  d\right)   &amp;  =P \left[  d\left(  y\right)
=d_{1} \mid \theta\right]  \text{ if }\theta\in\Theta_{0}\text{ }(H_{0}\text{ is true})\\
\beta_{\theta}\left(  d\right)   &amp;  =P \left[  d\left(  y\right)
=d_{0} \mid \theta\right]  \text{ if }\theta\in\Theta_{1}\text{ }(H_{1}\text{ is true})\text{.}%
\end{align*}\]</span> where both types of errors explicitly depend on the decision and the true parameter value. Notice that both of these quantities are determined by the population properties of the data. In the case of a composite null hypothesis, the size of the test (the probability of making a type I error) is defined as <span class="math display">\[
\alpha = \underset{\theta\in\Theta_{0}}{\sup}~\alpha_{\theta}\left( d\right)
\]</span> The supremum (<span class="math inline">\(\sup\)</span>) is the least upper bound of a set. For finite sets, <span class="math inline">\(\sup=\max\)</span>. For a standard reference, see <span class="citation" data-cites="billingsley1995probability">Billingsley (<a href="references.html#ref-billingsley1995probability" role="doc-biblioref">1995</a>)</span>. and the power is defined as <span class="math inline">\(1-\beta_{\theta}\left( d\right)\)</span>. It is always possible to set either <span class="math inline">\(\alpha_{\theta}\left( d\right)\)</span> or <span class="math inline">\(\beta_{\theta }\left( d\right)\)</span> equal to zero, by finding a test that always rejects the alternative or null, respectively.</p>
<p>The total probability of making an error is <span class="math inline">\(\alpha_{\theta}\left(d\right) +\beta_{\theta}\left(d\right)\)</span>, and ideally one would seek to minimize the total error probability, absent additional information. The optimal action <span class="math inline">\(d^*\)</span> minimizes the posterior expected loss; <span class="math inline">\(d^* = d_0 = 0\)</span> if the posterior probability of hypothesis <span class="math inline">\(H_0\)</span> exceeds 1/2, and <span class="math inline">\(d^* = d_1=1\)</span> otherwise <span class="math display">\[
d^* = 1\left(  P \left(  \theta \in \Theta_0 \mid y\right) &lt; P \left(  \theta \in \Theta_1 \mid y\right)\right)  = 1\left(P \left(  \theta \in \Theta_0 \mid y\right)&lt;1/2\right).
\]</span> Formally, a decision rule selects the hypothesis with higher posterior probability.</p>
<p>The easiest way to reduce the error probability is to gather more data, as the additional evidence should lead to more accurate decisions. In some cases, it is easy to characterize optimal tests, those that minimize the sum of the errors. Simple hypothesis tests of the form <span class="math inline">\(H_{0}:\theta=\theta_{0}\)</span> versus <span class="math inline">\(H_{1}:\theta=\theta_{1}\)</span>, are one such case admitting optimal tests. Defining <span class="math inline">\(d^{\ast}\)</span> as a test accepting <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(a_{0}f\left( y \mid \theta_{0}\right) &gt;a_{1}f\left( y \mid \theta_{1}\right)\)</span> and <span class="math inline">\(H_{1}\)</span> if <span class="math inline">\(a_{0}f\left( y \mid \theta_{0}\right) &lt;a_{1}f\left( y \mid \theta _{1}\right)\)</span>, for some <span class="math inline">\(a_{0}\)</span> and <span class="math inline">\(a_{1}\)</span>. Either <span class="math inline">\(H_{0}\)</span> or <span class="math inline">\(H_{1}\)</span> can be accepted if <span class="math inline">\(a_{0}f\left(y \mid \theta_{0}\right) =a_{1}f\left( y \mid \theta_{1}\right)\)</span>. Then, for any other test <span class="math inline">\(d\)</span>, it is not hard to show that <span class="math display">\[
a_{0}\alpha\left(  d^{\ast}\right)  +a_{1}\beta\left(  d^{\ast}\right)  \leq
a_{0}\alpha\left(  d\right)  +a_{1}\beta\left(  d\right),
\]</span> where <span class="math inline">\(\alpha_{d}=\alpha_{d}\left( \theta\right)\)</span> and <span class="math inline">\(\beta_{d}=\beta_{d}\left( \theta\right)\)</span>. This result highlights the optimality of tests defining rejection regions in terms of the likelihood ratio statistic, <span class="math inline">\(f\left( y \mid \theta_{0}\right)/f\left( y \mid \theta_{1}\right)\)</span>. It turns out that the results are in fact stronger. In terms of decision theoretic properties, tests that define rejection regions based on likelihood ratios are not only admissible decisions, but form a minimal complete class, the strongest property possible.</p>
<p>One of the main problems in hypothesis testing is that there is often a tradeoff between the two goals of reducing type I and type II errors: decreasing <span class="math inline">\(\alpha\)</span> leads to an increase in <span class="math inline">\(\beta\)</span>, and vice-versa. Because of this, it is common to fix <span class="math inline">\(\alpha_{\theta}\left( d\right)\)</span>, or <span class="math inline">\(\sup~\alpha_{\theta}\left( d\right)\)</span>, and then find a test to minimize <span class="math inline">\(\beta_{d}\left( \theta\right)\)</span>. This leads to “most powerful” tests. There is an important result from decision theory: test procedures that use the same size level of <span class="math inline">\(\alpha\)</span> in problems with different sample sizes are inadmissible. This is commonly done where significance is indicated by a fixed size, say 5%. The implications of this will be clearer below in examples.</p>
<section id="likelihood-principle" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="likelihood-principle"><span class="header-section-number">6.1</span> Likelihood Principle</h2>
<p>Given observed data <span class="math inline">\(y\)</span> and likelihood function <span class="math inline">\(l(\theta) = p(y\mid \theta)\)</span>, the likelihood principle states that all relevant experimental information is contained in the likelihood function for the observed <span class="math inline">\(y\)</span>. Furthermore, two likelihood functions contain the same information about <span class="math inline">\(\theta\)</span> if they are proportional to each other. For example, the widely used maximum-likelihood estimation does satisfy the likelihood principle. However, frequentist hypothesis testing procedures often violate this principle. The likelihood principle is a fundamental principle in statistical inference, and it is a key reason why Bayesian procedures are often preferred.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Decision Theoretic Concepts
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Admissibility</strong>: A decision rule <span class="math inline">\(\delta\)</span> is <em>admissible</em> if there exists no other rule <span class="math inline">\(\delta^{\prime}\)</span> such that <span class="math inline">\(R\left( \theta,\delta^{\prime}\right) \leq R\left( \theta,\delta\right)\)</span> for all <span class="math inline">\(\theta\)</span>, with strict inequality for at least one <span class="math inline">\(\theta\)</span>. In other words, an admissible rule cannot be uniformly improved upon.</p></li>
<li><p><strong>Complete Class</strong>: A class of rules <span class="math inline">\(\mathcal{C}\)</span> is <em>essentially complete</em> if for any rule <span class="math inline">\(\delta \notin \mathcal{C}\)</span>, there exists a rule <span class="math inline">\(\delta^{\prime} \in \mathcal{C}\)</span> that dominates it. A <em>minimal complete class</em> is the smallest such set of rules containing all admissible rules.</p></li>
</ul>
</div>
</div>
<div id="exm-likelihood" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 (Testing fairness)</strong></span> Suppose we are interested in testing <span class="math inline">\(\theta\)</span>, the unknown probability of heads for a possibly biased coin. Suppose, <span class="math display">\[
H_0 :~\theta=1/2 \quad\text{v.s.} \quad  H_1 :~\theta&gt;1/2.
\]</span> An experiment is conducted and 9 heads and 3 tails are observed. This information is not sufficient to fully specify the model <span class="math inline">\(p(y\mid \theta)\)</span>. There are two approaches.</p>
<p><em>Scenario 1</em>: Number of flips, <span class="math inline">\(n = 12\)</span> is predetermined. Then number of heads <span class="math inline">\(Y \mid \theta\)</span> is binomial <span class="math inline">\(B(n, \theta)\)</span>, with probability mass function <span class="math display">\[
p(y\mid \theta)= {n \choose y} \theta^{y}(1-\theta)^{n-y} = 220 \cdot \theta^9(1-\theta)^3
\]</span> For a frequentist, the p-value of the test is <span class="math display">\[
P(Y \geq 9\mid H_0)=\sum_{y=9}^{12} {12 \choose y} (1/2)^y(1-1/2)^{12-y} = (1+12+66+220)/2^{12} =0.073,
\]</span> and if you recall the classical testing, <span class="math inline">\(H_0\)</span> is not rejected at level <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<p><em>Scenario 2</em>: The number of tails (successes) <span class="math inline">\(\alpha = 3\)</span> is predetermined; that is, flipping continues until 3 tails are observed. Then, <span class="math inline">\(Y\)</span>, the number of heads (failures) observed until 3 tails appear, follows a Negative Binomial distribution <span class="math inline">\(NB(3, 1- \theta)\)</span>, <span class="math display">\[
p(y\mid \theta)= {\alpha+y-1 \choose \alpha-1} \theta^{y}(1-\theta)^{\alpha} = {3+9-1 \choose 3-1} \theta^9(1-\theta)^3 = 55\cdot \theta^9(1-\theta)^3.
\]</span> For a frequentist, large values of <span class="math inline">\(Y\)</span> are critical and the p-value of the test is <span class="math display">\[
P(Y \geq 9\mid H_0)=\sum_{y=9}^{\infty} {3+y-1 \choose 2} (1/2)^{y}(1/2)^{3} = 0.0327.
\]</span> We used the following identity here <span class="math display">\[
\sum_{x=k}^{\infty} {2+x \choose 2}\dfrac{1}{2^x} = \dfrac{8+5k+k^2}{2^k}.
\]</span> The hypothesis <span class="math inline">\(H_0\)</span> is rejected, and this change in decision is not caused by observations.</p>
<p>According to the Likelihood Principle, all relevant information is in the likelihood <span class="math inline">\(l(\theta) \propto \theta^9(1 - \theta)^3\)</span>, and Bayesians could not agree more!</p>
<p>Edwards, Lindman, and Savage (1963, 193) note: The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.</p>
</div>
</section>
<section id="the-bayesian-approach" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="the-bayesian-approach"><span class="header-section-number">6.2</span> The Bayesian Approach</h2>
<p>Formally, the Bayesian approach to hypothesis testing is a special case of the model comparison results to be discussed later. The Bayesian approach just computes the posterior distribution of each hypothesis. By Bayes <span class="math display">\[
P \left(  H_{i} \mid y\right)  =\frac{p\left(  y \mid H_{i}\right)  P \left(  H_{i}\right)  }{p\left(  y\right)}    , ~\text{for} ~ i=0,1
\]</span> where <span class="math inline">\(P \left( H_{i}\right)\)</span> is the prior probability of <span class="math inline">\(H_{i}\)</span>, <span class="math display">\[
p\left( y \mid H_{i}\right) =\int_{\theta \in \Theta_i} p\left( y \mid \theta\right) p\left( \theta \mid H_{i}\right) d\theta
\]</span> is the marginal likelihood under <span class="math inline">\(H_{i}\)</span>, <span class="math inline">\(p\left( \theta \mid H_{i}\right)\)</span> is the parameter prior under <span class="math inline">\(H_{i}\)</span>, and <span class="math display">\[
p\left(  y\right)  = \sum_{i=0,1} p\left(  y \mid H_{i}\right)  P \left( H_{i}\right).
\]</span></p>
<p>If the hypotheses are mutually exclusive, <span class="math inline">\(P \left( H_{0}\right) =1-P \left( H_{1}\right)\)</span>.</p>
<p>The posterior <em>odds</em> of the null to the alternative is <span class="math display">\[
\text{Odds}_{0,1}=\frac{P \left(  H_{0} \mid y\right)  }{P %
\left(  H_{1} \mid y\right)  }=\frac{p\left(  y \mid H_{0}\right)
}{p\left(  y \mid H_{1}\right)  }\frac{P \left(  H_{0}\right)  }{P \left(  H_{1}\right)  }\text{.}%
\]</span></p>
<p>The odds ratio updates the prior odds, <span class="math inline">\(P \left( H_{0}\right) /P \left( H_{1}\right)\)</span>, using the Bayes Factor, <span class="math display">\[
\mathrm{BF}_{0,1}=\dfrac{p\left(y \mid H_{0}\right)}{p\left( y \mid H_{1}\right)}.
\]</span> With exhaustive competing hypotheses, <span class="math inline">\(P \left( H_{0} \mid y\right)\)</span> simplifies to <span class="math display">\[
P \left(  H_{0} \mid y\right)  =\left(  1+\left(  \mathrm{BF}_{0,1}\right)  ^{-1}\frac{\left(  1-P \left(  H_{0}\right)
\right)  }{P \left(  H_{0}\right)  }\right)  ^{-1}\text{,}%
\]</span> and with equal prior probability, <span class="math inline">\(P\left( H_{0} \mid y\right) =\left( 1+\left( \mathrm{BF}_{0,1}\right) ^{-1}\right) ^{-1}\)</span>. Both Bayes factors and posterior probabilities can be used for comparing hypotheses. Jeffreys (1961) advocated using Bayes factors, and provided a scale for measuring the strength of evidence that was given earlier. Bayes factors merely indicate that the null hypothesis is more likely if <span class="math inline">\(\mathrm{BF}_{0,1}&gt;1\)</span>, <span class="math inline">\(p\left( y \mid H_{0}\right) &gt;p\left( y \mid H_{1}\right)\)</span>. The Bayesian approach merely compares density ordinates of <span class="math inline">\(p\left( y \mid H_{0}\right)\)</span> and <span class="math inline">\(p\left( y \mid H_{1}\right)\)</span>, which mechanically involves plugging in the observed data into the functional form of the marginal likelihood.</p>
<p>For a point null, <span class="math inline">\(H_{0}:\theta=\theta_{0}\)</span>, the parameter prior is <span class="math inline">\(p\left( \theta \mid H_{0}\right) =\delta_{\theta_{0}}\left( \theta\right)\)</span> (a Dirac mass at <span class="math inline">\(\theta_{0}\)</span>), which implies that <span class="math display">\[
p\left( y \mid H_{0}\right) =\int p\left( y \mid \theta_{0}\right) p\left( \theta \mid H_{0}\right) d\theta=p\left( y \mid \theta_{0}\right).
\]</span> With a general alternative, <span class="math inline">\(H_{1}:\theta\neq\theta_{0}\)</span>, the probability of the null is <span class="math display">\[
P \left(  \theta=\theta_{0} \mid y\right)  =\frac{p\left(  y \mid \theta
_{0}\right)  P \left(  H_{0}\right)  }{p\left(  y \mid \theta
_{0}\right)  P \left(  H_{0}\right)  +\left(  1-P\left( H_{0}\right)  \right)  \int_{\Theta}p\left(  y \mid \theta,H_{1}\right)  p\left(  \theta \mid H_{1}\right)  d\theta},
\]</span> where <span class="math inline">\(p\left( \theta \mid H_{1}\right)\)</span> is the parameter prior under the alternative. This formula will be used below.</p>
<p>Bayes factors and posterior null probabilities measure the relative weight of evidence of the hypotheses. Traditional hypothesis testing involves an additional decision or action: to accept or reject the null hypothesis. For Bayesians, this typically requires some statement of the utility/loss that codifies the benefits/costs of making a correct or incorrect decision. The simplest situation occurs if one assumes a zero loss of making a correct decision. The loss incurred when accepting the null (alternative) when the alternative is true (false) is <span class="math inline">\(L\left( d_{0} \mid H_{1}\right)\)</span> and <span class="math inline">\(L\left( d_{1} \mid H_{0}\right)\)</span>, respectively.</p>
<p>The Bayesian will accept or reject based on the posterior expected loss. If the expected loss of accepting the null is less than the alternative, the rational decision maker will accept the null. The posterior loss of accepting the null is <span class="math display">\[
\mathbb{E}\left[  \mathcal{L}\mid d_{0},y\right]  =L\left(  d_{0} \mid H_{0}\right)
P \left(  H_{0} \mid y\right)  +L\left(  d_{0} \mid H_{1}\right)  P \left(  H_{1} \mid y\right)  =L\left( d_{0} \mid H_{1}\right)  P \left(  H_{1} \mid y\right)  ,
\]</span> since the loss of making a correct decision, <span class="math inline">\(L\left( d_{0} \mid H_{0}\right)\)</span>, is zero. Similarly, <span class="math display">\[
\mathbb{E}\left[  \mathcal{L} \mid d_{1},y\right]  =L\left(  d_{1} \mid H_{0}\right)
P \left(  H_{0} \mid y\right)  +L\left(  d_{1} \mid H_{1}\right)  P \left(  H_{1} \mid y\right)  =L\left( d_{1} \mid H_{0}\right)  P \left(  H_{0} \mid y\right)  .
\]</span> Thus, the null is accepted if <span class="math display">\[
\mathbb{E}\left[  \mathcal{L} \mid d_{0},y\right]  &lt;\mathbb{E}\left[  \mathcal{L} \mid d_{1},y\right]
\Longleftrightarrow L\left(  d_{0} \mid H_{1}\right)  P \left( H_{1} \mid y\right)  &lt;L\left(  d_{1} \mid H_{0}\right)
P \left(  H_{0} \mid y\right)  ,
\]</span> which further simplifies to <span class="math display">\[
\frac{L\left(  d_{0} \mid H_{1}\right)  }{L\left(  d_{1} \mid H_{0}\right)  }&lt;\frac{P \left(  H_{0} \mid y\right)  }{P \left(  H_{1} \mid y\right)  }.
\]</span> In the case of equal losses, this simplifies to accept the null if <span class="math inline">\(P \left( H_{1} \mid y\right) &lt;P \left( H_{0} \mid y\right)\)</span>. One advantage of Bayes procedures is that the resulting estimators and decisions are always admissible.</p>
<div id="exm-enigma" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 (Enigma Code Breaking)</strong></span> Consider an alphabet of <span class="math inline">\(A=26\)</span> letters. Let <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> be two transmitted messages of length <span class="math inline">\(T\)</span>. We want to determine if they were encoded by the same Enigma machine setting (<span class="math inline">\(H_1\)</span>) or by different/random settings (<span class="math inline">\(H_0\)</span>).</p>
<p>To compute the Bayes factor, we compare the likelihood of the observed pair <span class="math inline">\((x, y)\)</span> under each hypothesis: <span class="math display">\[
P( x,y\mid  H_0 ) \; \; \mathrm{ and} \; \; P( x,y\mid  H_1 ).
\]</span> Under <span class="math inline">\(H_0\)</span> (different settings), the two messages are effectively independent random sequences. The probability of any specific pair of letters is <span class="math inline">\((1/A)^2\)</span>, so for length <span class="math inline">\(T\)</span>: <span class="math display">\[
P(x, y \mid H_0) = \prod_{i=1}^T \left(\frac{1}{A}\right)^2 = \left(\frac{1}{A}\right)^{2T}.
\]</span></p>
<p>Under <span class="math inline">\(H_1\)</span> (same setting), the messages are correlated. Specifically, if the letters at position <span class="math inline">\(i\)</span> are the same (<span class="math inline">\(x_i = y_i\)</span>), it implies a ‘match’. The probability of a match, denoted by <span class="math inline">\(m\)</span>, depends on the language’s letter frequencies <span class="math inline">\(p_t\)</span> (for English, <span class="math inline">\(m = \sum p_t^2 \approx 0.066\)</span> or about <span class="math inline">\(2/26\)</span>). If they don’t match, the probability is distributed among the remaining pairs. Thus: <span class="math display">\[
P( x_i , y_i \mid H_1 ) = \begin{cases}
\frac{m}{A} &amp; \text{if } x_i = y_i \text{ (match)} \\
\frac{1-m}{A(A-1)} &amp; \text{if } x_i \neq y_i \text{ (mismatch)}
\end{cases}
\]</span> The term <span class="math inline">\(1/A\)</span> appears because we approximate the marginal probability of <span class="math inline">\(x_i\)</span> as uniform, but the conditional probability <span class="math inline">\(P(y_i|x_i)\)</span> is boosted to <span class="math inline">\(m\)</span> if <span class="math inline">\(x_i=y_i\)</span>.</p>
<p>The log Bayes factor is the sum of contributions from matches (<span class="math inline">\(M\)</span>) and mismatches (<span class="math inline">\(N\)</span>): <span class="math display">\[\begin{align*}
\ln \frac{P( x,y\mid  H_1 )}{P( x,y\mid  H_0 )} &amp; = M \ln \frac{ m/A}{1/A^2} +N \ln \frac{ ( 1-m ) / A(A-1) }{ 1/ A^2} \\
&amp; = M \ln (mA)  + N \ln \frac{ ( 1-m )A }{A-1 }
\end{align*}\]</span> Substituting values for English (<span class="math inline">\(A=26, m \approx 0.066\)</span>): The first term (match) adds <span class="math inline">\(\ln(0.066 \times 26) \approx 0.54\)</span>. The second term (mismatch) subtracts <span class="math inline">\(\ln(\frac{0.934 \times 26}{25}) \approx -0.01\)</span>. In base 10 (decibans), a match provides roughly 2.3 decibans of evidence, while a mismatch provides a slight penalty.</p>
<p>Example: With <span class="math inline">\(T=51\)</span> letters, suppose we observe <span class="math inline">\(M=4\)</span> matches and <span class="math inline">\(N=47\)</span> mismatches. This yields: <span class="math display">\[
4 \times 2.3 - 47 \times 0.03 \approx 9.2 - 1.41 = 7.79 \text{ decibans}.
\]</span> This corresponds to a Bayes factor of roughly <span class="math inline">\(10^{0.78} \approx 6\)</span>, providing evidence for <span class="math inline">\(H_1\)</span>.</p>
<p>How long a sequence do you need to look at? Calculate the expected log odds. Turing and Good figured you needed sequences of about length <span class="math inline">\(400\)</span>. Can also look at doubles and triples.</p>
</div>
<div id="exm-dice-odds" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 (Dice and Odds Updating.)</strong></span> Suppose that you wish to assess whether a die is loaded or not. $ H_0 : p= 1/6$ vs $ H_1 : p = 1 / 5 $. How will the evidence accumulate in each case?</p>
<p>Let $ x = # 6$’s and $ y = # $ non-<span class="math inline">\(6\)</span>’s. Then $ x+ y = n $. The posterior odds will update via the likelihood ratio (a.k.a. Bayes factor) as <span class="math display">\[\begin{align*}
O ( H_0 | D ) &amp; = \left ( \frac{1/6}{1/5} \right )^x \left ( \frac{5/6}{4/5} \right )^y O ( H_0 )  \\
  &amp; = \left ( \frac{5}{6} \right )^x \left ( \frac{25}{24} \right )^y O ( H_0 )
\end{align*}\]</span> Under $ H_0 : p= 1/6 $ we can replace the data with the empirical cdf (a.k.a. $ x/n = 1/6 $) and similarly under $ H_1 $ we have $ x/n = 1/5 $.</p>
<p>Hence, we have <span class="math display">\[
\frac{ O ( H_0 | D ) }{O(H_0)}  \approx \left \{ \left ( \frac{5}{6} \right )^{1/6} \left ( \frac{25}{24} \right )^{5/6}  \right \}^n   = ( 1.00364 )^n = 10^{0.00158 n }
\]</span> Hence, on a deciban scale (ten times the log-base-10 likelihood ratio, a term coined by I.J. Good), evidence accumulates at rate <span class="math inline">\(0.00158\)</span> in favor of <span class="math inline">\(H_0\)</span>.</p>
<p>Under <span class="math inline">\(H_1\)</span>, we have <span class="math display">\[
\frac{ O ( H_0 | D )}{O(H_0) }   \approx \left \{ \left ( \frac{5}{6} \right )^{1/5} \left ( \frac{25}{24} \right )^{4/5}  \right \}^n   = ( 0.9962 )^n  = 10^{- 0.00165 n }
\]</span> Hence, on a deciban scale (IJ Good), evidence accumulates at rate <span class="math inline">\(0.00165\)</span> against $H_0 $.</p>
<p>The Chernoff-Stein information lemma formalises this <span class="citation" data-cites="cover2006elements">(<a href="references.html#ref-cover2006elements" role="doc-biblioref">Cover and Thomas 2006</a>)</span>.</p>
</div>
<div id="exm-credset" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 (Signal Transmission)</strong></span> Suppose that the random variable <span class="math inline">\(X\)</span> is transmitted over a noisy communication channel. Assume that the received signal is given by <span class="math display">\[
Y=X+W,
\]</span> where <span class="math inline">\(W\sim N(0,\sigma^2)\)</span> is independent of <span class="math inline">\(X\)</span>. Suppose that <span class="math inline">\(X=1\)</span> with probability <span class="math inline">\(p\)</span>, and <span class="math inline">\(X=-1\)</span> with probability <span class="math inline">\(1-p\)</span>. The goal is to decide between <span class="math inline">\(X=1\)</span> and <span class="math inline">\(X=-1\)</span> by observing the random variable <span class="math inline">\(Y\)</span>. We will assume symmetric loss and will accept the hypothesis with the higher posterior probability. This is also sometimes called the maximum a posteriori (MAP) test.</p>
<p>We assume that <span class="math inline">\(H_0: ~ X = 1\)</span>, thus <span class="math inline">\(Y\mid H_0 \sim N(1,\sigma^2)\)</span>, and <span class="math inline">\(Y\mid H_1 \sim N(-1,\sigma^2)\)</span>. The Bayes factor is simply the likelihood ratio <span class="math display">\[
\dfrac{p(y\mid H_0)}{p(y \mid H_1)} =  \exp\left( \frac{2y}{\sigma^2}\right).
\]</span> The prior odds are <span class="math inline">\(p/(1-p)\)</span>, thus the posterior odds are <span class="math display">\[
\exp\left( \frac{2y}{\sigma^2}\right)\dfrac{p}{1-p}.
\]</span> We choose <span class="math inline">\(H_0\)</span> (true <span class="math inline">\(X\)</span> is 1), if the posterior odds are greater than 1, i.e., <span class="math display">\[
y &gt; \frac{\sigma^2}{2} \log\left( \frac{1-p}{p}\right) = c.
\]</span></p>
<p>Further, we can calculate the error probabilities of our test. <span class="math display">\[
p(d_1\mid H_0) = P(Y&lt;c\mid X=1) = \Phi\left( \frac{c-1}{\sigma}\right),
\]</span> and <span class="math display">\[
p(d_0\mid H_1) = P(Y&gt;c\mid X=-1) = 1- \Phi\left( \frac{c+1}{\sigma}\right).
\]</span> Let’s plot the total error rate as a function of <span class="math inline">\(p\)</span> and assuming <span class="math inline">\(\sigma=0.2\)</span> <span class="math display">\[
P_e = p(d_1\mid H_0) (1-p) + p(d_0\mid H_1) p
\]</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>,<span class="fl">0.01</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> sigma<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">/</span>p)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Pe <span class="ot">&lt;-</span> <span class="fu">pnorm</span>((c<span class="dv">-1</span>)<span class="sc">/</span>sigma)<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>((c<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>sigma))<span class="sc">*</span>p</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p,Pe,<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">"p"</span>,<span class="at">ylab=</span><span class="st">"Total Error Rate"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-normalhypothesis" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 (Hockey: Hypothesis Testing for Normal Mean)</strong></span> The general manager of Washington Capitals (an NHL hockey team) thinks that their star center player Evgeny Kuznetsov is underperforming and is thinking of trading him to a different team. He uses the number of goals per season as a metric of performance. He knows that historically, a top forward scores on average 30 goals per season with a standard deviation of 5, <span class="math inline">\(\theta \sim N(30,25)\)</span>. In the 2022-2023 season Kuznetsov scored 12 goals. For the number of goals <span class="math inline">\(X\mid \theta\)</span> he uses normal likelihood <span class="math inline">\(N(\theta, 36)\)</span>. Kuznetsov’s performance was not stable over the years, thus the high variance in the likelihood. Thus, the posterior is <span class="math inline">\(N(23,15)\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">=</span> <span class="dv">36</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sigma02 <span class="ot">=</span> <span class="dv">25</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu<span class="ot">=</span><span class="dv">30</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="dv">12</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">=</span> sigma02 <span class="sc">+</span> sigma2</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>mu1 <span class="ot">=</span> sigma2<span class="sc">/</span>k<span class="sc">*</span>mu <span class="sc">+</span> sigma02<span class="sc">/</span>k<span class="sc">*</span>y</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>sigma21 <span class="ot">=</span> sigma2<span class="sc">*</span>sigma02<span class="sc">/</span>k</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>mu1</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 23</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>sigma21</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 15</code></pre>
</div>
</div>
<p>The manager thinks that Kuznetsov simply had a bad year and his true performance is at least 24 goals per season <span class="math inline">\(H_0: \theta \geq 24\)</span>, <span class="math inline">\(H_1: \theta&lt;24\)</span>. The posterior probability of the <span class="math inline">\(H_0\)</span> hypothesis is</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="dv">24</span>,mu1,<span class="fu">sqrt</span>(sigma21))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>a</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.36</code></pre>
</div>
</div>
<p>It is less than 1/2, only 36%. Thus, we should reject the null hypothesis. The posterior odds in favor of the null hypothesis are</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>a<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>a)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.56</code></pre>
</div>
</div>
<p>If underestimating (and trading) Kuznetsov is two times more costly than overestimating him (fans will be upset and team spirit might be affected), that is <span class="math inline">\(L(d_1\mid H_0) = 2L(d_0\mid H_1)\)</span>, then we should accept the null when posterior odds are greater than 1/2. This is the case here, 0.55 is greater than 1/2. The posterior odds are in favor of the null hypothesis. Thus, the manager should not trade Kuznetsov.</p>
<p>Kuznetsov was traded to Carolina Hurricanes towards the end of the 2023-2024 season.</p>
<p>Notice, when we try to evaluate a newcomer to the league, we use the prior probability of <span class="math inline">\(\theta \geq 24\)</span>:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="dv">24</span>,mu,<span class="fu">sqrt</span>(sigma02))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(a)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.88</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>a<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>a)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 7.7</code></pre>
</div>
</div>
<p>Thus, the prior odds in favor of <span class="math inline">\(H_0\)</span> are 7.7.</p>
</div>
<div id="exm-twosided" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 (Hypothesis Testing for Normal Mean: Two-Sided Test)</strong></span> In the case of two sided test, we are interested in testing</p>
<ul>
<li><span class="math inline">\(H_0: \theta = \theta_0\)</span>, <span class="math inline">\(p\left( \theta \mid H_{0}\right) =\delta_{\theta_0}\left( \theta\right)\)</span></li>
<li><span class="math inline">\(H_1: \theta \neq \theta_0\)</span>, <span class="math inline">\(p\left( \theta \mid H_{1}\right) = N\left( \theta_0,\sigma^{2}/n_0\right)\)</span></li>
</ul>
<p>Where <span class="math inline">\(n\)</span> is the sample size and <span class="math inline">\(\sigma^2\)</span> is the variance (known) of the population. Observed samples are <span class="math inline">\(Y = (y_1, y_2, \ldots, y_n)\)</span> with <span class="math display">\[
y_i \mid \theta,\sigma^2 \sim N(\theta, \sigma^2).
\]</span></p>
<p>The Bayes factor can be calculated analytically <span class="math display">\[
BF_{0,1} = \frac{p(Y\mid \theta = \theta_0, \sigma^2 )}
{\int p(Y\mid \theta, \sigma^2) p(\theta \mid \theta_0, n_0, \sigma^2)\, d \theta}
\]</span> <span class="math display">\[
\int p(Y\mid \theta, \sigma^2) p(\theta \mid \theta_0, n_0, \sigma^2)\, d \theta = \frac{\sqrt{n_0}\exp\left\{-\frac{n_0(\theta_0-\bar y)^2}{2\left(n_0+n\right)\sigma^2}\right\}}{\sqrt{2\pi}\sigma^2\sqrt{\frac{n_0+n}{\sigma^2}}}
\]</span> <span class="math display">\[
p(Y\mid \theta = \theta_0, \sigma^2 ) = \frac{\exp\left\{-\frac{(\bar y-\theta_0)^2}{2 \sigma ^2}\right\}}{\sqrt{2 \pi } \sigma }
\]</span> Thus, the Bayes factor is <span class="math display">\[
BF_{0,1} = \frac{\sigma\sqrt{\frac{n_0+n}{\sigma^2}}e^{-\frac{(\theta_0-\bar y)^2}{2\left(n_0+n\right)\sigma^2}}}{\sqrt{n_0}}
\]</span></p>
<p><span class="math display">\[
BF_{0,1} =\left(\frac{n + n_0}{n_0} \right)^{1/2} \exp\left\{-\frac{1}{2} \frac{n }{n + n_0} Z^2 \right\}
\]</span></p>
<p><span class="math display">\[
Z =  \frac{(\bar{Y} - \theta_0)}{\sigma/\sqrt{n}}
\]</span></p>
<p>One way to interpret the scaling factor <span class="math inline">\(n_0\)</span> is to look at the standard effect size <span class="math display">\[
\delta = \frac{\theta - \theta_0}{\sigma}.
\]</span> The prior of the standard effect size is <span class="math display">\[
\delta \mid H_1 \sim N(0, 1/n_0).
\]</span> This allows us to think about a standardized effect independent of the units of the problem.</p>
<p>Let’s consider now example of Argon discovery.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>air <span class="ot">=</span>    <span class="fu">c</span>(<span class="fl">2.31017</span>, <span class="fl">2.30986</span>, <span class="fl">2.31010</span>, <span class="fl">2.31001</span>, <span class="fl">2.31024</span>, <span class="fl">2.31010</span>, <span class="fl">2.31028</span>, <span class="fl">2.31028</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>decomp <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">2.30143</span>, <span class="fl">2.29890</span>, <span class="fl">2.29816</span>, <span class="fl">2.30182</span>, <span class="fl">2.29869</span>, <span class="fl">2.29940</span>, <span class="fl">2.29849</span>, <span class="fl">2.29889</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Our null hypothesis is that the mean of the difference equals to zero. We assume that measurements made in the lab have normal errors, this the normal likelihood. We empirically calculate the standard deviation of our likelihood. The Bayes factor is</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> air <span class="sc">-</span> decomp</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y); m0 <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">var</span>(air) <span class="sc">+</span> <span class="fu">var</span>(decomp))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>n0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">=</span> (<span class="fu">mean</span>(y) <span class="sc">-</span> m0)<span class="sc">/</span>(sigma<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>BF <span class="ot">=</span> <span class="fu">sqrt</span>((n <span class="sc">+</span> n0)<span class="sc">/</span>n0)<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>n<span class="sc">/</span>(n <span class="sc">+</span> n0)<span class="sc">*</span>Z<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>BF</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000019</code></pre>
</div>
</div>
<p>We have extremely strong evidence in favor <span class="math inline">\(H_1: \theta \ne 0\)</span> hypothesis. The posterior probability of the alternative hypothesis is numerically 1!</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>BF)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>a</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1</code></pre>
</div>
</div>
</div>
<div id="exm-normalhypothesis" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.7 (Hypothesis Testing for Proportions)</strong></span> Let’s look at again at the effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">Algo1</th>
<th style="text-align: center;">Algo2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>success</td>
<td style="text-align: center;">1755</td>
<td style="text-align: center;">1818</td>
</tr>
<tr class="even">
<td>failure</td>
<td style="text-align: center;">745</td>
<td style="text-align: center;">682</td>
</tr>
<tr class="odd">
<td>total</td>
<td style="text-align: center;">2500</td>
<td style="text-align: center;">2500</td>
</tr>
</tbody>
</table>
<p>Here we assume binomial likelihood and use conjugate beta prior, for mathematical convenience. We are putting independent beta priors on the click-through rates of the two algorithms, <span class="math inline">\(p_1\sim Beta(\alpha_1,\beta_1)\)</span> and <span class="math inline">\(p_2\sim Beta(\alpha_2,\beta_2)\)</span>. The posterior for <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> are independent Beta distributions <span class="math display">\[
p(p_1, p_1 \mid y) \propto p_1^{\alpha_1 + 1755 - 1} (1-p_1)^{\beta_1 + 745 - 1}\times p_2^{\alpha_2 + 1818 - 1} (1-p_2)^{\beta_2 + 682 - 1}.
\]</span></p>
<p>The easiest way to explore this posterior is via Monte Carlo simulation of the posterior.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">92</span>) <span class="co">#Kuzy</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="dv">1755</span>; n1 <span class="ot">&lt;-</span> <span class="dv">2500</span>; alpha1 <span class="ot">&lt;-</span> <span class="dv">1</span>; beta1 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="dv">1818</span>; n2 <span class="ot">&lt;-</span> <span class="dv">2500</span>; alpha2 <span class="ot">&lt;-</span> <span class="dv">1</span>; beta2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(m, y1 <span class="sc">+</span> alpha1, n1 <span class="sc">-</span> y1 <span class="sc">+</span> beta1)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(m, y2 <span class="sc">+</span> alpha2, n2 <span class="sc">-</span> y2 <span class="sc">+</span> beta2)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>rd <span class="ot">&lt;-</span> p2 <span class="sc">-</span> p1</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(rd), <span class="at">main=</span><span class="st">"Posterior Difference in Click-Through Rates"</span>, </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab=</span><span class="st">"p2 - p1"</span>, <span class="at">ylab=</span><span class="st">"Density"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fu">quantile</span>(rd, <span class="fu">c</span>(.<span class="dv">05</span>, .<span class="dv">95</span>))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(q)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>    5%    95% 
0.0037 0.0465 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>q,<span class="at">col=</span><span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="interval-estimation-credible-sets" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="interval-estimation-credible-sets"><span class="header-section-number">6.3</span> Interval Estimation: Credible Sets</h2>
<p>The interval estimators of model parameters are called credible sets. If we use the posterior measure to assess the credibility, the credible set is a set of parameter values that are consistent with the data and gives us is a natural way to measure the uncertainty of the parameter estimate.</p>
<p>Those who are familiar with the concept of classical confidence intervals (CI’s) often make an error by stating that the probability that the CI interval <span class="math inline">\([L, U ]\)</span> contains parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(1 - \alpha\)</span>. The right statement seems convoluted, one needs to generate data from such model many times and for each data set to exhibit the CI. Now, the proportion of CI’s covering the unknown parameter is “tends to” <span class="math inline">\(1 - \alpha\)</span>. Bayesian interpretation of a credible set <span class="math inline">\(C\)</span> is natural: The probability of a parameter belonging to the set <span class="math inline">\(C\)</span> is <span class="math inline">\(1 - \alpha\)</span>. A formal definition follows. Assume the set <span class="math inline">\(C\)</span> is a subset of domain of the parameter <span class="math inline">\(\Theta\)</span>. Then, <span class="math inline">\(C\)</span> is credible set with credibility <span class="math inline">\((1 - \alpha)\cdot 100\%\)</span> if <span class="math display">\[
p(\theta \in C \mid y) = \int_{C}p(\theta\mid y)d\theta \ge 1 - \alpha.
\]</span> If the posterior is discrete, then the integral becomes sum (counting measure) and <span class="math display">\[
p(\theta \in C \mid y) = \sum_{\theta_i\in C}p(\theta_i\mid y) \ge 1 - \alpha.
\]</span> This is the definition of a <span class="math inline">\((1 - \alpha)100\%\)</span> credible set, and of course for a given posterior function such set is not unique.</p>
<p>For a given credibility level <span class="math inline">\((1 - \alpha)100\%\)</span>, the shortest credible set is of interest. To minimize size the sets should correspond to highest posterior probability (density) areas. Thus the acronym HPD.</p>
<div id="def-hpd" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 (Highest Posterior Density (HPD) Credible Set)</strong></span> The <span class="math inline">\((1 - \alpha)100\%\)</span> HPD credible set for parameter <span class="math inline">\(\theta\)</span> is a set <span class="math inline">\(C \subset \Theta\)</span> of the form <span class="math display">\[
C = \{ \theta \in \Theta : p(\theta \mid y) \ge k(\alpha) \},
\]</span> where <span class="math inline">\(k(\alpha)\)</span> is the largest value such that <span class="math display">\[
P(\theta\in C \mid y) = \int_{C}p(\theta\mid y)d\theta \ge 1 - \alpha.
\]</span> Geometrically, if the posterior density is cut by a horizontal line at the height <span class="math inline">\(k(\alpha)\)</span>, the set <span class="math inline">\(C\)</span> is the projection on the <span class="math inline">\(\theta\)</span> axis of the region where the posterior density lies above the line.</p>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div id="lem-hpd" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1</strong></span> The HPD set <span class="math inline">\(C\)</span> minimizes the size among all sets <span class="math inline">\(D \subset \Theta\)</span> for which <span class="math display">\[
P(\theta \in D) = 1 - \alpha.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is essentially a special case of Neyman-Pearson lemma. If <span class="math inline">\(I_C(\theta) = 1(\theta \in C)\)</span> and <span class="math inline">\(I_D(\theta) = 1(\theta \in D)\)</span>, then the key observation is <span class="math display">\[
\left(p(\theta\mid y) - k(\alpha)\right)(I_C(\theta) - I_D(\theta)) \ge 0.
\]</span> Indeed, for <span class="math inline">\(\theta\)</span>’s in <span class="math inline">\(C\cap D\)</span> and <span class="math inline">\((C\cup D)^c\)</span>, the factor <span class="math inline">\(I_C(\theta)-I_D(\theta) = 0\)</span>. If <span class="math inline">\(\theta \in C\cap D^c\)</span>, then <span class="math inline">\(I_C(\theta)-I_D(\theta) = 1\)</span> and <span class="math inline">\(p(\theta\mid y)-k(\alpha) \ge 0\)</span>. If, on the other hand, <span class="math inline">\(\theta \in D\cap C^c\)</span>, then <span class="math inline">\(I_C(\theta)-I_D(\theta) = -1\)</span> and <span class="math inline">\(p(\theta\mid y)-k(\alpha) \le 0\)</span>. Thus, <span class="math display">\[
\int_{\Theta}(p(\theta\mid y) - k(\alpha))(I_C(\theta) - I_D(\theta))d\theta \ge 0.
\]</span> The statement of the theorem now follows from the chain of inequalities, <span class="math display">\[
\int_{C}(p(\theta\mid y) - k(\alpha))d\theta \ge \int_{D}(p(\theta\mid y) - k(\alpha))d\theta
\]</span> <span class="math display">\[
(1-\alpha) - k(\alpha)\text{size}(C) \ge (1-\alpha) - k(\alpha)\text{size}(D)
\]</span> <span class="math display">\[
size(C) \le size(D).
\]</span> The size of a set is simply its total length if the parameter space <span class="math inline">\(\theta\)</span> is one dimensional, total area, if <span class="math inline">\(\theta\)</span> is two dimensional, and so on.</p>
</div>
<p>Note, when the distribution <span class="math inline">\(p(\theta \mid y)\)</span> is unimodal and symmetric using quantiles of the posterior distribution is a good way to obtain the HPD set.</p>
<p>An equal-tailed interval (also called a central interval) of confidence level<br>
<span class="math display">\[
I_{\alpha} = [q_{\alpha/2}, q_{1-\alpha/2}],
\]</span> here <span class="math inline">\(q\)</span>’s are the quantiles of the posterior distribution. This is an interval on whose both right and left side lies <span class="math inline">\((1-\alpha/2)100\%\)</span> of the probability mass of the posterior distribution; hence the name equal-tailed interval.</p>
<p>Usually, when a credible interval is mentioned without specifying which type of the credible interval it is, an equal-tailed interval is meant.</p>
<p>However, unless the posterior distribution is unimodal and symmetric, there are points outside of the equal-tailed credible interval having a higher posterior density than some points of the interval. If we want to choose the credible interval so that this not happen, we can do it by using the highest posterior density criterion for choosing it.</p>
<div id="exm-Cauchy" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.8 (Cauchy.)</strong></span> Assume that the observed samples</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="sc">-</span><span class="dv">7</span>,<span class="dv">4</span>,<span class="sc">-</span><span class="dv">6</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>come from Cauchy distribution. The likelihood is <span class="math display">\[
p(y\mid \theta, \gamma) = \frac{1}{\pi\gamma} \prod_{i=1}^{4} \frac{1}{1+\left(\dfrac{y_i-\theta}{\gamma}\right)^2}.
\]</span> We assume unknown location parameter <span class="math inline">\(\theta\)</span> and scale parameter <span class="math inline">\(\gamma=1\)</span>. For the flat prior <span class="math inline">\(\pi(\theta) = 1\)</span>, the posterior is proportional to the likelihood.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>lhood <span class="ot">=</span> <span class="cf">function</span>(theta) <span class="dv">1</span><span class="sc">/</span><span class="fu">prod</span>(<span class="dv">1</span><span class="sc">+</span>(y<span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>,<span class="dv">10</span>,<span class="fl">0.1</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">sapply</span>(theta,lhood)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="dv">10</span><span class="sc">*</span>post<span class="sc">/</span><span class="fu">sum</span>(post)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(theta,post,<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="fu">expression</span>(theta),<span class="at">ylab=</span><span class="st">"Posterior Density"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">c</span>(<span class="fl">0.008475</span>, <span class="fl">0.0159</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>),<span class="at">col=</span><span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The four horizontal lines correspond to four credible sets</p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 54%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(k\)</span></th>
<th style="text-align: left;"><span class="math inline">\(C\)</span></th>
<th style="text-align: left;"><span class="math inline">\(P(\theta \in C \mid y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.008475</td>
<td style="text-align: left;">[-8.498, 5.077]</td>
<td style="text-align: left;">99%</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.0159</td>
<td style="text-align: left;">[-8.189, -3.022] <span class="math inline">\(\cup\)</span> [-0.615, 4.755]</td>
<td style="text-align: left;">95%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.1</td>
<td style="text-align: left;">[-7.328, -5.124] <span class="math inline">\(\cup\)</span> [1.591, 3.120]</td>
<td style="text-align: left;">64.2%</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">[-6.893, -5.667]</td>
<td style="text-align: left;">31.2%</td>
</tr>
</tbody>
</table>
<p>Notice that for <span class="math inline">\(k = 0.0159\)</span> and <span class="math inline">\(k = 0.1\)</span> the credible set is not a compact. This shows that two separate intervals “clash” for the ownership of <span class="math inline">\(\theta\)</span> and this is a useful information. This non-compactness can also point out that the prior is not agreeing with the data. There is no frequentist counterpart for the CI for <span class="math inline">\(\theta\)</span> in the above model.</p>
</div>
</section>
<section id="alternative-approaches" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="alternative-approaches"><span class="header-section-number">6.4</span> Alternative Approaches</h2>
<p>The two main alternatives to the Bayesian approach are significance testing using <span class="math inline">\(p-\)</span>values, developed by Ronald Fisher, and the Neyman-Pearson approach.</p>
<section id="significance-testing-using-p-values" class="level3">
<h3 class="anchored" data-anchor-id="significance-testing-using-p-values">Significance testing using p-values</h3>
<p>Fisher’s approach posits a test statistic, <span class="math inline">\(T\left( y\right)\)</span>, based on the observed data. In Fisher’s mind, if the value of the statistic was highly unlikely to have occured under <span class="math inline">\(H_{0}\)</span>, then the <span class="math inline">\(H_{0}\)</span> should be rejected. Formally, the <span class="math inline">\(p-\)</span>value is defined as <span class="math display">\[
p=P \left[  T\left(  Y\right)  &gt;T\left(  y\right)   \mid H_{0}\right]  ,
\]</span> where <span class="math inline">\(y\)</span> is the observed sample and <span class="math inline">\(Y=\left( Y_{1}, \ldots ,Y_{T}\right)\)</span> is a random sample generated from model <span class="math inline">\(p\left( Y \mid H_{0}\right)\)</span>, that is, the null distribution of the test-statistic in repeated samples. Thus, the <span class="math inline">\(p-\)</span>value is the probability that a data set would generate a more extreme statistic under the null hypothesis, and not the probability of the null, conditional on the data.</p>
<p>The testing procedure is simple. Fisher (1946, p.&nbsp;80) argues that: If P (the p-value) is between* <span class="math inline">\(0.1\)</span> and <span class="math inline">\(0.9\)</span>, there is certainly no reason to suspect the hypothesis tested. If it is below <span class="math inline">\(0.02\)</span>, it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not be astray if we draw a line at 0.05 and consider that higher values of <span class="math inline">\(\mathcal{X}^{2}\)</span> indicate a real discrepancy. Defining <span class="math inline">\(\alpha\)</span> as the significance level, the tests rejects <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(p&lt;\alpha\)</span>. Fisher advocated a fixed significance level of <span class="math inline">\(5\%\)</span>, based largely that <span class="math inline">\(5\%\)</span> is roughly the tail area of a mean zero normal distribution more than two standard deviations from <span class="math inline">\(0\)</span>, indicating a statistically significant departure. In practice, testing with <span class="math inline">\(p-\)</span>values involves identifying a critical value, <span class="math inline">\(t_{\alpha}\)</span>, and rejecting the null if the observed statistic <span class="math inline">\(t\left( y\right)\)</span> is more extreme than <span class="math inline">\(t_{\alpha}\)</span>. For example, for a significance test of the sample mean, <span class="math inline">\(t\left( y\right) =\left( \overline{y}-\theta_{0}\right) /se\left( \overline{y}\right)\)</span>, where <span class="math inline">\(se\left( \overline{y}\right)\)</span> is the standard error of <span class="math inline">\(\overline{y}\)</span>; the <span class="math inline">\(5\%\)</span> critical value is 1.96; and Fisher would reject the null if <span class="math inline">\(t\left( y\right) &gt;t_{\alpha}\)</span>.</p>
<p>Fisher interpreted the <span class="math inline">\(p-value\)</span> as the weight or measure of evidence of the null hypothesis. The alternative hypothesis is noticeable in its absence in Fisher’s approach. Fisher largely rejected the consideration of alternatives, believing that researchers should weigh the evidence or draw conclusions about the observed data rather than making decisions such as accepting or rejecting hypotheses based on it.</p>
<p>There are a number of issues with Fisher’s approach. The first and most obvious criticism is that it is possible to reject the null, when the alternative hypothesis is less likely. This is an inherent problem in using population tail probabilities–essentially rare events. Just because a rare event has occurred does not mean the null is incorrect, unless there is a more likely alternative. This situation often arises in court cases, where a rare event like a murder has occurred. Decisions based on p-values generates a problem called prosecutor’s Fallacy, which is discussed below. Second, Fisher’s approach relies on population properties (the distribution of the statistic under the null) that would only be revealed in repeated samples or asymptotically. Thus, the testing procedure relies on data that is not yet seen, a violation of what is known as the likelihood principle. As noted by Jeffreys’ (1939, pp.&nbsp;315-316): “<em>What the use of P implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable data that have not occurred. This seems a remarkable procedure</em>” <!-- \label{Jeffreys, p. 385}   --></p>
<p>Third, Fisher is agnostic regarding the source of the test statistics, providing no discussion of how the researcher decides to focus on one test statistic over another. In some simple models, the distribution of properly scaled sufficient statistics provides natural test statistics (e.g., the <span class="math inline">\(t-\)</span>test). In more complicated models, Fisher is silent on the sources. In many cases, there are numerous test statistics (e.g., testing for normality), and test choice is clearly subjective. For example, in Generalized Method of Moments (GMM) tests, the choice of test moments is clearly a subjective choice. Finally, from a practical perspective, <span class="math inline">\(p-\)</span>values have a serious deficiency: tests using <span class="math inline">\(p\)</span>-values often appear to give the wrong answer, in the sense that they provide a highly misleading impression of the weight of evidence in many samples. A number of examples of this will be given below, but in all cases, Fisher’s approach tends to over-reject the null hypotheses.</p>
</section>
<section id="bayes-vs-p-value" class="level3">
<h3 class="anchored" data-anchor-id="bayes-vs-p-value">Bayes vs P-value</h3>
<p>The fundamental difference between the Bayesian and frequentist approaches can be summarized by how they quantify evidence. As R.A. Fisher famously wrote: “… for the one chance in a million will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us …”. This quote highlights that rare events do happen under the null hypothesis.</p>
<p>The Bayesian evidence is quantified by the Bayes Factor: <span class="math display">\[
BF = \frac{P(D|H_0)}{P(D|H_1)}
\]</span> If we consider the data definition used in frequentist testing, where <span class="math inline">\(D = \{T(x) &gt; t\}\)</span>, then the numerator <span class="math inline">\(P(D|H_0)\)</span> corresponds precisely to the <strong>p-value</strong>. The crucial limitation of the p-value is that it considers only the numerator. Even if this probability is small (suggesting the data is rare under the null), the denominator <span class="math inline">\(P(D|H_1)\)</span>—the probability of observing such data under the alternative—might be <strong>even smaller</strong>!</p>
<p>In a case with mutually exhaustive hypotheses (<span class="math inline">\(P(H_0) + P(H_1) = 1\)</span>), it is perfectly possible for the p-value to be small (e.g., 0.05), yet for the Bayes Factor to be greater than 1 (<span class="math inline">\(BF &gt; 1\)</span>), indicating that the evidence actually favors <span class="math inline">\(H_0\)</span>. This highlights the danger of assessing hypotheses in isolation rather than comparing them relative to one another.</p>
<div id="exm-novick1965bayesian" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.9 (Ulcer Treatment Clinical Trial)</strong></span> One of the earliest applications of Bayesian methods to clinical trials was presented by <span class="citation" data-cites="novick1965bayesian">Novick and Grizzle (<a href="references.html#ref-novick1965bayesian" role="doc-biblioref">1965</a>)</span>, who analyzed data from an ongoing experiment comparing four operative treatments for duodenal ulcers. Doctors assessed patient outcomes as Excellent, Fair, or Death. The data, collected sequentially over the course of the trial, are shown below:</p>
<div id="tbl-ulcer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ulcer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Outcomes by treatment for duodenal ulcer surgery
</figcaption>
<div aria-describedby="tbl-ulcer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Treatment</th>
<th style="text-align: center;">Excellent</th>
<th style="text-align: center;">Fair</th>
<th style="text-align: center;">Death</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="even">
<td style="text-align: center;">B</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="even">
<td style="text-align: center;">D</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>When I present this data to students and ask which treatment they would prefer, most choose Treatment B. The reasoning is intuitive: Treatment B has the highest number of excellent outcomes (89) and the lowest death rate (only 1 death, tied with C). Treatment A, despite being listed first, has only 76 excellent outcomes and 7 deaths, the worst performance on both metrics. The students’ intuition is correct, but can we quantify how confident we should be that B is truly better than A?</p>
<p>A classical chi-square test of homogeneity across treatments yields a p-value greater than 0.05, leading to the conclusion that we <em>cannot reject</em> the null hypothesis that all treatments are equally effective at the 5% significance level. This non-rejection is often misinterpreted as evidence that the treatments are equivalent.</p>
<p>But consider what the p-value actually measures: <span class="math inline">\(p = P(D \mid H_0)\)</span>, the probability of observing data at least as extreme as what we saw, assuming the null hypothesis is true. The critical insight is that <span class="math inline">\(P(D \mid H_1)\)</span>, the probability of the data under any specific alternative, can be much smaller than <span class="math inline">\(P(D \mid H_0)\)</span>. A small p-value does not mean the alternative is more likely; it only means the observed data would be rare under the null. This is the essence of the Bayesian critique: inference should be <em>relative</em>, comparing the evidence for different hypotheses, not absolute.</p>
<p>The Bayesian approach directly addresses the question of interest: given the observed data, what is the probability that one treatment is better than another? Let <span class="math inline">\(p_i\)</span> denote the death rate under treatment <span class="math inline">\(i\)</span>. Using independent Beta priors for each treatment’s death rate, say <span class="math inline">\(p_i \sim \text{Beta}(1, 1)\)</span> (uniform), the posteriors after observing the data are: <span class="math display">\[
p_A \mid \text{data} \sim \text{Beta}(1 + 7, 1 + 93) = \text{Beta}(8, 94)
\]</span> <span class="math display">\[
p_B \mid \text{data} \sim \text{Beta}(1 + 1, 1 + 99) = \text{Beta}(2, 100)
\]</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior distributions for death rates</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment A: 7 deaths out of 100</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment B: 1 death out of 100</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>alpha_A <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">7</span>; beta_A <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">93</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>alpha_B <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span>; beta_B <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">99</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo estimate of P(p_A &gt; p_B | data)</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>p_A_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_A, beta_A)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>p_B_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_B, beta_B)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>prob_A_worse <span class="ot">&lt;-</span> <span class="fu">mean</span>(p_A_samples <span class="sc">&gt;</span> p_B_samples)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"P(death rate A &gt; death rate B | data) ="</span>, <span class="fu">round</span>(prob_A_worse, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>P(death rate A &gt; death rate B | data) = 0.98 </code></pre>
</div>
</div>
<p>The posterior probability that Treatment A has a higher death rate than Treatment B is approximately 0.98. This is a direct, interpretable answer to the clinical question: there is a 98% probability that patients receiving Treatment A face higher mortality risk than those receiving Treatment B.</p>
<p>The contrast with the frequentist conclusion is striking. The chi-square test fails to reject equality at the 5% level, which many would interpret as “no difference.” The Bayesian analysis reveals that we can be 98% confident that Treatment A is worse than Treatment B in terms of mortality. The difference arises because the Bayesian approach compares hypotheses directly, while the p-value only measures how surprising the data would be under the null.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Frequentist chi-square test of homogeneity</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct the contingency table (Excellent, Fair, Death) x (Treatment A, B, C, D)</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>outcome_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="dv">76</span>, <span class="dv">17</span>, <span class="dv">7</span>,   <span class="co"># Treatment A</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="dv">89</span>, <span class="dv">10</span>, <span class="dv">1</span>,   <span class="co"># Treatment B</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>  <span class="dv">86</span>, <span class="dv">13</span>, <span class="dv">1</span>,   <span class="co"># Treatment C</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">88</span>, <span class="dv">9</span>, <span class="dv">3</span>     <span class="co"># Treatment D</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>), <span class="at">nrow =</span> <span class="dv">4</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(outcome_matrix) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>, <span class="st">"D"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(outcome_matrix) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Excellent"</span>, <span class="st">"Fair"</span>, <span class="st">"Death"</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Chi-square test</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>chisq_result <span class="ot">&lt;-</span> <span class="fu">suppressWarnings</span>(<span class="fu">chisq.test</span>(outcome_matrix))</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Chi-square statistic:"</span>, <span class="fu">round</span>(chisq_result<span class="sc">$</span>statistic, <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Chi-square statistic: 12 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Degrees of freedom:"</span>, chisq_result<span class="sc">$</span>parameter, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Degrees of freedom: 6 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"P-value:"</span>, <span class="fu">round</span>(chisq_result<span class="sc">$</span>p.value, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>P-value: 0.053 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fisher's exact test for just the death column (A vs B)</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>death_table <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">93</span>, <span class="dv">1</span>, <span class="dv">99</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(death_table) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(death_table) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Death"</span>, <span class="st">"Survival"</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>fisher_result <span class="ot">&lt;-</span> <span class="fu">fisher.test</span>(death_table, <span class="at">alternative =</span> <span class="st">"greater"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Fisher's exact test (A vs B, deaths only):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Fisher's exact test (A vs B, deaths only):</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"P-value:"</span>, <span class="fu">round</span>(fisher_result<span class="sc">$</span>p.value, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>P-value: 0.032 </code></pre>
</div>
</div>
<p>The chi-square test yields a p-value of 0.053, which exceeds the conventional 0.05 threshold. Even Fisher’s exact test comparing only the death rates between Treatments A and B gives a p-value around 0.03, which is only marginally significant and provides no sense of the magnitude of the difference or our confidence in it. The Bayesian approach, by contrast, tells us directly that there is a 98% probability that Treatment A has a higher death rate.</p>
<p>This example illustrates a fundamental principle: <em>Bayesian inference is relative</em>. We do not ask whether the data are unlikely in some absolute sense; we ask which hypothesis better explains the data. There are no absolutes in Bayesian inference, only comparisons. A treatment is not declared “effective” or “ineffective” in isolation; it is compared to alternatives, with uncertainty fully quantified.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize posterior distributions</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>p_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.2</span>, <span class="at">length.out =</span> <span class="dv">500</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p_grid, <span class="fu">dbeta</span>(p_grid, alpha_A, beta_A), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Death rate"</span>, <span class="at">ylab =</span> <span class="st">"Posterior density"</span>,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Posterior distributions of death rates"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p_grid, <span class="fu">dbeta</span>(p_grid, alpha_B, beta_B), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Treatment A"</span>, <span class="st">"Treatment B"</span>),</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">bty =</span> <span class="st">"n"</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Add posterior means</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> alpha_A<span class="sc">/</span>(alpha_A <span class="sc">+</span> beta_A), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> alpha_B<span class="sc">/</span>(alpha_B <span class="sc">+</span> beta_B), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The posterior distributions show clear separation between the two treatments. Treatment A’s death rate is concentrated around 7-8%, while Treatment B’s is concentrated around 1-2%. The overlap is minimal, corresponding to our finding that <span class="math inline">\(P(p_A &gt; p_B \mid \text{data}) \approx 0.98\)</span>.</p>
<p><strong>Prior Sensitivity Analysis</strong></p>
<p>A natural concern with Bayesian analysis is whether the conclusions depend heavily on the choice of prior. We used a uniform Beta(1,1) prior, but what if we had used different priors? The table below shows how <span class="math inline">\(P(p_A &gt; p_B \mid \text{data})\)</span> varies across several reasonable prior specifications:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Prior Sensitivity Analysis</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior sensitivity analysis</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Data: Treatment A: 7 deaths, 93 survivals; Treatment B: 1 death, 99 survivals</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>deaths_A <span class="ot">&lt;-</span> <span class="dv">7</span>; survivals_A <span class="ot">&lt;-</span> <span class="dv">93</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>deaths_B <span class="ot">&lt;-</span> <span class="dv">1</span>; survivals_B <span class="ot">&lt;-</span> <span class="dv">99</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Different priors to consider</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>priors <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">Prior =</span> <span class="fu">c</span>(<span class="st">"Uniform Beta(1,1)"</span>, </span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Jeffreys Beta(0.5,0.5)"</span>, </span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Skeptical Beta(2,20)"</span>,</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Optimistic Beta(1,10)"</span>,</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Strong Beta(5,50)"</span>),</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">5</span>),</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">beta =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate P(p_A &gt; p_B | data) for each prior</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(priors), <span class="cf">function</span>(i) {</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>  a <span class="ot">&lt;-</span> priors<span class="sc">$</span>alpha[i]</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">&lt;-</span> priors<span class="sc">$</span>beta[i]</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Posterior parameters</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>  alpha_A_post <span class="ot">&lt;-</span> a <span class="sc">+</span> deaths_A</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>  beta_A_post <span class="ot">&lt;-</span> b <span class="sc">+</span> survivals_A</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>  alpha_B_post <span class="ot">&lt;-</span> a <span class="sc">+</span> deaths_B</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>  beta_B_post <span class="ot">&lt;-</span> b <span class="sc">+</span> survivals_B</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Monte Carlo</span></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>  p_A <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_A_post, beta_A_post)</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>  p_B <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_B_post, beta_B_post)</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(p_A <span class="sc">&gt;</span> p_B)</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>priors<span class="sc">$</span><span class="st">`</span><span class="at">Prior Mean</span><span class="st">`</span> <span class="ot">&lt;-</span> <span class="fu">round</span>(priors<span class="sc">$</span>alpha <span class="sc">/</span> (priors<span class="sc">$</span>alpha <span class="sc">+</span> priors<span class="sc">$</span>beta), <span class="dv">3</span>)</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>priors<span class="sc">$</span><span class="st">`</span><span class="at">P(p_A &gt; p_B | data)</span><span class="st">`</span> <span class="ot">&lt;-</span> <span class="fu">round</span>(results, <span class="dv">3</span>)</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(priors[, <span class="fu">c</span>(<span class="st">"Prior"</span>, <span class="st">"Prior Mean"</span>, <span class="st">"P(p_A &gt; p_B | data)"</span>)],</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Sensitivity of posterior probability to prior specification"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Sensitivity of posterior probability to prior specification</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Prior</th>
<th style="text-align: right;">Prior Mean</th>
<th style="text-align: right;">P(p_A &gt; p_B | data)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Uniform Beta(1,1)</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.98</td>
</tr>
<tr class="even">
<td style="text-align: left;">Jeffreys Beta(0.5,0.5)</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.99</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Skeptical Beta(2,20)</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.97</td>
</tr>
<tr class="even">
<td style="text-align: left;">Optimistic Beta(1,10)</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.98</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Strong Beta(5,50)</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.94</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The results demonstrate remarkable robustness. Regardless of whether we use a uniform prior, the Jeffreys prior, or informative priors centered on different mortality rates, the posterior probability that Treatment A has a higher death rate than Treatment B remains above 0.93. This robustness occurs because the sample size (100 patients per treatment) is large enough that the likelihood generally dominates the prior. The data speak loudly, and the conclusion is not an artifact of our prior beliefs.</p>
<p>This sensitivity analysis is particularly important in clinical settings, where different stakeholders may have different prior beliefs about treatment efficacy. The fact that all reasonable priors lead to the same qualitative conclusion (strong evidence that A is worse than B) strengthens the case for preferring Treatment B.</p>
</div>
</section>
<section id="neyman-pearson" class="level3">
<h3 class="anchored" data-anchor-id="neyman-pearson">Neyman-Pearson</h3>
<p>The motivation for the Neyman-Pearson (NP) approach was W.S. Gosset, the famous <code>Student</code> who invented the <span class="math inline">\(t-\)</span>test. In analyzing a hypothesis, Student argued that a hypothesis is not rejected unless an alternative is available that provides a more plausible explanation of the data, in which case. Mathematically, this suggests analyzing the likelihood ratio, <span class="math display">\[
\mathcal{LR}_{0,1}=\frac{p\left(  y \mid H_{0}\right)  }{p\left( y \mid H_{1}\right)  }\text{,}%
\]</span> and rejecting the null in favor of the alternative when the likelihood ratio is small enough, <span class="math inline">\(\mathcal{LR}_{0,1}&lt;k\)</span>. This procedures conforms in spirit with the Bayesian approach.</p>
<p>The main problem was one of finding a value of the cut off parameter <span class="math inline">\(k.\)</span> From the discussion above, by varying <span class="math inline">\(k\)</span>, one varies the probabilities of type one and type two errors in the testing procedure. Neyman and Pearson (1933a) argued that the balance between Type I and II errors is subjective: “<em>how the balance (between the type I and II errors) should be struck must be left to the investigator</em>”. This approach, however, was not “objective”, and they then advocated fixing <span class="math inline">\(\alpha\)</span>, the probability of a type I error, in order to determine <span class="math inline">\(k\)</span>. This led to their famous lemma:</p>
<div id="lem-np" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.2 (Neyman-Pearson Lemma)</strong></span> Consider the simple hypothesis test of <span class="math inline">\(H_{0}:\theta=\theta_{0}\)</span> versus <span class="math inline">\(H_{1}:\theta =\theta_{1}\)</span> and suppose that the null is rejected if <span class="math inline">\(\mathcal{LR}_{0,1}&lt;k_{\alpha}\)</span>, where <span class="math inline">\(k_{\alpha}\)</span> is chosen to fix the probability of a type I error at <span class="math inline">\(\alpha:\)</span>% <span class="math display">\[
\alpha=P \left[  y:\mathcal{LR}_{0,1}&lt;k_{\alpha} \mid H_{0}\right]  \text{.}%
\]</span> Then, this test is the most powerful test of size <span class="math inline">\(\alpha\)</span> in the sense that any other test with greater power, must have a higher size.</p>
</div>
<p>In the case of composite hypothesis tests, parameter estimation is required under the alternative, which can be done via maximum likelihood, leading to the likelihood ratio <span class="math display">\[
\mathcal{LR}_{0,1}=\frac{p\left(  y \mid H_{0}\right)  }{\underset
{\theta\in\Theta}{\sup}p\left(  y \mid \Theta\right)  }=\frac{p\left( y \mid H_{0}\right)  }{p\left(  y \mid \widehat{\theta}\right)  }\text{,}%
\]</span> where <span class="math inline">\(\widehat{\theta}\)</span> is the MLE. Because of this, <span class="math inline">\(0\leq\mathcal{LR}_{0,1}\leq 1\)</span> for composite hypotheses. In multi-parameter cases, finding the distribution of the likelihood ratio is more difficult, requiring asymptotic approximations to calibrate <span class="math inline">\(k_{\alpha}.\)</span></p>
<p>At first glance, the NP approach appears similar to the Bayesian approach, as it takes into account the likelihood ratio. However, like the <span class="math inline">\(p-\)</span>value, the NP approach has a critical flaw. Neyman and Pearson fix the Type I error, and then minimizes the type II error. In many practical cases, <span class="math inline">\(\alpha\)</span> is set at <span class="math inline">\(5\%\)</span> and the resulting <span class="math inline">\(\beta\)</span> is often very small, close to 0. Why is this a reasonable procedure? Given the previous discussion, this is essentially a very strong prior over the relative benefits/costs of different types of errors. While these assumptions may be warranted in certain settings, it is difficult to a priori understand why this procedure would generically make sense. The next section highlights how the <span class="math inline">\(p-\)</span>value and NP approaches can generate counterintuitive and even absurd results in standard settings.</p>
</section>
</section>
<section id="sequential-analysis" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="sequential-analysis"><span class="header-section-number">6.5</span> Sequential Analysis</h2>
<p>Sequential analysis represents one of the most natural applications of Bayesian reasoning, allowing researchers to evaluate evidence as it accumulates and make principled decisions about when sufficient information has been gathered. Unlike fixed-sample designs that predetermine the number of observations, sequential methods continuously update beliefs about hypotheses and can terminate data collection once a satisfactory conclusion is reached.</p>
<p>The foundations of sequential analysis were laid by Abraham Wald during World War II, with his development of the Sequential Probability Ratio Test (SPRT) <span class="citation" data-cites="wald1945sequential">(<a href="references.html#ref-wald1945sequential" role="doc-biblioref">Wald 1945</a>)</span>. Wald’s work, later expanded in his book <em>Sequential Analysis</em> <span class="citation" data-cites="wald1947sequential">(<a href="references.html#ref-wald1947sequential" role="doc-biblioref">Wald 1947</a>)</span>, demonstrated that sequential procedures could reduce the expected sample size by up to 50% compared to fixed-sample tests while maintaining the same error rates. The key insight was that rather than collecting a predetermined number of observations and then analyzing them, one could examine the data after each observation and stop as soon as the evidence was sufficiently strong in either direction.</p>
<section id="the-bayesian-framework-for-sequential-testing" class="level3">
<h3 class="anchored" data-anchor-id="the-bayesian-framework-for-sequential-testing">The Bayesian Framework for Sequential Testing</h3>
<p>The Bayesian approach to sequential testing provides a coherent framework for deciding when to stop collecting data. Consider testing <span class="math inline">\(H_0: \theta \in \Theta_0\)</span> versus <span class="math inline">\(H_1: \theta \in \Theta_1\)</span>. After observing data <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>, the posterior odds are <span class="math display">\[
\frac{P(H_0 \mid y_{1:n})}{P(H_1 \mid y_{1:n})} = \frac{p(y_{1:n} \mid H_0)}{p(y_{1:n} \mid H_1)} \cdot \frac{P(H_0)}{P(H_1)}.
\]</span></p>
<p>A natural stopping rule is to continue sampling until the posterior probability of one hypothesis exceeds some threshold. For example, stop and accept <span class="math inline">\(H_0\)</span> if <span class="math inline">\(P(H_0 \mid y_{1:n}) &gt; 1 - \alpha\)</span>, or stop and accept <span class="math inline">\(H_1\)</span> if <span class="math inline">\(P(H_1 \mid y_{1:n}) &gt; 1 - \beta\)</span>. This approach has a compelling interpretation: we continue gathering evidence until we are sufficiently confident in our conclusion.</p>
<p>The Bayesian perspective offers a key advantage over frequentist sequential procedures: the posterior probability is always valid, regardless of when or why sampling stopped. As <span class="citation" data-cites="edwards1963bayesian">Edwards, Lindman, and Savage (<a href="references.html#ref-edwards1963bayesian" role="doc-biblioref">1963</a>)</span> noted, the likelihood principle implies that the rules governing when data collection stops are irrelevant to data interpretation. This property, sometimes called <em>optional stopping</em>, means that Bayesian inference is immune to the criticism that plagues frequentist sequential analysis, where p-values become invalid if the stopping rule is not prespecified.</p>
</section>
<section id="walds-sequential-probability-ratio-test" class="level3">
<h3 class="anchored" data-anchor-id="walds-sequential-probability-ratio-test">Wald’s Sequential Probability Ratio Test</h3>
<p>While Wald’s SPRT was developed from a frequentist perspective, it has deep connections to Bayesian testing. For simple hypotheses <span class="math inline">\(H_0: \theta = \theta_0\)</span> versus <span class="math inline">\(H_1: \theta = \theta_1\)</span>, the SPRT accumulates the likelihood ratio <span class="math display">\[
\Lambda_n = \frac{p(y_{1:n} \mid \theta_1)}{p(y_{1:n} \mid \theta_0)} = \prod_{i=1}^{n} \frac{p(y_i \mid \theta_1)}{p(y_i \mid \theta_0)}
\]</span> and stops when <span class="math inline">\(\Lambda_n \leq A\)</span> (accept <span class="math inline">\(H_0\)</span>) or <span class="math inline">\(\Lambda_n \geq B\)</span> (accept <span class="math inline">\(H_1\)</span>), where the boundaries <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are chosen to achieve desired error rates.</p>
<p>This is precisely the Bayes factor for simple hypotheses. With equal prior probabilities, the SPRT stopping rule becomes: stop and accept <span class="math inline">\(H_0\)</span> when the posterior probability exceeds <span class="math inline">\(B/(1+B)\)</span>, or stop and accept <span class="math inline">\(H_1\)</span> when it exceeds <span class="math inline">\(1/(1+A)\)</span>. Wald proved that among all tests with the same error rates, the SPRT minimizes the expected sample size under both hypotheses, a remarkable optimality property.</p>
</section>
<section id="applications-in-clinical-trials" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-clinical-trials">Applications in Clinical Trials</h3>
<p>Clinical trials represent perhaps the most important application of sequential analysis, where the ethical imperative to minimize patient exposure to inferior treatments aligns with the statistical goal of efficient inference. Peter Armitage pioneered the application of sequential methods to clinical trials in the 1950s and 1960s <span class="citation" data-cites="armitage1975sequential">(<a href="references.html#ref-armitage1975sequential" role="doc-biblioref">Armitage 1975</a>)</span>, demonstrating how these methods could reduce trial duration and the number of patients receiving suboptimal treatments.</p>
<p>Modern clinical trial design frequently employs <em>group sequential designs</em>, which allow interim analyses at predetermined points during patient accrual. The key challenge is controlling the overall Type I error rate when multiple looks at the data are permitted. Frequentist approaches use spending functions to allocate alpha across interim analyses <span class="citation" data-cites="demets1994interim">(<a href="references.html#ref-demets1994interim" role="doc-biblioref">DeMets and Lan 1994</a>)</span>, while Bayesian approaches naturally handle multiple looks through the posterior probability framework.</p>
<p>The Bayesian approach to clinical trial design has been championed by Donald Berry, whose seminal work <span class="citation" data-cites="berry1985interim">(<a href="references.html#ref-berry1985interim" role="doc-biblioref">D. A. Berry 1985</a>)</span> critiqued traditional hypothesis testing and advocated for a framework where sampling stops when the posterior probability that one treatment is superior exceeds a specified threshold. Berry’s methodology, later expanded in <span class="citation" data-cites="berry2006bayesian">S. M. Berry et al. (<a href="references.html#ref-berry2006bayesian" role="doc-biblioref">2010</a>)</span>, integrates Bayesian decision theory into trial design and monitoring, enabling continuous assessment of accumulating data and facilitating decisions on early termination for efficacy or futility.</p>
<section id="berrys-bayesian-framework-for-clinical-trials" class="level4">
<h4 class="anchored" data-anchor-id="berrys-bayesian-framework-for-clinical-trials">Berry’s Bayesian Framework for Clinical Trials</h4>
<p>Consider a clinical trial comparing a new treatment to a control, where the primary outcome is binary (success or failure). Let <span class="math inline">\(p_T\)</span> denote the success probability under treatment and <span class="math inline">\(p_C\)</span> under control. The quantity of interest is typically the treatment effect, which can be parameterized as the risk difference <span class="math inline">\(\delta = p_T - p_C\)</span>, the relative risk <span class="math inline">\(p_T/p_C\)</span>, or the odds ratio.</p>
<p>Berry’s approach specifies conjugate beta priors for each success probability: <span class="math display">\[
p_C \sim \text{Beta}(\alpha_C, \beta_C) \quad \text{and} \quad p_T \sim \text{Beta}(\alpha_T, \beta_T).
\]</span> Non-informative priors correspond to <span class="math inline">\(\alpha = \beta = 1\)</span> (uniform) or <span class="math inline">\(\alpha = \beta = 0.5\)</span> (Jeffreys). After observing <span class="math inline">\(x_C\)</span> successes in <span class="math inline">\(n_C\)</span> control patients and <span class="math inline">\(x_T\)</span> successes in <span class="math inline">\(n_T\)</span> treated patients, the posteriors are <span class="math display">\[
p_C \mid \text{data} \sim \text{Beta}(\alpha_C + x_C, \beta_C + n_C - x_C)
\]</span> <span class="math display">\[
p_T \mid \text{data} \sim \text{Beta}(\alpha_T + x_T, \beta_T + n_T - x_T).
\]</span></p>
<p>The key quantity for decision-making is the posterior probability that treatment is superior: <span class="math display">\[
P(p_T &gt; p_C \mid \text{data}) = \int_0^1 \int_{p_C}^{1} f(p_C \mid \text{data}) f(p_T \mid \text{data}) \, dp_T \, dp_C,
\]</span> where <span class="math inline">\(f(\cdot \mid \text{data})\)</span> denotes the posterior density. This integral can be computed via Monte Carlo simulation by drawing samples from each posterior and computing the proportion where <span class="math inline">\(p_T^{(s)} &gt; p_C^{(s)}\)</span>.</p>
<p>Berry’s stopping rules are defined in terms of posterior probability thresholds:</p>
<ul>
<li><p><em>Efficacy stopping</em>: Stop and declare treatment effective if <span class="math inline">\(P(p_T &gt; p_C + \delta_{\min} \mid \text{data}) &gt; \theta_E\)</span>, where <span class="math inline">\(\delta_{\min}\)</span> is the minimum clinically meaningful difference and <span class="math inline">\(\theta_E\)</span> is the efficacy threshold (e.g., 0.95 or 0.99).</p></li>
<li><p><em>Futility stopping</em>: Stop for futility if <span class="math inline">\(P(p_T &gt; p_C \mid \text{data}) &lt; \theta_F\)</span>, where <span class="math inline">\(\theta_F\)</span> is the futility threshold (e.g., 0.05 or 0.10).</p></li>
</ul>
<p>A more sophisticated approach uses <em>predictive probability of success</em> (PPoS), which accounts for future data that might be collected. The PPoS at interim analysis is the probability, given current data, that the trial will demonstrate efficacy if continued to the planned maximum sample size: <span class="math display">\[
\text{PPoS} = P\left( P(p_T &gt; p_C \mid \text{all data}) &gt; \theta_E \mid \text{current data} \right).
\]</span> This is computed by averaging over the predictive distribution of future outcomes. If PPoS falls below a threshold (e.g., 0.05), continuing the trial is unlikely to yield a positive result, making futility stopping appropriate.</p>
</section>
<section id="operating-characteristics" class="level4">
<h4 class="anchored" data-anchor-id="operating-characteristics">Operating Characteristics</h4>
<p>The operating characteristics of a Bayesian sequential design, including the Type I error rate, power, and expected sample size, are determined through simulation. For a given set of true parameter values <span class="math inline">\((\pi_C, \pi_T)\)</span>, one simulates many trials, applies the stopping rules at each interim analysis, and tabulates the outcomes. This approach allows calibration of the thresholds <span class="math inline">\(\theta_E\)</span> and <span class="math inline">\(\theta_F\)</span> to achieve desired frequentist properties if regulatory requirements demand it.</p>
<p>Berry emphasized that Bayesian methods do not require such calibration for logical validity, the posterior probability is always a coherent measure of evidence regardless of the stopping rule. However, demonstrating acceptable operating characteristics facilitates regulatory acceptance and provides assurance that the design performs well across plausible scenarios.</p>
<p>The FDA has increasingly recognized the value of Bayesian methods in drug development, issuing guidance documents on their use <span class="citation" data-cites="fda2019bayesian">(<a href="references.html#ref-fda2019bayesian" role="doc-biblioref">U.S. Food and Drug Administration 2010</a>)</span>. Berry Consultants has designed hundreds of adaptive trials using these methods, demonstrating their practical viability across therapeutic areas including oncology, cardiovascular disease, and rare diseases.</p>
<div id="exm-berry-trial" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.10 (Bayesian Sequential Trial for a Rare Disease)</strong></span> Consider a Phase II trial for a rare autoimmune condition where the standard of care has a 30% response rate. A new therapy is hypothesized to improve response to 50%. Due to the rarity of the condition, we plan for a maximum of 60 patients (30 per arm) with interim analyses every 10 patients per arm.</p>
<p>We use weakly informative Beta(1,1) priors for both response rates. The efficacy threshold is <span class="math inline">\(\theta_E = 0.95\)</span> for declaring <span class="math inline">\(P(p_T &gt; p_C \mid \text{data}) &gt; 0.95\)</span>, and the futility threshold is <span class="math inline">\(\theta_F = 0.10\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Simulate Bayesian Sequential Trial</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2024</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameters (unknown to trialists)</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>p_control_true <span class="ot">&lt;-</span> <span class="fl">0.30</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>p_treatment_true <span class="ot">&lt;-</span> <span class="fl">0.50</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior parameters (Beta(1,1) = uniform)</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>alpha_prior <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>beta_prior <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Trial parameters</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>n_max_per_arm <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>interim_points <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>)  <span class="co"># Patients per arm at each look</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">10000</span>  <span class="co"># Monte Carlo samples for posterior probability</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Efficacy and futility thresholds</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>theta_E <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>theta_F <span class="ot">&lt;-</span> <span class="fl">0.10</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate patient outcomes</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>n_control <span class="ot">&lt;-</span> n_max_per_arm</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>n_treatment <span class="ot">&lt;-</span> n_max_per_arm</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>control_outcomes <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_control, <span class="dv">1</span>, p_control_true)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>treatment_outcomes <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_treatment, <span class="dv">1</span>, p_treatment_true)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now we perform the sequential analysis according to the rules we have set up. We stop the trial if the posterior probability of the treatment being superior to the control is greater than 0.95 or less than 0.10.</p>
<p>The sequential analysis proceeds as follows. At each interim analysis, we compute the posterior distributions for both response rates using the accumulated data. For the control arm, the posterior is <span class="math inline">\(\text{Beta}(\alpha_{\text{prior}} + x_C, \beta_{\text{prior}} + n - x_C)\)</span>, where <span class="math inline">\(x_C\)</span> is the number of responders observed. Similarly, for the treatment arm, the posterior is <span class="math inline">\(\text{Beta}(\alpha_{\text{prior}} + x_T, \beta_{\text{prior}} + n - x_T)\)</span>.</p>
<p>To evaluate the probability that the treatment response rate exceeds the control rate, <span class="math inline">\(P(p_T &gt; p_C \mid \text{data})\)</span>, we draw Monte Carlo samples from both posterior distributions and compute the proportion of samples where <span class="math inline">\(p_T &gt; p_C\)</span>. This probability is then compared against our decision thresholds: if it exceeds <span class="math inline">\(\theta_E = 0.95\)</span>, we stop for efficacy; if it falls below <span class="math inline">\(\theta_F = 0.10\)</span>, we stop for futility; otherwise, we continue enrolling patients.</p>
<p>The table below shows the results at each interim analysis, including the accumulated number of responders in each arm and the posterior probability of treatment superiority:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Sequential Analysis</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage for results</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">interim =</span> <span class="fu">integer</span>(),</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_per_arm =</span> <span class="fu">integer</span>(),</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_control =</span> <span class="fu">integer</span>(),</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">x_treatment =</span> <span class="fu">integer</span>(),</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_superior =</span> <span class="fu">numeric</span>(),</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">decision =</span> <span class="fu">character</span>()</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Conduct sequential analysis</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>final_decision <span class="ot">&lt;-</span> <span class="st">"Continue"</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="fu">seq_along</span>(interim_points)) {</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>  n_current <span class="ot">&lt;-</span> interim_points[k]</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Accumulated data</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>  x_C <span class="ot">&lt;-</span> <span class="fu">sum</span>(control_outcomes[<span class="dv">1</span><span class="sc">:</span>n_current])</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>  x_T <span class="ot">&lt;-</span> <span class="fu">sum</span>(treatment_outcomes[<span class="dv">1</span><span class="sc">:</span>n_current])</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Posterior parameters</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>  alpha_C_post <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> x_C</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>  beta_C_post <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> n_current <span class="sc">-</span> x_C</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>  alpha_T_post <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> x_T</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>  beta_T_post <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> n_current <span class="sc">-</span> x_T</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Monte Carlo estimate of P(p_T &gt; p_C | data)</span></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>  p_C_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_C_post, beta_C_post)</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>  p_T_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_sims, alpha_T_post, beta_T_post)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>  p_superior <span class="ot">&lt;-</span> <span class="fu">mean</span>(p_T_samples <span class="sc">&gt;</span> p_C_samples)</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Decision</span></span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (p_superior <span class="sc">&gt;</span> theta_E) {</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>    decision <span class="ot">&lt;-</span> <span class="st">"Stop for Efficacy"</span></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>    final_decision <span class="ot">&lt;-</span> decision</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> <span class="cf">if</span> (p_superior <span class="sc">&lt;</span> theta_F) {</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>    decision <span class="ot">&lt;-</span> <span class="st">"Stop for Futility"</span></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>    final_decision <span class="ot">&lt;-</span> decision</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>    decision <span class="ot">&lt;-</span> <span class="st">"Continue"</span></span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">data.frame</span>(</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>    <span class="at">interim =</span> k,</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_per_arm =</span> n_current,</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_control =</span> x_C,</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>    <span class="at">x_treatment =</span> x_T,</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_superior =</span> <span class="fu">round</span>(p_superior, <span class="dv">4</span>),</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>    <span class="at">decision =</span> decision</span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (decision <span class="sc">!=</span> <span class="st">"Continue"</span>) <span class="cf">break</span></span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(results, </span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>             <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Interim"</span>, <span class="st">"N per arm"</span>, <span class="st">"Control successes"</span>, </span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>                          <span class="st">"Treatment successes"</span>, <span class="st">"P(Treatment &gt; Control)"</span>, <span class="st">"Decision"</span>),</span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Sequential analysis results"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Sequential analysis results</caption>
<colgroup>
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 20%">
<col style="width: 22%">
<col style="width: 26%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">Interim</th>
<th style="text-align: right;">N per arm</th>
<th style="text-align: right;">Control successes</th>
<th style="text-align: right;">Treatment successes</th>
<th style="text-align: right;">P(Treatment &gt; Control)</th>
<th style="text-align: left;">Decision</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">0.81</td>
<td style="text-align: left;">Continue</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">0.83</td>
<td style="text-align: left;">Continue</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.78</td>
<td style="text-align: left;">Continue</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The trial data shows the accumulation of evidence across interim analyses. Let us visualize the posterior distributions at the final interim analysis:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Plot Posterior Distributions at Final Analysis</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot posterior distributions at final analysis</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>final_n <span class="ot">&lt;-</span> results<span class="sc">$</span>n_per_arm[<span class="fu">nrow</span>(results)]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>final_x_C <span class="ot">&lt;-</span> results<span class="sc">$</span>x_control[<span class="fu">nrow</span>(results)]</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>final_x_T <span class="ot">&lt;-</span> results<span class="sc">$</span>x_treatment[<span class="fu">nrow</span>(results)]</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior parameters</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>alpha_C_final <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> final_x_C</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>beta_C_final <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> final_n <span class="sc">-</span> final_x_C</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>alpha_T_final <span class="ot">&lt;-</span> alpha_prior <span class="sc">+</span> final_x_T</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>beta_T_final <span class="ot">&lt;-</span> beta_prior <span class="sc">+</span> final_n <span class="sc">-</span> final_x_T</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create plot</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>p_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">500</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p_grid, <span class="fu">dbeta</span>(p_grid, alpha_T_final, beta_T_final), <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">8</span>),</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Response probability"</span>, <span class="at">ylab =</span> <span class="st">"Posterior density"</span>,</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Posterior distributions at final analysis"</span>)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(p_grid, <span class="fu">dbeta</span>(p_grid, alpha_C_final, beta_C_final), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> p_control_true, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> p_treatment_true, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Treatment"</span>, <span class="st">"Control"</span>, <span class="st">"True values"</span>),</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"gray"</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">bty=</span><span class="st">"n"</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can also examine the posterior distribution of the treatment effect (risk difference):</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior of risk difference via Monte Carlo</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>p_C_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">50000</span>, alpha_C_final, beta_C_final)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>p_T_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">50000</span>, alpha_T_final, beta_T_final)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>risk_diff <span class="ot">&lt;-</span> p_T_samples <span class="sc">-</span> p_C_samples</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output cell-output-stdout">
<pre><code>Posterior summary for risk difference (p_T - p_C):</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  Mean: 0.095 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  95% Credible Interval: [ -0.14 , 0.33 ]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  P(p_T &gt; p_C): 0.78 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  P(p_T &gt; p_C + 0.10): 0.49 </code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-operating-chars" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.11 (Operating Characteristics via Simulation)</strong></span> To understand how the Bayesian sequential design performs across different scenarios, we simulate its operating characteristics. This involves running many hypothetical trials under various true parameter values and recording the outcomes. The simulation proceeds by generating a large number of hypothetical trials under each scenario. For each trial, patient outcomes are generated according to the true response probabilities, and the sequential monitoring procedure is applied at each interim analysis point. At each look, posterior distributions are updated and the probability that treatment is superior to control is computed via Monte Carlo sampling. The trial stops early for efficacy if this probability exceeds the efficacy threshold <span class="math inline">\(\theta_E = 0.95\)</span>, or for futility if it falls below the futility threshold <span class="math inline">\(\theta_F = 0.10\)</span>. If neither stopping criterion is met at any interim analysis, the trial continues to the maximum sample size.</p>
<p>The simulation tracks three key operating characteristics: the decision reached (efficacy, futility, or maximum sample size), the final sample size at which the trial stopped, and the posterior probability of treatment superiority at that point. By replicating this process across many trials under different true parameter values—ranging from no treatment effect to large effects—we can assess the design’s ability to correctly identify effective treatments, avoid false positives, and efficiently use resources by stopping early when appropriate.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Simulate Operating Characteristics</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to simulate one trial</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>simulate_trial <span class="ot">&lt;-</span> <span class="cf">function</span>(p_C_true, p_T_true, <span class="at">n_max =</span> <span class="dv">30</span>, </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">interim_points =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>),</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">theta_E =</span> <span class="fl">0.95</span>, <span class="at">theta_F =</span> <span class="fl">0.10</span>,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>                           <span class="at">alpha_prior =</span> <span class="dv">1</span>, <span class="at">beta_prior =</span> <span class="dv">1</span>, <span class="at">n_mc =</span> <span class="dv">5000</span>) {</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Generate all outcomes upfront</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>  control_outcomes <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_max, <span class="dv">1</span>, p_C_true)</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>  treatment_outcomes <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_max, <span class="dv">1</span>, p_T_true)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n_current <span class="cf">in</span> interim_points) {</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    x_C <span class="ot">&lt;-</span> <span class="fu">sum</span>(control_outcomes[<span class="dv">1</span><span class="sc">:</span>n_current])</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    x_T <span class="ot">&lt;-</span> <span class="fu">sum</span>(treatment_outcomes[<span class="dv">1</span><span class="sc">:</span>n_current])</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior samples</span></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>    p_C_samp <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_mc, alpha_prior <span class="sc">+</span> x_C, beta_prior <span class="sc">+</span> n_current <span class="sc">-</span> x_C)</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    p_T_samp <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n_mc, alpha_prior <span class="sc">+</span> x_T, beta_prior <span class="sc">+</span> n_current <span class="sc">-</span> x_T)</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    p_superior <span class="ot">&lt;-</span> <span class="fu">mean</span>(p_T_samp <span class="sc">&gt;</span> p_C_samp)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (p_superior <span class="sc">&gt;</span> theta_E) {</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">decision =</span> <span class="st">"Efficacy"</span>, <span class="at">n_final =</span> n_current, <span class="at">p_superior =</span> p_superior))</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> <span class="cf">if</span> (p_superior <span class="sc">&lt;</span> theta_F) {</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">decision =</span> <span class="st">"Futility"</span>, <span class="at">n_final =</span> n_current, <span class="at">p_superior =</span> p_superior))</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Reached max sample size</span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">decision =</span> <span class="st">"Max reached"</span>, <span class="at">n_final =</span> n_max, <span class="at">p_superior =</span> p_superior))</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate under different scenarios</span></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>n_trials <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>scenarios <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">scenario =</span> <span class="fu">c</span>(<span class="st">"Null (no effect)"</span>, <span class="st">"Small effect"</span>, <span class="st">"Moderate effect"</span>, <span class="st">"Large effect"</span>),</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_C =</span> <span class="fu">c</span>(<span class="fl">0.30</span>, <span class="fl">0.30</span>, <span class="fl">0.30</span>, <span class="fl">0.30</span>),</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a>  <span class="at">p_T =</span> <span class="fu">c</span>(<span class="fl">0.30</span>, <span class="fl">0.40</span>, <span class="fl">0.50</span>, <span class="fl">0.60</span>)</span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>results_all <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(scenarios)) {</span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>  trials <span class="ot">&lt;-</span> <span class="fu">replicate</span>(n_trials, </span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">simulate_trial</span>(scenarios<span class="sc">$</span>p_C[i], scenarios<span class="sc">$</span>p_T[i]),</span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>                      <span class="at">simplify =</span> <span class="cn">FALSE</span>)</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>  decisions <span class="ot">&lt;-</span> <span class="fu">sapply</span>(trials, <span class="cf">function</span>(x) x<span class="sc">$</span>decision)</span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>  n_finals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(trials, <span class="cf">function</span>(x) x<span class="sc">$</span>n_final)</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>  results_all[[i]] <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a>    <span class="at">Scenario =</span> scenarios<span class="sc">$</span>scenario[i],</span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">True p_C</span><span class="st">`</span> <span class="ot">=</span> scenarios<span class="sc">$</span>p_C[i],</span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">True p_T</span><span class="st">`</span> <span class="ot">=</span> scenarios<span class="sc">$</span>p_T[i],</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">P(Efficacy)</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">mean</span>(decisions <span class="sc">==</span> <span class="st">"Efficacy"</span>),</span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">P(Futility)</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">mean</span>(decisions <span class="sc">==</span> <span class="st">"Futility"</span>),</span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">E[N per arm]</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">mean</span>(n_finals),</span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">check.names =</span> <span class="cn">FALSE</span></span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a>op_chars <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, results_all)</span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(op_chars, <span class="at">digits =</span> <span class="dv">3</span>,</span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">"Operating characteristics of the Bayesian sequential design"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Operating characteristics of the Bayesian sequential design</caption>
<colgroup>
<col style="width: 23%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Scenario</th>
<th style="text-align: right;">True p_C</th>
<th style="text-align: right;">True p_T</th>
<th style="text-align: right;">P(Efficacy)</th>
<th style="text-align: right;">P(Futility)</th>
<th style="text-align: right;">E[N per arm]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Null (no effect)</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.095</td>
<td style="text-align: right;">0.181</td>
<td style="text-align: right;">26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Small effect</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.281</td>
<td style="text-align: right;">0.071</td>
<td style="text-align: right;">26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Moderate effect</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">0.543</td>
<td style="text-align: right;">0.028</td>
<td style="text-align: right;">23</td>
</tr>
<tr class="even">
<td style="text-align: left;">Large effect</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">0.807</td>
<td style="text-align: right;">0.002</td>
<td style="text-align: right;">20</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Under the null hypothesis (no treatment effect), the probability of declaring efficacy represents the Type I error rate. Under the alternative hypotheses, this probability represents power. The expected sample size shows the efficiency gains from sequential stopping. Notice that under the null, many trials stop early for futility, substantially reducing the expected sample size compared to a fixed design that would always enroll 30 patients per arm.</p>
</div>
</section>
</section>
<section id="sequential-analysis-for-rare-diseases" class="level3">
<h3 class="anchored" data-anchor-id="sequential-analysis-for-rare-diseases">Sequential Analysis for Rare Diseases</h3>
<p>Sequential and adaptive designs are particularly valuable for clinical trials in rare diseases, where small patient populations make traditional fixed-sample designs impractical. With potentially only hundreds or even dozens of patients worldwide having a particular condition, every enrolled patient provides precious information that must be used efficiently.</p>
<p>Several innovative approaches have emerged for rare disease trials. The small n Sequential Multiple Assignment Randomized Trial (snSMART) design <span class="citation" data-cites="wei2018small">(<a href="references.html#ref-wei2018small" role="doc-biblioref">Wei et al. 2018</a>)</span> provides a framework for testing multiple treatments within a single trial, re-randomizing non-responding patients to alternative therapies in subsequent stages. This design achieves increased statistical power over traditional single-stage designs by leveraging information from all treatment sequences.</p>
<p><span class="citation" data-cites="hilgers2008efficient">Hilgers, Roes, and Stallard (<a href="references.html#ref-hilgers2008efficient" role="doc-biblioref">2016</a>)</span> provided an overview of design options for achieving valid randomized clinical trials in rare diseases, emphasizing that sequential procedures can substantially reduce the expected sample size while maintaining statistical validity. Their work demonstrated that group sequential designs and adaptive approaches can cut average trial sizes by 30-50% compared to fixed designs.</p>
<p>The application of Bayesian methods specifically to rare disease trials has been systematically reviewed by <span class="citation" data-cites="chen2022application">Chen et al. (<a href="references.html#ref-chen2022application" role="doc-biblioref">2022</a>)</span>, who identified both the opportunities and hurdles in this setting. The Bayesian framework naturally accommodates informative priors based on historical data, external controls, or expert elicitation, which is particularly valuable when patient populations are too small for purely data-driven inference. However, the authors caution that prior sensitivity analyses are essential when sample sizes are small, as the prior can dominate posterior conclusions.</p>
<p><span class="citation" data-cites="benda2016sequential">Benda et al. (<a href="references.html#ref-benda2016sequential" role="doc-biblioref">2016</a>)</span> evaluated the performance of sequential methods specifically in small-sample settings with normally distributed responses. They found that careful attention to the distribution of test statistics is necessary to maintain nominal Type I error rates, as large-sample approximations may not hold. Their recommendations provide practical guidance for implementing sequential designs in trials with limited participants.</p>
</section>
<section id="sec-mab-experiments" class="level3">
<h3 class="anchored" data-anchor-id="sec-mab-experiments">Multi-Armed Bandit Experiments</h3>
<p>A closely related approach to sequential analysis is the multi-armed bandit, a framework that has revolutionized how online experiments are conducted in the digital economy. While classical A/B tests require predetermining sample sizes and waiting for experiments to conclude before making decisions, multi-armed bandits adaptively allocate traffic based on accumulating evidence. This approach, implemented in platforms like Google Analytics Content Experiments <span class="citation" data-cites="scott2013bandit scott2015multiarmed">(<a href="references.html#ref-scott2013bandit" role="doc-biblioref">Steven L. Scott 2013</a>; <a href="references.html#ref-scott2015multiarmed" role="doc-biblioref">Steven L. Scott 2015</a>)</span>, represents a practical application of Bayesian sequential analysis at massive scale.</p>
<p>A multi-armed bandit is a type of experiment characterized by two key features: the goal is to find the best or most profitable action (or <em>arm</em>), and the randomization distribution can be updated as the experiment progresses. The colorful name originates from a hypothetical scenario where a gambler faces several slot machines (colloquially called one-armed bandits) with potentially different, unknown payoff rates. The gambler wants to find the machine with the best payout, but also wants to maximize winnings during the search. This creates the fundamental tension between <em>exploiting</em> arms that have performed well in the past and <em>exploring</em> new or seemingly inferior arms that might perform even better.</p>
<section id="the-bayesian-approach-to-bandits" class="level4">
<h4 class="anchored" data-anchor-id="the-bayesian-approach-to-bandits">The Bayesian Approach to Bandits</h4>
<p>The Bayesian framework provides an elegant solution to the bandit problem. Suppose we have <span class="math inline">\(K\)</span> arms (variations), each with an unknown success probability <span class="math inline">\(\theta_k\)</span> for <span class="math inline">\(k = 1, \ldots, K\)</span>. Using Bayes’ theorem, we can compute the probability that each arm is the best based on observed data. Let <span class="math inline">\(x_k\)</span> denote the number of successes and <span class="math inline">\(n_k\)</span> the number of trials for arm <span class="math inline">\(k\)</span>. With a Beta prior <span class="math inline">\(\theta_k \sim \text{Beta}(\alpha, \beta)\)</span>, the posterior after observing data is <span class="math display">\[
\theta_k \mid x_k, n_k \sim \text{Beta}(\alpha + x_k, \beta + n_k - x_k).
\]</span></p>
<p>The probability that arm <span class="math inline">\(k\)</span> is optimal is <span class="math display">\[
P(\theta_k &gt; \theta_j \text{ for all } j \neq k \mid \text{data}),
\]</span> which can be computed via Monte Carlo integration by sampling from the posterior distributions and counting how often arm <span class="math inline">\(k\)</span> produces the largest sample.</p>
<p>The key insight from <span class="citation" data-cites="scott2015multiarmed">Steven L. Scott (<a href="references.html#ref-scott2015multiarmed" role="doc-biblioref">2015</a>)</span> is that these posterior probabilities of optimality can serve directly as allocation weights. An arm that appears to be doing well receives more traffic, while an arm that is clearly underperforming receives less. The adjustments consider sample size and performance metrics together, providing confidence that we are adjusting for real performance differences rather than random chance.</p>
</section>
<section id="thompson-sampling" class="level4">
<h4 class="anchored" data-anchor-id="thompson-sampling">Thompson Sampling</h4>
<p>The algorithm that implements this Bayesian allocation is called Thompson sampling <span class="citation" data-cites="thompson1933likelihood">(<a href="references.html#ref-thompson1933likelihood" role="doc-biblioref">Thompson 1933</a>)</span>, one of the oldest heuristics for the bandit problem, dating to 1933. For binary outcomes with Beta priors, the algorithm proceeds as follows:</p>
<ol type="1">
<li><p>Initialize <span class="math inline">\(\alpha_k = \beta_k = 1\)</span> for each arm (uniform prior).</p></li>
<li><p>At each decision point, sample <span class="math inline">\(\tilde{\theta}_k\)</span> from the current posterior <span class="math inline">\(\text{Beta}(\alpha_k, \beta_k)\)</span> for each arm.</p></li>
<li><p>Select the arm with the highest sampled value: <span class="math inline">\(a_t = \arg\max_k \tilde{\theta}_k\)</span>.</p></li>
<li><p>Observe the outcome <span class="math inline">\(Y_t \in \{0, 1\}\)</span> and update: <span class="math inline">\(\alpha_{a_t} \leftarrow \alpha_{a_t} + Y_t\)</span>, <span class="math inline">\(\beta_{a_t} \leftarrow \beta_{a_t} + (1 - Y_t)\)</span>.</p></li>
</ol>
<p>Thompson sampling automatically balances exploration and exploitation. Arms with high posterior means are selected frequently (exploitation), but arms with high posterior variance, which indicates uncertainty, also have a chance of producing high samples (exploration). As data accumulates, posteriors concentrate around true values, and the algorithm increasingly exploits the best arm.</p>
</section>
<section id="benefits-over-classical-ab-testing" class="level4">
<h4 class="anchored" data-anchor-id="benefits-over-classical-ab-testing">Benefits Over Classical A/B Testing</h4>
<p>Experiments based on multi-armed bandits are typically much more efficient than classical A/B experiments based on hypothesis testing. As <span class="citation" data-cites="scott2013bandit">Steven L. Scott (<a href="references.html#ref-scott2013bandit" role="doc-biblioref">2013</a>)</span> explains, they are just as statistically valid but can produce answers far more quickly. The efficiency gains arise from two sources: traffic moves toward winning variations gradually rather than waiting for a final answer, and samples that would have gone to obviously inferior variations can be assigned to potential winners.</p>
<p>Consider a concrete example from <span class="citation" data-cites="scott2013bandit">Steven L. Scott (<a href="references.html#ref-scott2013bandit" role="doc-biblioref">2013</a>)</span>. Suppose a website has a 4% conversion rate, and a new variation actually converts at 5%. A standard power calculation for detecting this difference at 95% confidence requires 22,330 observations (11,165 per arm). At 100 visits per day, this experiment would take 223 days to complete. In a classical experiment, you wait 223 days, run the hypothesis test, and get your answer.</p>
<p>With a multi-armed bandit, the experiment can finish much sooner. In simulations, the bandit found the correct arm in 96.4% of cases (about the same error rate as the classical test), but the average experiment duration was only 66 days, saving 157 days of testing. The savings compound when experiments have more arms, because the classical approach requires Bonferroni-type corrections for multiple comparisons while the bandit naturally handles multiple arms through the posterior probability framework.</p>
<div id="exm-scott-bandit" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.12 (Reproducing Scott’s Bandit Experiment)</strong></span> We reproduce the simulation from <span class="citation" data-cites="scott2013bandit">Steven L. Scott (<a href="references.html#ref-scott2013bandit" role="doc-biblioref">2013</a>)</span>. The setup involves an original page with 4% conversion rate and a variation with 5% conversion rate, with 100 visits per day. A classical power calculation requires 223 days; we will see how the bandit performs.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Single Trial Simulation</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True conversion rates</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>theta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.04</span>, <span class="fl">0.05</span>)  <span class="co"># Original, Variation</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>arm_names <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Original"</span>, <span class="st">"Variation"</span>)</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Experiment parameters</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>visits_per_day <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>max_days <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>classical_days <span class="ot">&lt;-</span> <span class="dv">223</span>  <span class="co"># From power calculation</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Beta(1,1) priors</span></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>alpha_params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>beta_params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Storage</span></span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>prob_optimal_history <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, max_days, K)</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>n_mc <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Run single trial</span></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>end_day <span class="ot">&lt;-</span> max_days</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (day <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_days) {</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute probability each arm is optimal</span></span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>  theta_samples <span class="ot">&lt;-</span> <span class="fu">cbind</span>(</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rbeta</span>(n_mc, alpha_params[<span class="dv">1</span>], beta_params[<span class="dv">1</span>]),</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rbeta</span>(n_mc, alpha_params[<span class="dv">2</span>], beta_params[<span class="dv">2</span>])</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>  prob_optimal <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(theta_samples <span class="sc">==</span> <span class="fu">apply</span>(theta_samples, <span class="dv">1</span>, max))</span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>  prob_optimal_history[day, ] <span class="ot">&lt;-</span> prob_optimal</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Allocate today's traffic using Thompson sampling</span></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (v <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>visits_per_day) {</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>    theta_sampled <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(K, alpha_params, beta_params)</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">&lt;-</span> <span class="fu">which.max</span>(theta_sampled)</span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, theta_true[a])</span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a>    alpha_params[a] <span class="ot">&lt;-</span> alpha_params[a] <span class="sc">+</span> y</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a>    beta_params[a] <span class="ot">&lt;-</span> beta_params[a] <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> y)</span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check stopping: 95% confident one arm is best</span></span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">max</span>(prob_optimal) <span class="sc">&gt;</span> <span class="fl">0.95</span>) {</span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>    end_day <span class="ot">&lt;-</span> day</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot: Optimal arm probabilities over time</span></span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>end_day, prob_optimal_history[<span class="dv">1</span><span class="sc">:</span>end_day, <span class="dv">1</span>], <span class="at">type =</span> <span class="st">"l"</span>, </span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Time period"</span>, <span class="at">ylab =</span> <span class="st">"Probability of being optimal"</span>,</span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Optimal Arm Probabilities"</span>)</span>
<span id="cb49-54"><a href="#cb49-54" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>end_day, prob_optimal_history[<span class="dv">1</span><span class="sc">:</span>end_day, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb49-55"><a href="#cb49-55" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"gray40"</span>)</span>
<span id="cb49-56"><a href="#cb49-56" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"right"</span>, <span class="at">legend =</span> arm_names, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>), </span>
<span id="cb49-57"><a href="#cb49-57" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">bty =</span> <span class="st">"n"</span>)</span>
<span id="cb49-58"><a href="#cb49-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-59"><a href="#cb49-59" aria-hidden="true" tabindex="-1"></a><span class="co"># True success rates</span></span>
<span id="cb49-60"><a href="#cb49-60" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(theta_true, <span class="at">names.arg =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>),</span>
<span id="cb49-61"><a href="#cb49-61" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab =</span> <span class="st">"Arm"</span>, <span class="at">ylab =</span> <span class="st">""</span>, <span class="at">main =</span> <span class="st">"True success rate"</span>,</span>
<span id="cb49-62"><a href="#cb49-62" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.06</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="864"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Single Trial Simulation</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Experiment ended on day:"</span>, end_day, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Experiment ended on day: 46 </code></pre>
</div>
<details class="code-fold">
<summary>Single Trial Simulation</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Days saved vs classical:"</span>, classical_days <span class="sc">-</span> end_day, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Days saved vs classical: 177 </code></pre>
</div>
</div>
<p>The left panel shows the probability that each arm is optimal over time. The two curves are complementary (summing to 1) and fluctuate until eventually one crosses the 95% threshold. The right panel shows the true success rates that are unknown to the algorithm.</p>
<p>To understand the distribution of outcomes, we run 500 simulations and compare to the classical experiment:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2024</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>theta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.04</span>, <span class="fl">0.05</span>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>visits_per_day <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>classical_days <span class="ot">&lt;-</span> <span class="dv">223</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>n_mc <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to run one bandit trial</span></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>run_bandit_trial <span class="ot">&lt;-</span> <span class="cf">function</span>(theta_true, visits_per_day, <span class="at">max_days =</span> <span class="dv">300</span>) {</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>  alpha_params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>  beta_params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>  total_conversions <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (day <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_days) {</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute probability each arm is optimal</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    theta_samples <span class="ot">&lt;-</span> <span class="fu">cbind</span>(</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">rbeta</span>(n_mc, alpha_params[<span class="dv">1</span>], beta_params[<span class="dv">1</span>]),</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>      <span class="fu">rbeta</span>(n_mc, alpha_params[<span class="dv">2</span>], beta_params[<span class="dv">2</span>])</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    prob_optimal <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(theta_samples <span class="sc">==</span> <span class="fu">apply</span>(theta_samples, <span class="dv">1</span>, max))</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Allocate traffic</span></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (v <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>visits_per_day) {</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>      theta_sampled <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">2</span>, alpha_params, beta_params)</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="fu">which.max</span>(theta_sampled)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>      y <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, theta_true[a])</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>      alpha_params[a] <span class="ot">&lt;-</span> alpha_params[a] <span class="sc">+</span> y</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>      beta_params[a] <span class="ot">&lt;-</span> beta_params[a] <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> y)</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>      total_conversions <span class="ot">&lt;-</span> total_conversions <span class="sc">+</span> y</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check stopping</span></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">max</span>(prob_optimal) <span class="sc">&gt;</span> <span class="fl">0.95</span>) {</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>      winner <span class="ot">&lt;-</span> <span class="fu">which.max</span>(prob_optimal)</span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">day =</span> day, <span class="at">conversions =</span> total_conversions, </span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>                  <span class="at">winner =</span> winner, <span class="at">correct =</span> (winner <span class="sc">==</span> <span class="dv">2</span>)))</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>  winner <span class="ot">&lt;-</span> <span class="fu">which.max</span>(prob_optimal)</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">day =</span> max_days, <span class="at">conversions =</span> total_conversions, </span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a>              <span class="at">winner =</span> winner, <span class="at">correct =</span> (winner <span class="sc">==</span> <span class="dv">2</span>)))</span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Run simulations</span></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n_simulations, <span class="cf">function</span>(i) <span class="fu">run_bandit_trial</span>(theta_true, visits_per_day))</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract results</span></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a>bandit_days <span class="ot">&lt;-</span> <span class="fu">sapply</span>(results, <span class="cf">function</span>(x) x<span class="sc">$</span>day)</span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a>bandit_conversions <span class="ot">&lt;-</span> <span class="fu">sapply</span>(results, <span class="cf">function</span>(x) x<span class="sc">$</span>conversions)</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a>correct_arm <span class="ot">&lt;-</span> <span class="fu">sapply</span>(results, <span class="cf">function</span>(x) x<span class="sc">$</span>correct)</span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Classical experiment: fixed allocation for 223 days</span></span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected conversions per day: 50 * 0.04 + 50 * 0.05 = 4.5</span></span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>classical_conversions_per_day <span class="ot">&lt;-</span> visits_per_day <span class="sc">*</span> <span class="fu">mean</span>(theta_true)</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>classical_total_conversions <span class="ot">&lt;-</span> classical_days <span class="sc">*</span> classical_conversions_per_day</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a><span class="co"># For bandit, compute expected conversions if we ran to classical_days</span></span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a><span class="co"># with optimal allocation after stopping</span></span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>expected_bandit_conversions <span class="ot">&lt;-</span> bandit_conversions <span class="sc">+</span> </span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>  (classical_days <span class="sc">-</span> bandit_days) <span class="sc">*</span> visits_per_day <span class="sc">*</span> <span class="fu">max</span>(theta_true)</span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Days saved and conversions saved</span></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a>days_saved <span class="ot">&lt;-</span> classical_days <span class="sc">-</span> bandit_days</span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a>conversions_saved <span class="ot">&lt;-</span> expected_bandit_conversions <span class="sc">-</span> classical_total_conversions</span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histograms</span></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(days_saved, <span class="at">breaks =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="st">"lightgreen"</span>, <span class="at">border =</span> <span class="st">"white"</span>,</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Days of Testing Saved"</span>, <span class="at">xlab =</span> <span class="st">"Number of Days"</span>,</span>
<span id="cb54-74"><a href="#cb54-74" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Frequency"</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">50</span>, <span class="dv">250</span>))</span>
<span id="cb54-75"><a href="#cb54-75" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">mean</span>(days_saved), <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb54-76"><a href="#cb54-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-77"><a href="#cb54-77" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(conversions_saved, <span class="at">breaks =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="st">"lightgreen"</span>, <span class="at">border =</span> <span class="st">"white"</span>,</span>
<span id="cb54-78"><a href="#cb54-78" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Conversions Saved"</span>, <span class="at">xlab =</span> <span class="st">"Number of Conversions"</span>,</span>
<span id="cb54-79"><a href="#cb54-79" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Frequency"</span>)</span>
<span id="cb54-80"><a href="#cb54-80" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">mean</span>(conversions_saved), <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="06-hyp_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="864"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary statistics</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Simulation results (n ="</span>, n_simulations, <span class="st">"):</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Simulation results (n = 500 ):</code></pre>
</div>
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"  Correct arm identified:"</span>, <span class="fu">sum</span>(correct_arm), <span class="st">"/"</span>, n_simulations, </span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"("</span>, <span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span><span class="fu">mean</span>(correct_arm), <span class="dv">1</span>), <span class="st">"%)</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Correct arm identified: 469 / 500 ( 94 %)</code></pre>
</div>
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"  Average days to finish:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(bandit_days), <span class="dv">1</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Average days to finish: 47 </code></pre>
</div>
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"  Average days saved:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(days_saved), <span class="dv">1</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Average days saved: 176 </code></pre>
</div>
<details class="code-fold">
<summary>500 Simulation Runs</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"  Average conversions saved:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(conversions_saved), <span class="dv">1</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  Average conversions saved: 99 </code></pre>
</div>
</div>
<p>The histograms show the distribution of days saved (left) and conversions saved (right) compared to the classical 223-day experiment. On average, the bandit saves approximately 150-170 days of testing while achieving the same statistical validity (finding the correct arm about 96% of the time). The conversion savings come from two sources: ending the experiment earlier and allocating more traffic to the better-performing variation during the experiment.</p>
</div>
</section>
<section id="stopping-rules-value-remaining-and-regret" class="level4">
<h4 class="anchored" data-anchor-id="stopping-rules-value-remaining-and-regret">Stopping Rules, Value Remaining, and Regret</h4>
<p>Several questions arise in practice: when should we stop the experiment? How do we quantify the potential benefit of continuing? <span class="citation" data-cites="scott2015multiarmed">Steven L. Scott (<a href="references.html#ref-scott2015multiarmed" role="doc-biblioref">2015</a>)</span> describes two complementary stopping criteria. The first is the probability that each variation beats the original; if we are 95% confident that a variation is best, a winner can be declared. The second criterion is the <em>value remaining in the experiment</em>, which measures the expected improvement from switching away from the current champion. When there is at least a 95% probability that the value remaining is less than 1% of the champion’s conversion rate, the experiment can end.</p>
<p>Another perspective is <em>regret</em>, the cumulative cost of not always selecting the optimal arm. These concepts, along with detailed mathematical treatment and code examples for computing stopping criteria, are covered in <a href="09-rl.html" class="quarto-xref"><span>Chapter 9</span></a> (see the section on Multi-Armed Bandits, specifically “When to End Experiments”).</p>
<p>This framework also extends to clinical trials, where <span class="citation" data-cites="villar2015multi">Villar, Bowden, and Wason (<a href="references.html#ref-villar2015multi" role="doc-biblioref">2015</a>)</span> reviewed how response-adaptive randomization can reduce the number of patients receiving inferior treatments. For contextual bandits, design considerations, and extensions to reinforcement learning, see <a href="09-rl.html" class="quarto-xref"><span>Chapter 9</span></a>.</p>
</section>
</section>
<section id="practical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h3>
<p>Sequential analysis requires careful consideration of several practical issues. First, the definition of <em>sufficient evidence</em> must be specified in advance, whether through posterior probability thresholds, Bayes factor bounds, or expected utility calculations. Second, the frequency of interim analyses affects operational aspects of trials, including regulatory interactions and data monitoring committee responsibilities. Third, the potential for early stopping must be balanced against the need for long-term safety data and secondary endpoint analyses.</p>
<p>The choice between Bayesian and frequentist sequential methods often depends on regulatory requirements and institutional preferences. However, the coherence of Bayesian inference under optional stopping, the natural incorporation of prior information, and the interpretability of posterior probabilities make Bayesian sequential analysis an increasingly attractive option, particularly in challenging settings like rare disease trials where traditional approaches are impractical.</p>
</section>
</section>
<section id="examples-and-paradoxes" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="examples-and-paradoxes"><span class="header-section-number">6.6</span> Examples and Paradoxes</h2>
<p>This section provides a number of paradoxes arising when using different hypothesis testing procedures. The common strands of the examples will be discussed at the end of the section.</p>
<div id="exm-nptest" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.13 (Neyman-Pearson tests)</strong></span> Consider testing <span class="math inline">\(H_{0}:\mu=\mu_{0}\)</span> versus <span class="math inline">\(H_{1}:\mu=\mu_{1}\)</span>, <span class="math inline">\(y_{t}\sim\mathcal{N}\left( \mu,\sigma^{2}\right)\)</span> and <span class="math inline">\(\mu_{1}&gt;\mu_{0}\)</span>. For this simple test, the likelihood ratio is given by <span class="math display">\[
\mathcal{LR}_{0,1}=\frac{\exp\left(  -\frac{1}{2\sigma^{2}}%
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
\left(  y_{t}-\mu_{0}\right)  ^{2}\right)  }{\exp\left(  -\frac{1}{2\sigma
^{2}}%
%TCIMACRO{\tsum \nolimits_{t=1}^{T}}%
%BeginExpansion
{\textstyle\sum\nolimits_{t=1}^{T}}
%EndExpansion
\left(  y_{t}-\mu_{1}\right)  ^{2}\right)  }=\exp\left(  -\frac{T}{\sigma^{2}%
}\left(  \mu_{1}-\mu_{0}\right)  \left(  \overline{y}-\frac{1}{2}\left( \mu_{0}+\mu_{1}\right)  \right)  \right)  \text{.}%
\]</span> Since <span class="math inline">\(\mathrm{BF}_{0,1}=\mathcal{LR}_{0,1}\)</span>, assuming equal prior probabilities and symmetric losses, the Bayesian accepts <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(\mathrm{BF}_{0,1}&gt;1\)</span>. Thus, the Bayes procedure rejects <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(\overline{y}&gt;\frac{1}{2}\left( \mu_{0}+\mu_{1}\right)\)</span> for any <span class="math inline">\(T\)</span> and <span class="math inline">\(\sigma^{2}\)</span>, with <span class="math inline">\(\mu_{0}\)</span>,<span class="math inline">\(\mu_{1}\)</span>, <span class="math inline">\(T,\)</span>and <span class="math inline">\(\sigma^{2}\)</span> determining the strength of the rejection. If <span class="math inline">\(\mathrm{BF}_{0,1}=1\)</span>, there is equal evidence for the two hypotheses.</p>
<p>The NP procedure proceeds by first setting <span class="math inline">\(\alpha=0.05,\)</span> and rejects when <span class="math inline">\(\mathcal{LR}_{0,1}\)</span> is large. This is equivalent to rejecting when <span class="math inline">\(\overline{y}\)</span> is large, generating an `optimal’ rejection region of the form <span class="math inline">\(\overline{y}&gt;c\)</span>. The cutoff value <span class="math inline">\(c\)</span> is calibrated via the size of the test, <span class="math display">\[
P \left[  reject\text{ }H_{0} \mid H_{0}\right]
=P \left[  \overline{y}&gt;c \mid \mu_{0}\right]  =P \left[
\frac{\left(  \overline{y}-\mu_{0}\right)  }{\sigma/\sqrt{T}}&gt;\frac{\left( c-\mu_{0}\right)  }{\sigma/\sqrt{T}} \mid H_{0}\right] .
\]</span> The size equals <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(\sqrt{T}\left( c-\mu_{0}\right) /\sigma =z_{\alpha}\)</span>. Thus, the NP test rejects if then if <span class="math inline">\(\overline{y}&gt;\mu _{0}+\sigma z_{\alpha}/\sqrt{T}\)</span>. Notice that the test rejects regardless of the value of <span class="math inline">\(\mu_{1}\)</span>, which is rather odd, since <span class="math inline">\(\mu_{1}\)</span> does not enter into the size of the test only the power. The probability of a type II error is <span class="math display">\[
\beta=P \left[  \text{accept }H_{0} \mid H_{1}\right]
=P \left[  \overline{y}\leq\mu_{0}+\frac{\sigma}{\sqrt{T}}z_{\alpha
} \mid H_{1}\right]  =\int_{-\infty}^{\mu_{0}+\frac{\sigma}{\sqrt{T}%
}z_{\alpha}}p\left(  \overline{y} \mid \mu_{1}\right)  d\overline{y}\text{,}%
\]</span> where <span class="math inline">\(p\left( \overline{y} \mid \mu_{1}\right) \sim\mathcal{N}\left( \mu _{1},\sigma^{2}/T\right)\)</span>.</p>
<p>These tests can generate strikingly different conclusions. Consider a test of <span class="math inline">\(H_{0}:\mu=0\)</span> versus <span class="math inline">\(H_{1}:\mu=5\)</span>, based on <span class="math inline">\(T=100\)</span> observations drawn from <span class="math inline">\(y_{t}\sim\mathcal{N}\left( \mu,10^{2}\right)\)</span> with <span class="math inline">\(\overline{y}=2\)</span>. For NP, since <span class="math inline">\(\sigma/\sqrt{T}=1\)</span>, <span class="math inline">\(\overline{y}\)</span> is two standard errors away from <span class="math inline">\(0\)</span>, thus <span class="math inline">\(H_{0}\)</span> is rejected at the 5% level (the same conclusion holds for <span class="math inline">\(p-\)</span>values). Since <span class="math inline">\(p(\overline {y}=2 \mid H_{0})=0.054\)</span> and <span class="math inline">\(p(\overline{y}=2 \mid H_{1})=0.0044\)</span>, the Bayes factor is <span class="math inline">\(\mathrm{BF}_{0,1}=12.18\)</span> and <span class="math inline">\(P \left( H_{0} \mid y\right) =92.41\%\)</span>. Thus, the Bayesian is quite sure the null is true, while Neyman-Pearson reject the null.</p>
<p>The paradox can be seen in two different ways. First, although <span class="math inline">\(\overline{y}\)</span> is actually closer to <span class="math inline">\(\mu_{0}\)</span> than <span class="math inline">\(\mu_{1}\)</span>, the NP test rejects <span class="math inline">\(H_{0}\)</span>. This is counterintuitive and makes little sense. The problem is one of calibration. The classical approach develops a test such that 5% of the time, a correct null would be rejected. The power of the test is easy to compute and implies that <span class="math inline">\(\beta=0.0012\)</span>. Thus, this testing procedure will virtually never accept the null if the alternative is correct. For Bayesian procedure, assuming the prior odds is <span class="math inline">\(1\)</span> and <span class="math inline">\(L_{0}=L_{1}\)</span>, then <span class="math inline">\(\alpha=\beta=0.0062\)</span>. Notice that the overall probability of making an error is 1.24% in the Bayesian procedure compared to 5.12% in the classical procedure. It should seem clear that the Bayesian approach is more reasonably, absent a specific motivation for inflating <span class="math inline">\(\alpha\)</span>. Second, suppose the null and alternative were reversed, testing <span class="math inline">\(H_{0}:\mu=\mu_{1}\)</span> versus <span class="math inline">\(H_{1}:\mu=\mu_{0}\)</span> In the previous example, the Bayes approach gives the same answer, while NP once again rejects the null hypothesis! Again, this result is counterintuitive and nonsensical, but is common when arbitrarily fixing <span class="math inline">\(\alpha\)</span>, which essentially hardwires the test to over-reject the null.</p>
</div>
<div id="exm-lindleyparadox" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.14 (Lindley’s paradox)</strong></span> Consider the case of testing whether or not a coin is fair, based on observed coin flips, <span class="math display">\[
H_{0}:\theta=\frac{1}{2}\text{ versus }H_{1}:\theta
\neq\frac{1}{2}\text{,}%
\]</span> based on <span class="math inline">\(T\)</span> observations from <span class="math inline">\(y_{t}\sim Ber\left( \theta\right)\)</span>. As an example, <a href="#tbl-lindley" class="quarto-xref">Table&nbsp;<span>6.2</span></a> provides 4 datasets of differing lengths. Prior to considering the formal hypothesis tests, form your own opinion on the strength of evidence regarding the hypothesis in each data set. It is common for individuals, when confronted with this data to conclude that the fourth sample provides the strongest of evidence for the null and the first sample the weakest.</p>
<div id="tbl-lindley" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lindley-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.2: Lindley’s paradox
</figcaption>
<div aria-describedby="tbl-lindley-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>#1</th>
<th>#2</th>
<th>#3</th>
<th>#4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td># Flips</td>
<td>50</td>
<td>100</td>
<td>400</td>
<td>10,000</td>
</tr>
<tr class="even">
<td># Heads</td>
<td>32</td>
<td>60</td>
<td>220</td>
<td>5098</td>
</tr>
<tr class="odd">
<td>Percentage of heads</td>
<td>64</td>
<td>60</td>
<td>55</td>
<td>50.98</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Fisher’s solution to the problem posits an unbiased estimator, the sample mean, and computes the <span class="math inline">\(t-\)</span>statistic, which is calculated under <span class="math inline">\(H_{0}\)</span>: <span class="math display">\[
t\left(  y\right)  =\frac{\overline{y}-E\left[  \overline{y} \mid \theta
_{0}\right]  }{se\left(  \overline{y}\right)  }=\sqrt{T}\left(  2\widehat
{\theta}-1\right)  \text{,}%
\]</span> where <span class="math inline">\(se\left(\overline{y}\right)\)</span> is the standard error of <span class="math inline">\(\overline{y}\)</span>. The Bayesian solution requires marginal likelihood under the null and alternative, which are <span id="eq-Bernoulli_marginal"><span class="math display">\[
p\left(  y \mid \theta_{0}=1/2\right)  =\prod_{t=1}^{T}p\left(  y_{t} \mid \theta
_{0}\right)  =\left(  \frac{1}{2}\right)  ^{\sum_{t=1}^{T}y_{t}}\left( \frac{1}{2}\right)  ^{T-\sum_{t=1}^{T}y_{t}}=\left(  \frac{1}{2}\right)  ^{T},
\tag{6.1}\]</span></span> and, from <a href="#eq-Bernoulli_marginal" class="quarto-xref">Equation&nbsp;<span>6.1</span></a>, <span class="math inline">\(p\left( y \mid H_{1}\right) =B\left( \alpha_{T},\beta_{T}\right) /B\left(\alpha,\beta\right)\)</span> assuming a beta prior distribution.</p>
<p>To compare the results, note first that in the datasets given above, <span class="math inline">\(\widehat{\theta}\)</span> and <span class="math inline">\(T\)</span> generate <span class="math inline">\(t_{\alpha}=1.96\)</span> in each case. Thus, for a significance level of <span class="math inline">\(\alpha=5\%\)</span>, the null is rejected for each sample size. Assuming a flat prior distribution, the Bayes factors are <span class="math display">\[
\mathrm{BF}_{0,1}=\left\{
\begin{array}
[c]{l}%
0.8178\text{ for }N=50\text{ }\\
1.0952\text{ for }N=100\\
2.1673\text{ for }N=400\\
11.689\text{ for }N=10000
\end{array}
\right.  ,
\]</span> showing increasingly strong evidence in favor of <span class="math inline">\(H_{0}\)</span>. Assuming equal prior weight for the hypotheses, the posterior probabilities are 0.45, 0.523, 0.684, and 0.921, respectively. For the smallest samples, the Bayes factor implies roughly equal odds of the null and alternative. As the sample size increase, the weight of evidence favors the null, with a 92% probability for <span class="math inline">\(N=10K\)</span>.</p>
<p>Next, consider testing <span class="math inline">\(H_{0}:\theta_{0}=0\)</span> vs.&nbsp;<span class="math inline">\(H_{1}:\theta_{0}\neq0,\)</span> based on <span class="math inline">\(T\)</span> observations from <span class="math inline">\(y_{t}\sim \mathcal{N}\left( \theta_{0},\sigma^{2}\right)\)</span>, where <span class="math inline">\(\sigma^{2}\)</span> is known. This is the formal example used by Lindley to generate his paradox. Using <span class="math inline">\(p-\)</span>values, the hypothesis is rejected if the <span class="math inline">\(t-\)</span>statistic is greater than <span class="math inline">\(t_{\alpha}\)</span>. To generate the paradox, consider datasets that are exactly <span class="math inline">\(t_{\alpha}\)</span> standard errors away from <span class="math inline">\(\overline{y}\)</span>, that is, <span class="math inline">\(\overline {y}^{\ast}=\theta_{0}+\sigma t_{\alpha}/\sqrt{n}\)</span>, and a uniform prior over the interval <span class="math inline">\(\left( \theta_{0}-I/2,\theta_{0}+I/2\right)\)</span>. If <span class="math inline">\(p_{0}\)</span> is the probability of the null, then, <span class="math display">\[\begin{align*}
P \left(  \theta=\theta_{0} \mid \overline{y}^{\ast}\right)   &amp;
=\frac{\exp\left(  -\frac{1}{2}\frac{T\left(  \overline{y}^{\ast}-\theta
_{0}\right)  ^{2}}{\sigma^{2}}\right)  p_{0}}{\exp\left(  -\frac{1}{2}%
\frac{T\left(  \overline{y}^{\ast}-\theta_{0}\right)  ^{2}}{\sigma^{2}%
}\right)  p_{0}+\left(  1-p_{0}\right)  \int_{\theta_{0}-I/2}^{\theta_{0}%
+I/2}\exp\left(  -\frac{1}{2}\frac{T\left(  \overline{y}^{\ast}-\theta\right)
^{2}}{\sigma^{2}}\right)  I^{-1}d\theta}\\
&amp;  =\frac{\exp\left(  -\frac{1}{2}t_{\alpha}^{2}\right)  p_{0}}{\exp\left( -\frac{1}{2}t_{\alpha}^{2}\right)  p_{0}+\frac{\left(  1-p_{0}\right)  }%
{I}\int_{\theta_{0}-I/2}^{\theta_{0}+I/2}\exp\left(  -\frac{1}{2}\left( \frac{\left(  \overline{y}^{\ast}-\theta\right)  }{\sigma/\sqrt{T}}\right)^2
\right)  d\theta}\\
&amp;  \geq\frac{\exp\left(  -\frac{1}{2}t_{\alpha}^{2}\right)  p_{0}}{\exp\left( -\frac{1}{2}t_{\alpha}^{2}\right)  p_{0}+\frac{\left(  1-p_{0}\right)  }%
{I}\sqrt{2\pi\sigma^{2}/T}}\rightarrow1\text{ as }T\rightarrow\infty\text{.}%
\end{align*}\]</span> In large samples, the posterior probability of the null approaches 1, whereas Fisher always reject the null. It is important to note that this holds for any <span class="math inline">\(t_{\alpha}\)</span>, thus even if the test were performed at the 1% level or lower, the posterior probability would eventually reject the null.</p>
</div>
</section>
<section id="prior-sensitivity" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="prior-sensitivity"><span class="header-section-number">6.7</span> Prior Sensitivity</h2>
<p>The paradoxes discussed above, particularly Lindley’s Paradox, often invite criticism regarding the choice of the prior distribution. A common frequentist counter-argument is that the Bayesian result is driven by a specific, perhaps “biased”, prior. How do we know that the chosen prior is not artificially inflating the probability of the null? One elegant way to address this is to search over classes of priors to find the one that <em>minimizes</em> the evidence for the null hypothesis, effectively constructing a “worst-case” Bayesian scenario.</p>
<p>To see this, consider the case of testing <span class="math inline">\(H_{0}:\mu_{0}=0\)</span> vs.&nbsp;<span class="math inline">\(H_{1}:\mu_{0}\neq0\)</span> with observations drawn from <span class="math inline">\(y_{t} \sim\mathcal{N}\left( \theta_{0},\sigma^{2}\right)\)</span>, with <span class="math inline">\(\sigma\)</span> known. With equal prior null and alternative probability, the probability of the null is <span class="math inline">\(p\left( H_{0} \mid y\right) =\left( 1+\left( \mathrm{BF}_{0,1}\right) ^{-1}\right) ^{-1}\)</span>. Under the null, <span class="math display">\[
p\left(  y \mid H_{0}\right)  =\left(  \frac{1}{2\pi\sigma^{2}}\right)
^{\frac{T}{2}}\exp\left(  -\frac{1}{2}\left(  \frac{\left(  \overline
{y}-\theta_{0}\right)  }{\sigma/\sqrt{T}}\right)  ^{2}\right)  \text{.}%
\]</span> The criticism applies to the priors under the alternative. To analyze the sensitivity, consider four classes of priors under the alternative: (a) the class of normal priors, <span class="math inline">\(p\left( \theta \mid H_{1}\right) \sim\mathcal{N}\left( a,A\right)\)</span>; (b) the class of all symmetric unimodal prior distributions; (c) the class of all symmetric prior distributions; and (d) the class of all proper prior distributions. These classes provide varying degrees of prior information, allowing a thorough examination of the strength of evidence.</p>
<p>In the first case, consider the standard conjugate prior distribution, <span class="math inline">\(p\left( \mu \mid H_{1}\right) \sim\mathcal{N}\left( \mu_{0},A\right)\)</span>. Under the alternative, <span class="math display">\[\begin{align*}
p\left(  y \mid H_{1}\right)   &amp;  =\int p\left(  y \mid \mu,H_{1}\right)  p\left(  \mu \mid H_{1}\right)  d\mu\\
&amp;  =\int p\left(  \overline{y} \mid \mu,H_{1}\right)  p\left( \mu \mid H_{1}\right)  d\mu\text{,}%
\end{align*}\]</span> using the fact that <span class="math inline">\(\overline{y}\)</span> is a sufficient statistic. Noting that <span class="math inline">\(p\left( \overline{y} \mid \mu,H_{1}\right) \sim N\left( \mu ,\sigma^{2}/T\right)\)</span> and <span class="math inline">\(p\left( \mu \mid H_{1}\right) \sim N\left( \mu_{0},A\right)\)</span>, we can use the “substitute” instead of integrate trick to assert that <span class="math display">\[
\overline{y}=\mu_{0}+\sqrt{A}\eta+\sqrt{\sigma^{2}/T}\varepsilon\text{,}%
\]</span> where <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\varepsilon\)</span> are standard normal. Then, <span class="math inline">\(p\left( \overline{y} \mid H_{1}\right) \sim\mathcal{N}\left( \mu_{0},A+\sigma^{2}/T\right)\)</span>. Thus, <span class="math display">\[
\mathrm{BF}_{0,1}=\frac{p\left(  y \mid H_{0}\right)  }{p\left( y \mid H_{1}\right)  }=\frac{p\left(  \overline{y} \mid H_{0}\right)
}{p\left(  \overline{y} \mid H_{1}\right)  }=\frac{\left(  \sigma^{2}/T\right)
^{-\frac{1}{2}}}{\left(  \sigma^{2}/T+A\right)  ^{-\frac{1}{2}}}\frac
{\exp\left(  -\frac{1}{2}t^{2}\right)  }{\exp\left(  -\frac{1}{2}\frac
{z^{2}\sigma^{2}/T}{A+\sigma^{2}/T}\right)  }\text{.} \label{BF_normal}%
\]</span> To operationalize the test, <span class="math inline">\(A\)</span> must be selected. <span class="math inline">\(A\)</span> is chosen to minimizing the posterior probabilities of the null, with <span class="math inline">\(P_{norm}\left( H_{0} \mid y\right)\)</span> being the resulting lower bound on the posterior probability of the null. For <span class="math inline">\(z\geq1\)</span>, the lower bound on the posterior probability of the null is <span class="math display">\[
P_{norm}\left(  H_{0} \mid y\right)  =\left[
1+\sqrt{e}\exp\left(  -.5t^{2}\right)  \right]  ^{-1},
\]</span> which is derived in a reference cited in the notes. This choice provides a maximal bias of the Bayesian approach toward rejecting the null. It is important to note that this is not a reasonable prior, as it was intentionally constructed to bias the null toward rejection.</p>
<p>For the class of all proper prior distributions, it is also easy to derive the bound. From equation above, minimizing the posterior probability is equivalent to minimizing the Bayes factor, <span class="math display">\[
\mathrm{BF}_{0,1}=\frac{p\left(  y \mid H_{0}\right)  }{p\left( y \mid H_{1}\right)  }\text{.}%
\]</span> Since <span class="math display">\[
p\left(  y \mid H_{1}\right)  =\int p\left(  y \mid \theta,H_{1}\right)  p\left(  \theta \mid H_{1}\right)  d\theta\leq p\left( y \mid \widehat{\theta}_{MLE},H_{1}\right)  \text{,}%
\]</span> where <span class="math inline">\(\widehat{\theta}_{MLE}=\arg\underset{\theta\neq0}{\max}p\left( y \mid \theta\right)\)</span>. The maximum likelihood estimator, maximizes the probability of the alternative, and provides a lower bound on the Bayes factor, <span class="math display">\[
\underline{\mathrm{BF}}_{0,1}=\frac{p\left(  y \mid H_{0}\right)
}{\underset{\theta\neq0}{\sup}p\left(  y \mid \theta\right)  }\text{.}%
\]</span> In this case, the bound is particularly easy to calculate and is given by <span class="math display">\[
P_{all}\left(  H_{0} \mid y\right)  =\left( 1+\exp\left(  -\frac{t^{2}}{2}\right)  \right)  ^{-1}\text{.}%
\]</span> A reference cited in the notes provides the bounds for the second and third cases, generating <span class="math inline">\(P_{s,u}\left( H_{0} \mid y\right)\)</span> and <span class="math inline">\(P_{s}\left( H_{0} \mid y\right)\)</span>, respectively. All of the bounds only depend on the <span class="math inline">\(t-\)</span>statistic and constants.</p>
<p><a href="#tbl-berger" class="quarto-xref">Table&nbsp;<span>6.3</span></a> reports the <span class="math inline">\(t-\)</span>statistics and associated <span class="math inline">\(p-\)</span>values, with the remaining columns provide the posterior probability bounds. For the normal prior and choosing the prior parameter <span class="math inline">\(A\)</span> to minimize the probability of the null, the posterior probability of the null is much larger than the <span class="math inline">\(p-\)</span>value, in every case. For the standard case of a <span class="math inline">\(t-\)</span>statistic of 1.96, <span class="math inline">\(P\left( H_{0} \mid y\right)\)</span> is more than six times greater than the <span class="math inline">\(p-\)</span>value. For <span class="math inline">\(t=2.576\)</span>, <span class="math inline">\(P\left( H_{0} \mid y\right)\)</span> is almost 13 times greater than the <span class="math inline">\(p-\)</span>value. These probabilities fall slightly for more general priors. For example, for the class of all priors, a t-statistic of 1.96/2.576 generates a lower bound for the posterior probability of 0.128/0.035<span class="math inline">\(,\)</span> more than 2/3 times the <span class="math inline">\(p-\)</span>value.</p>
<div id="tbl-berger" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-berger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.3: Comparison of strength of evidence against the point null hypothesis. The numbers are reproduced from Berger (1986).
</figcaption>
<div aria-describedby="tbl-berger-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(t\)</span>-stat</th>
<th style="text-align: center;"><span class="math inline">\(p\)</span>-value</th>
<th style="text-align: center;"><span class="math inline">\(P_{norm}\left(H_{0} \mid y\right)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(P_{s,u}\left( H_{0} \mid y\right)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(P_{s}\left(H_{0} \mid y\right)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(P_{all}\left(H_{0} \mid y\right)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1.645</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.205</td>
</tr>
<tr class="even">
<td style="text-align: center;">1.960</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.128</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.576</td>
<td style="text-align: center;">0.010</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.068</td>
<td style="text-align: center;">0.035</td>
</tr>
<tr class="even">
<td style="text-align: center;">3.291</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;">0.0235</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.0088</td>
<td style="text-align: center;">0.0044</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="the-difference-between-p-values-and-bayesian-evidence" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="the-difference-between-p-values-and-bayesian-evidence"><span class="header-section-number">6.8</span> The difference between p-values and Bayesian evidence</h2>
<p>Suppose that you routinely reject two-sided hypotheses at a fixed level of significance, <span class="math inline">\(\alpha = 0.05\)</span>. Furthermore, suppose that half the experiments under the null are actually true: <span class="math inline">\(p(H_0) = p(H_1) = \frac{1}{2}\)</span>.</p>
<p>The observed p-value is not a probability in any real sense. The observed t-value is a realization of a statistic that happens to be <span class="math inline">\(N(0,1)\)</span> under the null hypothesis. Suppose that we observe <span class="math inline">\(t = 1.96\)</span>.</p>
<p>Then the <em>maximal evidence</em> against the null hypothesis, which corresponds to <span class="math inline">\(t = 0\)</span>, will be achieved by evaluating the likelihood ratio at the observed t-ratio. We get <span class="math display">\[
\frac{p(y \mid H_0)}{p(y \mid H_1)} \geq \frac{p(y \mid \theta = \theta_0)}{p(y \mid \theta = \hat{\theta})}
\]</span></p>
<p>Technically, <span class="math inline">\(p(y \mid H_1) = \int p(y \mid \theta) p(\theta \mid H_1) d\theta \leq p(y \mid \hat{\theta})\)</span>.</p>
<p>For testing, this gives: <span class="math display">\[
\frac{p(y \mid H_0)}{p(y \mid H_1)} \geq \frac{\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} \cdot 1.96^2}}{\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} \cdot 0^2}} = 0.146
\]</span></p>
<p>In terms of probabilities, with <span class="math inline">\(p(H_0) = p(H_1)\)</span>, we have: <span class="math display">\[
p(H_0 \mid y) = \frac{1}{1 + \frac{p(y \mid H_1)}{p(y \mid H_0)} \frac{p(H_1)}{p(H_0)}} \geq 0.128
\]</span></p>
<p>Hence, there’s still a 12.8% chance that the null is true! That’s very different from the p-value of 5%.</p>
<p>Moreover, among experiments with p-values of 0.05, at least 28.8% will actually turn out to be true nulls <span class="citation" data-cites="sellke2001calibration">(<a href="references.html#ref-sellke2001calibration" role="doc-biblioref">Sellke, Bayarri, and Berger 2001</a>)</span>! Put another way, the probability of rejecting a true null <em>conditional</em> on the observed <span class="math inline">\(p = 0.05\)</span> is at least 30%. You are throwing away good null hypotheses and claiming you have found effects!</p>
</section>
<section id="jeffreys-decision-rule" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="jeffreys-decision-rule"><span class="header-section-number">6.9</span> Jeffreys’ Decision Rule</h2>
<p><span class="citation" data-cites="jeffreys1998theory">Jeffreys (<a href="references.html#ref-jeffreys1998theory" role="doc-biblioref">1998</a>)</span> provided a famous rule for hypothesis testing. Consider testing <span class="math inline">\(H_0: \beta = 0\)</span> versus <span class="math inline">\(H_1: \beta \neq 0\)</span> with a t-statistic <span class="math inline">\(\frac{\hat{\beta}}{s_{\hat{\beta}}}\)</span>. Jeffreys proposed the rule: <span class="math display">\[
\frac{\hat{\beta}}{s_{\hat{\beta}}} &gt; \log\left(\frac{2n}{\pi}\right)
\]</span></p>
<p>This result can be understood through the <strong>Dickey-Savage density ratio</strong>, which states that for a sharp null hypothesis <span class="math inline">\(H_0: \beta=0\)</span> nested within an alternative <span class="math inline">\(H_1\)</span>, the Bayes Factor is the ratio of the posterior density to the prior density evaluated at the null value: <span class="math display">\[
BF_{01} = \frac{p(\beta = 0 \mid y, H_1)}{p(\beta = 0 \mid H_1)}.
\]</span> Assuming a Cauchy(<span class="math inline">\(0, \sigma\)</span>) prior for <span class="math inline">\(\beta\)</span> under <span class="math inline">\(H_1\)</span>, the prior density at the null is <span class="math inline">\(p(0 \mid H_1) = 1/(\pi\sigma)\)</span>. For the posterior, we can approximate it using a normal distribution centered at the MLE <span class="math inline">\(\hat{\beta}\)</span> with standard error <span class="math inline">\(s_{\hat{\beta}}\)</span>. Evaluating this approximate posterior at 0 gives: <span class="math display">\[
p(0 \mid y, H_1) \approx \frac{1}{\sqrt{2\pi}s_{\hat{\beta}}} \exp\left(-\frac{1}{2}\left(\frac{0 - \hat{\beta}}{s_{\hat{\beta}}}\right)^2\right).
\]</span> Substituting these into the ratio, the condition for evidence neutrality (<span class="math inline">\(BF=1\)</span>) becomes: <span class="math display">\[
\frac{\frac{1}{\sqrt{2\pi}s_{\hat{\beta}}} e^{-t^2/2}}{1/(\pi\sigma)} = 1 \implies \sqrt{\frac{n\pi}{2}} e^{-t^2/2} \approx 1,
\]</span> where we’ve used the approximation <span class="math inline">\(s_{\hat{\beta}} \approx \sigma/\sqrt{n}\)</span>. Jeffreys’ rule <span class="math inline">\(\hat{\beta}/s_{\hat{\beta}} &gt; \log(2n/\pi)\)</span> provides a heuristic approximation to this boundary.</p>
<p>These critical values differ substantially from the fixed frequentist thresholds:</p>
<table class="caption-top table">
<caption>Jeffreys’ decision rule critical values <span class="citation" data-cites="jeffreys1998theory">Jeffreys (<a href="references.html#ref-jeffreys1998theory" role="doc-biblioref">1998</a>)</span> (p.&nbsp;379)</caption>
<thead>
<tr class="header">
<th><span class="math inline">\(n\)</span></th>
<th><span class="math inline">\(\hat{\beta}/\sigma_{\hat{\beta}}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5</td>
<td>1.16</td>
</tr>
<tr class="even">
<td>10</td>
<td>1.85</td>
</tr>
<tr class="odd">
<td>100</td>
<td>4.15</td>
</tr>
<tr class="even">
<td>100,000</td>
<td>11.06</td>
</tr>
</tbody>
</table>
<p>For instance, when <span class="math inline">\(n = 10\)</span>, we have <span class="math inline">\(\hat{\beta}/\sigma_{\hat{\beta}} = \log(20/\pi) = 1.85\)</span>, and for <span class="math inline">\(n = 100\)</span>, we have <span class="math inline">\(\hat{\beta}/\sigma_{\hat{\beta}} = \log(200/\pi) = 4.15\)</span>.</p>
<p>Jeffreys then explains the consequences of this sample-size dependence: traditional fixed critical values like 1.96 do not properly account for the evidence provided by larger sample sizes.</p>
</section>
<section id="cromwells-rule" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="cromwells-rule"><span class="header-section-number">6.10</span> Cromwell’s Rule</h2>
<p>The discussion of hypothesis testing throughout this chapter reveals a fundamental tension between the desire for certainty and the reality of uncertainty in statistical inference. This tension is captured by <strong>Cromwell’s Rule</strong>, a principle that serves as a philosophical foundation for Bayesian hypothesis testing.</p>
<p>We can write Bayes rule for updating models as follows: <span class="math display">\[
p(M \mid D) = \frac{p(D \mid M)}{p(D)} p(M)
\]</span></p>
<p>Thus, if <span class="math inline">\(p(M) = 0\)</span>, then <span class="math inline">\(p(M \mid D) = 0\)</span> for all <span class="math inline">\(D\)</span>.</p>
<p>This mathematical result has profound implications: if you assign zero prior probability to a hypothesis, no amount of evidence can ever change your mind. The posterior probability remains zero regardless of how strongly the data might support that hypothesis.</p>
<p>This principle is named after Oliver Cromwell’s famous plea to the Church of Scotland in 1650:</p>
<blockquote class="blockquote">
<p><em>I beseech you, in the bowels of Christ, think it possible you may be mistaken</em></p>
</blockquote>
<p>Cromwell’s appeal for intellectual humility resonates deeply with the Bayesian approach to hypothesis testing. The rule suggests that we should never assign zero probability to hypotheses that could plausibly be true, as doing so makes us unable to learn from any amount of contradictory evidence.</p>
<p>The rule emphasizes the importance of careful prior specification. As we saw in the section on prior sensitivity, even when we try to bias our priors against the null hypothesis, the resulting posterior probabilities often remain substantially higher than corresponding p-values. Cromwell’s Rule reminds us that assigning zero probability to any reasonable hypothesis is not just mathematically problematic—it’s epistemologically unsound.</p>
<p>Cromwell’s Rule further aligns with the likelihood principle discussed earlier. Just as the likelihood principle states that all relevant experimental information is contained in the likelihood function, Cromwell’s Rule ensures that we remain open to learning from all possible evidence. By avoiding zero prior probabilities, we maintain the ability to update our beliefs based on observed data.</p>
<p>This principle serves as a philosophical foundation that unifies the various approaches to hypothesis testing discussed in this chapter, emphasizing the importance of intellectual humility and the willingness to learn from evidence in statistical inference.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-armitage1975sequential" class="csl-entry" role="listitem">
Armitage, Peter. 1975. <em>Sequential Medical Trials</em>. 2nd ed. Oxford: Blackwell Scientific Publications.
</div>
<div id="ref-benda2016sequential" class="csl-entry" role="listitem">
Benda, Norbert, Michael Branson, Willi Maurer, and Tim Friede. 2016. <span>“Sequential Designs with Small Samples: <span>Evaluation</span> and Recommendations for Normal Responses.”</span> <em>Statistics in Medicine</em> 35 (19): 3215–30.
</div>
<div id="ref-berry1985interim" class="csl-entry" role="listitem">
Berry, Donald A. 1985. <span>“Interim Analyses in Clinical Trials: <span>Classical</span> Vs. <span>Bayesian</span> Approaches.”</span> <em>Statistics in Medicine</em> 4 (4): 521–26.
</div>
<div id="ref-berry2006bayesian" class="csl-entry" role="listitem">
Berry, Scott M., Bradley P. Carlin, J. Jack Lee, and Peter Müller. 2010. <em>Bayesian Adaptive Methods for Clinical Trials</em>. Boca Raton: CRC Press.
</div>
<div id="ref-billingsley1995probability" class="csl-entry" role="listitem">
Billingsley, Patrick. 1995. <em>Probability and Measure</em>. 3rd ed. New York: John Wiley &amp; Sons.
</div>
<div id="ref-chen2022application" class="csl-entry" role="listitem">
Chen, Cong, Naitee Li, Shuai Yuan, Zoran Antonijevic, Wei Guo, et al. 2022. <span>“Application of <span>Bayesian</span> Methods to Accelerate Rare Disease Drug Development: <span>Scopes</span> and Hurdles.”</span> <em>Orphanet Journal of Rare Diseases</em> 17: 186.
</div>
<div id="ref-cover2006elements" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2006. <em>Elements of Information Theory</em>. John Wiley &amp; Sons.
</div>
<div id="ref-demets1994interim" class="csl-entry" role="listitem">
DeMets, David L., and K. K. Gordon Lan. 1994. <span>“Interim Analysis: <span>The</span> Alpha Spending Function Approach.”</span> <em>Statistics in Medicine</em> 13 (13-14): 1341–52.
</div>
<div id="ref-edwards1963bayesian" class="csl-entry" role="listitem">
Edwards, Ward, Harold Lindman, and Leonard J. Savage. 1963. <span>“Bayesian Statistical Inference for Psychological Research.”</span> <em>Psychological Review</em> 70 (3): 193–242.
</div>
<div id="ref-hilgers2008efficient" class="csl-entry" role="listitem">
Hilgers, Ralf-Dieter, Kit Roes, and Nigel Stallard. 2016. <span>“Efficient Ways Exist to Obtain the Optimal Sample Size in Clinical Trials in Rare Diseases.”</span> <em>Journal of Clinical Epidemiology</em> 80: 68–76.
</div>
<div id="ref-jeffreys1998theory" class="csl-entry" role="listitem">
Jeffreys, Harold. 1998. <em>Theory of <span>Probability</span></em>. Third Edition, Third Edition. Oxford <span>Classic Texts</span> in the <span>Physical Sciences</span>. Oxford, New York: Oxford University Press.
</div>
<div id="ref-novick1965bayesian" class="csl-entry" role="listitem">
Novick, Melvin R., and James E. Grizzle. 1965. <span>“A <span>Bayesian Approach</span> to the <span>Analysis</span> of <span>Data</span> from <span>Clinical Trials</span>.”</span> <em>Journal of the American Statistical Association</em> 60 (309): 81–96.
</div>
<div id="ref-scott2015multiarmed" class="csl-entry" role="listitem">
Scott, Steven L. 2015. <span>“Multi-Armed Bandit Experiments in the Online Service Economy.”</span> <em>Applied Stochastic Models in Business and Industry</em> 31 (1): 37–45.
</div>
<div id="ref-scott2013bandit" class="csl-entry" role="listitem">
Scott, Steven L. 2013. <span>“Multi-Armed Bandit Experiments.”</span>
</div>
<div id="ref-sellke2001calibration" class="csl-entry" role="listitem">
Sellke, Thomas, M. J Bayarri, and James O Berger. 2001. <span>“Calibration of <span><em><span class="math inline">\(\rho\)</span></em></span> <span>Values</span> for <span>Testing Precise Null Hypotheses</span>.”</span> <em>The American Statistician</em> 55 (1): 62–71.
</div>
<div id="ref-thompson1933likelihood" class="csl-entry" role="listitem">
Thompson, William R. 1933. <span>“On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.”</span> <em>Biometrika</em> 25 (3/4): 285–94.
</div>
<div id="ref-fda2019bayesian" class="csl-entry" role="listitem">
U.S. Food and Drug Administration. 2010. <span>“Guidance for the Use of <span>Bayesian</span> Statistics in Medical Device Clinical Trials.”</span>
</div>
<div id="ref-villar2015multi" class="csl-entry" role="listitem">
Villar, Sofía S., Jack Bowden, and James Wason. 2015. <span>“Multi-Armed Bandit Models for the Optimal Design of Clinical Trials: <span>Benefits</span> and Challenges.”</span> <em>Statistical Science</em> 30 (2): 199–215.
</div>
<div id="ref-wald1945sequential" class="csl-entry" role="listitem">
Wald, Abraham. 1945. <span>“Sequential Tests of Statistical Hypotheses.”</span> <em>The Annals of Mathematical Statistics</em> 16 (2): 117–86.
</div>
<div id="ref-wald1947sequential" class="csl-entry" role="listitem">
———. 1947. <em>Sequential Analysis</em>. New York: John Wiley &amp; Sons.
</div>
<div id="ref-wei2018small" class="csl-entry" role="listitem">
Wei, Bo, Thomas M. Braun, Roy N. Tamura, and Kelley Kidwell. 2018. <span>“A Small n Sequential Multiple Assignment Randomized Trial Design for Use in Rare Disease Research.”</span> <em>Statistics in Medicine</em> 37 (26): 3836–52.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-ab.html" class="pagination-link" aria-label="A/B Testing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-sp.html" class="pagination-link" aria-label="Stochastic Processes">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>