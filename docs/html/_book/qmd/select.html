<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayes, AI and Deep Learning - 16&nbsp; Model Selection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../qmd/tree.html" rel="next">
<link href="../qmd/rct.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/data.html">AI</a></li><li class="breadcrumb-item"><a href="../qmd/select.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Principles of Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching and Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RCT: Field vs Observational</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/select.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/dlopt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/arch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Image Processing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/data.html">AI</a></li><li class="breadcrumb-item"><a href="../qmd/select.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>A parametric model that we choose to fit to data is chosen from a family of functions. Then, we use optimization to find the best model from that family. To find the best model we either minimize empirical loss or maximize the likelihood. We also established, that when <span class="math inline">\(y \sim N(f(x\mid \beta),\sigma^2)\)</span> then meas squared error loss and negative log-likelihood are the same function. <span class="math display">\[
\E{y | x} = f(\beta^Tx)
\]</span></p>
<p>For a regression model, an empirical loss measures a distance between fitted values and measurements and the goal is to minimize it. A typical choice of loss function for regression is <span class="math display">\[
L (y,\hat y) =  \dfrac{1}{n}\sum_{i=1}^n |y_i -  f(\beta^Tx_i)|^p
\]</span> When <span class="math inline">\(p=1\)</span> we have MAE (mean absolute error), <span class="math inline">\(p=2\)</span> we have MSE (mean squared error).</p>
<p>Finding an appropriate family of functions is a major problem and is called <strong>model selection</strong> problem. For example, the choice of input variable to be included in the model is part of the model choice process. In practice we can find several models for the same data set that perform nearly identically. To summarize, the properties of a good model are</p>
<ul>
<li>Good model is not the one that fits data very well</li>
<li>By including enough parameters we can make fit as close as we need</li>
<li>Can have perfect fit when number of observations = number of parameters</li>
<li>The goal not only to get a good fit but also to reduce complexity</li>
<li>Model selection: do not include parameters we do not need</li>
<li>Usually select a model from a relevant class</li>
</ul>
<p>When we select a model for our analysis, we need to keep the following goals in mind - When we have many predictors (with many possible interactions), it can be difficult to find a good model. - Which input variables do we include? - Which interactions do we include? - Model selection tries to “simplify” this task.</p>
<p>The model selection task is sometimes one of the most consuming parts of the data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem as yet another optimization problem, with the goal to find best family of functions that describe the data. With a small number of predictors we can do brute force (check all possible models). For example, with <span class="math inline">\(p\)</span> predictors there are <span class="math inline">\(2^p\)</span> possible models with no interactions. Thus, the number of potential family functions is huge even for modest values of <span class="math inline">\(p\)</span>. One cannot consider all transformations and interactions.</p>
<section id="out-of-sample-performance" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="out-of-sample-performance"><span class="header-section-number">16.1</span> Out of Sample Performance</h2>
<p>Our goal is to build a model that predicts well for out-of-sample data, e.g.&nbsp;the data that was not used for training. Eventually, we are interested in using our models for prediction and thus, the out of sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when predictive model need to be chosen, as one of the winners of Netflix prize put it, “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it”. The out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical" in its nature. To uncover the pattern we start with a training dataset, denoted by <span class="math display">\[
D = (y_i,x_i)_{i=1}^n
\]</span> and to test the validity of our mode we use out-of-sample testing dataset <span class="math display">\[
D^* = (y_j^*, x_j^*)_{j=1}^m,
\]</span> where <span class="math inline">\(x_i\)</span> is a set of <span class="math inline">\(p\)</span> predictors ans <span class="math inline">\(y_i\)</span> is response variable.</p>
<p>A good predictor will “generalize” well and provide low MSE out-of-sample. These are a number of methods/objective functions that we will use to find, <span class="math inline">\(\hat f\)</span>. In a parameter-based style we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map <span class="math inline">\(f\)</span> that approximates the process that generated the data. For example data could be representing some physical observations and our goal is recover the “laws of nature" that led to those observations. One of the pitfalls is to find a map <span class="math inline">\(f\)</span> that does not generalize. Generalization means that our model actually did learn the”laws of nature" and not just identified patterns presented in training. The lack of generalization of the model is called over-fitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of <span class="math inline">\(n\)</span> points can be approximated by a polynomial of degree <span class="math inline">\(n\)</span>, e.g we can alway draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to the observations that were not in our training data. In other words, the empirical risk measure for <span class="math inline">\(D^*\)</span> is likely to be very high. Let us illustrate that in-sample fit can be deceiving.</p>
<div id="exm-hard" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.1 (Hard Function)</strong></span> Example: Say we want to approximate the following function <span class="math display">\[
f(x) = \dfrac{1}{1+25x^2}.
\]</span> This function is simply a ratio of two polynomial functions and we will try to build a liner model to reconstruct this function</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">25</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximate with polynomial of degree 1 and 2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span><span class="fu">poly</span>(x,<span class="dv">2</span>))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Approximate with polynomial of degree 20 and 5</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>m20 <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span><span class="fu">poly</span>(x,<span class="dv">20</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>m5 <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span><span class="fu">poly</span>(x,<span class="dv">5</span>))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">25</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">col=</span><span class="st">'black'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">predict</span>(m1,<span class="fu">list</span>(<span class="at">x=</span>x)),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">predict</span>(m2,<span class="fu">poly</span>(x,<span class="dv">2</span>)),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">predict</span>(m5,<span class="fu">poly</span>(x,<span class="dv">5</span>)),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">predict</span>(m20,<span class="fu">poly</span>(x,<span class="dv">20</span>)),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"f(x)"</span>,<span class="st">"m1"</span>,<span class="st">"m2"</span>,<span class="st">"m5"</span>,<span class="st">"m20"</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"black"</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>), <span class="at">lty=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>, <span class="at">bty=</span><span class="st">'n'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-rungekutta" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rungekutta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="select_files/figure-html/fig-rungekutta-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rungekutta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.1: Runge-Kutta function
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-rungekutta" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-rungekutta</span></a> shows the function itself (black line) on the interval <span class="math inline">\([-3,3]\)</span>. We used observations of <span class="math inline">\(x\)</span> from the interval <span class="math inline">\([-2,2]\)</span> to train the data (solid line) and from <span class="math inline">\([-3,-2) \cup (2,3]\)</span> (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model <span class="math inline">\(\hat y = \beta_0 + \beta_1 x\)</span> is not a good model. However, as we increas the degree of the polynomial to 20, the resulting model <span class="math inline">\(\hat y = \beta_0 + \beta_1x + \beta_2 x^2 +\ldots+\beta_{20}x^{20}\)</span> does fit the training data set quite well, but does very poor job on the test data set. Thus, while in-sample performance is good, the out-of sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice in-sample out-of-simple loss or classification rates provide us with a metric for providing horse race between different predictors. It is worth mentioning here there should be a penalty for overly complex rules which fits extremely well in sample but perform poorly on out-of-sample data. As Einstein famous said “model should be simple, but not simpler.”</p>
</div>
<p>To a Bayesian, the solution to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.</p>
<p>These results have major implications for empirical work and practical applications, as they provide a guide for forecasting.</p>
</section>
<section id="statistical-decisions-and-risk" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="statistical-decisions-and-risk"><span class="header-section-number">16.2</span> Statistical Decisions and Risk</h2>
<p>The statistical decision making problem can be posed as follows. A decision maker (you) has to chose from a set of decisions or acts. The consequences of these decisions depend on an unknown state of the world. Let <span class="math inline">\(d\in\mathcal{D}\)</span> denote the decision and <span class="math inline">\(\theta\in\Theta\)</span> the state of the world. As an example, think of <span class="math inline">\(\theta\)</span> as the unknown parameter and the decision as choosing a parameter estimation or hypothesis testing procedure. To provide information about the parameter, the decision maker obtains a sample <span class="math inline">\(y\in\mathcal{Y}\)</span> that is generated from the likelihood function <span class="math inline">\(p\left(y|\theta\right)\)</span>. The resulting decision depends on the observed data, is denoted as <span class="math inline">\(d\left(  y\right)\)</span>, and is commonly called the decision rule.</p>
<p>To make the decision, the decision maker uses a “loss” function as a quantitative metric to assesses the consequences or performance of different decisions. For each state of the world <span class="math inline">\(\theta\)</span>, and decision <span class="math inline">\(d\)</span>, <span class="math inline">\(L\left(  \theta,d\right)\)</span> quantifies the “loss” made by choosing <span class="math inline">\(d\)</span> when the state of the world is <span class="math inline">\(\theta.\)</span> Common loss functions include a quadratic loss, <span class="math inline">\(L(\theta,d)=(\theta-d)^{2},\)</span> an absolute loss, <span class="math inline">\(L(\theta,d)=|\theta-d|\)</span>, and a <span class="math inline">\(0-1\)</span> loss, <span class="math display">\[
L(\theta,d)=L_{0}1_{\left[  \theta\in\Theta_{0}\right]  }+L_{1}1_{\left[  \theta\in\Theta_{1}\right]  }.
\]</span> For Bayesians, the utility function provides a natural loss function. Historically, decision theory was developed by classical statisticians, thus the development in terms of “objective” loss functions instead of “subjective” utility.</p>
<p>Classical decision theory takes a frequentist approach, treating parameters as “fixed but unknown” and evaluating decisions based on their population properties. Intuitively, this thought experiment entails drawing a dataset <span class="math inline">\(y\)</span> of given length and applying the same decision rule in a large number of repeated trials and averaging the resulting loss across those hypothetical samples. Formally, the classical risk function is defined as <span class="math display">\[
R(\theta,d)=\int_{\mathcal{Y}}L\left[  \theta,d(y)\right]  p(y|\theta )dy=\mathbb{E}\left[  L\left[  \theta,d(y)\right]  |\theta\right]  .
\]</span> Since the risk function integrates over the data, it does not depend on a given observed sample and is therefore an ex-ante or a-priori metric. In the case of quadratic loss, the risk function is the mean-squared error (MSE) and is <span class="math display">\[\begin{align*}
R(\theta,d)  &amp;  =\int_{\mathcal{Y}}\left[  \theta-d\left(  y\right)  \right]
^{2}p(y|\theta)dy\\
&amp;  =\mathbb{E}\left[  \left(  d\left(  y\right)  -E\left[  d\left(  y\right)
|\theta\right]  \right)  ^{2}|\theta\right]  +\mathbb{E}\left[  \left(
E\left[  d\left(  y\right)  |\theta\right]  -\theta\right)  ^{2}|\theta\right]
\\
&amp;  =Var\left(  d\left(  y\right)  |\theta\right)  +\left[  bias\left(
d\left(  y\right)  -\theta\right)  \right]  ^{2}%
\end{align*}\]</span> which can be interpreted as the bias of the decision/estimator plus the variance of the decision/estimator. Common frequentist estimators choose unbiased estimators so that the bias term is zero, which in most settings leads to unique estimators.</p>
<p>The goal of the decision maker is to minimize risk. Unfortunately, rarely is there a decision that minimizes risk uniformly for all parameter values. To see this, consider a simple example of <span class="math inline">\(y\sim N\left(  \theta,1\right)\)</span>, a quadratic loss, and two decision rules, <span class="math inline">\(d_{1}\left(  y\right)  =0\)</span> or <span class="math inline">\(d_{2}\left(  y\right)  =y\)</span>. Then, <span class="math inline">\(R\left(  \theta,d_{1}\right)  =\theta^{2}\)</span> and <span class="math inline">\(R\left(  \theta,d_{2}\right)  =1\)</span>. If <span class="math inline">\(\left\vert \theta\right\vert &lt;1\)</span>, then <span class="math inline">\(R\left(  \theta,d_{1}\right)  &lt;R\left(  \theta,d_{2}\right)\)</span>, with the ordering reversed for <span class="math inline">\(\left\vert \theta\right\vert &gt;1\)</span>. Thus, neither rule uniformly dominates the other.</p>
<p>One way to deal with the lack of uniform domination is to use the minimax principle:&nbsp;first maximize risk as function of <span class="math inline">\(\theta\)</span>, <span class="math display">\[
\theta^{\ast}=\underset{\theta\in\Theta}{\arg\max}R(\theta,d)\text{,}%
\]</span> and then minimize the resulting risk by choosing a decision:<br>
<span class="math display">\[
d_{m}^{\ast}=\underset{d\in\mathcal{D}}{\arg\min}\left[  R(\theta^{\ast },d)\right]  \text{.}%
\]</span> The resulting decision is known as a minimax decision rule. The motivation for minimax is game theory, with the idea that the statistician chooses the best decision rule against the other player, mother nature, who chooses the worst parameter.</p>
<p>The Bayesian approach treats parameters as random and specifies both a likelihood and prior distribution, denoted here by <span class="math inline">\(\pi\left(  \theta\right)\)</span>. The Bayesian decision maker recognizes that both the data and parameters are random, and accounts for both sources of uncertainty when calculating risk. The Bayes risk is defined as<br>
<span class="math display">\[\begin{align*}
r(\pi,d)  &amp;  =\int_{\mathcal{\Theta}}\int_{\mathcal{Y}}L\left[  \theta ,d(y)\right]  p(y|\theta)\pi\left(  \theta\right)  dyd\theta\\
&amp;  =\int_{\mathcal{\Theta}}R(\theta,d)\pi\left(  \theta\right)  d\theta =\mathbb{E}_{\pi}\left[  R(\theta,d)\right]  ,
\end{align*}\]</span> and thus the Bayes risk is an average of the classical risk, with the expectation taken under the prior distribution. The Bayes decision rule minimizes expected risk:<br>
<span class="math display">\[
d_{\pi}^{\ast}=\underset{d\in\mathcal{D}}{\arg\min}\text{ }r(\pi,d)\text{.}%
\]</span> The classical risk of a Bayes decision rule is defined as <span class="math inline">\(R\left(
\theta,d_{\pi}^{\ast}\right)\)</span>, where <span class="math inline">\(d_{\pi}^{\ast}\)</span> does not depend on <span class="math inline">\(\theta\)</span> or <span class="math inline">\(y\)</span>. Minimizing expected risk is consistent with maximizing posterior expected utility or, in this case, minimizing expected loss. Expected posterior risk is <span class="math display">\[
r(\pi,d)=\int_{\mathcal{Y}}\left[  \int_{\mathcal{\Theta}}L\left[
\theta,d(y)\right]  p(y|\theta)\pi\left(  \theta\right)  d\theta\right]  dy,
\]</span> where the term in the brackets is posterior expected loss. Minimizing posterior expected loss for every <span class="math inline">\(y\in\mathcal{Y},\)</span> is clearly equivalent to minimizing posterior expected risk, provided it is possibility to interchange the order of integration.</p>
<p>The previous definitions did not explicitly state that the prior distribution was proper, that is, that <span class="math inline">\(\int_{\mathcal{\Theta}}\pi\left(  \theta\right)d\theta=1\)</span>. In some applications and for some parameters, researchers may use priors that do not integrate, <span class="math inline">\(\int_{\Theta}\pi\left(  \theta\right)d\theta=\infty\)</span>, commonly called improper priors. A generalized Bayes rule is one that minimizes <span class="math inline">\(r(\pi,d),\)</span> where <span class="math inline">\(\pi\)</span> is not necessarily a distribution, if such a rule exists. If <span class="math inline">\(r(\pi,d)&lt;\infty\)</span>, then the mechanics of this rule is clear, although its meaning is less clear.</p>
</section>
<section id="bias-variance-trade-off" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="bias-variance-trade-off"><span class="header-section-number">16.3</span> Bias-Variance Trade-off</h2>
<div id="exm-stein" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.2 (Stein’s Paradox)</strong></span> Stein’s paradox, as explained <span class="citation" data-cites="efron1977">Efron and Morris (<a href="#ref-efron1977" role="doc-biblioref">1977</a>)</span>, is a phenomenon in statistics that challenges our intuitive understanding of estimation. The paradox arises when trying to estimate the mean of a multivariate normal distribution. Traditionally, the best guess about the future is usually obtained by computing the average of past events. However, Charles Stein showed that there are circumstances where there are estimators better than the arithmetic average. This is what’s known as Stein’s paradox.</p>
<p>In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.</p>
<p>In each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.</p>
<p>We reproduce the baseball bartting average example from <span class="citation" data-cites="efron1977">Efron and Morris (<a href="#ref-efron1977" role="doc-biblioref">1977</a>)</span>. The data is available in the <code>R</code> package <code>Lahman</code>. We will use the data from 2016 season.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Lahman)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Suppose that we have <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(y_{1},\ldots,y_{n}\)</span> from a <span class="math inline">\(N\left(  \theta,\sigma^{2}\right)\)</span> distribution. The maximum likelihood estimator is <span class="math inline">\(\widehat{\theta}=\bar{y}\)</span>, the sample mean. The Bayes estimator is the posterior mean, <span class="math inline">\(\widehat{\theta}=\mathbb{E}\left[  \theta\mid y\right]  =\frac{\sigma^{2}}{\sigma^{2}+n}% \bar{y}\)</span>. The Bayes estimator is a shrinkage estimator, it shrinks the MLE towards the prior mean. The amount of shrinkage is determined by the ratio of the variance of the prior and the variance of the likelihood. The Bayes estimator is also a function of the MLE, <span class="math inline">\(\widehat{\theta}=\frac{\sigma^{2}}{\sigma^{2}+n}\bar{y}+\frac{n}{\sigma^{2}+n}\widehat{\theta}\)</span>. This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior. The Bayes estimator is a function of the MLE, <span class="math inline">\(\widehat{\theta}=\frac{\sigma^{2}}{\sigma^{2}+n}\bar{y}+\frac{n}{\sigma^{2}+n}\widehat{\theta}\)</span>. This is a general property of Bayes estimators, they are functions of the MLE. This is a consequence of the fact that the posterior distribution is a function of the likelihood and the prior.</p>
</div>
<p>For any predictive model we seek to achieve best possible results, i.e.&nbsp;smallest MSE or misclassification rate. However, a model performance can be different as data used in one training/validation split may produce results dissimilar to another random split. In addition, a model that performed well on the test set may not produce good results given additional data. Sometimes we observe a situation, when a small change in the data leads to large change in the final estimated model, e.g.&nbsp;parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias produces large variance in the final results. Similarly, low bias results in low variance, but can also produce an oversimplification of the final model. While Bias/variance concept is depicted below.</p>
<div id="fig-bias-variance" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/bias-variance.drawio.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.2: Bias-variance trade-off
</figcaption>
</figure>
</div>
<p>:::{#exm-bias-variance} ## Bias-variance We demonstrate bias-variance concept using Boston housing example. We fit a model <span class="math inline">\(\mathrm{medv} = f(\mathrm{lstat})\)</span>. We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree <span class="math inline">\(1,\ldots,12\)</span> ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial <span class="math inline">\(f\)</span> we averaged MSE from each of the ten models.</p>
<p><a href="#fig-boston-bias-variance" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-boston-bias-variance</span></a> shows bias and variance for our twelve different models. As expected, bias increases while variance increases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!</p>
<div id="fig-boston" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boston-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-boston" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-boston-bias-variance" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-boston-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/boston-bias-variance.svg" class="img-fluid figure-img" data-ref-parent="fig-boston">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-boston-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Metrics for twelve polynomial functions fitted into Boston housing data set. As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-boston" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-boston-optimal-model" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-boston-optimal-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/boston-optimal-model.svg" class="img-fluid figure-img" data-ref-parent="fig-boston">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-boston-optimal-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Optimal complexity model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boston-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.3: Metrics for 12 models
</figcaption>
</figure>
</div>
<p>Let’s take another, a more formal, look at bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error <span class="math inline">\(\E{(y-\hat y)^2}\)</span> as a function of bias <span class="math inline">\(\E{y-\hat y}\)</span> and variance <span class="math inline">\(\Var{\hat y}\)</span>.</p>
<p>Here <span class="math inline">\(\hat y = \hat f_{\beta}(x)\)</span> prediction from the model, and <span class="math inline">\(y = f(x) + \epsilon\)</span> is the true value, which is measured with noise <span class="math inline">\(\Var{\epsilon} = \sigma^2\)</span>, <span class="math inline">\(f(x)\)</span> is the true unknown function. The expectation above measures squared error of our model on a random sample <span class="math inline">\(x\)</span>. <span class="math display">\[
\begin{aligned}
\E{(y - \hat{y})^2}
&amp; = \E{y^2 + \hat{y}^2 - 2 y\hat{y}} \\
&amp; = \E{y^2} + \E{\hat{y}^2} - \E{2y\hat{y}} \\
&amp; = \Var{y} + \E{y}^2 + \Var{\hat{y}} + \E{\hat{y}}^2 - 2f\E{\hat{y}} \\
&amp; = \Var{y} + \Var{\hat{y}} + (f^2 - 2f\E{\hat{y}} + \E{\hat{y}}^2) \\
&amp; = \Var{y} + \Var{\hat{y}} + (f - \E{\hat{y}})^2 \\
&amp; = \sigma^2 + \Var{\hat{y}} + \mathrm{Bias}(\hat{y})^2\end{aligned}
\]</span> Here we used the following identity: <span class="math inline">\(\Var{X} = \E{X^2} - \E{X}^2\)</span> and the fact that <span class="math inline">\(f\)</span> is deterministic and <span class="math inline">\(\E{\epsilon} = 0\)</span>, thus <span class="math inline">\(\E{y} = \E{f(x)+\epsilon} = f + \E{\epsilon} = f\)</span>.</p>
</section>
<section id="cross-validation" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">16.4</span> Cross-Validation</h2>
<p>If the data set at-hand is small and we cannot dedicate large enough sample size for testing, simply measuring error on test data set can lead to wrong conclusions. When size of the testing set <span class="math inline">\(D^*\)</span> is small, the estimated out-of-sample performance is of high variance, depending on precisely which observations are included in the test set. On the other hand, when training set <span class="math inline">\(D^*\)</span> is a large fraction of the entire sample available, estimated out-of-sample performance will be underestimated. Why?</p>
<p>A trivial solution is to perform the training/testing split randomly several times and then use average out-of-sample errors. This procedure has two parameters, the fraction of samples to be selected for testing <span class="math inline">\(p\)</span> and number of estimates to be performed <span class="math inline">\(K\)</span>. The resulting algorithm is as follows</p>
<pre><code>fsz = as.integer(p*n)
error = rep(0,K)
for (k in 1:K)
{
    test_ind = sample(1:n,size = fsz)
    training = d[-test_ind,]
    testing  = d[test_ind,]
    m = lm(y~x, data=training)
    yhat = predict(m,newdata = testing)
    error[k] = mean((yhat-testing$y)^2)
}
res = mean(error)</code></pre>
<p><a href="#fig-bootstrap" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bootstrap</span></a> shows the process of splitting data set randomly five times.</p>
<p>Cross validation modifies the random splitting approach uses more “disciplined” way to split data set for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations and thus, each data point is used once for testing. As the random approach, CV helps addressing the high variance issue of out-of-sample performance estimation when data set available is small. <a href="#fig-cv" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-cv</span></a> shows the process of splitting data set five times using cross-validation approach.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-bootstrap" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/bag5-excel.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.4: Bootstrap
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-cv" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/cv5-excel.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.5: Cross-validation
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Training set (red) and testing set (green)</p>
</div>
</div>
</div>
<div id="exm-simulated" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.3 (Simulated)</strong></span> We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used <span class="math inline">\(x=-2,-1.99,-1.98,\ldots,2\)</span> and <span class="math inline">\(y = 2+3x + \epsilon, ~ \epsilon \sim N(0,\sqrt{3})\)</span>. We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. <a href="#fig-test-error20" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-test-error20</span></a> compares empirical distribution of errors estimated from 35 samples.</p>
<div id="fig-test-error20" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-test-error20-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/test-error20.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-test-error20-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.6: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.
</figcaption>
</figure>
</div>
<p>As we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.</p>
</div>
</section>
<section id="small-sample-size" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="small-sample-size"><span class="header-section-number">16.5</span> Small Sample Size</h2>
<p>When sample size is small and it is not feasible to divide your data into training and validation data sets, an information criterion could be used to assess a model. We can think of information criterion as a metric that “approximates” out-os-sample performance of the model. Akaike’s Information Criterion (AIC) takes the form <span class="math display">\[
\mathrm{AIC} = log(\sigma_k^2) + \dfrac{n+2k}{n}
\]</span> <span class="math display">\[
\hat{\sigma}_k^2 = \dfrac{SSE_k}{n}
\]</span> Here <span class="math inline">\(k\)</span> = number of coefficients in regression model, <span class="math inline">\(SSE_k\)</span> = residual sum of square, <span class="math inline">\(\hat{\sigma}_k^2\)</span> = MLE estimator for variance. We do not need to proceed sequentially, each model individually evaluated</p>
<p>AIC is derived using the Kullback-Leibler information number. It is a ruler to measure the similarity between the statistical model and the true distribution. <span class="math display">\[
I(g ; f) = E_g\left(\log \left\{\dfrac{g(y)}{f(y)}\right\}\right) = \int_{-\infty}^{\infty}\log \left\{\dfrac{g(y)}{f(y)}\right\}g(y)dy.
\]</span> Here - <span class="math inline">\(I(g ; f) &gt; 0\)</span> - <span class="math inline">\(I(g ; f) = 0 \iff g(u) = f(y)\)</span> - <span class="math inline">\(f \rightarrow g\)</span> as <span class="math inline">\(I(g ; f) \rightarrow 0\)</span></p>
<p>To estimate <span class="math inline">\(I(g ; f)\)</span>, we write <span class="math display">\[
I(g ; f) = E_g\left(\log \left\{\dfrac{g(y)}{f(y)}\right\}\right) = E_g (\log g(y)) - E_g(\log f(y))
\]</span> Only the second term is important in evaluating the statistical model <span class="math inline">\(f(y)\)</span>. Thus we need to estimate <span class="math inline">\(E_g(\log f(y))\)</span>. Given sample <span class="math inline">\(z_1,...,z_n\)</span>, and estimated parameters <span class="math inline">\(\hat{\theta}\)</span> a naive estimate is <span class="math display">\[
\hat{E}_g(\log f(y)) =  \dfrac{1}{n} \sum_{i=1}^n \log f(z_i) = \dfrac{\ell(\hat{\theta})}{n}
\]</span> where <span class="math inline">\(\ell(\hat{\theta})\)</span> is the log-likelihood function for model under test.</p>
<ul>
<li>this estimate is very biased</li>
<li>data used used twice: to get the MLE and second to estimate the integral</li>
<li>it will favor those model that overfit</li>
</ul>
<p>Akaike showed that the bias is approximately <span class="math inline">\(k/n\)</span> where <span class="math inline">\(k\)</span> is the number of parameters <span class="math inline">\(\theta\)</span>. Therefore we use <span class="math display">\[
\hat{E}_g(\log f(y)) = \dfrac{\ell(\hat{\theta})}{n} - \dfrac{k}{n}
\]</span> Which leads to AIC <span class="math display">\[
AIC = 2n \hat{E}_g(\log f(y)) = 2 \ell(\hat{\theta}) - 2k
\]</span></p>
<p>Akaike’s Information Criterion (AIC) <span class="math display">\[
\mathrm{AIC} = \log(\sigma_k^2) + \dfrac{n+2k}{n}
\]</span> Controls for balance between model complexity (<span class="math inline">\(k\)</span>) and minimizing variance. The model selection process involve trying different <span class="math inline">\(k\)</span>, chose model with smallest AIC.</p>
<p>A slightly modified version designed for small samples is the bias corrected AIC (AICc). <span class="math display">\[
\mathrm{AICc} = \log(\hat{\sigma}_k^2) + \dfrac{n+k}{n-k-2}
\]</span> This criterion should be used for regression models with small samples</p>
<p>Yet, another variation designed for larger datasets is the Bayesian Information Criterion (BIC). <span class="math display">\[
\mathrm{BIC} = \log(\hat{\sigma}_k^2) + \dfrac{k \log(n)}{n}
\]</span> Is is the same as AIC but harsher penalty, this chooses simpler models. It works better for large samples when compared to AICc. The motivation fo BIC is from the posterior distribution over model space. Bayes rule lets you calculate the joint probability of parameter and models as <span class="math display">\[
p(\theta,M\mid D) = \dfrac{p(D\mid \theta,M(p(M,\theta)}{p(D)},~~ p(M\mid D) = \int p(\theta,M\mid D)d\theta \approx n^{p/2}p(D\mid \hat \theta M)p(M).
\]</span></p>
<p>Consider a problem of predicting mortality rates given pollution and temperature measurements. Let’s plot the data.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poll-temp-mort" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poll-temp-mort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/poll-temp-mort.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poll-temp-mort-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.7: Time Series Plot
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poll-temp-mort-scatter" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poll-temp-mort-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/poll-temp-mort-scatter.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poll-temp-mort-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.8: Scatter Plot
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Regression Model 1, which just uses the trend: <span class="math inline">\(M_t = \beta_1 + \beta_2 t + w_t\)</span>. We fit by calling <code>lm(formula = cmort ~ trend)</code> to get the following coefficients</p>
<pre><code>                Estimate    Std. Error t value  
    (Intercept) 3297.6062   276.3132   11.93
    trend         -1.6249     0.1399  -11.61</code></pre>
<p>Regression Model 2 regresses to time (trend) and temperature: <span class="math inline">\(M_t = \beta_1 + \beta_2 t + \beta_t(T_t - T)+ w_t\)</span>. The R call is <code>lm(formula = cmort ~ trend + temp)</code></p>
<pre><code>                Estimate    Std. Error t value 
    (Intercept) 3125.75988  245.48233   12.73 
    trend         -1.53785    0.12430  -12.37 
    temp          -0.45792    0.03893  -11.76 </code></pre>
<p>Regression Model 3, uses trend, temperature and mortality: <span class="math inline">\(M_t = \beta_1 + \beta_2 t + \beta_3(T_t - T)+ \beta_4(T_t - T)^2 + w_t\)</span>. The R call is <code>lm(formula = cmort ~ trend + temp + I(temp^2)</code></p>
<pre><code>                Estimate    Std. Error t value 
    (Intercept)  3.038e+03  2.322e+02  13.083 
    trend       -1.494e+00  1.176e-01 -12.710 
    temp        -4.808e-01  3.689e-02 -13.031 
    temp2        2.583e-02  3.287e-03   7.858 </code></pre>
<p>Regression Model 4 adds temperature squared: <span class="math inline">\(M_t = \beta_1 + \beta_2 t + \beta_3(T_t - T)+ \beta_4(T_t - T)^2 + \beta_5 P_t+ w_t\)</span>. The R call is <code>lm(formula = cmort ~ trend + temp +  I(temp^2) + part)</code></p>
<pre><code>                Estimate    Std. Error t value 
    (Intercept)  2.831e+03  1.996e+02   14.19 
    trend       -1.396e+00  1.010e-01  -13.82 
    temp        -4.725e-01  3.162e-02  -14.94 
    temp2        2.259e-02  2.827e-03    7.99 
    part         2.554e-01  1.886e-02   13.54 </code></pre>
<p>To choose the model, we look at the information criterion</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;"><span class="math inline">\(k\)</span></th>
<th style="text-align: center;">SSE</th>
<th style="text-align: center;">df</th>
<th style="text-align: center;">MSE</th>
<th style="text-align: center;"><span class="math inline">\(R^2\)</span></th>
<th style="text-align: center;">AIC</th>
<th style="text-align: center;">BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">40,020</td>
<td style="text-align: center;">506</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">.21</td>
<td style="text-align: center;">5.38</td>
<td style="text-align: center;">5.40</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">31,413</td>
<td style="text-align: center;">505</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">.38</td>
<td style="text-align: center;">5.14</td>
<td style="text-align: center;">5.17</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">27,985</td>
<td style="text-align: center;">504</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">.45</td>
<td style="text-align: center;">5.03</td>
<td style="text-align: center;">5.07</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20,508</td>
<td style="text-align: center;">503</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">.60</td>
<td style="text-align: center;">4.72</td>
<td style="text-align: center;">4.77</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(R^2\)</span> always decreases with number of covariates (that is what MLE does). Thus, cannot be used as a selection criteria. <span class="math inline">\(R^2\)</span> for out-of-sample data is useful!</p>
<p>The message to take home on model selection</p>
<ul>
<li><span class="math inline">\(R^2\)</span> is NOT a good metric for model selection</li>
<li>Value of likelihood function is NOT a god metric</li>
<li>are intuitive and work very well in practice (you should use those)</li>
<li>AIC is good for big <span class="math inline">\(n/df\)</span>, so it overfits in high dimensions</li>
<li>Should prefer AICc over AIC</li>
<li>BIC underfits for large <span class="math inline">\(n\)</span></li>
<li>Cross-validation is important, we will go over it later</li>
</ul>
</section>
<section id="regularization" class="level2" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="regularization"><span class="header-section-number">16.6</span> Regularization</h2>
<p>Regularization is a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.</p>
<p>::: {#exm-traffic} ## Traffic Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. <a href="#fig-speed-profile" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-speed-profile</span></a> shows speed measured data, averaged over five minute intervals on one of the weekdays.</p>
<div id="fig-speed-profile" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/day_295.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.9: Speed profile over 24 hour period on I-55, on October 22, 2013
</figcaption>
</figure>
</div>
<p>Speed measurements are noisy and prone to have outliers. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes.</p>
<p>Trend filtering, which is a variation of a well-know Hodrick-Prescott filter. In this case, the trend estimate is the minimizer of the weighted sum objective function <span class="math display">\[
(1/2) \sum_{t=1}^{n}(y_t - x_t)^2 + \lambda \sum_{t=1}^{n-1}|x_{t-1} - 2x_t + x_{t+1}|,
\]</span></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/traffic_l1.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filter for different penalty</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/day_295_tf.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filtering with optimal penalty</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Trend Filtering for Traffic Speed Data</p>
</div>
</div>
</div>
</section>
<section id="ridge-regression" class="level2" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="ridge-regression"><span class="header-section-number">16.7</span> Ridge Regression</h2>
<p>Gauss invented the concept of least squares and developed algorithms to solve the the optimization problem <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2
\]</span> where <span class="math inline">\(\beta = (\beta_1 , \ldots , \beta_p )\)</span>, we can use linear algebra algorithms, the solution given by <span class="math display">\[
\hat{\beta} = ( X^T X )^{-1} X^T y
\]</span> This can be numerically unstable when <span class="math inline">\(X^T X\)</span> is ill-conditioned, and happens when <span class="math inline">\(p\)</span> is large. Ridge regression addresses this problem by adding an extra term to the <span class="math inline">\(X^TX\)</span> matrix <span class="math display">\[
\hat{\beta}_{\text{ridge}} = ( X^T X + \lambda I )^{-1} X^T y.
\]</span> The corresponding optimization problem is <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2   + \lambda||\beta||_2^2.
\]</span> An alternative formulation is We can think of the constrain is of a budget on the size of <span class="math inline">\(\beta\)</span>.</p>
<p>The we choose <span class="math inline">\(\lambda\)</span> over a regularisation path. The penalty in ridge regression forces coefficients <span class="math inline">\(\beta\)</span> to be close to 0. Penalty is large for large values and very small for small ones. Tuning parameter <span class="math inline">\(\lambda\)</span> controls trade-off between how well model fits the data and how small <span class="math inline">\(\beta\)</span>s are. Different values of <span class="math inline">\(\lambda\)</span> will lead to different models. We select <span class="math inline">\(\lambda\)</span> using cross validation.</p>
<div id="exm-simulated-bias" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.4 (Shrinkage)</strong></span> Consider a simulated data with <span class="math inline">\(n=50\)</span>, <span class="math inline">\(p=30\)</span>, and <span class="math inline">\(\sigma^2=1\)</span>. The true model is linear with <span class="math inline">\(10\)</span> large coefficients between <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Our approximators <span class="math inline">\(\hat f_{\beta}\)</span> is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss <span class="math inline">\(1/n||y -\hat y||_2^2\)</span> and variance can be empirically calculated as <span class="math inline">\(1/n\sum  (\bar{\hat{y}} - \hat y_i)\)</span></p>
<p>Bias squared <span class="math inline">\(\mathrm{Bias}(\hat{y})^2=0.006\)</span> and variance <span class="math inline">\(\Var{\hat{y}} =0.627\)</span>. Thus, the prediction error = <span class="math inline">\(1 + 0.006 + 0.627 = 1.633\)</span></p>
<p>We’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/ridge_beta.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>True model coefficients</figcaption>
</figure>
</div>
<p>Now we see the accuracy of the model as a function of <span class="math inline">\(\lambda\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/ridge_mse.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Prediction error as a function of <span class="math inline">\(\lambda\)</span></figcaption>
</figure>
</div>
<p>Ridge Regression At best: Bias squared <span class="math inline">\(=0.077\)</span> and variance <span class="math inline">\(=0.402\)</span>.</p>
<p>Prediction error = <span class="math inline">\(1 + 0.077 + 0.403 = 1.48\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/ridge_bias_variance.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Ridge</figcaption>
</figure>
</div>
</div>
<p>The additional term <span class="math inline">\(\lambda||\beta||_2^2\)</span> in the optimization problem is called the regularization term. There are several ways to regularize an optimization problem. All of those techniques were developed in the middle of last century and were applied to solve problems of fitting physics models into observed data, those frequently arise in physics and engineering applications. Here are a few examples of such regularization techniques.</p>
<p><strong>Ivanov regularization</strong> <span class="math display">\[
\underset{x \in \mathbb{R^n}}{\mathrm{minimize}}\quad ||y - X\beta||_2^2~~~~ \mbox{s.t.}~~||\beta||_l \le k
\]</span></p>
<p><strong>Morozov regularization</strong> <span class="math display">\[
\underset{x \in \mathbb{R^n}}{\mathrm{minimize}}\quad ||\beta||_l~~~~ \mbox{s.t.}~~ ||y - X\beta||_2^2 \le \tau
\]</span> Here <span class="math inline">\(\tau\)</span> reflects the so called noise level, i.e.&nbsp;an estimate of the error which is made during the measurement of <span class="math inline">\(b\)</span>.</p>
<p><strong>Tikhonov regularization</strong> <span class="math display">\[
\underset{\beta\in \mathbb{R^n}}{\mathrm{minimize}}\quad ||y - X\beta||_2^2 + \lambda||\beta||_l
\]</span> - Tikhonov regularization with <span class="math inline">\(l=1\)</span> is lasso - Tikhonov regularization with <span class="math inline">\(l=2\)</span> is ridge regression - lasso + ridge = elastic net</p>
</section>
<section id="ell_1-regularization-lasso" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="ell_1-regularization-lasso"><span class="header-section-number">16.8</span> <span class="math inline">\(\ell_1\)</span> Regularization (LASSO)</h2>
<p>The Least Absolute Shrinkage and Selection Operator (LASSO) uses <span class="math inline">\(\ell_1\)</span> norm penalty and in case of linear regression leads to the following optimization problem <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2   + \lambda||\beta||_1
\]</span></p>
<p>In one dimensional case solves the following optimization problem <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad  \frac{1}{2} (y-\beta)^2 + \lambda | \beta |
\]</span> The solution is given by the soft-thresholding operator defined by <span class="math display">\[
\hat{\beta} = \mathrm{soft} (y; \lambda) = ( y - \lambda ~\mathrm{sgn}(y) )_+.
\]</span> Here sgn is the sign function and <span class="math inline">\(( x )_+ = \max (x,0)\)</span>. To demonstrate how this solution is derived, we can define a slack variable <span class="math inline">\(z = | \beta |\)</span> and solve the joint constrained optimisation problem which is differentiable.</p>
<p>Graphically, the soft-thresholding operator is</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/soft-thresh.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Soft-threshold operator.</figcaption>
</figure>
</div>
<p>LASSO has a nice feature that it forces some of the <span class="math inline">\(\hat{\beta}\)</span>’s to zero. It is an automatic variable selection! Finding optimal solution is computationally fast, it is a convex optimisation problem, though, it is non-smooth. As in ridge regression, we still have to pick <span class="math inline">\(\lambda\)</span> via cross-validation. Visually the process can be represented using regularization path graph, as in the following example Example: We model prostate cancer using LASSO</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/prostate_mse.svg" class="img-fluid figure-img"></p>
<figcaption>MSE.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/prostate_reg_path.svg" class="img-fluid figure-img"></p>
<figcaption>Path</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>MSE and Regularization path for Prostate Cancer data using LASSO</p>
</div>
</div>
</div>
<p>Now with ridge regression</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/prostate_mse_ridge.svg" class="img-fluid figure-img"></p>
<figcaption>MSE</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/prostate_reg_path_ridge.svg" class="img-fluid figure-img"></p>
<figcaption>Path</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>MSE and Regularization path for Prostate Cancer data using Ridge</p>
</div>
</div>
</div>
<div id="exm-horse" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.5 (Horse race prediction using logistic regression)</strong></span> We use the <code>run.csv</code> data from Kaggle (<a href="https://www.kaggle.com/gdaley/hkracing" class="uri">https://www.kaggle.com/gdaley/hkracing</a>). Thhis dataset contains the condition of horse races in Hong Kong, including race course, distance, track condition and dividends paid. We want to use individual variables to predict the chance of winning of a horse. For the simplicity of computation, we only consider horses with id <span class="math inline">\(\leq 500\)</span>, and train the model with <span class="math inline">\(\ell_1\)</span>-regularized logistic regression.</p>
<p>And we include <code>lengths_behind</code>, <code>horse_age</code>, <code>horse_country</code>, <code>horse_type</code>, <code>horse_rating</code>, <code>horse_gear</code>, <code>declared_weight</code>, <code>actual_weight</code>, <code>draw</code>, <code>win_odds</code>, <code>place_odds</code> as predicting variables in our model.</p>
<p>Since most of the variables, such as <code>country</code>, <code>gear</code>, <code>type</code>, are categorical, after spanning them into binary indictors, we have more than 800 columns in the design matrix.</p>
<p>We try two logistic regression model. The first one includes win_odds given by the gambling company. The second one does not include the win_odds and we use win_odds to test the power of our model. We tune both models with a 10-fold cross-validation to find the best penalty parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>In this model, we fit the logistic regression with full dataset. The best <span class="math inline">\(\lambda\)</span> we find is <span class="math inline">\(5.699782e-06\)</span>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/var_change_1.svg" class="img-fluid figure-img"></p>
<figcaption>Number of variables vs lambda</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/coef_rank_1.svg" class="img-fluid figure-img"></p>
<figcaption>Coefficient Ranking</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Logistic regression for full data</p>
</div>
</div>
</div>
<p>In this model, we randomly partition the dataset into training(70%) and testing(30%) parts. We fit the logistic regression with training dataset. The best <span class="math inline">\(\lambda\)</span> we find is <span class="math inline">\(4.792637e-06\)</span>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/var_change_2.svg" class="img-fluid figure-img"></p>
<figcaption>Number of variables vs lambda</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/coef_rank_2.svg" class="img-fluid figure-img"></p>
<figcaption>Coefficient Ranking</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Logistic regression for test data</p>
</div>
</div>
</div>
<p>The out-of-sample mean squared error for <code>win_odds</code> is 0.0668.</p>
</div>
<p><strong>Elastic Net</strong> combines Ridge and Lasso and chooses coefficients <span class="math inline">\(\beta_1,\ldots,\beta_p\)</span> for the input variables by minimizing the sum-of-squared residuals plus a penalty of the form <span class="math display">\[
\lambda||\beta||_1 + \alpha||\beta||_2^2.
\]</span></p>
</section>
<section id="bayesian-regularization" class="level2" data-number="16.9">
<h2 data-number="16.9" class="anchored" data-anchor-id="bayesian-regularization"><span class="header-section-number">16.9</span> Bayesian Regularization</h2>
<p>From Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian <span class="math inline">\(l_0\)</span> regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.</p>
<p>In Bayesian approach, regularization requires the specification of a loss, denoted by <span class="math inline">\(l\left(\beta\right)\)</span> and a penalty function, denoted by <span class="math inline">\(\phi_{\lambda}(\beta)\)</span>, where <span class="math inline">\(\lambda\)</span> is a global regularization parameter. From a Bayesian perspective, <span class="math inline">\(l\left(\beta\right)\)</span> and <span class="math inline">\(\phi_{\lambda}(\beta)\)</span> correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form <span class="math display">\[
\underset{\beta \in R^p}{\mathrm{minimize}\quad}
l\left(\beta\right) + \phi_{\lambda}(\beta) \; .
\]</span> Taking a probabilistic approach leads to a Bayesian hierarchical model <span class="math display">\[
p(y \mid \beta) \propto \exp\{-l(\beta)\} \; , \quad p(\beta) \propto \exp\{ -\phi_{\lambda}(\beta) \} \ .
\]</span> The solution to the minimization problem estimated by regularization corresponds to the posterior mode, <span class="math inline">\(\hat{\beta} = \mathrm{ arg \; max}_\beta \; p( \beta|y)\)</span>, where <span class="math inline">\(p(\beta|y)\)</span> denotes the posterior distribution. Consider a normal mean problem with <span class="math display">\[
\label{eqn:linreg}
y = \theta+ e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \theta \le \infty \ .
\]</span> What prior <span class="math inline">\(p(\theta)\)</span> should we place on <span class="math inline">\(\theta\)</span> to be able to separate the “signal” <span class="math inline">\(\theta\)</span> from “noise” <span class="math inline">\(e\)</span>, when we know that there is a good chance that <span class="math inline">\(\theta\)</span> is sparse (i.e.&nbsp;equal to zero). In the multivariate case we have <span class="math inline">\(y_i = \theta_i + e_i\)</span> and sparseness is measured by the number of zeros in <span class="math inline">\(\theta = (\theta_1\ldots,\theta_p)\)</span>. The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where <span class="math display">\[
p(\theta_i \mid b) = 0.5b\exp(-|\theta|/b).
\]</span> We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior <span class="math display">\[
\log p(\theta \mid y, b) \propto ||y-\theta||_2^2 + \dfrac{2\sigma^2}{b}||\theta||_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span> the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to the small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
<p>The Laplace distribution can be represented as scale mixture of Normal distribution<span class="citation" data-cites="andrews1974">(<a href="#ref-andrews1974" role="doc-biblioref">Andrews and Mallows 1974</a>)</span> <span class="math display">\[
\begin{aligned}
\theta_i \mid \sigma^2,\tau \sim &amp;N(0,\tau^2\sigma^2)\\
\tau^2  \mid \alpha \sim &amp;\exp (\alpha^2/2)\\
\sigma^2 \sim &amp; \pi(\sigma^2).\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau\)</span> <span class="math display">\[
p(\theta_i\mid \sigma^2,\alpha) =  \int_{0}^{\infty} \dfrac{1}{\sqrt{2\pi \tau}}\exp\left(-\dfrac{\theta_i^2}{2\sigma^2\tau}\right)\dfrac{\alpha^2}{2}\exp\left(-\dfrac{\alpha^2\tau}{2}\right)d\tau = \dfrac{\alpha}{2\sigma}\exp(-\alpha/\sigma|\theta_i|).
\]</span> Thus it is a Laplace distribution with location 0 and scale <span class="math inline">\(\alpha/\sigma\)</span>. Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from <span class="math inline">\(\theta \mid a,y\)</span> and <span class="math inline">\(b\mid \theta,y\)</span> to estimate joint distribution over <span class="math inline">\((\hat \theta, \hat b)\)</span>. Thus, we so not need to apply cross-validation to find optimal value of <span class="math inline">\(b\)</span>, the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.</p>
<p>When prior is Normal <span class="math inline">\(\theta_i \sim N(0,\sigma_{\theta}^2)\)</span>, the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional <span class="math inline">\(\lambda\propto 1/\sigma_{\theta}^2\)</span>.</p>
</section>
<section id="spike-and-slab-prior" class="level2" data-number="16.10">
<h2 data-number="16.10" class="anchored" data-anchor-id="spike-and-slab-prior"><span class="header-section-number">16.10</span> Spike-and-Slab Prior</h2>
<p>Our Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem <span class="math display">\[
y = \beta_1x_1+\ldots+\beta_px_p + e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \beta_i \le \infty \ .
\]</span> We would like to solve the problem of variable selections, i.e.&nbsp;identify which input variables <span class="math inline">\(x_i\)</span> to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.</p>
<p>To perform a model selection, we would like to specify a prior distribution <span class="math inline">\(p\left(\beta\right)\)</span>, which imposes a sparsity assumption on <span class="math inline">\(\beta\)</span>, where only a small portion of all <span class="math inline">\(\beta_i\)</span>’s are non-zero. In other words, <span class="math inline">\(\|\beta\|_0 = k \ll p\)</span>, where <span class="math inline">\(\|\beta\|_0 \defeq \#\{i : \beta_i\neq0\}\)</span>, the cardinality of the support of <span class="math inline">\(\beta\)</span>, also known as the <span class="math inline">\(l_0\)</span> (pseudo)norm of <span class="math inline">\(\beta\)</span>. A multivariate Gaussian prior (<span class="math inline">\(l_2\)</span> norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for <span class="math inline">\(\beta\)</span> can be constructed to impose sparsity include the double exponential (lasso).</p>
<p>Under spike-and-slab, each <span class="math inline">\(\beta_i\)</span> exchangeably follows a mixture prior consisting of <span class="math inline">\(\delta_0\)</span>, a point mass at <span class="math inline">\(0\)</span>, and a Gaussian distribution centered at zero. Hence we write,</p>
<p><span class="math display">\[
\label{eqn:ss}
\beta_i | \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N\left(0, \sigma_\beta^2\right) \ .
\]</span> Here <span class="math inline">\(\theta\in \left(0, 1\right)\)</span> controls the overall sparsity in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_\beta^2\)</span> accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.</p>
<p>A useful re-parameterization, the parameters <span class="math inline">\(\beta\)</span> is given by two independent random variable vectors <span class="math inline">\(\gamma = \left(\gamma_1, \ldots, \gamma_p\right)'\)</span> and <span class="math inline">\(\alpha = \left(\alpha_1, \ldots, \alpha_p\right)'\)</span> such that <span class="math inline">\(\beta_i  =  \gamma_i\alpha_i\)</span>, with probabilistic structure <span class="math display">\[
\label{eq:bg}
\begin{array}{rcl}
\gamma_i\mid\theta &amp; \sim &amp; \text{Bernoulli}(\theta) \ ;
\\
\alpha_i \mid \sigma_\beta^2 &amp;\sim &amp; N\left(0, \sigma_\beta^2\right) \ .
\\
\end{array}
\]</span> Since <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\alpha_i\)</span> are independent, the joint prior density becomes <span class="math display">\[
p\left(\gamma_i, \alpha_i \mid \theta, \sigma_\beta^2\right) =
\theta^{\gamma_i}\left(1-\theta\right)^{1-\gamma_i}\frac{1}{\sqrt{2\pi}\sigma_\beta}\exp\left\{-\frac{\alpha_i^2}{2\sigma_\beta^2}\right\}
\ , \ \ \ \text{for } 1\leq i\leq p \ .
\]</span> The indicator <span class="math inline">\(\gamma_i\in \{0, 1\}\)</span> can be viewed as a dummy variable to indicate whether <span class="math inline">\(\beta_i\)</span> is included in the model.</p>
<p>Let <span class="math inline">\(S = \{i: \gamma_i = 1\} \subseteq \{1, \ldots, p\}\)</span> be the “active set" of <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\|\gamma\|_0 = \sum\limits_{i = 1}^p\gamma_i\)</span> be its cardinality. The joint prior on the vector <span class="math inline">\(\{\gamma, \alpha\}\)</span> then factorizes as <span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right) &amp; = &amp; \prod\limits_{i = 1}^p p\left(\alpha_i, \gamma_i \mid \theta, \sigma_\beta^2\right) \\
&amp; = &amp;
\theta^{\|\gamma\|_0}
\left(1-\theta\right)^{p - \|\gamma\|_0}
\left(2\pi\sigma_\beta^2\right)^{-\frac p2}\exp\left\{-\frac1{2\sigma_\beta^2}\sum\limits_{i = 1}^p\alpha_i^2\right\} \ .
\end{array}
\]</span></p>
<p>Let <span class="math inline">\(X_\gamma \defeq \left[X_i\right]_{i \in S}\)</span> be the set of “active explanatory variables" and <span class="math inline">\(\alpha_\gamma \defeq \left(\alpha_i\right)'_{i \in S}\)</span> be their corresponding coefficients. We can write <span class="math inline">\(X\beta = X_\gamma \alpha_\gamma\)</span>. The likelihood can be expressed in terms of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\alpha\)</span> as <span class="math display">\[
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)
=
\left(2\pi\sigma_e^2\right)^{-\frac n2}
\exp\left\{
-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
\right\} \ .
\]</span></p>
<p>Under this re-parameterization by <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span>, the posterior is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y\right) &amp; \propto &amp;
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right)
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)\\
&amp; \propto &amp;
\exp\left\{-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
-\frac1{2\sigma_\beta^2}\left\|\alpha\right\|_2^2
-\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0
\right\} \ .
\end{array}
\]</span> Our goal then is to find the regularized maximum a posterior (MAP) estimator <span class="math display">\[
\arg\max\limits_{\gamma, \alpha}p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y \right) \ .
\]</span> By construction, the <span class="math inline">\(\gamma\)</span> <span class="math inline">\(\in\left\{0, 1\right\}^p\)</span> will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span> the regularized least squares objective function</p>
<p><span class="math display">\[
\label{obj:map}
\min\limits_{\gamma, \alpha}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2
+ 2\sigma_e^2\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0 \ .
\]</span> This objective possesses several interesting properties:</p>
<ol type="1">
<li><p>The first term is essentially the least squares loss function.</p></li>
<li><p>The second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) <span class="math inline">\(\sigma_\beta^2/\sigma_e^2\)</span>. Smaller SNR will be more likely to shrink the estimates towards <span class="math inline">\(0\)</span>. If <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want <span class="math inline">\(\sigma_\beta \to \infty\)</span> or to be “sufficiently large" in order to avoid imposing harsh shrinkage to non-zero signals.</p></li>
<li><p>If we further assume that <span class="math inline">\(\theta &lt; \frac12\)</span>, meaning that the coefficients are known to be sparse <em>a priori</em>, then <span class="math inline">\(\log\left(\left(1-\theta\right) / \theta\right) &gt; 0\)</span>, and the third term can be seen as an <span class="math inline">\(l_0\)</span> regularization.</p></li>
</ol>
<p>Therefore, our Bayesian objective inference is connected to <span class="math inline">\(l_0\)</span>-regularized least squares, which we summarize in the following proposition.</p>
<p>(Spike-and-slab MAP &amp; <span class="math inline">\(l_0\)</span> regularization)</p>
<p>For some <span class="math inline">\(\lambda &gt; 0\)</span>, assuming <span class="math inline">\(\theta &lt; \frac12\)</span>, <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the Bayesian MAP estimate defined by (<a href="#obj:map" data-reference-type="ref" data-reference="obj:map">[obj:map]</a>) is equivalent to the <span class="math inline">\(l_0\)</span> regularized least squares objective, for some <span class="math inline">\(\lambda &gt; 0\)</span>, <span class="math display">\[
\label{obj:l0}
\min\limits_{\beta}
\frac12\left\|y - X\beta\right\|_2^2
+ \lambda
\left\|\beta\right\|_0 \ .
\]</span></p>
<p>First, assuming that <span class="math display">\[
\theta &lt; \frac12, \ \ \  \sigma_\beta^2 \gg \sigma_e^2, \ \ \  \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2 \to 0 \ ,
\]</span> gives us an objective function of the form <span class="math display">\[
\min\limits_{\gamma, \alpha}
\label{obj:vs}
\frac12 \left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \lambda
\left\|\gamma\right\|_0,  \ \ \ \  \text{where } \lambda \defeq \sigma_e^2\log\left(\left(1-\theta\right) / \theta\right) &gt; 0 \ .
\]</span></p>
<p>Equation (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>) can be seen as a variable selection version of equation (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>). The interesting fact is that (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>) and (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>) are equivalent. To show this, we need only to check that the optimal solution to (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>) corresponds to a feasible solution to (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>) and vice versa. This is explained as follows.</p>
<p>On the one hand, assuming <span class="math inline">\(\hat\beta\)</span> is an optimal solution to (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>), then we can correspondingly define <span class="math inline">\(\hat\gamma_i \defeq I\left\{\hat\beta_i \neq 0\right\}\)</span>, <span class="math inline">\(\hat\alpha_i \defeq \hat\beta_i\)</span>, such that <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is feasible to (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>) and gives the same objective value as <span class="math inline">\(\hat\beta\)</span> gives (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>).</p>
<p>On the other hand, assuming <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is optimal to (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>), implies that we must have all of the elements in <span class="math inline">\(\hat\alpha_\gamma\)</span> should be non-zero, otherwise a new <span class="math inline">\(\tilde\gamma_i \defeq I\left\{\hat\alpha_i \neq 0\right\}\)</span> will give a lower objective value of (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>). As a result, if we define <span class="math inline">\(\hat\beta_i \defeq \hat\gamma_i\hat\alpha_i\)</span>, <span class="math inline">\(\hat\beta\)</span> will be feasible to (<a href="#obj:l0" data-reference-type="ref" data-reference="obj:l0">[obj:l0]</a>) and gives the same objective value as <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> gives (<a href="#obj:vs" data-reference-type="ref" data-reference="obj:vs">[obj:vs]</a>).</p>
</section>
<section id="horseshoe-prior" class="level2" data-number="16.11">
<h2 data-number="16.11" class="anchored" data-anchor-id="horseshoe-prior"><span class="header-section-number">16.11</span> Horseshoe Prior</h2>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/horseshoe.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>The sparse normal means problem is concerned with inference for the parameter vector <span class="math inline">\(\theta = ( \theta_1 , \ldots , \theta_p )\)</span> where we observe data <span class="math inline">\(y_i = \theta_i + \epsilon_i\)</span> where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by <span class="math inline">\(\pi_{HS}\)</span>, (a.k.a. log-prior, <span class="math inline">\(- \log p_{HS}\)</span>)? Lasso simply uses an <span class="math inline">\(L^1\)</span>-norm, <span class="math inline">\(\sum_{i=1}^K | \theta_i |\)</span>, as opposed to the horseshoe prior which (essentially) uses the penalty <span class="math display">\[
\pi_{HS} ( \theta_i | \tau ) = - \log p_{HS} ( \theta_i | \tau ) = - \log \log \left ( 1 + \frac{2 \tau^2}{\theta_i^2} \right ) .
\]</span> The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in <strong>both</strong> the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.</p>
<p>From a historical perspective, James-Stein (a.k.a <span class="math inline">\(L^2\)</span>-regularisation)<span class="citation" data-cites="stein1964">(<a href="#ref-stein1964" role="doc-biblioref">Stein 1964</a>)</span> is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with <span class="math inline">\(L^2\)</span>-regularisation. Consider the sparse <span class="math inline">\(r\)</span>-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).</p>
<p>Let the true parameter value be <span class="math inline">\(\theta_p = \left ( \sqrt{d/p} , \ldots , \sqrt{d/p} , 0 , \ldots , 0 \right )\)</span>. James-Stein is equivalent to the model <span class="math display">\[
y_i = \theta_i + \epsilon_i \; \mathrm{ and} \; \theta_i \sim \mathcal{N} \left ( 0 , \tau^2 \right )
\]</span> This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage <span class="math inline">\(\hat{\tau}\)</span> is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of <span class="math inline">\(p(\tau^2|y)\)</span> is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective <span class="math inline">\(E \Vert \hat{\theta}^{JS} - \theta \Vert \leq p , \forall \theta\)</span> showing the inadmissibility of the MLE. At origin the risk is <span class="math inline">\(2\)</span>, <strong>but!</strong> <span class="math display">\[
\frac{p \Vert \theta \Vert^2}{p + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_p \right ) \leq
2 + \frac{p \Vert \theta \Vert^2}{ d + \Vert \theta \Vert^2}.
\]</span> This implies that <span class="math inline">\(R \left ( \hat{\theta}^{JS} , \theta_p \right ) \geq (p/2)\)</span>. Hence, simple thresholding rule beats James-Stein this with a risk given by <span class="math inline">\(\sqrt{\log p }\)</span>. This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
</section>
<section id="bayesian-inference" class="level2" data-number="16.12">
<h2 data-number="16.12" class="anchored" data-anchor-id="bayesian-inference"><span class="header-section-number">16.12</span> Bayesian Inference</h2>
<p>Consider a linear regression without bias term <span class="math display">\[
f(x) = x^Tw + \epsilon,~~~\epsilon \sim N(0,\sigma_e).
\]</span> We put a zero mean Gaussian prior on the model parameters <span class="math display">\[
\beta \sim N(0,\Sigma).
\]</span> Bayesian inference is to calculate posterior given the data <span class="math display">\[
p(w\mid y,X) = \dfrac{p(y\mid X,w)p(w)}{p(y\mid X)}.
\]</span> Product of two Gaussian density functions lead to another Gaussian <span class="math display">\[
\begin{aligned}
p(w\mid y,X)  &amp; \propto \exp\left(-\dfrac{1}{2\sigma_e^2}(y-\beta^TX)^T(y-\beta^TX)\right)\exp\left(-\dfrac{1}{2}\beta^T\Sigma^{-1}\beta\right)\\
&amp; \propto \exp\left(-\dfrac{1}{2}(\beta - \bar\beta)^T\left(\dfrac{1}{\sigma_e^2XX^T + \Sigma^{-1}}\right)(\beta-\bar\beta)\right)
\end{aligned}
\]</span></p>
<p>Thus, the posterior is <span class="math display">\[
\beta\mid X,y \sim N(\bar\beta,A^{-1}),
\]</span> where <span class="math inline">\(A = \left(\sigma_e^{-2}XX^T + \Sigma\right)\)</span>, and <span class="math inline">\(\bar\beta = \sigma_e^{-2}A^{-1}Xy\)</span>.</p>
</section>
<section id="posterior" class="level2 {#exm-posterior]" data-number="16.13">
<h2 data-number="16.13" class="anchored" data-anchor-id="posterior"><span class="header-section-number">16.13</span> Posterior</h2>
<p>Consider a model with <span class="math inline">\(p = 1\)</span> <span class="math display">\[
y = \beta_0 + \beta_1 x + \epsilon, ~~ \beta_i \sim N(0,1),~~~\sigma_e = 1
\]</span> Let’s plot a sample from the prior set of functions</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/bayes-lm-prior.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Sample from prior distribution over possible linear models</figcaption>
</figure>
</div>
<p>Now, say we observed two points <span class="math inline">\((1,1)\)</span> and <span class="math inline">\((2,2)\)</span>, we can calculate the posterior <span class="math inline">\(\beta \mid X,y \sim N(0.833, 0.166)\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/bayes-lm-post.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Sample from posterior distribution over possible linear models</figcaption>
</figure>
</div>
<p>Why our posterior mean is not 1?</p>
</section>
<section id="bayesian-model-selection" class="level2" data-number="16.14">
<h2 data-number="16.14" class="anchored" data-anchor-id="bayesian-model-selection"><span class="header-section-number">16.14</span> Bayesian Model Selection</h2>
<p>When analyzing data, we deal with three types of quantities</p>
<ol type="1">
<li><span class="math inline">\(X\)</span> = observed variables</li>
<li><span class="math inline">\(Y\)</span> = hidden variable</li>
<li><span class="math inline">\(\theta\)</span> = parameters of the model that describes the relation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
</ol>
<p>A probabilistic models of interest are the joint probability distribution <span class="math inline">\(p(D,\theta)\)</span> (called a generative model) and <span class="math inline">\(P(Y,\theta \mid X)\)</span> (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative model requires modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying a topic of an article can be solved using discriminative distribution. The problem of generating a new article requires generative model.</p>
<p>While performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: left;">Given</th>
<th style="text-align: left;">Hidden</th>
<th style="text-align: left;">What to find</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Training</td>
<td style="text-align: left;"><span class="math inline">\(D = (X,Y) = \{x_i,y_i\}_{i=1}^n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\theta\)</span></td>
<td style="text-align: left;"><span class="math inline">\(p(\theta \mid D)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Prediction</td>
<td style="text-align: left;"><span class="math inline">\(x_{\text{new}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(y_{\text{new}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(p(y_{\text{new}}  \mid  x_{\text{new}}, D)\)</span></td>
</tr>
</tbody>
</table>
<p>The training can be performed via the Bayes rule <span class="math display">\[
p(\theta \mid D) = \dfrac{p(Y \mid \theta,X)p(\theta)}{\int p(Y \mid \theta,X)p(\theta)d\theta}.
\]</span> Now to perform the second step (prediction), we calculate <span class="math display">\[
p(y_{\text{new}}  \mid  x_{\text{new}}, D) = \int p(y_{\text{new}}  \mid  x_{\text{new}},\theta)p(\theta \mid D)d\theta
\]</span> Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with <span class="math display">\[
p(\theta \mid D) \approx \delta(\theta_{\text{MAP}}),~~\theta_{\text{MAP}} \in \argmax_{\theta}p(\theta \mid D)
\]</span> To calculate <span class="math inline">\(\theta_{\text{MAP}}\)</span>, we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by <span class="math display">\[
p(y_{\text{new}}  \mid  x_{\text{new}}, D) = \int p(y_{\text{new}}  \mid  x_{\text{new}},\theta)p(\theta \mid D)d\theta \approx p(y_{\text{new}}  \mid  x_{\text{new}},\theta_{\text{MAP}}).
\]</span></p>
<p>Now we consider a case, when we have several candidate density functions for performing the prediction <span class="math display">\[
p_1(Y,\theta  \mid  X), ~~p_2(Y,\theta \mid X),\ldots
\]</span> How do we choose the better model? We can choose the model with highest evidence value (due to David MacKay) <span class="math display">\[
j = \argmax_j p_j(Y  \mid  X) = \argmax_j \int p_j(Y  \mid  X,\theta)p(\theta)d\theta.
\]</span> Note, formally instead of <span class="math inline">\(p(\theta)\)</span> we need to write <span class="math inline">\(p(\theta \mid X)\)</span>, however since <span class="math inline">\(\theta\)</span> does not depend on <span class="math inline">\(X\)</span> we omit it.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/svg/model-selection.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Model Selection</figcaption>
</figure>
</div>
<p>Can you think of how the prior <span class="math inline">\(p(\theta)\)</span>, posterior <span class="math inline">\(p(\theta \mid D)\)</span> and the evidence <span class="math inline">\(p(Y \mid X)\)</span> distributions will look like? Which model is the best? Which model will have the highest <span class="math inline">\(\theta_{\text{MAP}}\)</span>?</p>
<div id="exm-racial" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.6 (Racial discrimination)</strong></span> Say we want to analyze racial discrimination by the US courts. We have three variables:</p>
<ul>
<li>Murderer: <span class="math inline">\(m \in {0,1}\)</span> (black/white)</li>
<li>Victim: <span class="math inline">\(v \in \{0,1\}\)</span> (black/white)</li>
<li>Verdict: <span class="math inline">\(d \in \{0,1\}\)</span> (prison/death penalty)</li>
</ul>
<p>Say we have the data</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">m</th>
<th style="text-align: center;">v</th>
<th style="text-align: center;">d</th>
<th style="text-align: center;">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">132</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">52</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">97</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
</tr>
</tbody>
</table>
<p>We would like to establish a causal relations between the race and verdict variables. For this, we consider several models</p>
<ol type="1">
<li><p><span class="math inline">\(p(d \mid m,v) = p(d) = \theta\)</span></p></li>
<li><p><span class="math inline">\(p(d \mid m,v) = p(d \mid v)\)</span>; <span class="math inline">\(p(d \mid v=0) = \alpha, ~p(d \mid v=1)=\beta\)</span></p></li>
<li><p><span class="math inline">\(p(d \mid v,m) = p(d \mid m)\)</span>; <span class="math inline">\(p(d \mid m=1) = \gamma,~p(d \mid m=1) = \delta\)</span></p></li>
<li><p><span class="math inline">\(p(d|v,m)\)</span> cannot be reduced, and<br>
</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(p(d=1 \mid m,v)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(m=0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(m=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(v=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\tau\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\chi\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(v=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\nu\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\zeta\)</span></td>
</tr>
</tbody>
</table></li>
</ol>
<p>We calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model <span class="math display">\[
p(Y ,\theta \mid X) = p(Y \mid X,\theta)p(\theta \mid X)
\]</span> Here <span class="math inline">\(X\)</span> is the number of cases, and <span class="math inline">\(Y\)</span> is the number of death penalties. We use uninformative prior <span class="math inline">\(\theta \sim U[0,1]\)</span>. To specify the likelihood, we use Binomial distribution <span class="math display">\[
Y \mid X,\theta \sim B(X,\theta),~~B(Y \mid X,\theta) = C_Y^Xp^Y(1-\theta)^{X-Y}
\]</span> We assume <span class="math inline">\(p(\theta)\sim Uniform\)</span>. Now lets calculate the evidence <span class="math display">\[
p(Y, \theta \mid X) = \int p(Y  \mid  X,\theta)p(\theta)d\theta
\]</span> for each of the four models</p>
<ol type="1">
<li><span class="math inline">\(p(Y \mid X) = \int B(19 \mid 151,\theta)B(0 \mid 9,\theta)B(11 \mid 63,\theta)B(6 \mid 103,\theta)d\theta\)</span> <span class="math inline">\(\propto \int_0^{1} \theta^{36}(1-\theta)^{290}d\theta = B(37,291) = 2.8\times 10^{-51}\)</span></li>
<li><span class="math inline">\(p(Y \mid X) = \int\int B(19 \mid 151,\alpha)B(0 \mid 9,\beta)B(11 \mid 63,\alpha)B(6 \mid 103,\beta)d\alpha d\beta \propto 4.7\times 10^{-51}\)</span></li>
<li><span class="math inline">\(p(d \mid v,m) = p(d \mid m)=\int\int B(19 \mid 151,\gamma)B(0 \mid 9,\gamma)B(11 \mid 63,\delta)B(6 \mid 103,\delta)d\gamma d\delta \propto 0.27\times10^{-51}\)</span></li>
<li><span class="math inline">\(p(d \mid v,m) = \int\int\int\int B(19 \mid 151,\tau)B(0 \mid 9,\nu)B(11 \mid 63,\chi)B(6 \mid 103,\zeta)d\tau d\nu d\chi d\zeta \propto 0.18\times10^{-51}\)</span></li>
</ol>
<p>The last model is too complex, it can explain any relations in the data and this, has the lowest evidence score! However, if we are to use ML estimates, the fourth model will have the highest likelihood. Bayesian approach allows to avoid over-fitting! You can also see that this data set contains the Simpson’s paradox. Check it! A related problem is Bertrand’s gold box problem.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-andrews1974" class="csl-entry" role="listitem">
Andrews, D. F., and C. L. Mallows. 1974. <span>“Scale <span>Mixtures</span> of <span>Normal Distributions</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 36 (1): 99–102. <a href="https://www.jstor.org/stable/2984774">https://www.jstor.org/stable/2984774</a>.
</div>
<div id="ref-efron1977" class="csl-entry" role="listitem">
Efron, Bradley, and Carl Morris. 1977. <span>“Stein’s Paradox in Statistics.”</span> <em>Scientific American</em> 236 (5): 119–27.
</div>
<div id="ref-stein1964" class="csl-entry" role="listitem">
Stein, Charles. 1964. <span>“Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 16 (1): 155–60.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../qmd/rct.html" class="pagination-link  aria-label=" &lt;span="" field="" vs="" observational&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RCT: Field vs Observational</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../qmd/tree.html" class="pagination-link" aria-label="<span class='chapter-number'>17</span>&nbsp; <span class='chapter-title'>Tree Models</span>">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>