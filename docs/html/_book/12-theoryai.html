<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Theory of AI – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-glm.html" rel="next">
<link href="./11-pattern.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8f57c241cdbc1f937d718a8870719880.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./12-theoryai.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-theoryai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification: Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials (RCT): Field vs Observational</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Deep Learners</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Robotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./12-theoryai.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We use observed input-output pairs <span class="math inline">\((x_i,y_i)\)</span> to learn a function <span class="math inline">\(f\)</span> that maps <span class="math inline">\(x_i\)</span> to <span class="math inline">\(y_i\)</span>. The goal is to learn a function <span class="math inline">\(f\)</span> that generalizes well to unseen data. We can measure the quality of a function <span class="math inline">\(f\)</span> by its risk, which is the expected loss of <span class="math inline">\(f\)</span> on a new input-output pair <span class="math inline">\((x,y)\)</span>:<br>
<span class="math display">\[
R(f) = \sum_{i=1}^N \left [ l(y_i,f(x_i)) \right ] + \lambda \phi(f)
\]</span> where <span class="math inline">\(L\)</span> is a loss function, <span class="math inline">\(\phi\)</span> is a regularization function, and <span class="math inline">\(\lambda\)</span> is a regularization parameter. The loss function <span class="math inline">\(L\)</span> measures the difference between the output of the function <span class="math inline">\(f\)</span> and the true output <span class="math inline">\(y\)</span>. The regularization function <span class="math inline">\(\phi\)</span> measures the complexity of the function <span class="math inline">\(f\)</span>. The regularization parameter <span class="math inline">\(\lambda\)</span> controls the tradeoff between the loss and the complexity.</p>
<p>When <span class="math inline">\(y\in R\)</span> is numeric, we use the squared loss <span class="math inline">\(l(y,f(x)) = (f(x)-y)^2\)</span>. When <span class="math inline">\(y\in \{0,1\}\)</span> is binary, we use the logistic loss <span class="math inline">\(l(y,f(x)) = \log(1+\exp(-yf(x)))\)</span>.</p>
<p>Here are a few important considerations when building predictive models:</p>
<p><strong>1. Model Selection:</strong> Choosing the right model for the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is crucial and involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is too complex (e.g., high-degree polynomials or deep neural networks with insufficient data) might overfit the data by memorizing training examples rather than learning the underlying pattern, resulting in excellent training performance but poor generalization to unseen examples. This problem becomes even more complex when dealing with non-linear relationships, high-dimensional data, or noisy data, where the optimal model complexity is not immediately obvious and may require systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning to find the right balance between capturing the true signal while avoiding noise.</p>
<p><strong>2. Overfitting and Underfitting:</strong> Overfitting occurs when the model fits the training data too closely, capturing not only the true underlying relationship but also random noise and idiosyncrasies specific to the training dataset. This phenomenon typically manifests when a model has too many parameters relative to the amount of training data available, allowing it to essentially “memorize” the training examples rather than learning the generalizable patterns. The model may achieve excellent performance metrics on the training data (low training error) but will perform poorly on new, unseen data (high generalization error). This is because the model has learned to recognize specific noise patterns in the training data that don’t exist in the broader population. Common signs of overfitting include a large gap between training and validation/test performance, or performance that improves on training data while degrading on validation data during training iterations.</p>
<p>Underfitting occurs when the model is too simple and fails to capture the true relationship between x and y, often due to insufficient model complexity or inadequate training. This can happen when using a model that is inherently too simple for the problem at hand (e.g., linear regression for a highly non-linear relationship), when the model hasn’t been trained for enough iterations, or when regularization is applied too aggressively. Underfitting results in poor performance on both training and test data, as the model lacks the capacity to learn the underlying patterns in the data. The model essentially misses important features or relationships that are necessary for accurate predictions. Unlike overfitting, underfitting typically shows similar poor performance across training, validation, and test sets, indicating that the model is not capturing the signal in the data regardless of the dataset.</p>
<p><strong>3. Data Quality and Quantity:</strong> The accuracy of predictions heavily relies on the quality and quantity of the available data. If the data is noisy, inaccurate, or incomplete, it can lead to misleading predictions. A sufficient amount of data is also crucial to ensure the model can learn the underlying relationship effectively. Insufficient data can result in underfitting and poor generalization.</p>
<p>Data quality issues can manifest in various forms, including missing values, inconsistent formatting, labeling errors, and biased sampling. These problems are particularly acute in machine learning applications where large volumes of labeled data are required for training. To address these challenges, companies have emerged that specialize in data quality improvement and annotation services.</p>
<p>Companies like Scale AI and Toloka provide platforms that help organizations improve data quality through human-in-the-loop annotation and validation processes. These platforms employ large networks of human annotators who can perform tasks such as image labeling, text classification, data validation, and quality control. Scale AI, for example, offers services for creating high-quality training datasets through human annotation, with built-in quality control mechanisms that include multiple annotators per task and consensus-based validation. Their platform can handle various data types including images, text, and video, making it suitable for computer vision, natural language processing, and other AI applications.</p>
<p>Toloka, similarly, provides a crowdsourcing platform that connects businesses with a global network of contributors who can perform data labeling, content moderation, and quality assessment tasks. Their platform includes quality control features such as skill-based routing, where tasks are assigned to annotators based on their demonstrated expertise, and dynamic overlap, where multiple workers verify the same data to ensure accuracy.</p>
<p>These platforms help address several key data quality challenges: they can identify and correct labeling errors through consensus mechanisms, handle missing data through targeted collection efforts, and ensure consistency in data formatting and annotation standards. By leveraging human expertise at scale, these services enable organizations to create more reliable training datasets, which in turn leads to better-performing machine learning models and more accurate predictions.</p>
<p><strong>4. Model Explainability:</strong> In many applications, it is crucial to understand how the model arrives at its predictions. This is particularly important in areas like healthcare or finance, where transparency and interpretability are essential. Some models, particularly complex ones like deep neural networks, can be difficult to interpret, making it challenging to understand the rationale behind their predictions. However, modern machine learning has developed several techniques to address this challenge and make complex models more interpretable.</p>
<p>The importance of explainability extends beyond mere curiosity about model behavior. In healthcare applications, doctors need to understand why a model recommended a particular diagnosis or treatment plan to make informed decisions and maintain trust in the system. A model that predicts a patient has a 90% chance of having cancer but cannot explain which symptoms or test results led to this conclusion would be of limited clinical value. Similarly, in financial services, regulators require explanations for credit decisions to ensure compliance with fair lending laws and to prevent discriminatory practices. When a loan application is denied, both the applicant and regulatory bodies need to understand the specific factors that influenced this decision.</p>
<p>In legal and compliance contexts, explainability becomes a legal requirement. The European Union’s General Data Protection Regulation (GDPR) includes a “right to explanation” that allows individuals to request information about automated decisions that affect them. This has created a legal imperative for organizations to develop explainable AI systems. In criminal justice applications, where AI systems might be used for risk assessment or sentencing recommendations, the stakes are particularly high. Judges, lawyers, and defendants all need to understand how these systems arrive at their conclusions to ensure fair and just outcomes.</p>
<p>One prominent approach is the use of interpretable surrogate models, such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). These methods work by approximating the complex model’s behavior in the vicinity of a specific prediction using simpler, more interpretable models like linear regression or decision trees. LIME, for instance, creates local explanations by sampling points around the prediction of interest and fitting a linear model to explain the model’s behavior in that neighborhood. This allows us to understand which features contributed most to a particular prediction, even for complex models like deep neural networks.</p>
<p>Another powerful technique is attention mechanisms, which have become increasingly popular in natural language processing and computer vision. Attention mechanisms allow models to “focus” on specific parts of the input when making predictions, providing a form of built-in interpretability. For example, in image classification tasks, attention maps can highlight which regions of an image the model is focusing on when making its prediction, making it easier to understand the model’s decision-making process.</p>
<p>Gradient-based methods offer another approach to model interpretability. Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) use gradients to identify which parts of the input are most important for the model’s prediction. By computing the gradient of the model’s output with respect to the input features, these methods can create heatmaps that show which features or regions contributed most to the final prediction.</p>
<p>For tree-based models like random forests and gradient boosting machines, built-in feature importance measures provide natural interpretability. These methods can rank features based on their contribution to the model’s predictive performance, offering insights into which variables are most important for making predictions.</p>
<p>Model distillation techniques represent another approach, where a complex model (the teacher) is used to train a simpler, more interpretable model (the student) that mimics the teacher’s behavior. The student model, being simpler, is easier to interpret while maintaining much of the teacher’s predictive performance.</p>
<p>Finally, counterfactual explanations provide a different type of interpretability by showing what changes to the input would be needed to change the model’s prediction. For example, if a loan application is rejected, a counterfactual explanation might show that the application would have been approved if the applicant’s income were $10,000 higher or if their credit score were 50 points better.</p>
<p>These modern interpretability techniques have made it possible to understand and explain the behavior of even the most complex models, addressing the “black box” problem that has historically limited the adoption of advanced machine learning methods in critical applications where transparency is essential.</p>
<p><strong>5. Computational Cost:</strong> Training and using prediction models can be computationally expensive, especially for complex models with large datasets. This can limit their applicability in resource-constrained environments. Finding a balance between model complexity, accuracy, and computational cost is critical for practical applications.</p>
<p>The computational demands of machine learning models have been significantly addressed through the development of specialized hardware, particularly Graphics Processing Units (GPUs). Originally designed for rendering graphics in video games, GPUs have become essential for deep learning due to their parallel processing architecture. Unlike traditional Central Processing Units (CPUs) that process tasks sequentially, GPUs can perform thousands of mathematical operations simultaneously, making them ideal for the matrix multiplications and tensor operations that are fundamental to neural network training. This parallel processing capability has reduced training times from weeks to hours or even minutes for many deep learning models, democratizing access to advanced machine learning techniques.</p>
<p>However, the computational cost challenge extends beyond just training to the deployment phase, where models need to run efficiently in production environments. This has led to the emergence of edge computing as a crucial solution. Edge computing involves processing data and running models closer to where the data is generated, rather than sending everything to centralized cloud servers. This approach offers several advantages for machine learning applications: reduced latency for real-time predictions, lower bandwidth costs by processing data locally, and improved privacy by keeping sensitive data on local devices.</p>
<p>Edge computing is particularly important for applications requiring real-time decision making, such as autonomous vehicles, industrial IoT systems, and mobile applications. For example, a self-driving car cannot afford the latency of sending sensor data to a cloud server and waiting for predictions to return; it needs to process information and make decisions locally within milliseconds. Similarly, smart manufacturing systems use edge computing to monitor equipment and predict maintenance needs in real-time without the delays associated with cloud processing.</p>
<p>Quantization and lower precision calculations have emerged as powerful techniques for reducing computational costs while maintaining model performance. Traditional neural networks use 32-bit floating-point numbers (FP32) for all calculations, which provides high precision but requires significant computational resources and memory. Quantization reduces the precision of these numbers, typically to 16-bit (FP16), 8-bit integers (INT8), or even 4-bit integers (INT4), dramatically reducing both memory usage and computational requirements. For example, converting from FP32 to INT8 can reduce memory usage by 75% and computational cost by 2-4x, while often maintaining acceptable accuracy levels. This is particularly valuable for deployment on edge devices with limited resources, such as smartphones, IoT devices, and embedded systems. Modern hardware, including specialized AI accelerators like Google’s Tensor Processing Units (TPUs) and NVIDIA’s Tensor Cores, are specifically designed to handle these lower precision calculations efficiently, further reducing the computational cost barrier.</p>
<p>The trade-offs between computational cost and model performance are becoming increasingly sophisticated. Techniques like model pruning, which removes unnecessary connections from neural networks, can create smaller, faster models. Knowledge distillation allows large, complex models to transfer their knowledge to smaller, more efficient models that can run on resource-constrained devices.</p>
<p>These developments have created a spectrum of deployment options, from powerful cloud-based systems that can run the most complex models to lightweight edge devices that can perform basic predictions locally. The choice depends on the specific requirements of the application, including latency requirements, accuracy needs, privacy concerns, and cost constraints. As hardware continues to improve and optimization techniques become more sophisticated, the computational cost barrier to deploying machine learning models continues to decrease, opening up new possibilities for AI applications in previously inaccessible domains.</p>
<p><strong>6. Ethical Considerations:</strong> Predictions can have significant real-world consequences, raising ethical concerns about bias, fairness, and potential misuse. It is crucial to consider the potential harms and unintended consequences of predictions and implement safeguards to mitigate them.</p>
<p>The ethical implications of predictive models have become increasingly prominent as these systems are deployed in critical domains such as healthcare, criminal justice, employment, and financial services. One of the most significant concerns is algorithmic bias, which can perpetuate or amplify existing societal inequalities. For example, facial recognition systems have been shown to have higher error rates for people of color, potentially leading to wrongful arrests or surveillance. Similarly, hiring algorithms trained on historical data may perpetuate gender or racial biases present in past hiring decisions, creating a feedback loop that reinforces discrimination.</p>
<p>Fairness in machine learning has emerged as a critical research area, with multiple definitions and approaches to ensure equitable treatment across different demographic groups. Statistical parity, equalized odds, and individual fairness are among the various fairness metrics that can be applied depending on the specific context and requirements of the application. However, achieving fairness often involves trade-offs with model accuracy, and different fairness definitions may conflict with each other, requiring careful consideration of which definition is most appropriate for a given use case.</p>
<p>The potential for misuse of predictive models is another significant concern. Models designed for legitimate purposes can be repurposed for harmful applications, such as using facial recognition for mass surveillance or employing predictive policing algorithms that disproportionately target certain communities. Additionally, the increasing sophistication of deepfake technology, which uses predictive models to generate realistic but fake images, videos, or audio, raises concerns about misinformation and manipulation.</p>
<p>Privacy concerns arise when predictive models require access to sensitive personal data. The collection, storage, and processing of personal information for training and deploying these models can violate individual privacy rights and create risks of data breaches. Differential privacy techniques, which add carefully calibrated noise to data or model outputs, have emerged as a promising approach to protect individual privacy while maintaining model utility.</p>
<p>Transparency and accountability are essential for addressing ethical concerns. Organizations deploying predictive models must be able to explain their decisions and be held accountable for any harms that result. This includes maintaining audit trails, implementing human oversight mechanisms, and establishing clear procedures for addressing complaints or errors. The concept of “algorithmic impact assessments” has been proposed as a framework for evaluating the potential social impacts of automated decision-making systems before deployment.</p>
<p>Regulatory frameworks are evolving to address these ethical challenges. The European Union’s General Data Protection Regulation (GDPR) includes provisions for automated decision-making and profiling, while various jurisdictions are developing specific regulations for AI systems. These regulations often require transparency, human oversight, and the ability to contest automated decisions.</p>
<p>Technical approaches to addressing ethical concerns include adversarial training to reduce bias, interpretability techniques to increase transparency, and robust testing procedures to identify potential harms before deployment. Regular monitoring and updating of deployed models is also crucial, as societal norms and legal requirements evolve over time.</p>
<p>Addressing these challenges requires careful consideration of the specific problem, selection of appropriate techniques, and continuous evaluation and improvement of the prediction model. It also requires collaboration between technical experts, domain specialists, ethicists, and stakeholders to ensure that predictive models serve the public good while minimizing potential harms.</p>
<section id="prediction-accuracy" class="level3" data-number="12.0.1">
<h3 data-number="12.0.1" class="anchored" data-anchor-id="prediction-accuracy"><span class="header-section-number">12.0.1</span> Prediction Accuracy</h3>
<p>After we fit our model and find the optimal value of the parameter <span class="math inline">\(\theta\)</span>, denoted by <span class="math inline">\(\hat \theta\)</span>, we need to evaluating the accuracy of a predictive model. It involves comparing the model’s predictions to actual outcomes. We can simply use the value of the loss function from the training step to evaluate model’s predictive power. However, this only tells us how well the model fits the training data. It doesn’t tell us how well the model will perform on unseen data. To evaluate the model’s performance on unseen data, we need to use a different approach.</p>
<p>The most common approach is to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate its performance. This approach is known as the train-test split. It is a simple and effective way to evaluate how well model predicts unseen for unseen inputs.</p>
<p>Another approach is to use cross-validation. It involves splitting the data into smaller subsets and using them to train and test the model multiple times. When our sample size is small, this allows for a more robust estimate of the model’s performance than simply splitting the data into a single training and test set. For small data sets, simple train-test split approach will be sensitive to choice of test samples, thus the estimated predicted performance will be unstable (high variance). Cross-validation helps to reduce this variance by averaging the performance across multiple folds. This makes the performance estimate more robust and less sensitive to the choice of test samples.</p>
<p>Cross-validation involves several steps:</p>
<ol type="1">
<li>Split the data: The data is randomly divided into <span class="math inline">\(k\)</span> equal-sized chunks (folds).</li>
<li>Train and test the model: For each fold, the model is trained on <span class="math inline">\(k-1\)</span> folds and tested on the remaining fold. This process is repeated <span class="math inline">\(k\)</span> times, ensuring each fold is used for testing once.</li>
<li>Evaluate the model: The performance of the model is evaluated on each fold using a chosen metric, such as accuracy, precision, recall, or F1 score.</li>
<li>Report the average performance: The average of the performance metrics across all k folds is reported as the final estimate of the model’s performance.</li>
</ol>
<p>A common choice for <span class="math inline">\(k\)</span> is 5 or 10. When <span class="math inline">\(K=n\)</span>, this is known as leave-one-out cross-validation. This method can be computationally expensive but is less likely to overfit the data. Stratified cross-validation ensures that each fold contains approximately the same proportion of each class as in the entire dataset. This is important for imbalanced datasets where one class is significantly larger than the others.</p>
<p>Notice, that cross-validation requires re-training the model multiple times, which can be computationally expensive. Thus, for large datasets, we typically prefer simple train-test split. However, for small datasets, cross-validation can provide a more robust estimate of the model’s performance.</p>
<p>Either method is limited to evaluating the model’s performance on data that is available to the modeler. What if we start using our model on data that is different from the training and test sets? Unlike in physics, when a model represents a law that is universal, in data science, we are dealing with data that is generated by a process that is not necessarily universal. For example, if we are building a model to predict the price of a house, we can train and test the model on data from a specific city. However, if we start using the model to predict the price of a house in a different city, the model might not perform as well. This is because the data from the new city might be different from the data used to train and test the model. This is known as the problem of generalization. It refers to the ability of a model to perform well on data that is different from the training and test sets.</p>
<section id="evaluation-metrics-for-regression" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics-for-regression">Evaluation Metrics for Regression</h4>
<p>There are several metrics that can be used to evaluate the performance of regression models. We can simply use the same function as we use for fitting the model, e.g.&nbsp;least squares <span class="math display">\[
\text{MSE} = \dfrac{1}{m}\sum_{i=1}^n (y_i -\hat y_i)^2,
\]</span> here <span class="math inline">\(\hat y_i\)</span> is the predicted value of the i-th data point by the model <span class="math inline">\(\hat y_i = f(x_i,\hat\theta)\)</span> and <span class="math inline">\(m\)</span> is the total number of data points used for the evaluation. This metric is called the <em>Mean Squared Error (MSE)</em>. It is the average squared difference between the actual and predicted values. Lower MSE indicates better model performance, as it means the model’s predictions are closer to the actual values.</p>
<p>A slight variation of this metric is Root Mean Squared Error (RMSE). This is the square root of MSE and is also commonly used due to its units being the same as the target variable. <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}}.
\]</span> However, MSE is sensitive to outliers, as it squares the errors, giving more weight to large errors. This can lead to misleading results when the data contains outliers.</p>
<p><em>Median Absolute Error (MAE)</em> solves the sensetivity to the outliers problem. It is the median of the absolute errors, providing a more robust measure than MAE for skewed error distributions <span class="math display">\[
\text{MAE} = \dfrac{1}{m}\sum_{i=1}^n |y_i -\hat y_i|.
\]</span> A variation of it is the <em>Mean Absolute Percentage Error (MAPE)</em>, which is the mean of the absolute percentage errors <span class="math display">\[
\text{MAPE} = \dfrac{1}{m}\sum_{i=1}^n \left | \dfrac{y_i -\hat y_i}{y_i} \right |.
\]</span></p>
<p>Alternative way to measure the predictive quility is to use the coefficient of determination, also known as the <em>R-squared</em> value, which measures the proportion of variance in the target variable that is explained by the model. Higher R-squared indicates better fit. However, R-squared can be misleading when comparing models with different numbers of features. R-squared is defined as follows <span class="math display">\[
R^2 = 1 - \dfrac{\sum_{i=1}^n (y_i -\hat y_i)^2}{\sum_{i=1}^n (y_i -\bar y_i)^2},
\]</span> where <span class="math inline">\(\bar y_i\)</span> is the mean of the target variable. R-squared is a relative measure of fit, so it can be used to compare different models. However, it is not an absolute measure of fit, so it cannot be used to determine whether a model is good or bad. It is also sensitive to the number of features in the model, so it cannot be used to compare models with different numbers of features.</p>
<p>Finally, we can use graphics to evaluate the model’s performance. For example, we can scatterplot the actual and predicted values of the target variable to visually compare them. We can also plot the histogram of a boxplot of the residuals (errors) to see if they are normally distributed.</p>
</section>
<section id="evaluation-metrics-for-classification" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics-for-classification">Evaluation Metrics for Classification</h4>
<p><em>Accuracy</em> is the most fundamental metric used to evaluate models. It is defined as the ratio of the number of correct predictions to the total number of predictions. The formula is given by <span class="math display">\[\text{Accuracy} = \frac{\text{TP+TN}}{\text{TP+TN+FP+FN}},\]</span> where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. However, it can be misleading for imbalanced datasets where one class is significantly larger than others. For example, if 95% of the data belongs to one class, a model that always predicts this class will be 95% accurate, even though it’s not very useful.</p>
<p>A more comprehensive understanding of model performance can be achieved by calculaitng the sensitivity (a.k.a precision) and specificity (a.k.a. recall) as well as confusion matrix discussed in <a href="02-bayes.html#sec-Sensitivity" class="quarto-xref"><span>Section 2.4</span></a>. The confusion matrix is</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Actual/Predicted</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Negative</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p><strong>Precision</strong> measures the proportion of positive predictions that are actually positive. It is useful for evaluating how good the model is at identifying true positives. <strong>Recall</strong> measures the proportion of actual positives that are correctly identified by the model. It is useful for evaluating how good the model is at not missing true positives.</p>
<p>Then we can use those to calculate <strong>F1 Score</strong> which is is a harmonic mean of precision and recall, providing a balanced view of both metrics. Higher F1 score indicates better overall performance. If misclassifying certain instances is more costly than others, weighted metrics account for these different costs. For imbalanced datasets, metrics like F1 score or balanced accuracy are important to avoid misleading interpretations.</p>
<p>Sometimes, we use multiple metrics to get a comprehensive assessment of the model’s performance. Additionally, consider comparing the model’s performance to a baseline model or other existing models for the same task. Sometimes, it is hard to beat a “coin flip” classification model, when the model predicts the class randomly with equal probability. In regression, a simple baseline model is <span class="math inline">\(f(x_i) = \bar y\)</span>, which is the mean of the target variable.</p>
</section>
</section>
<section id="some-examples-of-prediction-problems" class="level3" data-number="12.0.2">
<h3 data-number="12.0.2" class="anchored" data-anchor-id="some-examples-of-prediction-problems"><span class="header-section-number">12.0.2</span> Some Examples of Prediction Problems</h3>
<div id="exm-election" class="theorem example">
<p><span class="theorem-title"><strong>Example 12.1 (Obama Elections)</strong></span> Elections 2012: Bayes and Nate Silver</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plyr)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: "http://www.electoral-vote.com/evp2012/Pres/pres_polls.csv"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>election<span class="fl">.2012</span> <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/pres_polls.csv"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove a pollster: elect2012 &lt;- election.2012[!grepl('Rasmussen', election.2012$Pollster),]</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> election<span class="fl">.2012</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Aggregrate the data</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), subset, Day <span class="sc">==</span> <span class="fu">max</span>(Day))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), summarise, <span class="at">R.pct =</span> <span class="fu">mean</span>(GOP), <span class="at">O.pct =</span> <span class="fu">mean</span>(Dem), <span class="at">EV =</span> <span class="fu">mean</span>(EV))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(elect2012[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>,], <span class="at">caption =</span> <span class="st">"Election 2012 Data"</span>,<span class="at">longtable=</span><span class="cn">TRUE</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(elect2012[<span class="dv">26</span><span class="sc">:</span><span class="dv">51</span>,], <span class="at">caption =</span> <span class="st">"Election 2012 Data"</span>,<span class="at">longtable=</span><span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top table table-sm table-striped small">
<caption>Election 2012 Data</caption>
<thead>
<tr class="header">
<th style="text-align: left;">state</th>
<th style="text-align: right;">R.pct</th>
<th style="text-align: right;">O.pct</th>
<th style="text-align: right;">EV</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Alabama</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alaska</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Arizona</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">Arkansas</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">California</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">55</td>
</tr>
<tr class="even">
<td style="text-align: left;">Colorado</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Connecticut</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">D.C.</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Delaware</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">Florida</td>
<td style="text-align: right;">49</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Georgia</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hawaii</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">71</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Idaho</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Illinois</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Indiana</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">Iowa</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kansas</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Kentucky</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Louisiana</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">Maine</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maryland</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">Massachusetts</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Michigan</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: left;">Minnesota</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mississippi</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top table table-sm table-striped small">
<caption>Election 2012 Data</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">state</th>
<th style="text-align: right;">R.pct</th>
<th style="text-align: right;">O.pct</th>
<th style="text-align: right;">EV</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">26</td>
<td style="text-align: left;">Missouri</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">27</td>
<td style="text-align: left;">Montana</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">28</td>
<td style="text-align: left;">Nebraska</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">29</td>
<td style="text-align: left;">Nevada</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">30</td>
<td style="text-align: left;">New Hampshire</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">31</td>
<td style="text-align: left;">New Jersey</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">32</td>
<td style="text-align: left;">New Mexico</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">33</td>
<td style="text-align: left;">New York</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">34</td>
<td style="text-align: left;">North Carolina</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">15</td>
</tr>
<tr class="even">
<td style="text-align: left;">35</td>
<td style="text-align: left;">North Dakota</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">36</td>
<td style="text-align: left;">Ohio</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">18</td>
</tr>
<tr class="even">
<td style="text-align: left;">37</td>
<td style="text-align: left;">Oklahoma</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">38</td>
<td style="text-align: left;">Oregon</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">39</td>
<td style="text-align: left;">Pennsylvania</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">40</td>
<td style="text-align: left;">Rhode Island</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">41</td>
<td style="text-align: left;">South Carolina</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">42</td>
<td style="text-align: left;">South Dakota</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">43</td>
<td style="text-align: left;">Tennessee</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">44</td>
<td style="text-align: left;">Texas</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">38</td>
</tr>
<tr class="even">
<td style="text-align: left;">45</td>
<td style="text-align: left;">Utah</td>
<td style="text-align: right;">73</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">46</td>
<td style="text-align: left;">Vermont</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">47</td>
<td style="text-align: left;">Virginia</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">13</td>
</tr>
<tr class="odd">
<td style="text-align: left;">48</td>
<td style="text-align: left;">Washington</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="even">
<td style="text-align: left;">49</td>
<td style="text-align: left;">West Virginia</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">50</td>
<td style="text-align: left;">Wisconsin</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">51</td>
<td style="text-align: left;">Wyoming</td>
<td style="text-align: right;">69</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Run the Simulation and plot probabilities by state</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>prob.Obama <span class="ot">&lt;-</span> <span class="cf">function</span>(mydata) {</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="fu">rdirichlet</span>(<span class="dv">1000</span>, <span class="dv">500</span> <span class="sc">*</span> <span class="fu">c</span>(mydata<span class="sc">$</span>R.pct, mydata<span class="sc">$</span>O.pct, <span class="dv">100</span> <span class="sc">-</span> mydata<span class="sc">$</span>R.pct <span class="sc">-</span> </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        mydata<span class="sc">$</span>O.pct)<span class="sc">/</span><span class="dv">100</span> <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mean</span>(p[, <span class="dv">2</span>] <span class="sc">&gt;</span> p[, <span class="dv">1</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>win.probs <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), prob.Obama)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>win.probs<span class="sc">$</span>Romney <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> win.probs<span class="sc">$</span>V1</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(win.probs)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">"Obama"</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>win.probs<span class="sc">$</span>EV <span class="ot">&lt;-</span> elect2012<span class="sc">$</span>EV</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>win.probs <span class="ot">&lt;-</span> win.probs[<span class="fu">order</span>(win.probs<span class="sc">$</span>EV), ]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(win.probs) <span class="ot">&lt;-</span> win.probs<span class="sc">$</span>state</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(usmap)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_usmap</span>(<span class="at">data =</span> win.probs, <span class="at">values =</span> <span class="st">"Obama"</span>) <span class="sc">+</span> </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_continuous</span>(<span class="at">low =</span> <span class="st">"red"</span>, <span class="at">high =</span> <span class="st">"blue"</span>, <span class="at">name =</span> <span class="st">"Obama Win Probability"</span>, <span class="at">label =</span> scales<span class="sc">::</span>comma) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"right"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/obama-map-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Probabilities of Obama winning by state</figcaption>
</figure>
</div>
</div>
</div>
<p>We use those probabilities to simulate the probability of Obama winning the election. First, we calculate the probability of Obama having &gt;270 EV or more</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>sim.election <span class="ot">&lt;-</span> <span class="cf">function</span>(win.probs) {</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    winner <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">51</span>, <span class="dv">1</span>, win.probs<span class="sc">$</span>Obama)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(win.probs<span class="sc">$</span>EV <span class="sc">*</span> winner)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sim.EV <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">sim.election</span>(win.probs))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>oprob <span class="ot">&lt;-</span> <span class="fu">sum</span>(sim.EV <span class="sc">&gt;=</span> <span class="dv">270</span>)<span class="sc">/</span><span class="fu">length</span>(sim.EV)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>oprob</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.9654</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Lattice Graph</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">densityplot</span>(sim.EV, <span class="at">plot.points =</span> <span class="st">"rug"</span>, <span class="at">xlab =</span> <span class="st">"Electoral Votes for Obama"</span>, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">panel =</span> <span class="cf">function</span>(x, ...) {</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="fu">panel.densityplot</span>(x, ...)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">panel.abline</span>(<span class="at">v =</span> <span class="dv">270</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="fu">panel.text</span>(<span class="at">x =</span> <span class="dv">285</span>, <span class="at">y =</span> <span class="fl">0.01</span>, <span class="st">"270 EV to Win"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">panel.abline</span>(<span class="at">v =</span> <span class="dv">332</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="fu">panel.text</span>(<span class="at">x =</span> <span class="dv">347</span>, <span class="at">y =</span> <span class="fl">0.01</span>, <span class="st">"Actual Obama"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>}, <span class="at">main =</span> <span class="st">"Electoral College Results Probability"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Results of recent state polls in the 2008 United States Presidential Election between Barack Obama and John McCain.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: LearnBayes library</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>election<span class="fl">.2008</span> <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/election2008.csv"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(election<span class="fl">.2008</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(election<span class="fl">.2008</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  Dirichlet simulation</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>prob.Obama <span class="ot">=</span> <span class="cf">function</span>(j)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a> {</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a> p<span class="ot">=</span><span class="fu">rdirichlet</span>(<span class="dv">5000</span>,<span class="dv">500</span><span class="sc">*</span><span class="fu">c</span>(M.pct[j],O.pct[j],<span class="dv">100</span><span class="sc">-</span>M.pct[j]<span class="sc">-</span>O.pct[j])<span class="sc">/</span><span class="dv">100</span><span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">mean</span>(p[,<span class="dv">2</span>]<span class="sc">&gt;</span>p[,<span class="dv">1</span>])</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a> }</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sapply function to compute Obama win prob for all states</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>Obama.win.probs<span class="ot">=</span><span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">51</span>,prob.Obama)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="do">##  sim.EV function</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>sim.election <span class="ot">=</span> <span class="cf">function</span>()</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a> {</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a> winner <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">51</span>,<span class="dv">1</span>,Obama.win.probs)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a> <span class="fu">sum</span>(EV<span class="sc">*</span>winner)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a> }</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>sim.EV <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>,<span class="fu">sim.election</span>())</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="do">## histogram of simulated election</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(sim.EV,<span class="fu">min</span>(sim.EV)<span class="sc">:</span><span class="fu">max</span>(sim.EV),<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">prob=</span>T)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">365</span>,<span class="at">lwd=</span><span class="dv">3</span>)   <span class="co"># Obama received 365 votes</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">375</span>,<span class="dv">30</span>,<span class="st">"Actual </span><span class="sc">\n</span><span class="st"> Obama </span><span class="sc">\n</span><span class="st"> total"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="bayesain-model-selection-via-regularisation" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="bayesain-model-selection-via-regularisation"><span class="header-section-number">12.1</span> Bayesain Model Selection via Regularisation</h2>
<p>From Bayesian perspective regularization is nothing but incorporation of prior information into the model. Remember, that a Bayesian model is specified by likelihood and prior distributions. Bayesian regularization methods include the Bayesian bridge, horseshoe regularization, Bayesian lasso, Bayesian elastic net, spike-and-slab lasso, and global-local shrinkage priors. Bayesian <span class="math inline">\(\ell_0\)</span> regularization is an attractive solution for high dimensional variable selection as it directly penalizes the number of predictors. The caveat is the need to search over all possible model combinations, as a full solution requires enumeration over all possible models which is NP-hard.</p>
<p>In Bayesian approach, regularization requires the specification of a loss, denoted by <span class="math inline">\(l\left(\beta\right)\)</span> and a penalty function, denoted by <span class="math inline">\(\phi_{\lambda}(\beta)\)</span>, where <span class="math inline">\(\lambda\)</span> is a global regularization parameter. From a Bayesian perspective, <span class="math inline">\(l\left(\beta\right)\)</span> and <span class="math inline">\(\phi_{\lambda}(\beta)\)</span> correspond to the negative logarithms of the likelihood and prior distribution, respectively. Regularization leads to an maximum a posteriori (MAP) optimization problem of the form <span class="math display">\[
\underset{\beta \in R^p}{\mathrm{minimize}\quad}
l\left(\beta\right) + \phi_{\lambda}(\beta) \; .
\]</span> Taking a probabilistic approach leads to a Bayesian hierarchical model <span class="math display">\[
p(y \mid \beta) \propto \exp\{-l(\beta)\} \; , \quad p(\beta) \propto \exp\{ -\phi_{\lambda}(\beta) \} \ .
\]</span> The solution to the minimization problem estimated by regularization corresponds to the posterior mode, <span class="math inline">\(\hat{\beta} = \mathrm{ arg \; max}_\beta \; p( \beta|y)\)</span>, where <span class="math inline">\(p(\beta|y)\)</span> denotes the posterior distribution. Consider a normal mean problem with <span class="math display">\[
\label{eqn:linreg}
y = \theta+ e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \theta \le \infty \ .
\]</span> What prior <span class="math inline">\(p(\theta)\)</span> should we place on <span class="math inline">\(\theta\)</span> to be able to separate the “signal” <span class="math inline">\(\theta\)</span> from “noise” <span class="math inline">\(e\)</span>, when we know that there is a good chance that <span class="math inline">\(\theta\)</span> is sparse (i.e.&nbsp;equal to zero). In the multivariate case we have <span class="math inline">\(y_i = \theta_i + e_i\)</span> and sparseness is measured by the number of zeros in <span class="math inline">\(\theta = (\theta_1\ldots,\theta_p)\)</span>. The Bayesan Lasso assumes double exponential (a.k.a Laplace) prior distribution where <span class="math display">\[
p(\theta_i \mid b) = 0.5b\exp(-|\theta|/b).
\]</span> We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior <span class="math display">\[
\log p(\theta \mid y, b) \propto ||y-\theta||_2^2 + \dfrac{2\sigma^2}{b}||\theta||_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span> the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to the small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
<p>There is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model <span class="math inline">\(f\)</span>. The regularization parameter <span class="math inline">\(\lambda\)</span> is related to the variance of the prior distribution. When <span class="math inline">\(\lambda=0\)</span>, the function <span class="math inline">\(f\)</span> is the maximum likelihood estimate of the parameters. When <span class="math inline">\(\lambda\)</span> is large, the function <span class="math inline">\(f\)</span> is the prior mean of the parameters. When <span class="math inline">\(\lambda\)</span> is infinite, the function <span class="math inline">\(f\)</span> is the prior mode of the parameters. When <span class="math inline">\(\lambda\)</span> is negative, the function <span class="math inline">\(f\)</span> is the posterior mean of the parameters. When <span class="math inline">\(\lambda\)</span> is very negative, the function <span class="math inline">\(f\)</span> is the posterior mode of the parameters.</p>
<p>The goal is to find a function <span class="math inline">\(f\)</span> that minimizes the risk <span class="math inline">\(R(f)\)</span>. This is called the <strong>empirical risk minimization</strong> problem. Finding minimum is a difficult problem when the risk function <span class="math inline">\(R(f)\)</span> is non-convex. In practice, we often use gradient descent to find a local minimum of the risk function <span class="math inline">\(R(f)\)</span>.</p>
<p>There are multiple ways to choose the penalty term <span class="math inline">\(\phi(f)\)</span>. Sections below describe the most popular approaches.</p>
</section>
<section id="shrinkage-ell_2-norm" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="shrinkage-ell_2-norm"><span class="header-section-number">12.2</span> Shrinkage (<span class="math inline">\(\ell_2\)</span> Norm)</h2>
<p>We can estimate the risk bounds of <span class="math inline">\(\ell_2\)</span> Norm regularisation. The classic bias-variance tradeoff is given by the MSE risk bound. <span class="math display">\[
R(\theta,\hat \theta) = E_{y|\theta} \left [ \Vert \hat \theta - \theta \Vert^2 \right ] = \Vert \hat \theta - \theta \Vert^2 + E_{y|\theta} \left [ \Vert \hat \theta - \mathbb{E}(\hat \theta) \Vert^2 \right ]
\]</span></p>
<p>In a case of multiple parameters, the Stein bound is <span class="math display">\[
R(\theta,\hat \theta_{JS}) &lt; R(\theta,\hat \theta_{MLE}) \;\;\; \forall \theta \in \mathbb{R}^p, \;\;\; p \geq 3.
\]</span> In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with <span class="math inline">\(p=100\)</span> and <span class="math inline">\(n=100\)</span>, the risk of the MLE is <span class="math inline">\(R(\theta,\hat \theta_{MLE}) = 100\)</span> while the risk of the JS estimator is <span class="math inline">\(R(\theta,\hat \theta_{JS}) = 1.5\)</span>. The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.</p>
<p>JS is a specific estimate and it motivates the ridge estimator. The ridge estimator is a shrinkage estimator with the penalty function being the squared norm of the parameter vector. The ridge estimator is <span class="math display">\[
\hat \theta_{ridge} = \left (  X^T X + \lambda I \right )^{-1} X^T y
\]</span> where <span class="math inline">\(\lambda\)</span> is the regularization parameter.</p>
</section>
<section id="sparsity-ell_1-norm" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="sparsity-ell_1-norm"><span class="header-section-number">12.3</span> Sparsity (<span class="math inline">\(\ell_1\)</span> Norm)</h2>
<p>High-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks. There is a vast and growing literator with lasso (leat absolute selection) being the gold standard. Horseshoe priors are the Bayesian counterpart. Sparsity relies on the property of a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack. Suppose that we observe data from Normal means model <span class="math display">\[
y_i \mid \theta_i \sim N(\theta_i,1),~ 1\le i\le p, ~ \theta = (\theta_1,\ldots,\theta_p),
\]</span> where parameter <span class="math inline">\(\theta\)</span> lies in the ball <span class="math display">\[
||\theta||_{\ell_0} = \{\theta : \text{number of  }\theta_i \ne 0 \le p_n\}.
\]</span></p>
<p>Even threshholding can beat MLE, when the signal is sparse. The thresholding estimator is <span class="math display">\[
\hat \theta_{thr} = \left \{ \begin{array}{ll} \hat \theta_i &amp; \mbox{if} \; \hat \theta_i &gt; \sqrt{2 \ln p} \\ 0 &amp; \mbox{otherwise} \end{array} \right .
\]</span></p>
<p>Sparse signal detection provides a challenge to statistical methodology; consider the classical normal means inference problem. Suppose that we observe data from the probability model <span class="math inline">\(( y_i | \theta_i ) \sim N( \theta_i,1)\)</span>. We wish to provide an estimator <span class="math inline">\(\hat y_{hs}\)</span> for the vector of normal means <span class="math inline">\(\theta = ( \theta_1, \ldots , \theta_p )\)</span>. Sparsity occurs when a large portion of the parameter vector contains zeroes. The ’’ultra-sparse`` or “nearly black” vector case occurs when <span class="math inline">\(p_n\)</span>, denoting the number of non-zero parameter values, and for <span class="math inline">\(\theta \in l_0 [ p_n]\)</span>, which denotes the set <span class="math inline">\(\# ( \theta_i \neq 0 ) \leq p_n\)</span> where <span class="math inline">\(p_n = o(n)\)</span> where <span class="math inline">\(p_n \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The classic James-Stein shrinkage rule, <span class="math inline">\(\hat y_{js}\)</span>, uniformly dominates the traditional sample mean estimator, <span class="math inline">\(\hat{\theta}\)</span>, for all values of the true parameter <span class="math inline">\(\theta\)</span>. In classical MSE risk terms: <span class="math display">\[
R(\hat y_{js}, \theta) \defeq E_{y|\theta} {\Vert \hat y_{js} - \theta \Vert}^2 &lt; p
    = E_{y|\theta} {\Vert y - \theta \Vert}^2, \;\;\; \forall \theta
\]</span> For a sparse signal, however, <span class="math inline">\(\hat y_{js}\)</span> performs poorly when the true parameter is an <span class="math inline">\(r\)</span>-spike where <span class="math inline">\(\theta_r\)</span> has <span class="math inline">\(r\)</span> coordinates at <span class="math inline">\(\sqrt{p/r}\)</span> and the rest set at zero with norm <span class="math inline">\({\Vert \theta_r \Vert}^2 =p\)</span>.</p>
<p>The classical risk satisfies <span class="math inline">\(R \left ( \hat y_{js} , \theta_r \right ) \geq p/2\)</span> where the simple thresholding rule <span class="math inline">\(\sqrt{2 \ln p}\)</span> performs with risk <span class="math inline">\(\sqrt{\ln p}\)</span> in the <span class="math inline">\(r\)</span>-spike sparse case even though it is inadmissible in MSE for a non-sparse signal.</p>
<p>The horseshoe estimator, <span class="math inline">\(\hat y_{hs}\)</span>, was proposed by <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span> to provide a Bayes rule that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals. HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate <span class="math display">\[
\sup_{ \theta \in l_0[p_n] } \;
\mathbb{E}_{ y | \theta } \|\hat y_{hs} - \theta \|^2 \asymp
p_n \log \left ( n / p_n \right ).
\]</span> The “worst’’ <span class="math inline">\(\theta\)</span> is obtained at the maximum difference between <span class="math inline">\(\left| \hat y_{hs} - y \right|\)</span> where <span class="math inline">\(\hat y_{hs} = \mathbb{E}(\theta|y)\)</span> can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).</p>
<!-- In the case of classification -->
<p>The predictive rule is often represented by a mathematical model, such as a linear regression model or a neural network. The model is trained using historical data, which consists of observed inputs and outputs <span class="math inline">\((x_1,y_1),\ldots, (x_n,y_n)\)</span>.</p>
<p>The model is then used to predict the output <span class="math inline">\(y\)</span> for new inputs <span class="math inline">\(x\)</span>. The accuracy of the model is evaluated using a performance metric, such as the mean squared error or the mean absolute error. The model is then updated and retrained using new data to improve its accuracy. This process is repeated until the model achieves the desired level of accuracy.</p>
<!-- 

There are many ways to build a predictive rule $f(x)$ that estimates the conditional mean of the output y, given input x. Here are some of the most common approaches:

**1. Linear Regression:**

- This is a simple and widely used method that assumes a linear relationship between the input and output variables. The model is represented as:

$$
y = F(x) = \beta_0 + \beta_1x + \epsilon
$$

where $\beta_0$ and $\beta_1$ are the intercept and slope coefficients, respectively, and $\epsilon$ is the error term. The coefficients are estimated by minimizing the squared error between the predicted and actual values of $$y$.

**Advantages:**

- Simple to interpret and implement.
- Efficient for large datasets.

**Disadvantages:**

- Assumes a linear relationship between the input and output variables, which might not be true for all datasets.
- Sensitive to outliers.

**2. Polynomial Regression:**

- This is an extension of linear regression that allows for non-linear relationships between the input and output variables. The model is represented as:

$$
y = F(x) = \beta_0 + \beta_1x + \beta_2x^2 + ... + \beta_k x_k + \epsilon
$$

where k is the degree of the polynomial. The coefficients are estimated by minimizing the squared error between the predicted and actual values of y.

**Advantages:**

- More flexible than linear regression and can capture non-linear relationships.

**Disadvantages:**

- Can be prone to overfitting, especially for high-degree polynomials.
- More complex to interpret than linear regression.

**3. Support Vector Regression (SVR):**

- This is a non-linear regression method that uses kernel functions to map the input data to a higher-dimensional space. The model is represented as:

$$
y = F(x) = \sum \alpha_i K(x, x_i) + b
$$

where $\alpha_i$ are the Lagrange multipliers, $K(x, x_i)$ is the kernel function, and b is the bias term. The coefficients $\alpha_i$ and b are estimated by minimizing a loss function that penalizes both large errors and model complexity.

**Advantages:**

- Can capture non-linear relationships without overfitting.
- Robust to outliers.

**Disadvantages:**

- Can be computationally expensive for large datasets.
- Not as easy to interpret as linear regression.

**4. Random Forest Regression:**

- This is an ensemble method that combines the predictions of multiple decision trees. Each decision tree is built on a random subset of the data and makes predictions based on the input features. The final prediction is the average of the predictions from all trees.

**Advantages:**

- Can capture complex relationships between the input and output variables.
- Robust to outliers.

**Disadvantages:**

- Can be computationally expensive to train.
- Not as easy to interpret as individual decision trees.

**5. Neural Networks:**

- These are powerful models that can capture complex relationships between the input and output variables. They consist of multiple layers of interconnected nodes, which learn to process information and make predictions.

**Advantages:**

- Can capture complex relationships that other methods might miss.
- Highly flexible and can be applied to a wide range of problems.

**Disadvantages:**

- Can be prone to overfitting if not properly trained.
- Difficult to interpret and understand how they make predictions.

**Choosing the best method depends on several factors**:

- The size and nature of your dataset.
- The complexity of the relationship between the input and output variables.
- The desired level of interpretability.
- The available computational resources.

It is important to experiment with different methods and compare their performance on your specific dataset before choosing the best model for your task. -->
</section>
<section id="lasso" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="lasso"><span class="header-section-number">12.4</span> LASSO</h2>
<p>The Laplace distribution can be represented as scale mixture of Normal distribution<span class="citation" data-cites="andrews1974scale">(<a href="references.html#ref-andrews1974scale" role="doc-biblioref">Andrews and Mallows 1974</a>)</span> <span class="math display">\[
\begin{aligned}
\theta_i \mid \sigma^2,\tau \sim &amp;N(0,\tau^2\sigma^2)\\
\tau^2  \mid \alpha \sim &amp;\exp (\alpha^2/2)\\
\sigma^2 \sim &amp; \pi(\sigma^2).\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau\)</span> <span class="math display">\[
p(\theta_i\mid \sigma^2,\alpha) =  \int_{0}^{\infty} \dfrac{1}{\sqrt{2\pi \tau}}\exp\left(-\dfrac{\theta_i^2}{2\sigma^2\tau}\right)\dfrac{\alpha^2}{2}\exp\left(-\dfrac{\alpha^2\tau}{2}\right)d\tau = \dfrac{\alpha}{2\sigma}\exp(-\alpha/\sigma|\theta_i|).
\]</span> Thus it is a Laplace distribution with location 0 and scale <span class="math inline">\(\alpha/\sigma\)</span>. Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from <span class="math inline">\(\theta \mid a,y\)</span> and <span class="math inline">\(b\mid \theta,y\)</span> to estimate joint distribution over <span class="math inline">\((\hat \theta, \hat b)\)</span>. Thus, we so not need to apply cross-validation to find optimal value of <span class="math inline">\(b\)</span>, the Bayesian algorithm does it “automatically”. We will discuss Gibbs algorithm later in the book.</p>
<p>When prior is Normal <span class="math inline">\(\theta_i \sim N(0,\sigma_{\theta}^2)\)</span>, the posterior mode is equivalent to the ridge estimate. The relation between variance of the prior and the penalty weight in ridge regression is inverse proportional <span class="math inline">\(\lambda\propto 1/\sigma_{\theta}^2\)</span>.</p>
</section>
<section id="subset-selection-ell_0-norm" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="subset-selection-ell_0-norm"><span class="header-section-number">12.5</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</h2>
<p>Skike-and-slab (George and McCulloh) or Bernoulli-Gaussian (Polson Sun)</p>
</section>
<section id="bridge-ell_alpha" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="bridge-ell_alpha"><span class="header-section-number">12.6</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</h2>
<p>This is a non-convex penalty when <span class="math inline">\(0&lt;\alpha&lt;1\)</span>. It is an NP-hard problem. When <span class="math inline">\(\alpha=1\)</span> or <span class="math inline">\(\alpha=2\)</span> we have optimisation problems that are “solvable” for large scale cases. However, when <span class="math inline">\(0\le \alpha&lt;1\)</span> the current optimisation algorithms won’t work.</p>
<p>The real killer is that you can use data to estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> (let the data speak for itself) <span class="citation" data-cites="box1992bayesian">Box and Tiao (<a href="references.html#ref-box1992bayesian" role="doc-biblioref">1992</a>)</span>.</p>
<p>Bayesian analogue of the bridge estimator in regression is <span class="math display">\[
y = X\beta + \epsilon
\]</span></p>
<p>for some unknown vector <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)'\)</span>. Given choices of <span class="math inline">\(\alpha \in (0,1]\)</span> and <span class="math inline">\(\nu \in \mathbb{R}^+\)</span>, the bridge estimator <span class="math inline">\(\hat{\beta}\)</span> is the minimizer of</p>
<p><span id="eq-qy"><span class="math display">\[
Q_y(\beta) = \frac{1}{2} \|y - X\beta\|^2 + \nu \sum_{j=1}^p |\beta_j|^\alpha.
\tag{12.1}\]</span></span></p>
<p>This bridges a class of shrinkage and selection operators, with the best-subset-selection penalty at one end, and the <span class="math inline">\(\ell_1\)</span> (or lasso) penalty at the other. An early reference to this class of models can be found in <span class="citation" data-cites="frank1993statistical">Frank and Friedman (<a href="references.html#ref-frank1993statistical" role="doc-biblioref">1993</a>)</span>, with recent papers focusing on model-selection asymptotics, along with strategies for actually computing the estimator (<span class="citation" data-cites="huang2008asymptotic">Huang, Horowitz, and Ma (<a href="references.html#ref-huang2008asymptotic" role="doc-biblioref">2008</a>)</span>, <span class="citation" data-cites="mazumder2011sparsenet">Mazumder, and and Hastie (<a href="references.html#ref-mazumder2011sparsenet" role="doc-biblioref">2011</a>)</span>).</p>
<p>Bridge approach differs from this line of work in adopting a Bayesian perspective on bridge estimation. Specifically, we treat</p>
<p><span class="math display">\[
p(\beta \mid y) \propto \exp\{-Q_y(\beta)\}
\]</span></p>
<p>as a posterior distribution having the minimizer of <a href="#eq-qy" class="quarto-xref">Equation&nbsp;<span>12.1</span></a> as its global mode. This posterior arises in assuming a Gaussian likelihood for <span class="math inline">\(y\)</span>, along with a prior for <span class="math inline">\(\beta\)</span> that decomposes as a product of independent exponential-power priors (<span class="citation" data-cites="box1992bayesian">Box and Tiao (<a href="references.html#ref-box1992bayesian" role="doc-biblioref">1992</a>)</span>):</p>
<p><span class="math display">\[
p(\beta \mid \alpha, \nu) \propto \prod_{j=1}^p \exp\left(-\left|\frac{\beta_j}{\tau}\right|^\alpha\right), \quad \tau = \nu^{-1/\alpha}. \tag{2}
\]</span></p>
<p>Rather than minimizing (1), we proceed by constructing a Markov chain having the joint posterior for <span class="math inline">\(\beta\)</span> as its stationary distribution.</p>
<section id="spike-and-slap-prior" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="spike-and-slap-prior"><span class="header-section-number">12.6.1</span> Spike-and-Slap Prior</h3>
<p>Our Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem <span class="math display">\[
y = \beta_1x_1+\ldots+\beta_px_p + e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \beta_i \le \infty \ .
\]</span> We would like to solve the problem of variable selections, i.e.&nbsp;identify which input variables <span class="math inline">\(x_i\)</span> to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.</p>
<p>To perform a model selection, we would like to specify a prior distribution <span class="math inline">\(p\left(\beta\right)\)</span>, which imposes a sparsity assumption on <span class="math inline">\(\beta\)</span>, where only a small portion of all <span class="math inline">\(\beta_i\)</span>’s are non-zero. In other words, <span class="math inline">\(\|\beta\|_0 = k \ll p\)</span>, where <span class="math inline">\(\|\beta\|_0 \defeq \#\{i : \beta_i\neq0\}\)</span>, the cardinality of the support of <span class="math inline">\(\beta\)</span>, also known as the <span class="math inline">\(\ell_0\)</span> (pseudo)norm of <span class="math inline">\(\beta\)</span>. A multivariate Gaussian prior (<span class="math inline">\(l_2\)</span> norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for <span class="math inline">\(\beta\)</span> can be constructed to impose sparsity include the double exponential (lasso).</p>
<p>Under spike-and-slab, each <span class="math inline">\(\beta_i\)</span> exchangeably follows a mixture prior consisting of <span class="math inline">\(\delta_0\)</span>, a point mass at <span class="math inline">\(0\)</span>, and a Gaussian distribution centered at zero. Hence we write,</p>
<p><span class="math display">\[
\label{eqn:ss}
\beta_i | \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N\left(0, \sigma_\beta^2\right) \ .
\]</span> Here <span class="math inline">\(\theta\in \left(0, 1\right)\)</span> controls the overall sparsity in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_\beta^2\)</span> accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.</p>
<p>A useful re-parameterization, the parameters <span class="math inline">\(\beta\)</span> is given by two independent random variable vectors <span class="math inline">\(\gamma = \left(\gamma_1, \ldots, \gamma_p\right)'\)</span> and <span class="math inline">\(\alpha = \left(\alpha_1, \ldots, \alpha_p\right)'\)</span> such that <span class="math inline">\(\beta_i  =  \gamma_i\alpha_i\)</span>, with probabilistic structure <span class="math display">\[
\label{eq:bg}
\begin{array}{rcl}
\gamma_i\mid\theta &amp; \sim &amp; \text{Bernoulli}(\theta) \ ;
\\
\alpha_i \mid \sigma_\beta^2 &amp;\sim &amp; N\left(0, \sigma_\beta^2\right) \ .
\\
\end{array}
\]</span> Since <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\alpha_i\)</span> are independent, the joint prior density becomes <span class="math display">\[
p\left(\gamma_i, \alpha_i \mid \theta, \sigma_\beta^2\right) =
\theta^{\gamma_i}\left(1-\theta\right)^{1-\gamma_i}\frac{1}{\sqrt{2\pi}\sigma_\beta}\exp\left\{-\frac{\alpha_i^2}{2\sigma_\beta^2}\right\}
\ , \ \ \ \text{for } 1\leq i\leq p \ .
\]</span> The indicator <span class="math inline">\(\gamma_i\in \{0, 1\}\)</span> can be viewed as a dummy variable to indicate whether <span class="math inline">\(\beta_i\)</span> is included in the model.</p>
<p>Let <span class="math inline">\(S = \{i: \gamma_i = 1\} \subseteq \{1, \ldots, p\}\)</span> be the “active set" of <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\|\gamma\|_0 = \sum\limits_{i = 1}^p\gamma_i\)</span> be its cardinality. The joint prior on the vector <span class="math inline">\(\{\gamma, \alpha\}\)</span> then factorizes as <span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right) &amp; = &amp; \prod\limits_{i = 1}^p p\left(\alpha_i, \gamma_i \mid \theta, \sigma_\beta^2\right) \\
&amp; = &amp;
\theta^{\|\gamma\|_0}
\left(1-\theta\right)^{p - \|\gamma\|_0}
\left(2\pi\sigma_\beta^2\right)^{-\frac p2}\exp\left\{-\frac1{2\sigma_\beta^2}\sum\limits_{i = 1}^p\alpha_i^2\right\} \ .
\end{array}
\]</span></p>
<p>Let <span class="math inline">\(X_\gamma \defeq \left[X_i\right]_{i \in S}\)</span> be the set of “active explanatory variables" and <span class="math inline">\(\alpha_\gamma \defeq \left(\alpha_i\right)'_{i \in S}\)</span> be their corresponding coefficients. We can write <span class="math inline">\(X\beta = X_\gamma \alpha_\gamma\)</span>. The likelihood can be expressed in terms of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\alpha\)</span> as <span class="math display">\[
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)
=
\left(2\pi\sigma_e^2\right)^{-\frac n2}
\exp\left\{
-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
\right\} \ .
\]</span></p>
<p>Under this re-parameterization by <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span>, the posterior is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y\right) &amp; \propto &amp;
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right)
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)\\
&amp; \propto &amp;
\exp\left\{-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
-\frac1{2\sigma_\beta^2}\left\|\alpha\right\|_2^2
-\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0
\right\} \ .
\end{array}
\]</span> Our goal then is to find the regularized maximum a posterior (MAP) estimator <span class="math display">\[
\arg\max\limits_{\gamma, \alpha}p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y \right) \ .
\]</span> By construction, the <span class="math inline">\(\gamma\)</span> <span class="math inline">\(\in\left\{0, 1\right\}^p\)</span> will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span> the regularized least squares objective function</p>
<p><span id="eq-obj:map"><span class="math display">\[
\min\limits_{\gamma, \alpha}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2
+ 2\sigma_e^2\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0 \ .
\tag{12.2}\]</span></span> This objective possesses several interesting properties:</p>
<ol type="1">
<li><p>The first term is essentially the least squares loss function.</p></li>
<li><p>The second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) <span class="math inline">\(\sigma_\beta^2/\sigma_e^2\)</span>. Smaller SNR will be more likely to shrink the estimates towards <span class="math inline">\(0\)</span>. If <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want <span class="math inline">\(\sigma_\beta \to \infty\)</span> or to be “sufficiently large" in order to avoid imposing harsh shrinkage to non-zero signals.</p></li>
<li><p>If we further assume that <span class="math inline">\(\theta &lt; \frac12\)</span>, meaning that the coefficients are known to be sparse <em>a priori</em>, then <span class="math inline">\(\log\left(\left(1-\theta\right) / \theta\right) &gt; 0\)</span>, and the third term can be seen as an <span class="math inline">\(\ell_0\)</span> regularization.</p></li>
</ol>
<p>Therefore, our Bayesian objective inference is connected to <span class="math inline">\(\ell_0\)</span>-regularized least squares, which we summarize in the following proposition.</p>
<p>(Spike-and-slab MAP &amp; <span class="math inline">\(\ell_0\)</span> regularization)</p>
<p>For some <span class="math inline">\(\lambda &gt; 0\)</span>, assuming <span class="math inline">\(\theta &lt; \frac12\)</span>, <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the Bayesian MAP estimate defined by <a href="#eq-obj:map" class="quarto-xref">Equation&nbsp;<span>12.2</span></a> is equivalent to the <span class="math inline">\(\ell_0\)</span> regularized least squares objective, for some <span class="math inline">\(\lambda &gt; 0\)</span>, <span id="eq-obj:l0"><span class="math display">\[
\min\limits_{\beta}
\frac12\left\|y - X\beta\right\|_2^2
+ \lambda
\left\|\beta\right\|_0 \ .
\tag{12.3}\]</span></span></p>
<p>First, assuming that <span class="math display">\[
\theta &lt; \frac12, \ \ \  \sigma_\beta^2 \gg \sigma_e^2, \ \ \  \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2 \to 0 \ ,
\]</span> gives us an objective function of the form <span id="eq-obj:vs"><span class="math display">\[
\min\limits_{\gamma, \alpha}
\frac12 \left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \lambda
\left\|\gamma\right\|_0,  \ \ \ \  \text{where } \lambda \defeq \sigma_e^2\log\left(\left(1-\theta\right) / \theta\right) &gt; 0 \ .
\tag{12.4}\]</span></span></p>
<p>Equation <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a> can be seen as a variable selection version of equation <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a>. The interesting fact is that <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a> and <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a> are equivalent. To show this, we need only to check that the optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a> corresponds to a feasible solution to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a> and vice versa. This is explained as follows.</p>
<p>On the one hand, assuming <span class="math inline">\(\hat\beta\)</span> is an optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a>, then we can correspondingly define <span class="math inline">\(\hat\gamma_i \defeq I\left\{\hat\beta_i \neq 0\right\}\)</span>, <span class="math inline">\(\hat\alpha_i \defeq \hat\beta_i\)</span>, such that <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is feasible to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a> and gives the same objective value as <span class="math inline">\(\hat\beta\)</span> gives <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a>.</p>
<p>On the other hand, assuming <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is optimal to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a>, implies that we must have all of the elements in <span class="math inline">\(\hat\alpha_\gamma\)</span> should be non-zero, otherwise a new <span class="math inline">\(\tilde\gamma_i \defeq I\left\{\hat\alpha_i \neq 0\right\}\)</span> will give a lower objective value of <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a>. As a result, if we define <span class="math inline">\(\hat\beta_i \defeq \hat\gamma_i\hat\alpha_i\)</span>, <span class="math inline">\(\hat\beta\)</span> will be feasible to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>12.3</span></a> and gives the same objective value as <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> gives <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>12.4</span></a>.</p>
</section>
</section>
<section id="horseshoe-prior" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="horseshoe-prior"><span class="header-section-number">12.7</span> Horseshoe Prior</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//horseshoe.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>The sparse normal means problem is concerned with inference for the parameter vector <span class="math inline">\(\theta = ( \theta_1 , \ldots , \theta_p )\)</span> where we observe data <span class="math inline">\(y_i = \theta_i + \epsilon_i\)</span> where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by <span class="math inline">\(\pi_{HS}\)</span>, (a.k.a. log-prior, <span class="math inline">\(- \log p_{HS}\)</span>)? Lasso simply uses an <span class="math inline">\(L^1\)</span>-norm, <span class="math inline">\(\sum_{i=1}^K | \theta_i |\)</span>, as opposed to the horseshoe prior which (essentially) uses the penalty <span class="math display">\[
\pi_{HS} ( \theta_i | \tau ) = - \log p_{HS} ( \theta_i | \tau ) = - \log \log \left ( 1 + \frac{2 \tau^2}{\theta_i^2} \right ) .
\]</span> The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in <strong>both</strong> the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.</p>
<p>From a historical perspective, James-Stein (a.k.a <span class="math inline">\(L^2\)</span>-regularisation)<span class="citation" data-cites="stein1964inadmissibility">(<a href="references.html#ref-stein1964inadmissibility" role="doc-biblioref">Stein 1964</a>)</span> is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity. A simple sparsity example shows the issue with <span class="math inline">\(L^2\)</span>-regularisation. Consider the sparse <span class="math inline">\(r\)</span>-spike shows the problem with focusing solely on rules with the same shrinkage weight (albeit benefiting from pooling of information).</p>
<p>Let the true parameter value be <span class="math inline">\(\theta_p = \left ( \sqrt{d/p} , \ldots , \sqrt{d/p} , 0 , \ldots , 0 \right )\)</span>. James-Stein is equivalent to the model <span class="math display">\[
y_i = \theta_i + \epsilon_i \; \mathrm{ and} \; \theta_i \sim \mathcal{N} \left ( 0 , \tau^2 \right )
\]</span> This dominates the plain MLE but loses admissibility! This is due to the fact that a “plug-in” estimate of global shrinkage <span class="math inline">\(\hat{\tau}\)</span> is used. Tiao and Tan’s original “closed-form” analysis is particularly relevant here. They point out that the mode of <span class="math inline">\(p(\tau^2|y)\)</span> is zero exactly when the shrinkage weight turns negative (their condition 6.6). From a risk perspective <span class="math inline">\(E \Vert \hat{\theta}^{JS} - \theta \Vert \leq p , \forall \theta\)</span> showing the inadmissibility of the MLE. At origin the risk is <span class="math inline">\(2\)</span>, <strong>but!</strong> <span class="math display">\[
\frac{p \Vert \theta \Vert^2}{p + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_p \right ) \leq
2 + \frac{p \Vert \theta \Vert^2}{ d + \Vert \theta \Vert^2}.
\]</span> This implies that <span class="math inline">\(R \left ( \hat{\theta}^{JS} , \theta_p \right ) \geq (p/2)\)</span>. Hence, simple thresholding rule beats James-Stein this with a risk given by <span class="math inline">\(\sqrt{\log p }\)</span>. This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
<p>The horseshoe <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span> is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.</p>
<p>We introduce the horseshoe in the context of the normal means model, which is given by <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>. The horseshoe prior is given by <span class="math display">\[\begin{align*}
\beta_i &amp;\sim \mathcal{N}(0, \sigma^2 \tau^2 \lambda_i^2)\\
\lambda_i &amp;\sim C^+(0, 1),
\end{align*}\]</span> where <span class="math inline">\(C^+\)</span> denotes the half-Cauchy distribution. Optionally, hyperpriors on <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\sigma\)</span> may be specified, as is described further in the next two sections.</p>
<p>To illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for <span class="math inline">\(\beta_i\)</span> as a function of <span class="math inline">\(y_i\)</span> for three different values of <span class="math inline">\(\tau\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(horseshoe)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tau.values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.005</span>, <span class="fl">0.05</span>, <span class="fl">0.5</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>y.values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">tau =</span> <span class="fu">rep</span>(tau.values, <span class="at">each =</span> <span class="fu">length</span>(y.values)),</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">rep</span>(y.values, <span class="dv">3</span>),</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">post.mean =</span> <span class="fu">c</span>(<span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">1</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">2</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">3</span>], <span class="at">Sigma2=</span><span class="dv">1</span>)) )</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> post.mean, <span class="at">group =</span> tau, <span class="at">color =</span> <span class="fu">factor</span>(tau))) <span class="sc">+</span> </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette=</span><span class="st">"Dark2"</span>) <span class="sc">+</span> </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">colour =</span> <span class="st">"grey"</span>) <span class="sc">+</span> </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"Tau"</span>) <span class="sc">+</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Horseshoe posterior mean for three values of tau"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Smaller values of <span class="math inline">\(\tau\)</span> lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to <span class="math inline">\(\sqrt{2\sigma^2\log(1/\tau)}\)</span> are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of <span class="math inline">\(\tau\)</span> is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.</p>
</section>
<section id="the-normal-means-problem" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="the-normal-means-problem"><span class="header-section-number">12.8</span> The normal means problem</h2>
<p>The normal means model is: <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>First, we will be computing the posterior mean only, with known variance <span class="math inline">\(\sigma^2\)</span> The function <code>HS.post.mean</code> computes the posterior mean of <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span>. It does not require MCMC and is suitable when only an estimate of the vector <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span> is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for <span class="math inline">\(\sigma^2\)</span> is available, please see below for the function <code>HS.normal.means</code>.</p>
<p>The function <code>HS.post.mean</code> requires the observed outcomes, a value for <span class="math inline">\(\tau\)</span> and a value for <span class="math inline">\(\sigma\)</span>. Ideally, <span class="math inline">\(\tau\)</span> should be equal to the proportion of nonzero <span class="math inline">\(\beta_i\)</span>’s. Typically, this proportion is unknown, in which case it is recommended to use the function <code>HS.MMLE</code> to find the marginal maximum likelihood estimator for <span class="math inline">\(\tau\)</span>.</p>
<p>As an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 <span class="math inline">\(\beta_i\)</span>’s are equal to five and the remaining <span class="math inline">\(\beta_i\)</span>’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">index =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                 truth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">5</span>, <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">40</span>)),</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                 y <span class="ot">&lt;-</span> truth <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">50</span>) <span class="co">#observations</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We estimate <span class="math inline">\(\tau\)</span> using the MMLE, using the known variance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>(tau.est <span class="ot">&lt;-</span> <span class="fu">HS.MMLE</span>(df<span class="sc">$</span>y, <span class="at">Sigma2 =</span> <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.8264441</code></pre>
</div>
</div>
<p>We then use this estimate of <span class="math inline">\(\tau\)</span> to find the posterior mean, and add it to the plot in red.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>post.mean <span class="ot">&lt;-</span> <span class="fu">HS.post.mean</span>(df<span class="sc">$</span>y, tau.est, <span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean <span class="ot">&lt;-</span> post.mean</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>If the posterior variance is of interest, the function <code>HS.post.var</code> can be used. It takes the same arguments as <code>HS.post.mean</code>.</p>
<section id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2"><span class="header-section-number">12.8.1</span> Posterior mean, credible intervals and variable selection, possibly unknown <span class="math inline">\(\sigma^2\)</span></h3>
<p>The function <code>HS.normal.means</code> is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (<span class="math inline">\(\beta_i\)</span>’s, <span class="math inline">\(\tau\)</span>, <span class="math inline">\(\sigma\)</span>), the posterior median for the <span class="math inline">\(\beta_i\)</span>’s, and credible intervals for the <span class="math inline">\(\beta_i\)</span>’s.</p>
<p>The key choices to make are:</p>
<ul>
<li>How to handle <span class="math inline">\(\tau\)</span>. The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to <span class="math inline">\([1/n, 1]\)</span>). See the manual for other options.</li>
<li>How to handle <span class="math inline">\(\sigma\)</span>. The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.</li>
</ul>
<p>Other options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).</p>
<p>Let’s continue the example from the previous section. We first create a ‘horseshoe object’.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>hs.object <span class="ot">&lt;-</span> <span class="fu">HS.normal.means</span>(df<span class="sc">$</span>y, <span class="at">method.tau =</span> <span class="st">"truncatedCauchy"</span>, <span class="at">method.sigma =</span> <span class="st">"Jeffreys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We extract the posterior mean of the <span class="math inline">\(\beta_i\)</span>’s and plot them in red.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean.full <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>BetaHat</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We plot the marginal credible intervals (and remove the observations from the plot for clarity).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>lower.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>LeftCI</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>upper.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>RightCI</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI), <span class="at">width =</span> .<span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Red = estimates with 95% credible intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Finally, we perform variable selection using <code>HS.var.select</code>. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.CI <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)), </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)),</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as <span class="math inline">\(c_iy_i\)</span> where <span class="math inline">\(y_i\)</span> is the observation and <span class="math inline">\(c_i\)</span> some number between 0 and 1. A variable is selected if <span class="math inline">\(c_i \geq c\)</span> for some user-selected threshold <span class="math inline">\(c\)</span> (default is <span class="math inline">\(c = 0.5\)</span>). In the example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.thres <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"threshold"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)), </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)),</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="12-theoryai_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-andrews1974scale" class="csl-entry" role="listitem">
Andrews, D. F., and C. L. Mallows. 1974. <span>“Scale <span>Mixtures</span> of <span>Normal Distributions</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 36 (1): 99–102. <a href="https://www.jstor.org/stable/2984774">https://www.jstor.org/stable/2984774</a>.
</div>
<div id="ref-box1992bayesian" class="csl-entry" role="listitem">
Box, George E. P., and George C. Tiao. 1992. <em>Bayesian <span>Inference</span> in <span>Statistical Analysis</span></em>. New York: Wiley-Interscience.
</div>
<div id="ref-carvalho2010horseshoe" class="csl-entry" role="listitem">
Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. <span>“The Horseshoe Estimator for Sparse Signals.”</span> <em>Biometrika</em>, asq017.
</div>
<div id="ref-frank1993statistical" class="csl-entry" role="listitem">
Frank, Ildiko E., and Jerome H. Friedman. 1993. <span>“A <span>Statistical View</span> of <span>Some Chemometrics Regression Tools</span>.”</span> <em>Technometrics</em> 35 (2): 109–35. <a href="https://www.jstor.org/stable/1269656">https://www.jstor.org/stable/1269656</a>.
</div>
<div id="ref-huang2008asymptotic" class="csl-entry" role="listitem">
Huang, Jian, Joel L. Horowitz, and Shuangge Ma. 2008. <span>“Asymptotic Properties of Bridge Estimators in Sparse High-Dimensional Regression Models.”</span> <em>The Annals of Statistics</em> 36 (2): 587–613.
</div>
<div id="ref-mazumder2011sparsenet" class="csl-entry" role="listitem">
Mazumder, Rahul, Friedman, and Trevor and Hastie. 2011. <span>“<a href="https://www.ncbi.nlm.nih.gov/pubmed/25580042"><span>SparseNet</span>: <span>Coordinate Descent With Nonconvex Penalties</span></a>.”</span> <em>Journal of the American Statistical Association</em> 106 (495): 1125–38.
</div>
<div id="ref-stein1964inadmissibility" class="csl-entry" role="listitem">
Stein, Charles. 1964. <span>“Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 16 (1): 155–60.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11-pattern.html" class="pagination-link" aria-label="Pattern Matching">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-glm.html" class="pagination-link" aria-label="Linear and Multiple Regression">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>