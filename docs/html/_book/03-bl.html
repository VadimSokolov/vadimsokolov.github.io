<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.37">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Bayesian Learning – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04-dec.html" rel="next">
<link href="./02-bayes.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8f57c241cdbc1f937d718a8870719880.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./03-bl.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Theory of AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linear and Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification: Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Deep Learners</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Robotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./03-bl.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-bl" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“<em>Wise man proportions his beliefs to the evidence.</em>” – David Hume</p>
</blockquote>
<p>Statistics makes use of parametric families of distributions and make assumption that observed samples <span class="math inline">\(y = (y_1,\ldots,y_n)\)</span> are independent and identically distributed observations from a distribution with density function parametrized by <span class="math inline">\(\theta\)</span>, the notation is <span class="math inline">\(y \sim p(y \mid \theta)\)</span>. The functional form of <span class="math inline">\(p(y \mid \theta)\)</span> is assumed to be known, but the value of <span class="math inline">\(\theta\)</span> is unknown. The goal of statistical inference is to estimate <span class="math inline">\(\theta\)</span> from the observed data <span class="math inline">\(y_1,\ldots,y_n\)</span>. There are several tasks in statistical inference, including</p>
<ul>
<li>Estimation: use sample to estimate <span class="math inline">\(\theta\)</span> by a single value <span class="math inline">\(\hat{\theta}\)</span> or by an interval <span class="math inline">\([a,b]\)</span> that contains the true value of <span class="math inline">\(\theta\)</span> with a certain probability.</li>
<li>Hypothesis testing: test a hypothesis about the value of <span class="math inline">\(\theta\)</span>. For example, we may want to test whether <span class="math inline">\(\theta\)</span> is equal to a certain value <span class="math inline">\(\theta_0\)</span>.</li>
<li>Prediction: predict the value of a new observation <span class="math inline">\(y_{n+1}\)</span> given the observed data <span class="math inline">\(y_1,\ldots,y_n\)</span>.</li>
</ul>
<p>In this section we present a general framework for statistical inference, known as Bayesian inference, which is based on the use of probability distributions to represent uncertainty and make inferences about unknown parameters. We will use the Bayes rule to update our beliefs about the parameters of a model based on new evidence or data. Bayesian inference provides a principled approach to statistical modeling and decision-making, and is widely used in various fields such as machine learning, econometrics, and engineering.</p>
<p>In the context of artificial intelligence and statistical modeling, Bayesian parameter learning is particularly relevant when dealing with models that have uncertain or unknown parameters. The goal is to update the probability distribution over the parameters of the model as new data becomes available. Suppose that you are interested in the values of k unknown quantities <span class="math display">\[
\theta = (\theta_1, \ldots, \theta_k)
\]</span></p>
<p>The basic steps involved in Bayesian parameter learning include:</p>
<ol type="1">
<li><p><strong>Prior Distribution (Prior):</strong> Start with a prior distribution <span class="math display">\[
p(\theta)
\]</span> that represents your beliefs or knowledge about the parameters before observing any data. This distribution encapsulates the uncertainty about the parameters.</p></li>
<li><p><strong>Likelihood Function (Data Likelihood):</strong> Specify a likelihood function that describes the probability of observing the given data given the current values of the parameters. This function represents the likelihood of the observed data under different parameter values. Suppose you observe a set of data <span class="math inline">\(y = (y_1, \ldots, y_n)\)</span>, the likelihood function is given by <span class="math display">\[
p(y \mid \theta)
\]</span> where <span class="math inline">\(\theta\)</span> is the set of model parameters, and <span class="math inline">\(y\)</span> is the observed data. The likelihood function captures the information in the data about the parameters.</p></li>
<li><p><strong>Posterior Distribution (Posterior):</strong> Combine the prior distribution and the likelihood function using Bayes’ theorem to obtain the posterior distribution over the parameters. The posterior distribution represents the updated beliefs about the parameters after incorporating the observed data. <span class="math display">\[
p( \theta | y )  = \frac{p(y \mid  \theta)p( \theta)}{p(y)}
\]</span> The right hand side <span class="math inline">\(p( \theta | y )\)</span> is the posterior distribution, and <span class="math inline">\(p(y)\)</span> is the probability of the observed data (also known as the total probability) given by <span class="math display">\[
p(y)  = \int p(y \mid  \theta)p( \theta)d \theta
\]</span></p></li>
<li><p><strong>Posterior as the New Prior (Iterative Process):</strong> Use the posterior distribution obtained from one round of observation as the prior distribution for the next round when more data becomes available. This process can be iteratively repeated as new evidence is acquired.</p></li>
<li><p><strong>Bayesian Inference:</strong> Make predictions or draw inferences by summarizing the information in the posterior distribution. This may involve computing point estimates (e.g., mean, median) or credible intervals that capture a certain percentage of the parameter values.</p></li>
</ol>
<p>The key advantage of Bayesian parameter learning is its ability to incorporate prior knowledge and update beliefs based on observed data in a principled manner. It provides a framework for handling uncertainty and expressing the confidence or ambiguity associated with parameter estimates. However, it often requires computational methods, such as Markov Chain Monte Carlo (MCMC) or variational inference, to approximate or sample from the complex posterior distributions.</p>
<p>The Bayes rule allows us to combine the prior distribution and the likelihood function, sometimes we omit the total probability in the denominator on the right hand side and write Bayes rule as <span class="math display">\[
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
\]</span></p>
<p>The choice of prior distribution can significantly impact the ease of computation and the interpretation of the posterior distribution. Conjugate priors are a special type of prior distribution that, when combined with a specific likelihood function, result in a posterior distribution that belongs to the same family as the prior. This property simplifies the computation of the posterior distribution, and allows for analytical solution.</p>
<p>Common examples of conjugate priors include:</p>
<ul>
<li><p><strong>Normal distribution with known variance:</strong> If the likelihood is a normal distribution with known variance, then a normal distribution is a conjugate prior for the mean.</p></li>
<li><p><strong>Binomial distribution:</strong> If the likelihood is a binomial distribution, then a beta distribution is a conjugate prior for the probability of success.</p></li>
<li><p><strong>Poisson distribution:</strong> If the likelihood is a Poisson distribution, then a Gamma distribution is a conjugate prior for the rate parameter.</p></li>
</ul>
<p>Using conjugate priors simplifies the Bayesian analysis, especially in cases where analytical solutions are desirable. However, the choice of a conjugate prior is often a modeling assumption, and in some cases, non-conjugate priors may be more appropriate for capturing the true underlying uncertainty in the problem. The blind use of conjugate priors can lead to misleading results. We should never ignore the absence of evidence for use of a specific model.</p>
<section id="exchangeability-and-the-bayesian-view-of-probability-models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="exchangeability-and-the-bayesian-view-of-probability-models"><span class="header-section-number">3.1</span> Exchangeability and the Bayesian view of probability models</h2>
<p>At the basis of all statistical problems is a potential sample of data, <span class="math inline">\(y=\left( y_{1},\ldots,y_{T}\right)\)</span>, and assumptions over the data generating process such as independence, a model or models, and parameters. How should one view the relationship between models, parameters, and samples of data? How should one define a model and parameters? These questions have fundamental implications for statistical inference and can be answered from different perspectives. We will discuss the de Finetti’s representation theorem which provides a formal connection between data, models, and parameters.</p>
<p>To understand the issues, consider the simple example of an experiment consisting of tosses of a simple thumb tack in ideal “laboratory” conditions. The outcome of the experiment can be defined as a random variable <span class="math inline">\(y_{i},\)</span> where <span class="math inline">\(y_{i}=1\)</span> if the <span class="math inline">\(i^{th}\)</span> toss was a heads (the tack lands on the spike portion) and <span class="math inline">\(y_{i}=0\)</span> if the tack land tails (on its flat portion). How do we model these random variables? The frequentist or objective approach assumes tosses are independent and identically distributed. In this setting, independence implies that <span class="math display">\[
P\left(  y_{2}=1,y_{1}=1\right)  =P\left(  y_{2}=1\right)
P\left(  y_{1}=0\right)  \text{.}%
\]</span></p>
<p>Given this, are thumbtack tosses independent? Surprisingly, the answer is no. Or at least absolutely not under the current assumptions. Independence implies that <span class="math display">\[
P\left(  y_{2}=1 \mid y_{1}=1\right)  =P\left(  y_{2}=1\right)
\text{,}%
\]</span> which means that observing <span class="math inline">\(y_{1}=1\)</span> does not effect the probability that <span class="math inline">\(y_{2}=1\)</span>. To see the implications of this simple fact, suppose that the results of 500 tosses were available. If the tosses were independent, then <span class="math display">\[
P\left(  y_{501}=1\right)  =P\left(  y_{501}=1\mid {\textstyle\sum\nolimits_{t=1}^{500}}y_{t}=1\right)  =P\left(  y_{501}=1\mid {\textstyle\sum\nolimits_{t=1}^{500}}y_{t}=499\right)  \text{.}
\]</span> It is hard to imagine that anyone would believe this–nearly every observer would state that the second probability is near zero and the third probably is near 1 as the first 500 tosses contain a lot of information. Thus, the tosses are not independent.</p>
<p>To see the resolution of this apparent paradox, introduce a parameter, <span class="math inline">\(\theta\)</span>, which is the probability that a thumb tack toss is heads. If <span class="math inline">\(\theta\)</span> were known, then it is true that, conditional on the value of this parameter, the tosses are independent and <span class="math display">\[
P\left(  y_{2}=1\mid y_{1}=1,\theta\right)  =P\left(y_{2}=1\mid \theta\right)  =\theta\text{.}
\]</span> Thus, the traditional usage of independence, and independent sampling, requires that “true” parameter values are known. With unknown probabilities, statements about future tosses are heavily influenced by previous observations, clearly violating the independence assumption. Ironically, if the data was really independent, we would not need samples in the first place to estimate parameters because the probabilities would already be known! Given this, if you were now presented with a thumb tack from a box that was to be repeatedly tossed, do you think that the tosses are independent?</p>
<p>This example highlights the tenuous foundations, an odd circularity, and the internal inconsistency of the frequentist approach that proceeds under the assumption of a fixed “true” parameter. All frequentist procedures are founded on the assumption of known parameter values:sampling distributions of estimators are computed conditional on <span class="math inline">\(\theta\)</span>; confidence intervals consist of calculations of the form: <span class="math inline">\(P\left( f\left( y_{1}, \ldots ,y_{T}\right) \in\left( a,b\right) |\theta\right)\)</span>; and asymptotics also all rely on the assumption of known parameter values. None of these calculations are possible without assuming the known parameters.</p>
<p>In the frequentist approach, even though the parameter is completely unknown to the researcher, <span class="math inline">\(\theta\)</span> is not a random variable, does not have a distribution, and therefore inference is not governed by the rules of probability. Given this “fixed, but unknown” definition, it is impossible to discuss concepts like “parameter uncertainty.” This strongly violates our intuition, since things that are not known are typically thought of as random.</p>
<p>The Bayesian approach avoids this internal inconsistency by shedding the strong assumption of independence and assumption of a fixed but unknown parameter. Instead it assumes that <span class="math inline">\(\theta\)</span> is a random variable and describes the uncertainty about <span class="math inline">\(\theta\)</span> using a probability distribution, <span class="math inline">\(p\left( \theta\right)\)</span> (the prior). The joint distribution of the data is then <span class="math display">\[
p(y_{1}, \ldots ,y_{T},\theta)  = \int p(y_{1}, \ldots ,y_{T} \mid \theta)  p(\theta)d\theta = \int\prod_{t=1}^Tp(y_t\mid \theta)  p( \theta)d\theta.
\]</span> Notice, that the right-hand-side does not depend on the order of the data, and the joint distribution of the data is the same for all potential orderings. This is a natural assumption about the symmetry of the data, and is called <em>exchangeability</em>. The Bayesian approach makes no assumptions about the order in which the data may arrive, and each observation has the same marginal distribution, <span class="math inline">\(P\left( y_{i}=1\right) =P\left(y_{j}=1\right)\)</span> for any <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<p>Thus, we replace the independence assumption with a weaker and more natural assumption of exchangeability: collection of random variables, <span class="math inline">\(y_{1}, \ldots ,y_{T}\)</span>, is exchangeable if the distribution of <span class="math inline">\(y_{1}, \ldots ,y_{T}\)</span> is the same as the distribution of any permutation <span class="math inline">\(y_{\pi_{1}}, \ldots ,y_{\pi_{T}}\)</span>, where <span class="math inline">\(\pi=\left( \pi_{1}, \ldots ,\pi_{T}\right)\)</span> is a permutation of the integers <span class="math inline">\(1\)</span> to <span class="math inline">\(T\)</span>. Independent events are always exchangeable, but the converse is not true. Notice the differences between the assumptions in the Bayesian and frequentist approach: the Bayesian makes assumptions over potentially realized data, and there is no need to invent the construct of a fixed but unknown parameter, since exchangeability makes no reference to parameters.</p>
<p>In the case of the tack throwing experiment, exchangeability states that the ordering of heads and tails does not matter. Thus, if the experiment of 8 tosses generated 4 heads, it does not matter if the ordering was <span class="math inline">\(\left(1,0,1,0,1,0,1,0\right)\)</span> or <span class="math inline">\(\left( 0,1,1,0,1,0,0,1\right)\)</span>. This is a natural assumption about the symmetry of the tack tosses, capturing the idea that the information in any toss or sequence of tosses is the same as any other–the idea of a truly random sample. It is important to note that exchangeability is property that applies prior to viewing the data. After observation, data is no longer a random variable, but a realization of a random variable.</p>
<p>Bruno de Finetti introduced the notion of exchangeability, and then asked a simple question: “What do exchangeable sequences of random variables look like?” The answer to this question is given in the famous de Finetti’s theorem, which also <em>defines</em> models, parameters, and provides important linkages between frequentist and classical statistics.</p>
<section id="de-finettis-representation-theorem" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="de-finettis-representation-theorem"><span class="header-section-number">3.1.1</span> de Finetti’s Representation theorem</h3>
<p>de Finetti’s representation theorem provides the theoretical connection between data, models, and parameters. It is stated first in the simplest setting, where the observed data takes two values, either zero or one, and then extended below.</p>
<div id="thm-Representation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (de Finetti’s Representation theorem)</strong></span> Let <span class="math inline">\(\left( y_{1},y_{2},\ldots\right)\)</span> be an infinite sequence of 0-1 exchangeable random variables with joint density <span class="math inline">\(p\left(y_{1}, \ldots ,y_{T}\right)\)</span>. Then there exists a distribution function <span class="math inline">\(P\)</span> such that <span id="eq-deFinetti"><span class="math display">\[
p(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}\theta^{y_{t}}(1-\theta)^{1-y_{t}%
}dP(\theta)=\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta\right)  dP(\theta)
\tag{3.1}\]</span></span> where <span class="math display">\[
P(\theta)=\underset{T\rightarrow\infty}{\lim}\text{Prob}\left[  \frac{1}{T}\sum_{t=1}^{T}y_{t}\leq\theta\right]  \text{ and }\theta=\underset {T\rightarrow\infty}{\lim}\frac{1}{T}\sum_{t=1}^{T}y_{t}\text{.}%
\]</span> If the distribution function or measure admits a density with respect to Lebesgue measure, then <span class="math inline">\(dP(\theta)=p\left( \theta\right) d\theta\)</span>.</p>
</div>
<p>de Finetti’s representation theorem has profound implications for understanding models from a subjectivist perspective and in relating subjectivist to frequentist theories of inference. The theorem is interpreted as follows:</p>
<ul>
<li><p>Under exchangeability, parameters exist, and one can <em>act as if</em> the <span class="math inline">\(y_{t}\)</span>’<span class="math inline">\(s\)</span> are drawn independently from a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>. That is, they are draws from the model <span class="math inline">\(p\left(y_{t} \mid \theta\right) =\theta^{y_{t}}(1-\theta)^{1-y_{t}},\)</span> generating a likelihood function <span class="math inline">\(p\left( y \mid \theta\right) =\prod_{t=1}^{T}p\left(y_{t} \mid \theta\right)\)</span>. Formally, the likelihood function is defined via the density <span class="math inline">\(p\left( y \mid \theta\right)\)</span>, viewed as a function of <span class="math inline">\(\theta\)</span> for a fixed sample <span class="math inline">\(y=\left( y_{1}, \ldots ,y_{T}\right)\)</span>. More “likely” parameter values generate higher likelihood values, thus the name. The maximum likelihood estimate or MLE is <span class="math display">\[
\widehat{\theta}=\arg\underset{\theta\in\Theta}{\max}\text{ }p\left(y \mid \theta\right)  =\arg\underset{\theta\in\Theta}{\max}\ln p\left(y \mid \theta\right)  \text{,}%   
\]</span> where <span class="math inline">\(\Theta\)</span> is the parameter space.</p></li>
<li><p>Parameters are random variables. The limit <span class="math inline">\(\theta=\underset {T\rightarrow\infty}{\lim}T^{-1}\sum_{t=1}^{T}y_{t}\)</span> exists but is a random variable. This can be contrasted with the strong law of large numbers that requires independence and implies that <span class="math inline">\(T^{-1}\sum_{t=1}^{T}y_{t}\)</span> converges almost surely to a fixed value, <span class="math inline">\(\theta_{0}\)</span>. From this, one can interpret a parameter as a limit of observables and justifies the frequentist interpretation of <span class="math inline">\(\theta\)</span> as a limiting frequency of 1’s.</p></li>
<li><p>The distribution <span class="math inline">\(P\left( \theta\right)\)</span> or density <span class="math inline">\(p\left(\theta\right)\)</span> can be interpreted as beliefs about the limiting frequency <span class="math inline">\(\theta\)</span> prior to viewing the data. After viewing the data, beliefs are updated via Bayes rule resulting in the posterior distribution, <span class="math display">\[
p\left(  \theta \mid y\right)  \propto p\left(  y \mid \theta\right)  p(\theta).
\]</span> Since the likelihood function is fixed in this case, any distribution of observed data can be generated by varying the prior distribution.</p></li>
</ul>
<p>The main implication of de Finetti’s theorem is a complete justification for Bayesian practice of treating the parameters as random variables and specifying a likelihood and parameter distribution. Stated differently, a “model” consists of both a likelihood and a prior distribution over the parameters. Thus, parameters as random variables and priors are a necessity for statistical inference, and not some extraneous component motivated by philosophical concerns.</p>
<p>More general versions of de Finetti’s theorem are available. A general version is as follows. If <span class="math inline">\(\left\{ y_{t}\right\} _{t\geq1}\)</span>, <span class="math inline">\(y_{t}\in\mathbb{R}\)</span>, is a sequence of infinitely exchangeable random variables, then there exists a probability measure <span class="math inline">\(P\)</span> on the space of all distribution functions, such that <span class="math display">\[
P(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}F\left(  y_{t}\right)
P(dF)
\]</span> with mixing measure <span class="math display">\[
P\left(  F\right)  =\underset{T\rightarrow\infty}{\lim}P(F_{T}),
\]</span> where <span class="math inline">\(F_{T}\)</span> is the empirical distribution of the data. At this level of generality, the distribution function is infinite-dimensional. In practice, additional subjective assumptions are needed that usually restrict the distribution function to finite dimensional spaces, which implies that distribution function is indexed by a parameter vector <span class="math inline">\(\theta\)</span>: <span class="math display">\[
p(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta\right)
dP\left(  \theta\right)  \text{.}%
\]</span> To operationalize this result, the researcher needs to choose the likelihood function and the prior distribution of the parameters.</p>
<p>At first glance, de Finetti’s theorem may seem to suggest that there is a single model or likelihood function. This is not the case however, as models can be viewed in the same manner as parameters. Denoting a model specification by <span class="math inline">\(\mathcal{M}\)</span>, then de Finetti’s theorem would imply that <span class="math display">\[\begin{align*}
p(y_{1},\ldots,y_{T})  &amp;  =\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta ,\mathcal{M}\right)  p\left(  \theta \mid \mathcal{M}\right)  p\left(\mathcal{M}\right)  d\theta d\mathcal{M}\\
&amp;  =\int p(y_{1},\ldots,y_{T} \mid \mathcal{M})p\left(  \mathcal{M}\right)
d\mathcal{M}\text{,}%
\end{align*}\]</span> in the case of a continuum of models. Thus, under the mild assumption of exchangeability, it is <em>as if</em> the <span class="math inline">\(y_{t}\)</span>’<span class="math inline">\(s\)</span> are generated from <span class="math inline">\(p\left( y_{t} \mid \theta,\mathcal{M}\right)\)</span>, conditional on the random variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\mathcal{M}\)</span>, where <span class="math inline">\(p\left( \theta \mid \mathcal{M}\right)\)</span> are the beliefs over <span class="math inline">\(\theta\)</span> in model <span class="math inline">\(\mathcal{M}\)</span>, and <span class="math inline">\(p\left(\mathcal{M}_{j}\right)\)</span> are the beliefs over model specifications.</p>
<p>The objective approach has been a prevailing one in scientific applications. However, it only applies to events that can be repeated under the same conditions a very large number of times. This is rarely the case in many important applied problems. For example, it is hard to repeat an economic event, such as a Federal Reserve meeting or the economic conditions in 2008 infinitely often. This implies that at best, the frequentist approach is limited to laboratory situations. Even in scientific applications, when we attempt to repeat an experiment multiple times, an objective approach is not guaranteed to work. For example, the failure rate of phase 3 clinical trials in oncology is 60% (<span class="citation" data-cites="shen2021underperformance">Shen et al. (<a href="references.html#ref-shen2021underperformance" role="doc-biblioref">2021</a>)</span>,<span class="citation" data-cites="sun2022why">Sun et al. (<a href="references.html#ref-sun2022why" role="doc-biblioref">2022</a>)</span>). Prior to phase 3, the drug is usually tested on several hundred patients.</p>
<p>Subjective probability is a more general definition of probability than the frequentist definition, as it can be used for all types of events, both repeatable and unrepeatable events. A subjectivist has no problem discussing the probability a republican president will be re-elected in 2024, even though that event has never occurred before and cannot occur again. The main difficulty in operationalizing subjective probability is the process of actually quantifying subjective beliefs into numeric probabilities.</p>
<p>Instead of using repetitive experiments, subjective probabilities can be measured using betting odds, which have been used for centuries to gauge the uncertainty over an event. The probability attributed to winning a coin toss is revealed by the type of of odds one would accept to bet. Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.</p>
</section>
<section id="posterior-empirical-cdf" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="posterior-empirical-cdf"><span class="header-section-number">3.1.2</span> Posterior Empirical CDF</h3>
<p>Let <span class="math inline">\(\mathcal{M} = \{ f_\theta ( y ) : y \in \mathcal{Y} \}\)</span> be a model. When necessary we index the parameters in model <span class="math inline">\(m\)</span>, as <span class="math inline">\(\theta_m\)</span>. Let <span class="math inline">\(y = ( y_1 , \ldots , y_n )\)</span> be a vector of signals. The conditional likelihood, under <span class="math inline">\(m\)</span>, is given by <span class="math inline">\(f_\theta(y) =  \prod_{i=1}^n f_\theta ( y_i )\)</span>. We also allow for the possibility that the data is generated from a model <span class="math inline">\(f\)</span> that does not belong to the family of models <span class="math inline">\(f_\theta\)</span>.</p>
<p>Given a prior measure, <span class="math inline">\(\Pi ( d F )\)</span>, over <span class="math inline">\(\mathcal{F}\)</span> the set of distributions, we can calculate the predictive density <span class="math display">\[
f_n  ( y_{n+1} | y_1 , \ldots , y_n ) = \int f (y) \Pi_n ( d F ) \; {\rm where} \; \Pi_n ( d f ) = \frac{ \prod_{i=1}^n f( y_i ) \Pi( d f ) }{  \int  \prod_{i=1}^n f( y_i ) \Pi( d f ) }
\]</span> Under the family, $ f_<span class="math inline">\(, we can calculate the parameter posterior as\)</span>$ p( | y ) = ; {} ; m(y) = f_(y) p( ) d $$ Here <span class="math inline">\(p(\theta)\)</span> is a prior distribution over parameters and <span class="math inline">\(m(y)\)</span> is the marginal distribution of the data implied by the model. There are many applications in Bayesian non-parametrics statistics.</p>
</section>
</section>
<section id="sufficient-statistic-summary-statistic" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sufficient-statistic-summary-statistic"><span class="header-section-number">3.2</span> Sufficient Statistic (Summary Statistic)</h2>
<p>A statistic <span class="math inline">\(S(y)\)</span> is sufficient for <span class="math inline">\(\theta\)</span>, if the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(S(y)\)</span> is independent of <span class="math inline">\(\theta\)</span>, namely <span class="math display">\[
p(y\mid S(y),\theta) = p(y\mid S(y)).
\]</span> Then, we can view <span class="math inline">\(S(y)\)</span> as a dimension reducing map, as inference for <span class="math inline">\(\theta\)</span> can be solely determined by <span class="math inline">\(S\)</span>. This follows as <span class="math inline">\(S(y)\)</span> as a deterministic map of <span class="math inline">\(y\)</span>, so with <span class="math inline">\(P(S(y))&gt;0\)</span>, we have <span class="math display">\[
p(y,S(y)\mid \theta) = p(y\mid S(y),\theta)p(S(y)\mid \theta) = p(y\mid S(y))p(S(y)\mid \theta) \propto p(S(y)\mid\theta)
\]</span></p>
<p>In Bayesian inference, we need to compute the posterior over unknown model parameters <span class="math inline">\(\theta\)</span>, given data <span class="math inline">\(y\)</span>. The posterior density is denoted by <span class="math inline">\(p(\theta \mid y)\)</span>. Here <span class="math inline">\(y = ( y_1 , \ldots , y_n )\)</span> is high dimensional. A map from data to a real number or to a low-dimension vector <span class="math display">\[
S = S(y) = ( S_1(y) , \ldots , S_k(y) )
\]</span> is called a <em>statistic</em>. Since statistic is a deterministic function of the data, then <span class="math display">\[
p(y\mid \theta) = p(y,S\mid \theta) = p(S\mid \theta)p(y\mid S,\theta).
\]</span> If it happens that the likelihood is conditionally independent on <span class="math inline">\(\theta\)</span>, given <span class="math inline">\(S\)</span> then <span class="math display">\[
p(y\mid \theta) = p(S\mid \theta)p(y\mid S).
\]</span> In this case the statistic <span class="math inline">\(S\)</span> is called the <em>sufficient statistic</em> for parameter <span class="math inline">\(\theta\)</span> given data <span class="math inline">\(y\)</span>. In other words all the information needed for estimating <span class="math inline">\(\theta\)</span> is given by <span class="math inline">\(S\)</span>.</p>
<p>There is a nice connection between the posterior mean and the sufficient statistics, especially minimal sufficient statistics in the exponential family. If there exists a sufficient statistic <span class="math inline">\(S^*\)</span> for <span class="math inline">\(\theta\)</span>, then Kolmogorov (1942) shows that for almost every <span class="math inline">\(y\)</span>, <span class="math inline">\(p(\theta\mid y) = p(\theta\mid S^*(y))\)</span> , and further <span class="math inline">\(S(y) = E_{p}(\theta \mid y) = E_{p}(\theta \mid S^*(y))\)</span> is a function of <span class="math inline">\(S^*(y)\)</span>. In the special case of an exponential family with minimal sufficient statistic <span class="math inline">\(S^*\)</span> and parameter <span class="math inline">\(\theta\)</span>, the posterior mean <span class="math inline">\(S(y) = E_{p}(\theta \mid y)\)</span> is a one-to-one function of <span class="math inline">\(S^*(y)\)</span>, and thus is a minimal sufficient statistic.</p>
<p><strong>Summary Statistic</strong>: Let <span class="math inline">\(S(y)\)</span> is sufficient summary statistic in the Bayes sense (<span class="citation" data-cites="kolmogorov1942definition">Kolmogorov (<a href="references.html#ref-kolmogorov1942definition" role="doc-biblioref">1942</a>)</span>), if for every prior <span class="math inline">\(p\)</span> <span class="math display">\[
f_B (y) :=   p_{\theta \mid y}(\theta \in B\mid y) = p_{\theta \mid s(y)}(\theta \in B\mid s(y)).
\]</span> Then we need to use our pattern matching dataset <span class="math inline">\((y^{(i)} , \theta^{(i)})\)</span> which is simulated from the prior and forward model to “train” the set of functions <span class="math inline">\(f_B (y)\)</span>, where we pick the sets <span class="math inline">\(B = ( - \infty , q ]\)</span> for a quantile <span class="math inline">\(q\)</span>. Hence, we can then interpolate in between.</p>
<p>Estimating the full sequence of functions is then done by interpolating for all Borel sets <span class="math inline">\(B\)</span> and all new data points <span class="math inline">\(y\)</span> using a NN architecture and conditional density NN estimation.</p>
<p>The notion of a summary statistic is prevalent in the ABC literature and is tightly related to the notion of a Bayesian sufficient statistic <span class="math inline">\(S^*\)</span> for <span class="math inline">\(\theta\)</span>, then (Kolmogorov 1942), for almost every <span class="math inline">\(y\)</span>, <span class="math display">\[
p(\theta \mid  Y=y) = p(\theta \mid S^*(Y) = S^*(y))
\]</span> Furthermore, <span class="math inline">\(S(y) = \mathrm{E}\left(\theta \mid Y = y\right) = \mathrm{E}_{p}\left(\theta \mid S^*(Y) = S^*(y)\right)\)</span> is a function of <span class="math inline">\(S^*(y)\)</span>. In the case of exponential family, we have <span class="math inline">\(S(Y) = \mathrm{E}_{p}\left(\theta | Y \right)\)</span> is a one-to-one function of <span class="math inline">\(S^*(Y)\)</span>, and thus is a minimal sufficient statistic.</p>
<p>Sufficient statistics are generally kept for parametric exponential families, where <span class="math inline">\(S(\cdot)\)</span> is given by the specification of the probabilistic model. However, many forward models have an implicit likelihood and no such structures. The generalization of sufficiency is a summary statistics (a.k.a. feature extraction/selection in a neural network). Hence, we make the assumption that there exists a set of features such that the dimensionality of the problem is reduced.</p>
<div id="exm-coinposteror" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Posterior Distribution for Coin Toss)</strong></span> What if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants? In this case, we need to estimate the probability of heads <span class="math inline">\(\theta\)</span> from the data. Suppose we have observed 10 flips <span class="math display">\[
\{H, T, H, H, H, T, H, T, H, H\},
\]</span> and only three of them were tails. What is the probability that the next flip will be tail? The frequency-based answer would be <span class="math inline">\(3/10 = 0.3\)</span>. However, Bayes approach gives us more flexibility. Suppose we have a prior belief that the coin is fair, but we are not sure. We can model this belief by a prior distribution. Let discretize the variable <span class="math inline">\(\theta\)</span> and assign prior probabilities to each value of <span class="math inline">\(\theta\)</span> as follows</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.024</span>, <span class="fl">0.077</span>, <span class="fl">0.132</span>, <span class="fl">0.173</span>, <span class="fl">0.188</span>, <span class="fl">0.173</span>, <span class="fl">0.132</span>, <span class="fl">0.077</span>, <span class="fl">0.024</span>, <span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(prior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"prior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Prior distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>We put most of the mass to the fair assumption (<span class="math inline">\(\theta = 0.5\)</span>) and zero mass to the extreme values <span class="math inline">\(\theta = 0\)</span> and <span class="math inline">\(\theta = 1\)</span>. Our mass is exponentially decaying as we move away from 0.5. This is a reasonable assumption, since we are not sure about the fairness of the coin. Now, we can use Bayes rule to update our prior belief. The posterior distribution is given by <span class="math display">\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}.
\]</span> The denominator is the marginal likelihood, which is given by <span class="math display">\[
p(y) = \sum_{\theta} p(y \mid \theta) p(\theta).
\]</span> The likelihood is given by the Binomial distribution <span class="math display">\[
p(y \mid \theta) \propto \theta^3 (1 - \theta)^7.
\]</span> Notice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are <strong>sufficient</strong> for the inference about <span class="math inline">\(\theta\)</span>. The posterior distribution is given by</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, n, Y) {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  theta<span class="sc">^</span>Y <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>(n <span class="sc">-</span> Y)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(theta, <span class="dv">10</span>,<span class="dv">3</span>) <span class="sc">*</span> prior</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> posterior <span class="sc">/</span> <span class="fu">sum</span>(posterior) <span class="co"># normalize</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(posterior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"posterior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Posterior distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>If you are to keep collecting more observations and say observe a sequence of 100 flips, then the posterior distribution will be more concentrated around the value of <span class="math inline">\(\theta = 0.3\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(theta, <span class="dv">100</span>,<span class="dv">30</span>) <span class="sc">*</span> prior</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> posterior <span class="sc">/</span> <span class="fu">sum</span>(posterior) <span class="co"># normalize</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(posterior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"posterior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Posterior distribution for n=100</figcaption>
</figure>
</div>
</div>
</div>
<p>This demonstrates that for large sample sizes, the frequentist approach and Bayes approach agree.</p>
</div>
</section>
<section id="sec-betabinomial" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-betabinomial"><span class="header-section-number">3.3</span> Beta-Binomial Model</h2>
<p>The <strong>Beta-Binomial Bayesian model</strong> is a statistical model that is used when we are interested in learning about a proportion or probability of success, denoted by <span class="math inline">\(p\)</span>. This model is particularly useful when dealing with binary data such as conversions or clicks in A/B testing.</p>
<p>In the Beta-Binomial model, we assume that the probability of success <span class="math inline">\(\theta\)</span> in each of <span class="math inline">\(n\)</span> Bernoulli trials is not fixed but randomly drawn from a Beta distribution. The Beta distribution is defined by two shape parameters, <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>.</p>
<p>The model combines the prior information about <span class="math inline">\(\theta\)</span> (represented by the Beta distribution) and the observed data (represented by the Binomial distribution) to update our beliefs about <span class="math inline">\(p\)</span>. This is done using Bayes’ Rule, which in this context can be written as: <span class="math display">\[
p(\theta \mid Y) = \dfrac{p(Y \mid \theta)p(\theta)}{p(Y)}
\]</span> where <span class="math inline">\(p(\theta)\)</span> is the prior distribution (Beta), <span class="math inline">\(p(Y \mid \theta)\)</span> is the likelihood function (Binomial), and <span class="math inline">\(p(\theta\mid Y)\)</span> is the posterior distribution.</p>
<p>The Beta distribution is a family of continuous probability distributions defined on the interval [0,1] in terms of two positive parameters, denoted by alpha (<span class="math inline">\(\alpha\)</span>) and beta (<span class="math inline">\(\beta\)</span>), that appear as exponents of the variable and its complement to 1, respectively, and control the shape of the distribution. The Beta distribution is frequently used in Bayesian statistics, empirical Bayes methods, and classical statistics to model random variables with values falling inside a finite interval.</p>
<p>The probability density function (PDF) of the Beta distribution is given by: <span class="math display">\[
Beta(x; \alpha, \beta) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}
\]</span> where <span class="math inline">\(x \in [0, 1]\)</span>, <span class="math inline">\(\alpha &gt; 0\)</span>, <span class="math inline">\(\beta &gt; 0\)</span>, and <span class="math inline">\(B(\alpha, \beta)\)</span> is the beta function. It is simply a normalizing constant <span class="math display">\[
B\left( a,A\right)  =\int_{0}^{1}\theta^{a-1}\left(  1-\theta\right)
^{A-1}d\theta .
\]</span></p>
<p>The mean and variance of the Beta distribution are given by: <span class="math display">\[
\begin{aligned}
\mu &amp;= \frac{\alpha}{\alpha + \beta} \\
\sigma^2 &amp;= \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\end{aligned}
\]</span> where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>The Beta-Binomial model is one of the simplest Bayesian models and is widely used in various fields including epidemiology, intelligence testing, and marketing. It provides the tools we need to study the proportion of interest, <span class="math inline">\(p\)</span>, in a variety of settings.</p>
<p>The nice property of the Beta-Binomial model is that the posterior <span class="math inline">\(p(p\mid Y)\)</span> is yet another Beta distribution. Beta is called a <em>conjugate prior</em> for the Binomial likelihood and is a very useful property. Given that we observed <span class="math inline">\(x\)</span> successful outcome <span class="math display">\[
Y = \sum_{i=1}^n Y_i
\]</span> the posterior distribution is given by <span class="math display">\[
p(\theta\mid Y) =Beta(Y+\alpha, n-Y+\beta)
\]</span> Here the count of successful outcome <span class="math inline">\(Y\)</span> acts as a <em>sufficient statistic</em> for the parameter <span class="math inline">\(p\)</span>. This means that the posterior distribution depends on the data only through the sufficient statistic <span class="math inline">\(Y\)</span>. This is a very useful property and is a consequence of the conjugacy of the Beta prior and Binomial likelihood.</p>
<div id="exm-swan" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Black Swans)</strong></span> A related problem is the Black Swan inference problem. Suppose that after <span class="math inline">\(n\)</span> trials where <span class="math inline">\(n\)</span> is large you have only seen successes and that you assess the probability of the next trial being a success as <span class="math inline">\((T+1)/(T+2)\)</span> that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. <span class="citation" data-cites="taleb2007black">Taleb (<a href="references.html#ref-taleb2007black" role="doc-biblioref">2007</a>)</span> makes it sound as if the rules of probability are not rich enough to be able to handle Black Swan events. There is a related class of problems in finance known as <em>Peso problems</em> where countries decide to devalue their currencies and there is little a prior evidence from recent history that such an event is going to happen.</p>
<p>To obtain such a probability assessment we use a Binomial/Beta conjugate Bayes updating model. The key point is that it can also explain that there is still a large probability of a Black Swan event to happen <em>sometime</em> in the future. Independence model has difficulty doing this.</p>
<p>The Bayes Learning Beta-Binomial model will have no problem. We model with where <span class="math inline">\(Y_{t}=0\)</span> or <span class="math inline">\(1\)</span>, with probability <span class="math inline">\(P\left( Y_{t}=1\mid \theta\right) =\theta\)</span>. This is the classic Bernoulli “coin-flipping” model and is a component of more general specifications such as regime switching or outlier-type models.</p>
<p>Let <span class="math inline">\(Y = \sum_{t=1}^{T}y_{t}\)</span> be the number of observed successful outcomes. The likelihood for a sequence of Bernoulli observations is then <span class="math display">\[
p\left(  y\mid \theta\right)  =\prod_{t=1}^{T}p\left(  y_{t}\mid \theta\right)
=\theta^{Y}\left(  1-\theta\right)^{T-Y}.
\]</span> The maximum likelihood estimator is the sample mean, <span class="math inline">\(\widehat{\theta} = T^{-1}Y\)</span>. This makes little sense when you just observe white swans. It predicts <span class="math inline">\(\hat{\theta} = 1\)</span> and gets shocked when it sees a black swan (zero probability event). Bayes, on the other hand, allows for “learning”.</p>
<p>Bayes rule then tell us how to combine the likelihood and prior to obtain a posterior distribution, namely <span class="math inline">\(\theta \mid Y=y\)</span>. What do we believe about <span class="math inline">\(\theta\)</span> given a sequence of. Our predictor rule is then <span class="math inline">\(P(Y_{t=1} =1 \mid Y=y ) = \mathbb{E}(\theta \mid y)\)</span> it is straightforward to show that the posterior distribution is again a Beta distribution with <span class="math display">\[
p\left( \theta\mid y\right)  \sim Beta\left(  a_{T},A_{T}\right)  \; \mathrm{ and} \;  a_{T}=a+k , A_{T}=A+T-k.
\]</span></p>
<p>There is a “conjugate” form of the posterior: it is also a Beta distribution and the hyper-parameters <span class="math inline">\(a_{T}\)</span> and <span class="math inline">\(A_{T}\)</span> depend on the data only via the sufficient statistics, <span class="math inline">\(T\)</span> and <span class="math inline">\(k\)</span>. The posterior mean and variance are <span class="math display">\[
\mathbb{E}\left[ \theta\mid y\right]  =\frac{a_{T}}{a_{T}+A_{T}} \;\text{ and }\; \Var{
\theta\mid y}  =\frac{a_{T}A_{T}}{\left(  a_{T}+A_{T}\right)  ^{2}\left(   a_{T}+A_{T}+1\right)  }\text{,}
\]</span> respectively. This implies that for large samples, <span class="math inline">\(\E{\theta\mid y} \approx \bar{y} = \widehat{\theta}\)</span>, the MLE.</p>
<p>Suppose that after <span class="math inline">\(n\)</span> trials where <span class="math inline">\(n\)</span> is large you have only seen successes and that you assess the probability of the next trial being a success as <span class="math inline">\((T+1)/(T+2)\)</span> that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. (Taleb, 2008, The Black Swan: the Impact of the Highly Improbable).</p>
<p>To obtain such a probability assessment a natural model is Binomial/Beta conjugate Bayesian updating model. We can access the probability that a black Swan event to happen <em>sometime</em> in the future.</p>
<p>For the purpose of illustration, start with a uniform prior specification, <span class="math inline">\(\theta \sim \mathcal{U}(0,1)\)</span>, then we have the following probability assessment. After <span class="math inline">\(T\)</span> trials, suppose that we have only seen <span class="math inline">\(T\)</span> successes, namely, <span class="math inline">\(( y_1 , \ldots , y_T ) = ( 1 , \ldots , 1 )\)</span>. Then you assess the probability of the next trial being a success as <span class="math display">\[
p( Y_{T+1} =1 \mid y_1=1 , \ldots , y_T=1 ) = \frac{T+1}{T+2}
\]</span> This follows from the mean of the Beta posterior, <span class="math display">\[
\theta \mid y \sim \text{Beta}(T+1, T+1), ~ P(Y_{T+1} = 1 \mid y) = \mathbb{E}_{\theta \mid y}\left[P(Y_{T=1} \mid \theta) \right] = \mathbb{E}[\theta \mid y].
\]</span> For large <span class="math inline">\(T\)</span> this is almost certain.</p>
<p>Now consider a future set of <span class="math inline">\(n\)</span> trials, where <span class="math inline">\(n\)</span> is also large. The probability of <em>never</em> seeing a Black Swan is then given by <span class="math display">\[
p( y_{T+1} =1 , \ldots ,  y_{T+n} = 1 \mid y_1=1 , \ldots , y_T=1 ) = \frac{ T+1 }{ T+n+1 }
\]</span> For a fixed <span class="math inline">\(T\)</span>, and large <span class="math inline">\(n\)</span>, we have <span class="math inline">\(\frac{ T+1 }{ T+n+1 } \rightarrow 0\)</span>. Hence, we will see a Black Swan event with large probability — we just don’t know when! The exchangeable Beta-Binomial model then implies that a <em>Black Swan</em> event will eventually appear. One shouldn’t be that surprised when it actually happens.</p>
</div>
<div id="exm-normaltrials" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Clinical trials)</strong></span> Consider a problem of designing clinical trials in which <span class="math inline">\(K\)</span> possible drugs <span class="math inline">\(a\in 1,\dots,K\)</span> need to be tested. The outcome of the treatment with drug <span class="math inline">\(a\)</span> is binary <span class="math inline">\(y(a) \in \{0,1\}\)</span>. We use Bernoulli distribution with mean <span class="math inline">\(f(a)\)</span> to model the outcome. Thus, the full probabilistic model is described by <span class="math inline">\(w = f(1),\dots,f(K)\)</span>. Say we have observed a sample <span class="math inline">\(D = \{y_1,\dots,y_n\}\)</span>. We would like to compute posterior distribution over <span class="math inline">\(w\)</span>. We start with <span class="math inline">\(Beta\)</span> prior <span class="math display">\[
p(w\mid \alpha,\beta) = \prod_{a=1}^K Beta(w_a\mid \alpha,\beta)    
\]</span> Then posterior distribution is given by <span class="math display">\[
p(w\mid D) = \prod_{a=1}^K Beta(w_a\mid \alpha + n_{a,1},\beta + n_{a,0})   
\]</span></p>
<p>This setup allows us to perform sequential design of experiment. The simplest version of it is called the Thompson sampling. After observing <span class="math inline">\(n\)</span> patients, we draw a single sample <span class="math inline">\(\tilde w\)</span> from the posterior and then maximize the resulting surrogate <span class="math display">\[
a_{n+1} = \argmax_{a} f_{\tilde w}(a), ~~~ \tilde{w} \sim p(w\mid D)
\]</span></p>
</div>
<div id="exm-baseball" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 (Shrinkage and baseball batting averages)</strong></span> The batter-pitcher match-up is a fundamental element of a baseball game. There are detailed baseball records that are examined regularly by fans and professionals. This data provides a good illustration of Bayesian hierarchical methods. There is a great deal of prior information concerning the overall ability of a player. However, we only see a small amount of data about a particular batter-pitcher match-up. Given the relative small sample size, to determine our optimal estimator we build a hierarchical model taking into account the within pitcher variation.</p>
<p>Let’s analyze the variability in Jeter’s <span class="math inline">\(2006\)</span> season. Let <span class="math inline">\(p_{i}\)</span> denote Jeter’s ability against pitcher <span class="math inline">\(i\)</span> and assume that <span class="math inline">\(p_{i}\)</span> varies across the population of pitchers according to a particular probability distribution <span class="math inline">\((p_{i} \mid \alpha,\beta)\sim Be(\alpha,\beta)\)</span>. To account for extra-binomial variation we use a hierarchical model for the observed number of hits <span class="math inline">\(y_{i}\)</span> of the form <span class="math display">\[
(y_{i} \mid p_{i})\sim Bin(T_{i},p_{i})\;\;\mathrm{with}\;\;p_{i}\sim
Be(\alpha,\beta)
\]</span> where <span class="math inline">\(T_{i}\)</span> is the number of at-bats against pitcher <span class="math inline">\(i\)</span>. A priori we have a prior mean given by <span class="math inline">\(E(p_{i})=\alpha/(\alpha+\beta)=\bar{p}\)</span>. The extra heterogeneity leads to a prior variance <span class="math inline">\(Var(p_{i})=\bar{p}(1-\bar{p})\phi\)</span> where <span class="math inline">\(\phi=(\alpha+\beta+1)^{-1}\)</span>. Hence <span class="math inline">\(\phi\)</span> measures how concentrated the beta distribution is around its mean, <span class="math inline">\(\phi=0\)</span> means highly concentrated and <span class="math inline">\(\phi=1\)</span> means widely dispersed. <!-- where $T_{i}$ is the number of at-bats against pitcher $i$. A priori we have a prior mean given by $E(p_{i})=\alpha/(\alpha+\beta)=\bar{p}_{i}$. The extra heterogeneity leads to a prior variance $Var(p_{i})=\bar{p}_{i}(1-\bar{p}_{i})\phi$ where $\phi=(\alpha+\beta+1)^{-1}$. Hence $\phi$ measures how concentrated the beta distribution is around its mean, $\phi=0$ means highly concentrated and $\phi=1$ means widely dispersed. --></p>
<p>This model assumes that each player <span class="math inline">\(i\)</span> has a true ability <span class="math inline">\(p_{i}\)</span> that is drawn from a common distribution. The model is hierarchical in the sense that the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are estimated from the data. The model is also a shrinkage model in the sense that the estimates of <span class="math inline">\(p_{i}\)</span> are shrunk towards the overall mean <span class="math inline">\(\bar{p}_{i}\)</span>. In reality, we don’t know that each <span class="math inline">\(p_i\)</span> exists. We also don’t know if it follows a Binomial distribution with the Beta prior. We are making a model assumption. However, the model is a good approximation to the data and is a good way to estimate the parameters.</p>
<p><span class="citation" data-cites="stern2007inference">Stern et al. (<a href="references.html#ref-stern2007inference" role="doc-biblioref">2007</a>)</span> estimates the parameter <span class="math inline">\(\hat{\phi} = 0.002\)</span> for Derek Jeter, showing that his ability varies a bit but not very much across the population of pitchers. The effect of the shrinkage is not surprising. The extremes are shrunk the most with the highest degree of shrinkage occurring for the match-ups that have the smallest sample sizes. The amount of shrinkage is related to the large amount of prior information concerning Jeter’s overall batting average. Overall Jeter’s performance is extremely consistent across pitchers as seen from his estimates. Jeter had a season <span class="math inline">\(.308\)</span> average. We see that his Bayes estimates vary from<span class="math inline">\(.311\)</span> to<span class="math inline">\(.327\)</span> and that he is very consistent. If all players had a similar record then the assumption of a constant batting average would make sense.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Pitcher</th>
<th>At-bats</th>
<th>Hits</th>
<th>ObsAvg</th>
<th>EstAvg</th>
<th>95% Int</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>R. Mendoza</td>
<td>6</td>
<td>5</td>
<td>.833</td>
<td>.322</td>
<td>(.282, .394)</td>
</tr>
<tr class="even">
<td>H. Nomo</td>
<td>20</td>
<td>12</td>
<td>.600</td>
<td>.326</td>
<td>(.289, .407)</td>
</tr>
<tr class="odd">
<td>A.J.Burnett</td>
<td>5</td>
<td>3</td>
<td>.600</td>
<td>.320</td>
<td>(.275, .381)</td>
</tr>
<tr class="even">
<td>E. Milton</td>
<td>28</td>
<td>14</td>
<td>.500</td>
<td>.324</td>
<td>(.291, .397)</td>
</tr>
<tr class="odd">
<td>D. Cone</td>
<td>8</td>
<td>4</td>
<td>.500</td>
<td>.320</td>
<td>(.218, .381)</td>
</tr>
<tr class="even">
<td>R. Lopez</td>
<td>45</td>
<td>21</td>
<td>.467</td>
<td>.326</td>
<td>(.291, .401)</td>
</tr>
<tr class="odd">
<td>K. Escobar</td>
<td>39</td>
<td>16</td>
<td>.410</td>
<td>.322</td>
<td>(.281, .386)</td>
</tr>
<tr class="even">
<td>J. Wettland</td>
<td>5</td>
<td>2</td>
<td>.400</td>
<td>.318</td>
<td>(.275, .375)</td>
</tr>
<tr class="odd">
<td>T. Wakefield</td>
<td>81</td>
<td>26</td>
<td>.321</td>
<td>.318</td>
<td>(.279, .364)</td>
</tr>
<tr class="even">
<td>P. Martinez</td>
<td>83</td>
<td>21</td>
<td>.253</td>
<td>.312</td>
<td>(.254, .347)</td>
</tr>
<tr class="odd">
<td>K. Benson</td>
<td>8</td>
<td>2</td>
<td>.250</td>
<td>.317</td>
<td>(.264, .368)</td>
</tr>
<tr class="even">
<td>T. Hudson</td>
<td>24</td>
<td>6</td>
<td>.250</td>
<td>.315</td>
<td>(.260, .362)</td>
</tr>
<tr class="odd">
<td>J. Smoltz</td>
<td>5</td>
<td>1</td>
<td>.200</td>
<td>.314</td>
<td>(.253, .355)</td>
</tr>
<tr class="even">
<td>F. Garcia</td>
<td>25</td>
<td>5</td>
<td>.200</td>
<td>.314</td>
<td>(.253, .355)</td>
</tr>
<tr class="odd">
<td>B. Radke</td>
<td>41</td>
<td>8</td>
<td>.195</td>
<td>.311</td>
<td>(.247, .347)</td>
</tr>
<tr class="even">
<td>D. Kolb</td>
<td>5</td>
<td>0</td>
<td>.000</td>
<td>.316</td>
<td>(.258, .363)</td>
</tr>
<tr class="odd">
<td>J. Julio</td>
<td>13</td>
<td>0</td>
<td>.000</td>
<td>.312</td>
<td>(.243, .350 )</td>
</tr>
<tr class="even">
<td>Total</td>
<td>6530</td>
<td>2061</td>
<td>.316</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Some major league managers believe strongly in the importance of such data (Tony La Russa, <em>Three days in August</em>). One interesting example is the following. On Aug 29, 2006, Kenny Lofton (career <span class="math inline">\(.299\)</span> average, and current <span class="math inline">\(.308\)</span> average for <span class="math inline">\(2006\)</span> season) was facing the pitcher Milton (current record <span class="math inline">\(1\)</span> for <span class="math inline">\(19\)</span>). He was <em>rested</em> and replaced by a <span class="math inline">\(.273\)</span> hitter. Is putting in a weaker player rally a better bet? Was this just an over-reaction to bad luck in the Lofton-Milton match-up? Statistically, from Lofton’s record against Milton we have <span class="math inline">\(P\left( \leq 1\;\mathrm{hit\;in}\ 19\;\mathrm{attempts} \mid p=0.3\right) =0.01\)</span> an unlikely <span class="math inline">\(1\)</span>-in-<span class="math inline">\(100\)</span> event. However, we have not taken into account the multiplicity of different batter-pitcher match-ups. We know that Loftin’s batting percentage will vary across different pitchers, it’s just a question of how much? A hierarchical analysis of Lofton’s variability gave a <span class="math inline">\(\phi=0.008\)</span> – four times larger than Jeter’s <span class="math inline">\(\phi=0.002\)</span>. Lofton has batting estimates that vary from<span class="math inline">\(.265\)</span> to <span class="math inline">\(.340\)</span> with the lowest being against Milton. Hence, the optimal estimate for a pitch against Milton is <span class="math inline">\(.265&lt;.275\)</span> and resting Lofton against Milton is justified by this analysis.</p>
</div>
</section>
<section id="poisson-model-for-count-data" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="poisson-model-for-count-data"><span class="header-section-number">3.4</span> Poisson Model for Count Data</h2>
<p>The Poisson distribution is obtained as a result of the Binomial when <span class="math inline">\(p\)</span> is small and <span class="math inline">\(n\)</span> is large. In applications, the Poisson models count data. Suppose we want to model arrival rate of users to one of our stores. Let <span class="math inline">\(\lambda = np\)</span>, which is fixed and take the limit as <span class="math inline">\(n \rightarrow \infty\)</span>. There is a relationship between , <span class="math inline">\(p(x)\)</span> ans <span class="math inline">\(p(x+1)\)</span> given by <span class="math display">\[
\dfrac{p(x+1)}{p(x)}= \dfrac{\left(\dfrac{n}{x+1}\right)p^{x+1}(1-p)^{n-x-1}}{\left(\dfrac{n}{x}\right)p^{x}(1-p)^{n-x}} \approx \dfrac{np}{x+1}
\]</span> If we approximate <span class="math inline">\(p(x+1)\approx \lambda p(x)/(x+1)\)</span> with <span class="math inline">\(\lambda=np\)</span>, then we obtain the Poisson pdf given by <span class="math inline">\(p(x) = p(0)\lambda^x/x!\)</span>. To ensure that <span class="math inline">\(\sum_{x=0}^\infty p(x) = 1\)</span>, we set <span class="math display">\[
f(0) = \dfrac{1}{\sum_{x=0}^{\infty}\lambda^x/x!} = e^{-\lambda}.
\]</span> The above equality follows from the power series property of the exponent function <span class="math display">\[
e^{\lambda} = \sum_{x=0}^{\infty}\dfrac{\lambda^x}{x!}
\]</span> The <strong>Poisson distribution</strong> counts the occurrence of events. Given a rate parameter, denoted by <span class="math inline">\(\lambda\)</span>, we calculate probabilities as follows <span class="math display">\[
p( X = x ) = \frac{ e^{-\lambda} \lambda^x }{x!} \; \mathrm{ where} \; x=0,1,2,3, \ldots
\]</span> The mean and variance of the Poisson are given by:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Poisson Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = \lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = \lambda\)</span></td>
</tr>
</tbody>
</table>
<p>Here <span class="math inline">\(\lambda\)</span> denotes the rate of occurrence of an event.</p>
<p>Consider the problem of modeling soccer scores in the English Premier League (EPL) games. We use data from Betfair, a website, which posts odds on many football games. The goal is to calculate odds for the possible scores in a match. <span class="math display">\[
0-0, \; 1-0, \; 0-1, \; 1-1, \; 2-0, \ldots
\]</span> Another question we might ask, is what’s the odds of a team winning?</p>
<p>This is given by <span class="math inline">\(P\left ( X&gt; Y \right )\)</span>. The odds’s of a draw are given by <span class="math inline">\(P \left ( X = Y \right )\)</span>?</p>
<p>Professional sports betters rely on sophisticated statistical models to predict the outcomes. Instead, we present a simple, but useful model for predicting outcomes of EPL games. We follow the methodology given in <span class="citation" data-cites="spiegelhalter2009one">Spiegelhalter and Ng (<a href="references.html#ref-spiegelhalter2009one" role="doc-biblioref">2009</a>)</span>.</p>
<p>First, load the data and then model the number of goals scored using Poisson distribution.</p>
<p>sdcdsc</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/epl.csv"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(df[,<span class="fu">c</span>(<span class="st">"home_team_name"</span>,<span class="st">"away_team_name"</span>,<span class="st">"home_score"</span>,<span class="st">"guest_score"</span>)]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">home_team_name</th>
<th style="text-align: left;">away_team_name</th>
<th style="text-align: right;">home_score</th>
<th style="text-align: right;">guest_score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Arsenal</td>
<td style="text-align: left;">Liverpool</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bournemouth</td>
<td style="text-align: left;">Manchester United</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Burnley</td>
<td style="text-align: left;">Swansea</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chelsea</td>
<td style="text-align: left;">West Ham</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Crystal Palace</td>
<td style="text-align: left;">West Bromwich Albion</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Everton</td>
<td style="text-align: left;">Tottenham</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Let’s look at the empirical distribution across the number of goals scored by Manchester United</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>team_name<span class="ot">=</span><span class="st">"Manchester United"</span> </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>team_for  <span class="ot">=</span> <span class="fu">c</span>(df[df<span class="sc">$</span>home_team_name<span class="sc">==</span>team_name,<span class="st">"home_score"</span>],df[df<span class="sc">$</span>away_team_name<span class="sc">==</span>team_name,<span class="st">"guest_score"</span>]) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(team_for) </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>for_byscore <span class="ot">=</span> <span class="fu">table</span>(team_for)<span class="sc">/</span>n </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(for_byscore, <span class="at">col=</span><span class="st">"coral"</span>, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram of Goals Scored by MU</figcaption>
</figure>
</div>
</div>
</div>
<p>Hence the historical data fits closely to a Poisson distribution, the parameter <span class="math inline">\(\lambda\)</span> describes the average number of goals scored and we calculate it by calculating the sample mean, the maximum likelihood estimate. A Bayesian method where we assume that <span class="math inline">\(\lambda\)</span> has a Gamma prior is also available. This lets you incorporate outside information into the predictive model.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lambda_for <span class="ot">=</span> <span class="fu">mean</span>(team_for) </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">rbind</span>(<span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lambda =</span> lambda_for),for_byscore),<span class="at">beside =</span> T, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"aquamarine3"</span>,<span class="st">"coral"</span>), <span class="at">xlab=</span><span class="st">"Goals"</span>, <span class="at">ylab=</span><span class="st">"probability"</span>, <span class="at">main=</span><span class="st">""</span>) </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Poisson"</span>,<span class="st">"MU"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"aquamarine3"</span>, <span class="st">"coral"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram vs Poisson Model Prediction of Goals Scored by MU</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we will use Poisson model and Monte Carlo simulations to predict possible outcomes of the MU vs Hull games. First we estimate the rate parameter for goals by MU <code>lmb_mu</code> and goals by Hull <code>lmb_h</code>. Each team played a home and away game with every other team, thus 38 total games was played by all teams. We calculate the average by dividing total number of goals scored by the number of games</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sumdf <span class="ot">=</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(home_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">Goals_For_Home =</span> <span class="fu">sum</span>(home_score)) <span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(away_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_For_Away =</span> <span class="fu">sum</span>(guest_score)), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"home_team_name"</span> <span class="ot">=</span> <span class="st">"away_team_name"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(home_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_Against_Home =</span> <span class="fu">sum</span>(guest_score))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(away_team_name) <span class="sc">%&gt;%</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_Against_Away =</span> <span class="fu">sum</span>(home_score)), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"home_team_name"</span> <span class="ot">=</span> <span class="st">"away_team_name"</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">Team=</span>home_team_name)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(sumdf[sumdf<span class="sc">$</span>Team <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"Manchester United"</span>, <span class="st">"Hull"</span>),])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Team</th>
<th style="text-align: right;">Goals_For_Home</th>
<th style="text-align: right;">Goals_For_Away</th>
<th style="text-align: right;">Goals_Against_Home</th>
<th style="text-align: right;">Goals_Against_Away</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hull</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td style="text-align: left;">Manchester United</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">17</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>lmb_mu <span class="ot">=</span> (<span class="dv">26</span><span class="sc">+</span><span class="dv">28</span>)<span class="sc">/</span><span class="dv">38</span>; <span class="fu">print</span>(lmb_mu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lmb_h <span class="ot">=</span> (<span class="dv">28</span><span class="sc">+</span><span class="dv">9</span>)<span class="sc">/</span><span class="dv">38</span>; <span class="fu">print</span>(lmb_h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.97</code></pre>
</div>
</div>
<p>Now we simulate 100 games between the teams</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">100</span>,lmb_mu)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">100</span>,lmb_h)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x<span class="sc">&gt;</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 57</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x<span class="sc">==</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 18</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">table</span>(x,y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
<th style="text-align: right;">4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>From our simulation that 57 number of times MU wins and 18 there is a draw. The actual outcome was 0-0 (Hull at MU) and 0-1 (Mu at Hull). Thus our model fives a reasonable prediction.</p>
<p>The model can be improved by calculating different averages for home and away games. For example, Hull does much better at home games compared to away games. Further, we can include the characteristics of the opponent team to account for interactions between attack strength (number of scored) and defense weakness of the opponent. Now we modify our value of expected goals for each of the teams by calculating <span class="math display">\[
\hat \lambda = \lambda \times  \text{Defense weakness}
\]</span></p>
<p>Let’s model the MU at Hull game. The average away goals for MU <span class="math inline">\(28/19 = 1.47\)</span> and the defense weakness of Hull is <span class="math inline">\(36/19 = 1.84\)</span>, thus the adjusted expected number of goals to be scored by MU is 2.79. Similarly, the adjusted number of the goals Hull is expected to score is <span class="math inline">\(28/19 \times 17/19 = 1.32\)</span></p>
<p>As a result of the simulation, we obtain</p>
<div class="cell" data-out-heigth="7in" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">100</span>, <span class="dv">28</span> <span class="sc">/</span> <span class="dv">19</span> <span class="sc">*</span> <span class="dv">35</span> <span class="sc">/</span> <span class="dv">19</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">100</span>, <span class="dv">28</span> <span class="sc">/</span> <span class="dv">19</span> <span class="sc">*</span> <span class="dv">17</span> <span class="sc">/</span> <span class="dv">19</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">table</span>(x, y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
<th style="text-align: right;">4</th>
<th style="text-align: right;">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">7</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="at">z =</span> <span class="fu">table</span>(x, y), <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">7</span>, <span class="at">y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">xlab =</span> <span class="st">"MU Score"</span>, <span class="at">ylab =</span> <span class="st">"Hull Score"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now we can calculate the number of times MU wins:</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x <span class="sc">&gt;</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 67</code></pre>
</div>
</div>
<!--     Team      Expected Goals   0    1    2    3    4    5 -->
<!-- ----------- ---------------- ---- ---- ---- ---- ---- ---- -->
<!--     Man U          2.95        7    22   26   12   11   13 -->
<!--   Hull City        0.65        49   41   10   0    0    0 -->
<p>A model is only as good as its predictions. Let’s see how our model did in out-of-sample prediction,</p>
<ul>
<li>Man U wins 67 games out of 100, we should bet when odds ratio is below 67 to 100.</li>
<li>Most likely outcome is 1-2 (16 games out of 100)</li>
<li>The actual outcome was 0-1 (they played on August 27, 2016)</li>
<li>In out simulation 0-1 was the fourth most probable outcome (8 games out of 100).</li>
</ul>
<div id="exm-eplodds" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 (EPL Betting)</strong></span> Fen et al.&nbsp;(2016) employ a Skellam process (a difference of Poisson random variables) to model real-time betting odds for English Premier League (EPL) soccer games. Given a matrix of market odds on all possible score outcomes, we estimate the expected scoring rates for each team. The expected scoring rates then define the implied volatility of an EPL game. As events in the game evolve, they re-estimate the expected scoring rates and our implied volatility measure to provide a dynamic representation of the market’s expectation of the game outcome. They use real-time market odds data for a game between Everton and West Ham in the 2015-2016 season. We show how the implied volatility for the outcome evolves as goals, red cards, and corner kicks occur.</p>
<p>Gambling on soccer is a global industry with revenues of over $1 trillion a year (see “Football Betting - the Global Gambling Industry worth Billions,” BBC Sport). Betting on the result of a soccer match is a rapidly growing market, and online real-time odds exist (Betfair, Bet365, Ladbrokes). Market odds for all possible score outcomes (<span class="math inline">\(0-0, 1-0, 0-1, 2-0, \ldots\)</span>) as well as outright win, lose, and draw are available in real time. In this paper, we employ a two-parameter probability model based on a Skellam process and a non-linear objective function to extract the expected scoring rates for each team from the odds matrix. The expected scoring rates then define the implied volatility of the game.</p>
<p><strong>Skellam Process</strong></p>
<p>To model the outcome of a soccer game between team A and team B, we let the difference in scores, <span class="math inline">\(N(t) = N_A(t) - N_B(t)\)</span>, where <span class="math inline">\(N_A(t)\)</span> and <span class="math inline">\(N_B(t)\)</span> are the team scores at time point <span class="math inline">\(t\)</span>. Negative values of <span class="math inline">\(N(t)\)</span> indicate that team A is behind. We begin at <span class="math inline">\(N(0) = 0\)</span> and end at time one with <span class="math inline">\(N(1)\)</span> representing the final score difference. The probability <span class="math inline">\(\mathbb{P}(N(1) &gt; 0)\)</span> represents the ex-ante odds of team A winning. Half-time score betting, which is common in Europe, is available for the distribution of <span class="math inline">\(N(\frac{1}{2})\)</span>.</p>
<p>Then we find a probabilistic model for the distribution of <span class="math inline">\(N(1)\)</span> given <span class="math inline">\(N(t) = \ell\)</span>, where <span class="math inline">\(\ell\)</span> is the current lead. This model, together with the current market odds, can be used to infer the expected scoring rates of the two teams and then to define the implied volatility of the outcome of the match. We let <span class="math inline">\(\lambda^A\)</span> and <span class="math inline">\(\lambda^B\)</span> denote the expected scoring rates for the whole game. We allow for the possibility that the scoring abilities (and their market expectations) are time-varying, in which case we denote the expected scoring rates after time <span class="math inline">\(t\)</span> by <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, respectively, instead of <span class="math inline">\(\lambda^A(1-t)\)</span> and <span class="math inline">\(\lambda^B(1-t)\)</span>.</p>
<p>The Skellam distribution is defined as the difference between two independent Poisson variables given by:</p>
<p><span class="math display">\[
\begin{aligned}
N_A(t) &amp;= W_A(t) + W(t) \\
N_B(t) &amp;= W_B(t) + W(t)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(W_A(t)\)</span>, <span class="math inline">\(W_B(t)\)</span>, and <span class="math inline">\(W(t)\)</span> are independent processes with:</p>
<p><span class="math display">\[
W_A(t) \sim \text{Poisson}(\lambda^A t), \quad W_B(t) \sim \text{Poisson}(\lambda^B t).
\]</span></p>
<p>Here <span class="math inline">\(W(t)\)</span> is a non-negative integer-valued process to induce a correlation between the numbers of goals scored. By modeling the score difference, <span class="math inline">\(N(t)\)</span>, we avoid having to specify the distribution of <span class="math inline">\(W(t)\)</span> as the difference in goals scored is independent of <span class="math inline">\(W(t)\)</span>. Specifically, we have a Skellam distribution:</p>
<p><span id="eq-skellam"><span class="math display">\[
N(t) = N_A(t) - N_B(t) \sim \text{Skellam}(\lambda^A t, \lambda^B t).
\tag{3.2}\]</span></span></p>
<p>At time <span class="math inline">\(t\)</span>, we have the conditional distributions:</p>
<p><span class="math display">\[
\begin{aligned}
W_A(1) - W_A(t) &amp;\sim \text{Poisson}(\lambda^A(1-t)) \\
W_B(1) - W_B(t) &amp;\sim \text{Poisson}(\lambda^B(1-t)).
\end{aligned}
\]</span></p>
<p>Now letting <span class="math inline">\(N^*(1-t)\)</span>, the score difference of the sub-game which starts at time <span class="math inline">\(t\)</span> and ends at time 1 and the duration is <span class="math inline">\((1-t)\)</span>. By construction, <span class="math inline">\(N(1) = N(t) + N^*(1-t)\)</span>. Since <span class="math inline">\(N^*(1-t)\)</span> and <span class="math inline">\(N(t)\)</span> are differences of two Poisson process on two disjoint time periods, by the property of Poisson process, <span class="math inline">\(N^*(1-t)\)</span> and <span class="math inline">\(N(t)\)</span> are independent. Hence, we can re-express equation (<a href="#eq-skellam" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>) in terms of <span class="math inline">\(N^*(1-t)\)</span>, and deduce</p>
<p><span class="math display">\[
%N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \sim Skellam(\lambda^A (1-t),\lambda^B (1-t) )
N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \sim \text{Skellam}(\lambda^A_t,\lambda^B_t)
\]</span></p>
<p>where <span class="math inline">\(W^*_A(1-t) = W_A(1) - W_A(t)\)</span>, <span class="math inline">\(\lambda^A = \lambda^A_0\)</span> and <span class="math inline">\(\lambda^A_t=\lambda^A(1-t)\)</span>. A natural interpretation of the expected scoring rates, <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, is that they reflect the “net” scoring ability of each team from time <span class="math inline">\(t\)</span> to the end of the game. The term <span class="math inline">\(W(t)\)</span> models a common strength due to external factors, such as weather. The “net” scoring abilities of the two teams are assumed to be independent of each other as well as the common strength factor. We can calculate the probability of any particular score difference, given by <span class="math inline">\(\mathbb{P}(N(1)=x|\lambda^A,\lambda^B)\)</span>, at the end of the game where the <span class="math inline">\(\lambda\)</span>’s are estimated from the matrix of market odds. Team strength and “net” scoring ability can be influenced by various underlying factors, such as the offensive and defensive abilities of the two teams. The goal of our analysis is to only represent these parameters at every instant as a function of the market odds matrix for all scores.</p>
<p>Another quantity of interest is the conditional probability of winning as the game progresses. If the current lead at time <span class="math inline">\(t\)</span> is <span class="math inline">\(\ell\)</span>, and <span class="math inline">\(N(t)=\ell=N_A(t)-N_B(t)\)</span>, the Poisson property implied that the final score difference <span class="math inline">\((N(1)|N(t)=\ell)\)</span> can be calculated by using the fact that <span class="math inline">\(N(1)=N(t)+N^*(1-t)\)</span> and <span class="math inline">\(N(t)\)</span> and <span class="math inline">\(N^*(1-t)\)</span> are independent. Specifically, conditioning on <span class="math inline">\(N(t)=\ell\)</span>, we have the identity</p>
<p><span class="math display">\[ N(1)=N(t)+N^*(1-t)=\ell+\text{Skellam}(\lambda^A_t,\lambda^B_t). \]</span></p>
<p>We are now in a position to find the conditional distribution (<span class="math inline">\(N(1)=x|N(t)=\ell\)</span>) for every time point <span class="math inline">\(t\)</span> of the game given the current score. Simply put, we have the time homogeneous condition</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}(N(1)=x|\lambda^A_t,\lambda^B_t,N(t)=\ell) &amp;= \mathbb{P}(N(1)-N(t)=x-\ell |\lambda^A_t,\lambda^B_t,N(t)=\ell) \\
&amp;= \mathbb{P}(N^* (1-t)=x-\ell |\lambda^A_t,\lambda^B_t)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda^A_t\)</span>, <span class="math inline">\(\lambda^B_t\)</span>, <span class="math inline">\(\ell\)</span> are given by market expectations at time <span class="math inline">\(t\)</span>. See Feng et al.&nbsp;for details.</p>
<p><strong>Market Calibration</strong></p>
<p>Our information set at time <span class="math inline">\(t\)</span> includes the current lead <span class="math inline">\(N(t) = \ell\)</span> and the market odds for <span class="math inline">\(\{Win, Lose, Draw, Score\}_t\)</span>, where <span class="math inline">\(Score_t = \{ ( i - j ) : i, j = 0, 1, 2, ....\}\)</span>. These market odds can be used to calibrate a Skellam distribution which has only two parameters <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>. The best fitting Skellam model with parameters <span class="math inline">\(\{\hat\lambda^A_t,\hat\lambda^B_t\}\)</span> will then provide a better estimate of the market’s information concerning the outcome of the game than any individual market (such as win odds) as they are subject to a “vig” and liquidity. Suppose that the fractional odds for all possible final score outcomes are given by a bookmaker. In this case, the bookmaker pays out three times the amount staked by the bettor if the outcome is indeed 2-1. Fractional odds are used in the UK, while money-line odds are favored by American bookmakers with <span class="math inline">\(2:1\)</span> (“two-to-one”) implying that the bettor stands to make a $200 profit on a $100 stake. The market implied probability makes the expected winning amount of a bet equal to 0. In this case, the implied probability <span class="math inline">\(p=1/(1+3)=1/4\)</span> and the expected winning amount is <span class="math inline">\(\mu=-1*(1-1/4)+3*(1/4)=0\)</span>. We denote this odds as <span class="math inline">\(odds(2,1)=3\)</span>. To convert all the available odds to implied probabilities, we use the identity</p>
<p><span class="math display">\[ \mathbb{P}(N_A(1) = i, N_B(1) = j)=\frac{1}{1+odds(i,j)}. \]</span></p>
<p>The market odds matrix, <span class="math inline">\(O\)</span>, with elements <span class="math inline">\(o_{ij}=odds(i-1,j-1)\)</span>, <span class="math inline">\(i,j=1,2,3...\)</span> provides all possible combinations of final scores. Odds on extreme outcomes are not offered by the bookmakers. Since the probabilities are tiny, we set them equal to 0. The sum of the possible probabilities is still larger than 1 (see <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span> and <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span>). This “excess” probability corresponds to a quantity known as the “market vig.” For example, if the sum of all the implied probabilities is 1.1, then the expected profit of the bookmaker is 10%. To account for this phenomenon, we scale the probabilities to sum to 1 before estimation.</p>
<p>To estimate the expected scoring rates, <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, for the sub-game <span class="math inline">\(N^*(1-t)\)</span>, the odds from a bookmaker should be adjusted by <span class="math inline">\(N_A(t)\)</span> and <span class="math inline">\(N_B(t)\)</span>. For example, if <span class="math inline">\(N_A(0.5)=1\)</span>, <span class="math inline">\(N_B(0.5)=0\)</span> and <span class="math inline">\(odds(2,1)=3\)</span> at half time, these observations actually says that the odds for the second half score being 1-1 is 3 (the outcomes for the whole game and the first half are 2-1 and 1-0 respectively, thus the outcome for the second half is 1-1). The adjusted <span class="math inline">\({odds}^*\)</span> for <span class="math inline">\(N^*(1-t)\)</span> is calculated using the original odds as well as the current scores and given by</p>
<p><span class="math display">\[
{odds}^*(x,y)=odds(x+N_A(t),y+N_B(t)).
\]</span></p>
<p>At time <span class="math inline">\(t\)</span> <span class="math inline">\((0\leq t\leq 1)\)</span>, we calculate the implied conditional probabilities of score differences using odds information</p>
<p><span class="math display">\[
\mathbb{P}(N(1)=k|N(t)=\ell)=\mathbb{P}(N^*(1-t)=k-\ell)=\frac{1}{c}\sum_{i-j=k-\ell}\frac{1}{1+{odds}^*(i,j)}\]</span></p>
<p>where <span class="math inline">\(c=\sum_{i,j} \frac{1}{1+{odds}^*(i,j)}\)</span> is a scale factor, <span class="math inline">\(\ell=N_A(t)-N_B(t)\)</span>, <span class="math inline">\(i,j\geq 0\)</span> and <span class="math inline">\(k=0,\pm 1,\pm 2\ldots\)</span>.</p>
<p><strong>Example: Everton vs West Ham (3/5/2016)</strong></p>
<p>Table below shows the implied Skellam probabilities.</p>
<div id="tbl-Table1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-Table1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Table: Original odds data from Ladbrokes before the game started.
</figcaption>
<div aria-describedby="tbl-Table1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Everton &nbsp;West Ham</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>11/1</td>
<td>12/1</td>
<td>28/1</td>
<td>66/1</td>
<td>200/1</td>
<td>450/1</td>
</tr>
<tr class="even">
<td>1</td>
<td>13/2</td>
<td>6/1</td>
<td>14/1</td>
<td>40/1</td>
<td>100/1</td>
<td>350/1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>7/1</td>
<td>7/1</td>
<td>14/1</td>
<td>40/1</td>
<td>125/1</td>
<td>225/1</td>
</tr>
<tr class="even">
<td>3</td>
<td>11/1</td>
<td>11/1</td>
<td>20/1</td>
<td>50/1</td>
<td>125/1</td>
<td>275/1</td>
</tr>
<tr class="odd">
<td>4</td>
<td>22/1</td>
<td>22/1</td>
<td>40/1</td>
<td>100/1</td>
<td>250/1</td>
<td>500/1</td>
</tr>
<tr class="even">
<td>5</td>
<td>50/1</td>
<td>50/1</td>
<td>90/1</td>
<td>150/1</td>
<td>400/1</td>
<td></td>
</tr>
<tr class="odd">
<td>6</td>
<td>100/1</td>
<td>100/1</td>
<td>200/1</td>
<td>250/1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>7</td>
<td>250/1</td>
<td>275/1</td>
<td>375/1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>8</td>
<td>325/1</td>
<td>475/1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Table <a href="#tbl-Table1" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows the raw data of odds right the game. We need to transform odds data into probabilities. For example, for the outcome 0-0, 11/1 is equivalent to a probability of 1/12. Then we can calculate the marginal probability of every score difference from -4 to 5. We neglect those extreme scores with small probabilities and rescale the sum of event probabilities to one.</p>
<div id="tbl-Table2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-Table2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Market implied probabilities for the score differences versus Skellam implied probabilities at different time points. The estimated parameters <span class="math inline">\(\hat\lambda^A=2.33\)</span>, <span class="math inline">\(\hat\lambda^B=1.44\)</span>.
</figcaption>
<div aria-describedby="tbl-Table2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Score difference</th>
<th>-4</th>
<th>-3</th>
<th>-2</th>
<th>-1</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Market Prob. (%)</td>
<td>1.70</td>
<td>2.03</td>
<td>4.88</td>
<td>12.33</td>
<td>21.93</td>
<td>22.06</td>
<td>16.58</td>
<td>9.82</td>
<td>4.72</td>
<td>2.23</td>
</tr>
<tr class="even">
<td>Skellam Prob. (%)</td>
<td>0.78</td>
<td>2.50</td>
<td>6.47</td>
<td>13.02</td>
<td>19.50</td>
<td>21.08</td>
<td>16.96</td>
<td>10.61</td>
<td>5.37</td>
<td>2.27</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Table <a href="#tbl-Table2" class="quarto-xref">Table&nbsp;<span>3.2</span></a> shows the model implied probability for the outcome of score differences before the game, compared with the market implied probability. As we see, the Skellam model appears to have longer tails. Different from independent Poisson modeling in <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span>, our model is more flexible with the correlation between two teams. However, the trade-off of flexibility is that we only know the probability of score difference instead of the exact scores.</p>
<div id="fig-game2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-game2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/game2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-game2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: The betting market data for Everton and West Ham is from <a href="ladbrokes.com">ladbrokes.com</a>. Market implied probabilities (expressed as percentages) for three different results (Everton wins, West Ham wins and draw) are marked by three distinct colors, which vary dynamically as the game proceeds. The solid black line shows the evolution of the implied volatility. The dashed line shows significant events in the game, such as goals and red cards. Five goals in this game are 13’ Everton, 56’ Everton, 78’ West Ham, 81’ West Ham and 90’ West Ham.
</figcaption>
</figure>
</div>
<p>Figure <a href="#fig-game2" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> examines the behavior of the two teams and represent the market predictions on the final result. Notably, we see the probability change of win/draw/loss for important events during the game: goals scoring and a red card penalty. In such a dramatic game, the winning probability of Everton gets raised to 90% before the first goal of West Ham in 78th minutes. The first two goals scored by West Ham in the space of 3 minutes completely reverses the probability of winning. The probability of draw gets raised to 90% until we see the last-gasp goal of West Ham that decides the game.</p>
<p>Figure <a href="#fig-game2" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> plots the path of implied volatility throughout the course of the game. Instead of a downward sloping line, we see changes in the implied volatility as critical moments occur in the game. The implied volatility path provides a visualization of the conditional variation of the market prediction for the score difference. For example, when Everton lost a player by a red card penalty at 34th minute, our estimates <span class="math inline">\(\hat\lambda^A_t\)</span> and <span class="math inline">\(\hat\lambda^B_t\)</span> change accordingly. There is a jump in implied volatility and our model captures the market expectation adjustment about the game prediction. The change in <span class="math inline">\(\hat\lambda_A\)</span> and <span class="math inline">\(\hat\lambda_B\)</span> are consistent with the findings of <span class="citation" data-cites="vecer2009estimating">Vecer, Kopriva, and Ichiba (<a href="references.html#ref-vecer2009estimating" role="doc-biblioref">2009</a>)</span> where the scoring intensity of the penalized team drops while the scoring intensity of the opposing team increases. When a goal is scored in the 13th minute, we see the increase of <span class="math inline">\(\hat\lambda^B_t\)</span> and the market expects that the underdog team is pressing to come back into the game, an effect that has been well-documented in the literature. Another important effect that we observe at the end of the game is that as goals are scored (in the 78th and 81st minutes), the markets expectation is that the implied volatility increases again as one might expect.</p>
<div id="fig-ivcompare" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ivcompare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/iv2.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ivcompare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Red line: the path of implied volatility throughout the game, i.e., <span class="math inline">\(\sigma_{t}^{red} = \sqrt{\hat\lambda^A_t+\hat\lambda^B_t}\)</span>. Blue lines: the path of implied volatility with constant <span class="math inline">\(\lambda^A+\lambda^B\)</span>, i.e., <span class="math inline">\(\sigma_{t}^{blue} = \sqrt{(\lambda^A+\lambda^B)*(1-t)}\)</span>. Here <span class="math inline">\((\lambda^A+\lambda^B) = 1, 2, ..., 8\)</span>.
</figcaption>
</figure>
</div>
<div id="tbl-lambda" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: The calibrated <span class="math inline">\(\{\hat\lambda^A_t, \hat\lambda^B_t\}\)</span> divided by <span class="math inline">\((1-t)\)</span> and the implied volatility during the game. <span class="math inline">\(\{\lambda^A_t, \lambda^B_t\}\)</span> are expected goals scored for rest of the game. The less the remaining time, the less likely to score goals. Thus <span class="math inline">\(\{\hat\lambda^A_t, \hat\lambda^B_t\}\)</span> decrease as <span class="math inline">\(t\)</span> increases to 1. Diving them by <span class="math inline">\((1-t)\)</span> produces an updated version of <span class="math inline">\(\hat\lambda_{0}\)</span>’s for the whole game, which are in general time-varying (but not decreasing necessarily).
</figcaption>
<div aria-describedby="tbl-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>t</th>
<th>0</th>
<th>0.11</th>
<th>0.22</th>
<th>0.33</th>
<th>0.44</th>
<th>0.50</th>
<th>0.61</th>
<th>0.72</th>
<th>0.83</th>
<th>0.94</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat\lambda^A_t/(1-t)\)</span></td>
<td>2.33</td>
<td>2.51</td>
<td>2.53</td>
<td>2.46</td>
<td>1.89</td>
<td>1.85</td>
<td>2.12</td>
<td>2.12</td>
<td>2.61</td>
<td>4.61</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat\lambda^B_t/(1-t)\)</span></td>
<td>1.44</td>
<td>1.47</td>
<td>1.59</td>
<td>1.85</td>
<td>2.17</td>
<td>2.17</td>
<td>2.56</td>
<td>2.90</td>
<td>3.67</td>
<td>5.92</td>
<td>0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\((\hat\lambda^A_t+\hat\lambda^B_t)/(1-t)\)</span></td>
<td>3.78</td>
<td>3.98</td>
<td>4.12</td>
<td>4.31</td>
<td>4.06</td>
<td>4.02</td>
<td>4.68</td>
<td>5.03</td>
<td>6.28</td>
<td>10.52</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma_{IV,t}\)</span></td>
<td>1.94</td>
<td>1.88</td>
<td>1.79</td>
<td>1.70</td>
<td>1.50</td>
<td>1.42</td>
<td>1.35</td>
<td>1.18</td>
<td>1.02</td>
<td>0.76</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Figure <a href="#fig-ivcompare" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> compares the updating implied volatility of the game with implied volatilities of fixed <span class="math inline">\((\lambda^A+\lambda^B)\)</span>. At the beginning of the game, the red line (updating implied volatility) is under the “(<span class="math inline">\(\lambda^A+\lambda^B=4)\)</span>”-blue line; while at the end of the game, it’s above the “(<span class="math inline">\(\lambda^A+\lambda^B=8)\)</span>”-blue line. As we expect, the value of <span class="math inline">\((\hat\lambda^A_t + \hat\lambda^B_t)/(1-t)\)</span> in Table <a href="#tbl-lambda" class="quarto-xref">Table&nbsp;<span>3.3</span></a> increases throughout the game, implying that the game became more and more intense and the market continuously updates its belief in the odds.</p>
</div>
</section>
<section id="poisson-gamma-learning-about-a-poisson-intensity" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="poisson-gamma-learning-about-a-poisson-intensity"><span class="header-section-number">3.5</span> Poisson-Gamma: Learning about a Poisson Intensity</h2>
<p>Consider a continuous-time stochastic process, <span class="math inline">\(\left\{ N_{t}\right\} _{t\geq0}\)</span>, with <span class="math inline">\(N_{0}=0\)</span>, counting the number of events that have occurred up to time <span class="math inline">\(t\)</span>. The process is constant between event times, and jumps by one at event times: <span class="math inline">\(\Delta N_{t}=N_{t}-N_{t-}=1,\)</span> where <span class="math inline">\(N_{t-}\)</span> is the limit from the left. The probability of an event over the next short time interval, <span class="math inline">\(\Delta t\)</span> is <span class="math inline">\(\lambda\Delta t\)</span>, and <span class="math inline">\(N_{t}\)</span> is called a Poisson process because <span class="math display">\[
P\left[  N_{t}=k\right]  =\frac{e^{-\lambda t}\left(  \lambda
t\right)  ^{k}}{k!}\text{ for }k=1,\ldots
\]</span> which is the Poisson distribution, thus <span class="math inline">\(N_{t}\sim Poi\left(\lambda t\right)\)</span>. A more general version of the Poisson process is a Cox process, or doubly stochastic point process.</p>
<p>Here, there is additional conditioning information in the form of state variables, <span class="math inline">\(\left\{X_{t}\right\}_{t&gt;0}\)</span>. The process now has two sources of randomness, one associated with the discontinuous jumps and another in the form of random state variables, <span class="math inline">\(\left\{X_{t}\right\}_{t&gt;0}\)</span>, that drive the intensity of the process. The intensity of the Cox process is <span class="math inline">\(\lambda_{t}=\int_{0}^{t}\lambda\left( X_{s}\right) ds\)</span>, which is formally defined as <span class="math display">\[
P\left[  N_{t}-N_{s}=k \mid \left\{  X_{u}\right\}  _{s\leq u\leq
t}\right]  =\frac{\left(  \int_{s}^{t}\lambda\left(  X_{s}\right)  ds\right)
^{k}\exp\left(  -\int_{s}^{t}\lambda\left(  X_{s}\right)  ds\right)}{k!}, ~ k=0,1,\ldots
\]</span> Cox processes are very useful extensions to Poisson processes and are the basic building blocks of reduced form models of defaultable bonds.</p>
<p>The inference problem is to learn about <span class="math inline">\(\lambda\)</span> from a continuous-record of observation up to time <span class="math inline">\(t\)</span>. The likelihood function is given by <span class="math display">\[
p\left(  N_{t}=k \mid \lambda\right)  =\frac{\left(  \lambda t\right)  ^{k}%
\exp\left(  -\lambda t\right)  }{k!},
\]</span> and the MLE is <span class="math inline">\(\widehat{\lambda}=N_{t}/t\)</span>. The MLE has the unattractive property that prior to the first event <span class="math inline">\(\left\{ t:N_{t}=0\right\}\)</span>, the MLE is 0, despite the fact that the model explicitly assumes that events are possible. This problem often arises in credit risk contexts, where it would seem odd to assume that the probability of default is zero just because a default has not yet occurred.</p>
<p>A natural prior for this model is the Gamma distribution, which has the following pdf <span id="eq-gamma-pdf"><span class="math display">\[
p\left(  \lambda \mid a,A\right)  =\frac{A^{a}}{\Gamma(a)  }\lambda^{a-1}\exp\left(  -A\lambda\right)  \text{.}
\tag{3.3}\]</span></span> Like the beta distribution, a Gamma prior distribution allows for a variety of prior shapes and is parameterized by two hyperparameters. Combining the prior and likelihood, the posterior is also Gamma: <span class="math display">\[
p\left(  \lambda \mid N_{t}\right)  \propto\frac{\left(  \lambda\right)
^{N_{t}+a-1}\exp\left(  -\lambda\left(  t+A\right)  \right)  }{N_{t}!}%
\sim\mathcal{G}\left(  a_{t},A_{t}\right)  ,
\]</span> where <span class="math inline">\(a_{t}=N_{t}+a\)</span> and <span class="math inline">\(A_{t}=t+A\)</span>. The expected intensity, based on information up to time <span class="math inline">\(t\)</span>, is <span class="math display">\[
\mathbb{E}\left[  \lambda \mid N_{t}\right]  =\frac{a_{t}}{A_{t}}=\frac{N_{t}%
+a}{t+A}=w_{t}\frac{N_{t}}{t}+\left(  1-w_{t}\right)  \frac{a}{A},
\]</span> where the second line expresses the posterior mean in shrinkage form as a weighted average of the MLE and the prior mean where <span class="math inline">\(w_{t}=t/(t+A)\)</span>. In large samples, <span class="math inline">\(w_{t}\rightarrow1\)</span> and <span class="math inline">\(E\left( \lambda \mid N_{t}\right) \approx N_{t}/t=\widehat{\lambda}\)</span>.</p>
<p>To understand the updating mechanics, <a href="#fig-poiss" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> (right column) displays a simulated sample path, posterior means, and (5%,95%) posterior quantiles for various prior configurations. In this case, time is measured in years and the intensity used to simulate the data is <span class="math inline">\(\lambda=1\)</span>, implying on average one event per year. The four prior configurations embody different beliefs. In the first case, in the middle left panel, <span class="math inline">\(a=4\)</span> and <span class="math inline">\(A=1\)</span>, captures a high-activity prior, that posits that jumps occur, on average, four times per year, and there is substantial prior uncertainty over the arrival rate as the (5%,95%) prior quantiles are (1.75,6.7). In the second case, captures a prior that is centered over the true value with modest prior uncertainty. The third case captures a low-activity prior, with a prior mean of 0.2 jumps/year. The fourth case captures a dogmatic prior, that posits that jumps occur three times per year, with high confidence in these beliefs.</p>
<p>The priors were chosen to highlight different potential paths for Bayesian learning. The first thing to note from the priors is the discontinuity upward at event times, and the exponential decrease during periods of no events, both of which are generic properties of Bayesian learning in this model. If one thinks of the events as rare, this implies rapid revisions in beliefs at event times and a constant drop in estimates of the intensity in periods of no events. For the high-activity prior and the sample path observed, the posterior begins well above <span class="math inline">\(\lambda=1\)</span>, and slowly decreases, getting close to <span class="math inline">\(\lambda=1\)</span> at the end of the sample. This can be somewhat contrasted with the low-activity prior, which has drastic revisions upward at jump times. In the dogmatic case, there is little updating at event times. The prior parameters control how rapidly beliefs change, with noticeable differences across the priors.</p>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>) <span class="co"># Ovi</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>lmb <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">5</span>,t<span class="sc">*</span>lmb)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># A: rate (beta), a: shape (alpha)</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plotgamma <span class="ot">=</span> <span class="cf">function</span>(a,A,N) {</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="fu">dgamma</span>(x,a,A),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">"t"</span>,<span class="at">ylab=</span><span class="st">"Gamma(t)"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    at <span class="ot">=</span> N<span class="sc">+</span>a</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">=</span> t<span class="sc">+</span>A</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    mean <span class="ot">=</span> at<span class="sc">/</span>At</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(N, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">"orange"</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="at">xlab=</span><span class="st">"t"</span>, <span class="at">ylab=</span><span class="st">"N(t)"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(mean, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">qgamma</span>(<span class="fl">0.05</span>,at,At), <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">qgamma</span>(<span class="fl">0.95</span>,at,At), <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">4</span>,<span class="at">A=</span><span class="dv">1</span>, N)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">1</span>,<span class="at">A=</span><span class="dv">1</span>, N)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">1</span>,<span class="at">A=</span><span class="dv">5</span>, N)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">30</span>,<span class="at">A=</span><span class="dv">10</span>, N)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-poiss" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poiss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-1.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) a = 4, A = 1
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-2.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-3.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) a = 1, A = 1
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-4.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-5.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(e) a = 1, A = 5
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-6" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-6.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(f) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-7" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-7.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(g) a = 30, A = 10
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-8" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-poiss-8.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(h) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poiss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Sensitivity of Gamma Prior for Poisson Process
</figcaption>
</figure>
</div>
<p>Poisson event models are often embedded as portion of more complicated model to capture rare events such as stock market crashes, volatility surges, currency revaluations, or defaults. In these cases, prior distributions are often important–even essential–since it is common to build models with events that could, but have not yet occurred. These events are often called ‘Peso’ events. For example, in the case of modeling corporate defaults a researcher wants to allow for a jump to default. This requires positing a prior distribution that places non-zero probability on an event occurring. Classical statistical methods have difficulties dealing with these situations since the MLE of the jump probability is zero, until the first event occurs.</p>
</section>
<section id="normal-normal-model-for-continuous-data" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="normal-normal-model-for-continuous-data"><span class="header-section-number">3.6</span> Normal-Normal Model for Continuous Data</h2>
<p>The Normal or Gaussian distribution is central to probability and statistical inference. Suppose that we are trying to predict tomorrow’s return on the S&amp;P500. There’s a number of questions that come to mind</p>
<ol type="1">
<li><p>What is the random variable of interest?</p></li>
<li><p>How can we describe our uncertainty about tomorrow’s outcome?</p></li>
<li><p>Instead of listing all possible values we’ll work with intervals instead. The probability of an interval is defined by the area under the probability density function.</p></li>
</ol>
<p>Returns are are continuous (as opposed to discrete) random variables. Hence a normal distribution would be appropriate - but on what scale? We will see that on the log-scale a Normal distribution provides a good approximation.</p>
<p>The most widely used model for a continuous random variable is the normal distribution. Standard normal random variable <span class="math inline">\(Z\)</span> has the following properties</p>
<p>The standard Normal has mean <span class="math inline">\(0\)</span> and has a variance <span class="math inline">\(1\)</span>, and is written as <span class="math display">\[
Z \sim N(0,1)
\]</span> Then, we have the probability statements of interest <span class="math display">\[\begin{align*}  
P(-1 &lt;Z&lt; 1) &amp;=0.68\\
P(-1.96 &lt;Z&lt; 1.96) &amp;=0.95\\
\end{align*}\]</span></p>
<p>In <code>R</code>, we can find probabilities</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.96</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.98</code></pre>
</div>
</div>
<p>and quantiles</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.9750</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 2</code></pre>
</div>
</div>
<p>The quantile function <code>qnorm</code> is the inverse of <code>pnorm</code>.</p>
<p>A random variable that follows normal distribution with general mean and variance <span class="math inline">\(X \sim \mbox{N}(\mu, \sigma^2)\)</span>, has the following properties <span class="math display">\[\begin{align*}
  p(\mu - 2.58 \sigma &lt; X &lt; \mu + 2.58 \sigma) &amp;=&amp; 0.99 \\
  p(\mu - 1.96 \sigma &lt; X &lt; \mu + 1.96 \sigma) &amp;=&amp; 0.95 \, .
\end{align*}\]</span> The chance that <span class="math inline">\(X\)</span> will be within <span class="math inline">\(2.58 \sigma\)</span> of its mean is <span class="math inline">\(99\%\)</span>, and the chance that it will be within <span class="math inline">\(2\sigma\)</span> of its mean is about <span class="math inline">\(95\%\)</span>.</p>
<p>The probability model is written <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> is the mean, <span class="math inline">\(\sigma^2\)</span> is the variance. This can be transformed to a standardized normal via <span class="math display">\[
Z =\frac{X-\mu}{\sigma} \sim N(0,1).
\]</span> For a normal distribution, we know that <span class="math inline">\(X \in [\mu-1.96\sigma,\mu+1.96\sigma]\)</span> with probability 95%. We can make similar claims for any other distribution using the Chebyshev’s empirical rule, which is valid for any population:</p>
<ol type="1">
<li><p>At least 75% probability lies within 2<span class="math inline">\(\sigma\)</span> of the mean <span class="math inline">\(\mu\)</span></p></li>
<li><p>At least 89% lies within 3<span class="math inline">\(\sigma\)</span> of the mean <span class="math inline">\(\mu\)</span></p></li>
<li><p>At least <span class="math inline">\(100(1-1/m^2)\)</span>% lies within <span class="math inline">\(m\times \sigma\)</span> of the mean <span class="math inline">\(\mu\)</span>.</p></li>
</ol>
<p>This also holds true for the Normal distribution. The percentages are <span class="math inline">\(95\)</span>%, <span class="math inline">\(99\)</span>% and <span class="math inline">\(99.99\)</span>%.</p>
<div id="exm-googlestock" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6 (Google Stock 2019)</strong></span> Consider observations of daily log-returns of a Google stock for 2019 Daily log-return on day <span class="math inline">\(t\)</span> is calculated by taking a logarithm of the ratio of price at close of day <span class="math inline">\(t\)</span> and at close of day <span class="math inline">\(t-1\)</span> <span class="math display">\[
  y_t = \log\left(\dfrac{P_t}{P_{t-1}}\right)
\]</span> For example on January 3 of 2017, the open price is 778.81 and close price was 786.140, then the log-return is <span class="math inline">\(\log(786.140/778.81) =  -0.0094\)</span>. It was empirically observed that log-returns follow a Normal distribution. This observation is a basis for Black-Scholes model with is used to evaluate future returns of a stock.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/GOOG2019.csv"</span>)<span class="sc">$</span>Adj.Close; n <span class="ot">=</span> <span class="fu">length</span>(p) </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">log</span>(p[<span class="dv">2</span><span class="sc">:</span>n]<span class="sc">/</span>p[<span class="dv">1</span><span class="sc">:</span>(n<span class="dv">-1</span>)]) </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(r, <span class="at">breaks=</span><span class="dv">30</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="03-bl_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Observations on the far right correspond to the days when positive news was released and on the far left correspond to bad news. Typically, those are days when the quarterly earnings reports are released.</p>
<p>To estimate the expected value <span class="math inline">\(\mu\)</span> (return) and standard deviation <span class="math inline">\(\sigma\)</span> (a measure of risk), we simply calculate their sample counterparts <span class="math display">\[
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i, ~\mathrm{ and }~    s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x} )^2
\]</span> The empirical (or sample) values <span class="math inline">\(\bar x\)</span> and <span class="math inline">\(s^2\)</span> are called sample mean and sample variance. Here simply vie them as our best guess about the mean and variance of the normal distribution model then our probabilistic model for next day’s return is then given by <span class="math display">\[
R \sim N(\bar x, s^2).
\]</span></p>
<p>Say we are interested in investing into Google and would like to calculated the expected return of our investment as well as risk associated with this investment We assume that behavior of the returns in the future will be the same as in 2019.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(r) </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>rbar <span class="ot">=</span> <span class="fu">sum</span>(r)<span class="sc">/</span>n; <span class="fu">print</span>(rbar) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.00098</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">=</span> <span class="fu">sum</span>((r<span class="sc">-</span>rbar)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="dv">-1</span>); <span class="fu">print</span>(s2) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.00023</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.08</span>,<span class="fl">0.08</span>, <span class="at">length.out =</span> <span class="dv">200</span>) </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(r, <span class="at">breaks=</span><span class="dv">30</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">freq =</span> F, <span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">""</span>) </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">dnorm</span>(x,rbar,<span class="fu">sqrt</span>(s2)), <span class="at">col=</span><span class="st">"red"</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="03-bl_files/figure-html/normal-fit-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram (blue) and fitted normal curve (red) of for the Google stock daily return data.</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, assume, I invest all my portfolio into Google. I can predict my annual return to be <span class="math inline">\(251 \times 9.8\times 10^{-4}\)</span> = 0.25 and risk (volatility) of my investment is <span class="math inline">\(\sqrt{s^2}\)</span> = 0.02% a year.</p>
<p>I can predict the risk of loosing 3% or more in one day using my model is 1.93%.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fu">log</span>(<span class="dv">1</span><span class="fl">-0.03</span>), rbar, <span class="fu">sqrt</span>(s2))<span class="sc">*</span><span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.9</code></pre>
</div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>sp <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/SPMonthly.csv"</span>)<span class="sc">$</span>Adj.Close; n <span class="ot">=</span> <span class="fu">length</span>(sp) </span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>spret <span class="ot">=</span> sp[<span class="dv">602</span><span class="sc">:</span>n]<span class="sc">/</span>sp[<span class="dv">601</span><span class="sc">:</span>(n<span class="dv">-1</span>)]<span class="sc">-</span><span class="dv">1</span> <span class="co"># Calculate  1977-1987 returns </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(spret) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.012</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(spret)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.043</code></pre>
</div>
</div>
</div>
</section>
<section id="normal-with-unknown-mean" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="normal-with-unknown-mean"><span class="header-section-number">3.7</span> Normal With Unknown Mean</h2>
<p>Let <span class="math inline">\(Y\)</span> be a random variable with a normal distribution, <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>. The mean <span class="math inline">\(\mu\)</span> is unknown, but the variance <span class="math inline">\(\sigma^2\)</span> is known. The likelihood function is given by <span class="math display">\[
p(y \mid \mu) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-\mu)^2\right)
\]</span> The MLE of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\hat{\mu} = \bar{y}\)</span>, the sample mean. Normal prior for the mean parameter <span class="math inline">\(\mu\)</span> is conjugate to the normal likelihood. <span class="math display">\[
\mu \sim N(\mu_0, \sigma_0^2)
\]</span> The posterior distribution is also normal. <span class="math display">\[
p(\mu \mid y) \sim N(\mu_n, \sigma_n^2)
\]</span></p>
<p>where <span class="math display">\[
\mu_n = \frac{\sigma^2}{n\sigma_0^2 + \sigma^2}\mu_0 + \frac{n\sigma_0^2}{n\sigma_0^2 + \sigma^2}\bar{y}
\]</span> and <span class="math display">\[
\sigma_n^2 = \frac{\sigma^2\sigma_0^2}{n\sigma_0^2 + \sigma^2}
\]</span> The posterior mean is a weighted average of the prior mean and the sample mean, with the weights being proportional to the precision of the prior and the likelihood. The posterior variance is smaller than the prior variance, and the sample size <span class="math inline">\(n\)</span> appears in the denominator. The posterior mean is a shrinkage estimator of the sample mean, and the amount of shrinkage is controlled by the prior variance <span class="math inline">\(\sigma_0^2\)</span>. A couple of observations <span class="math display">\[
\frac{\sigma^2}{n\sigma_0^2 + \sigma^2} \rightarrow 0 \text{ and } \frac{n\sigma_0^2}{n\sigma_0^2 + \sigma^2}\rightarrow 1, \text{ as } n \rightarrow \infty.
\]</span> Further, <span class="math display">\[
\frac{\sigma^2\sigma_0^2}{n\sigma_0^2 + \sigma^2} \rightarrow 0 \text{ as } n \rightarrow \infty.
\]</span></p>
<div id="exm-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7 (Stylezed Example)</strong></span> Assuming the prior distribution <span class="math inline">\(\mu \sim N(-1,1)\)</span>, say we observed <span class="math inline">\(y=2\)</span> and we want to update our beliefs about <span class="math inline">\(\mu\)</span>. The likelihood function is <span class="math inline">\(p(y \mid \mu) = N(\mu,2)\)</span>, and the posterior distribution is <span class="math display">\[
p(\mu \mid y) \propto p(y \mid \mu) p(\mu) = N(y\mid \mu,2) N(\mu\mid -1,1) = N(-0.4,0.9).
\]</span></p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>sigma0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>mu1 <span class="ot">=</span> (mu0<span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> ybar<span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>sigma1 <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"Posterior mean: %f, Posterior variance: %f"</span>, mu1, sigma1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "Posterior mean: -0.400000, Posterior variance: 0.894427"</code></pre>
</div>
</div>
<p>Graphically we can represent this as follows</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The prior distribution </span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu,<span class="fu">dnorm</span>(mu,mu0,sigma0),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">"x"</span>,<span class="at">ylab=</span><span class="st">"p(x)"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>))</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The likelihood function</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(y,<span class="fu">dnorm</span>(y,ybar,sigma),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The posterior distribution</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(y,<span class="fu">dnorm</span>(y,mu1,sigma1),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"green"</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co"># legend</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Prior"</span>,<span class="st">" Data (Likelihood)"</span>,<span class="st">"Posterior"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-kalman" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kalman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-kalman-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kalman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Norm-Norm Updating
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note, the posterior mean is in between those of prior and likelihood and posterior variance is lower than variance of both prior and likelihood, this is effect of combining information from data and prior!</p>
</div>
<p>More generally, when we observe <span class="math inline">\(n\)</span> independent and identically distributed (i.i.d.) data points <span class="math inline">\(y_1,\ldots,y_n\)</span> from a normal distribution with known variance <span class="math inline">\(\sigma^2\)</span>, the likelihood function is given by <span class="math display">\[
p(y \mid \mu) = N(\bar y\mid \mu,\sigma^2/n),~ \text{where}~ \bar y = \frac{1}{n}\sum_{i=1}^n y_i.
\]</span> Note, that average over the observed data <span class="math inline">\(\bar y = \mathrm{Ave}(y_1,\ldots,y_n)\)</span> is the sufficient statistics for the mean <span class="math inline">\(\mu\)</span>. The prior distribution is given by <span class="math display">\[
p(\mu) = N(\mu\mid \mu_0,\sigma_0^2)
\]</span> The posterior distribution is given by <span class="math display">\[
\begin{split}
p(\mu\mid y)
&amp; \propto  \exp\Bigg[{\frac{-\mu^2+2\mu\mu_0-\mu_0^2}{2\sigma_0^2}}\Bigg]\exp\Bigg[{\frac{-\mu^2+2\mu\bar{y}-\bar{y}^2}{2\sigma^2/n}}\Bigg] \\
&amp; \propto  \exp\Bigg[{\frac{-\mu^2+2\mu\mu_0}{2\sigma_0^2}}\Bigg]\exp\Bigg[{\frac{-\mu^2+2\mu\bar{y}}{2\sigma^2/n}}\Bigg]. \\
\end{split}
\]</span> Now we combine the terms <span class="math display">\[
\begin{split}
p(\mu\mid y)
&amp; \propto  \exp\Bigg[{\frac{(-\mu^2+2\mu\mu_0)\sigma^2 +(-\mu^2+2\mu\bar{y})n\sigma_0^2}{2\sigma_0^2\sigma^2}}\Bigg]. \\
\end{split}
\]</span> Now re-arrange and combine <span class="math inline">\(\mu^2\)</span> and <span class="math inline">\(\mu\)</span> terms <span class="math display">\[
\begin{split}
p(\mu\mid y)
&amp; \propto  \exp\Bigg[{\frac{-\mu^2(n\sigma_0^2+\sigma^2)+2\mu(\mu_0\sigma^2+ \bar{y}n\sigma_0^2) }{2\sigma_0^2\sigma^2}}\Bigg] \\
&amp; \propto  \exp\Bigg[{\frac{-\mu^2+2\mu\left(\frac{\mu_0\sigma^2 + \bar{y}n\sigma_0^2}{n\sigma_0^2+\sigma^2}\right) }{2(\sigma_0^2\sigma^2) /(n\sigma_0^2+\sigma^2)}}\Bigg]. \\
\end{split}
\]</span> Now we add constants which do not depend upon <span class="math inline">\(\mu\)</span> to complete the square in the numerator: <span class="math display">\[
\begin{split}
p(\mu\mid y)
&amp; \propto  \exp\Bigg[{\frac{-\bigg(\mu - \frac{\mu_0\sigma^2 + \bar{y}n\sigma_0^2}{n\sigma_0^2+\sigma^2}\bigg)^2 }{2(\sigma_0^2\sigma^2) /(n\sigma_0^2+\sigma^2)}}\Bigg]. \\
\end{split}
\]</span> Finally we get the posterior mean <span class="math display">\[
\mu_n = \frac{\mu_0\sigma^2+ \bar{y}n\sigma_0^2}{n\sigma_0^2+\sigma^2} = \mu_0\frac{\sigma^2}{n\sigma_0^2+\sigma^2} + \bar{y}\frac{n\sigma_0^2}{n\sigma_0^2+\sigma^2}
\]</span> and the posterior variance <span class="math display">\[
\sigma_n^2 = \frac{\sigma_0^2\sigma^2}{n\sigma_0^2+\sigma^2}.
\]</span></p>
<div id="exm-bears" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8 (Chicago Bears 2014-2015 Season)</strong></span> The Chicago Bears are a professional American football team based in Chicago, Illinois. The Bears were a young team in 2014-2015, an were last in the their division. This season the Chicago Bears suffered back-to-back <span class="math inline">\(50\)</span>-points defeats and lost to Patriots and Packers.</p>
<ul>
<li>Patriots-Bears <span class="math inline">\(51-23\)</span></li>
<li>Packers-Bears <span class="math inline">\(55-14\)</span></li>
</ul>
<p>Their next game was at home against the Minnesota Vikings. Current line against the Vikings was <span class="math inline">\(-3.5\)</span> points. Slightly over a field goal. What’s the Bayes approach to learning the line? We use hierarchical data and Bayes learning to update our beliefs in light of new information. The current average win/lose this year can be modeled as a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. We assume that <span class="math inline">\(\mu\)</span> is normally distributed with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\tau\)</span>. <span class="math display">\[\begin{align*}
\bar{y} \mid \mu &amp; \sim N \left ( \mu , \frac{\sigma^2}{n} \right ) \sim N \left ( \mu , \frac{18.34^2}{9} \right )\\
\mu &amp; \sim N( 0 , \tau^2 )
\end{align*}\]</span> Here <span class="math inline">\(n =9\)</span> games so far. With <span class="math inline">\(s = 18.34\)</span> points. We assume the pre-season prior mean <span class="math inline">\(\mu_0 = 0\)</span>, standard deviation <span class="math inline">\(\tau = 4\)</span>. Base on the observed data so-far: <span class="math inline">\(\bar{y} = -9.22\)</span>.</p>
<p>The Bayes Shrinkage estimator is then <span class="math display">\[
\mathbb{E} \left( \mu \mid \tau, \bar y  \right) = \frac{ \tau^2 }{ \tau^2 + \frac{\sigma^2}{n} }\bar{y} .
\]</span></p>
<p>The shrinkage factor is <span class="math inline">\(0.3\)</span>! That’s quite a bit of shrinkage. Why? Our updated estimator is <span class="math display">\[
\mathbb{E} \left ( \mu | \bar{y} , \tau \right ) = - 2.75 &gt; -.3.5
\]</span> where current line is <span class="math inline">\(-3.5\)</span>.</p>
<ul>
<li>Based on our hierarchical model this is an over-reaction. One point change on the line is about <span class="math inline">\(3\)</span>% on a probability scale.</li>
<li>Alternatively, calculate a market-based <span class="math inline">\(\tau\)</span> given line <span class="math inline">\(=-3.5\)</span>. <span class="math display">\[
\tau^2 = \frac{\sigma^2}{n} \frac{1}{0.3^2} = 18.34^2 \frac{1}{0.3^2} = 180.
\]</span></li>
<li>The market-based <span class="math inline">\(\tau\)</span> is <span class="math inline">\(13.4\)</span> points.</li>
</ul>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>bears<span class="ot">=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="sc">-</span><span class="dv">21</span>,<span class="sc">-</span><span class="dv">7</span>,<span class="dv">14</span>,<span class="sc">-</span><span class="dv">13</span>,<span class="sc">-</span><span class="dv">28</span>,<span class="sc">-</span><span class="dv">41</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">mean</span>(bears))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> -9.2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">sd</span>(bears))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 18</code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>tau<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>sig2<span class="ot">=</span><span class="fu">sd</span>(bears)<span class="sc">*</span><span class="fu">sd</span>(bears)<span class="sc">/</span><span class="dv">9</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tau<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(sig2<span class="sc">+</span>tau<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fl">0.29997</span><span class="sc">*-</span><span class="fl">9.22</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> -2.8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">pnorm</span>(<span class="sc">-</span><span class="fl">2.76</span><span class="sc">/</span><span class="dv">18</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.44</code></pre>
</div>
</div>
<p>Home advantage is worth <span class="math inline">\(3\)</span> points. The actual result of the game is Bears 21, Vikings 13.</p>
</div>
<p><strong>Posterior Predictive</strong><br>
The posterior predictive distribution is the distribution of a new observation <span class="math inline">\(y_{n+1}\)</span> given the observed data <span class="math inline">\(y_1,\ldots,y_n\)</span>. The posterior predictive distribution is given by <span class="math display">\[
p(y_{n+1} \mid y_1,\ldots,y_n) = \int p(y_{n+1} \mid \mu) p(\mu \mid y_1,\ldots,y_n) d\mu = \int N(y_{n+1} \mid \mu, \sigma^2) N(\mu \mid \mu_n, \sigma_n^2) d\mu = N(y_{n+1} \mid \mu_n, \sigma_n^2 + \sigma^2).
\]</span> This follows from the general properties of the Gaussian distribution</p>
</section>
<section id="normal-with-unknown-variance" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="normal-with-unknown-variance"><span class="header-section-number">3.8</span> Normal With Unknown Variance</h2>
<p>Consider, another example, when mean <span class="math inline">\(\mu\)</span> is fixed and variance is a random variable which follows some distribution <span class="math inline">\(\sigma^2 \sim p(\sigma^2)\)</span>. Given an observed sample <span class="math inline">\(y\)</span>, we can update the distribution over variance using the Bayes rule <span class="math display">\[
p(\sigma^2 \mid  y) = \dfrac{p(y\mid \sigma^2 )p(\sigma^2)}{p(y)}.
\]</span> Now, the total probability in the denominator can be calculated as <span class="math display">\[
p(y) = \int p(y\mid \sigma^2 )p(\sigma^2) d\sigma^2.
\]</span></p>
<p>A conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. Thus, if <span class="math display">\[
\sigma^2 \mid  \alpha,\beta \sim IG(\alpha,\beta) = \dfrac{\beta^{\alpha}}{\Gamma(\alpha)}\sigma^{2(-\alpha-1)}\exp\left(-\dfrac{\beta}{\sigma^2}\right)
\]</span> and <span class="math display">\[
y \mid \mu,\sigma^2 \sim N(\mu,\sigma^2)
\]</span> Then the posterior distribution is another inverse Gamma <span class="math inline">\(IG(\alpha_{\mathrm{posterior}},\beta_{\mathrm{posterior}})\)</span>, with <span class="math display">\[
\alpha_{\mathrm{posterior}} = \alpha + 1/2, ~~\beta_{\mathrm{posterior}} = \beta + \dfrac{y-\mu}{2}.
\]</span></p>
<p>Now, the predictive distribution over <span class="math inline">\(y\)</span> can be calculated by <span class="math display">\[
p(y_{new}\mid y) = \int p(y_{new},\sigma^2\mid y)p(\sigma^2\mid y)d\sigma^2.
\]</span> Which happens to be a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(2\alpha_{\mathrm{posterior}}\)</span> degrees of freedom, mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\alpha_{\mathrm{posterior}}/\beta_{\mathrm{posterior}}\)</span>.</p>
<section id="the-normal-gamma-model" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="the-normal-gamma-model"><span class="header-section-number">3.8.1</span> The Normal-Gamma Model</h3>
<p>Now, consider the case when both mean and variance are unknow. To simplify the formulas, we will use precision <span class="math inline">\(\rho = 1/\sigma^2\)</span> instead of variance <span class="math inline">\(\sigma^2\)</span>. The normal-Gamma distribution is a conjugate prior for the normal distribution, when we do not know the precision and the mean. Given the observed data <span class="math inline">\(Y  = \{y_1,\ldots,y_n\}\)</span>, we assume normal likelihood <span class="math display">\[
y_i \mid \theta, \rho \sim N(\theta, 1/\rho)
\]</span></p>
<p>The normal-gamma prior distribution is defined as <span class="math display">\[
\theta\mid \mu,\rho,\nu \sim N(\mu, 1/(\rho \nu)), \quad \rho \mid \alpha, \beta \sim \text{Gamma}(\alpha, \beta).
\]</span> Thus, <span class="math inline">\(1/\rho\)</span> has inverse-Gamma distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Conditional on <span class="math inline">\(\rho\)</span>, the mean <span class="math inline">\(\theta\)</span> has normal distribution with mean <span class="math inline">\(\mu\)</span> and precision <span class="math inline">\(\nu\rho\)</span>. Notice that in this model the mean <span class="math inline">\(\theta\)</span> and precision <span class="math inline">\(\rho\)</span> are not independent. When the precision of observations <span class="math inline">\(\rho\)</span> is low, we are also less certain about the mean. However, when <span class="math inline">\(\nu=0\)</span>, we have an improper uniform distribution over <span class="math inline">\(\theta\)</span>, that is independent of <span class="math inline">\(\rho\)</span>. There is no conjugate distribution for <span class="math inline">\(\theta,\rho\)</span> in which <span class="math inline">\(\theta\)</span> is independent of <span class="math inline">\(\rho\)</span>. Given the normal likelihood <span class="math display">\[
p(y\mid \theta, \rho) = \left(\dfrac{\rho}{2\pi}\right)^{1/2}\exp\left(-\dfrac{\rho}{2}\sum_{i=1}^n(y_i-\theta)^2\right)
\]</span> and the normal-gamma prior <span class="math display">\[
p(\theta, \rho \mid \mu,\nu,\alpha,\beta) = \dfrac{\beta^\alpha}{\Gamma(\alpha)}\nu\rho^{\alpha-1}\exp(-\beta\rho)\left(\dfrac{\nu\rho}{2\pi}\right)^{1/2}\exp\left(-\dfrac{\nu\rho}{2}(\theta-\mu)^2\right)
\]</span> the posterior distribution is given by <span class="math display">\[
p(\theta, \rho\mid y) \propto p(y\mid \theta, \rho)p(\theta, \rho).
\]</span> The posterior distribution is a normal-Gamma distribution with parameters <span class="math display">\[
\begin{aligned}
\mu_n &amp;= \dfrac{\nu\mu + n\bar{y}}{\nu+n},\\
\nu_n &amp;= \nu+n,\\
\alpha_n &amp;= \alpha + \dfrac{n}{2},\\
\beta_n &amp;= \beta + \dfrac{1}{2}\sum_{i=1}^n(y_i-\bar{y})^2 + \dfrac{n\nu}{2(\nu+n)}(\bar{y}-\mu)^2.
\end{aligned}
\]</span> where <span class="math inline">\(\bar{y} = n^{-1}\sum_{i=1}^n y_i\)</span> is the sample mean and <span class="math inline">\(n\)</span> is the sample size. The posterior distribution is a normal-Gamma distribution with parameters <span class="math inline">\(\mu_n, \nu_n, \alpha_n, \beta_n\)</span>.</p>
</section>
<section id="credible-intervals-for-normal-gamma-model-posterior-parameters" class="level3" data-number="3.8.2">
<h3 data-number="3.8.2" class="anchored" data-anchor-id="credible-intervals-for-normal-gamma-model-posterior-parameters"><span class="header-section-number">3.8.2</span> Credible Intervals for Normal-Gamma Model Posterior Parameters</h3>
<p>The precission posterior follows a Gamma distribution with parameters <span class="math inline">\(\alpha_n, \beta_n\)</span>, thus we can use quantiles of the Gamma distribution to calculate credible intervals. A symmetric <span class="math inline">\(100(1-c)%\)</span> credible interval <span class="math inline">\([g_{c/2},g_{1-c/2}]\)</span> is given by <span class="math inline">\(c/2\)</span> and <span class="math inline">\(1-c/2\)</span> quantiles of the gamma distrinution. To find credible intterval for the variance <span class="math inline">\(v = 1/\rho\)</span>, we simply use <span class="math display">\[
[1/g_{1-c/2},1/g_{c/2}].
\]</span> and for standard deviation <span class="math inline">\(s = \sqrt{v}\)</span> we use <span class="math display">\[
[\sqrt{1/g_{1-c/2}},\sqrt{1/g_{c/2}}].
\]</span> To find credible interval over the mean <span class="math inline">\(\theta\)</span>, we need to integrate out the precision <span class="math inline">\(\rho\)</span> from the posterior distribution. The marginal distribution of <span class="math inline">\(\theta\)</span> is a Student’s t-distribution with parameters center at <span class="math inline">\(\mu_n\)</span>, variance <span class="math inline">\(\beta_n/(\nu_n\alpha_n)\)</span> and degrees of freedom <span class="math inline">\(2\alpha_n\)</span>.</p>
</section>
</section>
<section id="multivariate-normal" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="multivariate-normal"><span class="header-section-number">3.9</span> Multivariate Normal</h2>
<p>In the multivariate case, the normal-normal model is <span class="math display">\[
\theta \sim N(\mu_0,\Sigma_0), \quad y \mid \theta \sim N(\theta,\Sigma).
\]</span> The posterior distribution is <span class="math display">\[
\theta \mid y \sim N(\mu_1,\Sigma_1),
\]</span> where <span class="math display">\[
\Sigma_1 = (\Sigma_0^{-1} + \Sigma^{-1})^{-1}, \quad \mu_1 = \Sigma_1(\Sigma_0^{-1}\mu_0 + \Sigma^{-1}y).
\]</span> The predictive distribution is <span class="math display">\[
y_{new} \mid y \sim N(\mu_1,\Sigma_1 + \Sigma).
\]</span></p>
<div id="exm-portfolio" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9 (Satya Nadella: CEO of Microsoft)</strong></span> In 2014, Satya Nadella became the CEO of Microsoft. The stock price of Microsoft has been on a steady rise since then. Suppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.</p>
<p>Suppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&amp;P500 index and tracks overall market performance. We are interested in estimating the mean returns of the positions in our portfolio. You believe that the returns are normally distributed and are related to each other. You have prior beliefs about these returns, which are also normally distributed. We will use what is called the empirical prior for the mean returns. This is a prior that is based on historical data. The empirical prior is a good choice when you have a lot of historical data and you believe that the future mean returns will be similar to the historical mean returns. We assume the prior for the mean returns is a bivariate normal distribution, let <span class="math inline">\(\mu_0 = (\mu_{M}, \mu_{S})\)</span> represent the prior mean returns for the stocks. The covariance matrix <span class="math inline">\(\Sigma_0\)</span> captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. We will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix. The prior covariance matrix is given by <span class="math display">\[
\Sigma_0 = \begin{bmatrix} \sigma_{M}^2 &amp; \sigma_{MS} \\ \sigma_{MS} &amp; \sigma_{S}^2 \end{bmatrix},
\]</span> where <span class="math inline">\(\sigma_{M}^2\)</span> and <span class="math inline">\(\sigma_{S}^2\)</span> are the sample variances of the historical returns of MSFT and SPY, respectively, and <span class="math inline">\(\sigma_{MS}\)</span> is the sample covariance of the historical returns of MSFT and SPY. The prior mean is given by <span class="math display">\[
\mu_0 = \begin{bmatrix} \mu_{M} \\ \mu_{S} \end{bmatrix},
\]</span> where <span class="math inline">\(\mu_{M}\)</span> and <span class="math inline">\(\mu_{S}\)</span> are the sample means of the historical returns of MSFT and SPY, respectively. The likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns <span class="math inline">\(\mu = [\mu_A, \mu_B]\)</span>. The covariance matrix <span class="math inline">\(\Sigma\)</span> of the likelihood represents the uncertainty in your data. We will use the sample mean and covariance matrix of the observed returns as the likelihood mean and covariance matrix. The likelihood covariance matrix is given by <span class="math display">\[
\Sigma = \begin{bmatrix} \sigma_{M}^2 &amp; \sigma_{MS} \\ \sigma_{MS} &amp; \sigma_{S}^2 \end{bmatrix},
\]</span> where <span class="math inline">\(\sigma_{M}^2\)</span> and <span class="math inline">\(\sigma_{S}^2\)</span> are the sample variances of the observed returns of MSFT and SPY, respectively, and <span class="math inline">\(\sigma_{MS}\)</span> is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by <span class="math display">\[
\mu = \begin{bmatrix} \mu_{M} \\ \mu_{S} \end{bmatrix},
\]</span> where <span class="math inline">\(\mu_{M}\)</span> and <span class="math inline">\(\mu_{S}\)</span> are the sample means of the observed returns of MSFT and SPY, respectively. In a Bayesian framework, you update your beliefs (prior) about the mean returns using the observed data (likelihood). The posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution. The mean <span class="math inline">\(\mu_{\text{post}}\)</span> and covariance <span class="math inline">\(\Sigma_{\text{post}}\)</span> of the posterior are calculated using Bayesian updating formulas, which involve <span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\Sigma_0\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\Sigma\)</span>.</p>
<p>We use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014. Thus, our observed data includes July 2015 - Dec 2023 period. We assume the likelihood of observing this data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns. The covariance matrix <span class="math inline">\(Sigma\)</span> of the likelihood represents the uncertainty in your data and is calculated from the overall observed returns data 2001-2023.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">getSymbols</span>(<span class="fu">c</span>(<span class="st">"MSFT"</span>, <span class="st">"SPY"</span>), <span class="at">from =</span> <span class="st">"2001-01-01"</span>, <span class="at">to =</span> <span class="st">"2023-12-31"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "MSFT" "SPY" </code></pre>
</div>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>s <span class="ot">=</span> <span class="dv">3666</span> <span class="co"># 2015-07-30</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span>s</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">=</span> s<span class="sc">:</span><span class="fu">nrow</span>(MSFT) <span class="co"># post covid</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">dailyReturn</span>(MSFT))</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">dailyReturn</span>(SPY))</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">mean</span>(a[prior]), <span class="fu">mean</span>(c[prior]))</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>Sigma0 <span class="ot">=</span> <span class="fu">cov</span>(<span class="fu">data.frame</span>(<span class="at">a=</span>a[prior],<span class="at">c=</span>c[prior]))</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">mean</span>(a[obs]), <span class="fu">mean</span>(c[obs]))</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> <span class="fu">cov</span>(<span class="fu">data.frame</span>(<span class="at">a=</span>a,<span class="at">c=</span>c))</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>SigmaPost <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">solve</span>(Sigma0) <span class="sc">+</span> <span class="fu">solve</span>(Sigma))</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>muPost <span class="ot">=</span> SigmaPost <span class="sc">%*%</span> (<span class="fu">solve</span>(Sigma0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> <span class="fu">solve</span>(Sigma) <span class="sc">%*%</span> mu)</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(a[obs], c[obs], <span class="at">xlab=</span><span class="st">"MSFT"</span>, <span class="at">ylab=</span><span class="st">"SPY"</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.005</span>,<span class="fl">0.005</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.005</span>,<span class="fl">0.005</span>), <span class="at">pch=</span><span class="dv">16</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>, <span class="at">h=</span><span class="dv">0</span>, <span class="at">col=</span><span class="st">"grey"</span>)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mu0[<span class="dv">1</span>], <span class="at">h=</span>mu0[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"blue"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#prior</span></span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mu[<span class="dv">1</span>], <span class="at">h=</span>mu[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"red"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#data</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>muPost[<span class="dv">1</span>], <span class="at">h=</span>muPost[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"green"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#posterior</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"Prior"</span>, <span class="st">"Likelihood"</span>, <span class="st">"Posterior"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-portfolio" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="03-bl_files/figure-html/fig-portfolio-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Bayesian Portfolio Updating
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see the posterior mean for SPY is close to the prior mean, while the posterior mean for MSFT is further away. The performance of MSFT was significantly better past 2015 compared to SPY. The posterior mean (green) represents mean reversion value. We can think of it a expected mean return if the performance of MSFT starts reverting to its historical averages.</p>
<p>This model is particularly powerful because it can be extended to more dimensions (more stocks) and can include more complex relationships between the variables. It’s often used in finance, econometrics, and other fields where understanding the joint behavior of multiple normally-distributed variables is important.</p>
</div>
</section>
<section id="mixtures-of-conjugate-priors" class="level2" data-number="3.10">
<h2 data-number="3.10" class="anchored" data-anchor-id="mixtures-of-conjugate-priors"><span class="header-section-number">3.10</span> Mixtures of Conjugate Priors</h2>
<p>The mixture of conjugate priors is a powerful tool for modeling complex data. It allows us to combine multiple conjugate priors to create a more flexible model that can capture a wider range of data patterns. The mixture of conjugate priors is particularly useful when the data is generated from a mixture of distributions, where each component of the mixture is generated from a different distribution. <!-- http://www.mas.ncl.ac.uk/~nmf16/teaching/mas3301/week11.pdf --></p>
<p>If <span class="math inline">\(p_1(x),\ldots,p_k(x)\)</span> are proper density functions and <span class="math inline">\(\pi_1,\ldots,\pi_k\)</span> are non-negative weights that sum to 1, then the mixture distribution is given by <span class="math display">\[
p(x) = \sum_{i=1}^k \pi_i p_i(x).
\]</span> It is easy to show that <span class="math inline">\(p(x)\)</span> is a proper density. Indeed, given domain <span class="math inline">\(x\in A\subset \mathbb{R}\)</span> we have <span class="math display">\[
\int_A p(x)dx = \sum_{i=1}^k \pi_i \int_A p_i(x)dx  = \sum_{i=1}^k \pi_i = 1.
\]</span></p>
<p>Assume our prior is a mixture of distributions, that is <span class="math display">\[
\theta \sim p(\theta) = \sum_{k=1}^K \pi_k p_k(\theta).
\]</span> Then the posterior is also a mixture of normal distributions, that is <span class="math display">\[
p(\theta\mid y) = p(y\mid \theta)\sum_{k=1}^K \pi_k p_k(\theta)/C.
\]</span> We introduce a normalizing constant for each component <span class="math display">\[
C_k = \int p(y\mid \theta)p_k(\theta)d\theta.
\]</span> then <span class="math display">\[
p_k(\theta\mid y)  = p_k(\theta)p(y\mid \theta)/C_k
\]</span> is a proper distribution and our posterior is a mixture of these distributions <span class="math display">\[
p(\theta\mid y) = \sum_{k=1}^K \pi_k C_k p_k(\theta\mid y)/C.
\]</span> Meaning that we need to require <span class="math display">\[
\dfrac{\sum_{k=1}^K \pi_k C_k}{C} = 1.
\]</span> or <span class="math display">\[
C = \sum_{k=1}^K \pi_k C_k
\]</span> Then the posterior density is a mixture <span class="math display">\[
p(\theta\mid y) = \sum_{k=1}^K \hat \pi_k p_k(\theta \mid y),
\]</span> where <span class="math display">\[
\hat \pi_k = \dfrac{\pi_k C_k}{\sum_{i=1}^{K}\pi_i C_i}
\]</span> are the posterior weights.</p>
<p>Consider an example of a mixture of two normal distributions. The prior distribution is a mixture of two normal distributions, that is <span class="math display">\[
\mu \sim 0.5 N(0,1) + 0.5 N(5,1).
\]</span> The likelihood is a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance 1, that is <span class="math display">\[
y \mid \mu \sim N(\mu,1).
\]</span> The posterior distribution is a mixture of two normal distributions, that is <span class="math display">\[
p(\mu \mid y) \propto \phi(y\mid \mu,1) \left(0.5 \phi(\mu\mid 0,1) + 0.5 \phi(\mu\mid 5,1)\right),
\]</span> where <span class="math inline">\(\phi(x\mid \mu,\sigma^2)\)</span> is the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We can calculate it using property of a normal distribution <span class="math display">\[
\phi(x\mid \mu_1,\sigma_1^2)\phi(x\mid \mu_2,\sigma_2^2) = \phi(x\mid \mu_3,\sigma_3^2)\phi(\mu_1-\mu_2\mid 0,\sigma_1^2+\sigma_2^2)
\]</span> where <span class="math display">\[
\mu_3 = \dfrac{\mu_1/\sigma_2^2 + \mu_2/\sigma_1^2}{1/\sigma_1^2 + 1/\sigma_2^2}, \quad \sigma_3^2 = \dfrac{1}{1/\sigma_1^2 + 1/\sigma_2^2}.
\]</span></p>
<p>Given, we observed <span class="math inline">\(y = 2\)</span>, we can calculate the posterior distribution for <span class="math inline">\(\mu\)</span></p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>sigma02 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>pi <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>mu3 <span class="ot">=</span> (mu0<span class="sc">/</span>sigma02 <span class="sc">+</span> y) <span class="sc">/</span> (<span class="dv">1</span><span class="sc">/</span>sigma02 <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>sigma3 <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>sigma02 <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>C <span class="ot">=</span> <span class="fu">dnorm</span>(y<span class="sc">-</span>mu0,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">+</span>sigma02)<span class="sc">*</span>pi</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>w <span class="ot">=</span> C<span class="sc">/</span><span class="fu">sum</span>(C)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sprintf</span>(<span class="st">"Component parameters:</span><span class="sc">\n</span><span class="st">Mean = (%1.1f,%2.1f)</span><span class="sc">\n</span><span class="st">Var = (%1.1f,%1.1f)</span><span class="sc">\n</span><span class="st">weights = (%1.2f,%1.2f)"</span>, mu3[<span class="dv">1</span>],mu3[<span class="dv">2</span>], sigma3[<span class="dv">1</span>],sigma3[<span class="dv">2</span>],w[<span class="dv">1</span>],w[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "Component parameters:\nMean = (1.0,3.5)\nVar = (0.5,0.5)\nweights = (0.65,0.35)"</code></pre>
</div>
</div>
</section>
<section id="exponential-gamma-model" class="level2" data-number="3.11">
<h2 data-number="3.11" class="anchored" data-anchor-id="exponential-gamma-model"><span class="header-section-number">3.11</span> Exponential-Gamma Model</h2>
<p>Exponential distribution is a continuous distribution that is often used to model waiting times between events. For example, the time between two consecutive arrivals of a Poisson process is exponentially distributed. If the number of events in 1 unit of time has the Poisson distribution with rate parameter <span class="math inline">\(\lambda\)</span>, then the time between events has the exponential distribution with mean <span class="math inline">\(1/\lambda\)</span>. The probability density function (PDF) of an exponential distribution is defined as: <span class="math display">\[
f(x;\lambda) =  \lambda e^{-\lambda x}, ~ x \geq 0
\]</span> The exponential distribution is defined for <span class="math inline">\(x \geq 0\)</span>, and <span class="math inline">\(\lambda\)</span> is the rate parameter, which is the inverse of the mean (or expected value) of the distribution. It must be greater than 0. The exponential distribution is a special case of the Gamma distribution with shape 1 and scale <span class="math inline">\(1/\lambda\)</span>.</p>
<p>The mean and variance are give in terms of the rate parameter <span class="math inline">\(\lambda\)</span> as</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Exponential Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = 1/\lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = 1/\lambda^2\)</span></td>
</tr>
</tbody>
</table>
<p>Here are some examples of when exponential model provides a good fit</p>
<ul>
<li>Lifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.</li>
<li>Time Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.</li>
<li>Radioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution.</li>
</ul>
<p>In these examples, the key assumption is that events happen independently and at a constant average rate, which makes the exponential distribution a suitable model.</p>
<p>The Exponential-Gamma model, often used in Bayesian statistics, is a hierarchical model where the exponential distribution’s parameter is itself modeled as following a Gamma distribution. This approach is particularly useful in situations where there is uncertainty or variability in the rate parameter of the exponential distribution.</p>
<p>The <em>Exponential-Gamma</em> model assumes that the data follows an exponential distribution (likelihood). As mentioned earlier, the exponential distribution is suitable for modeling the time between events in processes where these events occur independently and at a constant rate. At the next level, the rate parameter <span class="math inline">\(\lambda\)</span> of the exponential distribution is assumed to follow a Gamma distribution. The Gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. <span class="math display">\[\begin{align*}
    \lambda &amp;\sim \text{Gamma}(\alpha, \beta) \\
    x_i &amp;\sim \text{Exponential}(\lambda)
\end{align*}\]</span></p>
<p>The probability density function of the Gamma distribution is given by <a href="#eq-gamma-pdf" class="quarto-xref">Equation&nbsp;<span>3.3</span></a> and has two parameters, shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>. The posterior distribution of the rate parameter <span class="math inline">\(\lambda\)</span> is given by: <span class="math display">\[
p(\lambda\mid x_1, \ldots, x_n) \propto \lambda^{\alpha - 1} e^{-\beta\lambda} \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^{\alpha + n - 1} e^{-(\beta + \sum_{i=1}^n x_i)\lambda}
\]</span> which is a Gamma distribution with shape parameter <span class="math inline">\(\alpha + n\)</span> and rate parameter <span class="math inline">\(\beta + \sum_{i=1}^n x_i\)</span>. The posterior mean and variance are given by: <span class="math display">\[
\mathbb{E}[\lambda|x_1, \ldots, x_n] = \frac{\alpha + n}{\beta + \sum_{i=1}^n x_i}, \quad \mathrm{Var}[\lambda|x_1, \ldots, x_n] = \frac{\alpha + n}{(\beta + \sum_{i=1}^n x_i)^2}.
\]</span> Notice, that <span class="math inline">\(\sum x_i\)</span> is the sufficient statistic for inference about parameter <span class="math inline">\(\lambda\)</span>!</p>
<p>Some applications of this model include the following:</p>
<ul>
<li>Reliability Engineering: In situations where the failure rate of components or systems may not be constant and can vary, the Exponential-Gamma model can be used to estimate the time until failure, incorporating uncertainty in the failure rate.</li>
<li>Medical Research: For modeling survival times of patients where the rate of mortality or disease progression is not constant and varies across a population. The variability in rates can be due to different factors like age, genetics, or environmental influences.</li>
<li>Ecology: In studying phenomena like the time between rare environmental events (e.g., extreme weather events), where the frequency of occurrence can vary due to changing climate conditions or other factors.</li>
</ul>
<p>In these applications, the Exponential-Gamma model offers a more nuanced approach than using a simple exponential model, as it accounts for the variability in the rate parameter.</p>
</section>
<section id="summary-of-conjugate-priors-for-common-likelihoods" class="level2" data-number="3.12">
<h2 data-number="3.12" class="anchored" data-anchor-id="summary-of-conjugate-priors-for-common-likelihoods"><span class="header-section-number">3.12</span> Summary of Conjugate Priors for Common Likelihoods</h2>
<p>Summary table of random variables</p>
<div id="tbl-rv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: Summary table of commonly used random variables
</figcaption>
<div aria-describedby="tbl-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 7%">
<col style="width: 32%">
<col style="width: 11%">
<col style="width: 29%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Parameters</th>
<th>PDF</th>
<th>Mean</th>
<th>Variance</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal</td>
<td><span class="math inline">\(\mu, \sigma^2\)</span></td>
<td><span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(x \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td>Exponential</td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda e^{-\lambda x}\)</span></td>
<td><span class="math inline">\(\frac{1}{\lambda}\)</span></td>
<td><span class="math inline">\(\frac{1}{\lambda^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\frac{e^{-\lambda}\lambda^x}{x!}\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(x \in \mathbb{N}\)</span></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(n, p\)</span></td>
<td><span class="math inline">\(\binom{n}{x}p^x(1-p)^{n-x}\)</span></td>
<td><span class="math inline">\(np\)</span></td>
<td><span class="math inline">\(np(1-p)\)</span></td>
<td><span class="math inline">\(x \in \{0, 1, \ldots, n\}\)</span></td>
</tr>
<tr class="even">
<td>Bernoulli</td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(p^x(1-p)^{1-x}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(p(1-p)\)</span></td>
<td><span class="math inline">\(x \in \{0, 1\}\)</span></td>
</tr>
<tr class="odd">
<td>Multinomial</td>
<td><span class="math inline">\(n, \boldsymbol{p}\)</span></td>
<td><span class="math inline">\(\frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots p_k^{x_k}\)</span></td>
<td><span class="math inline">\(np_i\)</span></td>
<td><span class="math inline">\(np_i(1-p_i)\)</span></td>
<td><span class="math inline">\(\sum x_i = n, x_i \in \mathbb{R}^+\)</span></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span></td>
<td><span class="math inline">\(x \in [0, 1]\)</span></td>
</tr>
<tr class="odd">
<td>Dirichlet</td>
<td><span class="math inline">\(\boldsymbol{\alpha}\)</span></td>
<td><span class="math inline">\(\frac{\Gamma(\sum \alpha_i)}{\prod \Gamma(\alpha_i)}\prod x_i^{\alpha_i-1}\)</span></td>
<td><span class="math inline">\(\frac{\alpha_i}{\sum \alpha_i}\)</span></td>
<td><span class="math inline">\(\frac{\alpha_i(\sum \alpha_i - \alpha_i)}{\sum \alpha_i^2(\sum \alpha_i + 1)}\)</span></td>
<td><span class="math inline">\(\sum x_i = 1, x_i \in [0, 1]\)</span></td>
</tr>
<tr class="even">
<td>Inverse Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{-\alpha-1}e^{-\frac{\beta}{x}}\)</span></td>
<td><span class="math inline">\(\frac{\beta}{\alpha-1}\)</span></td>
<td><span class="math inline">\(\frac{\beta^2}{(\alpha-1)^2(\alpha-2)}\)</span></td>
<td><span class="math inline">\(x &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-conjugate" class="quarto-xref">Table&nbsp;<span>3.5</span></a> summarizes the conjugate prior distributions for common likelihoods. Thus far, we’ve considered the Normal-Normal model with both known and unknown variance as well as Poisson-Gamma and Beta Binomial. The other pairs are left as an exercise. Given observed data <span class="math inline">\(x = (x_1,\ldots,x_n)\)</span> and <span class="math inline">\(s = \sum_{i=1}^nx_i\)</span>, <span class="math inline">\(\bar x = s/n\)</span>.</p>
<div id="tbl-conjugate" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-conjugate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.5: Conjugate prior table for common likelihoods
</figcaption>
<div aria-describedby="tbl-conjugate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 7%">
<col style="width: 10%">
<col style="width: 7%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Likelihood</th>
<th>Prior</th>
<th>Prior Parameters</th>
<th>Model Parameters</th>
<th>Posterior Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal (known <span class="math inline">\(\sigma^2\)</span>)</td>
<td>Normal</td>
<td><span class="math inline">\(\mu_0, \sigma^2_0\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\frac{n\sigma^2_0 \bar x + \sigma^2 \mu_0}{\sigma^2 + n\sigma^2_0},~\frac{\sigma^2\sigma_0^2}{n\sigma_0^2+\sigma^2}\)</span></td>
</tr>
<tr class="even">
<td>Normal (known <span class="math inline">\(\mu\)</span>)</td>
<td>Inverse Gamma</td>
<td><span class="math inline">\(\alpha,\beta\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\alpha+n/2, \beta + \frac{1}{2}\sum(x_i-\mu)^2\)</span></td>
</tr>
<tr class="odd">
<td>Binomial (<span class="math inline">\(m\)</span> trials)</td>
<td>Beta</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(\alpha + s, \beta + nm - s\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\alpha + s, \beta + n\)</span></td>
</tr>
<tr class="odd">
<td>Exponential</td>
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\alpha + n, \beta + s\)</span></td>
</tr>
<tr class="even">
<td>Multinomial</td>
<td>Dirichlet</td>
<td><span class="math inline">\(\alpha \in \mathbb{R}^k\)</span></td>
<td><span class="math inline">\(p \in \mathbb{R}^k\)</span></td>
<td><span class="math inline">\(\alpha+s\)</span></td>
</tr>
<tr class="odd">
<td>Normal</td>
<td>Normal-inverse gamma</td>
<td><span class="math inline">\(\mu_0, \nu, \alpha, \beta\)</span></td>
<td><span class="math inline">\(\mu, \sigma\)</span></td>
<td><span class="math inline">\(\frac{\nu\mu_0+n\bar{x}}{\nu+n} ,\, \nu+n,\, \alpha+\frac{n}{2} ,\,\)</span> <br> <span class="math inline">\(\beta + \tfrac{1}{2} \sum_{i=1}^n (x_i - \bar{x})^2 + \frac{n\nu}{\nu+n}\frac{(\bar{x}-\mu_0)^2}{2}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These conjugate relationships simplify Bayesian calculations by ensuring that the posterior distributions are in the same family as the priors.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-dixon1997modelling" class="csl-entry" role="listitem">
Dixon, Mark J., and Stuart G. Coles. 1997. <span>“Modelling <span>Association Football Scores</span> and <span>Inefficiencies</span> in the <span>Football Betting Market</span>.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 46 (2): 265–80.
</div>
<div id="ref-kolmogorov1942definition" class="csl-entry" role="listitem">
Kolmogorov, AN. 1942. <span>“Definition of Center of Dispersion and Measure of Accuracy from a Finite Number of Observations (in <span>Russian</span>).”</span> <em>Izv. Akad. Nauk SSSR Ser. Mat.</em> 6: 3–32.
</div>
<div id="ref-shen2021underperformance" class="csl-entry" role="listitem">
Shen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad Patell, and Dhruv S Kazi. 2021. <span>“Underperformance of Contemporary Phase <span>III</span> Oncology Trials and Strategies for Improvement.”</span> <em>Journal of the National Comprehensive Cancer Network</em> 19 (9): 1072–78.
</div>
<div id="ref-spiegelhalter2009one" class="csl-entry" role="listitem">
Spiegelhalter, David, and Yin-Lam Ng. 2009. <span>“One Match to Go!”</span> <em>Significance</em> 6 (4): 151–53.
</div>
<div id="ref-stern2007inference" class="csl-entry" role="listitem">
Stern, H, Adam Sugano, J Albert, and R Koning. 2007. <span>“Inference about Batter-Pitcher Matchups in Baseball from Small Samples.”</span> <em>Statistical Thinking in Sports</em>, 153–65.
</div>
<div id="ref-sun2022why" class="csl-entry" role="listitem">
Sun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. <span>“Why 90% of Clinical Drug Development Fails and How to Improve It?”</span> <em>Acta Pharmaceutica Sinica B</em> 12 (7): 3049–62.
</div>
<div id="ref-taleb2007black" class="csl-entry" role="listitem">
Taleb, Nassim Nicholas. 2007. <em>The <span>Black Swan</span>: <span>The Impact</span> of the <span>Highly Improbable</span></em>. Annotated edition. New York. N.Y: Random House.
</div>
<div id="ref-vecer2009estimating" class="csl-entry" role="listitem">
Vecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009. <span>“Estimating the <span>Effect</span> of the <span>Red Card</span> in <span>Soccer</span>: <span>When</span> to <span>Commit</span> an <span>Offense</span> in <span>Exchange</span> for <span>Preventing</span> a <span>Goal Opportunity</span>.”</span> <em>Journal of Quantitative Analysis in Sports</em> 5 (1).
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02-bayes.html" class="pagination-link" aria-label="Bayes Rule">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04-dec.html" class="pagination-link" aria-label="Utility, Risk and Decisions">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>