<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Stochastic Processes – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./08-gp.html" rel="next">
<link href="./06-hyp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="7&nbsp; Stochastic Processes – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="07-sp_files/figure-html/fig-brownian-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="7&nbsp; Stochastic Processes – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="07-sp_files/figure-html/fig-brownian-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./07-sp.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#brownian-motion" id="toc-brownian-motion" class="nav-link active" data-scroll-target="#brownian-motion"><span class="header-section-number">7.1</span> Brownian Motion</a></li>
  <li><a href="#black-scholes-model-for-sports-betting" id="toc-black-scholes-model-for-sports-betting" class="nav-link" data-scroll-target="#black-scholes-model-for-sports-betting"><span class="header-section-number">7.2</span> Black-Scholes Model for Sports Betting</a>
  <ul class="collapse">
  <li><a href="#implied-volatility-for-sports-games" id="toc-implied-volatility-for-sports-games" class="nav-link" data-scroll-target="#implied-volatility-for-sports-games">Implied Volatility for Sports Games</a></li>
  </ul></li>
  <li><a href="#poisson-process" id="toc-poisson-process" class="nav-link" data-scroll-target="#poisson-process"><span class="header-section-number">7.3</span> Poisson Process</a></li>
  <li><a href="#the-lévy-itô-decomposition-in-finance" id="toc-the-lévy-itô-decomposition-in-finance" class="nav-link" data-scroll-target="#the-lévy-itô-decomposition-in-finance"><span class="header-section-number">7.4</span> The Lévy-Itô Decomposition in Finance</a></li>
  <li><a href="#newton-and-the-south-sea-bubble" id="toc-newton-and-the-south-sea-bubble" class="nav-link" data-scroll-target="#newton-and-the-south-sea-bubble"><span class="header-section-number">7.5</span> Newton and the South Sea Bubble</a></li>
  <li><a href="#stochastic-volatility-financial-economics" id="toc-stochastic-volatility-financial-economics" class="nav-link" data-scroll-target="#stochastic-volatility-financial-economics"><span class="header-section-number">7.6</span> Stochastic Volatility: Financial Economics</a>
  <ul class="collapse">
  <li><a href="#motivation-combining-brownian-motion-and-jumps" id="toc-motivation-combining-brownian-motion-and-jumps" class="nav-link" data-scroll-target="#motivation-combining-brownian-motion-and-jumps">Motivation: Combining Brownian Motion and Jumps</a></li>
  <li><a href="#the-stochastic-volatility-model" id="toc-the-stochastic-volatility-model" class="nav-link" data-scroll-target="#the-stochastic-volatility-model">The Stochastic Volatility Model</a></li>
  <li><a href="#bayesian-inference-for-stochastic-volatility-models" id="toc-bayesian-inference-for-stochastic-volatility-models" class="nav-link" data-scroll-target="#bayesian-inference-for-stochastic-volatility-models">Bayesian Inference for Stochastic Volatility Models</a></li>
  <li><a href="#connection-to-monte-carlo-methods" id="toc-connection-to-monte-carlo-methods" class="nav-link" data-scroll-target="#connection-to-monte-carlo-methods">Connection to Monte Carlo Methods</a></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context">Historical Context</a></li>
  <li><a href="#practical-significance" id="toc-practical-significance" class="nav-link" data-scroll-target="#practical-significance">Practical Significance</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./07-sp.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Yet another fundamental concept that is useful for probabilistic reasoning is a stochastic process. An instance of a process is a function <span class="math inline">\(X\colon \Omega \rightarrow S\)</span> from an index set <span class="math inline">\(\Omega\)</span> to a set of possible values <span class="math inline">\(S\)</span>, called the state space. The state space of a stochastic process is the set of all possible states that the process can be in. Each state in the state space represents a possible outcome or condition of the system being modeled. The process then is the <em>distribution over the space of functions</em> from <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(S\)</span>. The term process is used because the function <span class="math inline">\(X\)</span> is often thought of as a time-varying quantity, and the index set <span class="math inline">\(\Omega\)</span> is often interpreted as time. However, the index set can be any set, and the process can be a random function of any other variable. Both the index set and the state space can be discrete or continuous. For example, a discrete time index can represent days or rounds and a continuous time index is a point on a time line. The state space can be discrete (composed of distinct states, like the number of customers in a store) or continuous (such as the price of a stock). The state space can be one-dimensional (only one aspect of the system is modeled) or multi-dimensional (multiple aspects are modeled simultaneously).</p>
<p>A stochastic process is a family of random variables that describes the evolution through time of some (physical) process. We denote this by <span class="math inline">\(X = \{X(t),~t\in T\}\)</span>, with <span class="math inline">\(t\)</span> representing time and <span class="math inline">\(X(t) = \omega\)</span> is the state of the process at time <span class="math inline">\(t\)</span>. We will get a realization (a.k.a. sample path). In the case when time is discrete, the realization is a sequence of observed <span class="math inline">\(X = \Omega = \{\omega_1,\omega_2,\ldots\}\)</span>. Common discrete time processes are Markov chains. Brownian motion is a central process in continuous time and state with almost surely continuous but nowhere differentiable paths. Poisson processes are commonly used to account for jumps in the process.</p>
<p>Here are some widely used stochastic processes:</p>
<ol type="1">
<li>Random Walk: A simple example where the next state depends on the current state and some random movement. In finance, stock prices are often modeled as a type of random walk.</li>
<li>Markov Chains: A process where the next state depends only on the current state and not on the path taken to reach the current state.</li>
<li>Poisson Process: Used to model the number of times an event occurs in a fixed interval of time or space, where events occur with a known constant mean rate and independently of the time since the last event.</li>
<li>Queuing Theory: Models used in operations research where the stochastic process represents the number of customers in a queue, varying over time as customers arrive and are served.</li>
<li>Brownian Motion: This process models the random movement of particles suspended in a fluid. It has applications in physics, finance (to model stock market prices), and biology.</li>
<li>Gaussian Processes: These are a collection of random variables, any finite number of which have a joint Gaussian distribution. They are used in machine learning for regression and classification tasks.</li>
</ol>
<p>In data analysis, both experimental and observational data can exhibit variability. This variability is often modeled using probability distributions. These distributions can either represent simple processes with independent elements (then we are back to i.i.d case) or more complex stochastic processes that display dependencies, whether they be serial, spatial, or of other types. Essentially, this modeling approach helps in understanding and predicting data behavior under various conditions. The early sections of <span class="citation" data-cites="davison2003statistical">Davison (<a href="references.html#ref-davison2003statistical" role="doc-biblioref">2003</a>)</span> work offer an insightful primer on how to develop and apply these stochastic models across various fields. This introduction is particularly useful for grasping the fundamental concepts and practical applications of these models.</p>
<section id="brownian-motion" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="brownian-motion"><span class="header-section-number">7.1</span> Brownian Motion</h2>
<p>Brownian Motion, named after botanist Robert Brown, is a fundamental concept in the theory of stochastic processes. It describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.</p>
<p>A one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process <span class="math inline">\(B_t_{t\ge 0}\)</span> with the following properties:</p>
<ul>
<li><span class="math inline">\(B(0) = 0\)</span> almost surely</li>
<li><span class="math inline">\(B_t\)</span> has stationary independent increments: <span class="math inline">\(B_t - B(s) \sim N(0, t-s)\)</span> for <span class="math inline">\(0 \le s &lt; t\)</span></li>
<li><span class="math inline">\(B_t\)</span> is a continuous function of <span class="math inline">\(t\)</span></li>
<li>For each time <span class="math inline">\(t &gt; 0\)</span>, the random variable <span class="math inline">\(B_t\)</span> is normally distributed with mean 0 and variance <span class="math inline">\(t\)</span>, i.e., <span class="math inline">\(B_t \sim N(0, t)\)</span>.</li>
</ul>
<p>Formally, Brownian motion is a continuous-time stochastic process <span class="math inline">\(B_t\)</span> for <span class="math inline">\(t \ge 0\)</span> used to model random continuous evolution.</p>
<p><a href="#fig-brownian" class="quarto-xref">Figure&nbsp;<span>7.1</span></a> below shows three sample paths of Brownian Motion.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Brownian Motion</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">92</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>t <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">xlab=</span><span class="st">"t"</span>, <span class="at">ylab=</span><span class="st">"B_t"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.2</span>, <span class="dv">2</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div id="fig-brownian" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-brownian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="07-sp_files/figure-html/fig-brownian-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-brownian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Brownian Motion
</figcaption>
</figure>
</div>
</div>
</div>
<p>Thus, for any times <span class="math inline">\(0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_n\)</span>, the random variables <span class="math inline">\(B(t_2) - B(t_1)\)</span>, <span class="math inline">\(B(t_3) - B(t_2)\)</span>, , <span class="math inline">\(B(t_n) - B(t_{n-1})\)</span> are independent and the function <span class="math inline">\(t \mapsto B_t\)</span> is continuous almost surely.</p>
<p>Some properties of Brownian Motion are:</p>
<ul>
<li>Scale Invariance: If <span class="math inline">\(B_t\)</span> is a Brownian motion, then for any <span class="math inline">\(a &gt; 0\)</span>, the process <span class="math inline">\(aB(t/a^2)\)</span> is also a Brownian motion.</li>
<li>Fractal Nature: Brownian motion paths are nowhere differentiable but continuous everywhere, reflecting a fractal-like nature.</li>
</ul>
<p>The crash was precipitated by ‘portfolio insurance,’ an automated strategy that sold equities as markets declined, creating a self-reinforcing downward spiral.</p>
<p>The normal assumption of asset returns was first proposed in 1900 in the PhD thesis of Louis Bachelier, who was a student of Henri Poincare. Bachelier was interested in developing statistical tools for pricing options (predicting asset returns) on the Paris stock exchange. Although Bachelier’s work laid the foundation for the modern theory of stochastic processes, he was never given credit by his contemporaries, including Einstein, Levy and Borel.</p>
<p>In 1905 Einstein published a paper which used the same statistical model as Bachelier to describe the 1827 discovery by botanist Robert Brown, who observed that pollen particles suspended in water followed irregular random trajectories. Thus, we call the stochastic process that describes these phenomena Brownian motion. Einstein’s advisor at the University of Zurich was Hermann Minkowski who was a friend and collaborator of Poincare. Thus, it is likely Einstein knew about the work of Bachelier, but he never mentioned it in his paper. This was not the first instance when Einstein did not give proper credit. Poincare published a paper <span class="citation" data-cites="poincare1898mesure">Poincaré (<a href="references.html#ref-poincare1898mesure" role="doc-biblioref">1898</a>)</span> on relativity theory in 1898, seven years before Einstein. This paper was published in a philosophy journal and thus Poincare avoided using any mathematical formulas except for the famous <span class="math inline">\(E=mc^2\)</span>. Poincare discussed his results on relativity theory with Minkowski. Minkowski asked Einstein to read Poincare’s work <span class="citation" data-cites="arnold2006forgotten">Arnol’d (<a href="references.html#ref-arnold2006forgotten" role="doc-biblioref">2006</a>)</span>. However, Einstein never referenced the work of Poincare until 1945. One of the reviewers for the 1905 paper on relativity by Einstein was Poincare and he wrote a very positive review mentioning it as a breakthrough. When Minkowski asked Poincare why he did not claim his priority on the theory, Poincare replied that our mission is to support young scientists. More about why credit is mistakenly given to Einstein for relativity theory is discussed by Logunov <span class="citation" data-cites="logunov2004henri">Logunov (<a href="references.html#ref-logunov2004henri" role="doc-biblioref">2004</a>)</span>.</p>
<p>Einstein was not the only one who ignored the work of Bachelier; Paul Levy did so as well. Paul Levy was considered a pioneer and authority on stochastic processes during Bachelier’s time, although Bruno de Finetti introduced a dual concept of infinite divisibility in 1929, before the works of Levy in the early 1930s on this topic. Levy never mentioned the work of the obscure and little known mathematician Bachelier. The first to give credit to Bachelier was Kolmogorov in his 1931 paper <span class="citation" data-cites="kolmogoroff1931uber">Kolmogoroff (<a href="references.html#ref-kolmogoroff1931uber" role="doc-biblioref">1931</a>)</span> (Russian translation <span class="citation" data-cites="a.n.kolmogorov1938analytic">A. N. Kolmogorov (<a href="references.html#ref-a.n.kolmogorov1938analytic" role="doc-biblioref">1938</a>)</span> and English translation <span class="citation" data-cites="shiryayev1992analytical">Shiryayev (<a href="references.html#ref-shiryayev1992analytical" role="doc-biblioref">1992</a>)</span>). Later L.J. Savage translated Bachelier’s work to English and showed it to Paul Samuelson. Samuelson extended the work of Bachelier by considering the log-returns rather than absolute numbers, popularized the work of Bachelier among economists and the translation of Bachelier’s thesis was finally published in English in 1964 <span class="citation" data-cites="cootner1967random">Cootner (<a href="references.html#ref-cootner1967random" role="doc-biblioref">1967</a>)</span>. Many economists who extended the work of Bachelier won Nobel prizes, including Eugene Fama known for work on the efficient markets hypothesis, Paul Samuelson, and Myron Scholes for the Black-Scholes model, as well as Robert Merton.</p>
<p>Although it was originally developed to model financial markets by Louis Bachelier in 1900, Brownian Motion has found applications in many other fields: biology (movement of biomolecules within cells), environmental science (diffusion processes, like the spread of pollutants in air or water), and mathematics (stochastic calculus and differential equations).</p>
<div id="exm-mc-portfolio" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1 (Yahoo Stock Price Simulation)</strong></span> Investing in volatile stocks can be very risky. The Internet stocks during the late 1990’s were notorious for their volatility. For example, the leading Internet stock Yahoo! started 1999 at $62,rose to $122, then fell back to $55 in August, only to end the year at $216. Even more remarkable is the fact that by January 2000, Yahoo! has risen more than 100-fold from its offering price of $1.32 on April 15, 1996. In comparison, theNasdaq 100, a benchmark market index, was up about 5-fold during the same period.</p>
<p>Stock prices fluctuate somewhat randomly. Maurice Kendall, in his seminal 1953 paper on the random walk nature of stock and commodity prices, observed that “<em>The series looks like a wandering one, almost as if once a week the Demon of Chance drew a random number from a symmetrical population of fixed dispersion and added to it the current price to determine next week’s price (p.&nbsp;87).</em>” While a pure random walk model for Yahoo!’s stock price is in fact not reasonable since its price cannot fall below zero, an alternative model that appears to provide reasonable results assumes that the logarithms of price changes, or returns, follow a random walk. This alternative model is the basis for the results in this example.</p>
<p>To evaluate a stock investment, we take the initial price as <span class="math inline">\(X_0\)</span> and then we need to determine what the stock price might be in year <span class="math inline">\(T\)</span>, namely <span class="math inline">\(X_T\)</span>. Our approach draws from the Black-Scholes Model for valuing stock options. Technically, the Black-Scholes Model assumes that <span class="math inline">\(X_T\)</span> is determined by the solution to a stochastic differential equation. This leads to the <em>Geometric Brownian Motion</em> <span class="math display">\[
X_T = X_0 \exp\left( (\mu - 1/2\sigma^2)T + \sigma B_T  \right),
\]</span> where <span class="math inline">\(B_T\)</span> is a standard Brownian motion; that is, <span class="math inline">\(B_0 = 0\)</span>, <span class="math inline">\(B_t - B_s\)</span> is independent of <span class="math inline">\(B_s\)</span>, and its distribution depends only on <span class="math inline">\(t-s\)</span> with <span class="math inline">\(B_t \sim N(0,t)\)</span>. Hence, <span class="math inline">\(B_t = \sqrt{t}Z\)</span>, where <span class="math inline">\(Z \sim N(0,1)\)</span>.</p>
<p>Then, the expected value is <span class="math display">\[\begin{align*}
E(X_T) = &amp;X_0 \exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma B_T))\\
&amp; = X_0\exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma \sqrt{T}Z))\\
&amp; = X_0\exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma \sqrt{T}Z)) \\
&amp;= X_0\exp\left( (\mu - 1/2\sigma^2)T \right) \exp\left( \frac{1}{2}\sigma^2T \right) = X_0\exp\left( \mu T \right).
\end{align*}\]</span> The <span class="math inline">\(E(\exp(\sigma \sqrt{T}Z)) = \exp\left( 1/2\sigma^2T \right)\)</span> is due to the moment property of the log-normal distribution. We can interpret <span class="math inline">\(\mu\)</span> as the expected rate of return <span class="math display">\[
\hat \mu = \frac{1}{T}\log\left( \frac{X_T}{X_0} \right).
\]</span> This provides a way to estimate the expected rate of return from the expected value of the stock price at time <span class="math inline">\(T\)</span>, by plugging in the observed values of <span class="math inline">\(X_0\)</span> and <span class="math inline">\(X_T\)</span>.</p>
<p>The variance is <span class="math display">\[\begin{align*}
\text{Var}(X_T) = &amp;X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \text{Var}(\exp(\sigma B_T))\\
&amp; = X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \text{Var}(\exp(\sigma \sqrt{T}Z))\\
&amp; = X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \exp\left( \sigma^2T \right) - X_0^2\exp\left( 2(\mu - 1/2\sigma^2)T \right)\\
&amp; = X_0^2\exp\left( 2\mu T \right)\left( \exp\left( \sigma^2T \right) - 1 \right).
\end{align*}\]</span></p>
<p>The important consequence of the model for predicting future prices is that <span class="math inline">\(\log(X_T/X_0)\)</span> has a normal distribution with mean <span class="math inline">\((\mu-\frac{1}{2} \sigma^2)T\)</span> and variance <span class="math inline">\(\sigma^2 T\)</span> which is equivalent to saying that the ratio <span class="math inline">\(X_T/X_0\)</span> has a log-normal distribution. It is interesting that although the Black-Scholes result is a standard tool for valuing options in finance the log-normal predictive distribution that follows from its assumptions is not commonly studied. In order to forecast <span class="math inline">\(X_T\)</span> we need to estimate the unknowns <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> (recall <span class="math inline">\(X_0\)</span> is known). The unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> can be interpreted as the instantaneous expected rate of return and the volatility, respectively. The mean parameter <span class="math inline">\(\mu\)</span> is known as the expected rate of return because the expected value of <span class="math inline">\(X_T\)</span> is <span class="math inline">\(X_0e^{\mu T}\)</span>. There are a number of ways of estimating the unknown parameters. One approach is to use an equilibrium model for returns, such as the Capital Asset Pricing Model or CAPM. We will discuss this model later. Another approach is to use historical data to estimate the parameters. For example, the expected rate of return can be estimated as the average historical return. The volatility can be estimated as the standard deviation of historical returns. The Black-Scholes model is a continuous time model, but in practice we use discrete time data. The Black-Scholes model can be adapted to discrete time by replacing the continuous time Brownian motion with a discrete time random walk.</p>
</div>
</section>
<section id="black-scholes-model-for-sports-betting" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="black-scholes-model-for-sports-betting"><span class="header-section-number">7.2</span> Black-Scholes Model for Sports Betting</h2>
<p>Sports betting involves wagering on the outcome of athletic events. Bettors’ assessments of these outcomes are aggregated in markets that provide key metrics like the point spread, which is the expected margin of victory, and moneyline odds, which imply the probability of a team winning. These market-based measures can be used to analyze the uncertainty, or volatility, inherent in a sports game.</p>
<p>To quantify the uncertainty in a game’s outcome, the score difference between two teams over time can be modeled as a stochastic process. Specifically, we use a Brownian motion model, first proposed by <span class="citation" data-cites="stern1994brownian">Stern (<a href="references.html#ref-stern1994brownian" role="doc-biblioref">1994</a>)</span>, to represent the evolution of a team’s lead. In this framework, the score difference at time <span class="math inline">\(t\)</span>, denoted as <span class="math inline">\(X_t\)</span>, is assumed to follow a normal distribution with a mean (or “drift”) that grows over time and a variance that also increases with time.</p>
<p>This can be expressed mathematically as: <span class="math display">\[
X_t = \mu t + \sigma B_t \sim N(\mu t, \sigma^2 t)
\]</span> where <span class="math inline">\(\mu\)</span> is the drift parameter, representing the favored team’s point advantage over the whole game (derived from the point spread), <span class="math inline">\(\sigma\)</span> is the volatility parameter, representing the standard deviation of the final outcome, and <span class="math inline">\(t\)</span> is the time elapsed in the game, scaled from 0 to 1.</p>
<p>This model allows for the calculation of a team’s win probability at any point in the game and provides a formal way to measure the uncertainty of the final score.</p>
<p>The concept of deriving a game’s volatility from betting markets is directly analogous to the Black-Scholes model in finance. In finance, the Black-Scholes formula is used to price options. If the market price of an option is known, one can work backward to solve for the volatility of the underlying stock; this is called <em>implied volatility</em>. The model in sports betting does the same: it uses the market-set point spread (<span class="math inline">\(\mu\)</span>) and win probability (<span class="math inline">\(p\)</span>) to solve for the game’s implied volatility (<span class="math inline">\(\sigma\)</span>).</p>
<p>Both models use a Brownian motion framework to describe how a variable changes over time. However, there is a key difference. The sports model uses a standard Brownian motion, where the score changes additively. In contrast, the Black-Scholes model uses a geometric Brownian motion, which assumes that a stock price changes by a certain percentage, not by a fixed amount.</p>
<p>Essentially, this approach applies the financial concept of implied volatility to the sports world, creating a lens through which betting market data can be interpreted to measure the expected uncertainty of a game.</p>
<section id="implied-volatility-for-sports-games" class="level3">
<h3 class="anchored" data-anchor-id="implied-volatility-for-sports-games">Implied Volatility for Sports Games</h3>
<p>The concept of <em>implied volatility</em> is central to understanding how market prices reflect uncertainty. In the context of sports betting, implied volatility represents the market’s assessment of the uncertainty in a game’s final outcome, derived from observable betting market data.</p>
<p>Given the point spread <span class="math inline">\(\mu\)</span> (which represents the expected margin of victory) and the win probability <span class="math inline">\(p\)</span> (derived from moneyline odds), we can solve for the implied volatility <span class="math inline">\(\sigma\)</span> using the relationship:</p>
<p><span class="math display">\[
p = \Phi\left(\frac{\mu}{\sigma}\right)
\]</span></p>
<p>Rearranging this equation, the implied volatility is given by:</p>
<p><span class="math display">\[
\sigma = \frac{\mu}{\Phi^{-1}(p)}
\]</span></p>
<p>where <span class="math inline">\(\Phi^{-1}\)</span> is the inverse of the standard normal cumulative distribution function (the quantile function).</p>
<p>This approach mirrors the methodology used in financial markets, where option prices are used to infer the market’s expectation of future stock price volatility. In sports betting, the “option price” is effectively the betting odds, and the “underlying asset” is the game outcome. Just as financial implied volatility reflects market sentiment about future price movements, sports implied volatility captures the market’s view of how uncertain or “volatile” a particular game is likely to be.</p>
<p>For example, a game between two closely matched teams might have high implied volatility, reflecting greater uncertainty in the outcome, while a game featuring a heavily favored team against a significant underdog would typically exhibit lower implied volatility, as the outcome is more predictable.</p>
<div id="exm-brownian" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2 (Black-Scholes Model for Super Bowl)</strong></span> In order to define the implied volatility of a sports game we begin with a distributional model for the evolution of the outcome in a sports game which we develop from Stern (1994). The model specifies the distribution of the lead of team A over team B, <span class="math inline">\(X_t\)</span> for any <span class="math inline">\(t\)</span> as a Brownian motion process. If <span class="math inline">\(B_t\)</span> denotes a standard Brownian motion with distributional property <span class="math inline">\(B_t \sim N(0,t)\)</span> and we incorporate drift, <span class="math inline">\(\mu\)</span>, and volatility, <span class="math inline">\(\sigma\)</span>, terms, then the evolution of the outcome <span class="math inline">\(X_t\)</span> that is given by: <span class="math display">\[
X_t=\mu t + \sigma B_t \sim N( \mu t , \sigma^2 t).
\]</span> This distribution of the game outcome is similar to the Black-Scholes model of the distribution of a stock price.</p>
<p>This specification results in several useful measures (or, this specification results in closed-form solutions for a number of measures of interest). The distribution of the final score follows a normal distribution, <span class="math inline">\(X_1\sim N(\mu, \sigma^2)\)</span>. We can calculate the probability of team A winning, denoted <span class="math inline">\(p=\mathbb{P}(X_1&gt;0)\)</span>, from the spread and probability distribution. Given the normality assumption, <span class="math inline">\(X_1 \sim N(\mu, \sigma^2)\)</span>, we have <span class="math display">\[
p = \mathbb{P}(X_1&gt;0) = \Phi \left ( \frac{\mu}{\sigma} \right )
\]</span> where <span class="math inline">\(\Phi\)</span> is the standard normal cdf. <a href="#tbl-probability" class="quarto-xref">Table&nbsp;<span>7.1</span></a> uses <span class="math inline">\(\Phi\)</span> to convert team A’s advantage <span class="math inline">\(\mu\)</span> to a probability scale using the information ratio <span class="math inline">\(\mu/\sigma\)</span>.</p>
<div id="tbl-probability" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-probability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.1: Probability of Winning <span class="math inline">\(p\)</span> versus the Sharpe Ratio <span class="math inline">\(\mu/\sigma\)</span>
</figcaption>
<div aria-describedby="tbl-probability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(\mu/\sigma\)</span></th>
<th>0</th>
<th>0.25</th>
<th>0.5</th>
<th>0.75</th>
<th>1</th>
<th>1.25</th>
<th>1.5</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p=\Phi(\mu/\sigma)\)</span></td>
<td>0.5</td>
<td>0.60</td>
<td>0.69</td>
<td>0.77</td>
<td>0.84</td>
<td>0.89</td>
<td>0.93</td>
<td>0.977</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>If teams are evenly matched and <span class="math inline">\(\mu/\sigma =0\)</span> then <span class="math inline">\(p=0.5\)</span>. <a href="#tbl-probability" class="quarto-xref">Table&nbsp;<span>7.1</span></a> provides a list of probabilities as a function of <span class="math inline">\(\mu/\sigma\)</span>. For example, if the point spread <span class="math inline">\(\mu=-4\)</span> and volatility is <span class="math inline">\(\sigma=10.6\)</span>, then the team has a <span class="math inline">\(\mu/\sigma = -4/10.6 = - 0.38\)</span> volatility point disadvantage. The probability of winning is <span class="math inline">\(\Phi(-0.38) = 0.353 &lt; 0.5\)</span>. A common scenario is that team A has an edge equal to half a volatility, so that <span class="math inline">\(\mu/\sigma =0.5\)</span> and then <span class="math inline">\(p= 0.69\)</span>.</p>
<p>Of particular interest here are conditional probability assessments made as the game progresses. For example, suppose that the current lead at time <span class="math inline">\(t\)</span> is <span class="math inline">\(l\)</span> points and so <span class="math inline">\(X_t = l\)</span>. The model can then be used to update your assessment of the distribution of the final score with the conditional distribution <span class="math inline">\((X_1 | X_t=l )\)</span>. To see this, we can re-write the distribution of <span class="math inline">\(X_1\)</span> given <span class="math inline">\(X_t\)</span> by noting that <span class="math inline">\(X_1 = X_t+ X_1 - X_t\)</span>. Using the formula above and substituting <span class="math inline">\(t\)</span> for <span class="math inline">\(1\)</span> where appropriate and noting that <span class="math inline">\(X_t = l\)</span> by assumption, this simplifies to <span class="math display">\[
X_1= l + \mu(1- t) + \sigma (B(1) - B_t).
\]</span> Here <span class="math inline">\(B(1) - B_t  \stackrel{D}{=} B(1-t)\)</span> which is independent of <span class="math inline">\(X_t\)</span> with distribution <span class="math inline">\(N(0,1-t)\)</span>. The mean and variance of <span class="math inline">\(X_1|X_t=l\)</span> decay to zero as <span class="math inline">\(t \rightarrow 1\)</span> and the outcome becomes certain at the realised value of <span class="math inline">\(X_1\)</span>. We leave open the possibility of a tied game and overtime to determine the outcome.</p>
<p>To determine this conditional distribution, we note that there are <span class="math inline">\(1-t\)</span> time units left together with a drift <span class="math inline">\(\mu\)</span> and as shown above in this case the uncertainty can be modeled as <span class="math inline">\(\sigma^2(1-t)\)</span>. Therefore, we can write the distribution of the final outcome after <span class="math inline">\(t\)</span> periods with a current lead of <span class="math inline">\(l\)</span> for team A as the conditional distribution: <span class="math display">\[
( X_1 | X_t=l) =  (X_1-X_t) + l   \sim N( l + \mu(1 - t) , \sigma^2 (1 - t) )
\]</span> From the conditional distribution <span class="math inline">\((X_1 | X_t=l) \sim N(l+\mu(1-t), \sigma^2 (1-t))\)</span>, we can calculate the conditional probability of winning as the game evolves. The probability of team A winning at time <span class="math inline">\(t\)</span> given a current lead of <span class="math inline">\(l\)</span> point is: <span class="math display">\[
p_t = P ( X_1 &gt; 0 | X_t = l) = \Phi \left ( \frac{ l + \mu ( 1 - t)  }{ \sigma \sqrt{ ( 1-t) } } \right )
\]</span></p>
<div id="fig-score-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-score-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hal-vol-new.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-score-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Score Evolution on a Discretized Grid
</figcaption>
</figure>
</div>
<p><a href="#fig-score-evolution" class="quarto-xref">Figure&nbsp;<span>7.2</span></a> A and B illustrate our methodology with an example. Suppose we are analyzing data for a Superbowl game between teams A and B with team A favored. Figure A presents the information available at the beginning of the game from the perspective of the underdog team B. If the initial point spread—or the market’s expectation of the expected outcome—is <span class="math inline">\(-4\)</span> and the volatility is <span class="math inline">\(10.6\)</span> (assumed given for the moment; more on this below) then the probability that the underdog team wins is <span class="math inline">\(p = \Phi ( \mu /\sigma ) = \Phi ( - 4/ 10.6) = 35.3\)</span>%. This result relies on our assumption of a normal outcome distribution on the outcome as previously explained. Another way of saying this is <span class="math inline">\(\mathbb{P}(X(1)&gt;0)=0.353\)</span> for an outcome distribution <span class="math inline">\(X(1) \sim N(-4, 10.6^2)\)</span>. Figure A illustrates this with the shaded red area under the curve.</p>
<p><a href="#fig-score-evolution" class="quarto-xref">Figure&nbsp;<span>7.2</span></a> B illustrates the information and potential outcomes at half-time. Here we show the evolution of the actual score until half time as the solid black line. From half-time onwards we simulate a set of possible Monte Carlo paths to the end of the game.</p>
<p>Specifically, we discretise the model with time interval <span class="math inline">\(\Delta =1/200\)</span> and simulate possible outcomes given the score at half time. The volatility plays a key role in turning the point spread into a probability of winning as the greater the volatility of the distribution of the outcome, <span class="math inline">\(X_1\)</span>, the greater the range of outcomes projected in the Monte Carlo simulation. Essentially the volatility provides a scale which calibrates the advantage implied by a given point spread.</p>
<p>We can use this relationship to determine how volatility decays over the course of the game. The conditional distribution of the outcome given the score at time <span class="math inline">\(t\)</span>, is <span class="math inline">\((X_1|X_t=l)\)</span> with a variance of <span class="math inline">\(\sigma^2(1-t)\)</span> and volatility of <span class="math inline">\(\sigma \sqrt{1-t}\)</span>. The volatility is a decreasing function of <span class="math inline">\(t\)</span>, illustrating that the volatility dissipates over the course of a game. For example, if there is an initial volatility of <span class="math inline">\(\sigma = 10.6\)</span>, then at half-time when <span class="math inline">\(t=\frac{1}{2}\)</span>, the volatility is <span class="math inline">\(10.6 / \sqrt{2} = 7.5\)</span> volatility points left. <a href="#tbl-volatility-decay" class="quarto-xref">Table&nbsp;<span>7.2</span></a>, below, illustrates this relationship for additional points over the game.</p>
<div id="tbl-volatility-decay" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-volatility-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.2: Volatility Decay over Time
</figcaption>
<div aria-describedby="tbl-volatility-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th>0</th>
<th><span class="math inline">\(\frac{1}{4}\)</span></th>
<th><span class="math inline">\(\frac{1}{2}\)</span></th>
<th><span class="math inline">\(\frac{3}{4}\)</span></th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sigma \sqrt{1-t}\)</span></td>
<td>10.6</td>
<td>9.18</td>
<td>7.50</td>
<td>5.3</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To provide insight into the final outcome given the current score, <a href="#tbl-probability" class="quarto-xref">Table&nbsp;<span>7.1</span></a> and <a href="#tbl-volatility-decay" class="quarto-xref">Table&nbsp;<span>7.2</span></a> can be combined to measure the current outcome, <span class="math inline">\(l\)</span>, in terms of standard deviations of the outcome.<br>
For example, suppose that you have Team B, an underdog, so from their perspective <span class="math inline">\(\mu = -4\)</span> and at half-time team B has a lead of 15, <span class="math inline">\(l= 15\)</span>. Team B’s expected outcome as presented earlier is <span class="math inline">\(l + \mu (1-t)\)</span> or <span class="math inline">\(15 - 4 \times \frac{1}{2} = 13\)</span>. If initial volatility is <span class="math inline">\(\sigma = 10.60\)</span> then the remaining volatility at half-time is <span class="math inline">\(10.6/\sqrt{2} = 7.50\)</span> and team B’s expected outcome of <span class="math inline">\(13\)</span> in terms of standard deviations is <span class="math inline">\(13/7.5 = 1.73\)</span>. Thus team B’s expected outcome is at the 99th percentile of the distribution, <span class="math inline">\(\Phi ( 1.73 ) = 0.96\)</span>, implying a 96% chance of winning.</p>
<p><strong>Implied Volatility</strong></p>
<p>The previous discussion assumed that the variance (or volatility) parameter <span class="math inline">\(\sigma\)</span> was a known constant. We return to this important quantity now. We are now in a position to define the <em>implied volatility</em> implicit in the two betting lines that are available. Given our model, we will use the <em>money-line</em> odds to provide a market assessment of the probability of winning, <span class="math inline">\(p\)</span>, and the <em>point spread</em> to assess the expected margin of victory, <span class="math inline">\(\mu\)</span>. The money line odds are shown for each team A and B and provide information on the payoff from a bet on the team winning. This calculation will also typically require an adjustment for the bookmaker’s spread. With these we can infer the <em>implied volatility</em>, <span class="math inline">\(\sigma_{IV}\)</span>, by solving <span class="math display">\[
\sigma_{IV}: \; \; \;  \; \; p = \Phi \left ( \frac{\mu}{\sigma_{IV}} \right ) \; \; \text{ which \; gives} \; \;
\sigma_{IV} = \frac{ \mu }{ \Phi^{-1} ( p ) } \; .
\]</span> Here <span class="math inline">\(\Phi^{-1}(p)\)</span> denotes the standard normal quantile function such that the area under the standard normal curve to the left of <span class="math inline">\(\Phi^{-1}(p)\)</span> is equal to <span class="math inline">\(p\)</span>. In our example we calculate this using the <code>qnorm</code> in <code>R</code>. Note that when <span class="math inline">\(\mu =0\)</span> and <span class="math inline">\(p= \frac{1}{2}\)</span> there’s no market information about the volatility as <span class="math inline">\(\mu / \Phi^{-1} (p)\)</span> is undefined. This is the special case where the teams are seen as evenly matched- the expected outcome has a zero point spread and there is an equal probability that either team wins.</p>
<p><strong>Time Varying Implied Volatility</strong></p>
<p>Up to this point the volatility rate has been assumed constant through the course of the game, i.e., that the same value of <span class="math inline">\(\sigma\)</span> is relevant. The amount of volatility remaining in the game is not constant but the basic underlying parameters has been assumed constant. This need not be true and more importantly the betting markets may provide some information about the best estimate of the volatility parameter at a given point of time. This is important because time-varying volatility provides an interpretable quantity that can allow one to assess the value of a betting opportunity.</p>
<p>With the advent of online betting there is a virtually continuous traded contract available to assess implied expectations of the probability of team A winning at any time <span class="math inline">\(t\)</span>. The additional information available from the continuous contract allows for further update of the implied conditional volatility. We assume that the online betting market gives us a current assessment of <span class="math inline">\(p_t\)</span>, that is the current probability that team A will win. We will then solve for <span class="math inline">\(\sigma^2\)</span> and in turn define resulting time-varying volatility, as <span class="math inline">\(\sigma_{IV,t}\)</span>, using the resulting equation to solve for <span class="math inline">\(\sigma_{IV,t}\)</span> with <span class="math display">\[
p_t = \Phi \left ( \frac{ l + \mu(1-t)  }{\sigma_{IV,t} \sqrt{1-t}} \right )
\; \text{ which \; gives} \; \;
\sigma_{IV,t} = \frac{ l + \mu ( 1-t ) }{ \Phi^{-1} ( p_t )  \sqrt{1-t}}
\]</span> We will use our methodology to find evidence of time-varying volatility in the SuperBowl XLVII probabilities.</p>
<p><strong>Super Bowl XLVII: Ravens vs San Francisco 49ers</strong></p>
<p>Super Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013 and featured the San Francisco 49ers against the Baltimore Ravens. Going into Super Bowl XLVII the San Francisco 49ers were favorites to win which was not surprising following their impressive season. It was a fairly bizarre Super Bowl with a <span class="math inline">\(34\)</span> minute power outage affecting the game by ultimately an exciting finish with the Ravens causing an upset victory <span class="math inline">\(34-31\)</span>. We will build our model from the viewpoint of the Ravens. Hence <span class="math inline">\(X_t\)</span> will correspond to the Raven’s score minus the San Francisco 49ers. <a href="#tbl-superbowl" class="quarto-xref">Table&nbsp;<span>7.3</span></a> provides the score at the end of each quarter.</p>
<div id="tbl-superbowl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.3: SuperBowl XLVII by Quarter
</figcaption>
<div aria-describedby="tbl-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th>0</th>
<th><span class="math inline">\(\frac{1}{4}\)</span></th>
<th><span class="math inline">\(\frac{1}{2}\)</span></th>
<th><span class="math inline">\(\frac{3}{4}\)</span></th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ravens</td>
<td>0</td>
<td>7</td>
<td>21</td>
<td>28</td>
<td>34</td>
</tr>
<tr class="even">
<td>49ers</td>
<td>0</td>
<td>3</td>
<td>6</td>
<td>23</td>
<td>31</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X_t\)</span></td>
<td>0</td>
<td>4</td>
<td>15</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To determine the parameters of our model we first use the <em>point spread</em> which was set at the Ravens being a four point underdog, i.e.&nbsp;<span class="math inline">\(\mu=-4\)</span>. This sets the mean of our outcome, <span class="math inline">\(X_1\)</span>, as <span class="math display">\[
\mu = \mathbb{E} \left (X_1 \right )=-4 .
\]</span> In reality, it was an exciting game with the Ravens upsetting the 49ers by <span class="math inline">\(34-31\)</span>. Hence, the realised outcome is <span class="math inline">\(X_1= 34-31=3\)</span> with the point spread being beaten by <span class="math inline">\(7\)</span> points or the equivalent of a touchdown.</p>
<div id="fig-superbowl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hal-superbowl.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.3: Superbowl XLVII: Ravens vs 49ers: TradeSports contracts traded and dynamic probability of the Ravens winning
</figcaption>
</figure>
</div>
<p>To determine the markets’ assessment of the probability that the Ravens would win at the beginning of the game we use the <em>money-line</em> odds. These odds were quoted as San Francisco <span class="math inline">\(-175\)</span> and Baltimore Ravens <span class="math inline">\(+155\)</span>. This implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155. We can convert both of these money-lines to <em>implied probabilities</em> of the each team winning, by the equations <span class="math display">\[
p_{SF} = \frac{175}{100+175} = 0.686 \; \; \text{ and} \; \; p_{Ravens} = \frac{100}{100+155} = 0.392
\]</span> The probability sum to one plus the market overround: <span class="math display">\[
p_{SF} + p_{Ravens} = 0.686+0.392 = 1.078
\]</span> namely a <span class="math inline">\(7.8\)</span>% edge for the bookmakers. Put differently, if bettors place money proportionally across both teams then the bookies <em>vig</em> will be <span class="math display">\[
\text{Vig} = \dfrac{0.078}{0.078+1} = 0.072
\]</span> This means that the bookmaker is expected to make a profit of 7.2% of the total amount staked, no matter what happens to the outcome of the game.</p>
<p>To account for this edge in our model, we use the mid-point of the spread to determine <span class="math inline">\(p\)</span> implying that <span class="math display">\[
p = \frac{1}{2} p_{Ravens} + \frac{1}{2} (1 - p_{SF} ) = 0.353
\]</span> From the Ravens perspective we have <span class="math inline">\(p = \mathbb{P}(X_1&gt;0) =0.353\)</span>.</p>
<p><a href="#fig-superbowl" class="quarto-xref">Figure&nbsp;<span>7.3</span></a> shows the evolution of the markets conditional probability of winning <span class="math inline">\(p_t\)</span> for the Ravens. The data are from the online betting website <code>TradeSports.com</code>. Starting at <span class="math inline">\(p=0.353\)</span> we see how dramatically the markets assessment of the Ravens winning can fluctuate. Given their commanding lead at half time, the probability has as high as <span class="math inline">\(0.90\)</span>. At the end of the four quarter when the 49ers nearly went into the lead with a touchdown, at one point the probability had dropped to <span class="math inline">\(30\)</span>%.</p>
<p>Our main question of interest is then: <em>What implied volatility is consistent with market expectations?</em></p>
<p>To calculate the implied volatility of the Superbowl we substitute the pair <span class="math inline">\((\mu,p)\)</span> into our definition and solve for <span class="math inline">\(\sigma_{IV}\)</span>. We obtain <span class="math display">\[
\sigma_{IV} = \frac{\mu}{\Phi^{-1}(p)} = \frac{-4}{-0.377}  = 10.60
\]</span> where we have used <span class="math inline">\(\Phi^{-1} ( p) = qnorm(0.353) = -0.377\)</span>. So on a volatility scale the <span class="math inline">\(4\)</span> point advantage assessed for the 49ers is under a <span class="math inline">\(\frac{1}{2} \sigma\)</span> favorite. From Table 2, this is consistent with a win probability of <span class="math inline">\(p=\Phi(\frac{1}{2})=0.69\)</span>. Another feature is that a <span class="math inline">\(\sigma=10.6\)</span> is historically low, as a typical volatility of an NFL game is <span class="math inline">\(14\)</span> (see Stern, 1991). However, the more competitive the game one might expect a lower volatility. In reality, the outcome <span class="math inline">\(X_1=3\)</span> was within one standard deviation of the model, which had an expectation of <span class="math inline">\(\mu=-4\)</span> and volatility <span class="math inline">\(\sigma=10.6\)</span>. Another question of interest is</p>
<p>What’s the probability of the Ravens winning given their lead at half time?</p>
<!-- To illustrate the dynamic nature of the odds and to infer a time-varying implied volatility we ask the question,    -->
<p>At half-time the Ravens were leading <span class="math inline">\(21\)</span> to <span class="math inline">\(6\)</span>. This gives us <span class="math inline">\(X(\frac{1}{2})=21-6=15\)</span>. From the online betting market we also have traded contracts on <code>TradeSports.com</code> that yield a current probability of <span class="math inline">\(p_{\frac{1}{2}} = 0.90\)</span>. <!-- Now we can ask  --></p>
<p>An alternative view is to assume that the market assesses time varying volatility and the prices fully reflect the underlying probability. Here we ask the question</p>
<p><em>What’s the implied volatility for the second half of the game?</em></p>
<p>We now have an implied volatility <span class="math display">\[
\sigma_{IV,t=\frac{1}{2}} = \frac{ l + \mu ( 1-t ) }{ \Phi^{-1} ( p_t )  \sqrt{1-t}} = \frac{15-2}{ \Phi^{-1}(0.9) / \sqrt{2} } = 14
\]</span> where <code>qnorm(0.9)=1.28</code>. Notice that <span class="math inline">\(14&gt; 10.6\)</span>, our assessment of the implied volatility at the beginning of the game.</p>
<p><em>What’s a valid betting strategy?</em></p>
<p>An alternative approach is to assume that the initial moneyline and point spread set the volatility and this stays constant throughout the game. This market is much larger than the online market and this is a reasonable assumption unless there has been material information as the game progresses such as a key injury.</p>
<p>Hence the market was expected a more typical volatility in the second half. If a bettor believed that the volatility was closer to the historical average of <span class="math inline">\(\sigma=14\)</span> (Stern, 1991), rather than the implied <span class="math inline">\(10.6\)</span>, then their assessment of the Ravens win probability would be lower. The remaining volatility would be <span class="math inline">\(14/\sqrt{2} \approx 9.9\)</span>, yielding a Z-score of <span class="math inline">\(13/9.9 \approx 1.31\)</span>. This corresponds to a win probability of <span class="math inline">\(\Phi(1.31) \approx 0.905\)</span>. Compared to the market probability of <span class="math inline">\(0.90\)</span>, this represents a small edge.</p>
<p>The Kelly criterion (Kelly,1956) yields the optimal betting fraction <span class="math inline">\(f^*\)</span>: <span class="math display">\[
f^* = p - \dfrac{q}{b} = 0.905 - \frac{0.095}{1/9} \approx 0.05
where $q = 1-p$ is the probability of losing, and $b$ is the odds received (net fractional odds).
\]</span> that is, <span class="math inline">\(5\)</span>% of capital. A more realistic strategy is to use the fractional Kelly criterion, which scales the bet by a risk-aversion parameter <span class="math inline">\(\gamma\)</span>. For example, in this case if <span class="math inline">\(\gamma =3\)</span>, we would bet <span class="math inline">\(0.05/3 \approx 0.017\)</span>, or <span class="math inline">\(1.7\)</span>% of our capital on this betting opportunity.</p>
<p>Finally, odds changes can be dramatic at the end of the fourth quarter, and this Super Bowl was no exception. With the score at <span class="math inline">\(34\)</span>–<span class="math inline">\(29\)</span> and only a few minutes remaining, the 49ers were at first-and-goal. A few minutes after this, the probability of the Ravens winning had dropped precipitously from over <span class="math inline">\(90\)</span>% to <span class="math inline">\(30\)</span>%, see <a href="#fig-superbowl" class="quarto-xref">Figure&nbsp;<span>7.3</span></a>. On San Francisco’s final offensive play of the game, Kaepernick threw a pass on fourth down to Michael Crabtree, but Ravens cornerback Jimmy Smith appeared to hold the wide receiver during the incompletion, No call was given and the final result was a Ravens win.</p>
</div>
</section>
</section>
<section id="poisson-process" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="poisson-process"><span class="header-section-number">7.3</span> Poisson Process</h2>
<p>A Poisson process is a fundamental stochastic process for modeling the occurrence of random events over time or space. It describes situations where events happen independently at a constant average rate, such as customer arrivals at a store, calls arriving at a call center, or goals scored in a soccer match.</p>
<p>Formally, a counting process <span class="math inline">\(\{N_t, t \geq 0\}\)</span> is a Poisson process with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span> if it satisfies the following properties:</p>
<ol type="1">
<li><span class="math inline">\(N(0) = 0\)</span> (the process starts at zero)</li>
<li>The process has independent increments: for any <span class="math inline">\(0 \leq t_1 &lt; t_2 &lt; \ldots &lt; t_n\)</span>, the random variables <span class="math inline">\(N(t_2) - N(t_1), N(t_3) - N(t_2), \ldots, N(t_n) - N(t_{n-1})\)</span> are independent</li>
<li>The process has stationary increments: for any <span class="math inline">\(s &lt; t\)</span>, the distribution of <span class="math inline">\(N_t - N(s)\)</span> depends only on the length of the interval <span class="math inline">\(t - s\)</span></li>
<li>For any interval of length <span class="math inline">\(t\)</span>, the number of events follows a Poisson distribution: <span class="math display">\[
P(N_t = k) = \frac{e^{-\lambda t}(\lambda t)^k}{k!}, \quad k = 0, 1, 2, \ldots
\]</span></li>
</ol>
<p>The parameter <span class="math inline">\(\lambda\)</span> represents the rate at which events occur per unit time. The expected number of events in an interval of length <span class="math inline">\(t\)</span> is <span class="math inline">\(\E{N_t} = \lambda t\)</span>, and the variance is <span class="math inline">\(\Var{N_t} = \lambda t\)</span>.</p>
<p><a href="#fig-poisson" class="quarto-xref">Figure&nbsp;<span>7.4</span></a> below shows three sample paths of a Poisson process with rate <span class="math inline">\(\lambda = 5\)</span> events per unit time.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-poisson" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="07-sp_files/figure-html/fig-poisson-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.4: Poisson Process Trajectories
</figcaption>
</figure>
</div>
</div>
</div>
<p>An equivalent characterization of the Poisson process is through the inter-arrival times between consecutive events. If <span class="math inline">\(T_1, T_2, \ldots\)</span> denote the times between successive events, then these are independent and identically distributed exponential random variables with mean <span class="math inline">\(1/\lambda\)</span>. This connection between the Poisson and exponential distributions is fundamental: the Poisson process counts events while the exponential distribution models the waiting time between events.</p>
<p>The Poisson process can be viewed from two complementary perspectives. From a <em>continuous-time</em> viewpoint, we track the evolution of the counting process <span class="math inline">\(N_t\)</span> as time progresses, asking questions about the probability of observing a certain number of events by time <span class="math inline">\(t\)</span> or the distribution of event times. From a <em>discrete count data</em> perspective, we observe the number of events that occurred during a fixed time interval and use this to make inferences about the underlying rate parameter <span class="math inline">\(\lambda\)</span>.</p>
<p><a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a> introduced Poisson models in the context of count data and Bayesian inference. The Poisson distribution (discussed in the section on Poisson Model for Count Data) emerges naturally when we observe a Poisson process over a fixed time interval. For instance, when modeling the number of goals scored by a soccer team in a match, we implicitly assume that goals occur according to a Poisson process with some rate <span class="math inline">\(\lambda\)</span>, and we observe the total count at the end of the match.</p>
<p>The Bayesian approach to learning about the rate parameter <span class="math inline">\(\lambda\)</span> (covered in the section on Poisson-Gamma: Learning about a Poisson Intensity in <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>) becomes particularly powerful in the continuous-time setting. When we observe a Poisson process over time, we can update our beliefs about <span class="math inline">\(\lambda\)</span> as new events occur. The Gamma distribution serves as a conjugate prior for <span class="math inline">\(\lambda\)</span>, meaning that if we start with a Gamma prior and observe events from a Poisson process, the posterior distribution remains in the Gamma family with updated parameters. This elegant updating mechanism allows us to refine our estimates of the event rate as we gather more data, balancing prior beliefs with observed evidence.</p>
<p>The connection between these perspectives is crucial for applications. In many real-world scenarios, we observe event counts over fixed intervals (discrete perspective) but need to make predictions about future events or the timing of the next event (continuous perspective). The Poisson process framework unifies these views, allowing us to seamlessly move between counting events and modeling their temporal dynamics.</p>
<div id="exm-eplodds" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.3 (EPL Betting)</strong></span> <span class="citation" data-cites="feng2016market">Feng, Polson, and Xu (<a href="references.html#ref-feng2016market" role="doc-biblioref">2016</a>)</span> employ a Skellam process (a difference of Poisson random variables) to model real-time betting odds for English Premier League (EPL) soccer games. Given a matrix of market odds on all possible score outcomes, we estimate the expected scoring rates for each team. The expected scoring rates then define the implied volatility of an EPL game. As events in the game evolve, they re-estimate the expected scoring rates and our implied volatility measure to provide a dynamic representation of the market’s expectation of the game outcome. They use real-time market odds data for a game between Everton and West Ham in the 2015-2016 season. We show how the implied volatility for the outcome evolves as goals, red cards, and corner kicks occur.</p>
<p>Gambling on soccer is a global industry with revenues of over $1 trillion a year (see “Football Betting - the Global Gambling Industry worth Billions,” BBC Sport). Betting on the result of a soccer match is a rapidly growing market, and online real-time odds exist (Betfair, Bet365, Ladbrokes). Market odds for all possible score outcomes (<span class="math inline">\(0-0, 1-0, 0-1, 2-0, \ldots\)</span>) as well as outright win, lose, and draw are available in real time. In this paper, we employ a two-parameter probability model based on a Skellam process and a non-linear objective function to extract the expected scoring rates for each team from the odds matrix. The expected scoring rates then define the implied volatility of the game.</p>
<p><strong>Skellam Process</strong></p>
<p>To model the outcome of a soccer game between team A and team B, we let the difference in scores, <span class="math inline">\(N_t = N_{A,t} - N_{B,t}\)</span>, where <span class="math inline">\(N_{A,t}\)</span> and <span class="math inline">\(N_{B,t}\)</span> are the team scores at time point <span class="math inline">\(t\)</span>. Negative values of <span class="math inline">\(N_t\)</span> indicate that team A is behind. We begin at <span class="math inline">\(N(0) = 0\)</span> and end at time one with <span class="math inline">\(N_1\)</span> representing the final score difference. The probability <span class="math inline">\(\mathbb{P}(N_1 &gt; 0)\)</span> represents the ex-ante odds of team A winning. Half-time score betting, which is common in Europe, is available for the distribution of <span class="math inline">\(N_{1/2}\)</span>.</p>
<p>Then we find a probabilistic model for the distribution of <span class="math inline">\(N_1\)</span> given <span class="math inline">\(N_t = \ell\)</span>, where <span class="math inline">\(\ell\)</span> is the current lead. This model, together with the current market odds, can be used to infer the expected scoring rates of the two teams and then to define the implied volatility of the outcome of the match. We let <span class="math inline">\(\lambda^A\)</span> and <span class="math inline">\(\lambda^B\)</span> denote the expected scoring rates for the whole game. We allow for the possibility that the scoring abilities (and their market expectations) are time-varying, in which case we denote the expected scoring rates after time <span class="math inline">\(t\)</span> by <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, respectively, instead of <span class="math inline">\(\lambda^A(1-t)\)</span> and <span class="math inline">\(\lambda^B(1-t)\)</span>.</p>
<p>The Skellam distribution is defined as the difference between two independent Poisson variables given by:</p>
<p><span class="math display">\[
\begin{aligned}
N_{A,t} &amp;= W_{A,t} + W_t \\
N_{B,t} &amp;= W_{B,t} + W_t
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(W_{A,t}\)</span>, <span class="math inline">\(W_{B,t}\)</span>, and <span class="math inline">\(W_t\)</span> are independent processes with:</p>
<p><span class="math display">\[
W_{A,t} \sim \text{Poisson}(\lambda^A t), \quad W_{B,t} \sim \text{Poisson}(\lambda^B t).
\]</span></p>
<p>Here <span class="math inline">\(W_t\)</span> is a Poisson process used to induce a correlation between the numbers of goals scored.</p>
<p><span id="eq-skellam"><span class="math display">\[
N_t = N_{A,t} - N_{B,t} \sim \text{Skellam}(\lambda^A t, \lambda^B t).
\tag{7.1}\]</span></span></p>
<p>At time <span class="math inline">\(t\)</span>, we have the conditional distributions:</p>
<p><span class="math display">\[
\begin{aligned}
W_A(1) - W_{A,t} &amp;\sim \text{Poisson}(\lambda^A(1-t)) \\
W_B(1) - W_{B,t} &amp;\sim \text{Poisson}(\lambda^B(1-t)).
\end{aligned}
\]</span></p>
<p>Now letting <span class="math inline">\(N^*(1-t)\)</span>, the score difference of the sub-game which starts at time <span class="math inline">\(t\)</span> and ends at time 1 and the duration is <span class="math inline">\((1-t)\)</span>. By construction, <span class="math inline">\(N_1 = N_t + N^*(1-t)\)</span>. Since <span class="math inline">\(N^*(1-t)\)</span> and <span class="math inline">\(N_t\)</span> are differences of two Poisson process on two disjoint time periods, by the property of Poisson process, <span class="math inline">\(N^*(1-t)\)</span> and <span class="math inline">\(N_t\)</span> are independent. Hence, we can re-express equation (<a href="#eq-skellam" class="quarto-xref">Equation&nbsp;<span>7.1</span></a>) in terms of <span class="math inline">\(N^*(1-t)\)</span>, and deduce</p>
<p><span class="math display">\[
%N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \sim Skellam(\lambda^A (1-t),\lambda^B (1-t) )
N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \sim \text{Skellam}(\lambda^A_t,\lambda^B_t)
\]</span></p>
<p>where <span class="math inline">\(W^*_A(1-t) = W_A(1) - W_{A,t}\)</span>, <span class="math inline">\(\lambda^A = \lambda^A_0\)</span> and <span class="math inline">\(\lambda^A_t=\lambda^A(1-t)\)</span>. A natural interpretation of the expected scoring rates, <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, is that they reflect the “net” scoring ability of each team from time <span class="math inline">\(t\)</span> to the end of the game. The term <span class="math inline">\(W_t\)</span> models a common strength due to external factors, such as weather. The “net” scoring abilities of the two teams are assumed to be independent of each other as well as the common strength factor. We can calculate the probability of any particular score difference, given by <span class="math inline">\(\mathbb{P}(N_1=x|\lambda^A,\lambda^B)\)</span>, at the end of the game where the <span class="math inline">\(\lambda\)</span>’s are estimated from the matrix of market odds. Team strength and “net” scoring ability can be influenced by various underlying factors, such as the offensive and defensive abilities of the two teams. The goal of our analysis is to only represent these parameters at every instant as a function of the market odds matrix for all scores.</p>
<p>Another quantity of interest is the conditional probability of winning as the game progresses. If the current lead at time <span class="math inline">\(t\)</span> is <span class="math inline">\(\ell\)</span>, and <span class="math inline">\(N_t=\ell=N_{A,t}-N_{B,t}\)</span>, the Poisson property implied that the final score difference <span class="math inline">\((N_1|N_t=\ell)\)</span> can be calculated by using the fact that <span class="math inline">\(N_1=N_t+N^*(1-t)\)</span> and <span class="math inline">\(N_t\)</span> and <span class="math inline">\(N^*(1-t)\)</span> are independent. Specifically, conditioning on <span class="math inline">\(N_t=\ell\)</span>, we have the identity</p>
<p><span class="math display">\[ N_1=N_t+N^*(1-t)=\ell+\text{Skellam}(\lambda^A_t,\lambda^B_t). \]</span></p>
<p>We are now in a position to find the conditional distribution (<span class="math inline">\(N_1=x|N_t=\ell\)</span>) for every time point <span class="math inline">\(t\)</span> of the game given the current score. Simply put, we have the time homogeneous condition</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}(N_1=x|\lambda^A_t,\lambda^B_t,N_t=\ell) &amp;= \mathbb{P}(N_1-N_t=x-\ell |\lambda^A_t,\lambda^B_t,N_t=\ell) \\
&amp;= \mathbb{P}(N^* (1-t)=x-\ell |\lambda^A_t,\lambda^B_t)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda^A_t\)</span>, <span class="math inline">\(\lambda^B_t\)</span>, <span class="math inline">\(\ell\)</span> are given by market expectations at time <span class="math inline">\(t\)</span>. See Feng et al.&nbsp;for details.</p>
<p><strong>Market Calibration</strong></p>
<p>Our information set at time <span class="math inline">\(t\)</span> includes the current lead <span class="math inline">\(N_t = \ell\)</span> and the market odds for <span class="math inline">\(\{Win, Lose, Draw, Score\}_t\)</span>, where <span class="math inline">\(Score_t = \{ ( i - j ) : i, j = 0, 1, 2, ....\}\)</span>. These market odds can be used to calibrate a Skellam distribution which has only two parameters <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>. The best fitting Skellam model with parameters <span class="math inline">\(\{\hat\lambda^A_t,\hat\lambda^B_t\}\)</span> will then provide a better estimate of the market’s information concerning the outcome of the game than any individual market (such as win odds) as they are subject to a “vig” and liquidity.</p>
<p>Suppose that the fractional odds for all possible final score outcomes are given by a bookmaker. Fractional odds, commonly used in the UK, express the ratio of profit to stake. For example, odds of <span class="math inline">\(3:1\)</span> (read as “three-to-one”) mean that for every $1 wagered, the bettor receives $3 in profit if the bet wins, plus the original $1 stake returned, for a total payout of $4. In this case, if the bookmaker offers <span class="math inline">\(3:1\)</span> odds on a 2-1 final score, the bookmaker pays out three times the amount staked by the bettor if the outcome is indeed 2-1. This contrasts with American money-line odds, where positive numbers indicate the profit on a $100 stake (e.g., +300 means $300 profit on $100 wagered), and negative numbers indicate the stake needed to win $100.</p>
<p>The market implied probability makes the expected winning amount of a bet equal to 0. For fractional odds of <span class="math inline">\(3:1\)</span>, the implied probability is calculated as <span class="math inline">\(p = \frac{1}{1+3} = \frac{1}{4} = 0.25\)</span> or 25%. We can verify this creates a fair bet: the expected winning amount is <span class="math inline">\(\mu = -1 \times (1-1/4) + 3 \times (1/4) = -0.75 + 0.75 = 0\)</span>. We denote these odds as <span class="math inline">\(odds(2,1) = 3\)</span>. To convert all the available odds to implied probabilities, we use the identity</p>
<p><span class="math display">\[ \mathbb{P}(N_A(1) = i, N_B(1) = j)=\frac{1}{1+odds(i,j)}. \]</span></p>
<p>The market odds matrix, <span class="math inline">\(O\)</span>, with elements <span class="math inline">\(o_{ij}=odds(i-1,j-1)\)</span>, <span class="math inline">\(i,j=1,2,3...\)</span> provides all possible combinations of final scores. Odds on extreme outcomes are not offered by the bookmakers. Since the probabilities are tiny, we set them equal to 0. The sum of the possible probabilities is still larger than 1 (see <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span> and <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span>). This “excess” probability corresponds to a quantity known as the “market vig.” For example, if the sum of all the implied probabilities is 1.1, then the expected profit of the bookmaker is 10%. To account for this phenomenon, we scale the probabilities to sum to 1 before estimation.</p>
<p>To estimate the expected scoring rates, <span class="math inline">\(\lambda^A_t\)</span> and <span class="math inline">\(\lambda^B_t\)</span>, for the sub-game <span class="math inline">\(N^*(1-t)\)</span>, the odds from a bookmaker should be adjusted by <span class="math inline">\(N_{A,t}\)</span> and <span class="math inline">\(N_{B,t}\)</span>. For example, if <span class="math inline">\(N_A(0.5)=1\)</span>, <span class="math inline">\(N_B(0.5)=0\)</span> and <span class="math inline">\(odds(2,1)=3\)</span> at half time, these observations actually says that the odds for the second half score being 1-1 is 3 (the outcomes for the whole game and the first half are 2-1 and 1-0 respectively, thus the outcome for the second half is 1-1). The adjusted <span class="math inline">\({odds}^*\)</span> for <span class="math inline">\(N^*(1-t)\)</span> is calculated using the original odds as well as the current scores and given by</p>
<p><span class="math display">\[
{odds}^*(x,y)=odds(x+N_{A,t},y+N_{B,t}).
\]</span></p>
<p>At time <span class="math inline">\(t\)</span> <span class="math inline">\((0\leq t\leq 1)\)</span>, we calculate the implied conditional probabilities of score differences using odds information</p>
<p><span class="math display">\[
\mathbb{P}(N_1=k|N_t=\ell)=\mathbb{P}(N^*(1-t)=k-\ell)=\frac{1}{c}\sum_{i-j=k-\ell}\frac{1}{1+{odds}^*(i,j)}
\]</span></p>
<p>where <span class="math inline">\(c=\sum_{i,j} \frac{1}{1+{odds}^*(i,j)}\)</span> is a scale factor, <span class="math inline">\(\ell=N_{A,t}-N_{B,t}\)</span>, <span class="math inline">\(i,j\geq 0\)</span> and <span class="math inline">\(k=0,\pm 1,\pm 2\ldots\)</span>.</p>
<p><strong>Example: Everton vs West Ham (3/5/2016)</strong></p>
<p>Table below shows the implied Skellam probabilities.</p>
<div id="tbl-Table1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-Table1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.4: Table: Original odds data from Ladbrokes before the game started.
</figcaption>
<div aria-describedby="tbl-Table1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Everton &nbsp;West Ham</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>11/1</td>
<td>12/1</td>
<td>28/1</td>
<td>66/1</td>
<td>200/1</td>
<td>450/1</td>
</tr>
<tr class="even">
<td>1</td>
<td>13/2</td>
<td>6/1</td>
<td>14/1</td>
<td>40/1</td>
<td>100/1</td>
<td>350/1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>7/1</td>
<td>7/1</td>
<td>14/1</td>
<td>40/1</td>
<td>125/1</td>
<td>225/1</td>
</tr>
<tr class="even">
<td>3</td>
<td>11/1</td>
<td>11/1</td>
<td>20/1</td>
<td>50/1</td>
<td>125/1</td>
<td>275/1</td>
</tr>
<tr class="odd">
<td>4</td>
<td>22/1</td>
<td>22/1</td>
<td>40/1</td>
<td>100/1</td>
<td>250/1</td>
<td>500/1</td>
</tr>
<tr class="even">
<td>5</td>
<td>50/1</td>
<td>50/1</td>
<td>90/1</td>
<td>150/1</td>
<td>400/1</td>
<td></td>
</tr>
<tr class="odd">
<td>6</td>
<td>100/1</td>
<td>100/1</td>
<td>200/1</td>
<td>250/1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>7</td>
<td>250/1</td>
<td>275/1</td>
<td>375/1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>8</td>
<td>325/1</td>
<td>475/1</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-Table1" class="quarto-xref">Table&nbsp;<span>7.4</span></a> shows the raw data of odds right the game. We need to transform odds data into probabilities. For example, for the outcome 0-0, 11/1 is equivalent to a probability of 1/12. Then we can calculate the marginal probability of every score difference from -4 to 5. We neglect those extreme scores with small probabilities and rescale the sum of event probabilities to one.</p>
<div id="tbl-Table2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-Table2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.5: Market implied probabilities for the score differences versus Skellam implied probabilities at different time points. The estimated parameters <span class="math inline">\(\hat\lambda^A=2.33\)</span>, <span class="math inline">\(\hat\lambda^B=1.44\)</span>.
</figcaption>
<div aria-describedby="tbl-Table2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Score difference</th>
<th>-4</th>
<th>-3</th>
<th>-2</th>
<th>-1</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Market Prob. (%)</td>
<td>1.70</td>
<td>2.03</td>
<td>4.88</td>
<td>12.33</td>
<td>21.93</td>
<td>22.06</td>
<td>16.58</td>
<td>9.82</td>
<td>4.72</td>
<td>2.23</td>
</tr>
<tr class="even">
<td>Skellam Prob. (%)</td>
<td>0.78</td>
<td>2.50</td>
<td>6.47</td>
<td>13.02</td>
<td>19.50</td>
<td>21.08</td>
<td>16.96</td>
<td>10.61</td>
<td>5.37</td>
<td>2.27</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-Table2" class="quarto-xref">Table&nbsp;<span>7.5</span></a> shows the model implied probability for the outcome of score differences before the game, compared with the market implied probability. As we see, the Skellam model appears to have longer tails. Different from independent Poisson modeling in <span class="citation" data-cites="dixon1997modelling">Dixon and Coles (<a href="references.html#ref-dixon1997modelling" role="doc-biblioref">1997</a>)</span>, our model is more flexible with the correlation between two teams. However, the trade-off of flexibility is that we only know the probability of score difference instead of the exact scores.</p>
<div id="fig-game2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-game2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/game2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-game2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.5: The betting market data for Everton and West Ham is from <a href="ladbrokes.com">ladbrokes.com</a>. Market implied probabilities (expressed as percentages) for three different results (Everton wins, West Ham wins and draw) are marked by three distinct colors, which vary dynamically as the game proceeds. The solid black line shows the evolution of the implied volatility. The dashed line shows significant events in the game, such as goals and red cards. Five goals in this game are 13’ Everton, 56’ Everton, 78’ West Ham, 81’ West Ham and 90’ West Ham.
</figcaption>
</figure>
</div>
<p><a href="#fig-game2" class="quarto-xref">Figure&nbsp;<span>7.5</span></a> examines the behavior of the two teams and represent the market predictions on the final result. Notably, we see the probability change of win/draw/loss for important events during the game: goals scoring and a red card penalty. In such a dramatic game, the winning probability of Everton gets raised to 90% before the first goal of West Ham in 78th minutes. The first two goals scored by West Ham in the space of 3 minutes completely reverses the probability of winning. The probability of draw gets raised to 90% until we see the last-gasp goal of West Ham that decides the game.</p>
<p><a href="#fig-game2" class="quarto-xref">Figure&nbsp;<span>7.5</span></a> plots the path of implied volatility throughout the course of the game. Instead of a downward sloping line, we see changes in the implied volatility as critical moments occur in the game. The implied volatility path provides a visualization of the conditional variation of the market prediction for the score difference. For example, when Everton lost a player by a red card penalty at 34th minute, our estimates <span class="math inline">\(\hat\lambda^A_t\)</span> and <span class="math inline">\(\hat\lambda^B_t\)</span> change accordingly. There is a jump in implied volatility and our model captures the market expectation adjustment about the game prediction. The change in <span class="math inline">\(\hat\lambda_A\)</span> and <span class="math inline">\(\hat\lambda_B\)</span> are consistent with the findings of <span class="citation" data-cites="vecer2009estimating">Vecer, Kopriva, and Ichiba (<a href="references.html#ref-vecer2009estimating" role="doc-biblioref">2009</a>)</span> where the scoring intensity of the penalized team drops while the scoring intensity of the opposing team increases. When a goal is scored in the 13th minute, we see the increase of <span class="math inline">\(\hat\lambda^B_t\)</span> and the market expects that the underdog team is pressing to come back into the game, an effect that has been well-documented in the literature. Another important effect that we observe at the end of the game is that as goals are scored (in the 78th and 81st minutes), the markets expectation is that the implied volatility increases again as one might expect.</p>
<div id="fig-ivcompare" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ivcompare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/iv2.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ivcompare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.6: Red line: the path of implied volatility throughout the game, i.e., <span class="math inline">\(\sigma_{t}^{red} = \sqrt{\hat\lambda^A_t+\hat\lambda^B_t}\)</span>. Blue lines: the path of implied volatility with constant <span class="math inline">\(\lambda^A+\lambda^B\)</span>, i.e., <span class="math inline">\(\sigma_{t}^{blue} = \sqrt{(\lambda^A+\lambda^B)*(1-t)}\)</span>. Here <span class="math inline">\((\lambda^A+\lambda^B) = 1, 2, ..., 8\)</span>.
</figcaption>
</figure>
</div>
<div id="tbl-lambda" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.6: The calibrated <span class="math inline">\(\{\hat\lambda^A_t, \hat\lambda^B_t\}\)</span> divided by <span class="math inline">\((1-t)\)</span> and the implied volatility during the game. <span class="math inline">\(\{\lambda^A_t, \lambda^B_t\}\)</span> are expected goals scored for rest of the game. The less the remaining time, the less likely to score goals. Thus <span class="math inline">\(\{\hat\lambda^A_t, \hat\lambda^B_t\}\)</span> decrease as <span class="math inline">\(t\)</span> increases to 1. Dividing them by <span class="math inline">\((1-t)\)</span> produces an updated version of <span class="math inline">\(\hat\lambda_{0}\)</span>’s for the whole game, which are in general time-varying (but not decreasing necessarily).
</figcaption>
<div aria-describedby="tbl-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>t</th>
<th>0</th>
<th>0.11</th>
<th>0.22</th>
<th>0.33</th>
<th>0.44</th>
<th>0.50</th>
<th>0.61</th>
<th>0.72</th>
<th>0.83</th>
<th>0.94</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat\lambda^A_t/(1-t)\)</span></td>
<td>2.33</td>
<td>2.51</td>
<td>2.53</td>
<td>2.46</td>
<td>1.89</td>
<td>1.85</td>
<td>2.12</td>
<td>2.12</td>
<td>2.61</td>
<td>4.61</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat\lambda^B_t/(1-t)\)</span></td>
<td>1.44</td>
<td>1.47</td>
<td>1.59</td>
<td>1.85</td>
<td>2.17</td>
<td>2.17</td>
<td>2.56</td>
<td>2.90</td>
<td>3.67</td>
<td>5.92</td>
<td>0</td>
</tr>
<tr class="odd">
<td><span class="math inline">\((\hat\lambda^A_t+\hat\lambda^B_t)/(1-t)\)</span></td>
<td>3.78</td>
<td>3.98</td>
<td>4.12</td>
<td>4.31</td>
<td>4.06</td>
<td>4.02</td>
<td>4.68</td>
<td>5.03</td>
<td>6.28</td>
<td>10.52</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma_{IV,t}\)</span></td>
<td>1.94</td>
<td>1.88</td>
<td>1.79</td>
<td>1.70</td>
<td>1.50</td>
<td>1.42</td>
<td>1.35</td>
<td>1.18</td>
<td>1.02</td>
<td>0.76</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-ivcompare" class="quarto-xref">Figure&nbsp;<span>7.6</span></a> compares the updating implied volatility of the game with implied volatilities of fixed <span class="math inline">\((\lambda^A+\lambda^B)\)</span>. At the beginning of the game, the red line (updating implied volatility) is under the “(<span class="math inline">\(\lambda^A+\lambda^B=4)\)</span>”-blue line; while at the end of the game, it’s above the “(<span class="math inline">\(\lambda^A+\lambda^B=8)\)</span>”-blue line. As we expect, the value of <span class="math inline">\((\hat\lambda^A_t + \hat\lambda^B_t)/(1-t)\)</span> in <a href="#tbl-lambda" class="quarto-xref">Table&nbsp;<span>7.6</span></a> increases throughout the game, implying that the game became more and more intense and the market continuously updates its belief in the odds.</p>
</div>
</section>
<section id="the-lévy-itô-decomposition-in-finance" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="the-lévy-itô-decomposition-in-finance"><span class="header-section-number">7.4</span> The Lévy-Itô Decomposition in Finance</h2>
<p>One of the most profound results in the theory of stochastic processes is the Lévy-Itô decomposition theorem, which provides a universal framework for understanding how randomness evolves over time. The theorem states that any Lévy process (a stochastic process with stationary and independent increments) can be uniquely decomposed into three fundamental components:</p>
<ol type="1">
<li>A deterministic drift term (linear trend)</li>
<li>A continuous Gaussian component (Brownian motion)</li>
<li>A pure jump component (compound Poisson process)</li>
</ol>
<p>Mathematically, any Lévy process <span class="math inline">\(X_t\)</span> can be written as: <span class="math display">\[
X_t = \mu t + \sigma B_t + J_t
\]</span> where <span class="math inline">\(\mu\)</span> is the drift coefficient, <span class="math inline">\(B_t\)</span> is standard Brownian motion with volatility <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(J_t\)</span> represents the jump component that can be expressed as: <span class="math display">\[
J_t = \sum_{i=1}^{N_t} Z_i
\]</span> where <span class="math inline">\(N_t\)</span> is a Poisson process counting the number of jumps up to time <span class="math inline">\(t\)</span>, and <span class="math inline">\(Z_i\)</span> are the jump sizes.</p>
<p>This decomposition is remarkable because it tells us that no matter how complex a stochastic process might appear, if it has independent and stationary increments, it can always be broken down into these three intuitive building blocks: a predictable trend, continuous random fluctuations, and discrete jumps.</p>
<p>The Lévy-Itô decomposition provides a natural motivation for studying Brownian motion and Poisson processes as fundamental objects. Brownian motion captures the continuous, infinitesimal random perturbations that accumulate over time, while the Poisson process models rare, discrete events that cause sudden changes in the system state. Together with a deterministic drift, these components form a complete toolkit for modeling virtually any phenomenon with independent increments.</p>
<p>The practical importance of this decomposition cannot be overstated. In finance, asset returns exhibit both continuous price movements (modeled by Brownian motion) and sudden jumps due to news announcements or market shocks (modeled by Poisson processes). In telecommunications, network traffic consists of a steady baseline load (drift) plus continuous fluctuations (Brownian component) and sudden spikes from large file transfers (jumps). In insurance, claim amounts follow a baseline trend with continuous variation and occasional catastrophic events.</p>
<div id="exm-levy-ito-finance" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.4 (Financial Asset Prices)</strong></span> Consider modeling the logarithm of a stock price. The Lévy-Itô decomposition suggests we should account for:</p>
<ul>
<li><em>Drift</em>: The expected return on the asset, reflecting long-term growth trends in the economy</li>
<li><em>Brownian component</em>: Day-to-day price fluctuations driven by the continuous arrival of market information and trading activity</li>
<li><em>Jump component</em>: Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks</li>
</ul>
<p>For instance, during the 2008 financial crisis, stock prices exhibited massive downward jumps that could not be explained by a pure Brownian motion model. The Lehman Brothers bankruptcy on September 15, 2008 caused the S&amp;P 500 to drop by 4.7% in a single day—an event that would have probability essentially zero under a Gaussian model but is naturally accommodated by the jump component in the Lévy-Itô framework.</p>
<p>The practical application of this decomposition led to the development of jump-diffusion models in quantitative finance, where option prices are calculated by accounting for both continuous price movements and discrete jumps. This approach provides more realistic pricing and risk assessment compared to the classical Black-Scholes model, which assumes only continuous price movements.</p>
<p>The Lévy-Itô decomposition thus provides both theoretical insight and practical tools. It explains why Brownian motion and Poisson processes are the fundamental building blocks for continuous-time stochastic modeling, and it gives practitioners a principled framework for decomposing complex random phenomena into interpretable components that can be estimated, simulated, and managed separately.</p>
</div>
</section>
<div id="newton-southsea">
<section id="newton-and-the-south-sea-bubble" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="newton-and-the-south-sea-bubble"><span class="header-section-number">7.5</span> Newton and the South Sea Bubble</h2>
<p>The South Sea Bubble of 1720 stands as one of history’s most spectacular financial disasters, demonstrating that even the greatest scientific minds can fall victim to speculative mania. The South Sea Company, established in 1711 ostensibly to trade with South America, proposed an audacious scheme: it would assume England’s national debt in exchange for company shares and exclusive trading privileges. By early 1720, the company’s directors launched an unprecedented campaign of stock manipulation, spreading rumors of fabulous wealth, bribing politicians and royalty, and offering generous credit terms that allowed investors to purchase shares with only a small down payment. The stock price soared from £128 at the start of 1720 to over £1,000 by June—an eightfold increase in six months (<a href="#fig-south-sea" class="quarto-xref">Figure&nbsp;<span>7.7</span></a>). Sir Isaac Newton, then Master of the Royal Mint, initially profited by selling his holdings in April 1720 for £7,000, but he could not resist re-entering the market after watching shares continue to climb. When the bubble burst in September, Newton lost £20,000—several years of his salary—leading him to famously remark, “I can calculate the movement of stars, but not the madness of men.” His experience reveals how market psychology and the fear of missing out can overwhelm even the most disciplined rational minds.</p>
<div id="fig-south-sea" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-south-sea-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/south-sea.jpeg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-south-sea-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.7: South Sea Bubble
</figcaption>
</figure>
</div>
<p>In the midst of this frenzy, Parliament passed the Bubble Act in June 1720, ironically just as South Sea stock reached its peak. While ostensibly designed to protect investors from fraudulent schemes, the Act was actually lobbied for by South Sea Company directors seeking to eliminate competition from rival ventures attracting investor capital. The Act required all joint-stock companies to obtain expensive royal charters and imposed severe penalties on unauthorized companies, effectively giving the South Sea Company a monopoly on investor enthusiasm. Paradoxically, by forcing the shutdown of smaller speculative ventures and drying up alternative investments, the Act may have hastened the bubble’s collapse by causing investors to question the sustainability of the broader market euphoria. The Act’s unintended consequences proved profound and long-lasting—it severely restricted the development of joint-stock companies in Britain for over a century until its repeal in 1825, arguably impeding industrialization by creating legal obstacles for large-scale ventures requiring significant capital. The episode offers enduring lessons about financial markets: price bubbles exhibit the characteristics of non-stationary stochastic processes with time-varying volatility and jump risk, leverage amplifies both gains and losses, regulatory interventions can create unintended consequences, and even rational agents can behave irrationally when caught in speculative manias—all phenomena that modern stochastic models attempt to capture.</p>
<p>The table below shows the prices from five historical bubbles, including the South Sea Bubble.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"../data/sea_bubble.csv"</span>,<span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(d)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 13%">
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 15%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Gregorian.Date</th>
<th style="text-align: right;">Bank.of.England</th>
<th style="text-align: right;">Royal.African.Company</th>
<th style="text-align: right;">Old.East.India.Company</th>
<th style="text-align: right;">South.Sea.Company</th>
<th style="text-align: right;">Mississippi.Company</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">21-8-1719</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">189</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">3400</td>
</tr>
<tr class="even">
<td style="text-align: left;">22-8-1719</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">188</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">3400</td>
</tr>
<tr class="odd">
<td style="text-align: left;">23-8-1719</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">188</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">3450</td>
</tr>
<tr class="even">
<td style="text-align: left;">24-8-1719</td>
<td style="text-align: right;">143</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">189</td>
<td style="text-align: right;">113</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: left;">25-8-1719</td>
<td style="text-align: right;">144</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">189</td>
<td style="text-align: right;">114</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: left;">26-8-1719</td>
<td style="text-align: right;">144</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">190</td>
<td style="text-align: right;">114</td>
<td style="text-align: right;">3600</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>The plot of the prices reveals the interconnected nature of early 18th-century financial manias and demonstrates the stochastic features that modern models attempt to capture. The South Sea Bubble exhibits the classic pattern of explosive growth followed by catastrophic collapse—a dramatic jump discontinuity in September 1720 that cannot be explained by continuous Brownian motion alone. Remarkably, the contagion spread across markets: the Bank of England, Royal African Company, and Old East India Company all show synchronized price movements during 1720, rising in sympathy with the South Sea speculation before experiencing their own sharp corrections. Most striking is the Mississippi Company plot, which tracks John Law’s concurrent bubble in France—it peaked slightly earlier than the South Sea Bubble and collapsed even more precipitously, suggesting that speculative manias can propagate across national borders. The synchronization across these four series illustrates volatility clustering and correlation jumps, phenomena that motivate the stochastic volatility models with correlated jumps discussed later in this chapter. These price paths exhibit all three components of the Lévy-Itô decomposition: drift during the accumulation phase, continuous Brownian fluctuations throughout, and sudden Poisson jumps at the moment of collapse.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="07-sp_files/figure-html/south-sea-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Historical Bubbles</figcaption>
</figure>
</div>
</div>
</div>
</section>
</div>
<section id="stochastic-volatility-financial-economics" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="stochastic-volatility-financial-economics"><span class="header-section-number">7.6</span> Stochastic Volatility: Financial Economics</h2>
<p>Financial markets exhibit time-varying volatility—periods of calm trading alternate with episodes of extreme price movements. The October 1987 crash dramatically illustrated the limitations of constant volatility models: the Dow Jones index fell 23% in a single day, an event that had probability essentially zero under the lognormal model assumed by the Black-Scholes framework. This observation motivated the development of stochastic volatility models that allow uncertainty itself to evolve randomly over time.</p>
<p>Robert Merton, who was a student of Samuelson, proposed a major extension to the work of Bachelier by introducing jumps to the model. The additive jump term addresses the issues of asymmetry and heavy tails in the distribution. Merton’s Jump Stochastic volatility model has a discrete-time version for log-returns, <span class="math inline">\(y_t\)</span>, with jump times, <span class="math inline">\(J_t\)</span>, jump sizes, <span class="math inline">\(Z_t\)</span>, and spot stochastic volatility, <span class="math inline">\(V_t\)</span>, given by the dynamics <span class="math display">\[\begin{align*}
    y_{t} &amp; \equiv \log \left( S_{t}/S_{t-1}\right) =\mu + V_t \varepsilon_{t}+J_{t}Z_{t} \\V_{t+1} &amp; = \alpha_v + \beta_v V_t + \sigma_v \sqrt{V_t} \varepsilon_{t}^v
\end{align*}\]</span> where <span class="math inline">\(\mathbb{P} \left ( J_t =1 \right ) = \lambda\)</span>, <span class="math inline">\(S_t\)</span> denotes a stock or asset price and log-returns <span class="math inline">\(y^t = (y_1,\ldots,y_t)\)</span> are the log-returns. The errors <span class="math inline">\((\varepsilon_{t},\varepsilon_{t}^v)\)</span> are possibly correlated bivariate normals. The investor must obtain optimal filters for <span class="math inline">\((V_t,J_t,Z_t)\)</span>, and learn the posterior densities of the parameters <span class="math inline">\((\mu, \alpha_v, \beta_v, \sigma_v^2 , \lambda )\)</span>. These estimates will be conditional on the information available at each time.</p>
<section id="motivation-combining-brownian-motion-and-jumps" class="level3">
<h3 class="anchored" data-anchor-id="motivation-combining-brownian-motion-and-jumps">Motivation: Combining Brownian Motion and Jumps</h3>
<p>The Lévy-Itô decomposition provides the theoretical foundation for modeling asset prices. Recall that any Lévy process can be decomposed into three fundamental components: deterministic drift, continuous Brownian fluctuations, and discrete jumps. In finance, this decomposition maps naturally to observed price dynamics:</p>
<ul>
<li><em>Drift</em> (<span class="math inline">\(\mu t\)</span>): The expected return on the asset, reflecting long-term growth trends</li>
<li><em>Brownian component</em> (<span class="math inline">\(\sigma B_t\)</span>): Continuous price fluctuations driven by the steady arrival of market information<br>
</li>
<li><em>Jump component</em> (<span class="math inline">\(\sum_{i=1}^{N_t} Z_i\)</span>): Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks</li>
</ul>
<p>Traditional models like Black-Scholes use only the first two components, assuming constant volatility <span class="math inline">\(\sigma\)</span>. However, empirical evidence overwhelmingly shows that volatility itself is stochastic and exhibits its own patterns: it clusters (high volatility follows high volatility), it mean-reverts to long-run averages, and it can experience sudden jumps during crises.</p>
<p>Stochastic volatility models extend the Lévy-Itô framework by allowing the volatility parameter to follow its own stochastic process. The most general formulation combines:</p>
<ol type="1">
<li><strong>Brownian motion</strong> for continuous price and volatility fluctuations</li>
<li><strong>Poisson processes</strong> for rare but important jump events in both prices and volatility</li>
<li><strong>Correlation structure</strong> to capture the <em>leverage effect</em>—the empirical observation that volatility tends to rise when prices fall</li>
</ol>
<p>This integration of the two fundamental stochastic processes creates a flexible modeling framework capable of capturing the rich dynamics observed in financial markets.</p>
<div id="exm-levy-finance-crashes" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.5 (Financial Crashes and the Need for Jumps)</strong></span> Consider the distribution of daily S&amp;P 500 returns. Under a Gaussian model with annualized volatility of 15%, a one-day drop of 5% should occur roughly once every 10,000 years. Yet such events occurred multiple times in recent decades: October 1987 (-20.5%), October 2008 (-9.0%), March 2020 (-12.0%). The empirical distribution exhibits <em>heavy tails</em>—extreme events occur far more frequently than predicted by the normal distribution.</p>
<p>Jump-diffusion models accommodate these events naturally. Instead of treating crashes as impossible outliers, they model them as rare but expected occurrences from the Poisson jump component. This provides more realistic risk assessment and option pricing, particularly for out-of-the-money puts that protect against market crashes.</p>
</div>
</section>
<section id="the-stochastic-volatility-model" class="level3">
<h3 class="anchored" data-anchor-id="the-stochastic-volatility-model">The Stochastic Volatility Model</h3>
<p>graph LR subgraph Latent_Process v_prev[v_{t-1}] –&gt; v_curr[v_t] v_curr –&gt; v_next[v_{t+1}] end subgraph Observed_Process y_prev[y_{t-1}] y_curr[y_t] y_next[y_{t+1}] end v_prev –&gt; y_curr v_curr –&gt; y_next J_curr[Jump J_t] –&gt; y_curr J_curr –&gt; v_curr ```basic stochastic volatility (SV) model extends the geometric Brownian motion of Black-Scholes by allowing volatility to evolve as a latent stochastic process. In continuous time, the log-price <span class="math inline">\(\log S_t\)</span> and its variance <span class="math inline">\(v_t\)</span> jointly evolve as:</p>
<p><span class="math display">\[\begin{align*}
d\log S_{t} &amp;= \mu dt + \sqrt{v_t} dB_{t}^{s}  \\
d\log v_{t} &amp;= \kappa_{v}(\theta_{v} - \log v_t) dt + \sigma_{v} dB_{t}^{v}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(B_{t}^{s}\)</span> and <span class="math inline">\(B_{t}^{v}\)</span> are (potentially correlated) Brownian motions. The variance follows a mean-reverting process in logs with:</p>
<ul>
<li><span class="math inline">\(\theta_v\)</span>: Long-run average log-variance</li>
<li><span class="math inline">\(\kappa_v\)</span>: Speed of mean reversion (how quickly volatility returns to its average)</li>
<li><span class="math inline">\(\sigma_v\)</span>: Volatility of volatility (how much randomness in the volatility process)</li>
</ul>
<p>Discretizing this model at daily or weekly intervals yields the discrete-time specification:</p>
<p><span class="math display">\[\begin{align*}
y_{t} &amp;= \mu + \sqrt{v_{t-1}} \varepsilon_{t}^{s}  \\
\log v_{t} &amp;= \alpha_{v} + \beta_{v} \log v_{t-1} + \sigma_{v} \varepsilon_{t}^{v}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(y_t = \log(S_t/S_{t-1})\)</span> are log-returns, <span class="math inline">\(\varepsilon_{t}^{s}, \varepsilon_{t}^{v} \sim N(0,1)\)</span> are standard normal innovations, and the parameters relate to the continuous-time specification via <span class="math inline">\(\alpha_v = \kappa_v \theta_v \Delta\)</span> and <span class="math inline">\(\beta_v = 1 - \kappa_v \Delta\)</span> for time interval <span class="math inline">\(\Delta\)</span>.</p>
<p>This model exhibits several desirable features:</p>
<ol type="1">
<li><strong>Volatility clustering</strong>: Since <span class="math inline">\(\log v_t\)</span> follows an AR(1), periods of high volatility tend to persist</li>
<li><strong>Stationarity</strong>: The mean-reverting specification ensures volatility doesn’t explode or collapse to zero</li>
<li><strong>Flexibility</strong>: The correlation between <span class="math inline">\(\varepsilon_t^s\)</span> and <span class="math inline">\(\varepsilon_t^v\)</span> allows for leverage effects</li>
</ol>
<p>An important empirical regularity in equity markets is that volatility tends to increase when prices fall—a phenomenon known as the <em>leverage effect</em>. While originally attributed to changing debt-to-equity ratios as stock prices move, it is now understood as a more general feature of risk dynamics.</p>
<p>To incorporate the leverage effect, we allow the innovations in returns and volatility to be correlated:</p>
<p><span class="math display">\[\begin{align*}
y_{t} &amp;= \mu + \sqrt{v_{t-1}} \varepsilon_{t}^{s}  \\
\log v_{t} &amp;= \alpha_{v} + \beta_{v} \log v_{t-1} + \sigma_{v}\left[\rho \varepsilon_{t}^{s} + \sqrt{1-\rho^2} \varepsilon_{t}^{v}\right]
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\rho &lt; 0\)</span> for equity returns. A negative return shock (<span class="math inline">\(\varepsilon_t^s &lt; 0\)</span>) directly increases log-volatility through the <span class="math inline">\(\rho\)</span> term, generating the observed inverse relationship between prices and volatility.</p>
<p>While stochastic volatility captures the time-varying nature of market uncertainty, it still relies on continuous Brownian motion for price movements. To accommodate the extreme events and heavy tails observed in returns, we augment the model with jump components—invoking the full Lévy-Itô decomposition.</p>
<p>The <strong>stochastic volatility with jumps</strong> (SVJ) model extends the basic SV specification by adding a Poisson-driven jump process to returns:</p>
<p><span class="math display">\[\begin{align*}
y_{t} &amp;= \mu + \sqrt{v_{t-1}} \varepsilon_{t}^{s} + J_t Z_t  \\
\log v_{t} &amp;= \alpha_{v} + \beta_{v} \log v_{t-1} + \sigma_{v} \varepsilon_{t}^{v}
\end{align*}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(J_t \sim \text{Bernoulli}(\lambda)\)</span> indicates whether a jump occurs at time <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(Z_t \sim N(\mu_Z, \sigma_Z^2)\)</span> is the jump size when a jump occurs</li>
<li><span class="math inline">\(\lambda\)</span> is the jump intensity (probability of a jump per period)</li>
</ul>
<p>The total variance of returns now decomposes into two sources:</p>
<p><span class="math display">\[
\text{Var}(y_t) = \E{v_{t-1}} + \lambda E[Z_t^2]
\]</span></p>
<p>The first term captures diffusive volatility from continuous fluctuations, while the second captures jump variance from discrete events. This allows the model to simultaneously fit the day-to-day variations (through <span class="math inline">\(v_t\)</span>) and occasional crashes (through jumps).</p>
<p>The 2008 financial crisis revealed another important feature: volatility itself experiences sudden jumps. The VIX index (a measure of market volatility expectations) more than doubled in a matter of days during the Lehman Brothers collapse. To capture this, we extend the model to allow jumps in both returns and volatility.</p>
<p>The <strong>stochastic volatility with correlated jumps</strong> (SVCJ) model specifies:</p>
<p><span class="math display">\[\begin{align*}
y_{t} &amp;= \mu + \sqrt{v_{t-1}} \varepsilon_{t}^{s} + J_t Z_t  \\
v_{t} &amp;= \alpha_{v} + \beta_{v} v_{t-1} + \sigma_{v}\sqrt{v_{t-1}} \varepsilon_{t}^{v} + J_t W_t
\end{align*}\]</span></p>
<p>where:</p>
<ul>
<li>The same Bernoulli <span class="math inline">\(J_t\)</span> triggers jumps in both returns and volatility (correlated jumps)</li>
<li><span class="math inline">\(Z_t | W_t \sim N(\mu_Z + \rho_J W_t, \sigma_Z^2)\)</span> allows jump sizes to be correlated</li>
<li><span class="math inline">\(W_t \sim \text{Exponential}(\mu_W)\)</span> ensures volatility jumps are positive</li>
</ul>
<p>The correlation parameter <span class="math inline">\(\rho_J &lt; 0\)</span> captures the empirical finding that large negative return jumps are typically accompanied by large positive volatility jumps. For example, during the March 2020 COVID-19 crash, the S&amp;P 500 fell sharply while the VIX spiked to record levels.</p>
<p>An even more flexible specification, the <strong>stochastic volatility with independent jumps</strong> (SVIJ) model, allows jumps in returns and volatility to occur independently, governed by separate Poisson processes with intensities <span class="math inline">\(\lambda_Z\)</span> and <span class="math inline">\(\lambda_W\)</span>. This provides maximum flexibility but requires more data to estimate reliably.</p>
<p>To understand the empirical importance of these model features, consider parameter estimates from S&amp;P 500 daily returns (1980-1999):</p>
<div id="tbl-sv-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sv-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7.7: Model features
</figcaption>
<div aria-describedby="tbl-sv-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th>SV</th>
<th>SVJ</th>
<th>SVCJ</th>
<th>SVIJ</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stochastic volatility</td>
<td>+</td>
<td>+</td>
<td>+</td>
<td>+</td>
</tr>
<tr class="even">
<td>Return jumps</td>
<td>–</td>
<td>+</td>
<td>+</td>
<td>+</td>
</tr>
<tr class="odd">
<td>Volatility jumps</td>
<td>–</td>
<td>–</td>
<td>+</td>
<td>+</td>
</tr>
<tr class="even">
<td>Independent jumps</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>+</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The estimated average annualized volatility across models is remarkably stable (around 15%), closely matching the sample standard deviation of 16%. However, the decomposition of variance sources differs:</p>
<ul>
<li><strong>SV model</strong>: All variation comes from the stochastic volatility component</li>
<li><strong>SVJ model</strong>: 85% from stochastic volatility, 15% from return jumps</li>
<li><strong>SVCJ model</strong>: 90% from stochastic volatility, 10% from return jumps</li>
<li><strong>SVIJ model</strong>: 92% from stochastic volatility, 8% from return jumps</li>
</ul>
<p>The diminishing role of return jumps as we add volatility jumps reflects an important finding: much of what appears as “jumps in returns” in simpler models is actually driven by <em>jumps in volatility</em>. When volatility suddenly spikes, even Brownian motion can generate large price movements that might be misidentified as jumps.</p>
<p>The mean reversion parameter <span class="math inline">\(\kappa_v\)</span> also varies across specifications. In the SVCJ and SVIJ models, <span class="math inline">\(\kappa_v\)</span> roughly doubles compared to the SV model, indicating that volatility reverts more quickly when jumps account for sudden large moves. The volatility-of-volatility parameter <span class="math inline">\(\sigma_v\)</span> correspondingly falls, as jumps handle the extreme variations.</p>
</section>
<section id="bayesian-inference-for-stochastic-volatility-models" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-inference-for-stochastic-volatility-models">Bayesian Inference for Stochastic Volatility Models</h3>
<p>Estimating stochastic volatility models presents a significant challenge: the volatility <span class="math inline">\(v_t\)</span> is never directly observed, appearing as a latent state variable. Classical maximum likelihood approaches require integrating out the entire volatility path, which is computationally intractable for nonlinear models with jumps.</p>
<p>The Bayesian approach via MCMC provides an elegant solution by treating the latent volatilities and jump indicators as parameters to be sampled alongside model parameters. The Clifford-Hammersley theorem structures the algorithm efficiently.</p>
<p>For the basic SV model with parameter vector <span class="math inline">\(\theta = (\alpha_v, \beta_v, \sigma_v^2)\)</span> and latent volatilities <span class="math inline">\(v = (v_1, \ldots, v_T)\)</span>, the joint posterior factors as:</p>
<p><span class="math display">\[
p(\theta, v | y) \propto p(y | v) p(v | \theta) p(\theta)
\]</span></p>
<p>The MCMC algorithm alternates between:</p>
<ol type="1">
<li><p><strong>Parameter update</strong>: <span class="math inline">\(p(\theta | v, y)\)</span></p>
<ul>
<li>Given volatilities, returns are conditionally normal: <span class="math inline">\(y_t | v_{t-1} \sim N(\mu, v_{t-1})\)</span></li>
<li>Log-volatilities follow AR(1): <span class="math inline">\(\log v_t | \log v_{t-1} \sim N(\alpha_v + \beta_v \log v_{t-1}, \sigma_v^2)\)</span></li>
<li>With conjugate priors, conditional posteriors are standard (Normal-Inverse-Gamma)</li>
</ul></li>
<li><p><strong>Volatility update</strong>: <span class="math inline">\(p(v_t | v_{t-1}, v_{t+1}, \theta, y)\)</span></p>
<ul>
<li>The conditional posterior for each <span class="math inline">\(v_t\)</span> combines information from:
<ul>
<li>The likelihood <span class="math inline">\(p(y_{t+1} | v_t)\)</span> (observed return depends on current volatility)</li>
<li>The state evolution <span class="math inline">\(p(v_t | v_{t-1})\)</span> (Markov dynamics from previous period)</li>
<li>The forward evolution <span class="math inline">\(p(v_{t+1} | v_t)\)</span> (Markov dynamics to next period)</li>
</ul></li>
<li>This distribution is non-standard and requires Metropolis-Hastings sampling</li>
</ul></li>
</ol>
<p>For the jump-augmented models (SVJ, SVCJ, SVIJ), we additionally sample:</p>
<ol start="3" type="1">
<li><p><strong>Jump indicators</strong>: <span class="math inline">\(p(J_t | v, Z, \theta, y)\)</span></p>
<ul>
<li>Each <span class="math inline">\(J_t \in \{0,1\}\)</span> follows a Bernoulli posterior</li>
<li>Large observed returns increase the probability of <span class="math inline">\(J_t = 1\)</span></li>
</ul></li>
<li><p><strong>Jump sizes</strong>: <span class="math inline">\(p(Z_t | J_t = 1, v, \theta, y)\)</span></p>
<ul>
<li>Conditional on a jump occurring, the jump size has a Normal posterior</li>
<li>The posterior mean balances the jump prior and the size needed to explain the observed return</li>
</ul></li>
</ol>
<p>This modular structure allows us to build up from simpler models (SV) to more complex specifications (SVIJ) by adding components one at a time, reusing the same basic algorithmic building blocks.</p>
<p>Stochastic volatility models with jumps have become standard tools in quantitative finance for several applications:</p>
<p><strong>Option Pricing</strong>: The Black-Scholes model systematically misprices options, particularly out-of-the-money puts. The <em>volatility smile</em>—the observation that implied volatilities increase for strikes far from the current price—reflects the market’s recognition of jump risk and stochastic volatility. Jump-diffusion models with stochastic volatility can reproduce these patterns, providing more accurate prices and hedging strategies.</p>
<p><strong>Risk Management</strong>: Value-at-Risk (VaR) and Expected Shortfall calculations based on constant-volatility Gaussian models dramatically underestimate tail risks. By properly accounting for stochastic volatility and jumps, firms can better quantify their exposure to extreme market movements. During the 2008 crisis, many institutions discovered their VaR models had severely underestimated potential losses.</p>
<p><strong>Portfolio Allocation</strong>: The presence of stochastic volatility creates hedging demands even for long-horizon investors. An investor who correctly anticipates that volatility is mean-reverting will reduce equity exposure when volatility is high (because expected returns are temporarily compressed) and increase exposure when volatility is low. This generates countercyclical trading strategies.</p>
<p><strong>Market Timing</strong>: The predictable component of volatility can be exploited for tactical asset allocation. Since volatility tends to mean-revert, unusually high volatility signals elevated future returns (as compensation for risk), making it an opportune time to increase risky asset exposure. Conversely, unusually low volatility may warrant defensive positioning.</p>
<p>The integration of Brownian motion and Poisson processes through stochastic volatility models exemplifies how the Lévy-Itô decomposition provides not just mathematical elegance, but practical power for understanding and managing financial risk in modern markets.</p>
<p>## Theoretical Foundations### Implications for Bayesian Learning</p>
<p>The law of large numbers provides theoretical justification for Bayesian learning. As we collect more data, the posterior distribution concentrates around the true parameter value, regardless of the prior. This phenomenon, known as <em>posterior consistency</em>, follows from the fact that the likelihood function—being a product of many terms—is dominated by the data for large samples.</p>
<p>Consider estimating the mean <span class="math inline">\(\mu\)</span> of a normal distribution from i.i.d. observations <span class="math inline">\(X_1, \ldots, X_n \sim N(\mu, \sigma^2)\)</span> with a prior <span class="math inline">\(\mu \sim N(\mu_0, \tau_0^2)\)</span>. The posterior mean is:</p>
<p><span class="math display">\[
\E{\mu \mid X_1, \ldots, X_n} = \frac{\tau_0^{-2}\mu_0 + n\sigma^{-2}\bar{X}_n}{\tau_0^{-2} + n\sigma^{-2}}
\]</span></p>
<p>As <span class="math inline">\(n \to \infty\)</span>, the data term <span class="math inline">\(n\sigma^{-2}\bar{X}_n\)</span> dominates, and by the law of large numbers, <span class="math inline">\(\bar{X}_n \to \mu\)</span> almost surely. Thus:</p>
<p><span class="math display">\[
\E{\mu \mid X_1, \ldots, X_n} \to \mu \quad \text{almost surely}
\]</span></p>
<p>The posterior concentrates at <span class="math inline">\(\mu\)</span>, regardless of the prior <span class="math inline">\(\mu_0\)</span>. The prior matters for small samples but becomes negligible for large samples—a reassuring property that ensures different researchers with different priors eventually reach consensus as evidence accumulates.</p>
</section>
<section id="connection-to-monte-carlo-methods" class="level3">
<h3 class="anchored" data-anchor-id="connection-to-monte-carlo-methods">Connection to Monte Carlo Methods</h3>
<p>The law of large numbers also underpins Monte Carlo simulation, a cornerstone of modern Bayesian computation. To approximate an expectation <span class="math inline">\(\E{f(X)}\)</span> where <span class="math inline">\(X \sim p(x)\)</span>, we draw i.i.d. samples <span class="math inline">\(X_1, \ldots, X_n \sim p(x)\)</span> and compute:</p>
<p><span class="math display">\[
\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^n f(X_i) \approx \E{f(X)}
\]</span></p>
<p>By the strong law of large numbers, <span class="math inline">\(\hat{\mu}_n \to \E{f(X)}\)</span> almost surely as <span class="math inline">\(n \to \infty\)</span>, provided <span class="math inline">\(\E{|f(X)|} &lt; \infty\)</span>. The approximation error decreases at rate <span class="math inline">\(O_p(n^{-1/2})\)</span> by the central limit theorem, giving us both convergence guarantees and quantifiable uncertainty.</p>
<p>This principle extends to Markov chain Monte Carlo (MCMC), where samples are dependent but ergodic. For an ergodic Markov chain with stationary distribution <span class="math inline">\(\pi(x)\)</span>, the ergodic theorem guarantees:</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^n f(X_i) \to \E[\pi]{f(X)} \quad \text{almost surely}
\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> are states visited by the chain. This justifies using MCMC to approximate posterior expectations in Bayesian inference, even though consecutive samples are correlated, see <span class="citation" data-cites="polson1996convergence">Polson (<a href="references.html#ref-polson1996convergence" role="doc-biblioref">1996</a>)</span> for a formal analysis.</p>
<section id="the-birth-of-monte-carlo-the-manhattan-project" class="level4">
<h4 class="anchored" data-anchor-id="the-birth-of-monte-carlo-the-manhattan-project">The Birth of Monte Carlo: The Manhattan Project</h4>
<p>The formal development of Monte Carlo methods emerged from the Manhattan Project during World War II, when Stanislaw Ulam, recovering from illness in 1946, realized that complex probability problems could be solved by simulating random processes rather than through analytical calculations. Ulam shared this insight with John von Neumann, who recognized its potential for solving neutron diffusion problems critical to nuclear weapons design and implemented the algorithms on early electronic computers like ENIAC. Nicholas Metropolis coined the term “Monte Carlo” as a reference to the Monaco casino, and the first unclassified paper appeared in 1949 <span class="citation" data-cites="metropolis1949monte">(<a href="references.html#ref-metropolis1949monte" role="doc-biblioref">Metropolis and Ulam 1949</a>)</span>. The 1953 Metropolis algorithm <span class="citation" data-cites="metropolis1953equation">(<a href="references.html#ref-metropolis1953equation" role="doc-biblioref">Metropolis et al. 1953</a>)</span> extended the method beyond simple averaging to sampling from complex distributions—precisely the situation in Bayesian inference—laying the groundwork for modern MCMC. The law of large numbers had existed for centuries, but only the combination of electronic computing and wartime urgency transformed this theoretical principle into the practical computational tool that underpins contemporary Bayesian statistics <span class="citation" data-cites="metropolis1987beginning">(<a href="references.html#ref-metropolis1987beginning" role="doc-biblioref">Metropolis 1987</a>)</span>.</p>
</section>
</section>
<section id="historical-context" class="level3">
<h3 class="anchored" data-anchor-id="historical-context">Historical Context</h3>
<p>The journey to Kolmogorov’s formalization spans several decades and illustrates the evolution of probability from intuitive reasoning to rigorous mathematics. The weak law of large numbers was first proved by Jakob Bernoulli in 1713 for the special case of binomial random variables—an achievement that took him over twenty years. The result was later generalized by Poisson (1837) and Chebyshev (1867) to broader classes of random variables.</p>
<p>The strong law required deeper mathematical machinery. Émile Borel proved a version for Bernoulli trials in 1909, but the general result awaited the development of measure-theoretic probability. Francesco Cantelli made progress in the 1910s, but it was Kolmogorov who provided the definitive treatment in 1933, unifying diverse results under a single rigorous framework.</p>
<p>Kolmogorov’s work transformed probability theory from a collection of special cases and heuristics into a branch of mathematics with the same rigor as analysis or algebra. His measure-theoretic foundations enabled precise statements about almost sure convergence, clarified the distinction between different modes of convergence, and opened the door to modern probability theory and stochastic processes.</p>
</section>
<section id="practical-significance" class="level3">
<h3 class="anchored" data-anchor-id="practical-significance">Practical Significance</h3>
<p>The law of large numbers is not merely a theoretical curiosity—it forms the bedrock of statistical practice. Every time we estimate a population mean from a sample, test a hypothesis, or train a machine learning model, we implicitly rely on the law of large numbers. The confidence we place in larger samples, the use of cross-validation to assess model performance, and the convergence of stochastic gradient descent in deep learning all trace back to this fundamental result.</p>
<p>In the context of stochastic processes, the law of large numbers justifies estimating process parameters from a single long trajectory. When modeling financial returns, climate data, or network traffic, we typically observe one realization over time rather than multiple independent realizations. The ergodic theorem ensures that time averages from this single path converge to the true population moments, enabling inference from the data we actually have.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-a.n.kolmogorov1938analytic" class="csl-entry" role="listitem">
A. N. Kolmogorov. 1938. <span>“On the Analytic Methods of Probability Theory.”</span> <em>Rossíiskaya Akademiya Nauk</em>, no. 5: 5–41.
</div>
<div id="ref-arnold2006forgotten" class="csl-entry" role="listitem">
Arnol’d, Vladimir I. 2006. <span>“Forgotten and Neglected Theories of <span>Poincaré</span>.”</span> <em>Russian Mathematical Surveys</em> 61 (1): 1.
</div>
<div id="ref-cootner1967random" class="csl-entry" role="listitem">
Cootner, Paul H. 1967. <em>The Random Character of Stock Market Prices</em>. MIT press.
</div>
<div id="ref-davison2003statistical" class="csl-entry" role="listitem">
Davison, Anthony Christopher. 2003. <em>Statistical Models</em>. Vol. 11. Cambridge university press.
</div>
<div id="ref-dixon1997modelling" class="csl-entry" role="listitem">
Dixon, Mark J., and Stuart G. Coles. 1997. <span>“Modelling <span>Association Football Scores</span> and <span>Inefficiencies</span> in the <span>Football Betting Market</span>.”</span> <em>Journal of the Royal Statistical Society Series C: Applied Statistics</em> 46 (2): 265–80.
</div>
<div id="ref-feng2016market" class="csl-entry" role="listitem">
Feng, Guanhao, Nicholas G. Polson, and Jianeng Xu. 2016. <span>“The <span>Market</span> for <span>English Premier League</span> (<span>EPL</span>) <span>Odds</span>.”</span> <em>Journal of Quantitative Analysis in Sports</em> 12 (4). <a href="https://arxiv.org/abs/1604.03614">https://arxiv.org/abs/1604.03614</a>.
</div>
<div id="ref-kolmogoroff1931uber" class="csl-entry" role="listitem">
Kolmogoroff, Andrei. 1931. <span>“Über Die Analytischen <span>Methoden</span> in Der <span>Wahrscheinlichkeitsrechnung</span>.”</span> <em>Mathematische Annalen</em> 104 (1): 415–58.
</div>
<div id="ref-logunov2004henri" class="csl-entry" role="listitem">
Logunov, A. A. 2004. <span>“Henri Poincare and Relativity Theory.”</span> <a href="https://arxiv.org/abs/physics/0408077">https://arxiv.org/abs/physics/0408077</a>.
</div>
<div id="ref-metropolis1987beginning" class="csl-entry" role="listitem">
Metropolis, Nicholas. 1987. <span>“The <span>Beginning</span> of the <span>Monte Carlo Method</span>.”</span> <em>Los Alamos Science</em> 15: 125–30.
</div>
<div id="ref-metropolis1953equation" class="csl-entry" role="listitem">
Metropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. <span>“Equation of <span>State Calculations</span> by <span>Fast Computing Machines</span>.”</span> <em>The Journal of Chemical Physics</em> 21 (6): 1087–92.
</div>
<div id="ref-metropolis1949monte" class="csl-entry" role="listitem">
Metropolis, Nicholas, and Stanislaw Ulam. 1949. <span>“The <span>Monte Carlo Method</span>.”</span> <em>Journal of the American Statistical Association</em> 44 (247): 335–41.
</div>
<div id="ref-poincare1898mesure" class="csl-entry" role="listitem">
Poincaré, Henri. 1898. <span>“La Mesure Du Temps.”</span> <em>Revue de Métaphysique Et de Morale</em> 6 (1): 1–13.
</div>
<div id="ref-polson1996convergence" class="csl-entry" role="listitem">
Polson, Nicholas. 1996. <span>“Convergence of <span>Markov</span> Chain <span>Monte Carlo</span> Algorithms (with Discussion).”</span> <em>Bayesian Statistics</em> 5: 297–321.
</div>
<div id="ref-shiryayev1992analytical" class="csl-entry" role="listitem">
Shiryayev, A. N. 1992. <span>“On Analytical Methods in Probability Theory.”</span> In <em>Selected Works of a. <span>N</span>. <span>Kolmogorov</span>: <span>Volume II</span> Probability Theory and Mathematical Statistics</em>, edited by A. N. Shiryayev, 62–108. Dordrecht: Springer Netherlands.
</div>
<div id="ref-stern1994brownian" class="csl-entry" role="listitem">
Stern, Hal S. 1994. <span>“A <span>Brownian Motion Model</span> for the <span>Progress</span> of <span>Sports Scores</span>.”</span> <em>Journal of the American Statistical Association</em> 89 (427): 1128–34.
</div>
<div id="ref-vecer2009estimating" class="csl-entry" role="listitem">
Vecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009. <span>“Estimating the <span>Effect</span> of the <span>Red Card</span> in <span>Soccer</span>: <span>When</span> to <span>Commit</span> an <span>Offense</span> in <span>Exchange</span> for <span>Preventing</span> a <span>Goal Opportunity</span>.”</span> <em>Journal of Quantitative Analysis in Sports</em> 5 (1).
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06-hyp.html" class="pagination-link" aria-label="Bayesian Hypothesis Testing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./08-gp.html" class="pagination-link" aria-label="Gaussian Processes">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>