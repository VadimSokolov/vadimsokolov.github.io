<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>19&nbsp; Neural Networks – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./20-theorydl.html" rel="next">
<link href="./18-theoryai.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="19&nbsp; Neural Networks – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="19-nn_files/figure-html/circle-data-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="19&nbsp; Neural Networks – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="19-nn_files/figure-html/circle-data-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-nn.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">19.1</span> Introduction</a></li>
  <li><a href="#motivating-examples" id="toc-motivating-examples" class="nav-link" data-scroll-target="#motivating-examples"><span class="header-section-number">19.2</span> Motivating Examples</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">19.3</span> Activation Functions</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-nn.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The goal of this section is to provide an overview of Deep Learning (DL). To do this, we have discussed the model estimation procedure and demonstrated that DL is an extension of a generalized linear model. One goal of statistics is to build predictive models along with uncertainty and to develop an understanding of the data generating mechanism. Data models are well studied in statistical literature, but often do not provide enough flexibility to learn the input-output relations. Closed box predictive rules, such as trees and neural networks, are more flexible learners. However, in high-dimensional problems, finding good models is challenging, and this is where deep learning methods shine. We can think of deterministic DL model as a transformation of high dimensional input and outputs. Hidden features lie on the transformed space, and are empirically learned as opposed to theoretically specified.</p>
<p>Although DL models have been almost exclusively used for problems of image analysis and natural language processing, more traditional data sets, which arise in finance, science and engineering, such as spatial and temporal data can be efficiently analyzed using deep learning. Thus, DL provides an alternative for applications where traditional statistical techniques apply. There are a number of areas of future research for Statisticians. In particular, uncertainty quantification and model selection, such as architecture design, as well as algorithmic improvements and Bayesian deep learning. We hope this review will make DL models accessible for statisticians.</p>
<p>In recent years neural networks and deep learning have re-emerged as a dominant technology for natural language processing (NLP), image analysis and reinforcement learning. The majority of applications use feed-forward neural network architectures such as convolutional neural networks and transformers. In this chapter we will focus on the feed-forward neural networks and their applications to regression and classification problems. We will also discuss the computational aspects of deep learning and its implementation in <code>R</code> and <code>Python</code>.</p>
<section id="introduction" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">19.1</span> Introduction</h2>
<p>Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.</p>
<p>Deep learning is one of the widely used machine learning methods for analysis of large-scale and high-dimensional data sets. Large-scale means that we have many samples (observations) and high dimensional means that each sample is a vector with many entries, usually hundreds and up.</p>
<p>Machine learning is the engineer’s version of statistical data analysis. The major difference between ML and statistics is that ML focuses on practical aspects, such as computational efficiency and ease of use of techniques, while statistical analysis is more concerned with the rigorousness of the analysis and interpretability of the results.</p>
<p>Deep learning provides a powerful pattern matching tool suitable for many AI applications. Image recognition and text analysis are probably two of deep learning’s most successful applications. From a computational perspective, you can think of an image or text as high-dimensional matrices and vectors, respectively. The problem of recognizing objects in images or translating a text requires designing complex decision boundaries in the high dimensional space of inputs.</p>
<p>Although image analysis and natural language processing are the applications where deep learning is the dominating approach, more traditional engineering and science applications, such as spatio-temporal and financial analysis, are where DL has also shown superior performance compared to traditional statistical learning techniques <span class="citation" data-cites="polson2021deep polson2017deep dixon2019deep sokolov2017discussion bhadra2021merging bhadra2021merging behnia2021deep nareklishvili2022deep nareklishvili2023feature polson2023generative nareklishvili2023generative polson2020deep">(<a href="references.html#ref-polson2021deep" role="doc-biblioref">N. Polson, Sokolov, and Xu 2021</a>; <a href="references.html#ref-polson2017deep" role="doc-biblioref">Nicholas G. Polson, Sokolov, et al. 2017</a>; <a href="references.html#ref-dixon2019deep" role="doc-biblioref">Dixon, Polson, and Sokolov 2019</a>; <a href="references.html#ref-sokolov2017discussion" role="doc-biblioref">Sokolov 2017</a>; <a href="references.html#ref-bhadra2021merging" role="doc-biblioref">Bhadra et al. 2021</a>, <a href="references.html#ref-bhadra2021merging" role="doc-biblioref">2021</a>; <a href="references.html#ref-behnia2021deep" role="doc-biblioref">Behnia, Karbowski, and Sokolov 2021</a>; <a href="references.html#ref-nareklishvili2022deep" role="doc-biblioref">Nareklishvili, Polson, and Sokolov 2022</a>, <a href="references.html#ref-nareklishvili2023feature" role="doc-biblioref">2023b</a>, <a href="references.html#ref-nareklishvili2023generative" role="doc-biblioref">2023a</a>; <a href="references.html#ref-polson2023generative" role="doc-biblioref">Nicholas G. Polson and Sokolov 2023</a>; <a href="references.html#ref-polson2020deep" role="doc-biblioref">N. Polson and Sokolov 2020</a>)</span></p>
<p>There are several deep learning architectures - each has its own uses and purposes. Convolutional Neural Networks (CNN) deal with 2-dimensional input objects, i.e., images, and have been shown to outperform other techniques. Recurrent Neural Networks (RNN) have shown the best performance on speech and text analysis tasks.</p>
<p>In general, a neural network can be described as follows. Let <span class="math inline">\(f_1 , \ldots , f_L\)</span> be given univariate activation functions for each of the <span class="math inline">\(L\)</span> layers. Activation functions are nonlinear transformations of weighted data. A semi-affine activation rule is then defined by <span class="math display">\[
f_l^{W,b} = f_l \left ( \sum_{j=1}^{N_l} W_{lj} X_j + b_l \right ) = f_l ( W_l X_l + b_l )\,
\]</span> which implicitly needs the specification of the number of hidden units <span class="math inline">\(N_l\)</span>. Our deep predictor, given the number of layers <span class="math inline">\(L\)</span>, then becomes the composite map</p>
<p><span class="math display">\[
\hat{Y}(X) = F(X) = \left ( f_l^{W_1,b_1} \circ \ldots \circ f_L^{W_L,b_L} \right ) ( X)\,.
\]</span></p>
<p>The fact that DL forms a universal “basis” which we recognize in this formulation dates to Poincare and Hilbert is central. From a practical perspective, given a large enough data set of “test cases”, we can empirically learn an optimal predictor. Similar to a classic basis decomposition, the deep approach uses univariate activation functions to decompose a high dimensional <span class="math inline">\(X\)</span>.</p>
<p>Let <span class="math inline">\(Z^{(l)}\)</span> denote the <span class="math inline">\(l\)</span>th layer, and so <span class="math inline">\(X = Z^{(0)}\)</span>. The final output <span class="math inline">\(Y\)</span> can be numeric or categorical. The explicit structure of a deep prediction rule is then <span class="math display">\[
\begin{aligned}
\hat{Y} (X) &amp; = W^{(L)} Z^{(L)} + b^{(L)} \\
Z^{(1)} &amp; = f^{(1)} \left ( W^{(0)} X + b^{(0)} \right ) \\
Z^{(2)} &amp; = f^{(2)} \left ( W^{(1)} Z^{(1)} + b^{(1)} \right ) \\
\ldots  &amp; \\
Z^{(L)} &amp; = f^{(L)} \left ( W^{(L-1)} Z^{(L-1)} + b^{(L-1)} \right )\,.
\end{aligned}
\]</span> Here <span class="math inline">\(W^{(l)}\)</span> is a weight matrix and <span class="math inline">\(b^{(l)}\)</span> are threshold or activation levels. Designing a good predictor depends crucially on the choice of univariate activation functions <span class="math inline">\(f^{(l)}\)</span>. The <span class="math inline">\(Z^{(l)}\)</span> are hidden features which the algorithm will extract.</p>
<p>Put differently, the deep approach employs hierarchical predictors comprising a series of <span class="math inline">\(L\)</span> nonlinear transformations applied to <span class="math inline">\(X\)</span>. Each of the <span class="math inline">\(L\)</span> transformations is referred to as a <em>layer</em>, where the original input is <span class="math inline">\(X\)</span>, the output of the first transformation is the first layer, and so on, with the output <span class="math inline">\(\hat Y\)</span> as the final layer. The layers <span class="math inline">\(1\)</span> to <span class="math inline">\(L\)</span> are called <em>hidden layers</em>. The number of layers <span class="math inline">\(L\)</span> represents the depth of our network.</p>
</section>
<section id="motivating-examples" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="motivating-examples"><span class="header-section-number">19.2</span> Motivating Examples</h2>
<p>Interaction terms, <span class="math inline">\(x_1 x_2\)</span> and <span class="math inline">\((x_1 x_2)^2\)</span>, and max functions, <span class="math inline">\(\max(x_1, x_2)\)</span> can be expressed as nonlinear functions of semi-affine combinations. Specifically:</p>
<p><span class="math display">\[
x_1x_2 = \frac{1}{4} ( x_1+x_2 )^2 - \frac{1}{4} (x_1-x_2)^2
\]</span></p>
<p><span class="math display">\[
\max(x_1,x_2) = \frac{1}{2} | x_1+x_2 | + \frac{1}{2} | x_1-x_2 |
\]</span></p>
<p><span class="math display">\[
(x_1x_2)^2 = \frac{1}{4} ( x_1+x_2 )^4 + \frac{7}{4 \cdot 3^3} (x_1-x_2)^4 - \frac{1}{2 \cdot 3^3} ( x_1+ 2 x_2)^4 - \frac{2^3}{3^3} ( x_1 + \frac{1}{2} x_2 )^4
\]</span></p>
<p><span class="citation" data-cites="diaconis1981generating">Diaconis and Shahshahani (<a href="references.html#ref-diaconis1981generating" role="doc-biblioref">1981</a>)</span> provides further discussion for Projection Pursuit Regression, where the network uses a layered model <span class="math inline">\(\sum_{i=1}^N f ( w_i^\top X )\)</span>. They provide an ergodic view of composite iterated functions, a precursor to the use of multiple layers of single operators that can model complex multivariate systems.</p>
<p>Deep ReLU architectures can be viewed as Max-Sum networks via the following simple identity. Define <span class="math inline">\(x^+ = \max(x,0)\)</span>. Let <span class="math inline">\(f_x(b) = (x + b)^+\)</span> where <span class="math inline">\(b\)</span> is an offset. Then <span class="math inline">\((x + y^+)^+ = \max(0, x, x+y)\)</span>. This is generalized in <span class="citation" data-cites="feller1971introduction">Feller (<a href="references.html#ref-feller1971introduction" role="doc-biblioref">1971</a>)</span> (p.272) who shows by induction that</p>
<p><span class="math display">\[
( f_{x_1} \circ \ldots \circ f_{x_k} ) (0) = ( x_1 + ( x_2 + \ldots + ( x_{k-1} + x_k^+ )^+ )^+ = \max_{1 \leq j \leq k} ( x_1 + \ldots + x_j )^+
\]</span></p>
<p>A composition or convolution of <span class="math inline">\(\max\)</span>-layers is then a one layer max-sum network.</p>
<p>Auto-encoding is an important data reduction technique. An auto-encoder is a deep learning architecture designed to replicate <span class="math inline">\(X\)</span> itself, namely <span class="math inline">\(X=Y\)</span>, via a <em>bottleneck</em> structure. This means we select a model <span class="math inline">\(F^{W,b}(X)\)</span> which aims to concentrate the information required to recreate <span class="math inline">\(X\)</span>. See Heaton et al (2017) for an application to smart indexing in finance.</p>
<p>Suppose that we have <span class="math inline">\(N\)</span> input vectors <span class="math inline">\(X = \{ x_1 , \ldots , x_N \} \in \mathbb{R}^{M\times N}\)</span> and <span class="math inline">\(N\)</span> output (or target) vectors <span class="math inline">\(\{ x_1 , \ldots , x_N \} \in \mathbb{R}^{M\times N}\)</span>. Setting biases to zero, for the purpose of illustration, and using only one hidden layer (<span class="math inline">\(L=2\)</span>) with <span class="math inline">\(K &lt; N\)</span> factors, gives for <span class="math inline">\(j=1, \ldots, N\)</span>:</p>
<p><span class="math display">\[
Y_j(x) = F^m_{W} ( X )_j = \sum_{k=1}^K W^{jk}_2 f \left ( \sum_{i=1}^N W^{ki}_1 x_i \right ) =  \sum_{k=1}^K W^{jk}_2 Z_j \quad \text{for } Z_j =  f \left ( \sum_{i=1}^N W^{ki}_1 x_i \right )
\]</span></p>
<p>Since, in an auto-encoder, we fit the model <span class="math inline">\(X = F_{W}( X)\)</span>, and <em>train</em> the weights <span class="math inline">\(W = (W_1, W_2)\)</span> with regularization penalty in a</p>
<p><span class="math display">\[
\mathcal{L} ( W )  =  \operatorname{argmin}_W \Vert X - F_W (X) \Vert^2  + \lambda \phi(W)
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\phi(W) = \sum_{i,j,k} | W^{jk}_1 |^2 +  | W^{ki}_2 |^2.
\]</span></p>
<p>Writing the DL objective as an augmented Lagrangian (as in ADMM) with a hidden factor <span class="math inline">\(Z\)</span>, leads to a two step algorithm: an encoding step (a penalty for <span class="math inline">\(Z\)</span>), and a decoding step for reconstructing the output signal via</p>
<p><span class="math display">\[
\operatorname{argmin}_{W,Z} \Vert X - W_2 Z \Vert^2 + \lambda \phi(Z) + \Vert Z -  f( W_1, X ) \Vert^2,
\]</span></p>
<p>where the regularization on <span class="math inline">\(W_1\)</span> induces a penalty on <span class="math inline">\(Z\)</span>. The last term is the encoder, the first two the decoder.</p>
<p>If <span class="math inline">\(W_2\)</span> is estimated from the structure of the training data matrix, then we have a traditional factor model, and the <span class="math inline">\(W_1\)</span> matrix provides the factor loadings. PCA, PLS, SIR fall into this category (see Cook 2007 for further discussion). If <span class="math inline">\(W_2\)</span> is trained based on the pair <span class="math inline">\(\hat{X}=\{Y,X\}\)</span> then we have a sliced inverse regression model. If <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are simultaneously estimated based on the training data <span class="math inline">\(X\)</span>, then we have a two layer deep learning model.</p>
<p>Auto-encoding demonstrates that deep learning does not directly model variance-covariance matrix explicitly as the architecture is already in predictive form. Given a hierarchical non-linear combination of deep learners, an implicit variance-covariance matrix exists, but that is not the driver of the algorithm.</p>
<p>Another interesting area for future research are long short term memory models (LSTMs). For example, a dynamic one layer auto-encoder for a financial time series <span class="math inline">\((Y_t)\)</span> is a coupled system of the form</p>
<p><span class="math display">\[
Y_t = W_x X _t + W_y Y_{t-1} \quad \text{and} \quad \begin{pmatrix} X_t \\ Y_{t-1} \end{pmatrix} = W Y_t
\]</span></p>
<p>Here, the state equation encodes and the matrix <span class="math inline">\(W\)</span> decodes the <span class="math inline">\(Y_t\)</span> vector into its history <span class="math inline">\(Y_{t-1}\)</span> and the current state <span class="math inline">\(X_t\)</span>.</p>
<p>We will start with applying a feed-forward neural network with one hidden layer to a problem of binary classification on a simulated data set. We start by generating a simple dataset shown in Figure below. The data is generated from a mixture of two distributions (Gaussian and truncated Gaussian). The red points are the positive class and the green points are the negative class. The goal is to find a model boundary that discriminates the two classes.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-data-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Let’s try to use a simple logistic regression model to separate the two classes.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glm</span>(label<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data=</span><span class="fu">as.data.frame</span>(d), <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">'logit'</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training dataset</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d[,<span class="dv">2</span>],d[,<span class="dv">3</span>], <span class="at">col=</span>d[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>th <span class="ot">=</span> fit<span class="sc">$</span>coefficients</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span>th[<span class="dv">1</span>]<span class="sc">/</span>th[<span class="dv">3</span>], <span class="sc">-</span>th[<span class="dv">2</span>]<span class="sc">/</span>th[<span class="dv">3</span>], <span class="at">col=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-lr-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that a logistic regression could not do it. It uses a single line to separate observations of two classes. We can see that the data is not linearly separable. However, we can use multiple lines to separate the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1<span class="sc">~</span>x2, <span class="at">data=</span>d,<span class="at">col=</span>d[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot lines that separate once class (red) from another (green)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1, <span class="sc">-</span>x1 <span class="sc">-</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1, <span class="sc">-</span>x1 <span class="sc">+</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,  x1 <span class="sc">-</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="dv">4</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,  x1 <span class="sc">+</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/sep-lines-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now, we do the same thing as in simple logistic regression and apply logistic function to each of those lines</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define sigmoid function</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sigmoid  <span class="ot">=</span> <span class="cf">function</span>(z) <span class="fu">exp</span>(z)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(z))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define hidden layer of our neural network</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>features <span class="ot">=</span> <span class="cf">function</span>(x1,x2) {</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  z1 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">+</span> x1 <span class="sc">+</span> x2; a1 <span class="ot">=</span> <span class="fu">sigmoid</span>(z1)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  z2 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">-</span> x1 <span class="sc">-</span> x2; a2 <span class="ot">=</span> <span class="fu">sigmoid</span>(z2)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  z3 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">-</span> x1 <span class="sc">+</span> x2; a3 <span class="ot">=</span> <span class="fu">sigmoid</span>(z3)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  z4 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">+</span> x1 <span class="sc">-</span> x2; a4 <span class="ot">=</span> <span class="fu">sigmoid</span>(z4)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(a1,a2,a3,a4))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using the matrix notation, we have <span class="math display">\[
z = \sigma(Wx + b), ~ W = \begin{bmatrix} 1 &amp; 1 \\ -1 &amp; -1 \\ -1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}, ~ b = \begin{bmatrix} 6 \\ 6 \\ 6 \\ 6 \end{bmatrix}, ~ \sigma(z) = \frac{1}{1+e^{-z}}
\]</span></p>
<p>The model shown above is the first layer of our neural network. It takes a two-dimensional input <span class="math inline">\(x\)</span> and produces a four-dimensional output <span class="math inline">\(z\)</span> which is called a feature vector. The feature vector is then passed to the output layer, which applies simple logistic regression to the feature vector. <span class="math display">\[
\hat{y} = \sigma(w^Tz + b), ~ w = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, ~ b = -3.1, ~ \sigma(z) = \frac{1}{1+e^{-z}}
\]</span></p>
<p>The output of the output layer is the probability of the positive class.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction (classification) using our neural network</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>predict_prob <span class="ot">=</span> <span class="cf">function</span>(x){</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> x[<span class="dv">1</span>]; x2 <span class="ot">=</span> x[<span class="dv">2</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">=</span> <span class="fu">features</span>(x1,x2)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(z)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">sum</span>(z) <span class="sc">-</span> <span class="fl">3.1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(mu)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sigmoid</span>(mu)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use our model to do the predictions now</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the probability of the positive class for a given point</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict_prob</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.71</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict_prob</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.26</code></pre>
</div>
</div>
<p>The model generates sensible predictions, let’s plot the decision boundary to see how well it separates the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">11</span>,<span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">11</span>,<span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>gr <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">expand.grid</span>(x1,x2));</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 10000     2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">apply</span>(gr,<span class="dv">1</span>,predict_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 10000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1,x2,<span class="fu">matrix</span>(yhat,<span class="at">ncol =</span> <span class="dv">100</span>), <span class="at">col =</span> <span class="fu">heat.colors</span>(<span class="dv">20</span>,<span class="fl">0.7</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-boundary-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>How about a regression model? We will use a one-layer neural network to fit a quadratic function. We simulate noisy data from the following model <span class="math display">\[
y = 0.5 + 0.3x^2 + \epsilon, ~ \epsilon \sim N(0,0.02^2)
\]</span> And use 3 hidden units in the first hidden layer and two units in the second hidden layer. The output layer is a single unit. We will use the hyperbolic tangent (<code>tanh</code>) activation function for all layers. The model is defined as follows</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>relu <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">max</span>(<span class="dv">0</span>,x)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">=</span> <span class="cf">function</span>(W,<span class="at">f=</span>relu) {</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">=</span> W[<span class="dv">1</span>]; w0<span class="ot">=</span>W[<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>];b1 <span class="ot">=</span> W[<span class="dv">5</span>]; w1 <span class="ot">=</span> W[<span class="dv">6</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    z0 <span class="ot">=</span> <span class="fu">apply</span>(b0 <span class="sc">+</span> <span class="fu">outer</span>(x,w0,<span class="st">'*'</span>),<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,f)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">=</span> b1 <span class="sc">+</span> z0 <span class="sc">%*%</span> w1</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">yhat =</span> yhat[,<span class="dv">1</span>],<span class="at">z0=</span>z0))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hidden layer has three outputs (neurons) and uses the ReLU activation function. The output linear layer has a single output. Thus, the prediction <code>yhat</code> is generated as a linear model of the feature vector <code>z0</code>. The model has 8 parameters. Let’s generate training data and fit the model. We will use the BFGS optimization algorithm to minimize the loss function (negative log-likelihood) of the model.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">99</span>) <span class="co">#gretzky</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>nl  <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.02</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x),<span class="dv">0</span>,<span class="fl">0.02</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">sum</span>((<span class="fu">nn</span>(W)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(params, loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> -0.24  1.39 -0.82  0.46  0.50  0.17  0.46  0.39</code></pre>
</div>
</div>
<p><a href="#fig-relu" class="quarto-xref">Figure&nbsp;<span>19.1</span></a> shows the quadratic function and the neural network model. The solid black line is the neural network model, and the dashed lines are the basis functions. The model fits the data well.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y); <span class="fu">lines</span>(x,o<span class="sc">$</span>yhat, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">1</span>],<span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">3</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-relu" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-relu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="19-nn_files/figure-html/fig-relu-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-relu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.1: Noisy quadratic function approximated by a neural network with ReLU activation function.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s try the <span class="math inline">\(\tanh\)</span> function</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>) <span class="co">#gretzky</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">mean</span>((<span class="fu">nn</span>(W,<span class="at">f=</span>tanh)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(params, loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> -0.98 -0.23  0.83 -1.14  0.84 -0.65  0.59  0.53</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par, <span class="at">f=</span>tanh)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y, <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.4</span>,<span class="fl">0.95</span>)); <span class="fu">lines</span>(x,o<span class="sc">$</span>yhat, <span class="at">lwd=</span><span class="dv">2</span>);</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">1</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">2</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">3</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Noisy quadratic function approximated by a neural network with tanh activation function.</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that we did not have to explicitly specify that our model needed to have a quadratic term; the model learned it from the data. This is the power of deep learning. The model is able to learn the structure of the data from the data itself.</p>
<p>We can apply the same approach to interactions. Say the true model for the data is as follows: <span class="math display">\[
y = 0.5 + 0.1x_1 + 0.2x_2  + 0.5x_1x_2+ \epsilon, ~ \epsilon \sim N(0,0.02^2)
\]</span> We can use the same model as above, but with two input variables. The model will learn the interaction term from the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">99</span>) <span class="co">#ovi</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> x1</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.1</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x1<span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x1),<span class="dv">0</span>,<span class="fl">0.02</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scatterplot3d"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>s3d <span class="ot">=</span> <span class="fu">scatterplot3d</span>(x1,x2,y, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">cbind</span>(x1,x2)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">=</span> <span class="cf">function</span>(W,<span class="at">f=</span>relu) {</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">=</span> W[<span class="dv">1</span>]; w0 <span class="ot">=</span> W[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>]; b1 <span class="ot">=</span> W[<span class="dv">6</span>]; w1 <span class="ot">=</span> W[<span class="dv">7</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    w0 <span class="ot">=</span> <span class="fu">matrix</span>(w0,<span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    z0 <span class="ot">=</span> <span class="fu">apply</span>(b0 <span class="sc">+</span> x<span class="sc">%*%</span>w0,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,f)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">=</span> b1 <span class="sc">+</span> z0 <span class="sc">%*%</span> w1</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">yhat =</span> yhat[,<span class="dv">1</span>],<span class="at">z0=</span>z0))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">4</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">2</span>))</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">sum</span>((<span class="fu">nn</span>(W, <span class="at">f=</span>tanh)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(W, <span class="at">fn=</span>loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  0.78  0.50 -1.39  0.63 -0.94 -2.06 -2.88  6.78</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par, <span class="at">f=</span>tanh)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>yhat, <span class="at">col=</span><span class="dv">2</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>z0[,<span class="dv">1</span>], <span class="at">col=</span><span class="dv">3</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>z0[,<span class="dv">2</span>], <span class="at">col=</span><span class="dv">4</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Effectively, you can think of the neural network as a flexible function approximator, equivalent to a nonparametric regression approach which learns the basis functions from data. The model can learn the structure of the data from the data itself. This is the power of deep learning.</p>
</section>
<section id="activation-functions" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">19.3</span> Activation Functions</h2>
<p>The last output layer of a neural network has sigmoid activation function for binary output variable (classification) and no activation function for continuous output variable regression. The hidden layers can have different activation functions. The most common activation functions are the hyperbolic tangent function and the rectified linear unit (ReLU) function.</p>
<p>A typical approach is to use the same activation function for all hidden layers. The hyperbolic tangent function is defined as <span class="math display">\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]</span> Notice that the hyperbolic tangent function is a scaled version of the sigmoid function, with <span class="math inline">\(\tanh(0) = 0\)</span>. It is a smooth function which is differentiable everywhere. However,</p>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="3" data-null_prefix="true">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>tanh</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Hard tanh</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-3.png" class="img-fluid figure-img" width="576"></p>
<figcaption>softplus</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-4.png" class="img-fluid figure-img" width="576"></p>
<figcaption>ReLU</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-5.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Leaky ReLU</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-6.png" class="img-fluid figure-img" width="576"></p>
<figcaption>sigmoid</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Typically <span class="math inline">\(\tanh\)</span> is preferred to the sigmoid function because it is zero-centered. The major drawback of sigmoid and <span class="math inline">\(\tanh\)</span> functions is that they saturate when the input is very large or very small. When we try to learn the weights of the network, the optimization algorithms make small steps in the space of the parameters and when the weights are large the small changes won’t affect the values of the layers’ outputs and optimization will “stagnate.”</p>
<p>This means that the gradient of the function is very small, which makes learning slow. The ReLU function is defined as <span class="math display">\[
\text{ReLU}(z) = \max(0,z)
\]</span> The ReLU function is a piecewise linear function which is computationally efficient and easy to optimize. The ReLU function is the most commonly used activation function in deep learning. The ReLU function is not differentiable at <span class="math inline">\(z=0\)</span>, but it is differentiable everywhere else. The derivative of the ReLU function is <span class="math display">\[
\text{ReLU}'(z) = \begin{cases} 0 &amp; \text{if } z &lt; 0 \\ 1 &amp; \text{if } z &gt; 0 \end{cases}
\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-behnia2021deep" class="csl-entry" role="listitem">
Behnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2021. <span>“Deep Generative Models for Vehicle Speed Trajectories.”</span> <em>arXiv Preprint arXiv:2112.08361</em>. <a href="https://arxiv.org/abs/2112.08361">https://arxiv.org/abs/2112.08361</a>.
</div>
<div id="ref-bhadra2021merging" class="csl-entry" role="listitem">
Bhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and Jianeng Xu. 2021. <span>“Merging Two Cultures: Deep and Statistical Learning.”</span> <em>arXiv Preprint arXiv:2110.11561</em>. <a href="https://arxiv.org/abs/2110.11561">https://arxiv.org/abs/2110.11561</a>.
</div>
<div id="ref-diaconis1981generating" class="csl-entry" role="listitem">
Diaconis, Persi, and Mehrdad Shahshahani. 1981. <span>“Generating a Random Permutation with Random Transpositions.”</span> <em>Probability Theory and Related Fields</em> 57 (2): 159–79.
</div>
<div id="ref-dixon2019deep" class="csl-entry" role="listitem">
Dixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. <span>“Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.”</span> <em>Applied Stochastic Models in Business and Industry</em> 35 (3): 788–807.
</div>
<div id="ref-feller1971introduction" class="csl-entry" role="listitem">
Feller, William. 1971. <em>An Introduction to Probability Theory and Its Applications</em>. Wiley.
</div>
<div id="ref-nareklishvili2022deep" class="csl-entry" role="listitem">
Nareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022. <span>“Deep Partial Least Squares for Iv Regression.”</span> <em>arXiv Preprint arXiv:2207.02612</em>. <a href="https://arxiv.org/abs/2207.02612">https://arxiv.org/abs/2207.02612</a>.
</div>
<div id="ref-nareklishvili2023generative" class="csl-entry" role="listitem">
———. 2023a. <span>“Generative <span>Causal Inference</span>,”</span> June. <a href="https://arxiv.org/abs/2306.16096">https://arxiv.org/abs/2306.16096</a>.
</div>
<div id="ref-nareklishvili2023feature" class="csl-entry" role="listitem">
———. 2023b. <span>“Feature <span>Selection</span> for <span>Personalized Policy Analysis</span>,”</span> July. <a href="https://arxiv.org/abs/2301.00251">https://arxiv.org/abs/2301.00251</a>.
</div>
<div id="ref-polson2023generative" class="csl-entry" role="listitem">
Polson, Nicholas G., and Vadim Sokolov. 2023. <span>“Generative <span>AI</span> for <span>Bayesian Computation</span>.”</span> <a href="https://arxiv.org/abs/2305.14972">https://arxiv.org/abs/2305.14972</a>.
</div>
<div id="ref-polson2017deep" class="csl-entry" role="listitem">
Polson, Nicholas G, Vadim Sokolov, et al. 2017. <span>“Deep <span>Learning</span>: <span>A Bayesian Perspective</span>.”</span> <em>Bayesian Analysis</em> 12 (4): 1275–1304.
</div>
<div id="ref-polson2020deep" class="csl-entry" role="listitem">
Polson, Nicholas, and Vadim Sokolov. 2020. <span>“Deep Learning: <span>Computational</span> Aspects.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 12 (5): e1500.
</div>
<div id="ref-polson2021deep" class="csl-entry" role="listitem">
Polson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. <span>“Deep <span>Learning Partial Least Squares</span>.”</span> <em>arXiv Preprint arXiv:2106.14085</em>. <a href="https://arxiv.org/abs/2106.14085">https://arxiv.org/abs/2106.14085</a>.
</div>
<div id="ref-sokolov2017discussion" class="csl-entry" role="listitem">
Sokolov, Vadim. 2017. <span>“Discussion of <span>‘<span>Deep</span> Learning for Finance: Deep Portfolios’</span>.”</span> <em>Applied Stochastic Models in Business and Industry</em> 33 (1): 16–18.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./18-theoryai.html" class="pagination-link" aria-label="Theory of AI: From MLE to Bayesian Regularization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./20-theorydl.html" class="pagination-link" aria-label="Theory of Deep Learning">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>