<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>19&nbsp; Neural Networks – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./20-theorydl.html" rel="next">
<link href="./18-theoryai.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="19&nbsp; Neural Networks – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="19-nn_files/figure-html/circle-data-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="19&nbsp; Neural Networks – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="19-nn_files/figure-html/circle-data-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-nn.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">19.1</span> Introduction</a></li>
  <li><a href="#mathematical-foundations" id="toc-mathematical-foundations" class="nav-link" data-scroll-target="#mathematical-foundations"><span class="header-section-number">19.2</span> Mathematical Foundations</a>
  <ul class="collapse">
  <li><a href="#relu-networks-as-max-sum-networks" id="toc-relu-networks-as-max-sum-networks" class="nav-link" data-scroll-target="#relu-networks-as-max-sum-networks">ReLU Networks as Max-Sum Networks</a></li>
  <li><a href="#auto-encoders" id="toc-auto-encoders" class="nav-link" data-scroll-target="#auto-encoders">Auto-Encoders</a></li>
  </ul></li>
  <li><a href="#classification-with-neural-networks" id="toc-classification-with-neural-networks" class="nav-link" data-scroll-target="#classification-with-neural-networks"><span class="header-section-number">19.3</span> Classification with Neural Networks</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">19.4</span> Activation Functions</a></li>
  <li><a href="#modern-deep-learning-architectures" id="toc-modern-deep-learning-architectures" class="nav-link" data-scroll-target="#modern-deep-learning-architectures"><span class="header-section-number">19.5</span> Modern Deep Learning Architectures</a>
  <ul class="collapse">
  <li><a href="#transfer-learning-and-pre-trained-blocks" id="toc-transfer-learning-and-pre-trained-blocks" class="nav-link" data-scroll-target="#transfer-learning-and-pre-trained-blocks">Transfer Learning and Pre-trained Blocks</a></li>
  <li><a href="#mixture-of-experts" id="toc-mixture-of-experts" class="nav-link" data-scroll-target="#mixture-of-experts">Mixture of Experts</a></li>
  <li><a href="#ensembles" id="toc-ensembles" class="nav-link" data-scroll-target="#ensembles">Ensembles</a></li>
  <li><a href="#architectural-innovations" id="toc-architectural-innovations" class="nav-link" data-scroll-target="#architectural-innovations">Architectural Innovations</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">19.6</span> Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./19-nn.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“We have a chance to build machines that will be smarter than us. We need to be thoughtful about what we are asking for.” — Geoffrey Hinton</p>
</blockquote>
<p>In 2012, a neural network called AlexNet stunned the computer vision community by winning the ImageNet competition with an error rate nearly half that of the previous year’s winner. This breakthrough marked the beginning of deep learning’s modern era—a period in which neural networks have transformed fields from medical diagnosis to language translation, from game playing to autonomous driving.</p>
<p>The goal of this chapter is to provide a comprehensive overview of deep learning (DL) methods. From a statistical perspective, deep learning extends the framework of generalized linear models by introducing multiple layers of nonlinear transformations. While traditional statistical models are well-studied and interpretable, they often lack the flexibility to capture complex input-output relationships. <em>Black-box</em> predictive methods, such as decision trees and neural networks, offer greater flexibility at the cost of interpretability. Deep learning excels precisely where high-dimensional problems make traditional model selection challenging.</p>
<p>Deep learning thus offers a powerful alternative in domains traditionally served by statistical methods, opening rich avenues for future research—including uncertainty quantification, principled architecture selection, and Bayesian deep learning. Although DL models have been most prominently applied to image analysis and natural language processing, they have also demonstrated superior performance in traditional engineering and scientific domains such as spatio-temporal modeling and financial forecasting <span class="citation" data-cites="polson2021deep polson2017deep dixon2019deep sokolov2017discussion bhadra2021merging behnia2021deep nareklishvili2022deep nareklishvili2023feature polson2023generative nareklishvili2023generative polson2020deep">(<a href="references.html#ref-polson2021deep" role="doc-biblioref">N. Polson, Sokolov, and Xu 2021</a>; <a href="references.html#ref-polson2017deep" role="doc-biblioref">Nicholas G. Polson, Sokolov, et al. 2017</a>; <a href="references.html#ref-dixon2019deep" role="doc-biblioref">Dixon, Polson, and Sokolov 2019</a>; <a href="references.html#ref-sokolov2017discussion" role="doc-biblioref">Sokolov 2017</a>; <a href="references.html#ref-bhadra2021merging" role="doc-biblioref">Bhadra et al. 2021</a>; <a href="references.html#ref-behnia2021deep" role="doc-biblioref">Behnia, Karbowski, and Sokolov 2021</a>; <a href="references.html#ref-nareklishvili2022deep" role="doc-biblioref">Nareklishvili, Polson, and Sokolov 2022</a>, <a href="references.html#ref-nareklishvili2023feature" role="doc-biblioref">2023b</a>, <a href="references.html#ref-nareklishvili2023generative" role="doc-biblioref">2023a</a>; <a href="references.html#ref-polson2023generative" role="doc-biblioref">Nicholas G. Polson and Sokolov 2023</a>; <a href="references.html#ref-polson2020deep" role="doc-biblioref">N. Polson and Sokolov 2020</a>)</span>.</p>
<p>In this chapter, we focus on feed-forward neural networks and their applications to regression and classification. We begin with the mathematical foundations, work through illustrative examples, and discuss practical implementation in <code>R</code> and <code>Python</code>.</p>
<section id="introduction" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">19.1</span> Introduction</h2>
<p>Machine learning and statistical analysis share common foundations, but they emphasize different priorities: ML focuses on scalable algorithms and predictive accuracy, while statistics prioritizes interpretability and inferential rigor. Deep learning sits at the intersection, providing powerful pattern-matching capabilities suitable for AI applications such as image recognition and text analysis.</p>
<p>From a computational perspective, images and text can be represented as high-dimensional matrices and vectors, respectively. The challenge of recognizing objects in images or understanding natural language requires learning complex decision boundaries in these high-dimensional input spaces. Deep learning addresses this challenge by using hierarchical layers of transformations that progressively extract meaningful features from raw data. This approach differs from traditional statistical models in that the features are <em>learned</em> from data rather than specified <em>a priori</em></p>
<p>There are several deep learning architectures - each has its own uses and purposes. Convolutional Neural Networks (CNN) deal with 2-dimensional input objects, i.e., images, and have been shown to outperform other techniques. Recurrent Neural Networks (RNN) have shown the best performance on speech and text analysis tasks.</p>
<p>In general, a neural network can be described as follows. Let <span class="math inline">\(f_1 , \ldots , f_L\)</span> be given univariate activation functions for each of the <span class="math inline">\(L\)</span> layers. Activation functions are nonlinear transformations of weighted data. A semi-affine activation rule is then defined by <span class="math display">\[
f_l^{W,b} = f_l \left ( \sum_{j=1}^{N_l} W_{lj} X_j + b_l \right ) = f_l ( W_l X_l + b_l )\,
\]</span> which implicitly needs the specification of the number of hidden units <span class="math inline">\(N_l\)</span>. Our deep predictor, given the number of layers <span class="math inline">\(L\)</span>, then becomes the composite map</p>
<p><span class="math display">\[
\hat{Y}(X) = F(X) = \left ( f_l^{W_1,b_1} \circ \ldots \circ f_L^{W_L,b_L} \right ) ( X)\,.
\]</span></p>
<p>Deep learning’s capacity to serve as a universal function approximator has deep mathematical roots—a principle traceable to the functional analysis of the nineteenth century. From a practical perspective, given a large enough dataset, we can empirically learn an optimal predictor. Similar to a classic basis decomposition, the deep approach uses univariate activation functions to decompose a high-dimensional input <span class="math inline">\(X\)</span>.</p>
<p>Let <span class="math inline">\(Z^{(l)}\)</span> denote the <span class="math inline">\(l\)</span>th layer, and so <span class="math inline">\(X = Z^{(0)}\)</span>. The final output <span class="math inline">\(Y\)</span> can be numeric or categorical. The explicit structure of a deep prediction rule is then <span class="math display">\[
\begin{aligned}
\hat{Y} (X) &amp; = W^{(L)} Z^{(L)} + b^{(L)} \\
Z^{(1)} &amp; = f^{(1)} \left ( W^{(0)} X + b^{(0)} \right ) \\
Z^{(2)} &amp; = f^{(2)} \left ( W^{(1)} Z^{(1)} + b^{(1)} \right ) \\
\ldots  &amp; \\
Z^{(L)} &amp; = f^{(L)} \left ( W^{(L-1)} Z^{(L-1)} + b^{(L-1)} \right )\,.
\end{aligned}
\]</span> Here <span class="math inline">\(W^{(l)}\)</span> is a weight matrix and <span class="math inline">\(b^{(l)}\)</span> are bias terms that shift the activation functions. Designing a good predictor depends crucially on the choice of univariate activation functions <span class="math inline">\(f^{(l)}\)</span>. The <span class="math inline">\(Z^{(l)}\)</span> are hidden features (or <em>latent representations</em>) that the algorithm extracts from the data.</p>
<p>Put differently, the deep approach employs hierarchical predictors comprising a series of <span class="math inline">\(L\)</span> nonlinear transformations applied to <span class="math inline">\(X\)</span>. Each of the <span class="math inline">\(L\)</span> transformations is referred to as a <em>layer</em>, where the original input is <span class="math inline">\(X\)</span>, the output of the first transformation is the first layer, and so on, with the output <span class="math inline">\(\hat Y\)</span> as the final layer. The layers <span class="math inline">\(1\)</span> to <span class="math inline">\(L\)</span> are called <em>hidden layers</em>. The number of layers <span class="math inline">\(L\)</span> represents the depth of our network.</p>
<p>The following diagram illustrates the structure of a simple feedforward neural network with two hidden layers:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph Input["Input Layer"]
        X1["$X_1$"]
        X2["$X_2$"]
        X3["$X_3$"]
    end
    
    subgraph H1["Hidden Layer 1"]
        Z11["$Z_1^{(1)}$"]
        Z12["$Z_2^{(1)}$"]
        Z13["$Z_3^{(1)}$"]
        Z14["$Z_4^{(1)}$"]
    end
    
    subgraph H2["Hidden Layer 2"]
        Z21["$Z_1^{(2)}$"]
        Z22["$Z_2^{(2)}$"]
    end
    
    subgraph Output["Output Layer"]
        Y["$\hat{Y}$"]
    end
    
    X1 --&gt; Z11 &amp; Z12 &amp; Z13 &amp; Z14
    X2 --&gt; Z11 &amp; Z12 &amp; Z13 &amp; Z14
    X3 --&gt; Z11 &amp; Z12 &amp; Z13 &amp; Z14
    
    Z11 --&gt; Z21 &amp; Z22
    Z12 --&gt; Z21 &amp; Z22
    Z13 --&gt; Z21 &amp; Z22
    Z14 --&gt; Z21 &amp; Z22
    
    Z21 --&gt; Y
    Z22 --&gt; Y
</pre>
</div>
<p></p><figcaption> Architecture of a feedforward neural network with two hidden layers.</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>Each arrow represents a weight connecting neurons in adjacent layers. The input is transformed through successive layers of weighted combinations and nonlinear activations until it reaches the output.</p>
</section>
<section id="mathematical-foundations" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="mathematical-foundations"><span class="header-section-number">19.2</span> Mathematical Foundations</h2>
<p>Before exploring practical examples, it is instructive to see how neural networks can represent common mathematical operations. Interaction terms such as <span class="math inline">\(x_1 x_2\)</span> and <span class="math inline">\((x_1 x_2)^2\)</span>, as well as max functions like <span class="math inline">\(\max(x_1, x_2)\)</span>, can be expressed as nonlinear functions of semi-affine combinations. Specifically:</p>
<p><span class="math display">\[
x_1x_2 = \frac{1}{4} ( x_1+x_2 )^2 - \frac{1}{4} (x_1-x_2)^2
\]</span></p>
<p><span class="math display">\[
\max(x_1,x_2) = \frac{1}{2} | x_1+x_2 | + \frac{1}{2} | x_1-x_2 |
\]</span></p>
<p><span class="math display">\[
(x_1x_2)^2 = \frac{1}{4} ( x_1+x_2 )^4 + \frac{7}{4 \cdot 3^3} (x_1-x_2)^4 - \frac{1}{2 \cdot 3^3} ( x_1+ 2 x_2)^4 - \frac{2^3}{3^3} ( x_1 + \frac{1}{2} x_2 )^4
\]</span></p>
<p>These identities reveal that products and maxima—operations central to modeling interactions and thresholds—can be constructed from combinations of univariate functions applied to linear projections of the input. This is precisely what neural networks do.</p>
<p><span class="citation" data-cites="diaconis1981generating">Diaconis and Shahshahani (<a href="references.html#ref-diaconis1981generating" role="doc-biblioref">1981</a>)</span> provides further discussion in the context of <em>Projection Pursuit Regression</em>, where the model uses a layered structure <span class="math inline">\(\sum_{i=1}^N f(w_i^\top X)\)</span>. Their work on composite iterated functions foreshadowed the modern use of multiple layers to model complex multivariate systems.</p>
<section id="relu-networks-as-max-sum-networks" class="level3">
<h3 class="anchored" data-anchor-id="relu-networks-as-max-sum-networks">ReLU Networks as Max-Sum Networks</h3>
<p>Deep ReLU architectures can be viewed as max-sum networks via a simple identity. Define <span class="math inline">\(x^+ = \max(x,0)\)</span>. Let <span class="math inline">\(f_x(b) = (x + b)^+\)</span> where <span class="math inline">\(b\)</span> is an offset. Then <span class="math inline">\((x + y^+)^+ = \max(0, x, x+y)\)</span>. This is generalized in <span class="citation" data-cites="feller1971introduction">Feller (<a href="references.html#ref-feller1971introduction" role="doc-biblioref">1971</a>)</span> (p.272), who shows by induction that</p>
<p><span class="math display">\[
( f_{x_1} \circ \ldots \circ f_{x_k} ) (0) = ( x_1 + ( x_2 + \ldots + ( x_{k-1} + x_k^+ )^+ )^+ = \max_{1 \leq j \leq k} ( x_1 + \ldots + x_j )^+
\]</span></p>
<p>A composition of max-layers thus reduces to a single-layer max-sum network. This mathematical insight helps explain why deep ReLU networks are so effective: they can efficiently represent a rich class of piecewise linear functions.</p>
</section>
<section id="auto-encoders" class="level3">
<h3 class="anchored" data-anchor-id="auto-encoders">Auto-Encoders</h3>
<p>Auto-encoding is an important dimensionality reduction technique. An auto-encoder is a neural network architecture designed to replicate its input <span class="math inline">\(X\)</span>, i.e., <span class="math inline">\(Y = X\)</span>, via a <em>bottleneck</em> structure. The model <span class="math inline">\(F^{W,b}(X)\)</span> learns to compress the information required to recreate <span class="math inline">\(X\)</span> into a lower-dimensional representation. <span class="citation" data-cites="heaton2016deep">Heaton, Polson, and Witte (<a href="references.html#ref-heaton2016deep" role="doc-biblioref">2016</a>)</span> provide an application to smart indexing in finance.</p>
<p>Suppose that we have <span class="math inline">\(N\)</span> input vectors <span class="math inline">\(X = \{ x_1 , \ldots , x_N \} \in \mathbb{R}^{M\times N}\)</span> and <span class="math inline">\(N\)</span> output (or target) vectors <span class="math inline">\(\{ x_1 , \ldots , x_N \} \in \mathbb{R}^{M\times N}\)</span>. Setting biases to zero, for the purpose of illustration, and using only one hidden layer (<span class="math inline">\(L=2\)</span>) with <span class="math inline">\(K &lt; N\)</span> factors, gives for <span class="math inline">\(j=1, \ldots, N\)</span>:</p>
<p><span class="math display">\[
Y_j(x) = F^m_{W} ( X )_j = \sum_{k=1}^K W^{jk}_2 f \left ( \sum_{i=1}^N W^{ki}_1 x_i \right ) =  \sum_{k=1}^K W^{jk}_2 Z_j \quad \text{for } Z_j =  f \left ( \sum_{i=1}^N W^{ki}_1 x_i \right )
\]</span></p>
<p>Since, in an auto-encoder, we fit the model <span class="math inline">\(X = F_{W}( X)\)</span>, and <em>train</em> the weights <span class="math inline">\(W = (W_1, W_2)\)</span> with regularization penalty in a</p>
<p><span class="math display">\[
\mathcal{L} ( W )  =  \operatorname{argmin}_W \Vert X - F_W (X) \Vert^2  + \lambda \phi(W)
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\phi(W) = \sum_{i,j,k} | W^{jk}_1 |^2 +  | W^{ki}_2 |^2.
\]</span></p>
<p>Writing the DL objective as an augmented Lagrangian (as in ADMM) with a hidden factor <span class="math inline">\(Z\)</span>, leads to a two step algorithm: an encoding step (a penalty for <span class="math inline">\(Z\)</span>), and a decoding step for reconstructing the output signal via</p>
<p><span class="math display">\[
\operatorname{argmin}_{W,Z} \Vert X - W_2 Z \Vert^2 + \lambda \phi(Z) + \Vert Z -  f( W_1, X ) \Vert^2,
\]</span></p>
<p>where the regularization on <span class="math inline">\(W_1\)</span> induces a penalty on <span class="math inline">\(Z\)</span>. The last term is the encoder, the first two the decoder.</p>
<p>If <span class="math inline">\(W_2\)</span> is estimated from the structure of the training data matrix, then we have a traditional factor model, and the <span class="math inline">\(W_1\)</span> matrix provides the factor loadings. PCA, PLS, SIR fall into this category (see Cook 2007 for further discussion). If <span class="math inline">\(W_2\)</span> is trained based on the pair <span class="math inline">\(\hat{X}=\{Y,X\}\)</span> then we have a sliced inverse regression model. If <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span> are simultaneously estimated based on the training data <span class="math inline">\(X\)</span>, then we have a two layer deep learning model.</p>
<p>Auto-encoding demonstrates that deep learning does not directly model variance-covariance matrix explicitly as the architecture is already in predictive form. Given a hierarchical non-linear combination of deep learners, an implicit variance-covariance matrix exists, but that is not the driver of the algorithm.</p>
<p>Another interesting area for future research are long short term memory models (LSTMs). For example, a dynamic one layer auto-encoder for a financial time series <span class="math inline">\((Y_t)\)</span> is a coupled system of the form</p>
<p><span class="math display">\[
Y_t = W_x X _t + W_y Y_{t-1} \quad \text{and} \quad \begin{pmatrix} X_t \\ Y_{t-1} \end{pmatrix} = W Y_t
\]</span></p>
<p>Here, the state equation encodes and the matrix <span class="math inline">\(W\)</span> decodes the <span class="math inline">\(Y_t\)</span> vector into its history <span class="math inline">\(Y_{t-1}\)</span> and the current state <span class="math inline">\(X_t\)</span>.</p>
</section>
</section>
<section id="classification-with-neural-networks" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="classification-with-neural-networks"><span class="header-section-number">19.3</span> Classification with Neural Networks</h2>
<p>To illustrate the concepts introduced above, we apply a feed-forward neural network with one hidden layer to a binary classification problem. Consider the simulated dataset shown below, where points are generated from two distributions: the inner cluster (green) represents one class, while the outer ring (red) represents the other. The goal is to learn a decision boundary that separates these two classes.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-data-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>As a baseline, we first attempt to classify the data using logistic regression:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glm</span>(label<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data=</span><span class="fu">as.data.frame</span>(d), <span class="at">family=</span><span class="fu">binomial</span>(<span class="at">link=</span><span class="st">'logit'</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training dataset</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(d[,<span class="dv">2</span>],d[,<span class="dv">3</span>], <span class="at">col=</span>d[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">xlab=</span><span class="st">"x1"</span>, <span class="at">ylab=</span><span class="st">"x2"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>th <span class="ot">=</span> fit<span class="sc">$</span>coefficients</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span>th[<span class="dv">1</span>]<span class="sc">/</span>th[<span class="dv">3</span>], <span class="sc">-</span>th[<span class="dv">2</span>]<span class="sc">/</span>th[<span class="dv">3</span>], <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-lr-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that a logistic regression could not do it. It uses a single line to separate observations of two classes. We can see that the data is not linearly separable. However, we can use multiple lines to separate the data.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1<span class="sc">~</span>x2, <span class="at">data=</span>d,<span class="at">col=</span>d[,<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot lines that separate once class (red) from another (green)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1, <span class="sc">-</span>x1 <span class="sc">-</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1, <span class="sc">-</span>x1 <span class="sc">+</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,  x1 <span class="sc">-</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="dv">4</span>,<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x1,  x1 <span class="sc">+</span> <span class="dv">6</span>); <span class="fu">text</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/sep-lines-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now, we do the same thing as in simple logistic regression and apply logistic function to each of those lines</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define sigmoid function</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sigmoid  <span class="ot">=</span> <span class="cf">function</span>(z) <span class="fu">exp</span>(z)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(z))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define hidden layer of our neural network</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>features <span class="ot">=</span> <span class="cf">function</span>(x1,x2) {</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  z1 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">+</span> x1 <span class="sc">+</span> x2; a1 <span class="ot">=</span> <span class="fu">sigmoid</span>(z1)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  z2 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">-</span> x1 <span class="sc">-</span> x2; a2 <span class="ot">=</span> <span class="fu">sigmoid</span>(z2)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  z3 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">-</span> x1 <span class="sc">+</span> x2; a3 <span class="ot">=</span> <span class="fu">sigmoid</span>(z3)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  z4 <span class="ot">=</span>  <span class="dv">6</span> <span class="sc">+</span> x1 <span class="sc">-</span> x2; a4 <span class="ot">=</span> <span class="fu">sigmoid</span>(z4)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(a1,a2,a3,a4))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Using the matrix notation, we have <span class="math display">\[
z = \sigma(Wx + b), ~ W = \begin{bmatrix} 1 &amp; 1 \\ -1 &amp; -1 \\ -1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}, ~ b = \begin{bmatrix} 6 \\ 6 \\ 6 \\ 6 \end{bmatrix}, ~ \sigma(z) = \frac{1}{1+e^{-z}}
\]</span></p>
<p>The model shown above is the first layer of our neural network. It takes a two-dimensional input <span class="math inline">\(x\)</span> and produces a four-dimensional output <span class="math inline">\(z\)</span> which is called a feature vector. The feature vector is then passed to the output layer, which applies simple logistic regression to the feature vector. <span class="math display">\[
\hat{y} = \sigma(w^Tz + b), ~ w = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, ~ b = -3.1, ~ \sigma(z) = \frac{1}{1+e^{-z}}
\]</span></p>
<p>The output of the output layer is the probability of the positive class.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction (classification) using our neural network</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>predict_prob <span class="ot">=</span> <span class="cf">function</span>(x){</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> x[<span class="dv">1</span>]; x2 <span class="ot">=</span> x[<span class="dv">2</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  z <span class="ot">=</span> <span class="fu">features</span>(x1,x2)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(z)</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">sum</span>(z) <span class="sc">-</span> <span class="fl">3.1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># print(mu)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sigmoid</span>(mu)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can use our model to do the predictions now</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict the probability of the positive class for a given point</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict_prob</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.71</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict_prob</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.26</code></pre>
</div>
</div>
<p>The model generates sensible predictions, let’s plot the decision boundary to see how well it separates the data.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">11</span>,<span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">11</span>,<span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>gr <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">expand.grid</span>(x1,x2));</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 10000     2</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">apply</span>(gr,<span class="dv">1</span>,predict_prob)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 10000</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1,x2,<span class="fu">matrix</span>(yhat,<span class="at">ncol =</span> <span class="dv">100</span>), <span class="at">col =</span> <span class="fu">heat.colors</span>(<span class="dv">20</span>,<span class="fl">0.7</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/circle-boundary-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>How about a regression model? We will use a one-layer neural network to fit a quadratic function. We simulate noisy data from the following model <span class="math display">\[
y = 0.5 + 0.3x^2 + \epsilon, ~ \epsilon \sim N(0,0.02^2)
\]</span> And use 3 hidden units in the first hidden layer and two units in the second hidden layer. The output layer is a single unit. We will use the hyperbolic tangent (<code>tanh</code>) activation function for all layers. The model is defined as follows</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>relu <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">max</span>(<span class="dv">0</span>,x)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">=</span> <span class="cf">function</span>(W,<span class="at">f=</span>relu) {</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">=</span> W[<span class="dv">1</span>]; w0<span class="ot">=</span>W[<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>];b1 <span class="ot">=</span> W[<span class="dv">5</span>]; w1 <span class="ot">=</span> W[<span class="dv">6</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    z0 <span class="ot">=</span> <span class="fu">apply</span>(b0 <span class="sc">+</span> <span class="fu">outer</span>(x,w0,<span class="st">'*'</span>),<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,f)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">=</span> b1 <span class="sc">+</span> z0 <span class="sc">%*%</span> w1</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">yhat =</span> yhat[,<span class="dv">1</span>],<span class="at">z0=</span>z0))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The hidden layer has three outputs (neurons) and uses the ReLU activation function. The output linear layer has a single output. Thus, the prediction <code>yhat</code> is generated as a linear model of the feature vector <code>z0</code>. The model has 8 parameters. Let’s generate training data and fit the model. We will use the BFGS optimization algorithm to minimize the loss function (negative log-likelihood) of the model.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">99</span>) <span class="co">#gretzky</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>nl  <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.02</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x),<span class="dv">0</span>,<span class="fl">0.02</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">sum</span>((<span class="fu">nn</span>(W)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(params, loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] -0.24  1.39 -0.84  0.46  0.50  0.18  0.45  0.37</code></pre>
</div>
</div>
<p><a href="#fig-relu" class="quarto-xref">Figure&nbsp;<span>19.1</span></a> shows the quadratic function and the neural network model. The solid black line is the neural network model, and the dashed lines are the basis functions. The model fits the data well.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y); <span class="fu">lines</span>(x,o<span class="sc">$</span>yhat, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">1</span>],<span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">3</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">+</span>o<span class="sc">$</span>z0[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">4</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div id="fig-relu" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-relu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="19-nn_files/figure-html/fig-relu-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-relu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19.1: Noisy quadratic function approximated by a neural network with ReLU activation function.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s try the <span class="math inline">\(\tanh\)</span> function</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>) <span class="co">#gretzky</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">3</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">mean</span>((<span class="fu">nn</span>(W,<span class="at">f=</span>tanh)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(params, loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] -0.98 -0.23  0.83 -1.14  0.84 -0.65  0.59  0.53</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par, <span class="at">f=</span>tanh)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y, <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.4</span>,<span class="fl">0.95</span>)); <span class="fu">lines</span>(x,o<span class="sc">$</span>yhat, <span class="at">lwd=</span><span class="dv">2</span>);</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">1</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">2</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>); <span class="fu">lines</span>(x,<span class="fl">0.5</span><span class="sc">*</span>o<span class="sc">$</span>z0[,<span class="dv">3</span>]<span class="sc">+</span><span class="fl">0.9</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Noisy quadratic function approximated by a neural network with tanh activation function.</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that we did not need to explicitly specify a quadratic term; the neural network discovered this structure from the data. The model automatically learned an appropriate basis function representation.</p>
<p>The same approach extends naturally to interaction terms. Consider the following data-generating process: <span class="math display">\[
y = 0.5 + 0.1x_1 + 0.2x_2  + 0.5x_1x_2+ \epsilon, ~ \epsilon \sim N(0,0.02^2)
\]</span> We can use the same model as above, but with two input variables. The model will learn the interaction term from the data.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">99</span>) <span class="co">#ovi</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="fl">0.01</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> x1</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">+</span> <span class="fl">0.1</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span>x2 <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x1<span class="sc">*</span>x2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x1),<span class="dv">0</span>,<span class="fl">0.02</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"scatterplot3d"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>s3d <span class="ot">=</span> <span class="fu">scatterplot3d</span>(x1,x2,y, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">cbind</span>(x1,x2)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>nn <span class="ot">=</span> <span class="cf">function</span>(W,<span class="at">f=</span>relu) {</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    b0 <span class="ot">=</span> W[<span class="dv">1</span>]; w0 <span class="ot">=</span> W[<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>]; b1 <span class="ot">=</span> W[<span class="dv">6</span>]; w1 <span class="ot">=</span> W[<span class="dv">7</span><span class="sc">:</span><span class="dv">8</span>]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    w0 <span class="ot">=</span> <span class="fu">matrix</span>(w0,<span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    z0 <span class="ot">=</span> <span class="fu">apply</span>(b0 <span class="sc">+</span> x<span class="sc">%*%</span>w0,<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,f)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    yhat <span class="ot">=</span> b1 <span class="sc">+</span> z0 <span class="sc">%*%</span> w1</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">yhat =</span> yhat[,<span class="dv">1</span>],<span class="at">z0=</span>z0))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">4</span>),<span class="dv">0</span>,<span class="fu">rnorm</span>(<span class="dv">2</span>))</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>loss <span class="ot">=</span> <span class="cf">function</span>(W) <span class="fu">sum</span>((<span class="fu">nn</span>(W, <span class="at">f=</span>tanh)<span class="sc">$</span>yhat <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">optim</span>(W, <span class="at">fn=</span>loss, <span class="at">method=</span><span class="st">'BFGS'</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>res<span class="sc">$</span>par</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1]  0.78  0.50 -1.39  0.63 -0.94 -2.08 -2.90  6.85</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>o <span class="ot">=</span> <span class="fu">nn</span>(res<span class="sc">$</span>par, <span class="at">f=</span>tanh)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>yhat, <span class="at">col=</span><span class="dv">2</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>z0[,<span class="dv">1</span>], <span class="at">col=</span><span class="dv">3</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>s3d<span class="sc">$</span><span class="fu">points3d</span>(x1,x2,o<span class="sc">$</span>z0[,<span class="dv">2</span>], <span class="at">col=</span><span class="dv">4</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Effectively, neural networks serve as flexible function approximators, analogous to nonparametric regression methods but with the crucial advantage of learning the basis functions directly from data.</p>
</section>
<section id="activation-functions" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">19.4</span> Activation Functions</h2>
<p>The last output layer of a neural network has sigmoid activation function for binary output variable (classification) and no activation function for continuous output variable regression. The hidden layers can have different activation functions. The most common activation functions are the hyperbolic tangent function and the rectified linear unit (ReLU) function.</p>
<p>A typical approach is to use the same activation function for all hidden layers. The hyperbolic tangent function is defined as <span class="math display">\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]</span> Notice that the hyperbolic tangent function is a scaled version of the sigmoid function, with <span class="math inline">\(\tanh(0) = 0\)</span>. It is a smooth function which is differentiable everywhere.</p>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>tanh</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Hard tanh</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-3.png" class="img-fluid figure-img" width="576"></p>
<figcaption>softplus</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-4.png" class="img-fluid figure-img" width="576"></p>
<figcaption>ReLU</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-5.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Leaky ReLU</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="19-nn_files/figure-html/unnamed-chunk-5-6.png" class="img-fluid figure-img" width="576"></p>
<figcaption>sigmoid</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Typically <span class="math inline">\(\tanh\)</span> is preferred to the sigmoid function because it is zero-centered. The major drawback of sigmoid and <span class="math inline">\(\tanh\)</span> functions is that they saturate when the input is very large or very small. When we try to learn the weights of the network, the optimization algorithms make small steps in the space of the parameters and when the weights are large the small changes won’t affect the values of the layers’ outputs and optimization will “stagnate.”</p>
<p>This means that the gradient of the function is very small, which makes learning slow. The ReLU function is defined as <span class="math display">\[
\text{ReLU}(z) = \max(0,z)
\]</span> The ReLU function is a piecewise linear function which is computationally efficient and easy to optimize. The ReLU function is the most commonly used activation function in deep learning. The ReLU function is not differentiable at <span class="math inline">\(z=0\)</span>, but it is differentiable everywhere else. The derivative of the ReLU function is <span class="math display">\[
\text{ReLU}'(z) = \begin{cases} 0 &amp; \text{if } z &lt; 0 \\ 1 &amp; \text{if } z &gt; 0 \end{cases}
\]</span></p>
</section>
<section id="modern-deep-learning-architectures" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="modern-deep-learning-architectures"><span class="header-section-number">19.5</span> Modern Deep Learning Architectures</h2>
<p>The evolution of deep learning has led to increasingly sophisticated architectural patterns that go beyond simple stacked layers. Modern systems often combine multiple neural networks, reuse pre-trained components, and employ ensemble methods to achieve state-of-the-art performance. In this section, we explore three key architectural paradigms that have become central to contemporary deep learning practice: transfer learning with pre-trained blocks, mixture of experts models, and ensemble methods.</p>
<section id="transfer-learning-and-pre-trained-blocks" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning-and-pre-trained-blocks">Transfer Learning and Pre-trained Blocks</h3>
<p>One of the most significant developments in deep learning is the recognition that neural networks learn hierarchical representations, where early layers capture general features and deeper layers encode task-specific patterns. This observation has given rise to <em>transfer learning</em>, where models trained on large-scale datasets can be adapted to new tasks with limited data.</p>
<p>Consider the analogy of learning to drive different vehicles. Once you learn to drive a car, adapting to a truck requires learning the differences rather than starting from scratch. Similarly, a neural network trained to recognize objects in millions of images has learned fundamental visual features—edges, textures, shapes—that are useful for many computer vision tasks.</p>
<p>The typical transfer learning workflow involves taking a pre-trained model, removing its final layers, and replacing them with new layers appropriate for the target task. For example, a convolutional neural network (CNN) trained on ImageNet [we discuss CNNs in detail in Chapter 23] can be adapted for medical image classification by freezing the early convolutional layers and retraining only the final classification layers. The mathematical formulation is straightforward. Let <span class="math inline">\(F_{\text{pre}}(X; \theta_{\text{frozen}})\)</span> represent the pre-trained feature extractor with frozen parameters, and <span class="math inline">\(G(Z; \theta_{\text{new}})\)</span> the new task-specific layers. The complete model becomes:</p>
<p><span class="math display">\[
\hat{Y} = G(F_{\text{pre}}(X; \theta_{\text{frozen}}); \theta_{\text{new}})
\]</span></p>
<p>During training, we only optimize <span class="math inline">\(\theta_{\text{new}}\)</span> while keeping <span class="math inline">\(\theta_{\text{frozen}}\)</span> fixed. This approach has several advantages. First, it dramatically reduces the amount of labeled data required for the new task. Second, it decreases training time since we only update a subset of parameters. Third, it often leads to better generalization, especially when target task data is limited.</p>
<p>Modern frameworks make transfer learning remarkably accessible. In natural language processing, transformer-based models like BERT <span class="citation" data-cites="devlin2018bert">(<a href="references.html#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2019</a>)</span> and GPT <span class="citation" data-cites="radford2018improving brown2020language">(<a href="references.html#ref-radford2018improving" role="doc-biblioref">Radford et al. 2018</a>; <a href="references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span> are routinely fine-tuned for downstream tasks such as sentiment analysis, named entity recognition, and question answering. These models, pre-trained on billions of tokens, capture rich linguistic representations that transfer across diverse language tasks [we explore transformers and language models in Chapters 24 and 25].</p>
<p>The practice of using pre-trained blocks extends beyond simple transfer learning. Modern architectures often incorporate multiple pre-trained components as building blocks. For instance, a system for visual question answering might combine a pre-trained CNN for image encoding with a pre-trained transformer for language understanding, with only a small fusion network trained from scratch.</p>
</section>
<section id="mixture-of-experts" class="level3">
<h3 class="anchored" data-anchor-id="mixture-of-experts">Mixture of Experts</h3>
<p>As models grow in scale and complexity, a natural question arises: must every parameter be active for every input? The <em>mixture of experts</em> (MoE) architecture provides an elegant answer by using different sub-networks (experts) for different inputs, with a gating mechanism that decides which experts to activate.</p>
<p>The core idea traces back to work by <span class="citation" data-cites="jacobs1991adaptive">Jacobs et al. (<a href="references.html#ref-jacobs1991adaptive" role="doc-biblioref">1991</a>)</span>, but has seen renewed interest in modern large-scale systems. Think of a medical diagnosis system where different specialists (cardiologist, neurologist, oncologist) examine patients based on their symptoms. A triage system routes each patient to the appropriate specialist. Similarly, in an MoE model, a gating network routes inputs to specialized expert networks.</p>
<p>Formally, let <span class="math inline">\(f_1, \ldots, f_K\)</span> denote <span class="math inline">\(K\)</span> expert models (networks), each implementing a function <span class="math inline">\(f_k(X; \theta_k)\)</span>. A gating network <span class="math inline">\(g(X; \phi)\)</span> produces a probability distribution over experts, for example: <span class="math display">\[
g(X; \phi) = \text{softmax}(W_g X + b_g) \in \mathbb{R}^K
\]</span></p>
<p>The final output is a weighted combination of expert predictions:</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{k=1}^K g_k(X; \phi) \cdot f_k(X; \theta_k).
\]</span></p>
<p>In practice, to improve computational efficiency, only the top-<span class="math inline">\(N\)</span> experts with highest gating scores are activated for each input, setting others to zero. This sparse activation pattern allows MoE models to have enormous capacity while maintaining reasonable computational costs.</p>
<p>From a statistical perspective, <span class="citation" data-cites="jiang1999hierarchical">Jiang and Tanner (<a href="references.html#ref-jiang1999hierarchical" role="doc-biblioref">1999a</a>)</span> provided important theoretical foundations for mixture of experts models. They demonstrated that hierarchical mixtures of experts (HME) can approximate arbitrary mean functions at a rate of <span class="math inline">\(O(m^{-2/s})\)</span> in <span class="math inline">\(L_p\)</span> norm, where <span class="math inline">\(m\)</span> is the number of experts and <span class="math inline">\(s\)</span> is the input dimensionality. This result establishes that MoE models are universal approximators with quantifiable convergence rates, connecting them to the broader statistical literature on nonparametric regression.</p>
<p>The work by <span class="citation" data-cites="stroud2003nonlinear">Stroud, Müller, and Polson (<a href="references.html#ref-stroud2003nonlinear" role="doc-biblioref">2003</a>)</span> demonstrates an elegant application of mixture-of-experts principles within Bayesian inference for complex time series models. Their approach addresses the fundamental challenge of estimating hidden states in nonlinear state-space models where exact inference is mathematically intractable. The core innovation lies in using an auxiliary mixture model to approximate the intractable nonlinear components. This auxiliary model functions as a mixture of experts, where each expert is a simple linear regression model of the form <span class="math inline">\(p^{a}(y_{t}|x_{t},z_{t}=k) = \mathcal{N}(\alpha_{k}+\beta_{k}x_{t},\tau_{k}^{2})\)</span>. These linear experts provide local approximations of the true nonlinear function around specific “knots” in the state space. The gating mechanism employs state-dependent weights <span class="math inline">\(\pi_{k}(x_{t})\)</span> determined by Gaussian kernels centered at different locations. The current hidden state value <span class="math inline">\(x_t\)</span> serves as input to this gating network, which determines the relevance of each linear expert for that particular region of the state space.</p>
<p>A critical theoretical challenge with MoE models is <em>identifiability</em>: can the model parameters be uniquely determined from the data? <span class="citation" data-cites="jiang1999identifiability">Jiang and Tanner (<a href="references.html#ref-jiang1999identifiability" role="doc-biblioref">1999b</a>)</span> showed that without constraints, MoE models suffer from inherent non-identifiability due to invariant transformations. For example, permuting expert labels or translating gating parameters can yield identical predictions. They established that by imposing order restrictions on expert parameters (such as requiring experts to be ordered by their intercepts) and proper initialization of gating parameters, MoE systems become identifiable. This work provides the statistical rigor necessary for reliable inference and interpretation of mixture of experts models, complementing the computational advances in modern implementations.</p>
<p>Recent work has demonstrated the power of this approach at scale. <span class="citation" data-cites="fedus2022switch">Fedus, Zoph, and Shazeer (<a href="references.html#ref-fedus2022switch" role="doc-biblioref">2022</a>)</span> trained a mixture of experts language model with 1.6 trillion parameters, where only a small fraction are active for any given input. The model achieves strong performance while requiring less computation per token than dense models of comparable quality. Similarly, <span class="citation" data-cites="riquelme2021scaling">Riquelme et al. (<a href="references.html#ref-riquelme2021scaling" role="doc-biblioref">2021</a>)</span> showed that vision transformers with mixture of experts layers can achieve better accuracy-computation trade-offs than their dense counterparts.</p>
<p>The gating mechanism introduces an interesting learning problem: the network must simultaneously learn to specialize experts and to route inputs appropriately. Various training strategies have been proposed, including auxiliary losses that encourage load balancing across experts and techniques to promote expert specialization <span class="citation" data-cites="shazeer2017outrageously">(<a href="references.html#ref-shazeer2017outrageously" role="doc-biblioref">Shazeer et al. 2017</a>)</span>.</p>
</section>
<section id="ensembles" class="level3">
<h3 class="anchored" data-anchor-id="ensembles">Ensembles</h3>
<p>While mixture of experts learns a single integrated model with internal routing, <em>ensemble methods</em> combine predictions from multiple independently trained models. The rationale is straightforward: different models trained on the same data often make different errors, and averaging their predictions can reduce variance and improve overall performance.</p>
<p>Ensemble methods have a long history in statistics and machine learning. The classical result from <span class="citation" data-cites="dietterich2000ensemble">Dietterich (<a href="references.html#ref-dietterich2000ensemble" role="doc-biblioref">2000</a>)</span> shows that an ensemble is effective when individual models are accurate and diverse. Consider <span class="math inline">\(M\)</span> models <span class="math inline">\(f_1, \ldots, f_M\)</span>. For regression, a simple ensemble averages predictions:</p>
<p><span class="math display">\[
\hat{Y}_{\text{ensemble}}(X) = \frac{1}{M} \sum_{m=1}^M f_m(X)
\]</span></p>
<p>For classification, we can use majority voting or average the predicted probabilities. The bias-variance decomposition provides insight into why ensembles work. The expected squared error of a single model can be written as:</p>
<p><span class="math display">\[
\E{(Y - f(X))^2} = \text{Bias}^2 + \text{Variance} + \text{Noise}
\]</span></p>
<p>If we average <span class="math inline">\(M\)</span> models with the same bias but independent errors, the variance term reduces by a factor of <span class="math inline">\(M\)</span>, assuming the models are uncorrelated. In practice, models are not perfectly independent, but ensembles still typically achieve substantial variance reduction.</p>
<p>In deep learning, several ensemble strategies are commonly employed:</p>
<ol type="1">
<li><p><em>Model averaging</em>: Train multiple neural networks with different random initializations or architectures, then average their predictions at test time.</p></li>
<li><p><em>Snapshot ensembles</em> <span class="citation" data-cites="huang2017snapshot">(<a href="references.html#ref-huang2017snapshot" role="doc-biblioref">Huang et al. 2017</a>)</span>: Save model checkpoints at various points during training (particularly at local minima) and ensemble them.</p></li>
<li><p><em>Multi-architecture ensembles</em>: Combine models with different architectures (e.g., CNNs, transformers, and recurrent networks) to capture complementary patterns.</p></li>
</ol>
<p>A particularly effective technique is <em>dropout as ensemble</em> <span class="citation" data-cites="srivastava2014dropout">(<a href="references.html#ref-srivastava2014dropout" role="doc-biblioref">Srivastava et al. 2014</a>)</span>. During training, dropout randomly deactivates neurons with probability <span class="math inline">\(p\)</span>. At test time, all neurons are active but their outputs are scaled by <span class="math inline">\((1-p)\)</span>. <span class="citation" data-cites="gal2016dropout">Gal and Ghahramani (<a href="references.html#ref-gal2016dropout" role="doc-biblioref">2016</a>)</span> showed that this can be interpreted as approximate Bayesian inference, where dropout at test time (called Monte Carlo dropout) produces an ensemble of sub-networks, providing both predictions and uncertainty estimates.</p>
<p>The computational cost of ensembles is their main drawback—inference requires running multiple models. However, the reliability gains are often worth the expense, particularly in high-stakes applications like medical diagnosis, autonomous driving, and financial forecasting. Modern competitions and benchmarks frequently use ensembles: winning solutions in Kaggle competitions typically combine dozens of models <span class="citation" data-cites="chen2015higgs">(<a href="references.html#ref-chen2015higgs" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>.</p>
</section>
<section id="architectural-innovations" class="level3">
<h3 class="anchored" data-anchor-id="architectural-innovations">Architectural Innovations</h3>
<p>These three paradigms—pre-trained blocks, mixture of experts, and ensembles—represent different philosophies for building capable systems. Transfer learning emphasizes knowledge reuse, recognizing that representations learned on large datasets are broadly useful. Mixture of experts emphasizes conditional computation, activating only relevant capacity for each input. Ensembles emphasize diversity and robustness, combining multiple perspectives to improve reliability.</p>
<p>Modern systems often combine these approaches. For instance, a production system might ensemble multiple models, each of which uses pre-trained components and employs mixture of experts layers for efficiency. The field continues to develop new architectural patterns, but these fundamental ideas—reusing learned knowledge, routing computation dynamically, and aggregating diverse predictions—are likely to remain central to deep learning practice.</p>
</section>
</section>
<section id="summary" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">19.6</span> Summary</h2>
<p>In this chapter, we introduced the mathematical foundations of neural networks and deep learning. We began by establishing that neural networks are hierarchical compositions of nonlinear transformations—a framework that extends generalized linear models while providing the flexibility to learn complex patterns from data. Key concepts covered include:</p>
<ul>
<li><p><em>Feedforward architecture</em>: A neural network applies successive layers of weighted linear combinations followed by nonlinear activation functions, extracting increasingly abstract features at each layer.</p></li>
<li><p><em>Activation functions</em>: Functions like ReLU, <span class="math inline">\(\tanh\)</span>, and sigmoid introduce nonlinearity, enabling networks to approximate arbitrary functions. ReLU has become the default choice for hidden layers due to its computational efficiency and favorable gradient properties.</p></li>
<li><p><em>Auto-encoders</em>: These networks learn compressed representations by training to reconstruct their inputs through a bottleneck layer, providing a powerful approach to unsupervised dimensionality reduction.</p></li>
<li><p><em>Transfer learning and pre-trained blocks</em>: Knowledge learned on large datasets can be transferred to new tasks, dramatically reducing data and computation requirements.</p></li>
<li><p><em>Mixture of experts</em>: Conditional computation allows models to activate only relevant sub-networks for each input, enabling massive scale with manageable computational costs.</p></li>
<li><p><em>Ensemble methods</em>: Combining diverse models improves robustness and reduces variance, with techniques ranging from simple averaging to dropout-based approximations.</p></li>
</ul>
<p>The implementations presented in this chapter—from simple classification problems to regression with learned basis functions—demonstrate that neural networks can be understood through the lens of traditional statistical methods while offering capabilities that extend far beyond them.</p>
<p>Looking ahead, Chapter 23 covers <em>convolutional neural networks</em> (CNNs), which exploit spatial structure in images through weight sharing and local connectivity. Chapters 24 and 25 introduce <em>transformers</em> and <em>large language models</em>, architectures that have revolutionized natural language processing through attention mechanisms and massive scale. These architectures build directly on the principles established in this chapter: layered representations, learned features, and the power of deep hierarchical computation.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-behnia2021deep" class="csl-entry" role="listitem">
Behnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2021. <span>“Deep Generative Models for Vehicle Speed Trajectories.”</span> <em>arXiv Preprint arXiv:2112.08361</em>. <a href="https://arxiv.org/abs/2112.08361">https://arxiv.org/abs/2112.08361</a>.
</div>
<div id="ref-bhadra2021merging" class="csl-entry" role="listitem">
Bhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and Jianeng Xu. 2021. <span>“Merging Two Cultures: Deep and Statistical Learning.”</span> <em>arXiv Preprint arXiv:2110.11561</em>. <a href="https://arxiv.org/abs/2110.11561">https://arxiv.org/abs/2110.11561</a>.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language <span>Models</span> Are <span>Few-Shot Learners</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-chen2015higgs" class="csl-entry" role="listitem">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“<span>XGBoost</span>: <span>A Scalable Tree Boosting System</span>.”</span> In <em>Proceedings of the 22nd <span>ACM SIGKDD International Conference</span> on <span>Knowledge Discovery</span> and <span>Data Mining</span></em>, 785–94. New York, NY, USA: ACM.
</div>
<div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span class="nocase">Pre-training</span> of <span>Deep Bidirectional Transformers</span> for <span>Language Understanding</span>.”</span> In <em>Proceedings of the 2019 <span>Conference</span> of the <span>North American Chapter</span> of the <span>Association</span> for <span>Computational Linguistics</span>: <span>Human Language Technologies</span>, <span>Volume</span> 1 (<span>Long</span> and <span>Short Papers</span>)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics.
</div>
<div id="ref-diaconis1981generating" class="csl-entry" role="listitem">
Diaconis, Persi, and Mehrdad Shahshahani. 1981. <span>“Generating a Random Permutation with Random Transpositions.”</span> <em>Probability Theory and Related Fields</em> 57 (2): 159–79.
</div>
<div id="ref-dietterich2000ensemble" class="csl-entry" role="listitem">
Dietterich, Thomas G. 2000. <span>“Ensemble <span>Methods</span> in <span>Machine Learning</span>.”</span> In <em>Multiple <span>Classifier Systems</span></em>, 1–15. Berlin, Heidelberg: Springer.
</div>
<div id="ref-dixon2019deep" class="csl-entry" role="listitem">
Dixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. <span>“Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.”</span> <em>Applied Stochastic Models in Business and Industry</em> 35 (3): 788–807.
</div>
<div id="ref-fedus2022switch" class="csl-entry" role="listitem">
Fedus, William, Barret Zoph, and Noam Shazeer. 2022. <span>“Switch <span>Transformers</span>: <span>Scaling</span> to <span>Trillion Parameter Models</span> with <span>Simple</span> and <span>Efficient Sparsity</span>.”</span> <em>Journal of Machine Learning Research</em> 23 (120): 1–39.
</div>
<div id="ref-feller1971introduction" class="csl-entry" role="listitem">
Feller, William. 1971. <em>An Introduction to Probability Theory and Its Applications</em>. Wiley.
</div>
<div id="ref-gal2016dropout" class="csl-entry" role="listitem">
Gal, Yarin, and Zoubin Ghahramani. 2016. <span>“Dropout as a <span>Bayesian</span> Approximation: <span>Representing</span> Model Uncertainty in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 1050–59.
</div>
<div id="ref-heaton2016deep" class="csl-entry" role="listitem">
Heaton, J. B., N. G. Polson, and Jan Hendrik Witte. 2016. <span>“Deep Learning for Finance: Deep Portfolios.”</span> <em>Applied Stochastic Models in Business and Industry</em>.
</div>
<div id="ref-huang2017snapshot" class="csl-entry" role="listitem">
Huang, Gao, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. 2017. <span>“Snapshot <span>Ensembles</span>: <span>Train</span> 1, Get <span>M</span> for <span>Free</span>.”</span> In <em>International <span>Conference</span> on <span>Learning Representations</span></em>.
</div>
<div id="ref-jacobs1991adaptive" class="csl-entry" role="listitem">
Jacobs, Robert A., Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. <span>“Adaptive <span>Mixtures</span> of <span>Local Experts</span>.”</span> <em>Neural Computation</em> 3 (1): 79–87.
</div>
<div id="ref-jiang1999hierarchical" class="csl-entry" role="listitem">
Jiang, Wenxin, and Martin A. Tanner. 1999a. <span>“Hierarchical <span class="nocase">Mixtures-of-Experts</span> for <span>Generalized Linear Models</span>: <span>Some Results</span> on <span>Denseness</span> and <span>Consistency</span>.”</span> In <em>Proceedings of the <span>Sixteenth International Conference</span> on <span>Machine Learning</span></em>, 214–22. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-jiang1999identifiability" class="csl-entry" role="listitem">
———. 1999b. <span>“On the <span>Identifiability</span> of <span class="nocase">Mixtures-of-Experts</span>.”</span> <em>Neural Networks</em> 12 (9): 1253–58.
</div>
<div id="ref-nareklishvili2022deep" class="csl-entry" role="listitem">
Nareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022. <span>“Deep Partial Least Squares for Iv Regression.”</span> <em>arXiv Preprint arXiv:2207.02612</em>. <a href="https://arxiv.org/abs/2207.02612">https://arxiv.org/abs/2207.02612</a>.
</div>
<div id="ref-nareklishvili2023generative" class="csl-entry" role="listitem">
———. 2023a. <span>“Generative <span>Causal Inference</span>,”</span> June. <a href="https://arxiv.org/abs/2306.16096">https://arxiv.org/abs/2306.16096</a>.
</div>
<div id="ref-nareklishvili2023feature" class="csl-entry" role="listitem">
———. 2023b. <span>“Feature <span>Selection</span> for <span>Personalized Policy Analysis</span>,”</span> July. <a href="https://arxiv.org/abs/2301.00251">https://arxiv.org/abs/2301.00251</a>.
</div>
<div id="ref-polson2023generative" class="csl-entry" role="listitem">
Polson, Nicholas G., and Vadim Sokolov. 2023. <span>“Generative <span>AI</span> for <span>Bayesian Computation</span>.”</span> <a href="https://arxiv.org/abs/2305.14972">https://arxiv.org/abs/2305.14972</a>.
</div>
<div id="ref-polson2017deep" class="csl-entry" role="listitem">
Polson, Nicholas G, Vadim Sokolov, et al. 2017. <span>“Deep <span>Learning</span>: <span>A Bayesian Perspective</span>.”</span> <em>Bayesian Analysis</em> 12 (4): 1275–1304.
</div>
<div id="ref-polson2020deep" class="csl-entry" role="listitem">
Polson, Nicholas, and Vadim Sokolov. 2020. <span>“Deep Learning: <span>Computational</span> Aspects.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 12 (5): e1500.
</div>
<div id="ref-polson2021deep" class="csl-entry" role="listitem">
Polson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. <span>“Deep <span>Learning Partial Least Squares</span>.”</span> <em>arXiv Preprint arXiv:2106.14085</em>. <a href="https://arxiv.org/abs/2106.14085">https://arxiv.org/abs/2106.14085</a>.
</div>
<div id="ref-radford2018improving" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. <span>“Improving <span>Language Understanding</span> by <span>Generative Pre-Training</span>.”</span> OpenAI.
</div>
<div id="ref-riquelme2021scaling" class="csl-entry" role="listitem">
Riquelme, Carlos, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. <span>“Scaling <span>Vision</span> with <span>Sparse Mixture</span> of <span>Experts</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>, 34:8583–95.
</div>
<div id="ref-shazeer2017outrageously" class="csl-entry" role="listitem">
Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. <span>“Outrageously <span>Large Neural Networks</span>: <span class="nocase">The Sparsely-Gated Mixture-of-Experts Layer</span>.”</span> In <em>International <span>Conference</span> on <span>Learning Representations</span></em>.
</div>
<div id="ref-sokolov2017discussion" class="csl-entry" role="listitem">
Sokolov, Vadim. 2017. <span>“Discussion of <span>‘<span>Deep</span> Learning for Finance: <span>Deep</span> Portfolios’</span>.”</span> <em>Applied Stochastic Models in Business and Industry</em> 33 (1): 16–18.
</div>
<div id="ref-srivastava2014dropout" class="csl-entry" role="listitem">
Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>Journal of Machine Learning Research</em> 15 (1): 1929–58.
</div>
<div id="ref-stroud2003nonlinear" class="csl-entry" role="listitem">
Stroud, Jonathan R., Peter Müller, and Nicholas G. Polson. 2003. <span>“Nonlinear <span>State-Space Models</span> with <span>State-Dependent Variances</span>.”</span> <em>Journal of the American Statistical Association</em> 98 (462): 377–86. <a href="https://www.jstor.org/stable/30045247">https://www.jstor.org/stable/30045247</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./18-theoryai.html" class="pagination-link" aria-label="Theory of AI: From MLE to Bayesian Regularization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./20-theorydl.html" class="pagination-link" aria-label="Theory of Deep Learning">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>