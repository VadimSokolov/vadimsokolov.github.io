<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayes, AI and Deep Learning - 5&nbsp; Bayesian Parameter Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../qmd/ab.html" rel="next">
<link href="../qmd/dec.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="../qmd/bl.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Principles of Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bl.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Field vs Observational</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="../qmd/bl.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Statistics makes use of parametric families of distributions and make assumption that observed samples <span class="math inline">\(x_1,\ldots,x_n\)</span> are independent and identically distributed observations from a distribution with density function parametrized by <span class="math inline">\(\theta\)</span>, the notation is <span class="math inline">\(x_i \sim p(x \mid \theta)\)</span>. The functional form of <span class="math inline">\(p(x \mid \theta)\)</span> is assumed to be known, but the value of <span class="math inline">\(\theta\)</span> is unknown. The goal of statistical inference is to estimate <span class="math inline">\(\theta\)</span> from the observed data <span class="math inline">\(x_1,\ldots,x_n\)</span>. There are several tasks in statistical inference, including</p>
<ul>
<li>Estimation: use sample to estimate <span class="math inline">\(\theta\)</span> by a single value <span class="math inline">\(\hat{\theta}\)</span> or by an interval <span class="math inline">\([a,b]\)</span> that contains the true value of <span class="math inline">\(\theta\)</span> with a certain probability.</li>
<li>Hypothesis testing: test a hypothesis about the value of <span class="math inline">\(\theta\)</span>. For example, we may want to test whether <span class="math inline">\(\theta\)</span> is equal to a certain value <span class="math inline">\(\theta_0\)</span>.</li>
<li>Prediction: predict the value of a new observation <span class="math inline">\(x_{n+1}\)</span> given the observed data <span class="math inline">\(x_1,\ldots,x_n\)</span>.</li>
</ul>
<p>Bayesian parameter learning is a statistical approach used to update our beliefs about the parameters of a model based on new evidence or data. It is rooted in Bayesian statistics, which is a framework for reasoning about uncertainty and making probabilistic inferences.</p>
<p>In the context of artificial intelligence and statistical modeling, Bayesian parameter learning is particularly relevant when dealing with models that have uncertain or unknown parameters. The goal is to update the probability distribution over the parameters of the model as new data becomes available. The basic steps involved in Bayesian parameter learning include:</p>
<ol type="1">
<li><p><strong>Prior Distribution (Prior):</strong> Start with a prior distribution that represents your beliefs or knowledge about the parameters before observing any data. This distribution encapsulates the uncertainty about the parameters.</p></li>
<li><p><strong>Likelihood Function (Data Likelihood):</strong> Specify a likelihood function that describes the probability of observing the given data given the current values of the parameters. This function represents the likelihood of the observed data under different parameter values.</p></li>
<li><p><strong>Posterior Distribution (Posterior):</strong> Combine the prior distribution and the likelihood function using Bayes’ theorem to obtain the posterior distribution over the parameters. The posterior distribution represents the updated beliefs about the parameters after incorporating the observed data. <span class="math display">\[
p( \theta | y )  = \frac{p(y \mid  \theta)p( \theta)}{p(y)}
\]</span> Here <span class="math inline">\(\theta\)</span> is the set of model parameters, <span class="math inline">\(y\)</span> is the observed data. The right hand side <span class="math inline">\(p( \theta | y )\)</span> is the posterior distribution, <span class="math inline">\(p(y \mid \theta)\)</span> is the likelihood, <span class="math inline">\(p(\theta)\)</span> is the prior distribution, and <span class="math inline">\(p(y)\)</span> is the probability of the observed data (also known as the total probability) given by <span class="math display">\[
p(y)  = \int p(y \mid  \theta)p( \theta)d \theta
\]</span></p></li>
<li><p><strong>Posterior as the New Prior (Iterative Process):</strong> Use the posterior distribution obtained from one round of observation as the prior distribution for the next round when more data becomes available. This process can be iteratively repeated as new evidence is acquired.</p></li>
<li><p><strong>Bayesian Inference:</strong> Make predictions or draw inferences by summarizing the information in the posterior distribution. This may involve computing point estimates (e.g., mean, median) or credible intervals that capture a certain percentage of the parameter values.</p></li>
</ol>
<p>The key advantage of Bayesian parameter learning is its ability to incorporate prior knowledge and update beliefs based on observed data in a principled manner. It provides a framework for handling uncertainty and expressing the confidence or ambiguity associated with parameter estimates. However, it often requires computational methods, such as Markov Chain Monte Carlo (MCMC) or variational inference, to approximate or sample from the complex posterior distributions.</p>
<p>The Bayes rule allows us to combine the prior distribution and the likelihood function, sometimes we omit the total probability in the denominator on the right hand side and write Bayes rule as <span class="math display">\[
\text{Posterior} \propto \text{Likelihood} \times \text{Prior}
\]</span></p>
<p>The choice of prior distribution can significantly impact the ease of computation and the interpretation of the posterior distribution. Conjugate priors are a special type of prior distribution that, when combined with a specific likelihood function, result in a posterior distribution that belongs to the same family as the prior. This property simplifies the computation of the posterior distribution, and allows for analytical solution.</p>
<p>Common examples of conjugate priors include:</p>
<ul>
<li><p><strong>Normal distribution with known variance:</strong> If the likelihood is a normal distribution with known variance, then a normal distribution is a conjugate prior for the mean.</p></li>
<li><p><strong>Binomial distribution:</strong> If the likelihood is a binomial distribution, then a beta distribution is a conjugate prior for the probability of success.</p></li>
<li><p><strong>Poisson distribution:</strong> If the likelihood is a Poisson distribution, then a gamma distribution is a conjugate prior for the rate parameter.</p></li>
</ul>
<p>Using conjugate priors simplifies the Bayesian analysis, especially in cases where analytical solutions are desirable. However, the choice of a conjugate prior is often a modeling assumption, and in some cases, non-conjugate priors may be more appropriate for capturing the true underlying uncertainty in the problem.</p>
<section id="exchangeability-and-the-bayesian-view-of-probability-models" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="exchangeability-and-the-bayesian-view-of-probability-models"><span class="header-section-number">5.1</span> Exchangeability and the Bayesian view of probability models</h2>
<p>At the basis of all statistical problems is a potential sample of data, <span class="math inline">\(y=\left( y_{1},\ldots,y_{T}\right)\)</span>, and assumptions over the data generating process such as independence, a model or models, and parameters. How should one view the relationship between models, parameters, and samples of data? How should one define a model and parameters?&nbsp;These questions have fundamental implications for statistical inference and can be answered from different perspectives. This section discusses de Finetti’s representation theorem which provides a formal connection between data, models, and parameters.</p>
<p>To understand the issues, consider the simple example of an experiment consisting of tosses of a simple thumb tack in ideal `laboratory’ conditions. The outcome of the experiment can be defined as a random variable <span class="math inline">\(y_{i},\)</span> where <span class="math inline">\(y_{i}=1\)</span> if the <span class="math inline">\(i^{th}\)</span> toss was a heads (the tack lands on the spike portion) and <span class="math inline">\(y_{i}=0\)</span> if the tack land tails (on its flat portion). How do we model these random variables?&nbsp;The frequentist or objective approach assumes tosses are independent and identically distributed. In this setting, independence implies that% <span class="math display">\[
P\left(  y_{2}=1,y_{1}=1\right)  =P\left(  y_{2}=1\right)
P\left(  y_{1}=0\right)  \text{.}%
\]</span></p>
<p>Given this, are thumbtack tosses independent?&nbsp;Surprisingly, the answer is no. Or at least absolutely not under the current assumptions. Independence implies that <span class="math display">\[
P\left(  y_{2}=1 \mid y_{1}=1\right)  =P\left(  y_{2}=1\right)
\text{,}%
\]</span> which means that observing <span class="math inline">\(y_{1}=1\)</span> does not effect the probability that <span class="math inline">\(y_{2}=1\)</span>. To see the implications of this simple fact, suppose that the results of 500 tosses were available. If the tosses were independent, then <span class="math display">\[
P\left(  y_{501}=1\right)  =P\left(  y_{501}=1\mid {\textstyle\sum\nolimits_{t=1}^{500}}y_{t}=1\right)  =P\left(  y_{501}=1\mid {\textstyle\sum\nolimits_{t=1}^{500}}y_{t}=499\right)  \text{.}
\]</span> It is hard to imagine that anyone would believe this–nearly every observer would state that the second probability is near zero and the third probably is near 1 as the first 500 tosses contain a lot of information. Thus, the tosses are not independent.</p>
<p>To see the resolution of this apparent paradox, introduce a parameter, <span class="math inline">\(\theta\)</span>, which is the probability that a thumb tack toss is heads. If <span class="math inline">\(\theta\)</span> were known, then it is true that, conditional on the value of this parameter, the tosses are independent and <span class="math display">\[
P\left(  y_{2}=1\mid y_{1}=1,\theta\right)  =P\left(y_{2}=1\mid \theta\right)  =\theta\text{.}
\]</span> Thus, the traditional usage of independence, and independent sampling, requires that `true’ parameter values are known. With unknown probabilities, statements about future tosses are heavily influenced by previous observations, clearly violating the independence assumption. Ironically, if the data was really independent, we would not need samples in the first place to estimate parameters because the probabilities would already be known! Given this, if you were now presented with a thumb tack from a box that was to be repeatedly tossed, do you think that the tosses are independent?&nbsp;</p>
<p>This example highlights the tenuous foundations, an odd circularity, and the internal inconsistency of the frequentist approach that proceeds under the assumption of a fixed `true’ parameter. All frequentist procedures are founded on the assumption of known parameter values:&nbsp;sampling distributions of estimators are computed conditional on <span class="math inline">\(\theta\)</span>; confidence intervals consist of calculations of the form:&nbsp;<span class="math inline">\(P\left( f\left( y_{1}, \ldots ,y_{T}\right) \in\left( a,b\right) |\theta\right)\)</span>; and asymptotics also all rely on the assumption of known parameter values. None of these calculations are possible without assuming the known parameters.</p>
<p>In the frequentist approach, even though the parameter is completely unknown to the researcher, <span class="math inline">\(\theta\)</span> is not a random variable, does not have a distribution, and therefore inference is not governed by the rules of probability. Given this “fixed, but unknown” definition, it is impossible to discuss concepts like “parameter uncertainty.” This strongly violates our intuition, since things that are not known are typically thought of as random.</p>
<p>The Bayesian approach avoids this internal inconsistency by shedding the strong assumption of independence and replacing it with a weaker and more natural assumption called exchangeability:&nbsp;collection of random variables, <span class="math inline">\(y_{1}, \ldots ,y_{T}\)</span>, is exchangeable if the distribution of <span class="math inline">\(y_{1}, \ldots ,y_{T}\)</span> is the same as the distribution of any permutation <span class="math inline">\(y_{\pi_{1}}, \ldots ,y_{\pi_{T}}\)</span>, where <span class="math inline">\(\pi=\left( \pi_{1}, \ldots ,\pi_{T}\right)\)</span> is a permutation of the integers <span class="math inline">\(1\)</span> to <span class="math inline">\(T\)</span>. In particular, this implies that the order in which the data may arrive does not matter, as their joint density is the same for all potential orderings, and that each observation has the same marginal distribution, <span class="math inline">\(P\left( y_{i}=1\right) =P\left(y_{j}=1\right)\)</span> for any <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. Exchangeability is a weak assumption about symmetry and is weaker than independence, as independent events are always exchangeable, but the converse is not true. Notice the differences between the assumptions in the Bayesian and frequentist approach:&nbsp;the Bayesian makes assumptions over potentially realized data, and there is no need to invent the construct of a fixed but unknown parameter, since exchangeability makes no reference to parameters.</p>
<p>In the case of the tack throwing experiment, exchangeability states that the ordering of heads and tails does not matter. Thus, if the experiment of 8 tosses generated 4 heads, it does not matter if the ordering was <span class="math inline">\(\left(1,0,1,0,1,0,1,0\right)\)</span> or <span class="math inline">\(\left( 0,1,1,0,1,0,0,1\right)\)</span>. This is a natural assumption about the symmetry of the tack tosses, capturing the idea that the information in any toss or sequence of tosses is the same as any other–the idea of a truly random sample. It is important to note that exchangeability is property that applies prior to viewing the data. After observation, data is no longer a random variable, but a realization of a random variable.</p>
<p>Bruno de Finetti introduced the notion of exchangeability, and then asked a simple question: “What do exchangeable sequences of random variables look like?” The answer to this question is given in the famous de Finetti’s theorem, which also <em>defines</em> models, parameters, and provides important linkages between frequentist and classical statistics.</p>
<section id="de-finettis-representation-theorem" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="de-finettis-representation-theorem"><span class="header-section-number">5.1.1</span> de Finetti’s Representation theorem</h3>
<p>de Finetti’s representation theorem provides the theoretical connection between data, models, and parameters. It is stated first in the simplest setting, where the observed data takes two values, either zero or one, and then extended below.</p>
<div id="thm-Representation" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 (de Finetti’s Representation theorem)</strong></span> Let <span class="math inline">\(\left( y_{1},y_{2},\ldots\right)\)</span> be an infinite sequence of <span class="math inline">\(0-1\)</span> exchangeable random variables with joint density <span class="math inline">\(p\left(y_{1}, \ldots ,y_{T}\right)\)</span>. Then there exists a distribution function <span class="math inline">\(P\)</span> such that<br>
<span id="eq-deFinetti"><span class="math display">\[
p(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}\theta^{y_{t}}(1-\theta)^{1-y_{t}%
}dP(\theta)=\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta\right)  dP(\theta)
\tag{5.1}\]</span></span> where <span class="math display">\[
P(\theta)=\underset{T\rightarrow\infty}{\lim}\text{Prob}\left[  \frac{1}%
{T}\sum_{t=1}^{T}y_{t}\leq\theta\right]  \text{ and }\theta=\underset {T\rightarrow\infty}{\lim}\frac{1}{T}\sum_{t=1}^{T}y_{t}\text{.}%
\]</span> If the distribution function or measure admits a density with respect to Lebesgue measure, then <span class="math inline">\(dP(\theta)=p\left( \theta\right) d\theta\)</span>.</p>
</div>
<p>de Finetti’s representation theorem has profound implications for understanding models from a subjectivist perspective and in relating subjectivist to frequentist theories of inference. The theorem is interpreted as follows:</p>
<ul>
<li><p>Under exchangeability, parameters exist, and one can <em>act as if</em> the <span class="math inline">\(y_{t}\)</span>’<span class="math inline">\(s\)</span> are drawn independently from a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>. That is, they are draws from the model <span class="math inline">\(p\left(y_{t} \mid \theta\right) =\theta^{y_{t}}(1-\theta)^{1-y_{t}},\)</span> generating a likelihood function <span class="math inline">\(p\left( y \mid \theta\right) =\prod_{t=1}^{T}p\left(y_{t} \mid \theta\right)\)</span>. Formally, the likelihood function is defined via the density <span class="math inline">\(p\left( y \mid \theta\right)\)</span>, viewed as a function of <span class="math inline">\(\theta\)</span> for a fixed sample <span class="math inline">\(y=\left( y_{1}, \ldots ,y_{T}\right)\)</span>. More “likely” parameter values generate higher likelihood values, thus the name. The maximum likelihood estimate or MLE is <span class="math display">\[
\widehat{\theta}=\arg\underset{\theta\in\Theta}{\max}\text{ }p\left(y \mid \theta\right)  =\arg\underset{\theta\in\Theta}{\max}\ln p\left(y \mid \theta\right)  \text{,}%   
\]</span> where <span class="math inline">\(\Theta\)</span> is the parameter space.</p></li>
<li><p>Parameters are random variables. The limit <span class="math inline">\(\theta=\underset {T\rightarrow\infty}{\lim}T^{-1}\sum_{t=1}^{T}y_{t}\)</span> exists but is a random variable. This can be contrasted with the strong law of large numbers that requires independence and implies that <span class="math inline">\(T^{-1}\sum_{t=1}^{T}y_{t}\)</span> converges almost surely to a fixed value, <span class="math inline">\(\theta_{0}\)</span>. From this, one can interpret a parameter as a limit of observables and justifies the frequentist interpretation of <span class="math inline">\(\theta\)</span> as a limiting frequency of 1’s.</p></li>
<li><p>The distribution <span class="math inline">\(P\left( \theta\right)\)</span> or density <span class="math inline">\(p\left(\theta\right)\)</span> can be interpreted as beliefs about the limiting frequency <span class="math inline">\(\theta\)</span> prior to viewing the data. After viewing the data, beliefs are updated via Bayes rule resulting in the posterior distribution, <span class="math display">\[
p\left(  \theta \mid y\right)  \propto p\left(  y \mid \theta\right)  p(\theta).
\]</span> Since the likelihood function is fixed in this case, any distribution of observed data can be generated by varying the prior distribution.</p></li>
</ul>
<p>The main implication of de Finetti’s theorem is a complete justification for Bayesian practice of treating the parameters as random variables and specifying a likelihood and parameter distribution. Stated differently, a `model’ consists of both a likelihood and a prior distribution over the parameters. Thus, parameters as random variables and priors are a necessity for statistical inference, and not some extraneous component motivated by philosophical concerns.</p>
<p>More general versions of de Finetti’s theorem are available. A general version is as follows. If <span class="math inline">\(\left\{ y_{t}\right\} _{t\geq1}\)</span>, <span class="math inline">\(y_{t}\in\mathbb{R}\)</span>, is a sequence of infinitely exchangeable random variables, then there exists a probability measure <span class="math inline">\(P\)</span> on the space of all distribution functions, such that <span class="math display">\[
P(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}F\left(  y_{t}\right)
P(dF)
\]</span> with mixing measure <span class="math display">\[
P\left(  F\right)  =\underset{T\rightarrow\infty}{\lim}P(F_{T}),
\]</span> where <span class="math inline">\(F_{T}\)</span> is the empirical distribution of the data. At this level of generality, the distribution function is infinite-dimensional. In practice, additional subjective assumptions are needed that usually restrict the distribution function to finite dimensional spaces, which implies that distribution function is indexed by a parameter vector <span class="math inline">\(\theta\)</span>:% <span class="math display">\[
p(y_{1},\ldots,y_{T})=\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta\right)
dP\left(  \theta\right)  \text{.}%
\]</span> To operationalize this result, the researcher needs to choose the likelihood function and the prior distribution of the parameters.</p>
<p>At first glance, de Finetti’s theorem may seem to suggest that there is a single model or likelihood function. This is not the case however, as models can be viewed in the same manner as parameters. Denoting a model specification by <span class="math inline">\(\mathcal{M}\)</span>, then de Finetti’s theorem would imply that <span class="math display">\[\begin{align*}
p(y_{1},\ldots,y_{T})  &amp;  =\int\prod_{t=1}^{T}p\left(  y_{t} \mid \theta ,\mathcal{M}\right)  p\left(  \theta \mid \mathcal{M}\right)  p\left(\mathcal{M}\right)  d\theta d\mathcal{M}\\
&amp;  =\int p(y_{1},\ldots,y_{T} \mid \mathcal{M})p\left(  \mathcal{M}\right)
d\mathcal{M}\text{,}%
\end{align*}\]</span> in the case of a continuum of models. Thus, under the mild assumption of exchangeability, it is <em>as if</em> the <span class="math inline">\(y_{t}\)</span>’<span class="math inline">\(s\)</span> are generated from <span class="math inline">\(p\left( y_{t} \mid \theta,\mathcal{M}\right)\)</span>, conditional on the random variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\mathcal{M}\)</span>, where <span class="math inline">\(p\left( \theta \mid \mathcal{M}\right)\)</span> are the beliefs over <span class="math inline">\(\theta\)</span> in model <span class="math inline">\(\mathcal{M}\)</span>, and <span class="math inline">\(p\left(\mathcal{M}_{j}\right)\)</span> are the beliefs over model specifications.</p>
<p>The objective approach has been a prevailing one in scientific applications. However, it only applies to events that can be repeated under the same conditions a very large number of times. This is rarely the case in many important applied problems. For example, it is hard to repeat an economic event, such as a Federal Reserve meeting or the economic conditions in 2008 infinitely often. This implies that at best, the frequentist approach is limited to laboratory situations. Even in scientific applications, when we attemt to repeat an experiment multiple times, an objective approach is not guaranteed to work. For example, the failure rate of phase 3 clinical trials in oncology is 60% (<span class="citation" data-cites="shen2021a">Shen et al. (<a href="references.html#ref-shen2021a" role="doc-biblioref">2021</a>)</span>,<span class="citation" data-cites="sun2022">Sun et al. (<a href="references.html#ref-sun2022" role="doc-biblioref">2022</a>)</span>). Prior to phase 3, the drug is usually tested on several hundred patients.</p>
<p>Subjective probability is a more general definition of probability than the frequentist definition, as it can be used for all types of events, both repeatable and unrepeatable events. A subjectivist has no problem discussing the probability a republican president will be re-elected in 2024, even though that event has never occurred before and cannot occur again. The main difficulty in operationalizing subjective probability is the process of actually quantifying subjective beliefs into numeric probabilities. This will be discussed below.</p>
<p>Instead of using repetitive experiments, subjective probabilities can be measured using betting odds, which have been used for centuries to gauge the uncertainty over an event. The probability attributed to winning a coin toss is revealed by the type of ofdds one would accept to bet. Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.</p>
</section>
</section>
<section id="sufficient-statistic-summary-statistic" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sufficient-statistic-summary-statistic"><span class="header-section-number">5.2</span> Sufficient Statistic (Summary Statistic)</h2>
<p>In Bayesian inference, we need to compute the posterior over unknown model parameters <span class="math inline">\(\theta\)</span>, given data <span class="math inline">\(y\)</span>. The posterior density is denoted by <span class="math inline">\(p(\theta \mid y)\)</span>. Here <span class="math inline">\(y = ( y_1 , \ldots , y_n )\)</span> is high dimensional. Moreover, we need the set of posterior probabilities <span class="math inline">\(f_B (y) := \pi_{\theta \mid y}(\theta \in B\mid y)\)</span> for all Borel sets <span class="math inline">\(B\)</span>. This problem is computationally hard in high dimensions. Hence, we need dimension reduction for <span class="math inline">\(y\)</span>. The whole idea is to estimate “maps: (a.k.a. transformations/feature extraction) of the output data <span class="math inline">\(y\)</span> so it is reduced to uniformity.</p>
<p>The dimension reduction problem is to find a low-dimensional representation <span class="math inline">\(S\)</span> of the data <span class="math inline">\(y\)</span> that preserves the information in the data.Learning <span class="math inline">\(S\)</span> can be achieved in a number of ways. Typical architectures include auto-encoders and traditional dimension reduction methods. <span class="citation" data-cites="polson2021">Polson, Sokolov, and Xu (<a href="references.html#ref-polson2021" role="doc-biblioref">2021</a>)</span> propose to use a theoretical result of Brillinger methods to perform a linear mapping <span class="math inline">\(S(y) = W y\)</span> and learn <span class="math inline">\(W\)</span> using partial least squares (PLS). <span class="citation" data-cites="nareklishvili2022">Nareklishvili, Polson, and Sokolov (<a href="references.html#ref-nareklishvili2022" role="doc-biblioref">2022</a>)</span> extend this to instrumental variables (IV) regression and casual inference problem.</p>
<p>There is a nice connection between the posterior mean and the sufficient statistics, especially minimal sufficient statistics in the exponential family. If there exists a sufficient statistic <span class="math inline">\(S^*\)</span> for <span class="math inline">\(\theta\)</span>, then Kolmogorov (1942) shows that for almost every <span class="math inline">\(y\)</span>, <span class="math inline">\(p(\theta\mid y) = p(\theta\mid S^*(y))\)</span> , and further <span class="math inline">\(S(y) = E_{\pi}(\theta \mid y) = E_{\pi}(\theta \mid S^*(y))\)</span> is a function of <span class="math inline">\(S^*(y)\)</span>. In the special case of an exponential family with minimal sufficient statistic <span class="math inline">\(S^*\)</span> and parameter <span class="math inline">\(\theta\)</span>, the posterior mean <span class="math inline">\(S(y) = E_{\pi}(\theta \mid y)\)</span> is a one-to-one function of <span class="math inline">\(S^*(y)\)</span>, and thus is a minimal sufficient statistic.</p>
<p>Deep learners are good interpolators (one of the folklore theorems of machine learning). Hence the set of posteriors <span class="math inline">\(p(\theta \mid y )\)</span> is characterized by the distributional identity <span class="math display">\[
\theta  \stackrel{D}{=} H ( S(y_N) , \tau_K ),  \; \; \mathrm{where}  \; y_N = ( y_1 , \ldots , y_N )  \; \; \tau \sim U(0,1 ) .
\]</span></p>
<p><strong>Summary Statistic</strong>: Let <span class="math inline">\(S(y)\)</span> is sufficient summary statistic in the Bayes sense <span class="citation" data-cites="kolmogorov1942">Kolmogorov (<a href="references.html#ref-kolmogorov1942" role="doc-biblioref">1942</a>)</span>, if for every prior <span class="math inline">\(\pi\)</span> <span class="math display">\[
f_B (y) :=   \pi_{\theta \mid y}(\theta \in B\mid y) = \pi_{\theta \mid s(y)}(\theta \in B\mid s(y)).
\]</span> Then we need to use our pattern matching dataset <span class="math inline">\((y^{(i)} , \theta^{(i)})\)</span> which is simulated from the prior and forward model to “train” the set of functions <span class="math inline">\(f_B (y)\)</span>, where we pick the sets <span class="math inline">\(B = ( - \infty , q ]\)</span> for a quantile <span class="math inline">\(q\)</span>. Hence, we can then interpolate inbetween.</p>
<p>Estimating the full sequence of functions is then done by interpolating for all Borel sets <span class="math inline">\(B\)</span> and all new data points <span class="math inline">\(y\)</span> using a NN architecture and conditional density NN estimation.</p>
<p>The notion of a summary statistic is prevalent in the ABC literature and is tightly related to the notion of a Bayesian sufficient statistic <span class="math inline">\(S^*\)</span> for <span class="math inline">\(\theta\)</span>, then (Kolmogorov 1942), for almost every <span class="math inline">\(y\)</span>, <span class="math display">\[
p(\theta \mid  Y=y) = p(\theta \mid S^*(Y) = S^*(y)
\]</span> Furthermore, <span class="math inline">\(S(y) = \mathrm{E}\left(\theta \mid Y = y\right) = \mathrm{E}_{p}\left(\theta \mid S^*(Y) = S^*(y)\right)\)</span> is a function of <span class="math inline">\(S^*(y)\)</span>. In the case of exponential family, we have <span class="math inline">\(S(Y) = \mathrm{E}_{p}\left(\theta | Y \right)\)</span> is a one-to-one function of <span class="math inline">\(S^*(Y)\)</span>, and thus is a minimal sufficient statistic.</p>
<p>Sufficient statistics are generally kept for parametric exponential families, where <span class="math inline">\(S(\cdot)\)</span> is given by the specification of the probabilistic model. However, many forward models have an implicit likelihood and no such structures. The generalization of sufficiency is a summary statistics (a.k.a. feature extraction/selection in a neural network). Hence, we make the assumption that there exists a set of features such that the dimensionality of the problem is reduced.</p>
</section>
<section id="bernoulli-distribution" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">5.3</span> Bernoulli Distribution</h2>
<p>The formal model of a coin toss was described by Bernoulli. He modelled the notion of <em>probability</em> for a coin toss, now known as the Bernoulli distribution, there <span class="math inline">\(X \in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p, P(X=0) = 1-p\)</span>. Laplace gave us the <em>principle of insufficient reason</em>: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete distribution on the set of possible outcomes.</p>
<p>A Bernoulli trial relates to an experiment with the following conditions</p>
<ol type="1">
<li>The result of each trial is either a success or failure.</li>
<li>The probability <span class="math inline">\(p\)</span> of a success is the same for all trials.</li>
<li>The trials are assumed to be <em>independent</em>.</li>
</ol>
<p>The Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by <span class="math inline">\(\text{Bernoulli}(p)\)</span>, where <span class="math inline">\(p\)</span> is the probability of success.</p>
<p>The probability mass function (PMF) of a Bernoulli distribution is defined as follows: <span class="math display">\[
P(X = x) = \begin{cases}
p &amp; \text{if } x = 1 \\
1 - p &amp; \text{if } x = 0
\end{cases}
\]</span> The expectation (mean) of a Bernoulli distributed random variable <span class="math inline">\(X\)</span> is given by: <span class="math display">\[\E{X} = p
\]</span> Simply speaking, if you are to toss a coin many times, you expect <span class="math inline">\(p\)</span> heads.</p>
<p>The variance of <span class="math inline">\(X\)</span> is given by: <span class="math display">\[
\Var{X} = p(1-p)
\]</span></p>
<div id="exm-Coin" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 (Coin Toss)</strong></span> The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is <span class="math inline">\(S = \{H,T\}\)</span>, and <span class="math inline">\(p(X = H) = p(X = T) = 1/2\)</span>. On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads <span class="math inline">\(X\)</span> out of two coin tosses is</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Given the probability mass function we can, for example, calculate the probability of at least one head as <span class="math inline">\(\prob{X \geq 1} = \prob{X =0} + \prob{X =1} = p(0)+p(1) = 3/4\)</span>.</p>
</div>
<p>The Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to <span class="math inline">\(X\)</span>, which is the number of successes. It’s probability distribution is calculated via: <span class="math display">\[
\prob{X=x} = {n \choose x} p^x(1-p)^{n-x}.
\]</span> Here <span class="math inline">\({n \choose x}\)</span> is the combinatorial function, <span class="math display">\[
{n \choose x} = \frac{n!}{x!(n-x)!},
\]</span> where <span class="math inline">\(n!=n(n-1)(n-2)\ldots 2 \cdot 1\)</span> counts the number of ways of getting <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials.</p>
<p>Table below shows the expected value and variance of Binomial random variable.</p>
<table class="table">
<caption>Mean and Variance of Binomial</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Binomial Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = n p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = n p ( 1 - p )\)</span></td>
</tr>
</tbody>
</table>
<p>For large sample sizes <span class="math inline">\(n\)</span>, this distribution is approximately normal with mean <span class="math inline">\(np\)</span> and variance of <span class="math inline">\(np(1-p)\)</span>.</p>
<p>Suppose we are about to toss two coins. Let <span class="math inline">\(X\)</span> denote the number of heads. Then the following table specifies the probability distribution <span class="math inline">\(p(x)\)</span> for all possible values <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>. This leads to the following table</p>
<table class="table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Thus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely Binomial distribution.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of heads in three flips. Each possible outcome (“realization”) of <span class="math inline">\(X\)</span> is an <em>event</em>. Now consider the event of getting only two heads <span class="math display">\[
\{ X= 2\} = \{ HHT, HTH, THH \} ,
\]</span> The probability distribution of <span class="math inline">\(X\)</span> is Binomial with parameters <span class="math inline">\(n = 3, p= 1/2\)</span>, where <span class="math inline">\(n\)</span> denotes the sample size (a.k.a. number of trials) and <span class="math inline">\(p\)</span> is the probability of heads, we have a fair coin. The notation is <span class="math inline">\(X \sim \mathrm{Bin} \left ( n = 3 , p = \frac{1}{2} \right )\)</span> where the sign <span class="math inline">\(\sim\)</span> is read as <em>distributed as</em>.</p>
<table class="table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">HHH</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;"><span class="math inline">\(p^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">HHT</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1- p)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1 - p)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\((1-p)p^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p( 1-p)^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p ( 1-p)^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">TTH</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^2 p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">TTT</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^3\)</span></td>
</tr>
</tbody>
</table>
<div id="exm-coinposteror" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 (Posterior Distribution)</strong></span> What if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants? In this case, we need to estimate the probability of heads <span class="math inline">\(\theta\)</span> from the data. Suppose we have observed 10 flips <span class="math display">\[
y = \{H, T, H, H, H, T, H, T, H, H\},
\]</span> and only three of them were tails. What is the probability that the next flip will be tail? The frequency-based answer would be <span class="math inline">\(3/10 = 0.3\)</span>. However, Bayes approach gives us more flexibility. Suppose we have a prior belief that the coin is fair, but we are not sure. We can model this belief by a prior distribution. Let discretize the variable <span class="math inline">\(\theta\)</span> and assign prior probabilities to each value of <span class="math inline">\(\theta\)</span> as follows</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.024</span>, <span class="fl">0.077</span>, <span class="fl">0.132</span>, <span class="fl">0.173</span>, <span class="fl">0.188</span>, <span class="fl">0.173</span>, <span class="fl">0.132</span>, <span class="fl">0.077</span>, <span class="fl">0.024</span>, <span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(prior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"prior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Prior distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>We put most of the mass to the fair assumption (<span class="math inline">\(\theta = 0.5\)</span>) and zero mass to the extreme values <span class="math inline">\(\theta = 0\)</span> and <span class="math inline">\(\theta = 1\)</span>. Our mass is exponentially decaying as we move away from 0.5. This is a reasonable assumption, since we are not sure about the fairness of the coin. Now, we can use Bayes rule to update our prior belief. The posterior distribution is given by <span class="math display">\[
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{p(y)}.
\]</span> The denominator is the marginal likelihood, which is given by <span class="math display">\[
p(y) = \sum_{\theta} p(y \mid \theta) p(\theta).
\]</span> The likelihood is given by the Binomial distribution <span class="math display">\[
p(y \mid \theta) \propto \theta^3 (1 - \theta)^7.
\]</span> Notice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are <strong>sufficient</strong> for the inference about <span class="math inline">\(\theta\)</span>. The posterior distribution is given by</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, n, Y) {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  theta<span class="sc">^</span>Y <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>(n <span class="sc">-</span> Y)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(theta, <span class="dv">10</span>,<span class="dv">3</span>) <span class="sc">*</span> prior</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> posterior <span class="sc">/</span> <span class="fu">sum</span>(posterior) <span class="co"># normalize</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(posterior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"posterior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Posterior distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>If you are to keep collecting more observations and say observe a sequence of 100 flips, then the posterior distribution will be more concentrated around the value of <span class="math inline">\(\theta = 0.3\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(theta, <span class="dv">100</span>,<span class="dv">30</span>) <span class="sc">*</span> prior</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> posterior <span class="sc">/</span> <span class="fu">sum</span>(posterior) <span class="co"># normalize</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(posterior, <span class="at">names.arg =</span> theta, <span class="at">xlab =</span> <span class="st">"theta"</span>, <span class="at">ylab =</span> <span class="st">"posterior"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Posterior distribution for n=100</figcaption>
</figure>
</div>
</div>
</div>
<p>This demonstrates that for large sample sizes, the frequentist approach and Bayes approach agree.</p>
</div>
</section>
<section id="sec-betabinomial" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-betabinomial"><span class="header-section-number">5.4</span> Beta-Binomial Model</h2>
<p>The <strong>Beta-Binomial Bayesian model</strong> is a statistical model that is used when we are interested in learning about a proportion or probability of success, denoted by <span class="math inline">\(p\)</span>. This model is particularly useful when dealing with binary data such as conversions or clicks in A/B testing.</p>
<p>In the Beta-Binomial model, we assume that the probability of success <span class="math inline">\(\theta\)</span> in each of <span class="math inline">\(n\)</span> Bernoulli trials is not fixed but randomly drawn from a Beta distribution. The Beta distribution is defined by two shape parameters, <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>.</p>
<p>The model combines the prior information about <span class="math inline">\(\theta\)</span> (represented by the Beta distribution) and the observed data (represented by the Binomial distribution) to update our beliefs about <span class="math inline">\(p\)</span>. This is done using Bayes’ Rule, which in this context can be written as: <span class="math display">\[
p(\theta \mid Y) = \dfrac{L(\theta \mid Y)p(\theta)}{p(Y)}
\]</span> where <span class="math inline">\(p(\theta)\)</span> is the prior distribution (Beta), <span class="math inline">\(L(\theta\mid Y)\)</span> is the likelihood function (Binomial), and <span class="math inline">\(p(\theta\mid Y)\)</span> is the posterior distribution.</p>
<p>The Beta distribution is a family of continuous probability distributions defined on the interval [0,1] in terms of two positive parameters, denoted by alpha (<span class="math inline">\(\alpha\)</span>) and beta (<span class="math inline">\(\beta\)</span>), that appear as exponents of the variable and its complement to 1, respectively, and control the shape of the distribution. The Beta distribution is frequently used in Bayesian statistics, empirical Bayes methods, and classical statistics to model random variables with values falling inside a finite interval.</p>
<p>The probability density function (PDF) of the Beta distribution is given by: <span class="math display">\[
Beta(x; \alpha, \beta) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}
\]</span></p>
<p>where <span class="math inline">\(x \in [0, 1]\)</span>, <span class="math inline">\(\alpha &gt; 0\)</span>, <span class="math inline">\(\beta &gt; 0\)</span>, and <span class="math inline">\(B(\alpha, \beta)\)</span> is the beta function.</p>
<p>The mean and variance of the Beta distribution are given by: <span class="math display">\[
\begin{aligned}
\mu &amp;= \frac{\alpha}{\alpha + \beta} \\
\sigma^2 &amp;= \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}
\end{aligned}
\]</span> where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>The Beta-Binomial model is one of the simplest Bayesian models and is widely used in various fields including epidemiology, intelligence testing, and marketing. It provides the tools we need to study the proportion of interest, <span class="math inline">\(p\)</span>, in a variety of settings.</p>
<p>The nice property of the Beta-Binomial model is that the posterior <span class="math inline">\(p(p\mid Y)\)</span> is yet another Beta distribution. Beta is called a <em>conjugate prior</em> for the Binomial likelihood and is a very useful property. Given that we observed <span class="math inline">\(x\)</span> successful outcome <span class="math display">\[
Y = \sum_{i=1}^n Y_i
\]</span> the posterior distribution is given by <span class="math display">\[
p(\theta\mid Y) =Beta(Y+\alpha, n-Y+\beta)
\]</span> where <span class="math inline">\(p(\theta)\)</span> is the prior distribution (Beta), <span class="math inline">\(L(\theta\mid Y)\)</span> is the likelihood function (Binomial), and <span class="math inline">\(p(\theta\mid Y)\)</span> is the posterior distribution. Here the count of successful outcome <span class="math inline">\(Y\)</span> acts as a <em>sufficient statistic</em> for the parameter <span class="math inline">\(p\)</span>. This means that the posterior distribution depends on the data only through the sufficient statistic <span class="math inline">\(Y\)</span>. This is a very useful property and is a consequence of the conjugacy of the Beta prior and Binomial likelihood.</p>
<div id="exm-swan" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 (Black Swans)</strong></span> A related problem is the Black Swan inference problem. Suppose that after <span class="math inline">\(n\)</span> trials where <span class="math inline">\(n\)</span> is large you have only seen successes and that you assess the probability of the next trial being a success as <span class="math inline">\((T+1)/(T+2)\)</span> that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. <span class="citation" data-cites="taleb2007">Taleb (<a href="references.html#ref-taleb2007" role="doc-biblioref">2007</a>)</span> makes it sound as if the rules of probability are not rich enough to be able to handle Black Swan events. There is a related class of problems in finance known as <em>Peso problems</em> where countries decide to devalue their currencies and there is little a prior evidence from recent history that such an event is going to happen.</p>
<p>To obtain such a probability assessment we use a Binomial/Beta conjugate Bayes updating model. The key point is that it can also explain that there is still a large probability of a Black Swan event to happen <em>sometime</em> in the future. Independence model has difficulty doing this.</p>
<p>The Bayes Learning Beta-Binomial model will have no problem. We model with where <span class="math inline">\(Y_{t}=0\)</span> or <span class="math inline">\(1\)</span>, with probability <span class="math inline">\(P\left( Y_{t}=1\mid \theta\right) =\theta\)</span>. This is the classic Bernoulli “coin-flipping” model and is a component of more general specifications such as regime switching or outlier-type models.</p>
<p>Let <span class="math inline">\(Y = \sum_{t=1}^{T}y_{t}\)</span> be the number of observed successful outcomes. The likelihood for a sequence of Bernoulli observations is then <span class="math display">\[
p\left(  y\mid \theta\right)  =\prod_{t=1}^{T}p\left(  y_{t}\mid \theta\right)
=\theta^{Y}\left(  1-\theta\right)^{T-Y}.
\]</span> The maximum likelihood estimator is the sample mean, <span class="math inline">\(\widehat{\theta} = T^{-1}Y\)</span>. This makes little sense when you just observe white swans. It predicts <span class="math inline">\(\hat{\theta} = 1\)</span> and gets shocked when it sees a black swan (zero probability event). Bayes, on the other hand, allows for ‘learning’.</p>
<p>To do this we need prior distribution for the ‘parameter’ <span class="math inline">\(\theta\)</span>. A natural choice is a Beta distribution, denoted by <span class="math inline">\(\theta\sim\text{Beta}\left( a,A\right)\)</span> with pdf is given by <span class="math display">\[
p\left(  \theta\mid a,A\right)  =\frac{\theta^{a-1}\left(  1-\theta\right)^{A-1}}{B\left(  a,A\right) },
\]</span> where <span class="math inline">\(B\left( \alpha,A\right)\)</span> denotes a Beta function. Since <span class="math inline">\(p\left( \theta\mid a,A\right)\)</span> is a density and integrates to 1, we have <span class="math display">\[
B\left( a,A\right)  =\int_{0}^{1}\theta^{a-1}\left(  1-\theta\right)
^{A-1}d\theta .
\]</span> Bayes rule then tell us how to combine the likelihood and prior to obtain a posterior distribution, namely <span class="math inline">\(\theta \mid Y=y\)</span>. What do we believe about <span class="math inline">\(\theta\)</span> given a sequence of. Our predictor rule is then <span class="math inline">\(P(Y_{t=1} =1 \mid Y=y ) = \mathbb{E}(\theta \mid y)\)</span> it is straightforward to show that the posterior distribution is again a Beta distribution with <span class="math display">\[
p\left( \theta\mid y\right)  \sim Beta\left(  a_{T},A_{T}\right)  \; \mathrm{ and} \;  a_{T}=a+k , A_{T}=A+T-k.
\]</span></p>
<p>There is a “conjugate” form of the posterior: it is also a Beta distribution and the hyper-parameters <span class="math inline">\(a_{T}\)</span> and <span class="math inline">\(A_{T}\)</span> depend on the data only via the sufficient statistics, <span class="math inline">\(T\)</span> and <span class="math inline">\(k\)</span>. The posterior mean and variance are <span class="math display">\[
\mathbb{E}\left[ \theta\mid y\right]  =\frac{a_{T}}{a_{T}+A_{T}} \;\text{ and }\; \Var{
\theta\mid y}  =\frac{a_{T}A_{T}}{\left(  a_{T}+A_{T}\right)  ^{2}\left(   a_{T}+A_{T}+1\right)  }\text{,}
\]</span> respectively. This implies that for large samples, <span class="math inline">\(\E{\theta\mid y} \approx \bar{y} = \widehat{\theta}\)</span>, the MLE.</p>
<p>Suppose that after <span class="math inline">\(n\)</span> trials where <span class="math inline">\(n\)</span> is large you have only seen successes and that you assess the probability of the next trial being a success as <span class="math inline">\((T+1)/(T+2)\)</span> that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. (Taleb, 2008, The Black Swan: the Impact of the Highly Improbable).</p>
<p>To obtain such a probability assessment a natural model is Binomial/Beta conjugate Bayesian updating model. We can access the probability that a black Swan event to happen <em>sometime</em> in the future.</p>
<p>For the purpose of illustration, start with a uniform prior specification, <span class="math inline">\(\theta \sim \mathcal{U}(0,1)\)</span>, then we have the following probability assessment. After <span class="math inline">\(T\)</span> trials, suppose that we have only seen <span class="math inline">\(T\)</span> successes, namely, <span class="math inline">\(( y_1 , \ldots , y_T ) = ( 1 , \ldots , 1 )\)</span>. Then you assess the probability of the next trial being a success as <span class="math display">\[
p( Y_{T+1} =1 \mid y_1=1 , \ldots , y_T=1 ) = \frac{T+1}{T+2}
\]</span> This follows from the mean of the Beta posterior, <span class="math display">\[
\theta \mid y \sim \text{Beta}(T+1, T+1), ~ P(Y_{T+1} = 1 \mid y) = \mathbb{E}_{\theta \mid y}\left[P(Y_{T=1} \mid \theta) \right] = \mathbb{E}[\theta \mid y].
\]</span> For large <span class="math inline">\(T\)</span> this is almost certain.</p>
<p>Now consider a future set of <span class="math inline">\(n\)</span> trials, where <span class="math inline">\(n\)</span> is also large. The probability of <em>never</em> seeing a Black Swan is then given by <span class="math display">\[
p( y_{T+1} =1 , \ldots ,  y_{T+n} = 1 \mid y_1=1 , \ldots , y_T=1 ) = \frac{ T+1 }{ T+n+1 }
\]</span> For a fixed <span class="math inline">\(T\)</span>, and large <span class="math inline">\(n\)</span>, we have <span class="math inline">\(\frac{ T+1 }{ T+n+1 } \rightarrow 0\)</span>. Hence, we will see a Black Swan event with large probability — we just don’t know when! The exchangeable Beta-Binomial model then implies that a <em>Black Swan</em> event will eventually appear. One shouldn’t be that surprised when it actually happens.</p>
</div>
<div id="exm-normaltrials" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 (Clinical trials)</strong></span> Consdier a problem of designing clinical trials in which <span class="math inline">\(K\)</span> possible drugs <span class="math inline">\(a\in 1,\dots,K\)</span> need to be tested. The outcome of the treatment with drug <span class="math inline">\(a\)</span> is binary <span class="math inline">\(y(a) \in \{0,1\}\)</span>. We use bernoully distribution with mean <span class="math inline">\(f(a)\)</span> to model the outcome. Thus, the full probabilitic model is described by <span class="math inline">\(w = f(1),\dots,f(K)\)</span>.Say we have observed a sample <span class="math inline">\(D = \{y_1,\dots,y_n\}\)</span>. We would like to compute posterior distribution over <span class="math inline">\(w\)</span>. We start with <span class="math inline">\(Beta\)</span> prior <span class="math display">\[
p(w\mid \alpha,\beta) = \prod_{a=1}^K Beta(w_a\mid \alpha,\beta)    
\]</span> Then posterior distribution is given by <span class="math display">\[
p(w\mid D) = \prod_{a=1}^K Beta(w_a\mid \alpha + n_{a,1},\beta + n_{a,0})   
\]</span></p>
<p>This setrup allows us to perform sequential design of experiment. The simplest version of it is called the Thompson sampling. After observing <span class="math inline">\(n\)</span> patients, we draw a single sample <span class="math inline">\(\tilde w\)</span> from the posterior and then maximize the resulting surrogate <span class="math display">\[
a_{n+1} = \argmax_{a} f_{\tilde w}(a), ~~~ \tilde{w} \sim p(w\mid D)
\]</span></p>
</div>
<div id="exm-baseball" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 (Shrinkage and baseball batting averages)</strong></span> The batter-pitcher match-up is a fundamental element of a baseball game. There are detailed baseball records that are examined regularly by fans and professionals. This data provides a good illustration of Bayesian hierarchical methods. There is a great deal of prior information concerning the overall ability of a player. However, we only see a small amount of data about a particular batter-pitcher match-up. Given the relative small sample size, to determine our optimal estimator we build a hierarchical model taking into account the within pitcher variation.</p>
<p>Let’s analyze the variability in Jeter’s <span class="math inline">\(2006\)</span> season. Let <span class="math inline">\(p_{i}\)</span> denote Jeter’s ability against pitcher <span class="math inline">\(i\)</span> and assume that <span class="math inline">\(p_{i}\)</span> varies across the population of pitchers according to a particular probability distribution <span class="math inline">\((p_{i} \mid \alpha,\beta)\sim Be(\alpha,\beta)\)</span>. To account for extra-binomial variation we use a hierarchical model for the observed number of hits <span class="math inline">\(y_{i}\)</span> of the form <span class="math display">\[
(y_{i} \mid p_{i})\sim Bin(T_{i},p_{i})\;\;\mathrm{with}\;\;p_{i}\sim
Be(\alpha,\beta)
\]</span> where <span class="math inline">\(T_{i}\)</span> is the number of at-bats against pitcher <span class="math inline">\(i\)</span>. A priori we have a prior mean given by <span class="math inline">\(E(p_{i})=\alpha/(\alpha+\beta)=\bar{p}_{i}\)</span>. The extra heterogeneity leads to a prior variance <span class="math inline">\(Var(p_{i})=\bar{p}_{i}(1-\bar{p}_{i})\phi\)</span> where <span class="math inline">\(\phi=(\alpha+\beta+1)^{-1}\)</span>. Hence <span class="math inline">\(\phi\)</span> measures how concentrated the beta distribution is around its mean, <span class="math inline">\(\phi=0\)</span> means highly concentrated and <span class="math inline">\(\phi=1\)</span> means widely dispersed.</p>
<p>Stern (2007) estimates the parameter <span class="math inline">\(\hat{\phi} = 0.002\)</span> for Derek Jeter, showing that his ability varies a bit but not very much across the population of pitchers. The effect of the shrinkage is not surprising. The extremes are shrunk the most with the highest degree of shrinkage occurring for the match-ups that have the smallest sample sizes. The amount of shrinkage is related to the large amount of prior information concerning Jeter’s overall batting average. Overall Jeter’s performance is extremely consistent across pitchers as seen from his estimates. Jeter had a season<span class="math inline">\(.308\)</span> average. We see that his Bayes estimates vary from<span class="math inline">\(.311\)</span> to<span class="math inline">\(.327\)</span> and that he is very consistent. If all players had a similar record then the assumption of a constant batting average would make sense.</p>
<table class="table">
<thead>
<tr class="header">
<th>Pitcher</th>
<th>At-bats</th>
<th>Hits</th>
<th>ObsAvg</th>
<th>EstAvg</th>
<th>95% Int</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>R. Mendoza</td>
<td>6</td>
<td>5</td>
<td>.833</td>
<td>.322</td>
<td>(.282</td>
<td>.394)</td>
</tr>
<tr class="even">
<td>H. Nomo</td>
<td>20</td>
<td>12</td>
<td>.600</td>
<td>.326</td>
<td>(.289</td>
<td>.407)</td>
</tr>
<tr class="odd">
<td>A.J.Burnett</td>
<td>5</td>
<td>3</td>
<td>.600</td>
<td>.320</td>
<td>(.275</td>
<td>.381)</td>
</tr>
<tr class="even">
<td>E. Milton</td>
<td>28</td>
<td>14</td>
<td>.500</td>
<td>.324</td>
<td>(.291</td>
<td>.397)</td>
</tr>
<tr class="odd">
<td>D. Cone</td>
<td>8</td>
<td>4</td>
<td>.500</td>
<td>.320</td>
<td>(.218</td>
<td>.381)</td>
</tr>
<tr class="even">
<td>R. Lopez</td>
<td>45</td>
<td>21</td>
<td>.467</td>
<td>.326</td>
<td>(.291</td>
<td>.401)</td>
</tr>
<tr class="odd">
<td>K. Escobar</td>
<td>39</td>
<td>16</td>
<td>.410</td>
<td>.322</td>
<td>(.281</td>
<td>.386)</td>
</tr>
<tr class="even">
<td>J. Wettland</td>
<td>5</td>
<td>2</td>
<td>.400</td>
<td>.318</td>
<td>(.275</td>
<td>.375)</td>
</tr>
<tr class="odd">
<td>T. Wakefield</td>
<td>81</td>
<td>26</td>
<td>.321</td>
<td>.318</td>
<td>(.279</td>
<td>.364)</td>
</tr>
<tr class="even">
<td>P. Martinez</td>
<td>83</td>
<td>21</td>
<td>.253</td>
<td>.312</td>
<td>(.254</td>
<td>.347)</td>
</tr>
<tr class="odd">
<td>K. Benson</td>
<td>8</td>
<td>2</td>
<td>.250</td>
<td>.317</td>
<td>(.264</td>
<td>.368)</td>
</tr>
<tr class="even">
<td>T. Hudson</td>
<td>24</td>
<td>6</td>
<td>.250</td>
<td>.315</td>
<td>(.260</td>
<td>.362)</td>
</tr>
<tr class="odd">
<td>J. Smoltz</td>
<td>5</td>
<td>1</td>
<td>.200</td>
<td>.314</td>
<td>(.253</td>
<td>.355)</td>
</tr>
<tr class="even">
<td>F. Garcia</td>
<td>25</td>
<td>5</td>
<td>.200</td>
<td>.314</td>
<td>(.253</td>
<td>.355)</td>
</tr>
<tr class="odd">
<td>B. Radke</td>
<td>41</td>
<td>8</td>
<td>.195</td>
<td>.311</td>
<td>(.247</td>
<td>.347)</td>
</tr>
<tr class="even">
<td>D. Kolb</td>
<td>5</td>
<td>0</td>
<td>.000</td>
<td>.316</td>
<td>(.258</td>
<td>.363)</td>
</tr>
<tr class="odd">
<td>J. Julio</td>
<td>13</td>
<td>0</td>
<td>.000</td>
<td>.312</td>
<td>(.243</td>
<td>.350 )</td>
</tr>
<tr class="even">
<td>Total</td>
<td>6530</td>
<td>2061</td>
<td>.316</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Some major league managers believe strongly in the importance of such data (Tony La Russa, <em>Three days in August</em>). One interesting example is the following. On Aug 29, 2006, Kenny Lofton (career<span class="math inline">\(.299\)</span> average, and current<span class="math inline">\(.308\)</span> average for <span class="math inline">\(2006\)</span> season) was facing the pitcher Milton (current record <span class="math inline">\(1\)</span> for <span class="math inline">\(19\)</span>). He was <em>rested</em> and replaced by a<span class="math inline">\(.273\)</span> hitter. Is putting in a weaker player rally a better bet? Was this just an over-reaction to bad luck in the Lofton-Milton match-up? Statistically, from Lofton’s record against Milton we have <span class="math inline">\(P\left( \leq 1\;\mathrm{hit\;in}\% 19\;\mathrm{attempts} \mid p=0.3\right) =0.01\)</span> an unlikely <span class="math inline">\(1\)</span>-in-<span class="math inline">\(100\)</span> event. However, we have not taken into account the multiplicity of different batter-pitcher match-ups. We know that Loftin’s batting percentage will vary across different pitchers, it’s just a question of how much? A hierarchical analysis of Lofton’s variability gave a <span class="math inline">\(\phi=0.008\)</span> – four times larger than Jeter’s <span class="math inline">\(\phi=0.002\)</span>. Lofton has batting estimates that vary from<span class="math inline">\(.265\)</span> to<span class="math inline">\(.340\)</span> with the lowest being against Milton. Hence, the optimal estimate for a pitch against Milton is<span class="math inline">\(.265&lt;.275\)</span> and resting Lofton against Milton is justified by this analysis.</p>
</div>
</section>
<section id="poisson-model-for-count-data" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="poisson-model-for-count-data"><span class="header-section-number">5.5</span> Poisson Model for Count Data</h2>
<p>The Poisson distribution is obtained as a result of the Binomial when <span class="math inline">\(p\)</span> is small and <span class="math inline">\(n\)</span> is large. In applications, the Poisson models count data. Suppose we want to model arrival rate of users to one of our stores. Let <span class="math inline">\(\lambda = np\)</span>, which is fixed and take the limit as <span class="math inline">\(n \rightarrow \infty\)</span>. There is a relationship between , <span class="math inline">\(p(x)\)</span> ans <span class="math inline">\(p(x+1)\)</span> given by <span class="math display">\[
\dfrac{p(x+1)}{p(x)}= \dfrac{\left(\dfrac{n}{x+1}\right)p^{x+1}(1-p)^{n-x-1}}{\left(\dfrac{n}{x}\right)p^{x}(1-p)^{n-x}} \approx \dfrac{np}{x+1}
\]</span> If, we approximate <span class="math inline">\(p(x+1)\approx \lambda p(x)/(x+1)\)</span> with <span class="math inline">\(\lambda=np\)</span>, then we obtain the Poission pdf given by <span class="math inline">\(p(x) = p(0)\lambda^x/x!\)</span>. To ensure that <span class="math inline">\(\sum_{x=0}^\infty p(x) = 1\)</span>, we set <span class="math display">\[
f(0) = \dfrac{1}{\sum_{x=0}^{\infty}\lambda^x/x!} = e^{-\lambda}.
\]</span> The above equality follows from the power series property of the exponent function <span class="math display">\[
e^{\lambda} = \sum_{x=0}^{\infty}\dfrac{\lambda^x}{x!}
\]</span> The <strong>Poisson distribution</strong> counts the occurrence of events. Given a rate parameter, denoted by <span class="math inline">\(\lambda\)</span>, we calculate probabilities as follows <span class="math display">\[
p( X = x ) = \frac{ e^{-\lambda} \lambda^x }{x!} \; \mathrm{ where} \; x=0,1,2,3, \ldots
\]</span> The mean and variance of the Poisson are given by:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Poisson Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = \lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = \lambda\)</span></td>
</tr>
</tbody>
</table>
<p>Here <span class="math inline">\(\lambda\)</span> denotes the rate of occurrence of an event.</p>
<p>Consider the problem of modeling soccer scores in the English Premier League (EPL) games. We use data from Betfair, a website, which posts odds on many football games. The goal is to calculate odds for the possible scores in a match. <span class="math display">\[
0-0, \; 1-0, \; 0-1, \; 1-1, \; 2-0, \ldots
\]</span> Another question we might ask, is what’s the odds of a team winning?</p>
<p>This is given by <span class="math inline">\(P\left ( X&gt; Y \right )\)</span>. The odds’s of a draw are given by <span class="math inline">\(P \left ( X = Y \right )\)</span>?</p>
<p>Professional sports betters rely on sophisticated statistical models to predict the outcomes. Instead, we present a simple, but useful model for predicting outcomes of EPL games. We follow the methodology given in <span class="citation" data-cites="spiegelhalter2009">Spiegelhalter and Ng (<a href="references.html#ref-spiegelhalter2009" role="doc-biblioref">2009</a>)</span>.</p>
<p>First, load the data and then model the number of goals scored using Poisson distribution.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../../data/epl.csv"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(df[,<span class="fu">c</span>(<span class="st">"home_team_name"</span>,<span class="st">"away_team_name"</span>,<span class="st">"home_score"</span>,<span class="st">"guest_score"</span>)]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">home_team_name</th>
<th style="text-align: left;">away_team_name</th>
<th style="text-align: right;">home_score</th>
<th style="text-align: right;">guest_score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Arsenal</td>
<td style="text-align: left;">Liverpool</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bournemouth</td>
<td style="text-align: left;">Manchester United</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Burnley</td>
<td style="text-align: left;">Swansea</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chelsea</td>
<td style="text-align: left;">West Ham</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Crystal Palace</td>
<td style="text-align: left;">West Bromwich Albion</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Everton</td>
<td style="text-align: left;">Tottenham</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Let’s look at the empirical distribution across the number of goals scored by Manchester United</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>team_name<span class="ot">=</span><span class="st">"Manchester United"</span> </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>team_for  <span class="ot">=</span> <span class="fu">c</span>(df[df<span class="sc">$</span>home_team_name<span class="sc">==</span>team_name,<span class="st">"home_score"</span>],df[df<span class="sc">$</span>away_team_name<span class="sc">==</span>team_name,<span class="st">"guest_score"</span>]) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(team_for) </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>for_byscore <span class="ot">=</span> <span class="fu">table</span>(team_for)<span class="sc">/</span>n </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(for_byscore, <span class="at">col=</span><span class="st">"coral"</span>, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram of Goals Scored by MU</figcaption>
</figure>
</div>
</div>
</div>
<p>Hence the historical data fits closely to a Poisson distribution, the parameter <span class="math inline">\(\lambda\)</span> describes the average number of goals scored and we calculate it by calculating the sample mean, the maximum likelihood estimate. A Bayesian method where we assume that <span class="math inline">\(\lambda\)</span> has a Gamma prior is also available. This lets you incorporate outside information into the predictive model.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lambda_for <span class="ot">=</span> <span class="fu">mean</span>(team_for) </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">rbind</span>(<span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lambda =</span> lambda_for),for_byscore),<span class="at">beside =</span> T, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"aquamarine3"</span>,<span class="st">"coral"</span>), <span class="at">xlab=</span><span class="st">"Goals"</span>, <span class="at">ylab=</span><span class="st">"probability"</span>, <span class="at">main=</span><span class="st">""</span>) </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Poisson"</span>,<span class="st">"MU"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"aquamarine3"</span>, <span class="st">"coral"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram vs Poisson Model Prediction of Goals Scored by MU</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we will use Poisson model and Monter Carlo simulations to predict possible outcomes of the MU vs Hull games. First we estimate the rate parameter for goals by MU <code>lmb_mu</code> and goals by Hull <code>lmb_h</code>. Each team played a home and away game with every other team, thus 38 total games was played by all teams. We calculate the average by dividing total number of goals scored by the number of games</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sumdf <span class="ot">=</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(home_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">Goals_For_Home =</span> <span class="fu">sum</span>(home_score)) <span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(away_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_For_Away =</span> <span class="fu">sum</span>(guest_score)), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"home_team_name"</span> <span class="ot">=</span> <span class="st">"away_team_name"</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(home_team_name) <span class="sc">%&gt;%</span> </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_Against_Home =</span> <span class="fu">sum</span>(guest_score))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">full_join</span>(df <span class="sc">%&gt;%</span> </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>              <span class="fu">group_by</span>(away_team_name) <span class="sc">%&gt;%</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>              <span class="fu">summarise</span>(<span class="at">Goals_Against_Away =</span> <span class="fu">sum</span>(home_score)), <span class="at">by =</span> <span class="fu">c</span>(<span class="st">"home_team_name"</span> <span class="ot">=</span> <span class="st">"away_team_name"</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            ) <span class="sc">%&gt;%</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">Team=</span>home_team_name)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(sumdf[sumdf<span class="sc">$</span>Team <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"Manchester United"</span>, <span class="st">"Hull"</span>),])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Team</th>
<th style="text-align: right;">Goals_For_Home</th>
<th style="text-align: right;">Goals_For_Away</th>
<th style="text-align: right;">Goals_Against_Home</th>
<th style="text-align: right;">Goals_Against_Away</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hull</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">45</td>
</tr>
<tr class="even">
<td style="text-align: left;">Manchester United</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">17</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>lmb_mu <span class="ot">=</span> (<span class="dv">26</span><span class="sc">+</span><span class="dv">28</span>)<span class="sc">/</span><span class="dv">38</span>; <span class="fu">print</span>(lmb_mu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lmb_h <span class="ot">=</span> (<span class="dv">28</span><span class="sc">+</span><span class="dv">9</span>)<span class="sc">/</span><span class="dv">38</span>; <span class="fu">print</span>(lmb_h)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.97</code></pre>
</div>
</div>
<p>Now we simulate 100 games between the teams</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">100</span>,lmb_mu)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">100</span>,lmb_h)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x<span class="sc">&gt;</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 55</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x<span class="sc">==</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 27</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">table</span>(x,y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
<th style="text-align: right;">4</th>
<th style="text-align: right;">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>From our simulaiton that 55 number of times MU wins and 27 there is a draw. The actual outcome was 0-0 (Hull at MU) and 0-1 (Mu at Hull). Thus our model fives a reasonable prediction.</p>
<p>The model can be improved by calculating different averages for home and away games. For example, Hull does much better at home games compared to away games. Further, we can include the characteristics of the opponent team to account for interactions between attack strengh (number of scored) and defence weakness of the opponent. Now we modify our value of expected goals for each of the teams by calculating <span class="math display">\[
\hat \lambda = \lambda \times  \text{Defense weakness}
\]</span></p>
<p>Let’s model the MU at Hull game. The average away goals for MU <span class="math inline">\(28/19 = 1.47\)</span> and the defence weakness of Hull is <span class="math inline">\(36/19 = 1.84\)</span>, thus the adjusted expected number of goals to be scored by MU is 2.79. Similarly, the adjusted number of the goals Hull is expected to score is <span class="math inline">\(28/19 \times 17/19 = 1.32\)</span></p>
<p>As a result of the simulation, we obtain</p>
<div class="cell" data-out-heigth="7in" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">100</span>, <span class="dv">28</span> <span class="sc">/</span> <span class="dv">19</span> <span class="sc">*</span> <span class="dv">35</span> <span class="sc">/</span> <span class="dv">19</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">100</span>, <span class="dv">28</span> <span class="sc">/</span> <span class="dv">19</span> <span class="sc">*</span> <span class="dv">17</span> <span class="sc">/</span> <span class="dv">19</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">table</span>(x, y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">0</th>
<th style="text-align: right;">1</th>
<th style="text-align: right;">2</th>
<th style="text-align: right;">3</th>
<th style="text-align: right;">4</th>
<th style="text-align: right;">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">3</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">4</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">7</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="at">z =</span> <span class="fu">table</span>(x, y), <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">7</span>, <span class="at">y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">xlab =</span> <span class="st">"MU Score"</span>, <span class="at">ylab =</span> <span class="st">"Hull Score"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now we can calculate the number of times MU wins:</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(x <span class="sc">&gt;</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 67</code></pre>
</div>
</div>
<!--     Team      Expected Goals   0    1    2    3    4    5 -->
<!-- ----------- ---------------- ---- ---- ---- ---- ---- ---- -->
<!--     Man U          2.95        7    22   26   12   11   13 -->
<!--   Hull City        0.65        49   41   10   0    0    0 -->
<p>A model is only as good as its predictions. Let’s see how our model did in out-of-sample prediction,</p>
<ul>
<li>Man U wins 67 games out of 100, we should bet when odds ratio is below 67 to 100.</li>
<li>Most likely outcome is 1-2 (16 games out of 100)</li>
<li>The actual outcome was 0-1 (they played on August 27, 2016)</li>
<li>In out simulation 0-1 was the fourth most probable outcome (8 games out of 100).</li>
</ul>
</section>
<section id="gamma-poisson-learning-about-a-poisson-intensity" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="gamma-poisson-learning-about-a-poisson-intensity"><span class="header-section-number">5.6</span> Gamma-Poisson: Learning about a Poisson Intensity</h2>
<p>Consider a continuous-time stochastic process, <span class="math inline">\(\left\{ N_{t}\right\} _{t\geq0}\)</span>, with <span class="math inline">\(N_{0}=0\)</span>, counting the number of events that have occured up to time <span class="math inline">\(t\)</span>. The process is constant between event times, and jumps by one at event times:&nbsp;<span class="math inline">\(\Delta N_{t}=N_{t}-N_{t-}=1,\)</span> where <span class="math inline">\(N_{t-}\)</span> is the limit from the left. The probability of an event over the next short time interval, <span class="math inline">\(\Delta t\)</span> is <span class="math inline">\(\lambda\Delta t\)</span>, and <span class="math inline">\(N_{t}\)</span> is called a Poisson process because <span class="math display">\[
P\left[  N_{t}=k\right]  =\frac{e^{-\lambda t}\left(  \lambda
t\right)  ^{k}}{k!}\text{ for }k=1,\ldots
\]</span> which is the Poisson distribution, thus <span class="math inline">\(N_{t}\sim Poi\left(\lambda t\right)\)</span>. A more general version of the Poisson process is a Cox process, or doubly stochastic point process.</p>
<p>Here, there is additional conditioning information in the form of state variables, <span class="math inline">\(\left\{X_{t}\right\}_{t&gt;0}\)</span>. The process now has two sources of randomness, oneassociated with the discontinuous jumps and another in the form of randomstate variables, <span class="math inline">\(\left\{X_{t}\right\}_{t&gt;0}\)</span>, that drive the intensity ofthe process. The intensity of the Cox process is <span class="math inline">\(\lambda_{t}=\int_{0}^{t}\lambda\left( X_{s}\right) ds\)</span>, which is formally defined as <span class="math display">\[
P\left[  N_{t}-N_{s}=k \mid \left\{  X_{u}\right\}  _{s\leq u\leq
t}\right]  =\frac{\left(  \int_{s}^{t}\lambda\left(  X_{s}\right)  ds\right)
^{k}\exp\left(  -\int_{s}^{t}\lambda\left(  X_{s}\right)  ds\right)}{k!}, ~ k=0,1,\ldots
\]</span> Cox processes are very usefull extensions to Poisson processes and are the basic building blocks of reduced form models of defaultable bonds.</p>
<p>The inference problem is to learn about <span class="math inline">\(\lambda\)</span> from a continuous-record of observation up to time <span class="math inline">\(t\)</span>. The likelihood function is given by <span class="math display">\[
p\left(  N_{t}=k \mid \lambda\right)  =\frac{\left(  \lambda t\right)  ^{k}%
\exp\left(  -\lambda t\right)  }{k!},
\]</span> and the MLE is <span class="math inline">\(\widehat{\lambda}=N_{t}/t\)</span>. The MLE has the unattractiveproperty that prior to the first event <span class="math inline">\(\left\{ t:N_{t}=0\right\}\)</span>, the MLEis 0, despite the fact that the model explicitly assumes that events arepossible. This problem often arises in credit risk contexts, where it wouldseem odd to assume that the probability of default is zero just because adefault has not yet occured.</p>
<p>A natural prior for this model is the Gamma distribution, which has the following pdf <span id="eq-gamma-pdf"><span class="math display">\[
p\left(  \lambda \mid a,A\right)  =\frac{A^{a}}{\Gamma(A)  }%
x^{a-1}\exp\left(  -Ax\right)  \text{.}%
\tag{5.2}\]</span></span> Like the beta distribution, a Gamma prior distribution allows for a variety of prior shapes and is parameterized by two hyperparameters. Combining the prior and likelihood, the posterior is also Gamma: <span class="math display">\[
p\left(  \lambda \mid N_{t}\right)  \propto\frac{\left(  \lambda\right)
^{N_{t}+a-1}\exp\left(  -\lambda\left(  t+A\right)  \right)  }{N_{t}!}%
\sim\mathcal{G}\left(  a_{t},A_{t}\right)  ,
\]</span> where <span class="math inline">\(a_{t}=N_{t}+a\)</span> and <span class="math inline">\(A_{t}=t+A\)</span>. The expected intensity, based on information up to time <span class="math inline">\(t\)</span>, is <span class="math display">\[
\mathbb{E}\left[  \lambda \mid N_{t}\right]  =\frac{a_{t}}{A_{t}}=\frac{N_{t}%
+a}{t+A}=w_{t}\frac{N_{t}}{t}+\left(  1-w_{t}\right)  \frac{a}{A},
\]</span> where the second line expresses the posterior mean in shrinkage form as a weighted average of the MLE and the prior mean where <span class="math inline">\(w_{t}=t/(t+A)\)</span>. In large samples, <span class="math inline">\(w_{t}\rightarrow1\)</span> and <span class="math inline">\(E\left( \lambda \mid N_{t}\right) \approx N_{t}/t=\widehat{\lambda}\)</span>.</p>
<p>To understand the updating mechanics, <a href="#fig-poiss" class="quarto-xref">Figure&nbsp;<span>5.1</span></a> (right column) displays a simulated sample path, posterior means, and (5%,95%) posterior quantiles for various prior configurations. In this case, time is measured in years and the intensity used to simulate the data is <span class="math inline">\(\lambda=1\)</span>, implying on average one event per year. The four prior configurations embody different beliefs. In the first case, in the middle left panel, <span class="math inline">\(a=4\)</span> and <span class="math inline">\(A=1\)</span>, captures a high-activity prior, that posits that jumps occur, on average, four times per year, and there is substantial prior uncertainty over the arrival rate as the (5%,95%) prior quantiles are (1.75,6.7). In the second case, in the middle right panel, captures a prior that is centered over the true value with modest prior uncertainty. The third case, the bottom left panel, captures a low-activity prior, with a prior mean of 0.2 jumps/year. The fouth case captures a dogmatic prior, that posits that jumps occur three times per year, with high confidence in these beliefs.</p>
<p>The priors were chosen to highlight different potential paths for Bayesian learning. The first thing to note from the priors is the discontinuity upward at event times, and the exponential decrease during periods of no events, both of which are generic properties of Bayesian learning in this model. If one thinks of the events as rare, this implies rapid revisions in beliefs at event times and a constant drop in estimates of the intensity in periods of no events. For the high-activity prior and the sample path observed, the posterior begins well above <span class="math inline">\(\lambda=1\)</span>, and slowly decreases, getting close to <span class="math inline">\(\lambda=1\)</span> at the end of the sample. This can be somewhat contrasted with the low-activity prior, which has drastic revisisions upward at jump times. In the dogmatic case, there is little updating at event times. The prior parameters control how rapidly beliefs change, with noticeable differences across the priors.%</p>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>) <span class="co"># Ovi</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>lmb <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">rpois</span>(<span class="dv">5</span>,t<span class="sc">*</span>lmb)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># A: rate (beta), a: shape (alpha)</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plotgamma <span class="ot">=</span> <span class="cf">function</span>(a,A,N) {</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="fu">dgamma</span>(x,a,A),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">"t"</span>,<span class="at">ylab=</span><span class="st">"Gamma(t)"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    at <span class="ot">=</span> N<span class="sc">+</span>a</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">=</span> t<span class="sc">+</span>A</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    mean <span class="ot">=</span> at<span class="sc">/</span>At</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(N, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">"orange"</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">5</span>), <span class="at">xlab=</span><span class="st">"t"</span>, <span class="at">ylab=</span><span class="st">"N(t)"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(mean, <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">qgamma</span>(<span class="fl">0.05</span>,at,At), <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">qgamma</span>(<span class="fl">0.95</span>,at,At), <span class="at">col=</span><span class="st">"grey"</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">4</span>,<span class="at">A=</span><span class="dv">1</span>, N)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">1</span>,<span class="at">A=</span><span class="dv">1</span>, N)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">1</span>,<span class="at">A=</span><span class="dv">5</span>, N)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plotgamma</span>(<span class="at">a=</span><span class="dv">30</span>,<span class="at">A=</span><span class="dv">10</span>, N)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-poiss" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poiss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-1.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) a = 4, A = 1
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-2.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-3" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-3.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) a = 1, A = 1
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-4" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-4.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-5" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-5.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(e) a = 1, A = 5
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-6" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-6.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-6-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(f) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-7" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-7.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-7-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(g) a = 30, A = 10
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-poiss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-poiss-8" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-poiss-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-poiss-8.png" class="img-fluid figure-img" data-ref-parent="fig-poiss" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-poiss-8-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(h) Posterior
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poiss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Sensitivity of Gamma Prior for Poisson Process
</figcaption>
</figure>
</div>
<p>Poisson event models are often embedded as portion of more complicated model to capture rare events such as stock market crashes, volatility surges, currency revaluations, or defaults. In these cases, prior distributions are often important–even essential–since it is common to build models with events that could, but have not yet occurred. These events are often called ‘Peso’ events. For example, in the case of modeling corporate defaults a researcher wants to allow for a jump to default. This requires positing a prior distribution that places non-zero probability on an event occuring. Classical statistical methods have difficulties dealing with these situations since the MLE of the jump probability is zero, until the first event occurs.</p>
</section>
<section id="normal-normal-model-for-continuous-data" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="normal-normal-model-for-continuous-data"><span class="header-section-number">5.7</span> Normal-Normal Model for Continuous Data</h2>
<p>The Normal or Gaussian distribution is central to probability and statistical inference. Suppose that we are trying to predict tomorrow’s return on the S&amp;P500. There’s a number of questions that come to mind</p>
<ol type="1">
<li><p>What is the random variable of interest?</p></li>
<li><p>How can we describe our uncertainty about tomorrow’s outcome?</p></li>
<li><p>Instead of listing all possible values we’ll work with intervals instead. The probability of an interval is defined by the area under the probability density function.</p></li>
</ol>
<p>Returns are are continuous (as opposed to discrete) random variables. Hence a normal distribution would be appropriate - but on what scale? We will see that on the log-scale a Normal distribution provides a good approximation.</p>
<p>The most widely used model for a continuous random variable is the normal distribution. Standard normal random variable <span class="math inline">\(Z\)</span> has the following properties</p>
<p>The standard Normal has mean <span class="math inline">\(0\)</span> and has a variance <span class="math inline">\(1\)</span>, and is written as <span class="math display">\[
Z \sim N(0,1)
\]</span> Then, we have the probability statements of interest <span class="math display">\[\begin{align*}  
P{-1 &lt;Z&lt; 1} &amp;=0.68\\
P{-1.96 &lt;Z&lt; 1.96} &amp;=0.95\\
\end{align*}\]</span></p>
<p>In <code>R</code>, we can find probabilities</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.96</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.98</code></pre>
</div>
</div>
<p>and quantiles</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.9750</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 2</code></pre>
</div>
</div>
<p>The quantile function <code>qnorm</code> is the inverse of <code>pnorm</code>.</p>
<p>A random variable that follows normal distribution with general mean and variance <span class="math inline">\(X \sim \mbox{N}(\mu, \sigma^2)\)</span>, has the following properties <span class="math display">\[\begin{align*}
  p(\mu - 2.58 \sigma &lt; X &lt; \mu + 2.58 \sigma) &amp;=&amp; 0.99 \\
  p(\mu - 1.96 \sigma &lt; X &lt; \mu + 1.96 \sigma) &amp;=&amp; 0.95 \, .
\end{align*}\]</span> The chance that <span class="math inline">\(X\)</span> will be within <span class="math inline">\(2.58 \sigma\)</span> of its mean is <span class="math inline">\(99\%\)</span>, and the chance that it will be within <span class="math inline">\(2\sigma\)</span> of its mean is about <span class="math inline">\(95\%\)</span>.</p>
<p>The probability model is written <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> is the mean, <span class="math inline">\(\sigma^2\)</span> is the variance. This can be transformed to a standardized normal via <span class="math display">\[
Z =\frac{X-\mu}{\sigma} \sim N(0,1).
\]</span> For a normal distribution, we know that <span class="math inline">\(X \in [\mu-1.96\sigma,\mu+1.96\sigma]\)</span> with probability 95%. We can make similar claims for any other distribution using the Chebyshev’s empirical rule, which is valid for any population:</p>
<ol type="1">
<li><p>At least 75% probability lies within 2<span class="math inline">\(\sigma\)</span> of the mean <span class="math inline">\(\mu\)</span></p></li>
<li><p>At least 89% lies within 3<span class="math inline">\(\sigma\)</span> of the mean <span class="math inline">\(\mu\)</span></p></li>
<li><p>At least <span class="math inline">\(100(1-1/m^2)\)</span>% lies within <span class="math inline">\(m\times \sigma\)</span> of the mean <span class="math inline">\(\mu\)</span>.</p></li>
</ol>
<p>This also holds true for the Normal distribution. The percentages are <span class="math inline">\(95\)</span>%, <span class="math inline">\(99\)</span>% and <span class="math inline">\(99.99\)</span>%.</p>
</section>
<section id="z-score" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="z-score"><span class="header-section-number">5.8</span> <span class="math inline">\(Z\)</span>-Score</h2>
<div id="exm-crash1987" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.6 (Stock market crash 1987)</strong></span> Prior to the October, 1987 crash SP500 monthly returns were 1.2% with a risk/volatility of 4.3%. The question is how extreme was the 1987 crash of <span class="math inline">\(-21.76\)</span>%? <span class="math display">\[
X \sim N \left(1.2, 4.3^2 \right )
\]</span> This probability distribution can be standardized to yield <span class="math display">\[
Z =\frac{X-\mu}{\sigma} = \frac{X - 1.2}{4.3} \sim N(0,1) .
\]</span> Now,, we calculate the observed <span class="math inline">\(Z\)</span>, given the outcome of the crash event <span class="math display">\[
Z = \frac{-0.2176 - 0.012}{0.043} = -5.27
\]</span> That’s is <span class="math inline">\(5\)</span>-sigma event in terms of the distribution of <span class="math inline">\(X\)</span>. Meaning that -0.2176 is 5 standard diviations away from the mean. Under a normal model that is equivalent to <span class="math inline">\(P(X &lt; -0.2176)\)</span> = 4.66^{-8}.</p>
<p>On August 24th, 2015, Chinese equities ended down $- 8.5 $% (Black Monday). In the last <span class="math inline">\(25\)</span> years, average is $0.09 $% with a volatility of $2.6 $%, and <span class="math inline">\(56\)</span>% time close within one standard deviation. SP500, average is <span class="math inline">\(0.03\)</span>% with a volatility of <span class="math inline">\(1.1\)</span>%. <span class="math inline">\(74\)</span>% time close within one standard deviation</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../fig/stock-monday.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Economist article, August 2015.</figcaption>
</figure>
</div>
</div>
</section>
<section id="kalman-filter-continious-normal-normal-updating" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="kalman-filter-continious-normal-normal-updating"><span class="header-section-number">5.9</span> Kalman Filter (Continious Normal-Normal Updating)</h2>
<p>The method of estimating the parameters of the normal distribution we used above (sample mean and variance) is called the method of moments, meaning that we estimate the variance and the mean (first two moments of a distribution) by simply calculating those from the observed sample.</p>
<p>We can formalise this fully in the Normal/ Normal Bayes learning model as follows. We add prior <span class="math inline">\(\theta\sim \mathcal{N}(\theta_0, \tau^2)\)</span> and likelihood, <span class="math inline">\(p(y \mid \theta, \sigma^2) \equiv p(\bar{y} \mid \theta, \sigma^2)\)</span>.</p>
<p>Notice that we have reduced the data by sufficiency and we only need <span class="math inline">\(\bar{y}\)</span> rather than the full dataset <span class="math inline">\(\left(y_1,y_2, \ldots, y_n \right)\)</span>. The Normal/ Normal Bayesian learning model provides the basis for shrinkage estimation of multiply means and the basis of the Kalman filter for dynamically tracking a path of an object.</p>
<p>The Kalman filter is arguable the most common application of Bayesian inference. The Kalman filter assumes a linear and Gaussian state spacemodel: <span class="math display">\[
y_{t}=x_{t}+\sigma\varepsilon_{t}^{y}\text{ and }x_{t}=x_{t-1}+\sigma
_{x}\varepsilon_{t}^{x},
\]</span> where <span class="math inline">\(\varepsilon_{t}^{y}\)</span> and <span class="math inline">\(\varepsilon_{t}^{x}\)</span> are i.i.d. standard normal and <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\sigma_{x}\)</span> are known. The observation equation posits that the observed data, <span class="math inline">\(y_{t}\)</span>, consists of the random-walk latent state, <span class="math inline">\(x_{t}\)</span>, that is polluted by noise, <span class="math inline">\(\sigma\varepsilon_{t}^{y}\)</span>. <span class="math inline">\(\sigma_{x}/\sigma\)</span>, the `signal-to-noise’ ratio, measures the information content of the signal. As <span class="math inline">\(\sigma\)</span> increases relatively to <span class="math inline">\(\sigma_{x}\)</span>, the observations become noisier and less informative. The model is initialized via a prior distribution over <span class="math inline">\(x_{0}\)</span>, which is for analytical tractability must be normally distributed, <span class="math inline">\(x_{0}\sim\mathcal{N}\left( \mu_{0},\sigma_{0}^{2}\right)\)</span>. Numerical examples of this model will be given later.</p>
<p>The posterior distribution solves the filtering problem and is defined recursively via Bayes rule: <span class="math display">\[
p\left(  x_{t+1} \mid y^{t+1}\right)  =\frac{p\left(  y_{t+1} \mid x_{t+1}\right)
p\left(  x_{t+1} \mid y^{t}\right)  }{p\left(  y_{t+1} \mid y^{t}\right)  }\propto
p\left(  y_{t+1} \mid x_{t+1}\right)  p\left(  x_{t+1} \mid y^{t}\right)  \text{.}%
\]</span> and the likelihood function, <span class="math inline">\(p\left( y_{t+1} \mid x_{t+1}\right)\)</span>. The predictive distribution summarizes all of the information about <span class="math inline">\(x_{t+1}\)</span> based on lagged observations. The likelihood function summarizes the new information in <span class="math inline">\(y_{t+1}\)</span> about <span class="math inline">\(x_{t+1}\)</span>.</p>
<p>The Kalman filter relies on an inductive argument:&nbsp;assume that <span class="math inline">\(p\left( x_{t} \mid y^{t}\right) \sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}\right)\)</span> and then verify that <span class="math inline">\(p\left( x_{t+1} \mid y^{t+1}\right) \sim\mathcal{N}\left( \mu_{t+1},\sigma_{t+1}^{2}\right)\)</span> with analytical expressions for the hyperparameters. To verify, note that since <span class="math inline">\(p\left(x_{t} \mid y^{t}\right) \sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}\right)\)</span>, <span class="math inline">\(x_{t}=\mu_{t}+\sigma_{t}\eta_{t}\)</span> for some standard normal <span class="math inline">\(\eta_{t}\)</span>. Substituting into the state evolution, the predictive is <span class="math inline">\(x_{t+1} \mid y^{t}\sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}+\sigma_{x}^{2}\right)\)</span>. Since <span class="math inline">\(p\left( y_{t+1} \mid x_{t+1}\right) \sim\mathcal{N}\left(x_{t+1},\sigma^{2}\right)\)</span>, the posterior is <span class="math display">\[\begin{align*}
p\left(  x_{t+1} \mid y^{t+1}\right)   &amp;  \propto p\left(  y_{t+1} \mid x_{t+1}\right)
p\left(  x_{t+1} \mid y^{t}\right)  \propto\exp\left[  -\frac{1}{2}\left( \frac{\left(  y_{t+1}-x_{t+1}\right)  ^{2}}{\sigma^{2}}+\frac{\left( x_{t+1}-\mu_{t}\right)  ^{2}}{\sigma_{t}^{2}+\sigma_{x}^{2}}\right)  \right]
\\
&amp;  \propto\exp\left(  -\frac{1}{2}\frac{\left(  x_{t+1}-\mu_{t+1}\right)
^{2}}{\sigma_{t+1}^{2}}\right)
\end{align*}\]</span> where <span class="math inline">\(\mu_{t+1}\)</span> and <span class="math inline">\(\sigma_{t+1}^{2}\)</span> are computed by completing the square: <span class="math display">\[\begin{equation}
\frac{\mu_{t+1}}{\sigma_{t+1}^{2}}=\frac{y_{t+1}}{\sigma^{2}}+\frac{\mu_{t}%
}{\sigma_{t}^{2}+\sigma_{x}^{2}}\text{ and }\frac{1}{\sigma_{t+1}^{2}}%
=\frac{1}{\sigma^{2}}+\frac{1}{\sigma_{t}^{2}+\sigma_{x}^{2}}\text{.}\nonumber
\end{equation}\]</span> Here, inference on <span class="math inline">\(x_{t}\)</span> is merely running the Kalman filter, that is, sequential computing <span class="math inline">\(\mu_{t}\)</span> and <span class="math inline">\(\sigma_{t}^{2}\)</span>, which are state suffiicent statistics.</p>
<p>The Kalman filter provides an excellent example of the mechanics of Bayesian inference:&nbsp;given a prior and likelihood, compute the posterior distribution. In this setting, it is hard to imagine an more intuitive or alternative approach. The same approach applied to learning fixed static parameters. In this case, <span class="math inline">\(y_{t}=\mu+\sigma\varepsilon_{t},\)</span> where <span class="math inline">\(\mu\sim\mathcal{N}\left( \mu_{0},\sigma_{0}^{2}\right)\)</span> is the initial distribution. Using the same arguments as above, it is easy to show that <span class="math inline">\(p\left(\mu \mid y^{t+1}\right) \sim\mathcal{N}\left( \mu_{t+1},\sigma_{t+1}^{2}\right)\)</span>, where <span class="math display">\[\begin{align*}
\frac{\mu_{t+1}}{\sigma_{t+1}^{2}}  &amp;  =\left(  \frac{y_{t+1}}{\sigma^{2}}+\frac{\mu_{t}}{\sigma_{t}^{2}}\right)  =\frac{\left(  t+1\right) \overline{y}_{t+1}}{\sigma^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\text{,}\\
\frac{1}{\sigma_{t+1}^{2}}  &amp;  =\frac{1}{\sigma^{2}}+\frac{1}{\sigma_{t}^{2}}=\frac{\left(  t+1\right)  }{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\text{,}
\end{align*}\]</span> and <span class="math inline">\(\overline{y}_{t}=t^{-1}\sum_{t=1}^{t}y_{t}\)</span>.</p>
<p>Now, given this example, the same statements can be posed as in the state variable learning problem: it is hard to think of a more intuitive or alternative approach for sequential learning. In this case, researchers often have different feelings about assuming a prior distribution over the state variable and a parameter. In the state filtering problem, it is difficult to separate the prior distribution and the likelihood. In fact, one could view the intial distribution over <span class="math inline">\(x_{0}\)</span>, the linear evolution for the state variable, and the Gaussian errors as the “prior” distribution.</p>
<p>Let’s look at stylezed example, assuming the proir distribution <span class="math inline">\(\theta \sim N(-1,1)\)</span>, say we observed <span class="math inline">\(y=2\)</span> and we want to update our beliefs about <span class="math inline">\(\theta\)</span>. The likelihood function is <span class="math inline">\(p(y \mid \theta) = N(\theta,2)\)</span>, and the posterior distribution is <span class="math display">\[
p(\theta \mid y) \propto p(y \mid \theta) p(\theta) = N(\theta,2) N(-1,1) = N(\theta, 1/2).
\]</span> Graphically we can represent this as follows</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The prior distribution </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>sigma0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y,<span class="fu">dnorm</span>(y,mu0,sigma0),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">"x"</span>,<span class="at">ylab=</span><span class="st">"p(x)"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>))</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The likelihood function</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(y,<span class="fu">dnorm</span>(y,ybar,sigma),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The posterior distribution</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>mu1 <span class="ot">=</span> (mu0<span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> ybar<span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>sigma1 <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>sigma0<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>sigma<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(y,<span class="fu">dnorm</span>(y,mu1,sigma1),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">"green"</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="co"># legend</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Prior"</span>,<span class="st">" Data (Likelihood)"</span>,<span class="st">"Posterior"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-kalman" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kalman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-kalman-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kalman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Kalman Filter
</figcaption>
</figure>
</div>
</div>
</div>
<p>Note, the posterior mean is in between those of prior and likelihood and posterior variance is lower than variance of both prior and likelihood, this is effect of combining information from data and prior!</p>
<p>Consider, another example, when mean <span class="math inline">\(\mu\)</span> is fixed and variance is a random variable which follows some distribution <span class="math inline">\(\sigma^2 \sim p(\sigma^2)\)</span>. Given an observed sample <span class="math inline">\(y\)</span>, we can update the distribution over variance using the Bayes rule <span class="math display">\[
p(\sigma^2 \mid  y) = \dfrac{p(y\mid \sigma^2 )p(\sigma^2)}{p(y)}.
\]</span> Now, the total probability in the denominator can be calculated as <span class="math display">\[
p(y) = \int p(y\mid \sigma^2 )p(\sigma^2) d\sigma^2.
\]</span></p>
<p>In general, this integral cannot be calculated analytically and Bayesian inference relies on numerical approximations. However, for specific choices of the prior distribution <span class="math inline">\(p(\sigma)\)</span> and likelihood <span class="math inline">\(p(y\mid \sigma^2 )\)</span>, it is possible to analytically calculate the integral and thus, the posterior distribution.</p>
<p>A prior that leads to analytically calculable integral (conjugate prior) for normal likelihood is the inverse Gamma. Thus, if <span class="math display">\[
\sigma^2 \mid  \alpha,\beta \sim IG(\alpha,\beta) = \dfrac{\beta^{\alpha}}{\Gamma(\alpha)}\sigma^{2(-\alpha-1)}\exp\left(-\dfrac{\beta}{\sigma^2}\right)
\]</span> and <span class="math display">\[
y \mid \mu,\sigma^2 \sim N(\mu,\sigma^2)
\]</span> Then the posterior distribution is another inverse Gamma <span class="math inline">\(IG(\alpha_{\mathrm{posterior}},\beta_{\mathrm{posterior}})\)</span>, with <span class="math display">\[
\alpha_{\mathrm{posterior}} = \alpha + 1/2, ~~\beta_{\mathrm{posterior}} = \beta + \dfrac{y-\mu}{2}.
\]</span></p>
<p>Now, the predictive distribution over <span class="math inline">\(y\)</span> can be calculated by <span class="math display">\[
p(y_{new}\mid y) = \int p(y_{new},\sigma^2\mid y)p(\sigma^2\mid y)d\sigma^2.
\]</span> Which happens to be a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(2\alpha_{\mathrm{posterior}}\)</span> degrees of freedom, mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\alpha_{\mathrm{posterior}}/\beta_{\mathrm{posterior}}\)</span>.</p>
<p>In the multivriate case, the normal-normal model is <span class="math display">\[
\theta \sim N(\mu_0,\Sigma_0), \quad y \mid \theta \sim N(\theta,\Sigma).
\]</span> The posterior distribution is <span class="math display">\[
\theta \mid y \sim N(\mu_1,\Sigma_1),
\]</span> where <span class="math display">\[
\Sigma_1 = (\Sigma_0^{-1} + \Sigma^{-1})^{-1}, \quad \mu_1 = \Sigma_1(\Sigma_0^{-1}\mu_0 + \Sigma^{-1}y).
\]</span> The predictive distribution is <span class="math display">\[
y_{new} \mid y \sim N(\mu_1,\Sigma_1 + \Sigma).
\]</span></p>
<div id="exm-portfolio" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 (Satya Nadella: CEO of Microsoft)</strong></span> In 2014, Satya Nadella became the CEO of Microsoft. The stock price of Microsoft has been on a steady rise since then. Suppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.</p>
<p>Suppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&amp;P500 index and tracks overall market performance. We ars interested in estimating the mean returns of the positions in our portfolio. You believe that the returns are normally distributed and are related to each other. You have prior beliefs about these returns, which are also normally distributed. We will use what is called the empirical prior for the mean returns. This is a prior that is based on historical data. The empirical prior is a good choice when you have a lot of historical data and you believe that the future mean returns will be similar to the historical mean returns. We assume the prior for the mean returns is a bivariate normal distribution, let <span class="math inline">\(\mu_0 = (\mu_{M}, \mu_{S})\)</span> represent the prior mean returns for the stocks. The covariance matrix <span class="math inline">\(\Sigma_0\)</span> captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. We will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix. The prior covariance matrix is given by <span class="math display">\[
\Sigma_0 = \begin{bmatrix} \sigma_{M}^2 &amp; \sigma_{MS} \\ \sigma_{MS} &amp; \sigma_{S}^2 \end{bmatrix},
\]</span> where <span class="math inline">\(\sigma_{M}^2\)</span> and <span class="math inline">\(\sigma_{S}^2\)</span> are the sample variances of the historical returns of MSFT and SPY, respectively, and <span class="math inline">\(\sigma_{MS}\)</span> is the sample covariance of the historical returns of MSFT and SPY. The prior mean is given by <span class="math display">\[
\mu_0 = \begin{bmatrix} \mu_{M} \\ \mu_{S} \end{bmatrix},
\]</span> where <span class="math inline">\(\mu_{M}\)</span> and <span class="math inline">\(\mu_{S}\)</span> are the sample means of the historical returns of MSFT and SPY, respectively. The likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns <span class="math inline">\(\mu = [\mu_A, \mu_B]\)</span>. The covariance matrix <span class="math inline">\(\Sigma\)</span> of the likelihood represents the uncertainty in your data. We will use the sample mean and covariance matrix of the observed returns as the likelihood mean and covariance matrix. The likelihood covariance matrix is given by <span class="math display">\[
\Sigma = \begin{bmatrix} \sigma_{M}^2 &amp; \sigma_{MS} \\ \sigma_{MS} &amp; \sigma_{S}^2 \end{bmatrix},
\]</span> where <span class="math inline">\(\sigma_{M}^2\)</span> and <span class="math inline">\(\sigma_{S}^2\)</span> are the sample variances of the observed returns of MSFT and SPY, respectively, and <span class="math inline">\(\sigma_{MS}\)</span> is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by <span class="math display">\[
\mu = \begin{bmatrix} \mu_{M} \\ \mu_{S} \end{bmatrix},
\]</span> where <span class="math inline">\(\mu_{M}\)</span> and <span class="math inline">\(\mu_{S}\)</span> are the sample means of the observed returns of MSFT and SPY, respectively. In a Bayesian framework, you update your beliefs (prior) about the mean returns using the observed data (likelihood). The posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution. The mean <span class="math inline">\(\mu_{\text{post}}\)</span> and covariance <span class="math inline">\(\Sigma_{\text{post}}\)</span> of the posterior are calculated using Bayesian updating formulas, which involve <span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\Sigma_0\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\Sigma\)</span>.</p>
<p>We use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014. Thus, our observed data includes July 2015 - Dec 2023 period. We assume the likelihood of observing this data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns. The covariance matrix <span class="math inline">\(Sigma\)</span> of the likelihood represents the uncertainty in your data and is calculated from the overall observed returns data 2001-2023.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">getSymbols</span>(<span class="fu">c</span>(<span class="st">"MSFT"</span>, <span class="st">"SPY"</span>), <span class="at">from =</span> <span class="st">"2001-01-01"</span>, <span class="at">to =</span> <span class="st">"2023-12-31"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "MSFT" "SPY" </code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>s <span class="ot">=</span> <span class="dv">3666</span> <span class="co"># 2015-07-30</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span>s</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">=</span> s<span class="sc">:</span><span class="fu">nrow</span>(MSFT) <span class="co"># post covid</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">dailyReturn</span>(MSFT))</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">dailyReturn</span>(SPY))</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">mean</span>(a[prior]), <span class="fu">mean</span>(c[prior]))</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>Sigma0 <span class="ot">=</span> <span class="fu">cov</span>(<span class="fu">data.frame</span>(<span class="at">a=</span>a[prior],<span class="at">c=</span>c[prior]))</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">mean</span>(a[obs]), <span class="fu">mean</span>(c[obs]))</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> <span class="fu">cov</span>(<span class="fu">data.frame</span>(<span class="at">a=</span>a,<span class="at">c=</span>c))</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>SigmaPost <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">solve</span>(Sigma0) <span class="sc">+</span> <span class="fu">solve</span>(Sigma))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>muPost <span class="ot">=</span> SigmaPost <span class="sc">%*%</span> (<span class="fu">solve</span>(Sigma0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> <span class="fu">solve</span>(Sigma) <span class="sc">%*%</span> mu)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(a[obs], c[obs], <span class="at">xlab=</span><span class="st">"MSFT"</span>, <span class="at">ylab=</span><span class="st">"SPY"</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.005</span>,<span class="fl">0.005</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.005</span>,<span class="fl">0.005</span>), <span class="at">pch=</span><span class="dv">16</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>, <span class="at">h=</span><span class="dv">0</span>, <span class="at">col=</span><span class="st">"grey"</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mu0[<span class="dv">1</span>], <span class="at">h=</span>mu0[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"blue"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#prior</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>mu[<span class="dv">1</span>], <span class="at">h=</span>mu[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"red"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#data</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span>muPost[<span class="dv">1</span>], <span class="at">h=</span>muPost[<span class="dv">2</span>], <span class="at">col=</span><span class="st">"green"</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#posterior</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"Prior"</span>, <span class="st">"Likelihood"</span>, <span class="st">"Posterior"</span>), <span class="at">pch=</span><span class="dv">15</span>, <span class="at">col=</span><span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">bty=</span><span class="st">"n"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-portfolio" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-portfolio-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-portfolio-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Bayesian Portfolio Updating
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see tha posterior mean for SPY is close to the prior mean, while the posterior mean for MSFT is further away. The perofrmance of MSFT was significantly better past 2015 compared to SPY. The posterior mean (green) represents mean reversion value. We can think of it a expected mean return if the perofrmance of MSFT starts reverting to its historical averages.</p>
<p>This model is particularly powerful because it can be extended to more dimensions (more stocks) and can include more complex relationships between the variables. It’s often used in finance, econometrics, and other fields where understanding the joint behavior of multiple normally-distributed variables is important.</p>
</div>
</section>
<section id="exponential-gamma-model" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="exponential-gamma-model"><span class="header-section-number">5.10</span> Exponential-Gamma Model</h2>
<p>Exponential distribution is a continuous distribution that is often used to model waiting times between events. For example, the time between two consecutive arrivals of a Poisson process is exponentially distributed. If the number of events in 1 unit of time has the Poisson distribution with rate parameter <span class="math inline">\(\lambda\)</span>, then the time between events has the exponential distribution with mean <span class="math inline">\(1/\lambda\)</span>. The probability density function (PDF) of an exponential distribution is defined as: <span class="math display">\[
f(x;\lambda) =  \lambda e^{-\lambda x}, ~ x \geq 0
\]</span> The exponential distribution is defined for <span class="math inline">\(x \geq 0\)</span>, abd <span class="math inline">\(\lambda\)</span> is the rate parameter, which is the inverse of the mean (or expected value) of the distribution. It must be greater than 0. The exponential distribution is a special case of the Gamma distribution with shape 1 and scale <span class="math inline">\(1/\lambda\)</span>.</p>
<p>The mean and variance are give in terms of the rate parameter <span class="math inline">\(\lambda\)</span> as</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Exponential Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = 1/\lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = 1/\lambda^2\)</span></td>
</tr>
</tbody>
</table>
<p>Here are some examples of when exponential model provides a good fit</p>
<ul>
<li>Lifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.</li>
<li>Time Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.</li>
<li>Radioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution.</li>
</ul>
<p>In these examples, the key assumption is that events happen independently and at a constant average rate, which makes the exponential distribution a suitable model.</p>
<p>The Exponential-Gamma model, often used in Bayesian statistics, is a hierarchical model where the exponential distribution’s parameter is itself modeled as following a gamma distribution. This approach is particularly useful in situations where there is uncertainty or variability in the rate parameter of the exponential distribution.</p>
<p>The <em>Exponential-Gamma</em> model assumes that the data follows an exponential distribution (likelihood). As mentioned earlier, the exponential distribution is suitable for modeling the time between events in processes where these events occur independently and at a constant rate. At the next level, the rate parameter <span class="math inline">\(\lambda\)</span> of the exponential distribution is assumed to follow a gamma distribution. The gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. <span class="math display">\[\begin{align*}
    \lambda &amp;\sim \text{Gamma}(\alpha, \beta) \\
    x_i &amp;\sim \text{Exponential}(\lambda)
\end{align*}\]</span></p>
<p>The probability density function of the gamma distribution is given by <a href="#eq-gamma-pdf" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> and has two parameters, shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>. The posterior distribution of the rate parameter <span class="math inline">\(\lambda\)</span> is given by: <span class="math display">\[
p(\lambda\mid x_1, \ldots, x_n) \propto \lambda^{\alpha - 1} e^{-\beta\lambda} \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^{\alpha + n - 1} e^{-(\beta + \sum_{i=1}^n x_i)\lambda}
\]</span> which is a gamma distribution with shape parameter <span class="math inline">\(\alpha + n\)</span> and rate parameter <span class="math inline">\(\beta + \sum_{i=1}^n x_i\)</span>. The posterior mean and variance are given by: <span class="math display">\[
\mathbb{E}[\lambda|x_1, \ldots, x_n] = \frac{\alpha + n}{\beta + \sum_{i=1}^n x_i}, \quad \mathrm{Var}[\lambda|x_1, \ldots, x_n] = \frac{\alpha + n}{(\beta + \sum_{i=1}^n x_i)^2}.
\]</span> Notice, that <span class="math inline">\(\sum x_i\)</span> is the sufficient statistic for inference about parameter <span class="math inline">\(\lambda\)</span>!</p>
<p>Some applicaitons of this model include the following:</p>
<ul>
<li>Reliability Engineering: In situations where the failure rate of components or systems may not be constant and can vary, the Exponential-Gamma model can be used to estimate the time until failure, incorporating uncertainty in the failure rate.</li>
<li>Medical Research: For modeling survival times of patients where the rate of mortality or disease progression is not constant and varies across a population. The variability in rates can be due to different factors like age, genetics, or environmental influences.</li>
<li>Ecology: In studying phenomena like the time between rare environmental events (e.g., extreme weather events), where the frequency of occurrence can vary due to changing climate conditions or other factors.</li>
</ul>
<p>In these applications, the Exponential-Gamma model offers a more nuanced approach than using a simple exponential model, as it accounts for the variability in the rate parameter.</p>
</section>
<section id="exploratory-data-analysis" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">5.11</span> Exploratory Data Analysis</h2>
<p>Before decideng on a parametric model for a dataset. There are several tools that we use to choose the appropriate model. These include</p>
<ol type="1">
<li>Theoretical assumptions underlying the distribution (our prior knowledge about the data)</li>
<li>Exploratory data analysis</li>
<li>Formal goodness-of-fit tests</li>
</ol>
<p>The two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms.</p>
<p>A Q-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). Quantile is the fraction (or percent) of points below the given value. That is, the <span class="math inline">\(i\)</span>-th quantile is the point <span class="math inline">\(x\)</span> for which <span class="math inline">\(i\)</span>% of the data lies below <span class="math inline">\(x\)</span>. On a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two data sets <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> come from the same distribution, then the points <span class="math inline">\((x_{(i)}, y_{(i)})\)</span> should lie roughly on the line <span class="math inline">\(y = x\)</span>. If <span class="math inline">\(y\)</span> comes from a distribution that’s linear in <span class="math inline">\(x\)</span>, then the points <span class="math inline">\((x_{(i)}, y_{(i)})\)</span> should lie roughly on a line, but not necessarily on the line <span class="math inline">\(y = x\)</span>.</p>
<div id="exm-qqplot" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 (Noraml Q-Q plot)</strong></span> <a href="#fig-qqplot" class="quarto-xref">Figure&nbsp;<span>5.4</span></a> shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in <a href="https://rdrr.io/cran/UsingR/man/babyboom.html"><code>UsingR manual</code></a>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(UsingR)  </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(babyboom<span class="sc">$</span>wt)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(babyboom<span class="sc">$</span>wt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-qqplot" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="bl_files/figure-html/fig-qqplot-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Normal Q-Q plot of baby weights
</figcaption>
</figure>
</div>
</div>
</div>
<p>Visually, the answer to answer the question “Are Birth Weights Normally Distributed?” is no. We can see that on the lefrt side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.</p>
<p>The Q-Q plots look different if we split the data based on the gender</p>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> babyboom <span class="sc">%&gt;%</span> <span class="fu">filter</span>(gender<span class="sc">==</span><span class="st">"girl"</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(wt) </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> babyboom <span class="sc">%&gt;%</span> <span class="fu">filter</span>(gender<span class="sc">==</span><span class="st">"boy"</span>)  <span class="sc">%&gt;%</span> <span class="fu">pull</span>(wt) </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(g); <span class="fu">qqline</span>(g)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(b); <span class="fu">qqline</span>(b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Girls</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-18-2.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Boys</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Histogram of baby weights by gender</p>
</div>
</div>
</div>
<p>How about the times in hours between births of babies?</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>hr <span class="ot">=</span> <span class="fu">ceiling</span>(babyboom<span class="sc">$</span>running.time<span class="sc">/</span><span class="dv">60</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>BirthsByHour <span class="ot">=</span> <span class="fu">tabulate</span>(hr)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of hours with 0, 1, 2, 3, 4 births</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>ObservedCounts <span class="ot">=</span> <span class="fu">table</span>(BirthsByHour) </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Average number of births per hour</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>BirthRate<span class="ot">=</span><span class="fu">sum</span>(BirthsByHour)<span class="sc">/</span><span class="dv">24</span>    </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected counts for Poisson distribution</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>ExpectedCounts<span class="ot">=</span><span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>,BirthRate)<span class="sc">*</span><span class="dv">24</span>    </span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># bind into matrix for plotting</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>ObsExp <span class="ot">&lt;-</span> <span class="fu">rbind</span>(ObservedCounts,ExpectedCounts) </span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(ObsExp,<span class="at">names=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">beside=</span><span class="cn">TRUE</span>,<span class="at">legend=</span><span class="fu">c</span>(<span class="st">"Observed"</span>,<span class="st">"Expected"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>What about the Q-Q plot?</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># birth intervals</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>birthinterval<span class="ot">=</span><span class="fu">diff</span>(babyboom<span class="sc">$</span>running.time) </span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a> <span class="co"># quantiles of standard exponential distribution (rate=1)   </span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>exponential.quantiles <span class="ot">=</span> <span class="fu">qexp</span>(<span class="fu">ppoints</span>(<span class="dv">43</span>)) </span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqplot</span>(exponential.quantiles, birthinterval)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>lmb<span class="ot">=</span><span class="fu">mean</span>(birthinterval)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(exponential.quantiles,exponential.quantiles<span class="sc">*</span>lmb) <span class="co"># Overlay a line</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="bl_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Here</p>
<ul>
<li><code>ppoints</code> function computes the sequence of probability points</li>
<li><code>qexp</code> function computes the quantiles of the exponential distribution</li>
<li><code>diff</code> function computes the difference between consecutive elements of a vector</li>
</ul>
</div>
</section>
<section id="summary-of-conjugate-priors-for-common-likelihoods" class="level2" data-number="5.12">
<h2 data-number="5.12" class="anchored" data-anchor-id="summary-of-conjugate-priors-for-common-likelihoods"><span class="header-section-number">5.12</span> Summary of Conjugate Priors for Common Likelihoods</h2>
<p>Summary table of random varaibles</p>
<div id="tbl-rv" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.1: Summary table of commonly used random variables
</figcaption>
<div aria-describedby="tbl-rv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 5%">
<col style="width: 8%">
<col style="width: 31%">
<col style="width: 12%">
<col style="width: 28%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Parameters</th>
<th>PDF</th>
<th>Mean</th>
<th>Variance</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal</td>
<td><span class="math inline">\(\mu, \sigma^2\)</span></td>
<td><span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(x \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td>Exponential</td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda e^{-\lambda x}\)</span></td>
<td><span class="math inline">\(\frac{1}{\lambda}\)</span></td>
<td><span class="math inline">\(\frac{1}{\lambda^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\frac{e^{-\lambda}\lambda^x}{x!}\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(x \in \mathbb{N}\)</span></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(n, p\)</span></td>
<td><span class="math inline">\(\binom{n}{x}p^x(1-p)^{n-x}\)</span></td>
<td><span class="math inline">\(np\)</span></td>
<td><span class="math inline">\(np(1-p)\)</span></td>
<td><span class="math inline">\(x \in \{0, 1, \ldots, n\}\)</span></td>
</tr>
<tr class="even">
<td>Bernoulli</td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(p^x(1-p)^{1-x}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\(p(1-p)\)</span></td>
<td><span class="math inline">\(x \in \{0, 1\}\)</span></td>
</tr>
<tr class="odd">
<td>Multinomial</td>
<td><span class="math inline">\(n, \boldsymbol{p}\)</span></td>
<td><span class="math inline">\(\frac{n!}{x_1!x_2!\cdots x_k!}p_1^{x_1}p_2^{x_2}\cdots p_k^{x_k}\)</span></td>
<td><span class="math inline">\(np_i\)</span></td>
<td><span class="math inline">\(np_i(1-p_i)\)</span></td>
<td><span class="math inline">\(\sum x_i = n, x_i \in \mathbb{R}^+\)</span></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\)</span></td>
<td><span class="math inline">\(x \in [0, 1]\)</span></td>
</tr>
<tr class="odd">
<td>Dirichlet</td>
<td><span class="math inline">\(\boldsymbol{\alpha}\)</span></td>
<td><span class="math inline">\(\frac{\Gamma(\sum \alpha_i)}{\prod \Gamma(\alpha_i)}\prod x_i^{\alpha_i-1}\)</span></td>
<td><span class="math inline">\(\frac{\alpha_i}{\sum \alpha_i}\)</span></td>
<td><span class="math inline">\(\frac{\alpha_i(\sum \alpha_i - \alpha_i)}{\sum \alpha_i^2(\sum \alpha_i + 1)}\)</span></td>
<td><span class="math inline">\(\sum x_i = 1, x_i \in [0, 1]\)</span></td>
</tr>
<tr class="even">
<td>Inverse Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{-\alpha-1}e^{-\frac{\beta}{x}}\)</span></td>
<td><span class="math inline">\(\frac{\beta}{\alpha-1}\)</span></td>
<td><span class="math inline">\(\frac{\beta^2}{(\alpha-1)^2(\alpha-2)}\)</span></td>
<td><span class="math inline">\(x &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta}\)</span></td>
<td><span class="math inline">\(\frac{\alpha}{\beta^2}\)</span></td>
<td><span class="math inline">\(x \geq 0\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-conjugate" class="quarto-xref">Table&nbsp;<span>5.2</span></a> summarizes the conjugate prior distributions for common likelihoods. Thus far, we’ve conidered the Normal-Normal model with both known and unknown variance as well as Poisson-Gamma and Beta Binomial. The other pairs are left as an exercise.</p>
<div id="tbl-conjugate" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-conjugate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.2: Conjugate prior table for common likelihoods
</figcaption>
<div aria-describedby="tbl-conjugate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Likelihood Distribution</th>
<th>Conjugate Prior Distribution</th>
<th>Prior Parameters</th>
<th>Likelihood Parameters</th>
<th>Posterior Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal (known <span class="math inline">\(\sigma^2\)</span>)</td>
<td>Normal</td>
<td><span class="math inline">\(\mu_0, \sigma^2_0\)</span></td>
<td><span class="math inline">\(\mu, \sigma^2\)</span></td>
<td><span class="math inline">\(\frac{\sigma^2_0 \cdot \mu + \sigma^2 \cdot \mu_0}{\sigma^2 + \sigma^2_0}, \frac{1}{\frac{1}{\sigma^2_0} + \frac{1}{\sigma^2}}\)</span></td>
</tr>
<tr class="even">
<td>Normal (unknown <span class="math inline">\(\sigma^2\)</span>)</td>
<td>Normal-Inverse Gamma</td>
<td><span class="math inline">\(\mu_0, \lambda_0, \alpha_0, \beta_0\)</span></td>
<td><span class="math inline">\(\mu, \sigma^2\)</span></td>
<td><span class="math inline">\(\mu_{n}, \lambda_{n}, \alpha_{n}, \beta_{n}\)</span></td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td>Beta</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(n, x\)</span></td>
<td><span class="math inline">\(\alpha + x, \beta + n - x\)</span></td>
</tr>
<tr class="even">
<td>Poisson</td>
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\alpha + \sum x_i, \beta + n\)</span></td>
</tr>
<tr class="odd">
<td>Exponential</td>
<td>Gamma</td>
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\alpha + n, \beta + \sum x_i\)</span></td>
</tr>
<tr class="even">
<td>Multinomial</td>
<td>Dirichlet</td>
<td><span class="math inline">\(\boldsymbol{\alpha}\)</span></td>
<td><span class="math inline">\(\boldsymbol{x}\)</span></td>
<td><span class="math inline">\(\boldsymbol{\alpha} + \boldsymbol{x}\)</span></td>
</tr>
<tr class="odd">
<td>Normal (unknown mean, unknown variance)</td>
<td>Normal-Inverse-Wishart</td>
<td><span class="math inline">\(\mu_0, \kappa_0, \nu_0, \Psi_0\)</span></td>
<td><span class="math inline">\(\mu, \Sigma\)</span></td>
<td>Updated <span class="math inline">\(\mu_{n}, \kappa_{n}, \nu_{n}, \Psi_{n}\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<ul>
<li><strong>Normal (known <span class="math inline">\(\sigma^2\)</span>)</strong>: The posterior mean is a weighted average of the prior mean and the sample mean, with weights inversely proportional to their variances. The posterior variance is the harmonic mean of the prior and sample variances.</li>
<li><strong>Normal-Inverse Gamma</strong>: This is a more complex case for normal data with unknown variance. The parameters <span class="math inline">\(\mu_{n}, \lambda_{n}, \alpha_{n}, \beta_{n}\)</span> are updated based on the data.</li>
<li><strong>Binomial and Beta</strong>: The Beta distribution is the conjugate prior for the Binomial likelihood. The parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are updated by adding the number of successes and failures, respectively.</li>
<li><strong>Poisson and Gamma</strong>: The Gamma distribution is the conjugate prior for the Poisson likelihood. The shape parameter <span class="math inline">\(\alpha\)</span> is increased by the sum of the data, and the rate parameter <span class="math inline">\(\beta\)</span> is increased by the number of observations.</li>
<li><strong>Exponential and Gamma</strong>: Similar to Poisson, but the rate parameter is updated differently.</li>
<li><strong>Multinomial and Dirichlet</strong>: The Dirichlet distribution is the conjugate prior for the Multinomial likelihood, with each count in the data vector <span class="math inline">\(\boldsymbol{x}\)</span> updating the corresponding parameter in the vector <span class="math inline">\(\boldsymbol{\alpha}\)</span>.</li>
<li><strong>Normal (unknown mean, unknown variance) and Normal-Inverse-Wishart</strong>: This is a more general case for multivariate normal data with unknown mean and covariance matrix. The parameters are updated based on the multivariate data.</li>
</ul>
<p>These conjugate relationships simplify Bayesian calculations by ensuring that the posterior distributions are in the same family as the priors.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-kolmogorov1942" class="csl-entry" role="listitem">
Kolmogorov, AN. 1942. <span>“Definition of Center of Dispersion and Measure of Accuracy from a Finite Number of Observations (in <span>Russian</span>).”</span> <em>Izv. Akad. Nauk SSSR Ser. Mat.</em> 6: 3–32.
</div>
<div id="ref-nareklishvili2022" class="csl-entry" role="listitem">
Nareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022. <span>“Deep Partial Least Squares for Iv Regression.”</span> <em>arXiv Preprint arXiv:2207.02612</em>. <a href="https://arxiv.org/abs/2207.02612">https://arxiv.org/abs/2207.02612</a>.
</div>
<div id="ref-polson2021" class="csl-entry" role="listitem">
Polson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. <span>“Deep <span>Learning Partial Least Squares</span>.”</span> <em>arXiv Preprint arXiv:2106.14085</em>. <a href="https://arxiv.org/abs/2106.14085">https://arxiv.org/abs/2106.14085</a>.
</div>
<div id="ref-shen2021a" class="csl-entry" role="listitem">
Shen, Changyu, Enrico G Ferro, Huiping Xu, Daniel B Kramer, Rushad Patell, and Dhruv S Kazi. 2021. <span>“Underperformance of Contemporary Phase <span>III</span> Oncology Trials and Strategies for Improvement.”</span> <em>Journal of the National Comprehensive Cancer Network</em> 19 (9): 1072–78.
</div>
<div id="ref-spiegelhalter2009" class="csl-entry" role="listitem">
Spiegelhalter, David, and Yin-Lam Ng. 2009. <span>“One Match to Go!”</span> <em>Significance</em> 6 (4): 151–53.
</div>
<div id="ref-sun2022" class="csl-entry" role="listitem">
Sun, Duxin, Wei Gao, Hongxiang Hu, and Simon Zhou. 2022. <span>“Why 90% of Clinical Drug Development Fails and How to Improve It?”</span> <em>Acta Pharmaceutica Sinica B</em> 12 (7): 3049–62.
</div>
<div id="ref-taleb2007" class="csl-entry" role="listitem">
Taleb, Nassim Nicholas. 2007. <em>The <span>Black Swan</span>: <span>The Impact</span> of the <span>Highly Improbable</span></em>. Annotated edition. <span>New York. N.Y</span>: <span>Random House</span>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../qmd/dec.html" class="pagination-link  aria-label=" &lt;span="" and="" decisions&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility and Decisions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../qmd/ab.html" class="pagination-link" aria-label="<span class='chapter-number'>6</span>&nbsp; <span class='chapter-title'>AB Testing</span>">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AB Testing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>