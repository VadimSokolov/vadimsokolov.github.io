<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayes, AI and Deep Learning - 9&nbsp; Stochastic Processes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../qmd/rl.html" rel="next">
<link href="../qmd/rct.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating fullcontent"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[1]{\operatorname{E}\left(#1\right)}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\]</span></p>
</div>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="../qmd/sp.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Principles of Data Science</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Parameter Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Field vs Observational</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/sp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../qmd/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../qmd/prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="../qmd/sp.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Yet another fundamental concept that is useful for probabilistic reasoning is stochastic process. An instance of a process is a function <span class="math inline">\(Y:~ U \rightarrow S\)</span> from domain of index set <span class="math inline">\(U\)</span> into another set of process values <span class="math inline">\(S\)</span>, called state-space. The state-space of a stochastic process is the set of all possible states that the process can be in. Each state in the state-space represents a possible outcome or condition of the system being modeled. The process then is the <em>distribution over the space of functions</em> from <span class="math inline">\(U\)</span> to <span class="math inline">\(S\)</span>. The term process is used because the function <span class="math inline">\(Y\)</span> is often thought of as a time-varying quantity, and the index set <span class="math inline">\(U\)</span> is often interpreted as time. However, the index set can be any set, and the process can be a random function of any other variable. Both index set and state-space can be discrete or continuous. For example, discrete time index can represent days or rounds and continuous time index is a point on a time line. The state-space can be discrete (composed of distinct states, like the number of customers in a store) or continuous (such as the price of a stock). The state-space can be one-dimensional (only one aspect of the system is modeled) or multi-dimensional (multiple aspects are modeled simultaneously).</p>
<p>Here are some widely used stochastic processes:</p>
<ol type="1">
<li>Random Walk: A simple example where the next state depends on the current state and some random movement. In finance, stock prices are often modeled as a type of random walk.</li>
<li>Markov Chains: A process where the next state depends only on the current state and not on the path taken to reach the current state.</li>
<li>Poisson Process: Used to model the number of times an event occurs in a fixed interval of time or space, where events occur with a known constant mean rate and independently of the time since the last event.</li>
<li>Queuing Theory: Models used in operations research where the stochastic process represents the number of customers in a queue, varying over time as customers arrive and are served.</li>
<li>Brownian Motion: This process models the random movement of particles suspended in a fluid. It has applications in physics, finance (to model stock market prices), and biology.</li>
<li>Gaussian Processes: These are a collection of random variables, any finite number of which have a joint Gaussian distribution. They are used in machine learning for regression and classification tasks.</li>
</ol>
<p>In contexts like agricultural field trials, the domain for analyzing yield is commonly referred to as the collection of plots. This term is broadly suitable for practical field purposes but is mathematically interpreted as the collection of planar Borel subsets across various growing seasons. In a basic clinical trial for a COVID-19 vaccine, like the AstraZeneca trial in 2021, the domain is typically referred to as the group of patients. This implies the inclusion of all eligible patients, regardless of whether they were actually recruited and observed in the trial. In research on speciation or sexual compatibility in fruit flies, the domain is defined as the set of male-female pairs, encompassing all potential pairs with the desired genetic traits. For a competition experiment, such as a chess or tennis tournament, the domain is described as the set of ordered pairs of participants, which includes all possible pairings, not just those who actually competed against each other at events like US Open in 2024.</p>
<p>In data analysis, both experimental and observational data can exhibit variability. This variability is often modeled using probability distributions. These distributions can either represent simple processes with independent elements (then we are back to i.i.d case) or more complex stochastic processes that display dependencies, whether they be serial, spatial, or of other types. Essentially, this modeling approach helps in understanding and predicting data behavior under various conditions. The early sections of <span class="citation" data-cites="davison2003statistical">Davison (<a href="references.html#ref-davison2003statistical" role="doc-biblioref">2003</a>)</span> work offer an insightful primer on how to develop and apply these stochastic models across various fields. This introduction is particularly useful for grasping the fundamental concepts and practical applications of these models.</p>
<section id="brownian-motion" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="brownian-motion"><span class="header-section-number">9.1</span> Brownian Motion</h2>
<p>Brownian Motion, named after botanist Robert Brown, is a fundamental concept in the theory of stochastic processes. It describes the random motion of particles suspended in a fluid (liquid or gas), as they are bombarded by the fast-moving molecules in the fluid.</p>
<p>A one-dimensional Brownian Motion (also known as Wiener process) is a continuous time stochastic process <span class="math inline">\(B(t)_{t\ge 0}\)</span> with the following properties</p>
<ul>
<li><span class="math inline">\(B(0) = 0\)</span> almost surely</li>
<li><span class="math inline">\(B(t)\)</span> has stationary independent increments: <span class="math inline">\(B(t) - B(s) \sim N(0, t-s)\)</span> for <span class="math inline">\(0 \le s &lt; t\)</span></li>
<li><span class="math inline">\(B(t)\)</span> is continuous function of <span class="math inline">\(t\)</span></li>
<li>For each time <span class="math inline">\(t &gt; 0\)</span>, the random variable <span class="math inline">\(B(t)\)</span> is normally distributed with mean 0 and variance <span class="math inline">\(t\)</span>, i.e., <span class="math inline">\(B(t) \sim N(0, t)\)</span>.</li>
</ul>
<p>Formally brownian motion is a stochastic process <span class="math inline">\(B(t)\)</span> is a family of real random variables indexed by the set of nonnegative real numbers <span class="math inline">\(t\)</span>.</p>
<p><a href="#fig-brownian" class="quarto-xref">Figure&nbsp;<span>9.1</span></a> below shows tree sample paths of Brownian Motion.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Brownian Motion</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">92</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>t <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">xlab=</span><span class="st">"t"</span>, <span class="at">ylab=</span><span class="st">"B(t)"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.2</span>, <span class="dv">2</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))), <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(<span class="dv">1001</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.001</span>))),<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-brownian" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-brownian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-brownian-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-brownian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: Brownian Motion
</figcaption>
</figure>
</div>
</div>
</div>
<p>Thus, for any times <span class="math inline">\(0 \leq t_1 &lt; t_2 &lt; ... &lt; t_n\)</span>, the random variables <span class="math inline">\(B(t_2) - B(t_1)\)</span>, <span class="math inline">\(B(t_3) - B(t_2)\)</span>, …, <span class="math inline">\(B(t_n) - B(t_{n-1})\)</span> are independent and the function <span class="math inline">\(t \mapsto B(t)\)</span> is continuous almost surely.</p>
<p>Some properties of Brownian Motion are:</p>
<ul>
<li>Scale Invariance: If <span class="math inline">\(B(t)\)</span> is a Brownian motion, then for any <span class="math inline">\(a &gt; 0\)</span>, the process <span class="math inline">\(aB(t/a^2)\)</span> is also a Brownian motion.</li>
<li>Time Inversion: If <span class="math inline">\(B(t)\)</span> is a Brownian motion, then <span class="math inline">\(tB(1/t)\)</span> is also a Brownian motion for <span class="math inline">\(t &gt; 0\)</span>.</li>
<li>Fractal Nature: Brownian motion paths are nowhere differentiable but continuous everywhere, reflecting a fractal-like nature.</li>
</ul>
<p>Historically, the most widely used models for stock market returns relied on the assumption that asset returns follow a normal or a lognormal distribution. The lognormal model for the asset returns was challenged after the October 1987 crash of the American stock market. On October 19 (black Monday) the Dow Jones index had fallen 508 points, or 23 percent. It was the worst single day in history for the US markets. The reason for the crash was rather simple, it was caused by the portfolio insurance product created by one of the financial firms. The idea of this insurance was to switch from equities to the US Treasury bills, as markets go down. Although the lognormal model does a good job at describing the historical data, the jump observed on that day had a probability close to zero, according to the lognormal model. The lognormal model underestimates the probability of a large change (thin tail). The widely used then Black-Sholes model for asset pricing was relying on the lognormal model, it was incapable of correctly pricing in the possibility of such a large drop.</p>
<p>The normal assumption of the asset returns was first proposed in 1900 in the PhD thesis of Louis Bachelier, who was a student of Henri Poincare. Bachelier was interested in developing statistical tools for pricing options (predicting asset returns) on the Paris stock exchange. Although Bachelier’s work laid the foundation for the modern theory of stochastic processes, he was never given credit by his contemporaries, including Einstein, Lévi and Borel. In 1905 Einstein published a paper which used the same statistical model as Bachelier to describe the 1827 discovery by a botanist Robert Brown, who observed that pollen particles suspended in water followed irregular random trajectories. Thus, we call the stochastic process that describes these phenomena a Brownian motion. Einstein’s advisor at the University of Zurich was Hermann Minkowski who was a friend and a collaborator of Poincare. Thus, it is likely Einstein knew about the work of Bachelier, but he never mentioned it in his paper. This was not the first instance of when Einstein did not give proper credit. Poincare published a paper <span class="citation" data-cites="poincare1898mesure">Poincaré (<a href="references.html#ref-poincare1898mesure" role="doc-biblioref">1898</a>)</span> on the relativity theory in 1898, seven years before Einstein. This paper was published in a philosophy journal and thus Poincare had avoided using any mathematical formulas except for the famous <span class="math inline">\(E=mc^2\)</span>. Poincare did discussed his results on the relativity theory with Minkowski. Minkowski asked Einstein to read Poincare’s work <span class="citation" data-cites="arnol2006forgotten">Arnol’d (<a href="references.html#ref-arnol2006forgotten" role="doc-biblioref">2006</a>)</span>. However, Einstein never referenced the work of Poincare until 1945. One of the reviewers for the 1905 paper on relativity by Einstein was Poincare and he wrote a very positive review mentioning it as a breakthrough. When Minkowski asked Poincare why he did not claim his priority on the theory, Poincare replied that our mission is to support young scientists. More about why credit is mistakenly given to Einstein for the relativity theory is discussed by Logunov <span class="citation" data-cites="logunov2004henri">Logunov (<a href="references.html#ref-logunov2004henri" role="doc-biblioref">2004</a>)</span>.</p>
<p>Einstein was not the only one who ignored the work of Bachelier, Paul Lévi did so as well. Paul Lévi was considered a pioneer and authority on stochastic processes during Bachelier’s time, although Bruno de Finetti introduced a dual concept of infinite divisibility in 1929, before the works of Lévi in early 1930s on this topic. Lévi never mentioned the work of the obscure and little known mathematician Bachelier. The first to give credit to Bachelier was Kolmogorov in his 1931 paper <span class="citation" data-cites="kolmogoroff1931analytischen">Kolmogoroff (<a href="references.html#ref-kolmogoroff1931analytischen" role="doc-biblioref">1931</a>)</span> (Russian translation <span class="citation" data-cites="Kol38">Kolmogorov (<a href="references.html#ref-Kol38" role="doc-biblioref">1938</a>)</span> and English translation <span class="citation" data-cites="Shiryayev1992">Shiryayev (<a href="references.html#ref-Shiryayev1992" role="doc-biblioref">1992</a>)</span>). Later Leonard Jimmie Savage translated Bachelier’s work to English and showed it to Paul Samuelson. Samuelson extended the work of Bachelier by considering the log-returns rather than absolute numbers, popularized the work of Bachelier among economists and the translation of Bachelier’s thesis was finally published in English in 1964 <span class="citation" data-cites="cootner1967random">Cootner (<a href="references.html#ref-cootner1967random" role="doc-biblioref">1967</a>)</span>. Many economists who extended the work of Bachelier won Nobel prizes, including Eugine Fama known for work on the efficient markets hypothesis, Paul Samuelson, and Myron Scholes for the Black-Sholes model, as well as Robert Merton.</p>
<p>Robert Merton, who was a student of Samuelson, who proposed a major extension to the work of Bachelier, by introducing jumps to the model. The additive jump term addresses the issues, asymmetry, and heavy tails in the distribution. Merton’s Jump Stochastic volatility model has a discrete-time version for log-returns, <span class="math inline">\(y_t\)</span>, with jump times, <span class="math inline">\(J_t\)</span>, jump sizes, <span class="math inline">\(Z_t\)</span>, and spot stochastic volatility, <span class="math inline">\(V_t\)</span>, given by the dynamics <span class="math display">\[\begin{align*}
    y_{t} &amp; \equiv \log \left( S_{t}/S_{t-1}\right) =\mu + V_t \varepsilon_{t}+J_{t}Z_{t} \\V_{t+1} &amp; = \alpha_v + \beta_v V_t + \sigma_v \sqrt{V_t} \varepsilon_{t}^v
\end{align*}\]</span> where <span class="math inline">\(\mathbb{P} \left ( J_t =1 \right ) = \lambda\)</span>, <span class="math inline">\(S_t\)</span> denote a stock or asset price and log-returns <span class="math inline">\(y^t\)</span> is the log-return. The errors <span class="math inline">\((\varepsilon_{t},\varepsilon_{t}^v)\)</span> are possibly correlated bivariate normals. The investor must obtain optimal filters for <span class="math inline">\((V_t,J_t,Z_t)\)</span>, and learn the posterior densities of the parameters <span class="math inline">\((\mu, \alpha_v, \beta_v, \sigma_v^2 , \lambda )\)</span>. These estimates will be conditional on the information available at each time.</p>
<p>Although it was originally developed to model the financial markets by Louis Bachelier in 1900, the Brownian Motion has found applications in many other fields, biology (movement of biomolecules within cells), environmental science (diffusion processes, like the spread of pollutants in the air or water), and mathematics (stochastic calculus and differential equations).</p>
<div id="exm-brownian" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.1 (Brownian Motion for Sport Scores)</strong></span> A Model for Sports Scores</p>
<p>In order to define the implied volatility of a sports game we begin with<br>
a distributional model for the evolution of the outcome in a sports game which we develop from Stern (1994). The model specifies the distribution of the lead of team A over team B, <span class="math inline">\(X(t)\)</span> for any <span class="math inline">\(t\)</span> as a Brownian motion process. If <span class="math inline">\(B(t)\)</span> denotes a standard Brownian motion with distributional property <span class="math inline">\(B(t) \sim N(0,t)\)</span> and we incorporate drift, <span class="math inline">\(\mu\)</span>, and volatility, <span class="math inline">\(\sigma\)</span>, terms, then the evolution of the outcome <span class="math inline">\(X(t)\)</span> that is given by: <span class="math display">\[
X(t)=\mu t + \sigma B(t) \sim N( \mu t , \sigma^2 t).
\]</span> This distribution of the game outcome is similar to the Black-Scholes model of the distribution of a stock price.</p>
<p>This specification results in several useful measures (or, this specification results in closed-form solutions for a number of measures of interest). The distribution of the final score follows a normal distribution, <span class="math inline">\(X(1)\sim N(\mu, \sigma^2)\)</span>. We can calculate the probability of team A winning, denoted <span class="math inline">\(p=\mathbb{P}(X(1)&gt;0)\)</span>, from the spread and probability distribution. Given the normality assumption, <span class="math inline">\(X(1) \sim N(\mu, \sigma^2)\)</span>, we have <span class="math display">\[
p = \mathbb{P}(X(1)&gt;0) = \Phi \left ( \frac{\mu}{\sigma} \right )
\]</span> where <span class="math inline">\(\Phi\)</span> is the standard normal cdf. Table 1 uses <span class="math inline">\(\Phi\)</span> to convert team A’s advantage <span class="math inline">\(\mu\)</span> to a probability scale using the information ratio <span class="math inline">\(\mu/\sigma\)</span>.</p>
<div id="tbl-probability" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-probability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.1: Probability of Winning <span class="math inline">\(p\)</span> versus the Sharpe Ratio <span class="math inline">\(\mu/\sigma\)</span>
</figcaption>
<div aria-describedby="tbl-probability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 31%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(\mu/\sigma\)</span></th>
<th>0</th>
<th>0.25</th>
<th>0.5</th>
<th>0.75</th>
<th>1</th>
<th>1.25</th>
<th>1.5</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p=\Phi(\mu/\sigma)\)</span></td>
<td>0.5</td>
<td>0.60</td>
<td>0.69</td>
<td>0.77</td>
<td>0.84</td>
<td>0.89</td>
<td>0.93</td>
<td>0.977</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>If teams are evenly matched and <span class="math inline">\(\mu/\sigma =0\)</span> then <span class="math inline">\(p=0.5\)</span>. Table 1 provides a list of probabilities as a function of <span class="math inline">\(\mu/\sigma\)</span>. For example, if the point spread <span class="math inline">\(\mu=-4\)</span> and volatility is <span class="math inline">\(\sigma=10.6\)</span>, then the team has a <span class="math inline">\(\mu/\sigma = -4/10.6 = - 0.38\)</span> volatility point disadvantage. The probability of winning is <span class="math inline">\(\Phi(-0.38) = 0.353 &lt; 0.5\)</span>. A common scenario is that team A has an edge equal to half a volatility, so that <span class="math inline">\(\mu/\sigma =0.5\)</span> and then <span class="math inline">\(p= 0.69\)</span>.</p>
<p>Of particular interest here are conditional probability assessments made as the game progresses. For example, suppose that the current lead at time <span class="math inline">\(t\)</span> is <span class="math inline">\(l\)</span> points and so <span class="math inline">\(X(t) = l\)</span>. The model can then be used to update your assessment of the distribution of the final score with the conditional distribution $ (X(1) | X(t)=l )$. To see this, we can re-write the distribution of <span class="math inline">\(X(1)\)</span> given <span class="math inline">\(X(t)\)</span> by noting that <span class="math inline">\(X(1) = X(t)+ X(1) - X(t)\)</span>. Using the formula above and substituting <span class="math inline">\(t\)</span> for <span class="math inline">\(1\)</span> where appropriate and noting that <span class="math inline">\(X(t) = l\)</span> by assumption, this simplifies to <span class="math display">\[
X(1)= l + \mu(1- t) + \sigma (B(1) - B(t)).
\]</span> Here <span class="math inline">\(B(1) - B(t)  \stackrel{D}{=} B(1-t)\)</span> which is independent of <span class="math inline">\(X(t)\)</span> with distribution <span class="math inline">\(N(0,1-t)\)</span>. The mean and variance of <span class="math inline">\(X(1)|X(t)=l\)</span> decay to zero as <span class="math inline">\(t \rightarrow 1\)</span> and the outcome becomes certain at the realised value of <span class="math inline">\(X(1)\)</span>. We leave open the possibility of a tied game and overtime to determine the outcome.</p>
<p>To determine this conditional distribution, we note that there are <span class="math inline">\(1-t\)</span> time units left together with a drift <span class="math inline">\(\mu\)</span> and as shown above in this case the uncertainty can be modeled as which is Therefore, we can write the distribution of the final outcome after <span class="math inline">\(t\)</span> periods with a current lead of <span class="math inline">\(l\)</span> for team A as the conditional distribution: <span class="math display">\[
( X(1) | X(t)=l) =  (X(1)-X(t)) + l   \sim N( l + \mu(1 - t) , \sigma^2 (1 - t) )
\]</span> From the conditional distribution <span class="math inline">\((X(1) | X(t)=l) \sim N(l+\mu(1-t), \sigma^2 (1-t))\)</span>, we can calculate the conditional probability of winning as the game evolves. The probability of team A winning at time <span class="math inline">\(t\)</span> given a current lead of <span class="math inline">\(l\)</span> point is: <span class="math display">\[
p_t = P ( X(1) &gt; 0 | X(t) = l) = \Phi \left ( \frac{ l + \mu ( 1 - t)  }{ \sigma \sqrt{ ( 1-t) } } \right )
\]</span></p>
<div id="fig-score-evolution" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-score-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/svg/hal-vol-new.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-score-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: Score Evolution on a Discretized Grid
</figcaption>
</figure>
</div>
<p><a href="#fig-score-evolution" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> A and B illustrate our methodology with an example. Suppose we are analyzing data for a Superbowl game between teams A and B with team A favored. Figure A presents the information available at the beginning of game from the perspective of the undergod team B. If the initial point spread–or the markets’ expectaion of the expected outcome–is <span class="math inline">\(-4\)</span> and the volatility is <span class="math inline">\(10.6\)</span>–assumed given for the moment (more on this below)–then the probability that the underdog team wins is <span class="math inline">\(p = \Phi ( \mu /\sigma ) = \Phi ( - 4/ 10.6) = 35.3\)</span>%. This result relies on our assumption of a normal outcome distribution on the outcome as previously explained. Another way of saying this is <span class="math inline">\(\mathbb{P}(X(1)&gt;0)=0.353\)</span> for an outcome distribution <span class="math inline">\(X(1) \sim N(-4, 10.6^2)\)</span>. Figure A illustrates this with the shaded red area under the curve.</p>
<p><a href="#fig-score-evolution" class="quarto-xref">Figure&nbsp;<span>9.2</span></a> B illustrates the information and potential outcomes at half-time. Here we show the evolution of the actual score until half time as the solid black line. From half-time onwards we simulate a set of possible Monte Carlo paths to the end of the game. %Specifically, we discretise the model with time interval <span class="math inline">\(\Delta =1/200\)</span> %and simulate possible outcomes given the score at half time. The volatility plays a key role in turning the point spread into a probability of winning as the greater the volatility of the distribution of the outcome, <span class="math inline">\(X(1)\)</span>, the greater the range of outcomes projected in the Monte Carlo simulation. Essentially the volatility provides a scale which calibrates the advantage implied by a given point spread.</p>
<p>We can use this relationship to determine how volatility decays over the course of the game. The conditional distribution of the outcome given the score at time <span class="math inline">\(t\)</span>, is <span class="math inline">\((X(1)|X(t)=l)\)</span> with a variance of <span class="math inline">\(\sigma^2(1-t)\)</span> and volatility of <span class="math inline">\(\sigma \sqrt{1-t}\)</span>. The volatility is a decreasing function of <span class="math inline">\(t\)</span>, illustrating that the volatility dissipates over the course of a game. For example, if there is an initial volatility of <span class="math inline">\(\sigma = 10.6\)</span>, then at half-time when <span class="math inline">\(t=\frac{1}{2}\)</span>, the volatility is <span class="math inline">\(10.6 / \sqrt{2} = 7.5\)</span> volatility points left. Table 2, below, illustrates this relationship for additional points over the game.</p>
<div id="tbl-volatility-decay" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-volatility-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.2: Volatility Decay over Time
</figcaption>
<div aria-describedby="tbl-volatility-decay-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 28%">
<col style="width: 8%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th>0</th>
<th><span class="math inline">\(\frac{1}{4}\)</span></th>
<th><span class="math inline">\(\frac{1}{2}\)</span></th>
<th><span class="math inline">\(\frac{3}{4}\)</span></th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\sigma \sqrt{1-t}\)</span></td>
<td>10.6</td>
<td>9.18</td>
<td>7.50</td>
<td>5.3</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To provide insight into the final outcome given the current score, Tables 1 and 2 can be combined to measure the current outcome, <span class="math inline">\(l\)</span>, in terms of standard deviations of the outcome.<br>
For example, suppose that you have Team B, an underdog, so from their perspective <span class="math inline">\(\mu = -4\)</span> and at half-time team B has a lead of 15, <span class="math inline">\(l= 15\)</span>. Team B’s expected outcome as presented earlier is <span class="math inline">\(l + \mu (1-t)\)</span> or <span class="math inline">\(15 - 4 \times \frac{1}{2} = 13\)</span>. If initial volatility is <span class="math inline">\(\sigma = 10.60\)</span> then the remaining volatility at half-time is <span class="math inline">\(10.6/\sqrt{2} = 7.50\)</span> and team B’s expected outcome of <span class="math inline">\(13\)</span> in terms of standard deviations is <span class="math inline">\(13/7.5 = 1.73\)</span>. Thus team B’s expected outcome is at the 99th percentile of the distribution, <span class="math inline">\(\Phi ( 1.73 ) = 0.96\)</span>, implying a 96% chance of winning.</p>
<p><strong>Implied Volatility</strong></p>
<p>The previous discussion assumed that the variance (or volatility) parameter <span class="math inline">\(\sigma\)</span> was a known constant. We return to this important quantity now.<br>
We are now in a position to define the <em>implied volatility</em> implicit in the two betting lines that are available. Given our model, we will use the <em>money-line</em> odds to provide a market assessment of the probability of winning, <span class="math inline">\(p\)</span>, and the <em>point spread</em> to assess the expected margin of victory, <span class="math inline">\(\mu\)</span>. The money line odds are shown for each team A and B and provide information on the payoff from a bet on the team winning. As shown in the example in section 3, this calculation will also typically require an adjustment for the bookmaker’s spread. With these we can infer the <em>implied volatility</em>, <span class="math inline">\(\sigma_{IV}\)</span>, by solving <span class="math display">\[
\sigma_{IV}: \; \; \;  \; \; p = \Phi \left ( \frac{\mu}{\sigma_{IV}} \right ) \; \; \text{ which \; gives} \; \;
\sigma_{IV} = \frac{ \mu }{ \Phi^{-1} ( p ) } \; .
\]</span> Here <span class="math inline">\(\Phi^{-1}(p)\)</span> denotes the standard normal quantile function such that the area under the standard normal curve to the left of <span class="math inline">\(\Phi^{-1}(p)\)</span> is equal to <span class="math inline">\(p\)</span>. In our example we calculate this using the <code>qnorm</code> in <code>R</code>. Note that when <span class="math inline">\(\mu =0\)</span> and <span class="math inline">\(p= \frac{1}{2}\)</span> there’s no market information about the volatility as <span class="math inline">\(\mu / \Phi^{-1} (p)\)</span> is undefined. This is the special case where the teams are seen as evenly matched- the expected outcome has a zero point spread and there is an equal probability that either team wins.</p>
<p><strong>Time Varying Implied Volatility</strong></p>
<p>Up to this point the volatility rate has been assumed constant through the course of the game, i.e., that the same value of <span class="math inline">\(\sigma\)</span> is relevant. The amount of volatility remaining in the game is not constant but the basic underlying parameters has been assumed constant. This need not be true and more importantly the betting markets may provide some information about the best estimate of the volatility parameter at a given point of time. This is important because time-varying volatility provides an interpretable quantity that can allow one to assess the value of a betting opportunity.</p>
<p>With the advent of on-line betting there is a virtually continuous traded contract available to assesses implied expectations of the probability of team A winning at any time <span class="math inline">\(t\)</span>. The additional information available from the continuous contract allows for further update of the implied conditional volatility. We assume that the online betting market gives us a current assessment of <span class="math inline">\(p_t\)</span>, that is the current probability that team A will win. We will then solve for <span class="math inline">\(\sigma^2\)</span> and in turn define resulting time-varying volatility, as <span class="math inline">\(\sigma_{IV,t}\)</span>, using the resulting equation to solve for <span class="math inline">\(\sigma_{IV,t}\)</span> with <span class="math display">\[
p_t = \Phi \left ( \frac{ l + \mu(1-t)  }{\sigma_{IV,t} \sqrt{1-t}} \right )
\; \text{ which \; gives} \; \;
\sigma_{IV,t} = \frac{ l + \mu ( 1-t ) }{ \Phi^{-1} ( p_t )  \sqrt{1-t}}
\]</span> We will use our methodology to find evidence of time-varying volatility in the SuperBowl XLVII probabilities.</p>
<p><strong>Super Bowl XLVII: Ravens vs San Francisco 49ers</strong></p>
<p>Super Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013 and featured the San Francisco 49ers against the Baltimore Ravens. Going into Super Bowl XLVII the San Francisco 49ers were favorites to win which was not surprising following their impressive season. It was a fairly bizarre Super Bowl with a <span class="math inline">\(34\)</span> minute power outage affecting the game by ultimately an exciting finish with the Ravens causing an upset victory <span class="math inline">\(34-31\)</span>. We will build our model from the viewpoint of the Ravens. Hence <span class="math inline">\(X(t)\)</span> will correspond to the Raven’s score minus the San Francisco 49ers. Table 3 provides the score at the end of each quarter.</p>
<div id="tbl-superbowl" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9.3: SuperBowl XLVII by Quarter
</figcaption>
<div aria-describedby="tbl-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(t\)</span></th>
<th>0</th>
<th><span class="math inline">\(\frac{1}{4}\)</span></th>
<th><span class="math inline">\(\frac{1}{2}\)</span></th>
<th><span class="math inline">\(\frac{3}{4}\)</span></th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ravens</td>
<td>0</td>
<td>7</td>
<td>21</td>
<td>28</td>
<td>34</td>
</tr>
<tr class="even">
<td>49ers</td>
<td>0</td>
<td>3</td>
<td>6</td>
<td>23</td>
<td>31</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X(t)\)</span></td>
<td>0</td>
<td>4</td>
<td>15</td>
<td>5</td>
<td>3</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To determine the parameters of our model we first use the <em>point spread</em> which was set at the Ravens being a four point underdog, i.e.&nbsp;<span class="math inline">\(\mu=-4\)</span>. This sets the mean of our outcome, <span class="math inline">\(X(1)\)</span>, as <span class="math display">\[
\mu = \mathbb{E} \left (X(1) \right )=-4 .
\]</span> In reality, it was an exciting game with the Ravens upsetting the 49ers by <span class="math inline">\(34-31\)</span>. Hence, the realised outcome is <span class="math inline">\(X(1)= 34-31=3\)</span> with the point spread being beaten by <span class="math inline">\(7\)</span> points or the equivalent of a touchdown.</p>
<div id="fig-superbowl" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/hal-superbowl.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-superbowl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.3: Superbowl XLVII: Ravens vs 49ers: TradeSports contracts traded and dynamic probability of the Ravens winning
</figcaption>
</figure>
</div>
<p>To determine the markets’ assessment of the probability that the Ravens would win at the being of the game we use the <em>money-line</em> odds. These odds were quoted as San Francisco <span class="math inline">\(-175\)</span> and Baltimore Ravens <span class="math inline">\(+155\)</span>. This implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155. We can convert both of these money-lines to <em>implied probabilities</em> of the each team winning, by the equations <span class="math display">\[
p_{SF} = \frac{175}{100+175} = 0.686 \; \; \text{ and} \; \; p_{Ravens} = \frac{100}{100+155} = 0.392
\]</span> The probability sum to one plus the market vig: <span class="math display">\[
p_{SF} + p_{Ravens} = 0.686+0.392 = 1.078
\]</span> namely a <span class="math inline">\(7.8\)</span>% edge for the bookmakers. Put differently, if bettors place money proportionally across both teams then the bookies will make <span class="math inline">\(7.8\)</span>% of the total staked no matter what happens to the outocme of the game. To account for this edge in our model, we use the mid-point of the spread to determine <span class="math inline">\(p\)</span> implying that <span class="math display">\[
p = \frac{1}{2} p_{Ravens} + \frac{1}{2} (1 - p_{SF} ) = 0.353
\]</span> From the Ravens perspective we have <span class="math inline">\(p = \mathbb{P}(X(1)&gt;0) =0.353\)</span>.</p>
<p><a href="#fig-superbowl" class="quarto-xref">Figure&nbsp;<span>9.3</span></a> shows the evolution of the markets conditional probability of winning <span class="math inline">\(p_t\)</span> for the Ravens. The data are from the online betting website <code>TradeSports.com</code>. Starting at <span class="math inline">\(p=0.353\)</span> we see how dramatically the markets assessment of the Ravens winning can fluctuate. (NGP: This is really confusing. We need to say a bit more about what we did here. How do you get these probabilities. Key point is what are you assuming about sigma given the abaove discussion.) Given their commanding lead at half time, the probability has as high as <span class="math inline">\(0.90\)</span>. At the end of the four quarter when the 49ers nearly went into the lead with a touchdown, at one point the probability had dropped to <span class="math inline">\(30\)</span>%.</p>
<p>Our main question of interest is then: <em>What implied volatility is consistent with market expectations?</em></p>
<p>To calculate the implied volatility of the Superbowl we substitute the pair <span class="math inline">\((\mu,p)\)</span> into our definition and solve for <span class="math inline">\(\sigma_{IV}\)</span>. We obtain <span class="math display">\[
\sigma_{IV} = \frac{\mu}{\Phi^{-1}(p)} = \frac{-4}{-0.377}  = 10.60
\]</span> where we have used <span class="math inline">\(\Phi^{-1} ( p) = qnorm(0.353) = -0.377\)</span>. So on a volatility scale the <span class="math inline">\(4\)</span> point advantage assessed for the 49ers is under a <span class="math inline">\(\frac{1}{2} \sigma\)</span> favorite. From Table 2, this is consistent with a win probability of <span class="math inline">\(p=\Phi(\frac{1}{2})=0.69\)</span>. Another feature is that a <span class="math inline">\(\sigma=10.6\)</span> is historically low, as a typical volatility of an NFL game is <span class="math inline">\(14\)</span> (see Stern, 1991). However, the more competitive the game one might expect a lower volatility. In reality, the outcome <span class="math inline">\(X(1)=3\)</span> was withing one standard deviation of the model which had an expectation of <span class="math inline">\(\mu=-4\)</span> and volatility of <span class="math inline">\(\sigma= 10.6\)</span>. Another question of interest is</p>
<p>What’s the probability of the Ravens winning given their lead at half time?</p>
<p>To illustrate the dynamic nature of the odds and to infer a time-varying implied volatility we ask the question,<br>
At half time the Ravens where leading <span class="math inline">\(21\)</span> to <span class="math inline">\(6\)</span>. This gives us <span class="math inline">\(X(\frac{1}{2})=21-6=15\)</span>. From the online betting market we also have traded contracts on <code>TradeSports.com</code> that yield a current probability of <span class="math inline">\(p_{\frac{1}{2}} = 0.90\)</span>. Now we can ask</p>
<p>An alternative view is to assume that the market assesses time varying volatility and the prices fully reflect the underlying probability. Here we ask the question</p>
<p><em>What’s the implied volatility for the second half of the game?</em></p>
<p>We now have an implied volatility <span class="math display">\[
\sigma_{IV,t=\frac{1}{2}} = \frac{ l + \mu ( 1-t ) }{ \Phi^{-1} ( p_t )  \sqrt{1-t}} = \frac{15-2}{ \Phi^{-1}(0.9) / \sqrt{2} } = 14
\]</span> where <code>qnorm(0.9)=1.28</code>. Notice that <span class="math inline">\(14&gt; 10.6\)</span>, our assessment of the implied volatility at the beginning of the game.</p>
<p><em>What’s a valid betting strategy?</em></p>
<p>An alternative approach is to assume that the initial moneyline and point spread set the volatility and this stays constant throughout the game. This market is much larger than the online market and this is a reasonable assumption unless there has been material information as the game progresses such as a key injury.</p>
<p>Hence the market was expected a more typical volatility in the second half. If a bettor believed that there was no reason that <span class="math inline">\(\sigma\)</span> had changed from the initial <span class="math inline">\(10.6\)</span> then their assessment of the Ravens win probability, under this models, would have been <span class="math inline">\(\Phi \left ( 13/ (10.6/\sqrt{2}) \right ) = 0.96\)</span> and the <span class="math inline">\(0.90\)</span> market rate would have been thought of as a betting opportunity.</p>
<p>The Kelly criterion (Kelly,1956) yields the betting rate <span class="math display">\[
\omega = p - \frac{q}{0} = 0.96 - \frac{0.1}{1/9} = 0.06
\]</span> that is, <span class="math inline">\(6\)</span>% of capital. A more realistic strategy is to use the fractional Kelly criterion whcih scales by a risk aversion parameter, <span class="math inline">\(\gamma\)</span>. For example, in this case if <span class="math inline">\(\gamma =3\)</span>, we would bet <span class="math inline">\(0.06/3=0.01\)</span>, or <span class="math inline">\(2\)</span>% of our capital on this betting opportunity.</p>
<p>Finally, odds changes can be dramatic at the end of the fourth quarter and this Super Bowl was no exception. With the score at <span class="math inline">\(34-29\)</span>, with <span class="math inline">\(x\)</span> minutes to go, the 49ers were at <span class="math inline">\(1\)</span>st and goal. in the minutes before this, the probability of the Ravens winning had dropped precipitously from over <span class="math inline">\(90\)</span>% to <span class="math inline">\(30\)</span>%, see <a href="#fig-superbowl" class="quarto-xref">Figure&nbsp;<span>9.3</span></a>. On San Francisco’s final offensive play of the game, Kaepernick threw a pass on fourth down to Michael Crabtree, but Ravens cornerback Jimmy Smith appeared to hold the wide receiver during the incompletion, No call was given and the final result was a Ravens win.</p>
</div>
<div id="exm-mc-portfolio" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.2 (Yahoo Stock Price Simulation)</strong></span> Investing in volatile stocks can be very risky. The Internet stocks during the late 1990’s were notorious for their volatility. For example, the leading Internet stock Yahoo! started 1999 at $62,rose to $122, then fell back to $55 in August, only to end the year at $216. Even more remarkable is the fact that by January 2000, Yahoo! has risen more than 100-fold from its offering price of $1.32 on April 15, 1996. In comparison, theNasdaq 100, a benchmark market index, was up about 5-fold during the same period.</p>
<p>Stock prices fluctuate somewhat randomly. Maurice Kendall, in his seminal 1953 paper on the random walk nature of stock and commodity prices, observed that “<em>The series looks like a wandering one, almost as if once a week the Demon of Chance drew a random number from a symmetrical population of fixed dispersion and added to it the current price to determine next week’s price (p.&nbsp;87).</em>” While a pure random walk model for Yahoo!’s stock price is in fact not reasonable since its price cannot fall below zero, an alternative model tha appears to provide reasonable results assumes that the logarithms o price changes, or returns, follow a random walk. This alternative mode is the basis for the results in this article.</p>
<p>To evaluate a stock investment, we take the initial price as <span class="math inline">\(X_0\)</span> and then we need to determine what the stock price might be in year <span class="math inline">\(T\)</span>, namely <span class="math inline">\(X_T\)</span>. Our approach draws from the Black-Scholes Model for valuing stock options. Technically, the Black-Scholes Model assumes that <span class="math inline">\(X_T\)</span> is determined by the solution to a stochastic differential equation. This leads to the <em>Geometric Brownian Motion</em> <span class="math display">\[
X_T = X_0 \exp\left( (\mu - 1/2\sigma^2)T + \sigma B_T  \right),
\]</span> where <span class="math inline">\(B_T\)</span> is a standard Brownian motion, namely: <span class="math inline">\(B_0 = 0\)</span>, <span class="math inline">\(B_t - B_s\)</span> is independent of <span class="math inline">\(B_s\)</span> and its distribution only depends onb <span class="math inline">\(t-s\)</span> and <span class="math inline">\(B_t \sim N(0,t)\)</span>. Hence, <span class="math inline">\(B_t = \sqrt{t}Z\)</span>, where <span class="math inline">\(Z \sim N(0,1)\)</span>.</p>
<p>Then, the expected value is <span class="math display">\[\begin{align*}
E(X_T) = &amp;X_0 \exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma B_T))\\
&amp; = X_0\exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma \sqrt{T}Z))\\
&amp; = X_0\exp\left( (\mu - 1/2\sigma^2)T \right) E(\exp(\sigma \sqrt{T}Z)) \\
&amp;= X_0\exp\left( (\mu - 1/2\sigma^2)T \right) \exp\left( \frac{1}{2}\sigma^2T \right) = X_0\exp\left( \mu T \right).
\end{align*}\]</span> The <span class="math inline">\(E(\exp(\sigma \sqrt{T}Z)) = \exp\left( 1/2\sigma^2T \right)\)</span> is due to the moment property of the log-normal distribution. We can interpret <span class="math inline">\(\mu\)</span> as the expected rate of return <span class="math display">\[
\hat \mu = \frac{1}{T}\log\left( \frac{X_T}{X_0} \right).
\]</span> This provides a way to estimate the expected rate of return from the expected value of the stock price at time <span class="math inline">\(T\)</span>, by plugging in the observed values of <span class="math inline">\(X_0\)</span> and <span class="math inline">\(X_T\)</span>.</p>
<p>The variance is <span class="math display">\[\begin{align*}
\text{Var}(X_T) = &amp;X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \text{Var}(\exp(\sigma B_T))\\
&amp; = X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \text{Var}(\exp(\sigma \sqrt{T}Z))\\
&amp; = X_0^2 \exp\left( 2(\mu - 1/2\sigma^2)T \right) \exp\left( \sigma^2T \right) - X_0^2\exp\left( 2(\mu - 1/2\sigma^2)T \right)\\
&amp; = X_0^2\exp\left( 2\mu T \right)\left( \exp\left( \sigma^2T \right) - 1 \right).
\end{align*}\]</span></p>
<p>The important consequence of the model for predicting future prices is that <span class="math inline">\(\log(X_T/X_0)\)</span> has a normal distribution with mean <span class="math inline">\((\mu-\frac{1}{2} \sigma^2)T\)</span> and variance <span class="math inline">\(\sigma^2 T\)</span> which is equivalent to saying that the ratio <span class="math inline">\(X_T/X_0\)</span> has a log-normal distribution. It is interesting that although the Black-Scholes result is a standard tool for valuing options in finance the log-normal predictive distribution that follows from its assumptions is not commonly studied. In order to forecast <span class="math inline">\(X_T\)</span> we need to estimate the unknowns <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> (recall <span class="math inline">\(X_0\)</span> is known). The unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> can be interpreted as the instantaneous expected rate of return and the volatility, respectively. The mean parameter <span class="math inline">\(\mu\)</span> is known as the expected rate of return because the expected value of <span class="math inline">\(X_T\)</span> is <span class="math inline">\(X_0e^{\mu T}\)</span>. There are a number of ways of estimating the unknown parameters. One approach is to use an equilibrium model for returns, such as the Capital Asset Pricing Model or CAPM. We will discuss this model later. Another approach is to use historical data to estimate the parameters. For example, the expected rate of return can be estimated as the average historical return. The volatility can be estimated as the standard deviation of historical returns. The Black-Scholes model is a continuous time model, but in practice we use discrete time data. The Black-Scholes model can be adapted to discrete time by replacing the continuous time Brownian motion with a discrete time random walk.</p>
</div>
</section>
<section id="gaussian-processes" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="gaussian-processes"><span class="header-section-number">9.2</span> Gaussian Processes</h2>
<p>A Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. It’s a powerful tool for modeling and predicting in various fields, particularly useful for regression and classification tasks in machine learning. A finite collection of <span class="math inline">\(n\)</span> points from Gaussian Process is completely specified by its <span class="math inline">\(n\)</span>-dimensional mean <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>. The index of the GP is a real number <span class="math inline">\(x\)</span> and values are also real numbers. The mean of the process (and a finite collection of points) is defined by function <span class="math inline">\(m(x)\)</span> and covariance is defined by function <span class="math inline">\(k(x, x')\)</span>, where <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are points in the index space. The mean function defines the average value of the function at point <span class="math inline">\(x\)</span>, and the covariance function, also known as the kernel, defines the extent to which the values of the function at two points <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are correlated.</p>
<p><span class="math inline">\(k(x, x')\)</span>, where <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are points in the input space. The mean function defines the average value of the function at point <span class="math inline">\(x\)</span>, and the covariance function, also known as the kernel, defines the extent to which the values of the function at two points <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> are correlated. In other words, the kernel function is a measure of similarity between two input points. The covariance between two points is higher if they are similar, and lower if they are dissimilar. Thus Gaussian Process is completely specified by its mean function and covariance function and an instance of a one-dimensional GP is a function <span class="math inline">\(f(x): \mathbb{R} \rightarrow \mathbb{R}\)</span>, a typical notation is <span class="math display">\[
f(x) \sim \mathcal{GP}(m(x), k(x, x')).
\]</span> The mean function* <span class="math inline">\(m(x) = \mathbb{E}[f(x)]\)</span> is then represents the expected value of the function at point <span class="math inline">\(x\)</span>, and the <strong>covariance function</strong>: <span class="math inline">\(k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\)</span> describes the amount of dependence between the values of the function at two different points in the input space.</p>
<p>Typically the mean function is less important than the covariance function. Most of the time data scientists will use a zero mean function, <span class="math inline">\(m(x)=0\)</span>, and focus on the covariance function. The kernel function is often chosen to be a function of the distance between the two points <span class="math inline">\(|x-x'|\)</span> or <span class="math inline">\(\|x-x'\|_2\)</span> in higher dimensions. The most commonly used kernel function is the squared exponential kernel, which is a function of the squared distance between the two points. The squared exponential kernel is given by: <span class="math display">\[
k(x, x') = \sigma^2 \exp\left(-\frac{(x - x')^2}{2l^2}\right)
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the variance parameter and <span class="math inline">\(l\)</span> is the length scale parameter. The variance parameter controls the vertical variation of the function (amplitude), and the length scale parameter controls the horizontal variation (number of “bumps”). The length scale parameter determines how far apart two points must be to be considered dissimilar. The larger the length scale, the smoother the function. The length scale parameter is also called the bandwidth parameter. In this case the covariance decays exponentially with the distance between the points. Observe, that <span class="math inline">\(k(x,x) = \sigma^2\)</span> and <span class="math inline">\(k(x,x') \rightarrow 0\)</span> as <span class="math inline">\(|x-x'| \rightarrow \infty\)</span>. T</p>
<p>Let’s demonstrate GP using a simulated example. We start by generating a sequence 100 inputs (process indexes)</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and then define the mean function and the covariance function</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(x))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sqexpcov <span class="ot">=</span> <span class="cf">function</span>(x, x1, <span class="at">l=</span><span class="dv">1</span>, <span class="at">sigma=</span><span class="dv">1</span>) {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> (x <span class="sc">-</span> x1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> l<span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The covariance function is a function of the distance between the two points and not the actual values of the points. The squared exponential kernel is infinitely differentiable, which means that the GP is a very smooth function. The squared exponential kernel is also called the radial basis function (RBF) kernel. The covariance matrix is then defined as</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>cov_mat <span class="ot">=</span> <span class="fu">outer</span>(x, x, sqexpcov)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>and we can generate a sample from the GP using the <code>mvrnorm</code> function from the <code>MASS</code> package and plot a sample</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, mean, cov_mat)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, Y, <span class="at">type=</span><span class="st">"l"</span>, <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="st">"y"</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="dv">2</span>), <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gp-sample" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gp-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-gp-sample-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gp-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.4: Sample from a Gaussian Process
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-gp-sample" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> shows a colleciton of 100 points of function <span class="math inline">\(f(x)\)</span> sampled from a Gaussian Process with zero mean and squared exponential kernel for the set of 100 indeces <span class="math inline">\(x =(0,0.1,0.2,\ldots,10)\)</span>. By visually inspecting The finite realization in <a href="#fig-gp-sample" class="quarto-xref">Figure&nbsp;<span>9.4</span></a> of the GP, we can see that the sampled function is smooth, with most of its values between -2 and 2. Notice, that each element of the covariance matrix is less than 1. Thus, by properties of the normal 95% of points of <span class="math inline">\(Y\)</span> should be within 1.96 of the zero (the mean). We see a few bumps on the plot, becasue values of <span class="math inline">\(Y\)</span> with indeces close to each other are highly correlated.</p>
<p>Let’s generate a few more samples from the same GP and plot them together</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Ys <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">3</span>, mean, cov_mat)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(x, <span class="fu">t</span>(Ys), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">ylab=</span><span class="st">"Y"</span>, <span class="at">lwd=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gp-samples" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gp-samples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-gp-samples-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gp-samples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.5: Samples from a Gaussian Process
</figcaption>
</figure>
</div>
</div>
</div>
<p>Each random finite collection is different than the next. They all have similar range, about the same number of bumps, and are smooth. That’s what it means to have function realizations under a GP prior: <span class="math inline">\(Y = f(x) \sim \mathcal{GP}(0, k(x, x'))\)</span></p>
<section id="making-predictions-with-gaussian-processes" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="making-predictions-with-gaussian-processes"><span class="header-section-number">9.2.1</span> Making Predictions with Gaussian Processes</h3>
<p>If we think that our observed data with input indeces <span class="math inline">\(X = (x_1,\ldots,x_n)\)</span> and outputs <span class="math inline">\(Y = (y_1,\ldots,y_n)\)</span> are a realization of a Gaussian Process, then we can use the GP to make predictions about the output values at new inputs <span class="math inline">\(x_* \in \mathbb{R}^q\)</span>. The joint distribution of the observed data <span class="math inline">\(Y\)</span> and the new data <span class="math inline">\(y_*\)</span> is given by <span class="math display">\[
\begin{bmatrix} Y \\ y_* \end{bmatrix} \sim \mathcal{N} \left ( \begin{bmatrix} \mu \\ \mu_* \end{bmatrix}, \begin{bmatrix} K &amp; K_* \\ K_*^T &amp; K_{**} \end{bmatrix} \right )
\]</span> where <span class="math inline">\(K = k(X, X)\in \mathbb{R}^{n\times n}\)</span>, <span class="math inline">\(K_* = k(X, x_*)\in \mathbb{R}^{n\times q}\)</span>, <span class="math inline">\(K_{**} = k(x_*, x_*) \in \mathbb{R}^{q\times q}\)</span>, <span class="math inline">\(\mu = \mathbb{E}[Y]\)</span>, and <span class="math inline">\(\mu_* = \mathbb{E}[y_*]\)</span>. The conditional distribution of <span class="math inline">\(y_*\)</span> given <span class="math inline">\(y\)</span> is then given by <span class="math display">\[
y_* \mid Y \sim \mathcal{N}(\mu_{\mathrm{post}}, \Sigma_{\mathrm{post}}).
% y_* \mid Y \sim \mathcal{N}(\mu_* + K_* K^{-1} (y - \mu), K_{**} - K_*^T K^{-1} K_*).
\]</span> The mean of the conditional distribution is given by <span id="eq-mupost"><span class="math display">\[
\mu_{\mathrm{post}} = \mu_* + K_*^TK^{-1} (Y - \mu)
\tag{9.1}\]</span></span> and the covariance is given by <span id="eq-Spost"><span class="math display">\[
\Sigma_{\mathrm{post}} = K_{**} - K_*^T K^{-1} K_*.
\tag{9.2}\]</span></span></p>
<p><a href="#eq-mupost" class="quarto-xref">Equation&nbsp;<span>9.1</span></a> and <a href="#eq-Spost" class="quarto-xref">Equation&nbsp;<span>9.2</span></a> are convinient properties of a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal distribution</a>.</p>
<div id="exm-gpsin" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.3 (Gaussian Process for <span class="math inline">\(\sin\)</span> function)</strong></span> Let’s use the GP to make predictions about the output values at new inputs <span class="math inline">\(x_*\)</span>. We use <span class="math inline">\(x\)</span> in the [0,<span class="math inline">\(2\pi\)</span>] range and <span class="math inline">\(y\)</span> to be the <span class="math inline">\(y = \sin(x)\)</span>. We start by simulating the “observed” <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> pairs.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">8</span>; eps<span class="ot">=</span><span class="fl">1e-6</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi, <span class="at">length=</span>n), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">sin</span>(X)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], sqexpcov) <span class="sc">+</span> <span class="fu">diag</span>(eps, n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The additive term <code>diag(eps, n)</code> <span class="math inline">\(=\epsilon I\)</span> adds a diagonal matrix with the small <span class="math inline">\(\epsilon\)</span> on the diagonal. This term shifts the spectrum of the resulting covariance matrix <span class="math inline">\(K\)</span> by <span class="math inline">\(\epsilon\)</span> to the right. This is done to add numerical stability in case one of the eignevalues is close to zero. It simply gives us a guarantee that solving linear system (inverting) with matrix <span class="math inline">\(K\)</span> will be a numerically stable operation. In machine learning they call this term the jitter. Now we implement a function that calculates the mean and covariance of the posterior distribution of <span class="math inline">\(y_*\)</span> given <span class="math inline">\(Y\)</span>.</p>
<p>Now we generate a new set of inputs <span class="math inline">\(x_*\)</span> and calculate the covariance matrices <span class="math inline">\(K_*\)</span> and <span class="math inline">\(K_{**}\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">2</span><span class="sc">*</span>pi <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">length=</span>q), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>KX <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>], XX[,<span class="dv">1</span>],sqexpcov)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>KXX <span class="ot">=</span> <span class="fu">outer</span>(XX[,<span class="dv">1</span>],XX[,<span class="dv">1</span>], sqexpcov) <span class="sc">+</span> <span class="fu">diag</span>(eps, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice, we did not add <span class="math inline">\(\epsilon I\)</span> to <span class="math inline">\(K_*\)</span> = <code>KX</code> matrix, but to add it to <span class="math inline">\(K_{**}\)</span> = <code>KXX</code> to gurarantee that the resulting posterior covariance matrix is non-singular (invertable). Now we can calculate the mean and covariance of the posterior distribution of <span class="math inline">\(y_*\)</span> given <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Si <span class="ot">=</span> <span class="fu">solve</span>(K)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>mup <span class="ot">=</span> <span class="fu">t</span>(KX) <span class="sc">%*%</span> Si <span class="sc">%*%</span> Y <span class="co"># we assume mu is 0</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>Sigmap <span class="ot">=</span> KXX <span class="sc">-</span> <span class="fu">t</span>(KX) <span class="sc">%*%</span> Si <span class="sc">%*%</span> KX</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can generate a sample from the posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>YY <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">100</span>, mup, Sigmap)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using our convinience function <code>plot_gp</code> we can plot the posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plot_gp <span class="ot">=</span> <span class="cf">function</span>(mup, Sigmap, X, Y, XX, YY){</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  q1 <span class="ot">=</span> mup <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.05</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">diag</span>(Sigmap)))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  q2 <span class="ot">=</span> mup <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fu">diag</span>(Sigmap)))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matplot</span>(XX, <span class="fu">t</span>(YY), <span class="at">type=</span><span class="st">"l"</span>, <span class="at">col=</span><span class="st">"gray"</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">xlab=</span><span class="st">"x"</span>, <span class="at">ylab=</span><span class="st">"y"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(X, Y, <span class="at">pch=</span><span class="dv">20</span>, <span class="at">cex=</span><span class="dv">2</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(XX, <span class="fu">sin</span>(XX), <span class="at">col=</span><span class="st">"blue"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(XX, mup, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(XX, q1, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(XX, q2, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_gp</span>(mup, Sigmap, X, Y, XX, YY)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gp-sin" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gp-sin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-gp-sin-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gp-sin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.6: Posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-gp-sim" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.4 (Gaussian Process for Simulated Data using MLE)</strong></span> In previous example we assumed that the <span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span> relations are modeled by a GP with <span class="math inline">\(\sigma^2 = 1\)</span> and <span class="math inline">\(2l^2 = 1\)</span>. However, we can use the observed data to estimate those to parameters. In the context of GP models, they are call hyper-parameters. We will use Maximum Likelihood Estimation (MLE) procedure to estimate those hype-parameters. The likelihood of a data that folows multivariate nornal distribution is given by <span class="math display">\[
p(Y \mid X, \sigma, l) = \frac{1}{(2\pi)^{n/2} |K|^{1/2}} \exp \left ( -\frac{1}{2} Y^T K^{-1} Y \right )
\]</span> where <span class="math inline">\(K = K(X,X)\)</span> is the covariance matrix. We assume mean is zero, to simplify the formulas. The log-likelihood is given by <span class="math display">\[
\log p(Y \mid X, \sigma, l) = -\frac{1}{2} \log |K| - \frac{1}{2} Y^T K^{-1} Y - \frac{n}{2} \log 2\pi.
\]</span></p>
<p>Let’s implement a function that calculates the log-likelihood of the data given the hyper-parameters <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(l\)</span> and use <code>optim</code> function to find the maximum of the log-likelihood function.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>loglik <span class="ot">=</span> <span class="cf">function</span>(par, X, Y) {</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> par[<span class="dv">1</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  l <span class="ot">=</span> par[<span class="dv">2</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], sqexpcov,l,sigma) <span class="sc">+</span> <span class="fu">diag</span>(eps, n)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  Si <span class="ot">=</span> <span class="fu">solve</span>(K)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">det</span>(K)) <span class="sc">-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">t</span>(Y) <span class="sc">%*%</span> Si <span class="sc">%*%</span> Y <span class="sc">-</span> (n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span> <span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi)))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>par <span class="ot">=</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), loglik, <span class="at">X=</span>X, <span class="at">Y=</span>Y)<span class="sc">$</span>par</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(par)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.5 2.4</code></pre>
</div>
</div>
<p>The <code>optim</code> function returns the hyper-parameters that maximise the log-likelihood function. We can now use those hyper-parameters to make predictions about the output values at new inputs <span class="math inline">\(x_*\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>l <span class="ot">=</span> par[<span class="dv">2</span>]; sigma <span class="ot">=</span> par[<span class="dv">1</span>]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>predplot <span class="ot">=</span> <span class="cf">function</span>(X, Y, XX, YY, l, sigma) {</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], sqexpcov,l,sigma) <span class="sc">+</span> <span class="fu">diag</span>(eps, n)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  KX <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>], XX[,<span class="dv">1</span>],sqexpcov,l,sigma)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  KXX <span class="ot">=</span> <span class="fu">outer</span>(XX[,<span class="dv">1</span>],XX[,<span class="dv">1</span>], sqexpcov,l,sigma) <span class="sc">+</span> <span class="fu">diag</span>(eps, q)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  Si <span class="ot">=</span> <span class="fu">solve</span>(K)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  mup <span class="ot">=</span> <span class="fu">t</span>(KX) <span class="sc">%*%</span> Si <span class="sc">%*%</span> Y <span class="co"># we assume mu is 0</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  Sigmap <span class="ot">=</span> KXX <span class="sc">-</span> <span class="fu">t</span>(KX) <span class="sc">%*%</span> Si <span class="sc">%*%</span> KX</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  YY <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">100</span>, mup, Sigmap)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_gp</span>(mup, Sigmap, X, Y, XX, YY)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="fu">predplot</span>(X, Y, XX, YY, l, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="sp_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that our unsertainty is much “tighter”, the posterior distribution is much narrower. This is because we used the observed data to estimate the hyper-parameters. We can also see that the posterior mean is closer to the true function <span class="math inline">\(y = \sin(x)\)</span>. Although our intial guess of <span class="math inline">\(\sigma^2 = 1\)</span> and <span class="math inline">\(2l^2 = 1\)</span> was not too far off, the model fits the data much better when we use the estimated hyper-parameters.</p>
<p>The functon <code>optim</code> we used above uses a derivative-based optimisation algorithm and when derivative is not provided by the user, it uses a numerical approximation. Although we can use numerical methods to calculate the derivative of the log-likelihood function, it is faster and more accurate to use analytical derivatives, when possible. In the case of the GP’s log-likelihood, the derivative can be analytically calculated. To do it, we need a couple of facts from matrix calculus. If elemenst of matrix <span class="math inline">\(K\)</span> are functions of some parameter <span class="math inline">\(\theta\)</span>, then <span class="math display">\[
\frac{\partial  Y^T K^{-1} Y}{\partial \theta} =  Y^T K^{-1} \frac{\partial K}{\partial \theta} K^{-1} Y.
\]</span> The derivative of the inverse matrix <span class="math display">\[
\frac{\partial K^{-1}}{\partial \theta} = -K^{-1} \frac{\partial K}{\partial \theta} K^{-1}.
\]</span> and the log of the determinant of a matrix <span class="math display">\[
\frac{\partial \log |K|}{\partial \theta} = \mathrm{tr} \left ( K^{-1} \frac{\partial K}{\partial \theta} \right ),
\]</span> we can calculate the derivative of the log-likehood function with respect to <span class="math inline">\(\theta\)</span> <span class="math display">\[
\frac{\partial \log p(Y \mid X,\theta)}{\partial \theta} = -\frac{1}{2}\frac{\partial \log |K|}{\partial \theta}  + \frac{1}{2} Y^T \frac{\partial K^{-1}}{\partial \theta}  Y.
\]</span> Puting it ll together, we get <span class="math display">\[
\frac{\partial \log p(Y \mid X,\theta)}{\partial \theta} = -\frac{1}{2} \mathrm{tr} \left ( K^{-1} \frac{\partial K}{\partial \theta} \right ) + \frac{1}{2} Y^T K^{-1} \frac{\partial K}{\partial \theta} K^{-1} Y.
\]</span> In the case of squared exponential kernel, the elements of the covariance matrix <span class="math inline">\(K\)</span> are given by <span class="math display">\[
K_{ij} = k(x_i, x_j) = \sigma^2 \exp \left ( -\frac{1}{2} \frac{(x_i - x_j)^2}{l^2} \right ).
\]</span> The derivative of the covariance matrix with respect to <span class="math inline">\(\sigma\)</span> is given by <span class="math display">\[
\frac{\partial K_{ij}}{\partial \sigma} = 2\sigma \exp \left ( -\frac{1}{2} \frac{(x_i - x_j)^2}{l^2} \right );~\frac{\partial K}{\partial \sigma} = \dfrac{2}{\sigma}K.
\]</span> The derivative of the covariance matrix with respect to <span class="math inline">\(l\)</span> is given by <span class="math display">\[
\frac{\partial K_{ij}}{\partial l} = \sigma^2 \exp \left ( -\frac{1}{2} \frac{(x_i - x_j)^2}{l^2} \right ) \frac{(x_i - x_j)^2}{l^3};~ \frac{\partial K}{\partial l}  = \frac{(x_i - x_j)^2}{l^3} K.
\]</span> Now we can implement a function that calculates the derivative of the log-likelihood function with respect to <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(l\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Derivative of the log-likelihood function with respect to sigma</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>dloglik_sigma <span class="ot">=</span> <span class="cf">function</span>(par, X, Y) {</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> par[<span class="dv">1</span>]; l <span class="ot">=</span> par[<span class="dv">2</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], sqexpcov,l,sigma) <span class="sc">+</span> <span class="fu">diag</span>(eps, n)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  Si <span class="ot">=</span> <span class="fu">solve</span>(K)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  dK <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>K<span class="sc">/</span>sigma</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  tr <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">diag</span>(Si <span class="sc">%*%</span> dK))</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> tr <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">t</span>(Y) <span class="sc">%*%</span> Si <span class="sc">%*%</span> dK <span class="sc">%*%</span> Si <span class="sc">%*%</span> Y))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Derivative of the log-likelihood function with respect to l</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>dloglik_l <span class="ot">=</span> <span class="cf">function</span>(par, X, Y) {</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> par[<span class="dv">1</span>]; l <span class="ot">=</span> par[<span class="dv">2</span>]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  K <span class="ot">=</span> <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], sqexpcov ,l,sigma) <span class="sc">+</span> <span class="fu">diag</span>(eps, n)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>  Si <span class="ot">=</span> <span class="fu">solve</span>(K)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>  dK <span class="ot">=</span>   <span class="fu">outer</span>(X[,<span class="dv">1</span>],X[,<span class="dv">1</span>], <span class="cf">function</span>(x, x1) (x <span class="sc">-</span> x1)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>l<span class="sc">^</span><span class="dv">3</span> <span class="sc">*</span> K</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>  tr <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">diag</span>(Si <span class="sc">%*%</span> dK))</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span>(<span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> tr <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">t</span>(Y) <span class="sc">%*%</span> Si <span class="sc">%*%</span> dK <span class="sc">%*%</span> Si <span class="sc">%*%</span> Y))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient funciton that returns a vector of derivatives</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>gnlg <span class="ot">=</span> <span class="cf">function</span>(par,X,Y) {</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(<span class="fu">dloglik_sigma</span>(par, X, Y), <span class="fu">dloglik_l</span>(par, X, Y)))</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the <code>optim</code> function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>par1 <span class="ot">=</span> <span class="fu">optim</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">fn=</span>loglik, <span class="at">gr=</span>gnlg ,<span class="at">X=</span>X, <span class="at">Y=</span>Y,<span class="at">method=</span><span class="st">"BFGS"</span>)<span class="sc">$</span>par</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>l <span class="ot">=</span> par1[<span class="dv">2</span>]; sigma <span class="ot">=</span> par1[<span class="dv">1</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(par1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1.5 2.4</code></pre>
</div>
</div>
<p>The result is the same compared to when we called <code>optim</code> without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimisation algorithm will be much faster.</p>
<p>Futhermore, insted of coding our own derivative functions, we can use an existing package, such as the <a href="https://cran.r-project.org/web/packages/laGP/index.html"><code>laGP</code></a> package, developed by Bobby Gramacy to estimate the hyper-parameters. The <code>laGP</code> package uses the same optimisation algorithm as we used above, but it also provides better seleciton of the covariance functions and implements approximate GP inference algorithms for large scale problems, when <span class="math inline">\(n\)</span> becomes large and inversion of the covariance matrix <span class="math inline">\(K\)</span> is prohibitively expensive.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(laGP)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>gp <span class="ot">=</span> <span class="fu">newGP</span>(X, Y, <span class="dv">1</span>, <span class="dv">0</span>, <span class="at">dK =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">mleGP</span>(gp, <span class="at">tmax=</span><span class="dv">20</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>l.laGP <span class="ot">=</span> <span class="fu">sqrt</span>(res<span class="sc">$</span>d<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(l.laGP)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 2.4</code></pre>
</div>
</div>
<p>In the <code>newGP</code> funciton defines a gaussian process with square exponential covariance function and assumes <span class="math inline">\(\sigma^2 = 1\)</span>, then <code>mleGP</code> function uses optimisation algorithm to maximise the log-likelihood and returns the estimated hyper-parameters <code>d</code> = <span class="math inline">\(2l^2\)</span>, we can see that the length scale is close to the one we estimated above. We will use the <code>predplot</code> convinience funciton to calculate the predicitons and plot the data vs fit.</p>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predplot</span>(X, Y, XX, YY, l, sigma)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predplot</span>(X, Y, XX, YY, l.laGP, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-gp-sin-mle" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gp-sin-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gp-sin-mle" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-gp-sin-mle-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gp-sin-mle-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-gp-sin-mle-1.png" class="img-fluid figure-img" data-ref-parent="fig-gp-sin-mle" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gp-sin-mle-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) MLE Fit
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-gp-sin-mle" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-gp-sin-mle-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-gp-sin-mle-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="sp_files/figure-html/fig-gp-sin-mle-2.png" class="img-fluid figure-img" data-ref-parent="fig-gp-sin-mle" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-gp-sin-mle-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) laGP Fit
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gp-sin-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.7: Posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span>
</figcaption>
</figure>
</div>
<p>We can see that there is visually no difference between the two fits. Thus, it seem irrelevan weather we keep sigma fixed <span class="math inline">\(\sigma=1\)</span> or estimate it using MLE. However, is other applicaitons when uncertainty is larger, the choice of <span class="math inline">\(\sigma\)</span> is important when we use GP for regression and classification tasks. Even for our example, if we ask our model to extrapolate</p>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>XX1 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span><span class="sc">*</span>pi, <span class="dv">6</span><span class="sc">*</span>pi <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">length=</span>q), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predplot</span>(X, Y, XX1, YY, l, sigma)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predplot</span>(X, Y, XX1, YY, l.laGP, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sp_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>MLE Fit</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sp_files/figure-html/unnamed-chunk-16-2.png" class="img-fluid figure-img" width="576"></p>
<figcaption>laGP Fit</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Extrapolation: Posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span></p>
</div>
</div>
</div>
<p>We can see that outside of the range of the observed data, the model with <span class="math inline">\(\sigma=1\)</span> is more “confident” in its predictions.</p>
</div>
<p>Now, instead of using GP to fit a known function (<span class="math inline">\(\sin\)</span>), we wil apply it to a real-world data set. We will use the motorcycle accident data set from the <code>MASS</code> package. The data set contains accelerometer readings taken through time in a simulated experiment on the efficacy of crash helmets.</p>
<div id="exm-gp" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.5 (Gaussian Process for Motorcycle Accident Data)</strong></span> We first estimate the lenght scale parameter <span class="math inline">\(l\)</span> using the <code>laGP</code> package.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">=</span> mcycle<span class="sc">$</span>times</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> mcycle<span class="sc">$</span>accel</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>gp <span class="ot">=</span> <span class="fu">newGP</span>(<span class="fu">matrix</span>(X), Y, <span class="dv">2</span>, <span class="fl">1e-6</span>, <span class="at">dK =</span> <span class="cn">TRUE</span>);</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mleGP</span>(gp, <span class="at">tmax=</span><span class="dv">10</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we plot the data and the fit using the estimated length scale parameter <span class="math inline">\(l\)</span>.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>XX <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">seq</span>(<span class="fl">2.4</span>, <span class="dv">55</span>, <span class="at">length =</span> <span class="dv">499</span>), <span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">predGP</span>(gp, XX)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">499</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>q1 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.05</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>q2 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>q3 <span class="ot">=</span> <span class="fu">qnorm</span>(<span class="fl">0.5</span>, <span class="at">mean =</span> p<span class="sc">$</span>mean, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(p<span class="sc">$</span>Sigma)))</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>X,<span class="at">y=</span>Y)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x=</span>XX,<span class="at">y=</span>q3)) <span class="sc">+</span> <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x=</span>XX,<span class="at">ymin=</span>q1, <span class="at">ymax=</span>q2), <span class="at">alpha=</span><span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sp_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Motorcycle Accident Data. Black line is the mean of the posterior distribution over <span class="math inline">\(y_*\)</span>, given <span class="math inline">\(Y\)</span>. Blue lines are the 95% confidence interval.</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that our model is modelis more confident for time values between 10 and 30. The confidence interval is wider for time values between 0 and 10 and between 30 and less confident at the end close to the 60 mark. For some reason the acceleation values were not measure evenly, if we look at the histogram of time values, we can see that there are more data points in the middle of the time range.</p>
<div class="cell" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sp_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Histogram of time values</figcaption>
</figure>
</div>
</div>
</div>
<p>The <span class="math inline">\(\sqrt{n}\)</span> decay in variance of the posterior distribution is a property of the squared exponential kernel.</p>
</div>
<p>In summary, Gaussian Processes provide a robust and flexible framework for modeling and predicting in situations where uncertainty and correlation among data points play a critical role. Their versatility and powerful predictive capabilities make them a popular choice in various scientific and engineering disciplines. GPs are considered non-parametric, which means they can model functions of arbitrary complexity. Through the choice of the kernel function, GPs can model a wide range of correlations between the data points. The mean and covariance functions can incorporate prior knowledge about the behavior of the function being modeled. There are many areas of applicaitons for GP. The two main applications are: (i) preditive modeling, (ii) optimization, (iii) uncertainty quantificatio. We will focus on the first two applications in the later sections. In predictive mdoeling we can use GPs to predict the value of a function at new points, taking into account the uncertainty of the prediction. GPs are particulary useful in spatial data analysis, where the correlation between data points is often related to their physical distance. Thus, GPs are quite often used for environmental modeling to analyze temperature or pollution levels, over geographical areas.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-arnol2006forgotten" class="csl-entry" role="listitem">
Arnol’d, Vladimir I. 2006. <span>“Forgotten and Neglected Theories of <span>P</span>oincar<span>é</span>.”</span> <em>Russian Mathematical Surveys</em> 61 (1): 1.
</div>
<div id="ref-cootner1967random" class="csl-entry" role="listitem">
Cootner, Paul H. 1967. <em>The Random Character of Stock Market Prices</em>. MIT press.
</div>
<div id="ref-davison2003statistical" class="csl-entry" role="listitem">
Davison, Anthony Christopher. 2003. <em>Statistical Models</em>. Vol. 11. Cambridge university press.
</div>
<div id="ref-kolmogoroff1931analytischen" class="csl-entry" role="listitem">
Kolmogoroff, Andrei. 1931. <span>“<span>Ü</span>ber Die Analytischen Methoden in Der Wahrscheinlichkeitsrechnung.”</span> <em>Mathematische Annalen</em> 104 (1): 415–58.
</div>
<div id="ref-Kol38" class="csl-entry" role="listitem">
Kolmogorov, A. N. 1938. <span>“On the Analytic Methods of Probability Theory.”</span> <em>Uspekhi Mat. Nauk</em>, no. 5: 5–41.
</div>
<div id="ref-logunov2004henri" class="csl-entry" role="listitem">
Logunov, A. A. 2004. <span>“Henri Poincare and Relativity Theory.”</span> <a href="https://arxiv.org/abs/physics/0408077">https://arxiv.org/abs/physics/0408077</a>.
</div>
<div id="ref-poincare1898mesure" class="csl-entry" role="listitem">
Poincaré, Henri. 1898. <span>“La Mesure Du Temps.”</span> <em>Revue de m<span>é</span>taphysique Et de Morale</em> 6 (1): 1–13.
</div>
<div id="ref-Shiryayev1992" class="csl-entry" role="listitem">
Shiryayev, A. N. 1992. <span>“On Analytical Methods in Probability Theory.”</span> In <em>Selected Works of a. N. Kolmogorov: Volume II Probability Theory and Mathematical Statistics</em>, edited by A. N. Shiryayev, 62–108. Dordrecht: Springer Netherlands.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../qmd/rct.html" class="pagination-link  aria-label=" &lt;span="" vs="" observational&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Field vs Observational</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../qmd/rl.html" class="pagination-link" aria-label="<span class='chapter-number'>10</span>&nbsp; <span class='chapter-title'>Reinforcement Learning</span>">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>