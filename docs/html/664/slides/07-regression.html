<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-29e2c20b02301cfff04dc8050bf30c7e.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.37">

  <meta name="author" content="Vadim Sokolov   George Mason University   Spring 2025">
  <title>Bayes AI</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-bbe7401fe57d4b791b917637bb662036.css">
  <link rel="stylesheet" href="style.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title" style="font-size:40px;">Bayes AI</h1>
  <p class="subtitle" style="color:blue;font-size:50px;">Unit 7: Bayesian Regression: Linear and Bayesian Trees</p>
<img style="height: 350px;" src="fig/page/08-regression.jpg">
  <p class="author">Vadim Sokolov <br> George Mason University <br> Spring 2025</p>

<p style="font-size:10px;"> 
<a href="https://vsokolov.org/courses/664.html">Course Page</a>, <a href="https://vsokolov.org/html/664/slides/">Slides</a>
</p>


</section>
<section>
<section id="temporal-data-filtering-event-detection-pandemics" class="title-slide slide level1 center">
<h1>Temporal Data: Filtering, Event Detection, Pandemics</h1>

</section>
<section id="example-history-of-pandemics" class="slide level2 smaller">
<h2>Example: History of Pandemics</h2>
<p><em>Bill Gates: 12/11/2009: “I’m most worried about a worldwide Pandemic”</em></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Early-period Pandemics</th>
<th style="text-align: left;">Dates</th>
<th style="text-align: left;">Size of Mortality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Plague of Athens</td>
<td style="text-align: left;">430 BC</td>
<td style="text-align: left;">25% population.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Black Death</td>
<td style="text-align: left;">1347</td>
<td style="text-align: left;">30% Europe</td>
</tr>
<tr class="odd">
<td style="text-align: left;">London Plague</td>
<td style="text-align: left;">1666 2</td>
<td style="text-align: left;">0% population</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: left;">Recent Flu Epidemics</th>
<th style="text-align: center;">Dates 1900-2010</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Spanish Flu</td>
<td style="text-align: center;">1918-19 40-</td>
<td style="text-align: center;">50 million</td>
</tr>
<tr class="even">
<td style="text-align: left;">Asian Flu</td>
<td style="text-align: center;">H2N2, 1957-58 2</td>
<td style="text-align: center;">million</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hong Kong Flu</td>
<td style="text-align: center;">H3N2, 1968-69 6</td>
<td style="text-align: center;">million</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Spanish Flu killed more than WW1</p>
<p>H1N1 Flu 2009: <span class="math inline">\(18,449\)</span> people killed World wide:</p>
</section>
<section id="seir-epidemic-models" class="slide level2">
<h2>SEIR Epidemic Models</h2>
<p>Growth <em>“self-reinforcing”</em>: More likely if more infectants</p>
<ul>
<li><p>An individual comes into contact with disease at rate <span class="math inline">\(\beta_1\)</span></p></li>
<li><p>The susceptible individual contracts the disease with probability <span class="math inline">\(\beta_2\)</span></p></li>
<li><p>Each infectant becomes infectious with rate <span class="math inline">\(\alpha\)</span> per unit time</p></li>
<li><p>Each infectant recovers with rate <span class="math inline">\(\gamma\)</span> per unit time</p></li>
</ul>
<p><span class="math inline">\(S_t + E_t + I_t + R_t = N\)</span></p>
</section>
<section id="current-models-seir" class="slide level2">
<h2>Current Models: SEIR</h2>
<p>susceptible-exposed-infectious-recovered model</p>
<p>Dynamic models that extend earlier models to include exposure and recovery.</p>
<p>The coupled SEIR model:<br>
<span class="math inline">\(\dot{S} = -\beta S I\)</span><br>
<span class="math inline">\(\dot{E} = \beta S I - \alpha E\)</span><br>
<span class="math inline">\(\dot{I} = \alpha E -\gamma I\)</span><br>
<span class="math inline">\(\dot{R} = \gamma I\)</span><br>
</p>
</section>
<section id="infectious-disease-models" class="slide level2">
<h2>Infectious disease models</h2>
<p>Daniel Bernoulli’s (1766) first model of disease transmission in smallpox:</p>
<p><em>“I wish simply that, in matters which so closely concern the well being of the human race, no decision shall be made without all knowledge which a little analysis and calculation can provide”</em></p>
<ul>
<li>R.A. Ross, (Nobel Medicine winner, 1902) – math model of malaria transmission, which ultimately lead to malaria control.</li>
</ul>
<p><em>Ross-McDonald model</em></p>
<ul>
<li>Kermack and McKendrick: susceptible-infectious-recovered (SIR)</li>
</ul>
<p>London Plague 1665-1666; Cholera: London 1865, Bombay, 1906.</p>
</section>
<section id="example-london-plague-1666-village-eyam-nr.-sheffield" class="slide level2">
<h2>Example: London Plague, 1666: Village Eyam nr. Sheffield</h2>
<p>Model of transmission from Infectants, <span class="math inline">\(I\)</span>, to susceptibles, <span class="math inline">\(S\)</span>.</p>
<table class="caption-top">
<thead>
<tr class="header">
<th>Date 1666 Su</th>
<th>sceptibles In</th>
<th style="text-align: left;">fectives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial</td>
<td>254</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td>July 3</td>
<td>235 15</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>July 19</td>
<td>201 22</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Aug 3 1</td>
<td>53 29</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>Aug 19</td>
<td>121 21</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Sept 3</td>
<td>108 8</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td>Sept 19</td>
<td>97 8</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td>Oct 3</td>
<td>–</td>
<td style="text-align: left;">–</td>
</tr>
<tr class="odd">
<td>Oct 19</td>
<td>83 0</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Initial Population <span class="math inline">\(N=261=S_0\)</span>; Final population <span class="math inline">\(S_\infty = 83\)</span>.</p>
</section>
<section id="modeling-growth-si" class="slide level2">
<h2>Modeling Growth: SI</h2>
<p>Coupled Differential eqn <span class="math inline">\(\dot{S} = - \beta SI , \dot{I} = ( \beta S - \alpha ) I\)</span></p>
<ul>
<li>Estimates <span class="math inline">\(\frac{\beta}{\alpha} = 6.54 \times 10^{-3} , \frac{\alpha}{\beta} = 1.53\)</span>.</li>
</ul>
<p><span class="math display">\[
\frac{ \hat{\beta} }{\alpha} = \frac{  \ln ( S_0 / S_\infty ) }{S_0 - S_\infty}
\]</span> Predicted maximum <span class="math inline">\(30.4\)</span>, very close to observed 29</p>
<p>Key: <span class="math inline">\(S\)</span> and <span class="math inline">\(I\)</span> are observed and <span class="math inline">\(\alpha , \beta\)</span> are estimated in <em>hindsight</em></p>
</section>
<section id="transmission-rates-r_0-for-1918-episode" class="slide level2">
<h2>Transmission Rates <span class="math inline">\(R_0\)</span> for 1918 Episode</h2>
<ul>
<li>1918-19 influenza pandemic:<br>
</li>
</ul>
<table class="caption-top">
<tbody>
<tr class="odd">
<td style="text-align: left;">Mills et al.&nbsp;2004:</td>
<td style="text-align: left;">45 US cities</td>
<td style="text-align: left;">3 (2-4)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Viboud et al.&nbsp;2006:</td>
<td style="text-align: left;">England and Wales</td>
<td style="text-align: left;">1.8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Massad et al.&nbsp;2007:</td>
<td style="text-align: left;">Sao Paulo Brazil</td>
<td style="text-align: left;">2.7</td>
</tr>
<tr class="even">
<td style="text-align: left;">Nishiura, 2007:</td>
<td style="text-align: left;">Prussia, Germany</td>
<td style="text-align: left;">3.41</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Chowell et al., 2006:</td>
<td style="text-align: left;">Geneva, Switzerland</td>
<td style="text-align: left;">2.7-3.8</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chowell et al., 2007:</td>
<td style="text-align: left;">San Francisco</td>
<td style="text-align: left;">2.7-3.5</td>
</tr>
</tbody>
</table>
<p>The larger the <span class="math inline">\(R_0\)</span> the more severe the epidemic.</p>
<p>Transmission parameters vary substantially from epidemic to epidemic</p>
</section>
<section id="boat-localization-example" class="slide level2 smaller">
<h2>Boat Localization Example</h2>
<p>Localization with measurement update</p>
<ul>
<li><p>A boat sails from one island to another</p></li>
<li><p>Boat is trying to identify its location <span class="math inline">\(\theta \sim N(m_0, C_0)\)</span></p></li>
<li><p>Using a sequence of measurements to one of the islands <span class="math inline">\(x_1,\ldots,x_n\)</span></p></li>
</ul>
<p>Measurements are noisy due to dilution of precision <a href="http://www.sailingmates.com/your-gps-can-kill-you/" class="uri">http://www.sailingmates.com/your-gps-can-kill-you/</a></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="fignick/GooglePointsAndErrors.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="fignick/gps-sat.png"></p>
</div>
</div>
</div>
</section>
<section id="reckoning" class="slide level2">
<h2>Reckoning</h2>
<p>Localization with no measurement updates is called reckoning</p>

<img data-src="fignick/traverse-board-2.jpg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">source: http://www.hakaimagazine.com/article-short/traversing-seas</p></section>
<section id="kalman-filter" class="slide level2">
<h2>Kalman Filter</h2>
<p><span class="math display">\[
\theta \sim N(m_0, C_0)
\]</span> <span class="math display">\[
x_t = \theta + w_t,~~~w_t \sim N(0,\sigma^2)
\]</span> <span class="math display">\[
x_1,x_2,\ldots \mid \theta \sim N(\theta,\sigma^2)
\]</span> The prior variance <span class="math inline">\(C_0\)</span> might be quite large if you are very uncertain about your guess <span class="math inline">\(m_0\)</span><br>
Given the measurements <span class="math inline">\(x^n = (x_1,\ldots,x_n)\)</span>, you update your opinion about <span class="math inline">\(\theta\)</span> computing its posterior density, using the Bayes formula</p>
</section>
<section id="normal-model" class="slide level2">
<h2>Normal Model</h2>
<p><span class="math display">\[
f(x) = \dfrac{1}{\sqrt{2 \pi \sigma^2}} \exp^{-\dfrac{1}{2}\dfrac{(x-\mu)^2}{\sigma^2}}
\]</span> Or multivariate equivalent <span class="math display">\[
f(x) = (2 \pi)^{-k/2} |\Sigma|^{-1/2}\exp^{-0.5(x-\mu)^T\Sigma^{-1}(x-\mu)}
\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a></a><span class="fu">curve</span>(<span class="fu">exp</span>(<span class="sc">-</span>x<span class="sc">*</span>x),<span class="sc">-</span><span class="dv">2</span>,<span class="dv">2</span>, <span class="at">bty=</span><span class="st">"newdataframe &lt;- na.omit(dataframe)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="07-regression_files/figure-revealjs/normal-1.png" width="960" class="r-stretch"></section>
<section id="the-conjugate-prior-for-the-normal-distribution" class="slide level2">
<h2>The Conjugate Prior for the Normal Distribution</h2>
<p>We will look at the Gaussian distribution from a Bayesian point of view. In the standard form, the likelihood has two parameters, the mean <span class="math inline">\(\mu\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
p(x^n | \mu, \sigma^2) \propto \dfrac{1}{\sigma^n}\exp\left(-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right)
\]</span></p>
</section>
<section id="normal-prior" class="slide level2">
<h2>Normal Prior</h2>
<p>In case when we know the variance <span class="math inline">\(\sigma^2\)</span>, but do not know mean <span class="math inline">\(\mu\)</span>, we assume <span class="math inline">\(\mu\)</span> is random. To have conjugate prior we choose <span class="math display">\[
p(\mu | \mu_0, \sigma_0) \propto \dfrac{1}{\sigma_0}\exp\left(-\dfrac{1}{2\sigma_0^2}(\mu-\mu_0^2)\right)
\]</span> In practice, when little is known about <span class="math inline">\(\mu\)</span>, it is common to set the location hyper-parameter to zero and the scale to some large value.</p>
</section>
<section id="normal-model-with-unknown-mean-known-variance" class="slide level2">
<h2>Normal Model with Unknown Mean, Known Variance</h2>
<p>Suppose we wish to estimate a model where the likelihood of the data is normal with an unknown mean <span class="math inline">\(\mu\)</span> and a known variance <span class="math inline">\(\sigma^2\)</span>.<br>
Our parameter of interest is <span class="math inline">\(\mu\)</span>. We can use a conjugate Normal prior on <span class="math inline">\(\mu\)</span>, with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma_0^2\)</span>. <span class="math display">\[
\begin{aligned}
p(\mu| x^n, \sigma^2) &amp; \propto p(x^n | \mu, \sigma^2)p(\mu) ~~~\mbox{(Bayes rule)}\\
N(\mu_1,\tau_1)     &amp; = N(\mu, \sigma^2)\times N(\mu_0, \sigma_0^2)
\end{aligned}
\]</span></p>
</section>
<section id="useful-identity" class="slide level2 smaller">
<h2>Useful Identity</h2>
<p>One of the most useful algebraic tricks for calculating posterior distribution is <strong>completing the square</strong>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><span class="math display">\[
\dfrac{(x-\mu_1)^2}{\sigma_1} + \dfrac{(x-\mu_2)^2}{\sigma_2} = \dfrac{(x - \mu_3)^2}{\sigma_3} + \dfrac{(\mu_1-\mu_2)^2}{\sigma_1 + \sigma_2}
\]</span> where <span class="math display">\[
\mu_3 = \sigma_3 (\mu_1/\sigma_1 + \mu_2/\sigma_2)
\]</span> and <span class="math display">\[
\sigma_3 = (1/\sigma_1 + 1/\sigma_2)^{-1}
\]</span></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Prior: <span class="math display">\[
\theta \sim \frac{e^{-\frac{(\theta-\mu )^2}{2 \sigma ^2}}}{\sqrt{2 \pi } \sigma }
\]</span> Likelihood: <span class="math display">\[
x \mid \theta \sim \frac{e^{-\frac{(\theta-y )^2}{2 r^2}}}{\sqrt{2 \pi } r}
\]</span> Posterior mean: <span class="math display">\[
\frac{x  \sigma ^2+\mu  r^2}{r^2+\sigma ^2}
\]</span> Posterior variance: <span class="math display">\[
\frac{1}{\frac{1}{r^2}+\frac{1}{\sigma ^2}}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="prior-likelihood-posterior" class="slide level2">
<h2>Prior, Likelihood, Posterior</h2>

<img data-src="fig/prior-meas-post.svg" style="width:80.0%" class="r-stretch"></section>
<section id="after-n-steps" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p><span class="math display">\[
\begin{aligned}
p(\mu  | x^n) &amp; \propto \prod_{i=1}^{n}\dfrac{1}{\sqrt{2\pi \sigma^2}}\exp\left(-\frac{(x_i - \mu )^2}{2\sigma^2}\right)\times\dfrac{1}{\sqrt{2\pi \sigma_0^2}}\exp\left(-\frac{(\mu  - \mu_0)^2}{2\sigma_0^2}\right)\\
&amp; \propto \exp\left(-\sum_{i=1}^{n}\frac{(x_i - \mu )^2}{2\sigma^2} - \frac{(\mu  - \mu_0)^2}{2\sigma_0^2}\right)\\
&amp; = \exp\left(-\dfrac{1}{2}\left[\sum_{i=1}^{n}\frac{(x_i - \mu )^2}{\sigma^2} + \frac{(\mu  - \mu_0)^2}{\sigma_0^2}\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i - \mu )^2 + \sigma^2 (\mu  - \mu_0)^2\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i^2 - 2\mu x_i+ \mu ^2) + \sigma^2 (\mu ^2 - 2\mu \mu_0 + \mu_0^2)\right]\right)\\\end{aligned}
\]</span></p>
</section>
<section id="after-n-steps-1" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p>We can multiply the <span class="math inline">\(2\mu x_i\)</span> term in the summation by <span class="math inline">\(n/n\)</span> in order to get the equations in terms of the sufficient statistic <span class="math inline">\(\bar{x}^n\)</span> <span class="math display">\[
\begin{aligned}
p(\mu  | x^n) &amp; \propto \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}(x_i^2 - \dfrac{n}{n}2\mu x_i+ \mu ^2) + \sigma^2 (\mu ^2 - 2\mu \mu_0 + \mu_0^2)\right]\right)\\
&amp; = \exp\left(-\dfrac{1}{2\sigma^2 \sigma_0^2}\left[\sigma_0^2\sum_{i=1}^{n}x_i^2 - \sigma_0^22\mu n\bar{x}^n+ \tau_n^0n\mu ^2 + \sigma^2 \mu ^2 - 2\mu \mu_0\sigma^2 + \mu_0^2\sigma^2\right]\right)\end{aligned}
\]</span> set <span class="math inline">\(k = \sigma_0^2\sum_{i=1}^{n}x_i^2 + \mu_0^2\sigma^2\)</span> (they do not contain <span class="math inline">\(\mu\)</span>) <span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left[\mu ^2\left(\dfrac{1}{\sigma_0^2} + \dfrac{n}{\sigma^2}\right) - 2\mu\left(\dfrac{\mu_0}{\sigma_0^2} + \dfrac{n\bar{x}^n}{\sigma^2}\right) + k\right]\right)
\]</span></p>
</section>
<section id="after-n-steps-2" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<p>Let’s multiply by <span class="math display">\[
\dfrac{1/\sigma_0^2 + n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}
\]</span> Now <span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left(1/\sigma_0^2 + n/\sigma^2\right)\left(\mu  - \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\right)^2\right)
\]</span></p>
<p><span class="math display">\[
p(\mu  | x^n)   \propto \exp\left(-\dfrac{1}{2}\left(1/\sigma_0^2 + n/\sigma^2\right)\left(\mu  - \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\right)^2\right)
\]</span></p>
</section>
<section id="after-n-steps-3" class="slide level2 smaller">
<h2>After <span class="math inline">\(n\)</span> steps</h2>
<ul>
<li><p>Posterior mean: <span class="math inline">\(\mu_n  =  \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\)</span></p></li>
<li><p>Posterior variance: <span class="math inline">\(\sigma_n^2 = \left(1/\sigma_0^2 + n/\sigma^2\right)^{-1}\)</span></p></li>
<li><p>Posterior precision:: <span class="math inline">\(\tau_n^2 = 1/\sigma_0^2 + n/\sigma^2\)</span></p></li>
</ul>
<p>Posterior Precision is just the sum of the prior precision and the data precision.</p>
</section>
<section id="posterior-mean" class="slide level2">
<h2>Posterior Mean</h2>
<p><span class="math display">\[
\begin{aligned}
\mu_n  &amp; =  \dfrac{\mu_0/\sigma_0^2 + n\bar{x}^n/\sigma^2}{1/\sigma_0^2 + n/\sigma^2}\\
&amp; = \dfrac{\mu_0\sigma^2}{\sigma^2 + n\sigma_0^2} + \dfrac{\sigma_0^2n\bar{x}^n}{\sigma^2 + n\sigma_0^2}\end{aligned}
\]</span></p>
<ul>
<li><p>As <span class="math inline">\(n\)</span> increases, data mean dominates prior mean.</p></li>
<li><p>As <span class="math inline">\(\sigma_0^2\)</span> decreases (less prior variance, greater prior precision), our prior mean becomes more important.</p></li>
</ul>
</section>
<section id="a-state-space-model" class="slide level2">
<h2>A state space model</h2>
<p>A state space model consists of two equations: <span class="math display">\[
\begin{aligned}
Z_t&amp;=HS_t+w_t\\
S_{t+1} &amp;= FS_t + v_t\end{aligned}
\]</span> where <span class="math inline">\(S_t\)</span> is a state vector of dimension <span class="math inline">\(m\)</span>, <span class="math inline">\(Z_t\)</span> is the observed time series, <span class="math inline">\(F\)</span>, <span class="math inline">\(G\)</span>, <span class="math inline">\(H\)</span> are matrices of parameters, <span class="math inline">\(\{w_t\}\)</span> and <span class="math inline">\(\{v_t\}\)</span> are <span class="math inline">\(iid\)</span> random vectors satisfying <span class="math display">\[
\mbox{E}(w_t)=0, \hspace{0.5cm} \mbox{E}(v_t)=0, \hspace{0.5cm}\mathrm{cov}(v_t)=V, \hspace{0.5cm} \mathrm{cov}(w_t)=W
\]</span> and <span class="math inline">\(\{w_t\}\)</span> and <span class="math inline">\(\{v_t\}\)</span> are independent.</p>
</section>
<section id="state-space-models" class="slide level2">
<h2>State Space Models</h2>
<ul>
<li><p>State space models consider a time series as the output of a dynamic system perturbed by random disturbances.</p></li>
<li><p>Natural interpretation of a time series as the combination of several components, such as trend, seasonal or regressive components.</p></li>
<li><p>Computations can be implemented by recursive algorithms.</p></li>
</ul>
</section>
<section id="types-of-inference" class="slide level2">
<h2>Types of Inference</h2>
<ul>
<li><p>Model building versus inferring unknown variable. Assume a linear model <span class="math inline">\(Z = HS + \epsilon\)</span></p></li>
<li><p>Model building: know signal <span class="math inline">\(S\)</span>, observe <span class="math inline">\(Z\)</span>, infer <span class="math inline">\(H\)</span> (a.k.a. model identification, learning)</p></li>
<li><p>Estimation: know <span class="math inline">\(H\)</span>, observe <span class="math inline">\(Z\)</span>, estimate <span class="math inline">\(S\)</span></p></li>
<li><p>Hypothesis testing: unknown takes one of few possible values; aim at small probability of incorrect decision</p></li>
<li><p>Estimation: aim at a small estimation error</p></li>
</ul>
</section>
<section id="time-series-estimation-tasks" class="slide level2">
<h2>Time Series Estimation Tasks</h2>
<ul>
<li><p>Filtering: To recover the state vector <span class="math inline">\(S_t\)</span> given <span class="math inline">\(Z^t\)</span></p></li>
<li><p>Prediction: To predict <span class="math inline">\(S_{t+h}\)</span> or <span class="math inline">\(Z_{t+h}\)</span> for <span class="math inline">\(h &gt; 0\)</span>, given <span class="math inline">\(Z^t\)</span></p></li>
<li><p>Smoothing: To estimate <span class="math inline">\(S_t\)</span> given <span class="math inline">\(Z^T\)</span> , where <span class="math inline">\(T &gt; t\)</span></p></li>
</ul>
</section>
<section id="property-of-multivariate-normal" class="slide level2">
<h2>Property of Multivariate Normal</h2>
<p>Under normality, we have</p>
<ul>
<li><p>that normal prior plus normal likelihood results in a normal posterior,</p></li>
<li><p>that if the random vector <span class="math inline">\((X, Y )\)</span> are jointly normal</p></li>
</ul>
<p><span class="math display">\[
\begin{bmatrix}
X\\
Y
\end{bmatrix}
\sim N\left(
\begin{bmatrix}
\mu_x\\
\mu_y
\end{bmatrix},
\begin{bmatrix}
\Sigma_{xx}&amp;\Sigma_{xy}\\
\Sigma_{yx}&amp;\Sigma_{yy}\\
\end{bmatrix}\right),
\]</span></p>
<ul>
<li>then the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is normal</li>
</ul>
<p><span class="math display">\[
X|Y = y\sim N\left[\mu_x+\Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y),\Sigma_{xx}-\Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}\right].
\]</span></p>
</section>
<section id="from-state-space-model" class="slide level2">
<h2>From State Space Model</h2>
<p><span class="math display">\[
\begin{aligned}
S_{t+1}^t &amp;= FS_t\\
Z_{t+1}^t &amp;=HS_{t+1}^t\\
P_{t+1}^t&amp;=FP_tF^T+GQG^T\\
V_{t+1}^t&amp;=HP_{t+1}^tH^T+R\\
C_{t+1}^t&amp;=HP_{t+1}^t
\end{aligned}
\]</span></p>
<ul>
<li><p><span class="math inline">\(P_{t+j}^t\)</span> = conditional covariance matrix of <span class="math inline">\(S_{t+j}\)</span> given <span class="math inline">\(\{Z_t , Z_{t-1} , \cdots\}\)</span> for <span class="math inline">\(j \geq 0\)</span></p></li>
<li><p><span class="math inline">\(S_{t+j}^t\)</span> = conditional mean of <span class="math inline">\(S_{t+j}\)</span> given <span class="math inline">\(\{Z_t , Z_{t-1} , \cdots\}\)</span></p></li>
<li><p><span class="math inline">\(V_{t+1}^t\)</span> = conditional variance of <span class="math inline">\(Z_{t+1}\)</span> given <span class="math inline">\(Z^t = \{Z_t , Z_{t-1} , \cdots\}\)</span></p></li>
<li><p><span class="math inline">\(C_{t+1}^t\)</span> = conditional covariance between <span class="math inline">\(Z_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></p></li>
</ul>
</section>
<section id="joint-conditional-distribution-ps_t1-z_t1-zt" class="slide level2">
<h2>Joint conditional distribution <span class="math inline">\(P(S_{t+1}, Z_{t+1} | Z^t)\)</span></h2>
<p><span class="math display">\[
\begin{bmatrix}
S_{t+1}\\
Z_{t+1}
\end{bmatrix}_t
\sim N
\left( \begin{bmatrix}
S_{t+1}^t\\
Z_{t+1}^t
\end{bmatrix},
\begin{bmatrix}
P_{t+1}^t &amp; P_{t+1}^tH'\\
HP_{t+1}^t &amp; HP_{t+1}^tH'+R
\end{bmatrix} \right)
\]</span></p>
</section>
<section id="ps_t1-z_t1" class="slide level2 smaller">
<h2><span class="math inline">\(P(S_{t+1}| Z_{t+1})\)</span></h2>
<p>Finally, when <span class="math inline">\(Z_{t+1}\)</span> becomes available, we may use the property of nromality to update the distribution of <span class="math inline">\(S_{t+1}\)</span> . More specifically, <span class="math display">\[
S_{t+1}=S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)
\]</span> and <span class="math display">\[
P_{t+1}=P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH'+R]^{-1}HP_{t+1}^t.
\]</span> Predictive residual: <span class="math display">\[
R_{t+1}^t=Z_{t+1}-Z_{t+1}^t=Z_{t+1}-HS_{t+1}^t \ne 0
\]</span> means there is new information about the system so that the state vector should be modified. The contribution of <span class="math inline">\(r_{t+1}^t\)</span> to the state vector, of course, needs to be weighted by the variance of <span class="math inline">\(r_{t+1}^t\)</span> and the conditional covariance matrix of <span class="math inline">\(S_{t+1}\)</span>.</p>
</section>
<section id="kalman-filter-1" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>Predict: <span class="math display">\[
\begin{aligned}
S_{t+1}^t &amp;= FS_t\\
Z_{t+1}^t &amp;=HS_{t+1}^t\\
P_{t+1}^t&amp;=FP_tF^T+GQG^T\\
V_{t+1}^t&amp;=HP_{t+1}^tH^T+R
\end{aligned}
\]</span></p></li>
<li><p>Update: <span class="math display">\[
\begin{aligned}
S_{t+1|t+1}=&amp; S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\\
P_{t+1|t+1}=&amp; P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}HP_{t+1}^t
\end{aligned}
\]</span></p></li>
</ul>
</section>
<section id="kalman-filter-2" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>starts with initial prior information <span class="math inline">\(S_{0}\)</span> and <span class="math inline">\(P_{0}\)</span></p></li>
<li><p>predicts <span class="math inline">\(Z_{1}^0\)</span> and <span class="math inline">\(V_{1}^0\)</span></p></li>
<li><p>Once the observation <span class="math inline">\(Z_1\)</span> is available, uses the updating equations to compute <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(P_{1}\)</span></p></li>
</ul>
<p><span class="math inline">\(S_{1|1}\)</span> and <span class="math inline">\(P_{1|1}\)</span> is the prior for the next observation.</p>
<p>This is the <span style="color: red">Kalman recusion</span>.</p>
</section>
<section id="kalman-filter-3" class="slide level2">
<h2>Kalman filter</h2>
<ul>
<li><p>effect of the initial values <span class="math inline">\(S_{0}\)</span> and <span class="math inline">\(P_{0}\)</span> is decresing as <span class="math inline">\(t\)</span> increases</p></li>
<li><p>for a stationary time series, all eigenvalues of the coefficient matrix <span class="math inline">\(F\)</span> are less than one in modulus</p></li>
<li><p>Kalman filter recursion ensures that the effect of the initial values indeed vanishes as <span class="math inline">\(t\)</span> increases</p></li>
<li><p>uncertainty about the state is always normal</p></li>
</ul>
</section>
<section id="local-trend-model" class="slide level2">
<h2>Local Trend Model</h2>
<p><span class="math display">\[
\begin{aligned}
y_t =&amp; \mu_t + e_t,~e_t \sim N(0,\sigma_e^2)\\\
\mu_{t+1} =&amp; \mu_t + \eta_t,~ \eta_t \sim N(0,\sigma_{\eta}^2 )
\end{aligned}
\]</span></p>
<ul>
<li><p><span class="math inline">\(\{e_t\}\)</span> and <span class="math inline">\(\{\eta_t\}\)</span> are iid Gaussian white noise</p></li>
<li><p><span class="math inline">\(\mu_0\)</span> is given (possible as a distributed value)</p></li>
<li><p>trend <span class="math inline">\(\mu_t\)</span> is not observable</p></li>
<li><p>we observe some noisy version of the trend <span class="math inline">\(y_t\)</span></p></li>
<li><p>such a model can be used to analyze realized volatility: <span class="math inline">\(\mu_t\)</span> is the log volatility and <span class="math inline">\(y_t\)</span> is constructed from high frequency transactions data</p></li>
</ul>
</section>
<section id="local-trend-model-1" class="slide level2">
<h2>Local Trend Model</h2>
<p><span class="math display">\[
\begin{aligned}
y_t =&amp; \mu_t + e_t,~e_t \sim N(0,\sigma_e^2)\\\
\mu_{t+1} =&amp; \mu_t + \eta_t,~ \eta_t \sim N(0,\sigma_{\eta}^2 )
\end{aligned}
\]</span></p>
<ul>
<li><p>if <span class="math inline">\(\sigma_e=0\)</span>, then we have ARIMA(0,1,0) model</p></li>
<li><p>if <span class="math inline">\(\sigma_e &gt; 0\)</span>, then we have ARIMA(0,1,1) model, satisfying</p></li>
</ul>
<p><span class="math display">\[
(1-B)y_t = (1-\theta B)a_t, ~ a_t \sim N(0,\sigma_a^2)
\]</span> <span class="math inline">\(\sigma_a\)</span> and <span class="math inline">\(\theta\)</span> are determined by <span class="math inline">\(\sigma_e\)</span> and <span class="math inline">\(\sigma_{\eta}\)</span> <span class="math display">\[
(1-B)y_t = \eta_{t-1} + e_t - e_{t-1}
\]</span></p>
</section>
<section id="liner-regression-time-dependent-parameters" class="slide level2">
<h2>Liner Regression (time dependent parameters)</h2>
<p><span class="math display">\[
\begin{aligned}
y_t  &amp;= \alpha_t + \beta_t \, x_t + \epsilon_t   \qquad &amp; \epsilon_t \, \sim N(0,\sigma^2) \\
\alpha_t &amp;= \quad \alpha_{t-1}  + \epsilon_t^{\alpha} \qquad &amp; \epsilon_t^{\alpha} \sim N(0,\sigma_{\alpha}^2) \\
\beta_t  &amp;= \quad \beta_{t-1}   + \epsilon_t^{\beta}   \qquad &amp; \epsilon_t^{\beta} \sim N(0, \sigma_{\beta}^2) \\
\end{aligned}
\]</span></p>
<p>dlm Package</p>
<ul>
<li><p><code>dlmModARMA</code>: for an ARMA process, potentially multivariate</p></li>
<li><p><code>dlmModPoly</code>: for an <span class="math inline">\(n^{th}\)</span> order polynomial</p></li>
<li><p><code>dlmModReg</code> : for Linear regression</p></li>
<li><p><code>dlmModSeas</code>: for periodic – Seasonal factors</p></li>
<li><p><code>dlmModTrig:</code> for periodic – Trigonometric form</p></li>
</ul>
</section>
<section id="local-linear-trend" class="slide level2">
<h2>Local Linear Trend</h2>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= \qquad \quad \mu_t  + \upsilon_t  \quad &amp;\upsilon_t \sim N(0,V) \\
\mu_t &amp;= \mu_{t-1}  + \delta_{t-1} + \omega_t^{\mu} \quad &amp; \omega_t^{\mu} \sim N(0,W^{\mu}) \\
\delta_t &amp;= \qquad \,\, \, \delta_{t-1} + \omega_t^{\delta} \quad &amp; \omega_t^{\delta} \sim N(0,W^{\delta}) \\
\end{aligned}
\]</span></p>
</section>
<section id="simple-exponential-smoothing-with-additive-errors" class="slide level2">
<h2>Simple exponential smoothing with additive errors</h2>
<p><span class="math display">\[
x_t = \ell_{t-1} + \varepsilon_t
\]</span> <span class="math display">\[
\ell_t=\ell_{t-1}+\alpha \varepsilon_t.
\]</span></p>
</section>
<section id="holts-linear-method-with-additive-errors" class="slide level2">
<h2>Holt’s linear method with additive errors</h2>
<p><span class="math display">\[
\begin{aligned}
y_t&amp;=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
\ell_t&amp;=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
b_t&amp;=b_{t-1}+\beta \varepsilon_t, \end{aligned}
\]</span></p>
</section>
<section id="relation-to-arma-models" class="slide level2">
<h2>Relation to ARMA models</h2>
<p>Consider relation with ARMA models. The basic relations are</p>
<ul>
<li><p>an ARMA model can be put into a state space form in “infinite" many ways;</p></li>
<li><p>for a given state space model in, there is an ARMA model.</p></li>
</ul>
</section>
<section id="state-space-model-to-arma-model" class="slide level2">
<h2>State space model to ARMA model</h2>
<p>The second possibility is that there is an observational noise. Then, the same argument gives <span class="math display">\[
(1+\alpha_1B+\cdots+\alpha_mB^m)(Z_{t+m}-\epsilon_{t+m})=(1-\theta_1B-\cdots -\theta_{m-1}B^{m-1})a_{t+m}
\]</span> By combining <span class="math inline">\(\epsilon_t\)</span> with <span class="math inline">\(a_t\)</span> , the above equation is an ARMA<span class="math inline">\((m, m)\)</span> model.</p>
</section>
<section id="arma-model-to-state-space-model-ar2" class="slide level2">
<h2>ARMA model to state space model: AR(2)</h2>
<p><span class="math display">\[
Z_t=\phi_1Z_{t-1}+\phi_2Z_{t-2}+a_t
\]</span> For such an AR(2) process, to compute the forecasts, we need <span class="math inline">\(Z_{t-1}\)</span> and <span class="math inline">\(Z_{t-2}\)</span> . Therefore, it is easily seen that <span class="math display">\[
\begin{bmatrix}
Z_{t+1}\\
Z_t
\end{bmatrix}
=
\begin{bmatrix}
\phi_1 &amp; \phi_2\\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
Z_t\\
Z_{t-1}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0
\end{bmatrix}
e_t,
\]</span> where <span class="math inline">\(e_t = a_{t+1}\)</span> and <span class="math display">\[
Z_t=[1, 0]S_t
\]</span> where <span class="math inline">\(S_t = (Z_t , Z_{t-1})^T\)</span> and there is no observational noise.</p>
</section>
<section id="arma-model-to-state-space-model-ma2" class="slide level2">
<h2>ARMA model to state space model: MA(2)</h2>
<p><span class="math display">\[
Z_t=a_t-\theta_1a_{t-1}-\theta_2a_{t-2}
\]</span> <u>Method 1:</u><br>
<span class="math display">\[
\begin{bmatrix}
a_t\\
a_{t-1}
\end{bmatrix}
=
\begin{bmatrix}
0 &amp; 0\\
1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a_{t-1}\\
a_{t-2}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0
\end{bmatrix}
a_t
\]</span></p>
<p><span class="math display">\[
Z_t=[-\theta_1, -\theta_2]S_{t-1} + a_t
\]</span> Here the innovation <span class="math inline">\(a_t\)</span> shows up in both the state transition equation and the observation equation. The state vector is of dimension 2.<br>
</p>
</section>
<section id="arma-model-to-state-space-model-ma2-1" class="slide level2">
<h2>ARMA model to state space model: MA(2)</h2>
<p><u>Method 2</u>: For an MA(2) model, we have <span class="math display">\[
\begin{aligned}
Z_{t}^t&amp;=Z_t\\
Z_{t+1}^t&amp;=-\theta_1a_t-\theta_2a_{t-1}\\
Z_{t+2}^t&amp;= -\theta_2a_t
\end{aligned}
\]</span> Let <span class="math inline">\(S_t = (Z_t , -\theta_1 a_t - \theta_2 a_{t-1} , -\theta_2 a_t )^T\)</span> . Then, <span class="math display">\[
S_{t+1}=
\begin{bmatrix}
0 &amp; 1&amp; 0\\
0&amp; 0&amp; 1\\
0&amp; 0&amp; 0
\end{bmatrix}
S_t+
\begin{bmatrix}
1\\
-\theta_1\\
-\theta_2
\end{bmatrix}
a_{t+1}
\]</span> and <span class="math display">\[
Z_t=[1,0,0]S_t
\]</span> Here the state vector is of dimension 3, but there is no observational noise.</p>
</section>
<section id="arma-model-to-state-space-model-akaikes-approach" class="slide level2">
<h2>ARMA model to state space model: Akaike’s approach</h2>
<p>Consider ARMA<span class="math inline">\((p, q)\)</span> process, let <span class="math inline">\(m = max\{p, q + 1\}\)</span>, <span class="math inline">\(\phi_i = 0\)</span> for <span class="math inline">\(i &gt; p\)</span> and <span class="math inline">\(\theta_j = 0\)</span> for <span class="math inline">\(j &gt; q\)</span>. <span class="math display">\[
S_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\cdots , Z_{t+m-1}^t )^T
\]</span> where <span class="math inline">\(Z_{t+\ell}^t\)</span> is the conditional expectation of <span class="math inline">\(Z_{t+\ell}\)</span> given <span class="math inline">\(\Psi_t = \{Z_t , Z_{t-1} , \cdots\}\)</span>. By using the updating equation <span class="math inline">\(f\)</span> forecasts (recall what we discussed before) <span class="math display">\[
Z_{t+1}(\ell -1)=Z_t(\ell)+\psi_{\ell-1}a_{t+1},
\]</span></p>
</section>
<section id="arma-model-to-state-space-model-akaikes-approach-1" class="slide level2">
<h2>ARMA model to state space model: Akaike’s approach</h2>
<p><span class="math display">\[
S_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\cdots , Z_{t+m-1}^t )^T
\]</span> &nbsp; <span class="math display">\[
S_{t+1}=FS_t+Ga_{t+1}
\]</span></p>
<p><span class="math display">\[
Z_t=[1,0, \cdots ,0]S_t
\]</span> where <span class="math display">\[
F=
\left[
\begin{array}{c|cccc}
0 &amp;1&amp; 0&amp; \cdots&amp; 0\\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; &amp; \\
\phi_m &amp; \phi_{m-1} &amp; \cdots &amp; \phi_2 &amp; \phi_1
\end{array}\right], G=
\begin{bmatrix}
1\\
\psi_1\\
\psi_2\\
\vdots\\
\psi_{m-1}
\end{bmatrix}
\]</span> The matrix <span class="math inline">\(F\)</span> is call a companion matrix of the polynomial <span class="math inline">\(1 - \phi_1 B - \cdots - \phi_m B^m\)</span>.</p>
</section>
<section id="arma-model-to-state-space-model-aokis-method" class="slide level2 smaller">
<h2>ARMA model to state space model: Aoki’s Method</h2>
<p>Two-step procedure: First, consider the MA<span class="math inline">\((q)\)</span> part: <span class="math display">\[
W_t = a_t - \theta_1 a_{t-1} - \cdots - \theta_q a_{t-q}
\]</span> <span class="math display">\[
\begin{bmatrix}
a_t\\
a_{t-1}\\
\vdots\\
a_{t-q+1}
\end{bmatrix}
=
\begin{bmatrix}
0&amp;0&amp;\cdots &amp; 0&amp;0\\
1&amp;0&amp; \cdots&amp;0&amp;0\\
\vdots &amp; &amp; &amp; &amp;\\
0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a_{t-1}\\
a_{t-2}\\
\vdots\\
a_{t-q}
\end{bmatrix}
+
\begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}
a_t
\]</span></p>
<p><span class="math display">\[
W_t=[-\theta_1,-\theta_2, \cdots, -\theta_q]S_t+a_t
\]</span></p>
</section>
<section id="arma-model-to-state-space-model-aokis-method-1" class="slide level2">
<h2>ARMA model to state space model: Aoki’s Method</h2>
<p>First, consider the AR<span class="math inline">\((p)\)</span> part: <span class="math display">\[
Z_t = \phi_1 Z_{t-1} + ... + \phi_p Z_{t-p} + W_t
\]</span> Define state-space vector as <span class="math display">\[
S_t=(Z_{t-1},Z_{t-2}, \cdots , Z_{t-p},a_{t-1}, \cdots, a_{t-q})'
\]</span> Then, we have <span class="math display">\[
\begin{bmatrix}
Z-t\\
Z_{t-1}\\
\vdots\\
Z_{t-p+1}\\
a_t\\
a_{t-1}\\
\vdots\\
a_{t-q+1}
\end{bmatrix}
=
\left[
\begin{array}{cccc|cccc}
\phi_1&amp;\phi_2&amp;\cdots&amp;\phi_p&amp;-\theta_1&amp;-\theta_2&amp;\cdots&amp;-\theta_q\\
1&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
\vdots &amp;&amp;&amp;&amp;\vdots&amp;&amp;&amp;\\
0&amp;\cdots&amp;1&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;0&amp;\cdots&amp;0\\
0&amp;0&amp;\cdots&amp;0&amp;1&amp;0&amp;\cdots&amp;0\\
\vdots &amp;&amp;&amp;&amp;0&amp;&amp;&amp;\\
0&amp;0&amp;\cdots&amp;0&amp;0&amp;\cdots&amp;1&amp;0
\end{array}\right]
\begin{bmatrix}
Z_{t-1}\\
Z_{t-2}\\
\vdots\\
Z_{t-p}\\
a_{t-1}\\
a_{t-2}\\
\vdots\\
a_{t-q}
\end{bmatrix}+
\begin{bmatrix}
1\\
0\\
\vdots\\
0\\
1\\
0\\
\vdots\\
0
\end{bmatrix}a_t
\]</span> and <span class="math display">\[
Z_t=[\phi_1,\cdots,\phi_p,-\theta_1,\cdots,-\theta_q]S_t+a_t
\]</span></p>
</section>
<section id="mle-estimation" class="slide level2">
<h2>MLE Estimation</h2>
<p>Innovations are given by <span class="math display">\[
\epsilon_t = Z_t - HS_t^{t-1}
\]</span> can be shown that <span class="math inline">\(\mathrm{var}(\epsilon_t) = \Sigma_t\)</span>, where <span class="math display">\[
\Sigma_t = HP_t^{t-1}H^T + R
\]</span> Incomplete Data Likelihood: <span class="math display">\[
-\ln L(\Theta) = \dfrac{1}{2}\sum_{t=1}^{n}\log|\Sigma_t(\Theta)| + \dfrac{1}{2}\sum_{t=1}^{n}\epsilon_t(\Theta)^T\Sigma(\Theta)^{-1}\epsilon_t(\Theta)
\]</span> Here <span class="math inline">\(\Theta = (F, Q, R)\)</span>. Use BFGS to find a sequence of <span class="math inline">\(\Theta\)</span>’s and stop when stagnation happens.</p>
</section>
<section id="kalman-smoother" class="slide level2">
<h2>Kalman Smoother</h2>
<ul>
<li><p>Input: initial distribution <span class="math inline">\(X_0\)</span> and data <span class="math inline">\(Z_1,...,Z_T\)</span></p></li>
<li><p>Algorithm: forward-backward pass</p></li>
<li><p>Forward pass: Kalman filter: compute <span class="math inline">\(S_{t+1}^t\)</span> and <span class="math inline">\(S_{t+1}^{t+1}\)</span> for <span class="math inline">\(0 \le t &lt; T\)</span></p></li>
<li><p>Backward pass: Compute <span class="math inline">\(S_t^T\)</span> for <span class="math inline">\(0 \le t &lt; T\)</span></p></li>
</ul>
</section>
<section id="backward-pass" class="slide level2">
<h2>Backward Pass</h2>
<ul>
<li><p>Compute <span class="math inline">\(X_t^T\)</span> given <span class="math inline">\(S_{t+1}^T \sim N(m_{t+1}^T,C_{t+1}^T)\)</span></p></li>
<li><p>Reverse arrow: <span class="math inline">\(S_t^t \leftarrow X_{t+1}^t\)</span></p></li>
<li><p>Same as incorporating measurement in filter</p></li>
<li><p>Compute joint <span class="math inline">\((S_t^t, S_{t+1}^t)\)</span></p></li>
<li><p>Compute conditional <span class="math inline">\((S_t^t \mid S_{t+1}^t )\)</span></p></li>
<li><p>New: <span class="math inline">\(S_{t+1}\)</span> is not “known”, we only know its distribution: <span class="math inline">\(S_{t+1} \sim S_{t+1}^T\)</span></p></li>
<li><p>“Uncondition” on <span class="math inline">\(S_{t+1}\)</span> to compute <span class="math inline">\(S_t^T\)</span> using laws of total expectation and variance</p></li>
</ul>
</section>
<section id="kalman-smoother-1" class="slide level2">
<h2>Kalman Smoother</h2>
<p>A smoothed version of data (an estimate, based on the entire data set) If <span class="math inline">\(S_n\)</span> and <span class="math inline">\(P_n\)</span> obtained via Kalman recursions, then for <span class="math inline">\(t=n,..,1\)</span> <span class="math display">\[
\begin{aligned}
S_{t-1}^t &amp;= S_{t-1} + J_{t-1}(S_t^n - S_t^{t-1})\\
P_{t-1}^n &amp;= P^{t-1} + J_{t-1}(P_t^n - P_t^{t-1})J^T_{t-1}\\
J_{t-1} &amp; = P_{t-1}F^T[P_t^{t-1}]^{-1}
\end{aligned}
\]</span></p>
</section>
<section id="kalman-and-histogran-filter-shortciomings" class="slide level2">
<h2>Kalman and Histogran Filter Shortciomings</h2>
<p>Kalman:</p>
<ul>
<li><p>linear dynamics</p></li>
<li><p>linear measurement model</p></li>
<li><p>normal errors</p></li>
<li><p>unimodal uncertainty</p></li>
</ul>
<p>Histogram:</p>
<ul>
<li><p>discrete states</p></li>
<li><p>approximation</p></li>
<li><p>inefficient in memory</p></li>
</ul>
</section>
<section id="mcmc-financial-econometrics" class="slide level2">
<h2>MCMC Financial Econometrics</h2>
<p>Set of tools for inference and pricing in continuous-time models.</p>
<ul>
<li><p>Simulation-based and provides a unified approach to state and parameter inference. Can also be applied sequentially.</p></li>
<li><p>Can handle Estimation and Model risk. Important implications for financial decision making</p></li>
<li><p>Bayesian inference. Uses conditional probability to solve an inverse problem and estimates expectations using Monte Carlo.</p></li>
</ul>
</section>
<section id="filtering-smoothing-learning-and-prediction" class="slide level2 smaller">
<h2>Filtering, Smoothing, Learning and Prediction</h2>
<p>Data <span class="math inline">\(y_{t}\)</span> depends on a , <span class="math inline">\(x_{t}\)</span>. <span class="math display">\[
\begin{aligned}
\text{Observation equation} &amp;  \text{:\ }y_{t}=f\left(  x_{t},\varepsilon
_{t}^{y}\right)  \\
\text{State evolution} &amp;  \text{: }x_{t+1}=g\left(  x_{t},\varepsilon
_{t+1}^{x}\right)  ,\end{aligned}
\]</span></p>
<ul>
<li><p>Posterior distribution of <span class="math inline">\(p\left(x_{t}|y^{t}\right)\)</span> where <span class="math inline">\(y^{t}=\left(
y_{1},...,y_{t}\right)\)</span></p></li>
<li><p>Prediction and Bayesian updating.</p></li>
</ul>
<p><span class="math display">\[
p\left(  x_{t+1}|y^{t}\right)  =\int p\left(  x_{t+1}|x_{t}\right)  p\left(
x_{t}|y^{t}\right)  dx_{t},\label{Predict}%
\]</span> updated by Bayes rule</p>
<p><span class="math display">\[
\underset{\text{Posterior}}{\underbrace{p\left(  x_{t+1}|y^{t+1}\right)  }%
}\propto\underset{\text{Likelihood}}{\underbrace{p\left(  y_{t+1}%
|x_{t+1}\right)  }}\underset{\text{Prior}}{\underbrace{p\left(  x_{t+1}%
|y^{t}\right)  }}.\label{Update}%
\]</span></p>
</section>
<section id="nonlinear-model" class="slide level2">
<h2>Nonlinear Model</h2>
<ul>
<li><p>The observation and evolution dynamics are <span class="math display">\[
\begin{aligned}
y_t &amp; = \frac{x_t}{1+ x_t^2} + v_t \; , {\rm where} \; v_t \sim N(0,1) \\
x_t &amp; = x_{t-1} + w_t  \; , {\rm where} \; w_t \sim N(0,0.5)\end{aligned}
\]</span></p></li>
<li><p>Initial condition <span class="math inline">\(x_0 \sim N( 1 , 10 )\)</span></p></li>
</ul>
<p>Fundamental question:</p>
<p><em>How do the filtering distributions <span class="math inline">\(p(x_t|y^t)\)</span> propagate in time?</em></p>
</section>
<section id="nonlinear-y_t-x_t-1-x_t2-v_t" class="slide level2">
<h2>Nonlinear: <span class="math inline">\(y_t = x_t / (1+ x_t^2) + v_t\)</span></h2>

<img data-src="fig/pl-nonlinear1.svg" class="r-stretch"></section>
<section id="simulate-data" class="slide level2">
<h2>Simulate Data</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a></a><span class="fu">set.seed</span>(<span class="dv">8</span>)</span>
<span id="cb2-2"><a></a><span class="co"># MC sample size</span></span>
<span id="cb2-3"><a></a>N <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb2-4"><a></a></span>
<span id="cb2-5"><a></a><span class="co">#Posterior at time t=0 p(x[0]|y[0])=N(1,10)</span></span>
<span id="cb2-6"><a></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(N,<span class="dv">1</span>,<span class="fu">sqrt</span>(<span class="dv">10</span>))</span>
<span id="cb2-7"><a></a><span class="fu">hist</span>(x,<span class="at">main=</span><span class="st">"Pr(x[0]|y[0])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="07-regression_files/figure-revealjs/unnamed-chunk-1-1.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a></a><span class="co">#Obtain draws from prior p(x[1]|y[0])</span></span>
<span id="cb3-2"><a></a>x1 <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fu">sqrt</span>(<span class="fl">0.5</span>))</span>
<span id="cb3-3"><a></a><span class="fu">hist</span>(x1,<span class="at">main=</span><span class="st">"Pr(x[1]|y[0])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="07-regression_files/figure-revealjs/unnamed-chunk-1-2.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a></a><span class="co">#Likelihood function p(y[1]|x[1])</span></span>
<span id="cb4-2"><a></a>y1  <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb4-3"><a></a>ths <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">30</span>,<span class="dv">30</span>,<span class="at">length=</span><span class="dv">1000</span>)</span>
<span id="cb4-4"><a></a><span class="fu">plot</span>(ths,<span class="fu">dnorm</span>(y1,ths<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>ths<span class="sc">^</span><span class="dv">2</span>),<span class="dv">1</span>),<span class="at">type=</span><span class="st">"l"</span>,<span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">""</span>)</span>
<span id="cb4-5"><a></a><span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">"p(y[1]="</span>,y1,<span class="st">"|x[1])"</span>,<span class="at">sep=</span><span class="st">""</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="07-regression_files/figure-revealjs/unnamed-chunk-1-3.png" width="960"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="nonlinear-filtering" class="slide level2">
<h2>Nonlinear Filtering</h2>

<img data-src="fig/pl-nonlinearcps.svg" class="r-stretch"></section>
<section id="resampling" class="slide level2">
<h2>Resampling</h2>
<p>Key: resample and propagate particles</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a></a><span class="co"># Computing resampling weights</span></span>
<span id="cb5-2"><a></a>w <span class="ot">=</span> <span class="fu">dnorm</span>(y1,x1<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>x1<span class="sc">^</span><span class="dv">2</span>),<span class="dv">1</span>)</span>
<span id="cb5-3"><a></a><span class="co"># Resample to obtain draws from p(x[1]|y[1])</span></span>
<span id="cb5-4"><a></a>k <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N,<span class="at">size=</span>N,<span class="at">replace=</span><span class="cn">TRUE</span>,<span class="at">prob=</span>w)</span>
<span id="cb5-5"><a></a>x <span class="ot">=</span> x1[k]</span>
<span id="cb5-6"><a></a><span class="fu">hist</span>(x,<span class="at">main=</span><span class="st">"Pr(x[1]|y[1])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="07-regression_files/figure-revealjs/unnamed-chunk-2-1.png" width="960"></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a></a><span class="co">#Obtain draws from prior p(x[2]|y[1])</span></span>
<span id="cb6-2"><a></a>x2 <span class="ot">=</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(N,<span class="dv">0</span>,<span class="fu">sqrt</span>(<span class="fl">0.5</span>))</span>
<span id="cb6-3"><a></a><span class="fu">hist</span>(x2,<span class="at">main=</span><span class="st">"Pr(x[2]|y[1])"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="07-regression_files/figure-revealjs/unnamed-chunk-2-2.png" width="960"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="propagation-of-mc-error" class="slide level2">
<h2>Propagation of MC error</h2>

<img data-src="fig/pl-MCerror.svg" class="r-stretch"></section>
<section id="dynamic-linear-model-dlm-kalman-filter" class="slide level2">
<h2>Dynamic Linear Model (DLM): Kalman Filter</h2>
<p>Kalman filter for linear Gaussian systems</p>
<ul>
<li>FFBS (Filter Forward Backwards Sample)</li>
</ul>
<p>This determines the posterior distribution of the states</p>
<p><span class="math display">\[
p( x_t | y^t ) \; {\rm and} \; p( x_t | y^T )
\]</span> Also the joint distribution <span class="math inline">\(p( x^T | y^T )\)</span> of the hidden states.</p>
<ul>
<li><p>Discrete Hidden Markov Model HMM (Baum-Welch, Viterbi)</p></li>
<li><p>With parameters <em>known</em> the Kalman filter gives the exact recursions.</p></li>
</ul>
</section>
<section id="simulate-dlm" class="slide level2">
<h2>Simulate DLM</h2>
<p>Dynamic Linear Models <span class="math display">\[
y_t = x_t + v_t \; \; {\rm and} \; \; x_t = \alpha + \beta x_{t-1} + w_t
\]</span> Simulate Data</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a></a>T1<span class="ot">=</span><span class="dv">50</span>; alpha<span class="ot">=</span><span class="sc">-</span><span class="fl">0.01</span>; beta<span class="ot">=</span><span class="fl">0.98</span>; sW<span class="ot">=</span><span class="fl">0.1</span>; sV<span class="ot">=</span><span class="fl">0.25</span>; W<span class="ot">=</span>sW<span class="sc">^</span><span class="dv">2</span>; V<span class="ot">=</span>sV<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb7-2"><a></a></span>
<span id="cb7-3"><a></a>y <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>T1)</span>
<span id="cb7-4"><a></a>ht <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>T1)</span>
<span id="cb7-5"><a></a></span>
<span id="cb7-6"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)){</span>
<span id="cb7-7"><a></a>    ht[t] <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,alpha<span class="sc">+</span>beta<span class="sc">*</span>ht[t<span class="dv">-1</span>],sW)</span>
<span id="cb7-8"><a></a>    y[t] <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>,ht[t],sV)</span>
<span id="cb7-9"><a></a>}</span>
<span id="cb7-10"><a></a>ht <span class="ot">=</span> ht[(T1<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)]</span>
<span id="cb7-11"><a></a>y <span class="ot">=</span> y[(T1<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>T1)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="exact-calculations" class="slide level2">
<h2>Exact calculations</h2>
<p>Kalman Filter recursions</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a></a>m<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,T1);C<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,T1); m0<span class="ot">=</span><span class="dv">0</span>; C0<span class="ot">=</span><span class="dv">100</span></span>
<span id="cb8-2"><a></a></span>
<span id="cb8-3"><a></a>R <span class="ot">=</span> C0<span class="sc">+</span>W; Q <span class="ot">=</span> R<span class="sc">+</span>V; A <span class="ot">=</span> R<span class="sc">/</span>Q</span>
<span id="cb8-4"><a></a>m[<span class="dv">1</span>] <span class="ot">=</span> m0<span class="sc">+</span>A<span class="sc">*</span>(y[<span class="dv">1</span>]<span class="sc">-</span>m0); C[<span class="dv">1</span>] <span class="ot">=</span> R<span class="sc">-</span>A<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>Q</span>
<span id="cb8-5"><a></a></span>
<span id="cb8-6"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T1){</span>
<span id="cb8-7"><a></a>    R    <span class="ot">=</span> C[t<span class="dv">-1</span>]<span class="sc">+</span>W</span>
<span id="cb8-8"><a></a>    Q    <span class="ot">=</span> R<span class="sc">+</span>V</span>
<span id="cb8-9"><a></a>    A    <span class="ot">=</span> R<span class="sc">/</span>Q</span>
<span id="cb8-10"><a></a>    m[t] <span class="ot">=</span> m[t<span class="dv">-1</span>]<span class="sc">+</span>A<span class="sc">*</span>(y[t]<span class="sc">-</span>m[t<span class="dv">-1</span>])</span>
<span id="cb8-11"><a></a>    C[t] <span class="ot">=</span> R<span class="sc">-</span>A<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>Q</span>
<span id="cb8-12"><a></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="dlm-data" class="slide level2">
<h2>DLM Data</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a></a><span class="fu">plot</span>(y,<span class="at">type=</span><span class="st">"b"</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">xlab=</span><span class="st">"Time"</span>,<span class="at">ylab=</span><span class="st">"y_t"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">bty=</span><span class="st">'n'</span>)</span>
<span id="cb9-2"><a></a><span class="fu">lines</span>(m,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb9-3"><a></a><span class="fu">lines</span>(m<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sqrt</span>(C),<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb9-4"><a></a><span class="fu">lines</span>(m<span class="dv">-2</span><span class="sc">*</span><span class="fu">sqrt</span>(C),<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<img data-src="07-regression_files/figure-revealjs/unnamed-chunk-3-1.png" width="960" class="r-stretch"></section>
<section id="bootstrap-filter" class="slide level2">
<h2>Bootstrap Filter</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a></a>M <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb10-2"><a></a>h <span class="ot">=</span> <span class="fu">rnorm</span>(M,m0,<span class="fu">sqrt</span>(C0))</span>
<span id="cb10-3"><a></a>hs <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb10-4"><a></a></span>
<span id="cb10-5"><a></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T1){</span>
<span id="cb10-6"><a></a>    h1 <span class="ot">=</span> <span class="fu">rnorm</span>(M,alpha<span class="sc">+</span>beta<span class="sc">*</span>h,sW)</span>
<span id="cb10-7"><a></a>    w  <span class="ot">=</span> <span class="fu">dnorm</span>(y[t],h1,sV)</span>
<span id="cb10-8"><a></a>    w  <span class="ot">=</span> w<span class="sc">/</span><span class="fu">sum</span>(w)</span>
<span id="cb10-9"><a></a>    h  <span class="ot">=</span> <span class="fu">sample</span>(h1,<span class="at">size=</span>M,<span class="at">replace=</span>T,<span class="at">prob=</span>w)</span>
<span id="cb10-10"><a></a>    hs <span class="ot">=</span> <span class="fu">cbind</span>(hs,h)</span>
<span id="cb10-11"><a></a>}</span>
<span id="cb10-12"><a></a><span class="co"># Quantiles</span></span>
<span id="cb10-13"><a></a>q025 <span class="ot">=</span> <span class="cf">function</span>(x){<span class="fu">quantile</span>(x,<span class="fl">0.025</span>)}</span>
<span id="cb10-14"><a></a>q975 <span class="ot">=</span> <span class="cf">function</span>(x){<span class="fu">quantile</span>(x,<span class="fl">0.975</span>)}</span>
<span id="cb10-15"><a></a>h025 <span class="ot">=</span> <span class="fu">apply</span>(hs,<span class="dv">2</span>,q025)</span>
<span id="cb10-16"><a></a>h975 <span class="ot">=</span> <span class="fu">apply</span>(hs,<span class="dv">2</span>,q975)</span>
<span id="cb10-17"><a></a>sd   <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">apply</span>(hs,<span class="dv">2</span>,var))</span>
<span id="cb10-18"><a></a>m <span class="ot">=</span> <span class="fu">colMeans</span>(hs)</span>
<span id="cb10-19"><a></a><span class="fu">plot</span>(y,<span class="at">type=</span><span class="st">"b"</span>,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">xlab=</span><span class="st">"Time"</span>,<span class="at">ylab=</span><span class="st">"y_t"</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">bty=</span><span class="st">'n'</span>)</span>
<span id="cb10-20"><a></a><span class="fu">lines</span>(m,<span class="at">col=</span><span class="st">"red"</span>)</span>
<span id="cb10-21"><a></a><span class="fu">lines</span>(h025,<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb10-22"><a></a><span class="fu">lines</span>(h975,<span class="at">col=</span><span class="st">"grey"</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>

</div>
<!-- ## DLM Filtering

![](fig/pl-DLM2.svg) -->
<img data-src="07-regression_files/figure-revealjs/unnamed-chunk-4-1.png" width="960" class="r-stretch"></section>
<section id="streaming-data-how-do-parameter-distributions-change-in-time" class="slide level2">
<h2>Streaming Data: How do Parameter Distributions change in Time?</h2>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p><img data-src="fignick/BayesThm.jpg"> Bayes theorem: <span class="math display">\[
p(\theta \mid y^t) \propto p(y_t \mid \theta) \,
p(\theta \mid y^{t-1})
\]</span></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Online Dynamic Learning</p>
<ul>
<li>Real-time surveillance</li>
<li>Bayes means sequential updating of information</li>
<li>Update posterior density <span class="math inline">\(p(\theta \mid y_t)\)</span> with every new observation (<span class="math inline">\(t = 1, \ldots, T\)</span>) - “sequential learning”</li>
</ul>
</div>
</div>
</div>
</section>
<section id="galton-1877-first-particle-filter" class="slide level2">
<h2>Galton 1877: First Particle Filter</h2>

<img data-src="fignick/1877.jpg" class="r-stretch"></section>
<section id="streaming-data-online-learning" class="slide level2 scrollable">
<h2>Streaming Data: Online Learning</h2>
<p>Construct an essential state vector <span class="math inline">\(Z_{t+1}\)</span>. <span class="math display">\[
\begin{aligned}
p(Z_{t+1}|y^{t+1}) &amp;= \int p(Z_{t+1}|Z_t, y_{t+1}) \;d\mathbb{P}(Z_t|y^{t+1}) \\
&amp;\propto  \int \underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \overbrace{ \underbrace{p(y_{t+1} | Z_t)}_{resample} \;d\mathbb{P}(Z_t|y^t)}\end{aligned}
\]</span></p>
<ol type="1">
<li><p><em>Re-sample</em> with weights proportional to <span class="math inline">\(p(y_{t+1} | Z_t^{(i)})\)</span> and generate <span class="math inline">\(\{Z_t^{\zeta(i)}\}_{i=1}^N\)</span></p></li>
<li><p><em>Propagate</em> with <span class="math inline">\(Z_{t+1}^{(i)} \sim
p(Z_{t+1}|Z_t^{\zeta(i)}, y_{t+1})\)</span> to obtain <span class="math inline">\(\{Z_{t+1}^{(i)}\}_{i=1}^N\)</span></p></li>
</ol>
<p>Parameters: <span class="math inline">\(p( \theta | Z_{t+1} )\)</span> drawn “offline”</p>
</section>
<section id="sample-resample" class="slide level2">
<h2>Sample – Resample</h2>

<img data-src="fig/sample-resample.svg" class="r-stretch quarto-figure-center"><p class="caption">sample-resample</p></section>
<section id="resample-sample" class="slide level2">
<h2>Resample – Sample</h2>

<img data-src="fig/resample-sample.svg" class="r-stretch quarto-figure-center"><p class="caption">resample-sample</p></section>
<section id="particle-methods-blind-propagation" class="slide level2">
<h2>Particle Methods: Blind Propagation</h2>

<img data-src="fignick/smc.png" class="r-stretch quarto-figure-center"><p class="caption">Propagate-Resample is replaced by Resample-Propagate</p></section>
<section id="traffic-problem" class="slide level2">
<h2>Traffic Problem</h2>

<img data-src="fignick/d_domain.png" style="width:80.0%" class="r-stretch quarto-figure-center"><p class="caption">State-Space</p></section>
<section id="wave-speed-propagation-is-a-mixture-distribution" class="slide level2">
<h2>Wave Speed Propagation is a Mixture Distribution</h2>
<p>Shock wave propagation speed is a mixture, when calculated using Godunov scheme <span class="math display">\[
w = \frac{q(\rho_l) - q(\rho_r)}{\rho_l-\rho_r}  \ \left[\frac{mi}{h}\right] = \left[\frac{veh}{h}\right]\left[\frac{mi}{veh}\right].
\]</span> Assume <span class="math inline">\(\rho_l \sim TN(32, 16, 0, 320)\)</span> and <span class="math inline">\(\rho_r \sim TN(48, 16, 0,320)\)</span><br>
<span class="math inline">\(q_c = 1600  \ veh/h\)</span>, <span class="math inline">\(\rho_c = 40 \ veh/mi\)</span>, and <span class="math inline">\(\rho_{jam} = 320 \ veh/mi\)</span><br>
</p>

<img data-src="fignick/sigma-mixture.png" style="width:70.0%" class="r-stretch"></section>
<section id="traffic-flow-speed-forecast-is-a-mixtrue-dsitribution" class="slide level2">
<h2>Traffic Flow Speed Forecast is a Mixtrue Dsitribution</h2>
<p><strong>Theorem</strong>: The solution (including numerical) to the LWR model with stochastic initial conditions is a mixture distribution.</p>

<img data-src="fignick/mixture.png" style="width:40.0%" class="r-stretch"><p>A moment based filters such as Kalman Filter or Extended Kalman Filter would not capture the mixture.</p>
</section>
<section id="problem-at-hand" class="slide level2">
<h2>Problem at Hand</h2>
<p>The Parameter Learning and State Estimation Problem</p>
<ul>
<li><p>Goal: given sparse sensor measurements, find the distribution over traffic state and underlying traffic flow parameters <span class="math inline">\(p(\theta_t, \phi|y_1, y_2,...,y_t); \ \phi=(q_c,\rho_c)\)</span></p></li>
<li><p>Parameters of the evolution equation (LWR) are stochastic</p></li>
<li><p>Distribution over state is a mixture</p></li>
<li><p>Can’t use moment based filters (KF, EKF,...)</p></li>
</ul>
</section>
<section id="data-assimilation-state-space-representation" class="slide level2">
<h2>Data Assimilation: State Space Representation</h2>

<img data-src="fignick/state-space.png" style="width:60.0%" class="r-stretch"><p>State space formulation allows to combine knowledge from analytical model with the one from field measurements, while taking model and measurement errors into account</p>
</section>
<section id="state-space-representation" class="slide level2">
<h2>State Space Representation</h2>
<ul>
<li><p>State vector <span class="math inline">\(\theta_t = ( \rho_{1t} , \ldots , \rho_{nt} )\)</span></p></li>
<li><p>Boundary conditionals <span class="math inline">\(\rho_{0t}\)</span> and <span class="math inline">\(\rho_{(n+1)t}\)</span></p></li>
<li><p>Underlying parameters <span class="math inline">\(\phi = (q_c, \rho_c)\)</span> are stochastic</p></li>
</ul>
<p><span class="math display">\[\begin{align}
\mbox{Observation: }&amp;y_{t+1} = H\theta_{t+1}  + v; \ v \sim N(0,V) \label{eqn-y}\\
\mbox{Evolution: }&amp;\theta_{t+1} = f_{\phi}(\theta_t) + w; \ w \sim N(0,W) \label{(eqn-x)}
\end{align}\]</span></p>
<p><span class="math inline">\(H: \mathbb{R}^{M} \rightarrow \mathbb{R}^k\)</span> in the measurement model. <span class="math inline">\(\phi  = (q_c,\rho_c, \rho_{max})\)</span>.</p>
<p>Parameter priors: <span class="math inline">\(q_c \sim N(\mu_q, \sigma_c^2)\)</span>, <span class="math inline">\(\rho_c = Uniform(\rho_{min}, \rho_{max})\)</span></p>
</section>
<section id="particle-parameter-learning" class="slide level2">
<h2>Particle Parameter Learning</h2>

<img data-src="fignick/hmm-learning.png" style="width:100.0%" class="r-stretch"></section>
<section id="sample-based-pdf-representation" class="slide level2">
<h2>Sample-based PDF Representation</h2>
<ul>
<li><p>Regions of high density: Many particles and Large weight of particles</p></li>
<li><p>Uneven partitioning</p></li>
<li><p>Discrete approximation for continuous pdf</p></li>
</ul>
<p><span class="math display">\[
p^{N}\left(  \theta_{t+1}|y^{t+1}\right) \propto \sum_{i=1}^{N}w_{t}^{\left(  i\right)  }p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right)
\]</span></p>
</section>
<section id="particle-filter" class="slide level2 smaller">
<h2>Particle Filter</h2>
<p>Bayes Rule: <span class="math display">\[
p(y_{t+1},\theta_{t+1}|\theta_t)=p(y_{t+1}|\theta_t)\,p(\theta_{t+1}|%
\theta_t,y_{t+1}).
\]</span></p>
<ul>
<li>Given a particle approximation to <span class="math inline">\(p^{N}\left(  \theta_t|y^{t}\right)\)</span> <span class="math display">\[
\begin{aligned}
p^{N}\left(  \theta_{t+1}|y^{t+1}\right)   &amp;  \propto\sum_{i=1}^{N}p\left(
y_{t+1}|\theta_t^{\left(  i\right)  }\right)  p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right) \label{Mixture2}\\
&amp;  =\sum_{i=1}^{N}w_{t}^{\left(  i\right)  }p\left(  \theta_{t+1}|\theta_t^{\left(
i\right)  },y_{t+1}\right)  \text{,}%
\end{aligned}
\]</span> where</li>
</ul>
<p><span class="math display">\[
w_{t}^{\left(  i\right)  }=\frac{p\left(  y_{t+1}|\theta_t^{\left(  i\right)
}\right)  }{\sum_{i=1}^{N}p\left(  y_{t+1}|\theta_t^{\left(  i\right)  }\right)
}\text{.}%
\]</span></p>
<ul>
<li>Essentially a mixture Kalman filter</li>
</ul>
</section>
<section id="particle-parameter-learning-1" class="slide level2 smaller">
<h2>Particle Parameter Learning</h2>
<p>Given particles (a.k.a. random draws) <span class="math inline">\((\theta^{(i)}_t,\phi^{(i)},s^{(i)}_t),\)</span> <span class="math inline">\(i=1,\ldots,N\)</span> <span class="math display">\[
p( \theta_t | y_{1:t} ) = \frac{1}{N} \sum_{i=1}^N \delta_{ \theta^{(i)} } \; .
\]</span></p>
<ul>
<li><p>First resample <span class="math inline">\((\theta^{k(i)}_t,\phi^{k(i)},s^{k(i)}_t)\)</span> with weights proportional to <span class="math inline">\(p(y_{t+1}|\theta^{k(i)}_t,\phi^{k(i)})\)</span> and <span class="math inline">\(s_t^{k(i)}=S(s^{(i)}_t,\theta^{k(i)}_t,y_{t+1})\)</span> and then propogate to <span class="math inline">\(p(\theta_{t+1}|y_{1:t+1})\)</span> by drawing <span class="math inline">\(\theta^{(i)}_{t+1}\,\)</span>from <span class="math inline">\(p(\theta_{t+1}|\theta^{k(i)}_t,\phi^{k(i)},y_{t+1}),\,i=1,\ldots,N\)</span>.</p></li>
<li><p>Next we update the sufficient statistic as</p></li>
</ul>
<p><span class="math display">\[
s_{t+1}=S(s_t^{k(i)},\theta^{(i)}_{t+1},y_{t+1}),
\]</span> for <span class="math inline">\(i=1,\ldots,N\)</span>, which represents a deterministic propogation.</p>
<ul>
<li>Finally, parameter learning is completed by drawing <span class="math inline">\(\phi^{(i)}\)</span> using <span class="math inline">\(p(\phi|s^{(i)}_{t+1})\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span>.</li>
</ul>
</section>
<section id="streaming-data-online-learning-1" class="slide level2 scrollable">
<h2>Streaming Data: Online Learning</h2>
<p>Construct an essential state vector <span class="math inline">\(Z_{t+1}\)</span>. <span class="math display">\[
\begin{aligned}
p(Z_{t+1}|y^{t+1}) &amp;= \int p(Z_{t+1}|Z_t, y_{t+1}) \;d\mathbb{P}(Z_t|y^{t+1}) \\
&amp;\propto  \int \underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \overbrace{ \underbrace{p(y_{t+1} | Z_t)}_{resample} \;d\mathbb{P}(Z_t|y^t)}
\end{aligned}
\]</span></p>
<ol type="1">
<li><p><em>Re-sample</em> with weights proportional to <span class="math inline">\(p(y_{t+1} | Z_t^{(i)})\)</span> and generate <span class="math inline">\(\{Z_t^{\zeta(i)}\}_{i=1}^N\)</span></p></li>
<li><p><em>Propagate</em> with <span class="math inline">\(Z_{t+1}^{(i)} \sim
p(Z_{t+1}|Z_t^{\zeta(i)}, y_{t+1})\)</span> to obtain <span class="math inline">\(\{Z_{t+1}^{(i)}\}_{i=1}^N\)</span></p></li>
</ol>
<p>Parameters: <span class="math inline">\(p( \theta | Z_{t+1} )\)</span> drawn “offline”</p>
</section>
<section id="resample-propagate" class="slide level2">
<h2>Resample – Propagate</h2>

<img data-src="fig/resample-sample.svg" class="r-stretch"></section>
<section id="algorithm" class="slide level2">
<h2>Algorithm</h2>
<p>These ingredients then define a particle filtering and learning algorithm for the sequence of joint posterior distributions <span class="math inline">\(p( \theta_t , \phi | y_{1:t} )\)</span>: <span class="math display">\[
\begin{aligned}
&amp;  \text{Step 1. (Resample) Draw an index } k_t \left(  i\right)  \sim
Mult_{N}\left(  w_{t}^{\left(  1\right)  },...,w_{t}^{\left(  N\right)
}\right), \\
&amp; \mbox{where the weights are given by } w_t^{(i)} \propto p(y_{t+1}|(\theta_t,\phi)^{(i)}), \   \text{ for }i=1,...,N\\
\text{ }  &amp;  \text{Step 2. (Propagate) Draw }\theta_{t+1}^{\left(  i\right)  }\sim
p\left(  \theta_{t+1}|\theta_t^{k_t \left(  i\right)  },y_{t+1}\right)  \text{ for
}i=1,...,N.\\
\text{ }  &amp;  \text{Step 3. (Update) } s_{t+1}^{(i)} =S(s_t^{k_t(i)},\theta_{t+1}^{(i)},y_{t+1})\\
\text{ }  &amp;  \text{Step 4. (Replenish) } \phi^{(i)} \sim p( \phi | s_{t+1}^{(i)} )
\end{aligned}
\]</span> There are a number of efficiency gains from such an approach, e.g.&nbsp;it does not suffer from degeneracy problems associated with traditional propagate-resample algorithms when <span class="math inline">\(y_{t+1}\)</span> is an outliers.</p>
</section>
<section id="obtaining-state-estimates-from-particles" class="slide level2 smaller">
<h2>Obtaining state estimates from particles</h2>
<ul>
<li>Any estimate of a function <span class="math inline">\(f(\theta_t)\)</span> can be calculated by discrete-approximation</li>
</ul>
<p><span class="math display">\[
\mbox{E}(f(\theta_t)) = \dfrac{1}{N}\sum_{j=1}^{N}w_t^{(j)}f(\theta_t^{(j)})
\]</span></p>
<ul>
<li>Mean:</li>
</ul>
<p><span class="math display">\[
\mbox{E}(\theta_t) = \dfrac{1}{N}\sum_{j=1}^{N}w_t^{(j)}\theta_t^{(j)}
\]</span></p>
<ul>
<li><p>MAP-estimate: particle with largest weight</p></li>
<li><p>Robust mean: mean within window around MAP-estimate</p></li>
</ul>
</section>
<section id="particle-filters-pluses" class="slide level2">
<h2>Particle Filters: Pluses</h2>
<ul>
<li><p>Estimation of full PDFs</p></li>
<li><p>Non-Gaussian distributions (multi-modal)</p></li>
<li><p>Non-linear state and observation model</p></li>
<li><p>Parallelizable</p></li>
</ul>
</section>
<section id="particle-filters-minuses" class="slide level2">
<h2>Particle Filters: Minuses</h2>
<ul>
<li><p>Degeneracy problem</p></li>
<li><p>High number of particles needed</p></li>
<li><p>Computationally expensive</p></li>
<li><p>Linear-Gaussian assumption is often sufficient</p></li>
</ul>
</section>
<section id="applications-localization" class="slide level2">
<h2>Applications: Localization</h2>
<ul>
<li><p>Track car position in given road map</p></li>
<li><p>Track car position from radio frequency measurements</p></li>
<li><p>Track aircraft position from estimated terrain elevation</p></li>
<li><p>Collision Avoidance (Prediction)</p></li>
</ul>
</section>
<section id="applications-model-estimation" class="slide level2">
<h2>Applications: Model Estimation</h2>
<ul>
<li><p>Tracking with multiple motion-models</p></li>
<li><p>Recovery of signal from noisy measurements</p></li>
<li><p>Neural Network model selection (on-line classification)</p></li>
</ul>
</section>
<section id="applications-other" class="slide level2">
<h2>Applications: Other</h2>
<ul>
<li><p>Visual Tracking</p></li>
<li><p>Prediction of (financial) time series</p></li>
<li><p>Quality control in semiconductor industry</p></li>
<li><p>Military applications: Target recognition from single or multiple images, Guidance of missiles</p></li>
<li><p>Reinforcement Learning</p></li>
</ul>
</section>
<section id="mixture-kalman-filter-for-traffic" class="slide level2 smaller">
<h2>Mixture Kalman Filter For Traffic</h2>
<p><span class="math display">\[
\begin{aligned}
\mbox{Observation: }&amp;y_{t+1} = Hx_{t+1}  + \gamma^Tz_{t+1} + v_{t+1} , \ v_{t+1} \sim N(0, V_{t+1})\\
\mbox{Evolution: }&amp;x_{t+1} = F_{\alpha_{t+1}}x_t + (1-F_{\alpha_{t+1}})\mu + \alpha_t\beta_{t} + \omega_{1} \\
&amp;\beta_{t+1} = \max(0,\beta_{t} + \omega_{2} \label{eqn-beta})\\
\mbox{Switching Evolution: }&amp;\alpha_{t+1} \sim p(\alpha_{t+1} |\alpha_{t},Z_{t})
\end{aligned}
\]</span> where <span class="math inline">\(z_t\)</span> is an exogenous variable that effects the sensor model, <span class="math inline">\(\mu\)</span> is an average free flow speed <span class="math display">\[
\alpha_t \in \{0,1,-1\}
\]</span> <span class="math display">\[
\omega = (\omega_{1}, \omega_{2})^T \sim N(0, W), \ v \sim N(0,V)
\]</span> <span class="math display">\[
F_{\alpha_t} = \left\{
\begin{aligned}
&amp;1,  \ \alpha_t \in \{1,-1\}\\
&amp;F, \  \alpha_t = 0\end{aligned}
\right.
\]</span> No boundary conditions estimation is needed. No capacity/critical density is needed.</p>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1226,

        height: 920,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>