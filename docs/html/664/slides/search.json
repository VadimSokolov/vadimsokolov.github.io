[
  {
    "objectID": "test.html#slide-1",
    "href": "test.html#slide-1",
    "title": "Bayes AI",
    "section": "SLide 1",
    "text": "SLide 1\n\\[\\begin{align}\n\\mbox{Observation: }&y_{t+1} = H\\theta_{t+1}  + v; \\ v \\sim N(0,V) \\label{eqn-y}\\\\\n\\mbox{Evolution: }&\\theta_{t+1} = f_{\\phi}(\\theta_t) + w; \\ w \\sim N(0,W) \\label{(eqn-x)}\n\\end{align}\\]"
  },
  {
    "objectID": "06-mcmc.html#mcmc-simulation",
    "href": "06-mcmc.html#mcmc-simulation",
    "title": "Bayes AI",
    "section": "MCMC Simulation",
    "text": "MCMC Simulation\nSuppose that \\(X \\sim F_X ( x )\\) and let \\(Y = g (X)\\).\nHow do we find \\(F_Y ( y )\\) and \\(f_Y ( y )\\) ?\n\nvon Neumann\n\nGiven a uniform \\(U\\), how do we find \\(X= g(U)\\)?\n\nIn the bivariate case \\((X,Y) \\rightarrow (U,V)\\).\n\nWe need to find \\(f_{(U,V)} ( u , v )\\) from \\(f_{X,Y}(x,y)\\)\n\nApplications: Simulation, MCMC and PF."
  },
  {
    "objectID": "06-mcmc.html#transformations",
    "href": "06-mcmc.html#transformations",
    "title": "Bayes AI",
    "section": "Transformations",
    "text": "Transformations\nThe cdf identity gives \\[\nF_Y ( y) = \\mathbb{P} ( Y \\leq y ) = \\mathbb{P} ( g( X) \\leq y )\n\\]\n\nHence if the function \\(g ( \\cdot )\\) is monotone we can invert to get\n\n\\[\nF_Y ( y ) = \\int_{ g( x) \\leq y } f_X ( x ) dx\n\\]\n\nIf \\(g\\) is increasing \\(F_Y ( y ) = P( X \\leq g^{-1} ( y ) ) = F_X ( g^{-1} ( y ) )\\)\n\nIf \\(g\\) is decreasing \\(F_Y ( y ) = P( X \\geq g^{-1} ( y ) ) = 1 - F_X ( g^{-1} ( y ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#transformation-identity",
    "href": "06-mcmc.html#transformation-identity",
    "title": "Bayes AI",
    "section": "Transformation Identity",
    "text": "Transformation Identity\n\nTheorem 1: Let \\(X\\) have pdf \\(f_X ( x)\\) and let \\(Y=g(X)\\). Then if \\(g\\) is a monotone function we have\n\n\\[\nf_Y ( y) = f_X ( g^{-1} ( y ) ) \\left  | \\frac{ d}{dy}  g^{-1} ( y ) \\right |\n\\] There’s also a multivariate version of this that we’ll see later.\n\nSuppose \\(X\\) is a continuous rv, what’s the pdf for \\(Y = X^2\\)?\nLet \\(X \\sim N ( 0 ,1 )\\), what’s the pdf for \\(Y = X^2\\)?"
  },
  {
    "objectID": "06-mcmc.html#probability-integral-transform",
    "href": "06-mcmc.html#probability-integral-transform",
    "title": "Bayes AI",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\ntheorem Suppose that \\(U \\sim U[0,1]\\), then for any continuous distribution function \\(F\\), the random variable \\(X= F^{-1} (U)\\) has distribution function \\(F\\).\n\nRemember that for \\(u \\in [0,1]\\), \\(\\mathbb{P} \\left ( U \\leq u \\right ) = u\\), so we have\n\n\\[\n\\mathbb{P} \\left (X \\leq x \\right )= \\mathbb{P} \\left ( F^{-1} (U) \\leq x \\right )= \\mathbb{P} \\left ( U \\leq F(x) \\right )=F(x)\n\\] Hence, \\(X = F_X^{-1}(U)\\)."
  },
  {
    "objectID": "06-mcmc.html#normal",
    "href": "06-mcmc.html#normal",
    "title": "Bayes AI",
    "section": "Normal",
    "text": "Normal\nSometimes thare are short-cut formulas to generate random draws\nNormal \\(N(0,I_2)\\): \\(x_1,x_2\\) uniform on \\([0,1]\\) then \\[\n\\begin{aligned}\ny_1 = & \\sqrt{-2\\log x_1}\\cos(2\\pi x_2)\\\\\ny_2 = & \\sqrt{-2\\log x_1}\\sin(2\\pi x_2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#simulation-and-transformations",
    "href": "06-mcmc.html#simulation-and-transformations",
    "title": "Bayes AI",
    "section": "Simulation and Transformations",
    "text": "Simulation and Transformations\nAn important application is how to transform multiple random variables?\n\nSuppose that we have random variables:\n\n\\[\n( X , Y ) \\sim  f_{ X , Y} ( x , y )\n\\] A transformation of interest given by: \\[\nU = g ( X , Y )\n\\; \\; {\\rm and} \\; \\;\nV = h ( X , Y )\n\\]\n\nThe problem is how to compute \\(f_{ U , V } ( u , v )\\) ? Jacobian\n\n\\[\nJ = \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) }  = \\left |  \\begin{array}{cc}\n\\frac{ \\partial x }{ \\partial u} & \\frac{ \\partial x }{ \\partial v} \\\\\n\\frac{ \\partial y }{ \\partial u} & \\frac{ \\partial y }{ \\partial v}\n\\end{array} \\right |\n\\]"
  },
  {
    "objectID": "06-mcmc.html#bivariate-change-of-variable",
    "href": "06-mcmc.html#bivariate-change-of-variable",
    "title": "Bayes AI",
    "section": "Bivariate Change of Variable",
    "text": "Bivariate Change of Variable\n\nTheorem: (change of variable)\n\n\\[\nf_{ U , V } ( u , v ) = f_{ X , Y} ( h_1 ( u , v )  , h_2 ( u , v ) )\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right |\n\\] The last term is the Jacobian.\nThis can be calculated in two ways.\n\\[\n\\left |  \\frac{ \\partial ( x , y ) }{ \\partial ( u , v ) } \\right | =\n1 / \\left |  \\frac{ \\partial ( u , v ) }{ \\partial ( x , y ) } \\right |\n\\]\n\nSo we don’t always need the inverse transformation \\(( x , y ) = ( g^{-1} ( u , v )  , h^{-1} ( u , v ) )\\)"
  },
  {
    "objectID": "06-mcmc.html#inequalities-and-identities",
    "href": "06-mcmc.html#inequalities-and-identities",
    "title": "Bayes AI",
    "section": "Inequalities and Identities",
    "text": "Inequalities and Identities\n\nMarkov\n\n\\[\n\\mathbb{P} \\left ( g( X ) \\geq c \\right ) \\leq \\frac{ \\mathbb{E} ( g(X) ) }{c }\n\\; \\; {\\rm  where} \\; \\;   g( X) \\geq 0\n\\]\n\nChebyshev\n\n\\[\n\\mathbb{P} \\left ( | X - \\mu | \\geq c \\right ) \\leq \\frac{ Var(X) }{c^2 }\n\\]\n\nJensen\n\n\\[\n\\mathbb{E} \\left ( \\phi ( X ) \\right ) \\leq \\phi \\left ( \\mathbb{E}( X ) \\right )\n\\]\n\nCauchy-Schwarz \\[\ncorr (X,Y) \\leq 1\n\\]\n\nChebyshev follows from Markov. Mike Steele and Cauchy-Schwarz."
  },
  {
    "objectID": "06-mcmc.html#markov-inequality",
    "href": "06-mcmc.html#markov-inequality",
    "title": "Bayes AI",
    "section": "Markov Inequality",
    "text": "Markov Inequality\nLet \\(f\\) be non-decreasing \\[\n\\begin{aligned}\nP ( Z &gt; t ) &= P ( f(Z) \\geq f(t) ) \\\\\n& = E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t )  ) \\right ) \\\\\n& \\leq E \\left (  \\mathbb{I}  ( f( Z) \\geq f(t ) )  \\frac{f(Z)}{f(t) }  \\right ) \\\\\n& =  E\\left  (  \\frac{f(Z)}{f(t) }  \\right )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06-mcmc.html#concentration-inequalities",
    "href": "06-mcmc.html#concentration-inequalities",
    "title": "Bayes AI",
    "section": "Concentration Inequalities",
    "text": "Concentration Inequalities\nLaw of Large Numbers \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( | Z - E(Z) | &gt; n \\epsilon  \\right ) = 0 \\; \\; \\forall \\epsilon &gt; 0\n\\]\nCentral Limt Theorem (CLT) \\[\n\\lim_{ n \\rightarrow \\infty } \\mathbb{P} \\left ( n^{- 1/2} ( | Z - E(Z) |  ) &gt;  \\epsilon  \\right ) = \\Phi ( x )\n\\]\nPosterior Concentration"
  },
  {
    "objectID": "06-mcmc.html#hoeffding-and-bernstein",
    "href": "06-mcmc.html#hoeffding-and-bernstein",
    "title": "Bayes AI",
    "section": "Hoeffding and Bernstein",
    "text": "Hoeffding and Bernstein\nLet \\(Z= \\sum_{i=1}^n X_i\\).\nHoeffding \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp \\left  ( - \\frac{ t^2}{2n} \\right )\n\\]\nBernstein \\[\nP ( Z &gt; E(Z) +  t ) \\leq \\exp  \\left ( - \\frac{ t^2}{ 2 ( Var(Z) + t/3 ) } \\right )\n\\] Large Deviations (Varadhan)"
  },
  {
    "objectID": "06-mcmc.html#special-distributions",
    "href": "06-mcmc.html#special-distributions",
    "title": "Bayes AI",
    "section": "Special Distributions",
    "text": "Special Distributions\nSee Common Distributions\n\nBernoulli and Binomial\nHypergeometric\nPoisson\nNegative Binomial\nNormal Distribution\nGamma Distribution\nBeta Distribution\nMultinomial Distribution\nBivariate Normal Distribution\nWishart Distribution\n\n\\(\\ldots\\)"
  },
  {
    "objectID": "06-mcmc.html#example-markov-dependence",
    "href": "06-mcmc.html#example-markov-dependence",
    "title": "Bayes AI",
    "section": "Example: Markov Dependence",
    "text": "Example: Markov Dependence\n\nWe can always factor a joint distribution as\n\n\\[\np( X_n , X_{n-1} , \\ldots , X_1 )  = p( X_n | X_{n-1} , \\ldots , X_1 ) \\ldots p( X_2 | X_1 ) p( X_1 )\n\\]\nexample - A process has the Markov Property if\n\\[\np( X_n | X_{n-1} , \\ldots , X_1 ) = p( X_n | X_{n-1} )\n\\]\n\nOnly the current history matter when determining the probabilities.\n\n:"
  },
  {
    "objectID": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "href": "06-mcmc.html#a-real-world-probability-model-hidden-markov-models",
    "title": "Bayes AI",
    "section": "A real world probability model: Hidden Markov Models",
    "text": "A real world probability model: Hidden Markov Models\nAre stock returns a random walk?\nHidden Markov Models (Baum-Welch, Viterbi)\n\nDaily returns on the SP500 stock market index.\n\nBuild a hidden Markov model to predict the ups and downs.\n\nSuppose that stock market returns on the next four days are \\(X_1 , \\ldots , X_4\\).\nLet’s empirical determine conditionals and marginals"
  },
  {
    "objectID": "06-mcmc.html#sp500-data",
    "href": "06-mcmc.html#sp500-data",
    "title": "Bayes AI",
    "section": "SP500 Data",
    "text": "SP500 Data\nMarginal and Bivariate Distributions\n\nEmpirically, what do we get? Daily returns from \\(1948-2007\\).\n\ncenter \\(x\\) Down Up —————- ——- ——- \\(P( X_i ) = x\\) 0.474 0.526\n\nFinding \\(p( X_2 | X_1 )\\) is twice as much computational effort: counting \\(UU,UD,DU,DD\\) transitions.\n\ncenter \\(X_i\\) Down Up —————— ——- ——- \\(X_{i-1} = Down\\) 0.519 0.481 \\(X_{i-1} = Up\\) 0.433 0.567"
  },
  {
    "objectID": "06-mcmc.html#conditioned-on-two-days",
    "href": "06-mcmc.html#conditioned-on-two-days",
    "title": "Bayes AI",
    "section": "Conditioned on two days",
    "text": "Conditioned on two days\n\nLet’s do \\(p( X_3 | X_2 , X_1 )\\)\n\ncenter \\(X_{i-2}\\) \\(X_{i-1}\\) Down Up ———– ———– ——- ——- Down Down 0.501 0.499 Down Up 0.412 0.588 Up Down 0.539 0.461 Up Up 0.449 0.551\n\nWe could do the distribution \\(p( X_2 , X_3 | X_1 )\\). This is a joint, marginal and conditional distribution all at the same time.\n\nJoint because more than one variable \\(( X_2 , X_3 )\\), marginal because it ignores \\(X_4\\) and conditional because its given \\(X_1\\)."
  },
  {
    "objectID": "06-mcmc.html#joint-probabilities",
    "href": "06-mcmc.html#joint-probabilities",
    "title": "Bayes AI",
    "section": "Joint Probabilities",
    "text": "Joint Probabilities\n\nUnder Markov dependence \\[\n\\begin{aligned}\nP( UUD ) & = p( X_1 = U) p( X_2 = U | X_1 = U) p( X_3 | X_2 = U , X_1 = U ) \\\\\n& = ( 0.526 ) ( 0.567 ) ( 0.433)\n\\end{aligned}\n\\]\nUnder independence we would have got \\[\n\\begin{aligned}\nP(UUD) & = P( X_1 = U) p( X_2 = U) p( X_3 = D ) \\\\\n& = (.526)(.526)(.474) \\\\\n& = 0.131\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "04-hypothesis.html#lindleys-paradox",
    "href": "04-hypothesis.html#lindleys-paradox",
    "title": "Bayes AI",
    "section": "Lindley’s Paradox",
    "text": "Lindley’s Paradox\nOften evidence which, for a Bayesian statistician, strikingly supports the null leads to rejection by standard classical procedures.\n\nDo Bayes and Classical always agree?\n\nBayes computes the probability of the null being true given the data \\(p ( H_0 | D )\\). That’s not the p-value. Why?\n\nSurely they agree asymptotically?\nHow do we model the prior and compute likelihood ratios \\(L ( H_0 | D )\\) in the Bayesianwork?"
  },
  {
    "objectID": "04-hypothesis.html#bayes-t-ratio",
    "href": "04-hypothesis.html#bayes-t-ratio",
    "title": "Bayes AI",
    "section": "Bayes \\(t\\)-ratio",
    "text": "Bayes \\(t\\)-ratio\nEdwards, Lindman and Savage (1963)\nSimple approximation for the likelihood ratio. \\[\nL ( p_0 ) \\approx \\sqrt{2 \\pi} \\sqrt{n} \\exp \\left ( - \\frac{1}{2} t^2 \\right )\n\\]\n\nKey: Bayes test will have the factor \\(\\sqrt{n}\\)\n\nThis will asymptotically favour the null.\n\nThere is only a big problem when \\(2 &lt; t &lt; 4\\) – but this is typically the most interesting case!"
  },
  {
    "objectID": "04-hypothesis.html#coin-tossing",
    "href": "04-hypothesis.html#coin-tossing",
    "title": "Bayes AI",
    "section": "Coin Tossing",
    "text": "Coin Tossing\nIntuition: Imagine a coin tossing experiment and you want to determine whether the coin is “fair” \\(H_0 : p = \\frac{1}{2}\\).\nThere are four experiments.\n\n\n\nExpt 1\n2\n3\n\n4\n\n\n\n\nn 50\n100\n400\n10,\n000\n\n\nr 32\n60\n220\n50\n98\n\n\n\\(L(p_0)\\)\n0.81\n1.09\n2.17\n11.68"
  },
  {
    "objectID": "04-hypothesis.html#coin-tossing-1",
    "href": "04-hypothesis.html#coin-tossing-1",
    "title": "Bayes AI",
    "section": "Coin Tossing",
    "text": "Coin Tossing\nImplications:\n\nClassical: In each case the \\(t\\)-ratio is approx \\(2\\). They we just \\(H_0\\) ( a fair coin) at the 5% level in each experiment.\nBayes: \\(L ( p_0 )\\) grows to infinity and so they is overwhelming evidence for \\(H _ 0\\). Connelly shows that the Monday effect disappears when you compute the Bayes version."
  },
  {
    "objectID": "02-utility.html#probability-and-psychology",
    "href": "02-utility.html#probability-and-psychology",
    "title": "Bayes AI",
    "section": "Probability and Psychology",
    "text": "Probability and Psychology\nHow do people form probabilities or expectations in reality?\nPsychologists have categorized many different biases that people have in their beliefs or judgments.\n\n\n\nLoss Aversion\n\n\nThe most important finding of Kahneman and Tversky is that people are loss averse.\n\n\n\nUtilities are defined over gains and losses rather than over final (or terminal) wealth, an idea first proposed by Markowitz. This is a violation of the EU postulates. Let \\((x,y)\\) denote a bet with gain \\(x\\) with probability \\(y\\).\nTo illustrate this subjects were asked:\nIn addition to whatever you own, you have been given $1000, now choose between the gambles \\(A = ( 1000 , 0.5 )\\) and \\(B = ( 500 , 1 )\\).\n\\(B\\) was the more popular choice."
  },
  {
    "objectID": "02-utility.html#example",
    "href": "02-utility.html#example",
    "title": "Bayes AI",
    "section": "Example",
    "text": "Example\nThe same subjects were then asked: In addition to whatever you own, you have been given $2000, now choose between the gambles \\(C = ( -1000 , 0.5 )\\) and \\(D = ( -500 , 1 )\\).\n\nThis time \\(C\\) was more popular.\nThe key here is that their final wealth positions are identical yet people chose differently. The subjects are apparently focusing only on gains and losses.\n\nWhen they are not given any information about prior winnings, they choose \\(B\\) over \\(A\\) and \\(C\\) over \\(D\\). Clearly for a risk averse people this is the rational choice.\n\nThis effect is known as loss aversion."
  },
  {
    "objectID": "02-utility.html#representativeness",
    "href": "02-utility.html#representativeness",
    "title": "Bayes AI",
    "section": "Representativeness",
    "text": "Representativeness\n\n\n\nRepresentativeness\n\n\nWhen people try to determine the probability that evidence \\(A\\) was generated by model \\(B\\), they often use the representative heuristic. This means that they evaluate the probability by the degree to which \\(A\\) reflects the essential characteristics of \\(B\\).\n\n\n\n\nA common bias is base rate neglect or ignoring prior evidence.\nFor example, in tossing a fair coin the sequence HHTHTHHTHH with seven heads is likely to appear and yet people draw conclusions from too few data points and think 7 heads is representative of the true process and conclude \\(p=0.7\\)."
  },
  {
    "objectID": "02-utility.html#expected-utility-eu-theory-normative",
    "href": "02-utility.html#expected-utility-eu-theory-normative",
    "title": "Bayes AI",
    "section": "Expected Utility (EU) Theory: Normative",
    "text": "Expected Utility (EU) Theory: Normative\nLet \\(P,Q\\) be two probability distributions or risky gambles/lotteries.\n\\(p P + (1 - p ) Q\\) is the compound or mixture lottery.\nThe rational agent (You) will have preferences between gambles.\n\nWe write \\(P \\succeq Q\\) if and only if You strictly prefer \\(P\\) to \\(Q\\). If two lotteries are indifferent we write \\(P \\sim Q\\).\nEU – a number of plausible axioms – completeness, transitivity, continuity and independence – then preferences are an expectation of a utility function.\nThe theory is a normative one and not necessarily descriptive. It suggests how a rational agent should formulate beliefs and preferences and not how they actually behave.\nExpected utility \\(U(P)\\) of a risky gamble is then\n\n\\[\nP \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q )\n\\]"
  },
  {
    "objectID": "02-utility.html#key-facts",
    "href": "02-utility.html#key-facts",
    "title": "Bayes AI",
    "section": "Key Facts",
    "text": "Key Facts\nThe two key facts then are uniqueness of probability and existence of expected utility. Formally,\n\nIf \\(P \\succeq R \\succeq Q\\) and \\(w P + (1 - w ) Q \\sim R\\) then \\(w\\) is unique.\nThere exists an expected utility \\(U(\\cdot )\\) such that \\(P \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q)\\). Furthermore \\[\nU \\left (w P + (1 - w ) Q \\right ) = wU (P) +(1 - w ) U(Q)\n\\] for any \\(P, Q\\) and \\(0 \\leq w \\leq 1\\).\n\nThis implies that \\(U\\) is additive and it is also unique up to affine transformation."
  },
  {
    "objectID": "02-utility.html#st.-petersburg-paradox",
    "href": "02-utility.html#st.-petersburg-paradox",
    "title": "Bayes AI",
    "section": "St. Petersburg Paradox",
    "text": "St. Petersburg Paradox\nWhat are you willing to pay to enter the following game?\n\nI toss a fair game and when the first head appears, on the \\(T\\)th toss, I pay you \\(\\$2^T\\) dollars.\nFirst, probability of first head on \\(T\\)th toss is \\(2^{-T}\\)\n\n\\[\n\\begin{aligned}\nE ( X) & = \\sum_{T=1}^{\\infty}  2^T 2^{-T} \\\\\n    & = 2 ( 1/2) + 4 (1/4) + 8(1/8) + \\ldots \\\\\n    & = 1 + 1 + 1+  \\ldots \\rightarrow \\infty\n\\end{aligned}\n\\] - Bernoulli (1754) constructed utility to value bets with \\(E( u(X) )\\).\nSome examples of utility functions are,\n\n\\(U(x) = V_0 (1-x^{-\\alpha})\\), \\(\\alpha &gt; 0\\), which gives an expected utility of \\(V_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\)\nLog utility, \\(U(x) = \\log(x)\\), with expected value \\(2 \\log(2)\\).\n\nNotice that after obtaining an expected utility value, you’ll have to find the corresponding reward/dollar amount."
  },
  {
    "objectID": "02-utility.html#attitudes-to-risk",
    "href": "02-utility.html#attitudes-to-risk",
    "title": "Bayes AI",
    "section": "Attitudes to Risk",
    "text": "Attitudes to Risk\nTwo gambles\n\nget \\(P_1\\) for sure\nget \\(P_2 = P_1+k\\) and \\(P_3 = P_1-k\\) with probability 1/2.\n\n\nRisk neutral: indifferent about fair bets, has linear utility.\nRisk averse: prefers certainty over fair bets, concave utility.\nRisk loving: prefers fair bets over certainty, convex utility.\n\nThen we will compare the utility of those gambles"
  },
  {
    "objectID": "02-utility.html#attitudes-to-risk-1",
    "href": "02-utility.html#attitudes-to-risk-1",
    "title": "Bayes AI",
    "section": "Attitudes to Risk",
    "text": "Attitudes to Risk\nThe solution depends on your risk preferences:\n\nRisk neutral: a risk neutral person is indifferent about fair bets.\nLinear Utility\nRisk averse: a risk averse person prefers certainty over fair bets. \\[\n\\mathbb{E}( U(X) ) &lt; U \\left ( \\mathbb{E}(X) \\right ) \\; .\n\\] Concave utility\nRisk loving: a risk loving person prefer fair bets over certainty.\n\nDepends on your preferences."
  },
  {
    "objectID": "02-utility.html#ellsberg-paradox",
    "href": "02-utility.html#ellsberg-paradox",
    "title": "Bayes AI",
    "section": "Ellsberg Paradox",
    "text": "Ellsberg Paradox\nProbability is counter-intuitive!!!\nTwo urns\n\n\\(100\\) balls with \\(50\\) red and \\(50\\) blue.\nA mix of red and blue but you don’t know the proportion.\n\n\nWhich urn would you like to bet on?\nPeople don’t like the “uncertainty” about the distribution of red/blue balls in the second urn."
  },
  {
    "objectID": "02-utility.html#allais-paradox",
    "href": "02-utility.html#allais-paradox",
    "title": "Bayes AI",
    "section": "Allais Paradox",
    "text": "Allais Paradox\nYou have to make a choice between the following gambles\nFirst compare the “Gambles”\n\n\n\n\n\n\n\n\n\nExperiment 1\n\n\n\n\n\n\n\nGamble \\({\\cal G}_1\\)\n\nGamble \\({\\cal G}_2\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n1\n$5m\n0.89\n\n\n$0m\n0\n$0m\n0.01\n\n\n\n\n\n\n\n\nExperiment 2\n\n\n\n\n\n\n\nGamble \\({\\cal G}_3\\)\n\nGamble \\({\\cal G}_4\\)\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25\n0\n$25m\n0.1\n\n\n$5\n0.11\n$5m\n0\n\n\n$0m\n0.89\n$0m\n0.9\n\n\n\n\n\n\nIf \\({\\cal G}_1 \\geq {\\cal G}_2\\) then \\({\\cal G}_3 \\geq {\\cal G}_4\\) and vice-versa."
  },
  {
    "objectID": "02-utility.html#solution-expected-utility",
    "href": "02-utility.html#solution-expected-utility",
    "title": "Bayes AI",
    "section": "Solution: Expected Utility",
    "text": "Solution: Expected Utility\nGiven (subjective) probabilities \\(P = ( p_1 , p_2 , p_3 )\\). Write \\(E ( U | P )\\) for expected utility. W.l.o.g. set \\(u ( 0 ) = 0\\) and for the high prize set \\(u(\\$25 \\; {\\rm million} ) = 1\\). Which leaves one free parameter \\(u = u (\\$5 \\; {\\rm million} )\\).\n\nHence to compare gambles with probabilities \\(P\\) and \\(Q\\) we look at the difference \\[\nE ( u | P ) - E ( u | Q ) = ( p_2 - q_2 ) u + ( p_3 - q_3 )\n\\]\nFor comparing \\({\\cal G}_1\\) and \\({\\cal G}_2\\) we get \\[\n\\begin{aligned}\n  E ( u | {\\cal G}_1 ) - E ( u | {\\cal G}_2 ) &= 0.11 u - 0.1 \\\\\n  E ( u | {\\cal G}_3 ) - E ( u | {\\cal G}_4 ) &= 0.11 u - 0.1\n  \\end{aligned}\n\\] The order is the same, given your \\(u\\).\nIf your utility satisfies \\(u &lt; 0.1/0.11 = 0.909\\) you take the “riskier” gamble."
  },
  {
    "objectID": "02-utility.html#power-utility",
    "href": "02-utility.html#power-utility",
    "title": "Bayes AI",
    "section": "Power Utility",
    "text": "Power Utility\nPower and log-utilities\n\nConstant relative risk aversion (CRRA).\nAdvantage that the optimal rule is unaffected by wealth effects. The CRRA utility of wealth takes the form\n\n\\[\nU_\\gamma (W) = \\frac{ W^{1-\\gamma} -1 }{1-\\gamma}\n\\] - The special case \\(U(W) = \\log (W )\\) for \\(\\gamma = 1\\).\nThis leads to a myopic Kelly criterion rule."
  },
  {
    "objectID": "02-utility.html#kelly-criterion",
    "href": "02-utility.html#kelly-criterion",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\nKelly Criterion corresponds to betting under binary uncertainty. - Consider a sequence of i.i.d. bets where\n\\[\np ( X_t = 1 ) = p \\; \\; {\\rm and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] The optimal allocation is \\(\\omega^\\star = p - q = 2 p - 1\\). - Maximising the expected long-run growth rate leads to the solution\n\\[\n\\begin{aligned}\n\\max_\\omega \\mathbb{E} \\left ( \\ln ( 1 + \\omega W_T ) \\right )\n& = p \\ln ( 1 + \\omega ) + (1 -p) \\ln (1 - \\omega ) \\\\\n& \\leq p \\ln p + q \\ln q + \\ln 2 \\; {\\rm and} \\; \\omega^\\star = p - q\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "02-utility.html#kelly-criterion-1",
    "href": "02-utility.html#kelly-criterion-1",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\n\nLet \\(p\\) denote the probability of a gain and \\(O = (1-p)/p\\) the odds. We can generalize the rule to the case of asymmetric payouts \\((a,b)\\) where\n\n\\[\np ( X_t = 1 ) = p \\; \\; {\\rm and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] - Then the expected utility function is\n\\[\np \\ln ( 1 + b \\omega ) + (1 -p) \\ln (1 - a \\omega )\n\\] - The optimal solution is\n\\[\n\\omega^\\star = \\frac{bp - a q}{ab} = \\frac{p-q}{\\sigma}\n\\]"
  },
  {
    "objectID": "02-utility.html#kelly-criterion-2",
    "href": "02-utility.html#kelly-criterion-2",
    "title": "Bayes AI",
    "section": "Kelly Criterion",
    "text": "Kelly Criterion\n\nIf \\(a=b=1\\) this reduces to the pure Kelly criterion.\nA common case occurs when \\(a=1\\) and market odds \\(b=O\\). The rule becomes \\[\n\\omega^\\star = \\frac{p \\cdot O  -q }{O}\n\\]\n\nTwo possible market opportunities: one where it offers you \\(4/1\\) when you have personal odds of \\(3/1\\) and a second one when it offers you \\(12/1\\) while you think the odds are \\(9/1\\).\nIn expected return these two scenarios are identical both offering a 33% gain.\nIn terms of maximizing long-run growth, however, they are not identical."
  },
  {
    "objectID": "02-utility.html#example-1",
    "href": "02-utility.html#example-1",
    "title": "Bayes AI",
    "section": "Example",
    "text": "Example\n\nTable below shows the Kelly criteria advises an allocation that is twice as much capital to the lower odds proposition: \\(1/16\\) weight versus \\(1/40\\).\n\n\n\n\nMarket\nYou\n\\(p\\)\n\\(\\omega^\\star\\)\n\n\n\n\n\\(4/1\\)\n\\(3/1\\)\n\\(1/4\\)\n\\(1/16\\)\n\n\n\\(12/1\\)\n\\(9/1\\)\n\\(1/10\\)\n\\(1/40\\)\n\n\n\n\nThe optimal allocation \\(\\omega^\\star = ( p O - q ) / O\\) is\n\n\\[\n\\frac{ (1/4) \\times 4 - (3/4) }{4} = \\frac{1}{16} \\; {\\rm and} \\;\n    \\frac{ (1/10) \\times 12 - (9/10) }{12} = \\frac{1}{40}\n\\]"
  },
  {
    "objectID": "02-utility.html#parrandos-paradoxes",
    "href": "02-utility.html#parrandos-paradoxes",
    "title": "Bayes AI",
    "section": "Parrando’s Paradoxes",
    "text": "Parrando’s Paradoxes\nTwo losing bets can be combined to a winner\n\n\n\n\n\n\n\nBernoulli market: \\(1+f\\) or \\(1-f\\) with \\(p=0.51\\) and \\(f = 0.05\\)\nPositive Expectation\nCaveat: Growth governed by the median/entropy \\[\n\\begin{aligned}\n  p \\log( 1 + f) & + (1-p)\\log(1-f) \\\\\n   & = -0.00025 &lt; 0\n  \\end{aligned}\n\\] Brownian Ratchets and cross-entropy of Markov processes\n\n\n\n\nTwo Losing Bets+Volatility"
  },
  {
    "objectID": "02-utility.html#parrando",
    "href": "02-utility.html#parrando",
    "title": "Bayes AI",
    "section": "Parrando",
    "text": "Parrando\n\n\n\n\n\n\n\n\n\nOptimal allocation \\(\\omega\\)\n\n\n\n\n\n\n\nEx Ante vs Ex Post Realisation"
  },
  {
    "objectID": "02-utility.html#breiman-kelly-merton-rule",
    "href": "02-utility.html#breiman-kelly-merton-rule",
    "title": "Bayes AI",
    "section": "Breiman-Kelly-Merton Rule",
    "text": "Breiman-Kelly-Merton Rule\nKelly Criterion: Optimal wager in binary setting \\[\n\\omega^\\star = \\frac{p \\cdot O  -q }{O}\n\\]\nMerton’s Rule: in continuous setting is Kelly \\[\n\\omega^\\star = \\frac{1}{\\gamma} \\frac{\\mu}{\\sigma^2}\n\\]\n\n\\(\\mu\\): (excess) expected return\n\\(\\sigma\\): volatility\n\\(\\gamma\\): risk aversion\n\\(\\omega^\\star\\): optimal position size\n\\(p= Prob(Up), q = Prob(Down), O = Odds\\)"
  },
  {
    "objectID": "02-utility.html#example-kelly-criterion-sp500",
    "href": "02-utility.html#example-kelly-criterion-sp500",
    "title": "Bayes AI",
    "section": "Example: Kelly Criterion S&P500:",
    "text": "Example: Kelly Criterion S&P500:\nConsider logarithmic utility (CRRA with \\(\\gamma=1\\)). This is a pure Kelly rule.\n\nWe assume iid log-normal stock returns with an annualized expected excess return of \\(5.7\\)% and a volatility of \\(16\\)% which is consistent will long-run equity returns. In our continuous time formulation \\(\\omega^\\star = 0.057/0.16^2 = 2.22\\) and the Kelly criterion which imply that the investor borrows \\(122\\)% of wealth to invest a total of \\(220\\)% in stocks. This is a the risk-profile of the Kelly criterion.\nOne also sees that the allocation is highly sensitive to estimation error in \\(\\hat{\\mu}\\). We consider dynamic learning in a later section and show how the long horizon and learning affects the allocation today."
  },
  {
    "objectID": "02-utility.html#fractional-kelly",
    "href": "02-utility.html#fractional-kelly",
    "title": "Bayes AI",
    "section": "Fractional Kelly",
    "text": "Fractional Kelly\nThe fractional Kelly rule leads to a more realistic allocation.\n\nSuppose that \\(\\gamma = 3\\). Then the informational ratio is \\[\n\\frac{\\mu}{\\sigma^2} = \\frac{0.057}{0.16} = 0.357 \\; {\\rm and} \\;\n  \\omega^\\star = \\frac{1}{3} \\frac{0.057}{0.16^2} = 74.2\\%\n\\]\nAn investor with such a level of risk aversion then has a more reasonable \\(74.2\\)% allocation.\nThis analysis ignores the equilibrium implications. If everyinvestor acted this way, then this would drive up prices and drive down the equity premium of \\(5.7\\)%."
  },
  {
    "objectID": "02-utility.html#rule",
    "href": "02-utility.html#rule",
    "title": "Bayes AI",
    "section": "60-40 Rule",
    "text": "60-40 Rule"
  },
  {
    "objectID": "02-utility.html#keynes",
    "href": "02-utility.html#keynes",
    "title": "Bayes AI",
    "section": "Keynes",
    "text": "Keynes\nOptimal Bayes Rebalancing\n\n\n\n\n\n\n\n\n\nOptimal allocation \\(\\omega\\)\n\n\n\n\n\n\n\nKeynes vs Universal vs Cash"
  },
  {
    "objectID": "02-utility.html#winners-curse",
    "href": "02-utility.html#winners-curse",
    "title": "Bayes AI",
    "section": "Winner’s Curse",
    "text": "Winner’s Curse\nImmediately after you have win, you should feel a little regret!\nClaiming racehorse whose value is uncertain\n\n\n\nValue\nOutcome\n\n\n\n\n0\nhorse never wins\n\n\n50,000\nhorse improves\n\n\n\nSimple expected value tells you \\[\nE(X) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 50,000 = \\$25,000.\n\\] In a $20,000 claiming race (you can buy the horse for this fixed fee ahead of time from the owner) it looks like a simple decision to claim the horse.\nAsymmetric information!"
  },
  {
    "objectID": "02-utility.html#lemons-problem",
    "href": "02-utility.html#lemons-problem",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nAsymmetric information.\nProposed by George Akerlof in his 1970 paper “The Market for Lemons: Quality Uncertainty and the Market Mechanism.”\nThe lemons principle: low-value cars force high-value cars out of the market because of the asymmetrical information\nSeller does not know what the true value of a used car is and, not willing to pay a premium o\nSellers are not willing to sell below the premium price so this results in only lemons being sold."
  },
  {
    "objectID": "02-utility.html#lemons-problem-1",
    "href": "02-utility.html#lemons-problem-1",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nSuppose that a dealer pays $20K for a car and wants to sell for $25K, a lemon is only worth $5K.\nLet’s first suppose only 10% of cars are lemons, the customer’s calculations are \\[\nE (X)= \\frac{9}{10} \\cdot 25 + \\frac{1}{10} \\cdot 5 = \\$ 23 K\n\\]\nDealer is missing $2000. Therefore, they should try and persuade the customer its not a lemon by offering a warranty for example."
  },
  {
    "objectID": "02-utility.html#lemons-problem-2",
    "href": "02-utility.html#lemons-problem-2",
    "title": "Bayes AI",
    "section": "Lemon’s Problem",
    "text": "Lemon’s Problem\n\nThe more interesting case is when \\(p=0.5\\). The customer now values the car at \\[\nE (X)  = \\frac{1}{2} \\cdot 25 + \\frac{1}{2} \\cdot 5 = \\$ 15K\n\\]\nThis is lower than the $20K – the reservation price that the dealer would have for a good car. Now what type of car and at what price do they sell?\nAt $15K dealer is only willing to sell a lemon.\nBut then if the customer computes a conditional expectation \\[\nE ( X \\mid L ) = 1 \\cdot  5 = \\$ 5K\n\\] Therefore only lemons sell, at $ 5K, even if the dealer has a perfectly good car the customer is not willing to buy!\n\nAgain what should the dealer do?"
  },
  {
    "objectID": "02-utility.html#medical-testing",
    "href": "02-utility.html#medical-testing",
    "title": "Bayes AI",
    "section": "Medical Testing",
    "text": "Medical Testing\n\nA patient goes to see a doctor.\nThe doctor performs a test which is 95% sensitive – that is 95 percent of people who are sick test positive and 99% specific – that is 99 percent of the healthy people test negative.\nThe doctor also knows that only 1 percent of the people in the country are sick. Now the question is: if the patient tests positive, what are the chances the patient is sick? The intuitive answer is 99 percent, but the correct answer is 66 percent."
  },
  {
    "objectID": "02-utility.html#decision-trees-medical-testing",
    "href": "02-utility.html#decision-trees-medical-testing",
    "title": "Bayes AI",
    "section": "Decision Trees: Medical Testing",
    "text": "Decision Trees: Medical Testing\n\n\\(D=1\\) that indicates you have a disease\n\\(T=1\\) that indicates you tested positive\n\n\n\nCode\nflowchart LR\n  D[D] --&gt;|0.02| D1(D=1)\n  D --&gt;|0.98| D0(D=0)\n  D1 --&gt;|0.95| D1T1(T=1)\n  D1 --&gt;|0.05| D1T0(T=0)\n  D0 --&gt;|0.01| D0T1(T=1)\n  D0 --&gt;|0.99| D0T0(T=0)\n\n\n\n\n\n\nflowchart LR\n  D[D] --&gt;|0.02| D1(D=1)\n  D --&gt;|0.98| D0(D=0)\n  D1 --&gt;|0.95| D1T1(T=1)\n  D1 --&gt;|0.05| D1T0(T=0)\n  D0 --&gt;|0.01| D0T1(T=1)\n  D0 --&gt;|0.99| D0T0(T=0)\n\n\n\n\nFigure 1: Medical Diagnostics Decision Tree."
  },
  {
    "objectID": "02-utility.html#medical-testing-intuition",
    "href": "02-utility.html#medical-testing-intuition",
    "title": "Bayes AI",
    "section": "Medical Testing: Intuition",
    "text": "Medical Testing: Intuition\n\nImagine that the above story takes place in a small town, with \\(1,000\\) people.\nPrior: 20 people, are sick, and \\(980\\) are healthy.\nAminister the test to everyone: 19 of the 20 sick people test positive, 9.8 of the healthy people test positive, we round it to 10.\nNow if the doctor sends everyone who tests positive to the national hospital, there will be 10 healthy and 19 sick patients. 1 to 2 ratio or 66 percent of the patients are healthy."
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility",
    "href": "02-utility.html#medical-testing-with-utility",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\n\nThe decision problem is to treat \\(a_T\\) or not to treat \\(a_N\\)\n\n\nUtility of the test and the treatment.\n\n\nA/S\n\\(a_T\\)\n\\(a_N\\)\n\n\n\n\n\\(D_0\\)\n90\n100\n\n\n\\(D_1\\)\n90\n0\n\n\n\nThen expected (unconditional) utility of the treatment is 90 and no treatment is 98. A huge difference. Given our prior knowledge, we should not treat everyone.\nHow does the utility will change when our probability of disease changes?"
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility-1",
    "href": "02-utility.html#medical-testing-with-utility-1",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\n\n\nCode\np = seq(0,1,0.01)\nplot(p, 100*(1-p), type = \"l\", xlab = \"p\", ylab = \"$E[U(a)]$\")\nabline(h=90, col=\"red\")\nlegend(\"bottomleft\", legend = c(\"$E[U(a_N)]$\", \"$E[U(a_T)]$\"), col = c(\"black\", \"red\"), lty = 1, bty='n')\n\n\n\nExpected utility of the treatment and no treatment as a function of the prior probability of disease.The crossover point is. \\[\n100(1-p) = 90, ~p = 0.1\n\\]"
  },
  {
    "objectID": "02-utility.html#medical-testing-with-utility-2",
    "href": "02-utility.html#medical-testing-with-utility-2",
    "title": "Bayes AI",
    "section": "Medical Testing: With utility",
    "text": "Medical Testing: With utility\nThe gap of of \\(0.9-100(1-p)\\) is the expected gain from treatment.\n\n\nCode\nplot(p, 90-100*(1-p), type = \"l\", xlab = \"p\", ylab = \"Utility gain from treatment\")"
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test",
    "href": "02-utility.html#medical-testing-the-value-of-test",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\nWe will need to calculate the posterior probabilities\n\n\nCode\n# P(D | T = 0) = P(T = 0 | D) P(D) / P(T = 0)\npdt0 = 0.05*0.02/(0.05*0.02 + 0.99*0.98) \nprint(pdt0)\n\n\n[1] 0.001029654\n\n\nCode\n# Expected utility given the test is negative \n# E[U(a_N | T=0)]\nUN0 = pdt0*0 + (1-pdt0)*100\nprint(UN0)\n\n\n[1] 99.89703\n\n\nCode\n# E[U(a_T | T=0)]\nUT0 = pdt0*90 + (1-pdt0)*90\nprint(UT0)\n\n\n[1] 90\n\n\nGiven test is negative, our best action is not to treat. Our utility is 100. What if the test is positive?"
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test-1",
    "href": "02-utility.html#medical-testing-the-value-of-test-1",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\n\n\nCode\n# P(D | T = 1) = P(T = 1 | D) P(D) / P(T = 1)\npdt = 0.95*0.02/(0.95*0.02 + 0.01*0.98)\nprint(pdt)\n\n\n[1] 0.6597222\n\n\nCode\n# E[U(a_N | T=1)]\nUN1 = pdt*0 + (1-pdt)*100\nprint(UN1)\n\n\n[1] 34.02778\n\n\nCode\n# E[U(a_T | T=1)]\nUT1 = pdt*90 + (1-pdt)*90\nprint(UT1)\n\n\n[1] 90\n\n\nThe best option is to treat now! Given the test our strategy is to treat if the test is positive and not treat if the test is negative."
  },
  {
    "objectID": "02-utility.html#medical-testing-the-value-of-test-2",
    "href": "02-utility.html#medical-testing-the-value-of-test-2",
    "title": "Bayes AI",
    "section": "Medical Testing: The value of test",
    "text": "Medical Testing: The value of test\nLet’s calculate the expected utility of this strategy.\n\n\nCode\n# P(T=1) = P(T=1 | D) P(D) + P(T=1 | D=0) P(D=0)\npt = 0.95*0.02 + 0.01*0.98\nprint(pt)\n\n\n[1] 0.0288\n\n\nCode\n# P(T=0) = P(T=0 | D) P(D) + P(T=0 | D=0) P(D=0)\npt0 = 0.05*0.02 + 0.99*0.98\nprint(pt0)\n\n\n[1] 0.9712\n\n\nCode\n# Expected utility of the strategy\npt*UT1 + pt0*UN0\n\n\n[1] 99.612\n\n\nThe utility of out strategy of 100 is above of the strategy prior to testing (98), this difference of 2 is called the value of information."
  },
  {
    "objectID": "02-utility.html#nash-equilibrium",
    "href": "02-utility.html#nash-equilibrium",
    "title": "Bayes AI",
    "section": "Nash Equilibrium",
    "text": "Nash Equilibrium\n\nWhen multiple decision makers interact with each other, meaning the decision of one player changes the state of the “world” and thus affects the decision of another player - Nash equilibrium: a set of strategies where no player can improve their payoff by unilaterally changing their strategy, assuming others keep their strategies constant.\nNo player has an incentive to deviate from their current strategy, given the strategies of the other players."
  },
  {
    "objectID": "02-utility.html#nash-equilibrium-1",
    "href": "02-utility.html#nash-equilibrium-1",
    "title": "Bayes AI",
    "section": "Nash Equilibrium",
    "text": "Nash Equilibrium\n\nPrisoner’s Dilemma: Two prisoners must decide whether to cooperate with each other or defect. The Nash equilibrium is for both to defect, even though they would be better off if they both cooperated.\nPricing Strategies: Firms in a market choose prices to maximize profits, taking into account their competitors’ pricing decisions. The equilibrium is the set of prices where no firm can increase profits by changing its price unilaterally.\nTraffic Flow: Drivers choose routes to minimize travel time, based on their expectations of other drivers’ choices. The equilibrium is the pattern of traffic flow where no driver can reduce their travel time by choosing a different route."
  },
  {
    "objectID": "02-utility.html#marble-game",
    "href": "02-utility.html#marble-game",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nTwo players \\(A\\) and \\(B\\) have both a red and a blue marble. They present one marble to each other. The payoff table is as follows:\n\nIf both present red, \\(A\\) wins $3.\nIf both present blue, \\(A\\) wins $1.\nIf the colors do not match, \\(B\\) wins $2\n\nThe tit-for-tat strategy, where you cooperate until your opponent defects. Then you match his last response."
  },
  {
    "objectID": "02-utility.html#marble-game-1",
    "href": "02-utility.html#marble-game-1",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nNash equilibrium will also allow us to study the concept of a randomized strategy (ie. picking a choice with a certain probability) which turns out to be optimal in many game theory problems.\nFirst, assume that the players have a \\(\\frac{1}{2}\\) probability of playing Red or Blue. Thus each player has the same expected payoff \\(E(A) = \\$1\\) \\[\\begin{align*}\n    E(A) &= \\frac{1}{4} \\cdot 3 + \\frac{1}{4} \\cdot 1 =1 \\\\\n    E(B) &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 =1\n\\end{align*}\\]"
  },
  {
    "objectID": "02-utility.html#marble-game-2",
    "href": "02-utility.html#marble-game-2",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nWe might go one step further and look at the risk (and measured by a standard deviation) and calculate the variances of each players payout \\[\\begin{align*}\n    Var (A) & = (1-1)^2 \\cdot \\frac{1}{4} +(3-1)^2 \\cdot \\frac{1}{4} + (0-1)^2 \\cdot \\frac{1}{2} = 1.5 \\\\\n    Var(B) & = 1^2 \\cdot \\frac{1}{2} + (2-1)^2 \\cdot \\frac{1}{2} = 1\n\\end{align*}\\] Therefore, under this scenario, if you are risk averse, player \\(B\\) position is favored."
  },
  {
    "objectID": "02-utility.html#marble-game-3",
    "href": "02-utility.html#marble-game-3",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nThe matrix of probabilities with equally likely choices is given by\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( red, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\nNow they is no reason to assume ahead of time that the players will decide to play \\(50/50\\). We will show that there’s a mixed strategy (randomized) that is a Nash equilibrium that is, both players won’t deviate from the strategy."
  },
  {
    "objectID": "02-utility.html#marble-game-4",
    "href": "02-utility.html#marble-game-4",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nWe’ll prove that the following equilibrium happens:\n\n\\(A\\) plays Red with probability 1/2 and blue 1/2\n\\(B\\) plays Red with probability 1/4 and blue 3/4\n\nIn this case the expected payoff to playing Red equals that of playing Blue for each player. We can simply calculate: \\(A\\)’s expected payoff is 3/4 and \\(B\\)’s is $1 \\[\nE(A) = \\frac{1}{8} \\cdot 3 + \\frac{3}{8} \\cdot 1 = \\frac{3}{4}\n\\] Moreover, \\(E(B) =1\\), thus \\(E(B) &gt; E(A)\\). We see that \\(B\\) is the favored position. It is simple that if I know that you are going to play this strategy and vice-versa, neither of us will deviate from this strategy – hence the Nash equilibrium concept."
  },
  {
    "objectID": "02-utility.html#marble-game-5",
    "href": "02-utility.html#marble-game-5",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nNash equilibrium probabilities are: \\(p=P( A \\; red )= 1/2, p_1 = P( B \\; red ) = 1/4\\) with payout matrix\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( red, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\\(P( blue, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( blue, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\nWe have general payoff probabilities: \\(p=P( A \\; red ), p_1 = P( B \\; red )\\)\n\\[\\begin{align*}\n    f_A ( p , p_1 ) =& 3 p p_1 + ( 1 -p ) ( 1 - p_1 ) \\\\\n    f_B ( p , p_1 ) =& 2 \\{ p(1 - p_1) + ( 1 -p ) p_1 \\}\n\\end{align*}\\]"
  },
  {
    "objectID": "02-utility.html#marble-game-6",
    "href": "02-utility.html#marble-game-6",
    "title": "Bayes AI",
    "section": "Marble Game",
    "text": "Marble Game\nTo find the equilibrium point \\[\\begin{align*}\n    ( \\partial / \\partial p ) f_A ( p , p_1 ) =& 3 p_1 - ( 1 - p_1 ) = 4 p_1 -1 \\; \\; \\mathrm{so} \\; \\; p_1= 1/4 \\\\\n    ( \\partial / \\partial p_1 ) f_B ( p , p_1 ) =& 2 ( 1 - 2p ) \\; \\; \\mathrm{so} \\; \\; p= 1/2\n\\end{align*}\\]\nMuch research has been directed to repeated games versus the one-shot game and is too large a topic to discuss further.\n What are the drawbacks of the equilibrium analysis?"
  },
  {
    "objectID": "01-intro.html#random-facts",
    "href": "01-intro.html#random-facts",
    "title": "Bayes AI",
    "section": "Random facts",
    "text": "Random facts\nOn this Day (January 27):\n\n1888: The National Geographic Society is founded in Washington, D.C.\n1945: The Red Army liberates the Auschwitz-Birkenau concentration\n1967: The United States, United Kingdom, and Soviet Union sign the Outer Space Treaty in Washington, D.C.\n1973: The Paris Peace Accords officially end the Vietnam War.\n2010: Apple Inc. unveils the iPad."
  },
  {
    "objectID": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "href": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "title": "Bayes AI",
    "section": "The first thoughts about artificial intelligence",
    "text": "The first thoughts about artificial intelligence\n\n\n\n\n\n\n\nHephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay.1\nParticularly wise rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas)."
  },
  {
    "objectID": "01-intro.html#mechanical-machines",
    "href": "01-intro.html#mechanical-machines",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-1",
    "href": "01-intro.html#mechanical-machines-1",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nJaquet-Droz automata (XVIII century):"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-2",
    "href": "01-intro.html#mechanical-machines-2",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …"
  },
  {
    "objectID": "01-intro.html#turing-test",
    "href": "01-intro.html#turing-test",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\n\nAI as a science begins with a Turing test (1950).\nThe ides of the Turing test is to check if a machine can imitate a human in a conversation.\nThe original formulaiton was more nuancesd."
  },
  {
    "objectID": "01-intro.html#shennons-theseus",
    "href": "01-intro.html#shennons-theseus",
    "title": "Bayes AI",
    "section": "Shennon’s Theseus",
    "text": "Shennon’s Theseus\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze."
  },
  {
    "objectID": "01-intro.html#stanford-cart",
    "href": "01-intro.html#stanford-cart",
    "title": "Bayes AI",
    "section": "Stanford Cart",
    "text": "Stanford Cart\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention."
  },
  {
    "objectID": "01-intro.html#turing-test-1",
    "href": "01-intro.html#turing-test-1",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop",
    "href": "01-intro.html#dartmouth-workshop",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop-1",
    "href": "01-intro.html#dartmouth-workshop-1",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."
  },
  {
    "objectID": "01-intro.html#great-hopes",
    "href": "01-intro.html#great-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Great hopes",
    "text": "1956-1960: Great hopes\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead."
  },
  {
    "objectID": "01-intro.html#big-hopes",
    "href": "01-intro.html#big-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Big Hopes",
    "text": "1956-1960: Big Hopes\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks)."
  },
  {
    "objectID": "01-intro.html#sknowledge-based-systems",
    "href": "01-intro.html#sknowledge-based-systems",
    "title": "Bayes AI",
    "section": "1970s:Knowledge Based Systems",
    "text": "1970s:Knowledge Based Systems\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors."
  },
  {
    "objectID": "01-intro.html#commercial-applications-industry-ai",
    "href": "01-intro.html#commercial-applications-industry-ai",
    "title": "Bayes AI",
    "section": "1980-2010: Commercial applications Industry AI",
    "text": "1980-2010: Commercial applications Industry AI\n\nThe first AI department was at Dec (Digital Equipment Corporation)It is argued that by 1986 he saved the Dec about  $ 10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations."
  },
  {
    "objectID": "01-intro.html#data-mining-machine-learning",
    "href": "01-intro.html#data-mining-machine-learning",
    "title": "Bayes AI",
    "section": "1990-2010: DATA MINING, MACHINE LEARNING",
    "text": "1990-2010: DATA MINING, MACHINE LEARNING\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes",
    "href": "01-intro.html#rule-based-system-vs-bayes",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations\ntraditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. - It lead to them being brittle in changing environments.\nOn the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning.\nAll of those are data-driven approaches.\nThese approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes-1",
    "href": "01-intro.html#rule-based-system-vs-bayes-1",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "01-intro.html#definition",
    "href": "01-intro.html#definition",
    "title": "Bayes AI",
    "section": "DEFINITION",
    "text": "DEFINITION\n\nHow to determine “learning”?\n\n\n\n\nDefinition:\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml",
    "href": "01-intro.html#tasks-and-concepts-of-ml",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML",
    "text": "Tasks and concepts of ML"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: Supervised Learning",
    "text": "Tasks and concepts of ML: Supervised Learning\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Compition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection. -Often the outputs answers are given for a small part of the data, then we call it semi -supervised Learning."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: reinforcement learning",
    "text": "Tasks and concepts of ML: reinforcement learning\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewardas possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: active learning",
    "text": "Tasks and concepts of ML: active learning\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai",
    "href": "01-intro.html#tasks-and-concepts-of-ai",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI",
    "text": "Tasks and concepts of AI"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "href": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Reasoning",
    "text": "Tasks and concepts of AI: Reasoning\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Representation",
    "text": "Tasks and concepts of AI: Representation\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th"
  },
  {
    "objectID": "01-intro.html#chess-and-ai",
    "href": "01-intro.html#chess-and-ai",
    "title": "Bayes AI",
    "section": "Chess and AI",
    "text": "Chess and AI\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\nKasparov vs IBM’s DeepBlue in 1997"
  },
  {
    "objectID": "01-intro.html#alphago-zero",
    "href": "01-intro.html#alphago-zero",
    "title": "Bayes AI",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two"
  },
  {
    "objectID": "01-intro.html#probability-in-machine-learning",
    "href": "01-intro.html#probability-in-machine-learning",
    "title": "Bayes AI",
    "section": "Probability in machine learning",
    "text": "Probability in machine learning\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values ​​will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "01-intro.html#references",
    "href": "01-intro.html#references",
    "title": "Bayes AI",
    "section": "References",
    "text": "References\n\nChristopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2007.\nKevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press, 2013.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., Springer, 2009."
  },
  {
    "objectID": "01-intro.html#probability",
    "href": "01-intro.html#probability",
    "title": "Bayes AI",
    "section": "Probability",
    "text": "Probability\nSubjective Probability (de Finetti, Ramsey, Savage, von Neumann, ... )\nPrinciple of Coherence:\nA set of subjective probability beliefs must avoid sure loss\n\nIf an event \\(A\\) is certain to occur, it has probability \\(1\\)\nEither an event \\(A\\) occurs or it does not. \\[\nP(A) = 1 - P(\\mbox{not }A)\n\\]\nIf two events are mutually exclusive (both cannot occur simultaneously) then \\[\nP(A \\mbox{ or } B) = P(A) + P(B)\n\\]\nJoint probability, when events are independent \\[\nP(A \\mbox{ and } B) = P( A) P(B)\n\\]"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nUse probability to describe outcomes involving more than one variable at a time. Need to be able to measure what we think will happen to one variable relative to another\nIn general the notation is ...\n\n\\(P(X=x, Y=y )\\) is the joint probability that \\(X =x\\) and \\(Y=y\\)\n\\(P(X=x  \\mid  Y=y )\\) is the conditional probability that \\(X\\) equals \\(x\\) given \\(Y=y\\)\n\\(P(X=x)\\) is the marginal probability of \\(X=x\\)"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nRelationship between the joint and conditional ... \\[\n\\begin{aligned}\nP(x,y) & = P(x) P(y \\mid x) \\\\\n& =  P(y) P(x \\mid y)\n\\end{aligned}\n\\]\nRelationship between the joint and marginal ... \\[\n\\begin{aligned}\nP(x) & = \\sum_y P(x,y) \\\\\nP(y) & =  \\sum_x P(x,y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-rule",
    "href": "01-intro.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nThe computation of \\(P(x \\mid y)\\) from \\(P(x)\\) and \\(P(y \\mid x)\\) is called Bayes theorem ... \\[\nP(x \\mid y) = \\frac{P(y,x)}{P(y)} = \\frac{P(y,x)}{\\sum_x P(y,x)} = \\frac{P(y \\mid x)P(x)}{\\sum_x P(y \\mid x)P(x)}\n\\]\nThis shows now the conditional distribution is related to the joint and marginal distributions.\nYou’ll be given all the quantities on the r.h.s."
  },
  {
    "objectID": "01-intro.html#bayes-rule-1",
    "href": "01-intro.html#bayes-rule-1",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nKey fact: \\(P(x \\mid y)\\) is generally different from \\(P(y \\mid x)\\)!\nExample: Most people would agree \\[\n\\begin{aligned}\nPr  & \\left ( Practice \\; hard  \\mid  Play \\; in \\; NBA \\right ) \\approx  1\\\\\nPr  & \\left ( Play \\; in \\; NBA  \\mid  Practice \\; hard  \\right ) \\approx  0\n\\end{aligned}\n\\]\nThe main reason for the difference is that \\(P( Play \\; in \\; NBA ) \\approx 0\\)."
  },
  {
    "objectID": "01-intro.html#independence",
    "href": "01-intro.html#independence",
    "title": "Bayes AI",
    "section": "Independence",
    "text": "Independence\nTwo random variable \\(X\\) and \\(Y\\) are independent if \\[\nP(Y = y  \\mid X = x) = P (Y = y)\n\\] for all possible \\(x\\) and \\(y\\) values. Knowing \\(X=x\\) tells you nothing about \\(Y\\)!\nExample: Tossing a coin twice. What’s the probability of getting \\(H\\) in the second toss given we saw a \\(T\\) in the first one?"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nimageSource: The Secret Betting Strategy That Beats Online Bookmakers"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nBookies set odds that reflect their best guess on probabilities of a win, draw, or loss. Plus their own margin\nBookies have risk aversion bias. When many people bet for an underdog (more popular team)\nBookies hedge their bets by offering more favorable odds to the opposed team\nSimple algorithm: calculate average odds across many bookies and find outliers with large deviation from the mean"
  },
  {
    "objectID": "01-intro.html#odds-oddschecker",
    "href": "01-intro.html#odds-oddschecker",
    "title": "Bayes AI",
    "section": "Odds: Oddschecker",
    "text": "Odds: Oddschecker\nWe can express probabilities in terms of Odds via \\[\nO(A) = \\frac{ 1- P(A) }{ P(A) }\n\\; \\; {\\rm or} \\; \\; P(A) = \\frac{ 1 }{ 1 + O(A) }\n\\]\n\nFor example if \\(O(A) = 1\\) then for ever $1 bet you will payout $1. An event with probability \\(\\frac{1}{2}\\).\nIf \\(O(A) = 2\\) or \\(2:1\\), then for a $1 bet you’ll payback $3.\n\nIn terms of probability \\(P = \\frac{1}{3}\\)."
  },
  {
    "objectID": "01-intro.html#envelope-paradox",
    "href": "01-intro.html#envelope-paradox",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nThe following problem is known as the “exchange paradox”.\n\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent.\nThe amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope."
  },
  {
    "objectID": "01-intro.html#envelope-paradox-1",
    "href": "01-intro.html#envelope-paradox-1",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nYou know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch!"
  },
  {
    "objectID": "01-intro.html#bayes-rule-2",
    "href": "01-intro.html#bayes-rule-2",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nWhere’s the flaw in this argument? Use Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami.\nSuch an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by\n\n\\[\nE(y) =  p \\left ( y = \\frac{1}{2} x \\; \\vert \\;  x \\right ) \\cdot  \\frac{1}{2} x +\np \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E( y) &gt; x\\) becomes a decision rule."
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma",
    "href": "01-intro.html#prisoners-dilemma",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nThree prisoners \\({\\cal A} , {\\cal B} , {\\cal C}\\).\nEach believe are equally likely to be set free.\nPrisoner \\({\\cal A}\\) goes to the warden \\({\\cal W}\\) and asks if s/he is getting axed.\n\nThe Warden can’t tell \\({\\cal A}\\) anything about him.\nHe provides the new information: \\({\\cal WB}\\) = “\\({\\cal B}\\) is to be executed”"
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma-1",
    "href": "01-intro.html#prisoners-dilemma-1",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nUniform Prior Probabilities: \\[\n\\begin{array}{c|ccc}\nPrior & {\\cal A}  & {\\cal B}  & {\\cal C}  \\\\\\hline\n{\\cal P} ( {\\rm Pardon} ) & 0.33 & 0.33 & 0.33\n\\end{array}\n\\]\nPosterior: Compute \\(P ( {\\cal A} | {\\cal WB} )\\)?\n What happens if \\({\\cal C}\\) overhears the conversation?\n Compute \\(P ( {\\cal C} | {\\cal WB} )\\)?"
  },
  {
    "objectID": "01-intro.html#game-show-problem",
    "href": "01-intro.html#game-show-problem",
    "title": "Bayes AI",
    "section": "Game Show Problem",
    "text": "Game Show Problem\nNamed after the host of the long-running TV show, Let’s make a Deal.\n\nA contestant is given the choice of 3 doors.\n\nThere is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats.\n\nThe optimal strategy is counter-intuitive"
  },
  {
    "objectID": "01-intro.html#puzzle",
    "href": "01-intro.html#puzzle",
    "title": "Bayes AI",
    "section": "Puzzle",
    "text": "Puzzle\nThe game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat.\nYou have the choice of switching doors.\n\n Is it advantageous to switch?\n Assume you pick door \\(A\\) at random. Then \\(P(A) = ( 1 /3 )\\).\nYou need to figure out \\(P( A | MB )\\) after Monte reveals \\(B\\) is a goat."
  },
  {
    "objectID": "01-intro.html#bayes-rule-3",
    "href": "01-intro.html#bayes-rule-3",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nIn its simplest form.\n\nTwo events \\({\\cal A}\\) and \\({\\cal B}\\). Bayes rule \\[\nP ( A | B ) = \\frac{P (  A \\cap  B )}{ P ( B )}\n= \\frac{P ( B | A ) P ( A )}{ P ( B )}\n\\]\nLaw of Total Probability \\[\nP ( B ) = P ( B | A ) P ( A ) + P ( B | \\bar{A} ) P ( \\bar{A} )\n\\] Hence we can calculate the denominator of Bayes rule."
  },
  {
    "objectID": "01-intro.html#bayes-theroem",
    "href": "01-intro.html#bayes-theroem",
    "title": "Bayes AI",
    "section": "Bayes Theroem",
    "text": "Bayes Theroem\nMany problems in decision making can be solved using Bayes rule.\n\nAI: Rule-based decision making.\nIt’s counterintuitive! But gives the “right” answer.\n\nBayes Rule: \\[\n\\mbox{P}(A|B) = \\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} = \\frac{  \\mbox{P}(B|A) \\mbox{P}(A)}{ \\mbox{P}(B)}\n\\] Law of Total Probability: \\[\n\\mbox{P}(B) =  \\mbox{P}(B|A) \\mbox{P}(A ) +  \\mbox{P}(B| \\bar{A} ) \\mbox{P}(\\bar{A} )\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch",
    "href": "01-intro.html#apple-watch",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\nThe Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4?\nBayes’ Theorem: \\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch-1",
    "href": "01-intro.html#apple-watch-1",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\n\n\n\n\natrial fibrillation\n1960\n980\n\n\nno atrial fibrillation\n40\n97020\n\n\n\n\\[\n0.6667\n=\n\\frac{0.98\\cdot 0.02}{\n0.0294}\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%."
  },
  {
    "objectID": "01-intro.html#abraham-wald",
    "href": "01-intro.html#abraham-wald",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nHow Abraham Wald improved aircraft survivability. Raw Reports from the Field\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n?\n\n\nCockpit\n36\n?\n\n\nFuselage\n105\n?\n\n\nNone\n146\n0\n\n\n\nThis fact would allow Wald to estimate: \\[\nP(\\text{damage on fuselage} \\mid \\text{returns safely}) = 105/316 \\approx 32\\%\n\\] You need the inverse probability : \\[\nP(\\text{returns safely} \\mid \\text{damage on fuselage})\n\\] Completely different!"
  },
  {
    "objectID": "01-intro.html#abraham-wald-1",
    "href": "01-intro.html#abraham-wald-1",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nImputation: fill-in missing data.\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n31\n\n\nCockpit\n36\n21\n\n\nFuselage\n105\n8\n\n\nNone\n146\n0\n\n\n\nThen Wald got: \\[\n\\begin{aligned}\nP(\\text{returns safely} \\mid \\text{damage on fuselage}) & =\\frac{105}{105+8}\\approx 93\\%\\\\\nP(\\text{returns safely} \\mid \\text{damage on engine}) & =\\frac{29}{29+31}\\approx 48\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#personalization-conditional-probability",
    "href": "01-intro.html#personalization-conditional-probability",
    "title": "Bayes AI",
    "section": "“Personalization\" \\(=\\)”Conditional Probability\"",
    "text": "“Personalization\" \\(=\\)”Conditional Probability\"\n\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\n\n Many Business Applications!! Suggestions vs Search…."
  },
  {
    "objectID": "01-intro.html#probability-as-evidence",
    "href": "01-intro.html#probability-as-evidence",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nevidence: known facts about criminal (e.g. blood type, DNA, ...)\nsuspect: matches a trait with evidence at scene of crime\nLet \\({\\cal G}\\) denote the event that the suspect is the criminal.\nBayes computes the conditional probability of guilt\n\\[\nP ( {\\cal G} | {\\rm evidence} )\n\\] Evidence \\({\\cal E}\\): suspect and criminal possess a common trait"
  },
  {
    "objectID": "01-intro.html#probability-as-evidence-1",
    "href": "01-intro.html#probability-as-evidence-1",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nBayes Theorem yields \\[\nP ( {\\cal G} | {\\rm evidence} )\n= \\frac{ P ( {\\rm evidence} | {\\cal G} ) P ( {\\cal G} ) }{ P ( {\\rm evidence} )}\n\\]\nIn terms of relative odds \\[\n\\frac{ P ( {\\cal I} | {\\rm evidence} ) }{ P ( {\\cal G} | {\\rm evidence} ) }\n= \\frac{ P ( {\\rm evidence} | {\\cal I} ) }{ P ( {\\rm evidence} | {\\cal G} ) }\n\\frac{ P ( {\\cal I} ) }{ P ( {\\cal G} ) }\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factors",
    "href": "01-intro.html#bayes-factors",
    "title": "Bayes AI",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nThere are two terms:\n\nPrior Odds of Guilt \\(O ( {\\cal G} ) = P ( {\\cal I} ) / P ( {\\cal G} )\\) ?\n\nHow many people on the island?\nSensitivity “what if” analysis?\n\nThe Bayes factor \\[\n\\frac{ P ( {\\rm evidence} | {\\cal I} ) }{ P ( {\\rm evidence} | {\\cal G} ) }\n\\] is common to all observers and updates everyone’s initials odds"
  },
  {
    "objectID": "01-intro.html#prosecutors-fallacy",
    "href": "01-intro.html#prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\nThe most common fallacy is confusing \\[\nP ( {\\rm evidence} | {\\cal G} ) \\; \\; {\\rm with} \\; \\;\nP ( {\\cal G} | {\\rm evidence} )\n\\]\nBayes rule yields \\[\nP ( {\\cal G} | {\\rm evidence} ) = \\frac{ P ( {\\rm evidence} | {\\cal G} ) p( {\\cal G} )}{ P ( {\\rm evidence}  )}\n\\] Your assessment of \\(P( {\\cal G} )\\) will matter."
  },
  {
    "objectID": "01-intro.html#island-problem",
    "href": "01-intro.html#island-problem",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nSuppose there’s a criminal on a island of \\(N+1\\) people.\n\nLet \\(I\\) denote innocence and \\(G\\) guilt.\nEvidence \\(E\\): the suspect matches a trait with the criminal.\nThe probabilities are \\[\np(E|I)=p\\;\\;\\mathrm{and}\\;\\;p(E|G)=1\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factor",
    "href": "01-intro.html#bayes-factor",
    "title": "Bayes AI",
    "section": "Bayes factor",
    "text": "Bayes factor\nBayes factors are likelihood ratios\n\nThe Bayes factor is given by \\[\n\\frac{p(E|I)}{p(E|G)}=p\n\\]\nIf we start with a uniform prior distribution we have\n\n\\[\np(I)=\\frac{1}{N+1}\\;\\;\\mathrm{and}\\;\\;odds(I)=N\n\\]\n\nPriors will matter!"
  },
  {
    "objectID": "01-intro.html#island-problem-1",
    "href": "01-intro.html#island-problem-1",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nPosterior Probability related to Odds \\[\np(I|y)=\\frac\n{1}{1+odds(I|y)}%\n\\]\n\nProsecutors’ fallacy\n\nThe posterior probability \\(p(I|y)\\neq p(y|I)=p\\).\n\nSuppose that \\(N=10^{3}\\) and \\(p=10^{-3}\\). Then\n\n\\[\np( I|y) = \\frac{1}{1 + 10^3 \\cdot 10^{-3}} = \\frac{1}{2}\n\\]\nThe odds on innocence are \\(odds(I|y)=1\\).\nThere’s a \\(50/50\\) chance that the criminal has been found."
  },
  {
    "objectID": "01-intro.html#sally-clark-case-independence-or-bayes",
    "href": "01-intro.html#sally-clark-case-independence-or-bayes",
    "title": "Bayes AI",
    "section": "Sally Clark Case: Independence or Bayes?",
    "text": "Sally Clark Case: Independence or Bayes?\nSally Clark was accused and convicted of killing her two children\nThey could have both died of SIDS.\n\nThe chance of a family which are non-smokers and over \\(25\\) having a SIDS death is around \\(1\\) in \\(8,500\\).\nThe chance of a family which has already had a SIDS death having a second is around \\(1\\) in \\(100\\).\nThe chance of a mother killing her two children is around \\(1\\) in \\(1,000,000\\)."
  },
  {
    "objectID": "01-intro.html#bayes-or-independence",
    "href": "01-intro.html#bayes-or-independence",
    "title": "Bayes AI",
    "section": "Bayes or Independence",
    "text": "Bayes or Independence\n\nUnder Bayes \\[\n\\begin{aligned}\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)   &  = P \\left(\n\\mathrm{first} \\; \\mathrm{SIDS} \\right)  P \\left(  \\mathrm{Second} \\; \\;\n\\mathrm{SIDS} | \\mathrm{first} \\; \\mathrm{SIDS} \\right) \\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}\n\\end{aligned}\n\\] The \\(\\frac{1}{100}\\) comes from taking into account genetics.\nIndependence, as the court did, gets you\n\n\\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000)\n\\]\n\nBy Bayes rule\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{P( E \\cap I)}{P( E \\cap G)}\n\\] \\(P( E \\cap I) = P(E|I )P(I)\\) needs discussion of \\(p(I)\\)."
  },
  {
    "objectID": "01-intro.html#comparison",
    "href": "01-intro.html#comparison",
    "title": "Bayes AI",
    "section": "Comparison",
    "text": "Comparison\n\nHence putting these two together gives the odds of guilt as\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1/850,000}{1/1,000,000} = 1.15\n\\] In terms of posterior probabilities\n\\[\np( G|E) = \\frac{1}{1 + O(G|E)} = 0.465\n\\]\n\nIf you use independence\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1}{73} \\; {\\rm and} \\; p( G|E) \\approx 0.99\n\\] The suspect looks guilty."
  },
  {
    "objectID": "01-intro.html#oj-simpson",
    "href": "01-intro.html#oj-simpson",
    "title": "Bayes AI",
    "section": "OJ Simpson",
    "text": "OJ Simpson\nThe O.J. Simpson trial was possibly the trail of the century\nThe murder of his wife Nicole Brown Simpson, and a friend, Ron Goldman, in June 1994 and the trial dominated the TV networks\n\nDNA evidence and probability: \\(p( E| G)\\)\nBayes Theorem: \\(p( G | E )\\)\nProsecutor’s Fallacy: \\(p( G|E ) \\neq p(E|G)\\)\n\nOdds ratio with gives \\[\n\\frac{ p( I|E) }{ p ( G | E ) } = \\frac{ p( E|I )}{ p( E|G) } \\frac{ p(I) }{p(G ) }\n\\] Prior odds conditioned on background information."
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem",
    "href": "01-intro.html#oj-simpson-bayes-theorem",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nSuppose that you are a juror in a murder case of a husband who is accused of killing his wife.\nThe husband is known is have battered her in the past.\nConsider the three events:\n\n\\(G\\) “husband murders wife in a given year”\n\\(M\\) “wife is murdered in a given year”\n\\(B\\) “husband is known to batter his wife”"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-1",
    "href": "01-intro.html#oj-simpson-bayes-theorem-1",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\n\nOnly \\(1/10\\)th of one percent of husbands who batter their wife actually murder them.\n\nConditional on eventually murdering their wife, there a one in ten chance it happens in a given year.\nIn 1994, \\(5000\\) women were murdered, \\(1500\\) by their husband\nGiven a population of \\(100\\) million women at the time \\[\np( M | I ) = \\frac{ 3500 }{ 10^8 } \\approx \\frac{1}{30,000} .\n\\] We’ll also need \\(p( M | I , B )  = p( M | I )\\)"
  },
  {
    "objectID": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "href": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "OJ Simpson: Prosecutor’s Fallacy",
    "text": "OJ Simpson: Prosecutor’s Fallacy\n\nLet \\(G =\\) Guilt and \\(E=\\) Evidence\nProsecutor’s Fallacy: \\(P(G|E) \\neq P(E|G)\\).\nDNA evidience gives \\(P( E | I )\\) – the \\(p\\)-value.\n\nWhat’s the “match probability” for a rare event?\nBayes theorem in Odds \\[\n\\frac{p(G|M,B)}{p(I|M,B)} = \\frac{p(M|G,B)}{p(M|I,B)} \\frac{p(G|B)}{p(I|B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-2",
    "href": "01-intro.html#oj-simpson-bayes-theorem-2",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nBy assumption,\n\n\\(p(M|G,B)=1\\)\n\\(p(M|I,B)= \\frac{1}{30,000}\\)\n\\(p( G|B) = \\frac{1}{1000}\\) and so\n\n\\[\n\\frac{p(G|B)}{p(I|B)} = \\frac{1}{999}\n\\]\nTherefore, \\[\n\\frac{p(G|M,B)}{p(I|M,B)} \\approx 30 \\; {\\rm and} \\; p(G|M,B) = \\frac{30}{31} \\approx 97\\%\n\\] More than a 50/50 chance that your spouse murdered you!"
  },
  {
    "objectID": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "href": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "title": "Bayes AI",
    "section": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)",
    "text": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)\nThe defense stated to the press: in any given year\n“Fewer than \\(1\\) in \\(2000\\) of batterers go on to murder their wives”.\n\nNow estimate \\(p( M | \\bar{G} , B ) = p( M| \\bar{G} ) = \\frac{1}{20,000}\\).\nThe Bayes factor is then\n\n\\[\n\\frac{ p( G | M , B ) }{ p( \\bar{G} | M , B ) } = \\frac{ 1/999 }{1 /20,000} = 20\n\\] which implies posterior probabilities\n\\[\np( \\bar{G} | M , B ) = \\frac{1}{1+20} \\; {\\rm and} \\; p( G | M , B ) = \\frac{20}{21}\n\\] Hence its over 95% chance that O.J. is guilty based on this information!\nDefense intended this information to exonerate O.J."
  },
  {
    "objectID": "01-intro.html#base-rate-fallacies",
    "href": "01-intro.html#base-rate-fallacies",
    "title": "Bayes AI",
    "section": "Base Rate Fallacies",
    "text": "Base Rate Fallacies\n“Witness” \\(80\\) % certain saw a “checker” \\(C\\) taxi in the accident.\n\nWhat’s your \\(P ( C | E )\\) ?\nNeed \\(P ( C )\\). Say \\(P( C ) = 0.2\\) and \\(P( E  | C) = 0.8\\).\nThen your posterior is\n\n\\[\nP ( C | E ) = \\frac{0.8 \\cdot 0.2}{ 0.8 \\cdot 0.2 + 0.2 \\cdot 0.8 } = 0.5\n\\]\nTherefore \\(O ( C ) = 1\\) a 50/50 bet."
  },
  {
    "objectID": "01-intro.html#updating-fallacies",
    "href": "01-intro.html#updating-fallacies",
    "title": "Bayes AI",
    "section": "Updating Fallacies",
    "text": "Updating Fallacies\nMost people don’t update quickly enough in light of new data\nWards Edwards 1960s\nWhen you have a small sample size, Bayes rule still updates probabilities\n\nTwo players: either \\(70\\) % A or \\(30\\) % A\nObserve \\(A\\) beats \\(B\\) \\(3\\) times out of \\(4\\).\nWhat’s \\(P ( A = 70 \\% \\; {\\rm player} )\\) ?"
  },
  {
    "objectID": "01-intro.html#a-random-image",
    "href": "01-intro.html#a-random-image",
    "title": "Bayes AI",
    "section": "A Random Image",
    "text": "A Random Image"
  },
  {
    "objectID": "03-conjugate.html#epl-odds",
    "href": "03-conjugate.html#epl-odds",
    "title": "Bayes AI",
    "section": "EPL Odds",
    "text": "EPL Odds"
  },
  {
    "objectID": "03-conjugate.html#english-premier-league-epl",
    "href": "03-conjugate.html#english-premier-league-epl",
    "title": "Bayes AI",
    "section": "English Premier League: EPL",
    "text": "English Premier League: EPL\nCalculate Odds for the possible scores in a match? \\[\n0-0, \\; 1-0, \\; 0-1, \\; 1-1, \\; 2-0, \\ldots\n\\] Let \\(X=\\) Goals scored by Arsenal\n\\(Y=\\) Goals scored by Liverpool\nWhat’s the odds of a team winning? \\(\\; \\; \\;  P \\left ( X&gt; Y \\right )\\) Odds of a draw? \\(\\; \\; \\;  P \\left ( X = Y \\right )\\)\nz1 = rpois(100,0.6) z2 = rpois(100,1.4) sum(z1&lt;z2)/100 # Team 2 wins sum(z1=z2)/100 # Draw"
  },
  {
    "objectID": "03-conjugate.html#chelsea-epl-2017",
    "href": "03-conjugate.html#chelsea-epl-2017",
    "title": "Bayes AI",
    "section": "Chelsea EPL 2017",
    "text": "Chelsea EPL 2017\nLet’s take a historical set of data on scores Then estimate \\(\\lambda\\) with the sample mean of the home and away scores\n\n\n\nhome team\nresults\n\nvisit team\n\n\n\n\nChelsea\n2\n1\nWest Ham\n\n\nChelsea\n5\n1\nSunderland\n\n\nWatford\n1\n2\nChelsea\n\n\nChelsea\n3\n0\nBurnley\n\n\n\\(\\dots\\)"
  },
  {
    "objectID": "03-conjugate.html#epl-chelsea",
    "href": "03-conjugate.html#epl-chelsea",
    "title": "Bayes AI",
    "section": "EPL Chelsea",
    "text": "EPL Chelsea\n\n\n\n\n\n\n\n\n\nChelsea against\n\n\n\n\n\n\n\nChelsea for\n\n\n\n\n\nOur Poisson model fits the empirical data!!"
  },
  {
    "objectID": "03-conjugate.html#epl-attack-and-defence-strength",
    "href": "03-conjugate.html#epl-attack-and-defence-strength",
    "title": "Bayes AI",
    "section": "EPL: Attack and Defence Strength",
    "text": "EPL: Attack and Defence Strength\nEach team gets an “attack” strength and “defence” weakness rating Adjust home and away average goal estimates"
  },
  {
    "objectID": "03-conjugate.html#epl-hull-vs-manu",
    "href": "03-conjugate.html#epl-hull-vs-manu",
    "title": "Bayes AI",
    "section": "EPL: Hull vs ManU",
    "text": "EPL: Hull vs ManU\nPoisson Distribution\nManU Average away goals \\(=1.47\\). Prediction: \\(1.47 \\times 1.46 \\times 1.37 = 2.95\\)\nAttack strength times Hull’s defense weakness times average\nHull Average home goals \\(=1.47\\). Prediction: \\(1.47 \\times 0.85 \\times 0.52 = 0.65\\). Simulation\n\n\n\nTeam Ex\npected Goals 0\n1\n2\n3\n4\n5\n\n\n\n\n\nMan U\n2.95 7\n22\n26\n12\n11\n13\n\n\n\nHull City\n0.65\n49\n41\n10\n0\n0\n0"
  },
  {
    "objectID": "03-conjugate.html#epl-predictions",
    "href": "03-conjugate.html#epl-predictions",
    "title": "Bayes AI",
    "section": "EPL Predictions",
    "text": "EPL Predictions\nA model is only as good as its predictions\n\nIn our simulation Man U wins 88 games out of 100, we should bet when odds ratio is below 88 to 100.\nMost likely outcome is 0-3 (12 games out of 100)\nThe actual outcome was 0-1 (they played on August 27, 2016)\nIn out simulation 0-1 was the fourth most probable outcome (9 games out of 100)"
  },
  {
    "objectID": "03-conjugate.html#bayesian-methods",
    "href": "03-conjugate.html#bayesian-methods",
    "title": "Bayes AI",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\nModern Statistical/Machine Learning\n\nBayes Rule and Probabilistic Learning\nComputationally challenging: MCMC and Particle Filtering\nMany applications in Finance:\n\nAsset pricing and corporate finance problems.\nLindley, D.V. Making Decisions\nBernardo, J. and A.F.M. Smith Bayesian Theory"
  },
  {
    "objectID": "03-conjugate.html#bayesian-books",
    "href": "03-conjugate.html#bayesian-books",
    "title": "Bayes AI",
    "section": "Bayesian Books",
    "text": "Bayesian Books\n\nHierarchical Models and MCMC\nBayesian Nonparametrics\n\nMachine Learning\n\nDynamic State Space Models \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#popular-books",
    "href": "03-conjugate.html#popular-books",
    "title": "Bayes AI",
    "section": "Popular Books",
    "text": "Popular Books\nMcGrayne (2012): The Theory that would not Die\n\nHistory of Bayes-Laplace\nCode breaking\nBayes search: Air France \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#nate-silver-538-and-nyt",
    "href": "03-conjugate.html#nate-silver-538-and-nyt",
    "title": "Bayes AI",
    "section": "Nate Silver: 538 and NYT",
    "text": "Nate Silver: 538 and NYT\nSilver (2012): The Signal and The Noise\n\nPresidential Elections\nBayes dominant methodology\nPredicting College Basketball/Oscars \\(\\ldots\\)"
  },
  {
    "objectID": "03-conjugate.html#things-to-know",
    "href": "03-conjugate.html#things-to-know",
    "title": "Bayes AI",
    "section": "Things to Know",
    "text": "Things to Know\nExplosion of Models and Algorithms starting in 1950s\n\nBayesian Regularisation and Sparsity\nHierarchical Models and Shrinkage\nHidden Markov Models\nNonlinear Non-Gaussian State Space Models\n\nAlgorithms\n\nMonte Carlo Method (von Neumann and Ulam, 1940s)\nMetropolis-Hastings (Metropolis, 1950s)\nGibbs Sampling (Geman and Geman, Gelfand and Smith, 1980s)\nSequential Particle Filtering"
  },
  {
    "objectID": "03-conjugate.html#probabilistic-reasoning",
    "href": "03-conjugate.html#probabilistic-reasoning",
    "title": "Bayes AI",
    "section": "Probabilistic Reasoning",
    "text": "Probabilistic Reasoning\n\nBayesian Probability (Ramsey, 1926, de Finetti, 1931)\n\nBeta-Binomial Learning: Black Swans\nElections: Nate Silver\nBaseball: Kenny Lofton and Derek Jeter\n\nMonte Carlo (von Neumann and Ulam, Metropolis, 1940s)\nShrinkage Estimation (Lindley and Smith, Efron and Morris, 1970s)"
  },
  {
    "objectID": "03-conjugate.html#bayesian-inference",
    "href": "03-conjugate.html#bayesian-inference",
    "title": "Bayes AI",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nKey Idea: Explicit use of probability for summarizing uncertainty.\n\nA probability distribution for data given parameters \\[\nf(y| \\theta ) \\; \\; \\text{Likelihood}\n\\]\nA probability distribution for unknown parameters \\[\np(\\theta) \\; \\; \\text{Prior}\n\\]\nInference for unknowns conditional on observed data\n\nInverse probability (Bayes Theorem);\nFormal decision making (Loss, Utility)"
  },
  {
    "objectID": "03-conjugate.html#posterior-inference",
    "href": "03-conjugate.html#posterior-inference",
    "title": "Bayes AI",
    "section": "Posterior Inference",
    "text": "Posterior Inference\nBayes theorem to derive posterior distributions \\[\n\\begin{aligned}\np( \\theta | y ) & = \\frac{p(y| \\theta)p( \\theta)}{p(y)} \\\\\np(y) & = \\int p(y| \\theta)p( \\theta)d \\theta\n\\end{aligned}\n\\] Allows you to make probability statements\n\nThey can be very different from p-values!\n\nHypothesis testing and Sequential problems\n\nMarkov chain Monte Carlo (MCMC) and Filtering (PF)"
  },
  {
    "objectID": "03-conjugate.html#conjugate-priors",
    "href": "03-conjugate.html#conjugate-priors",
    "title": "Bayes AI",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nDefinition: Let \\({\\cal F}\\) denote the class of distributions \\(f ( y | \\theta )\\).\n\nA class \\(\\Pi\\) of prior distributions is conjugate for \\({\\cal F}\\) if the posterior distribution is in the class \\(\\Pi\\) for all \\(f \\in {\\cal F} , \\pi \\in \\Pi , y \\in {\\cal Y}\\).\n\nExample: Binomial/Beta:\n\nSuppose that \\(Y_1 , \\ldots , Y_n \\sim Ber ( p )\\).\nLet \\(p \\sim Beta ( \\alpha , \\beta )\\) where \\(( \\alpha , \\beta )\\) are known hyper-parameters.\nThe beta-family is very flexible\nPrior mean \\(E ( p ) = \\frac{\\alpha}{ \\alpha + \\beta }\\)."
  },
  {
    "objectID": "03-conjugate.html#bayes-learning-beta-binomial",
    "href": "03-conjugate.html#bayes-learning-beta-binomial",
    "title": "Bayes AI",
    "section": "Bayes Learning: Beta-Binomial",
    "text": "Bayes Learning: Beta-Binomial\nHow do I update my beliefs about a coin toss?\nLikelihood for Bernoulli \\[\np\\left(  y|\\theta\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t}|\\theta\\right)\n=\\theta^{\\sum_{t=1}^{T}y_{t}}\\left(  1-\\theta\\right)  ^{T-\\sum_{t=1}^{T}y_{t}}.\n\\] Initial prior distribution \\(\\theta\\sim\\mathcal{B}\\left(  a,A\\right)\\) given by \\[\np\\left(  \\theta|a,A\\right)  =\\frac{\\theta^{a-1}\\left(  1-\\theta\\right)\n^{A-1}}{B\\left(  a,A\\right)  }\n\\]"
  },
  {
    "objectID": "03-conjugate.html#bayes-learning-beta-binomial-1",
    "href": "03-conjugate.html#bayes-learning-beta-binomial-1",
    "title": "Bayes AI",
    "section": "Bayes Learning: Beta-Binomial",
    "text": "Bayes Learning: Beta-Binomial\nUpdated posterior distribution is also Beta \\[\np\\left(\n\\theta|y\\right)  \\sim\\mathcal{B}\\left(  a_{T},A_{T}\\right)  \\; {\\rm and} \\;\na_{T}=a+\\sum_{t=1}^{T}y_{t} , A_{T}=A+T-\\sum_{t=1}^{T}y_{t}\n\\] The posterior mean and variance are \\[\nE\\left[  \\theta|y\\right]  =\\frac{a_{T}}{a_{T}+A_{T}}\\text{ and }var\\left[\n\\theta|y\\right]  =\\frac{a_{T}A_{T}}{\\left(  a_{T}+A_{T}\\right)  ^{2}\\left(\na_{T}+A_{T}+1\\right)  }\n\\]"
  },
  {
    "objectID": "03-conjugate.html#binomial-beta",
    "href": "03-conjugate.html#binomial-beta",
    "title": "Bayes AI",
    "section": "Binomial-Beta",
    "text": "Binomial-Beta\n\\(p ( p | \\bar{y} )\\) is the posterior distribution for \\(p\\)\n\\(\\bar{y}\\) is a sufficient statistic.\n\nBayes theorem gives \\[\n\\begin{aligned}\np ( p | y )\n& \\propto f ( y | p ) p ( p | \\alpha , \\beta )\\\\\n& \\propto p^{\\sum y_i} (1 - p )^{n - \\sum y_i } \\cdot p^{\\alpha - 1} ( 1 - p )^{\\beta - 1} \\\\\n& \\propto p^{ \\alpha + \\sum y_i - 1 } ( 1 - p )^{ n - \\sum y_i + \\beta - 1} \\\\\n& \\sim Beta ( \\alpha + \\sum y_i , \\beta + n - \\sum y_i )\n\\end{aligned}\n\\]\nThe posterior mean is a shrinkage estimator\n\nCombination of sample mean \\(\\bar{y}\\) and prior mean \\(E( p )\\)\n\\[\nE(p|y) = \\frac{\\alpha + \\sum_{i=1}^n y_i}{\\alpha + \\beta + n} = \\frac{n}{n+ \\alpha +\\beta} \\bar{y} + \\frac{\\alpha + \\beta}{\\alpha + \\beta+n} \\frac{\\alpha}{\\alpha+\\beta}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#black-swans",
    "href": "03-conjugate.html#black-swans",
    "title": "Bayes AI",
    "section": "Black Swans",
    "text": "Black Swans\nTaleb, The Black Swan: the Impact of the Highly Improbable\nSuppose you’re only see a sequence of White Swans, having never seen a Black Swan.\nWhat’s the Probability of Black Swan event sometime in the future?\nSuppose that after \\(T\\) trials you have only seen successes \\(( y_1 , \\ldots , y_T ) = ( 1 , \\ldots , 1 )\\). The next trial being a success has \\[\np( y_{T+1} =1 | y_1 , \\ldots , y_T ) = \\frac{T+1}{T+2}\n\\] For large \\(T\\) is almost certain. Here \\(a=A=1\\)."
  },
  {
    "objectID": "03-conjugate.html#black-swans-1",
    "href": "03-conjugate.html#black-swans-1",
    "title": "Bayes AI",
    "section": "Black Swans",
    "text": "Black Swans\nPrinciple of Induction (Hume)\nThe probability of never seeing a Black Swan is given by \\[\np( y_{T+1} =1 , \\ldots ,  y_{T+n} = 1 | y_1 , \\ldots , y_T ) = \\frac{ T+1 }{ T+n+1 } \\rightarrow 0\n\\]\nBlack Swan will eventually happen – don’t be surprised when it actually happens."
  },
  {
    "objectID": "03-conjugate.html#bayesian-learning-poisson-gamma",
    "href": "03-conjugate.html#bayesian-learning-poisson-gamma",
    "title": "Bayes AI",
    "section": "Bayesian Learning: Poisson-Gamma",
    "text": "Bayesian Learning: Poisson-Gamma\nPoisson/Gamma: Suppose that \\(Y_1 , \\ldots , Y_n \\mid \\lambda \\sim Poi ( \\lambda )\\).\nLet \\(\\lambda \\sim Gamma  ( \\alpha , \\beta )\\)\n\\(( \\alpha , \\beta )\\) are known hyper-parameters.\n\nThe posterior distribution is\n\n\\[\n\\begin{aligned}\np ( \\lambda | y ) & \\propto\n\\exp ( - n \\lambda ) \\lambda^{ \\sum y_i } \\lambda^{ \\alpha - 1 } \\exp ( - \\beta \\lambda ) \\\\\n& \\sim Gamma ( \\alpha + \\sum y_i , n + \\beta )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#example-clinical-trials",
    "href": "03-conjugate.html#example-clinical-trials",
    "title": "Bayes AI",
    "section": "Example: Clinical Trials",
    "text": "Example: Clinical Trials\nNovick and Grizzle: Bayesian Analysis of Clinical Trials\nFour treatments for duodenal ulcers.\nDoctors assess the state of the patient.\nSequential data\n(\\(\\alpha\\)-spending function, can only look at prespecified times).\n\n\n\nTreat\nExcellent\nFair\nDeath\n\n\n\n\nA\n76 1\n7\n7\n\n\nB\n89 1\n0\n1\n\n\nC\n86 1\n3\n1\n\n\nD\n88 9\n\n3\n\n\n\nConclusion: Cannot reject at the 5% level\nConjugate binomial/beta model+sensitivity analysis."
  },
  {
    "objectID": "03-conjugate.html#binomial-beta-1",
    "href": "03-conjugate.html#binomial-beta-1",
    "title": "Bayes AI",
    "section": "Binomial-Beta",
    "text": "Binomial-Beta\nLet \\(p_i\\) be the death rate proportion under treatment \\(i\\).\n\nTo compare treatment \\(A\\) to \\(B\\) directly compute \\(P ( p_1 &gt; p_2 | D )\\).\nPrior \\(beta ( \\alpha , \\beta )\\) with prior mean \\(E ( p ) = \\frac{\\alpha}{\\alpha + \\beta }\\).\n\nPosterior \\(beta ( \\alpha + \\sum x_i , \\beta + n - \\sum x_i )\\)\n\nFor \\(A\\), \\(beta ( 1 , 1 ) \\rightarrow beta ( 8 , 94 )\\)\n\nFor \\(B\\), \\(beta ( 1 , 1 ) \\rightarrow beta ( 2 , 100 )\\)\n\nInference: \\(P ( p_1 &gt; p_2 | D ) \\approx 0.98\\)"
  },
  {
    "objectID": "03-conjugate.html#sensitivity-analysis",
    "href": "03-conjugate.html#sensitivity-analysis",
    "title": "Bayes AI",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nImportant to do a sensitivity analysis.\n\n\n\nTreat\nExcellent\nFair\nDeath\n\n\n\n\nA\n76 1\n7\n7\n\n\nB\n89 1\n0\n1\n\n\nC\n86 1\n3\n1\n\n\nD\n88 9\n\n3\n\n\n\nPoisson-Gamma, prior \\(\\Gamma ( m , z)\\) and \\(\\lambda_i\\) be the expected death rate.\nCompute \\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; c | D \\right )\\)\n\n\n\nProb\n( 0 , 0 )\n( 100, 2)\n( 200 , 5)\n\n\n\n\n\\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; 1.3 | D \\right )\\)\n0.95\n0.88\n0.79\n\n\n\\(P \\left ( \\frac{ \\lambda_1 }{ \\lambda_2 } &gt; 1.6  | D \\right )\\)\n0.91\n0.80\n0.64"
  },
  {
    "objectID": "03-conjugate.html#bayesian-learning-normal-normal",
    "href": "03-conjugate.html#bayesian-learning-normal-normal",
    "title": "Bayes AI",
    "section": "Bayesian Learning: Normal-Normal",
    "text": "Bayesian Learning: Normal-Normal\nUsing Bayes rule we get \\[\np( \\mu | y ) \\propto p( y| \\mu ) p( \\mu )\n\\]\n\nPosterior is given by\n\n\\[\np( \\mu | y ) \\propto \\exp \\left ( - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n ( y_i - \\mu )^2 -\n\\frac{1}{2 \\tau^2} ( \\mu - \\mu_0 )^2 \\right )\n\\] Hence \\(\\mu | y \\sim N \\left ( \\hat{\\mu}_B , V_{\\mu} \\right )\\) where\n\\[\n\\hat{\\mu}_B = \\frac{ n / \\sigma^2 }{ n / \\sigma^2 + 1 / \\tau^2 } \\bar{y}\n+  \\frac{ 1 / \\tau^2 }{ n / \\sigma^2 + 1 / \\tau^2 }\\mu_0 \\; \\;\n{\\rm and} \\; \\;  V_{\\mu}^{-1} = \\frac{n}{ \\sigma^2 } + \\frac{1}{\\tau^2}\n\\] A shrinkage estimator."
  },
  {
    "objectID": "03-conjugate.html#sat-scores",
    "href": "03-conjugate.html#sat-scores",
    "title": "Bayes AI",
    "section": "SAT Scores",
    "text": "SAT Scores\nSAT (\\(200-800\\)): 8 high schools and estimate effects.\n\n\n\nSchool\nEstimated \\(y_j\\)\nSt. Error \\(\\sigma_j\\)\nAverage Treatment \\(\\theta_i\\)\n\n\n\n\nA\n28\n15\n?\n\n\nB\n8\n10\n?\n\n\nC\n-3\n16\n?\n\n\nD\n7\n11\n?\n\n\nE\n-1\n9\n?\n\n\nF\n1\n11\n?\n\n\nG\n18\n10\n?\n\n\nH\n12\n18\n?\n\n\n\n\n\\(\\theta_j\\) average effects of coaching programs\n\\(y_j\\) estimated treatment effects, for school \\(j\\), standard error \\(\\sigma_j\\)."
  },
  {
    "objectID": "03-conjugate.html#estimates",
    "href": "03-conjugate.html#estimates",
    "title": "Bayes AI",
    "section": "Estimates",
    "text": "Estimates\nTwo programs appear to work (improvements of \\(18\\) and \\(28\\))\n\nLarge standard errors. Overlapping Confidence Intervals?\nClassical hypothesis test fails to reject the hypothesis that the \\(\\theta_j\\)’s are equal.\nPooled estimate has standard error of \\(4.2\\) with\n\n\\[\n\\hat{\\theta} = \\frac{  \\sum_j ( y_j / \\sigma_j^2 ) }{ \\sum_j ( 1 / \\sigma_j^2 ) } = 7.9\n\\]\n\nNeither separate or pooled seems sensible.\n\nBayesian shrinkage!"
  },
  {
    "objectID": "03-conjugate.html#hierarchical-model",
    "href": "03-conjugate.html#hierarchical-model",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nHierarchical Model (\\(\\sigma_j^2\\) known) is given by \\[\n\\bar{y}_j | \\theta_j \\sim N ( \\theta_j , \\sigma_j^2 )\n\\] Unequal variances–differential shrinkage.\n\nPrior Distribution: \\(\\theta_j \\sim N ( \\mu , \\tau^2 )\\) for \\(1 \\leq j \\leq 8\\).\n\nTraditional random effects model.\nExchangeable prior for the treatment effects.\nAs \\(\\tau \\rightarrow 0\\) (complete pooling) and as \\(\\tau \\rightarrow \\infty\\) (separate estimates).\n\nHyper-prior Distribution: \\(p( \\mu , \\tau^2 ) \\propto 1 / \\tau\\).\n\nThe posterior \\(p( \\mu , \\tau^2 | y )\\) can be used to “estimate” \\(( \\mu , \\tau^2 )\\)."
  },
  {
    "objectID": "03-conjugate.html#posterior",
    "href": "03-conjugate.html#posterior",
    "title": "Bayes AI",
    "section": "Posterior",
    "text": "Posterior\nJoint Posterior Distribution \\(y = ( y_1 , \\ldots , y_J )\\) \\[\np( \\theta , \\mu , \\tau | y )\n\\propto  p( y| \\theta )   p( \\theta | \\mu , \\tau )p( \\mu , \\tau )\n\\] \\[\n\\propto p( \\mu , \\tau^2)\n\\prod_{i=1}^8 N ( \\theta_j | \\mu , \\tau^2 ) \\prod_{j=1}^8\nN ( y_j | \\theta_j )\n\\] \\[\n\\propto \\tau^{-9} \\exp \\left ( - \\frac{1}{2} \\sum_j \\frac{1}{\\tau^2} ( \\theta_j - \\mu )^2 -\n\\frac{1}{2} \\sum_j \\frac{1}{\\sigma_j^2} ( y_j - \\theta_j )^2 \\right )\n\\] MCMC!"
  },
  {
    "objectID": "03-conjugate.html#posterior-inference-1",
    "href": "03-conjugate.html#posterior-inference-1",
    "title": "Bayes AI",
    "section": "Posterior Inference",
    "text": "Posterior Inference\nReport posterior quantiles\n\n\n\nSchool\n2.5%\n25%\n50%\n75%\n97.5%\n\n\n\n\nA -\n2 6\n10\n16\n3\n2\n\n\nB -\n5 4\n8\n12\n2\n0\n\n\nC -1\n2 3\n7\n11\n2\n2\n\n\nD -\n6 4\n8\n12\n2\n1\n\n\nE -1\n0 2\n6\n10\n1\n9\n\n\nF -\n9 2\n6\n10\n1\n9\n\n\nG -\n1 6\n10\n15\n2\n7\n\n\nH -\n7 4\n8\n13\n2\n3\n\n\n\\(\\mu\\)\n-2\n5\n8\n11\n18\n\n\n\\(\\tau\\)\n0.3\n2.3\n5.1\n8.8\n21\n\n\n\nSchools \\(A\\) and \\(G\\) are similar!"
  },
  {
    "objectID": "03-conjugate.html#bayesian-shrinkage",
    "href": "03-conjugate.html#bayesian-shrinkage",
    "title": "Bayes AI",
    "section": "Bayesian Shrinkage",
    "text": "Bayesian Shrinkage\nBayesian shrinkage provides a way of modeling complex datasets.\n\nBaseball batting averages: Stein’s Paradox\nBatter-pitcher match-up: Kenny Lofton and Derek Jeter\nBayes Elections\nToxoplasmosis\nBayes MoneyBall\nBayes Portfolio Selection"
  },
  {
    "objectID": "03-conjugate.html#example-baseball",
    "href": "03-conjugate.html#example-baseball",
    "title": "Bayes AI",
    "section": "Example: Baseball",
    "text": "Example: Baseball\nBatter-pitcher match-up?\nPrior information on overall ability of a player.\nSmall sample size, pitcher variation.\n\nLet \\(p_i\\) denote Jeter’s ability. Observed number of hits \\(y_i\\)\n\n\\[\n(y_i | p_i ) \\sim Bin ( T_i , p_i ) \\; \\; {\\rm with} \\; \\; p_i \\sim Be ( \\alpha , \\beta )\n\\] where \\(T_i\\) is the number of at-bats against pitcher \\(i\\). A priori \\(E( p_i ) = \\alpha / (\\alpha+\\beta ) = \\bar{p}_i\\).\n\nThe extra heterogeneity leads to a prior variance \\(Var (p_i ) = \\bar{p}_i (1 - \\bar{p}_i ) \\phi\\) where \\(\\phi = ( \\alpha + \\beta + 1 )^{-1}\\)."
  },
  {
    "objectID": "03-conjugate.html#sports-data-baseball",
    "href": "03-conjugate.html#sports-data-baseball",
    "title": "Bayes AI",
    "section": "Sports Data: Baseball",
    "text": "Sports Data: Baseball\nKenny Lofton hitting versus individual pitchers.\n\n\n\nPitcher At\n-bats Hi\nts Ob\nsAvg\n\n\n\n\n\n\nJ.C. Romero\n9\n6\n.667\n\n\n\n\nS. Lewis\n5 3\n.\n600\n\n\n\n\nB. Tomko\n20 1\n1 .\n550\n\n\n\n\nT. Hoffman\n6\n3\n.500\n\n\n\n\nK. Tapani\n45\n22\n.489\n\n\n\n\nA. Cook\n9 4\n.\n444\n\n\n\n\nJ. Abbott\n34\n14\n.412\n\n\n\n\nA.J. Burnett\n15\n6\n.400\n\n\n\n\nK. Rogers\n43\n17\n.395\n\n\n\n\nA. Harang\n6\n2\n.333\n\n\n\n\nK. Appier\n49\n15\n.306\n\n\n\n\nR. Clemens\n62\n14\n.226\n\n\n\n\nC. Zambrano\n9\n2\n.222\n\n\n\n\nN. Ryan\n10 2\n.\n200\n\n\n\n\nE. Hanson\n41\n7\n.171\n\n\n\n\nE. Milton\n19\n1\n.056\n\n\n\n\nM. Prior\n7 0\n.\n000\n\n\n\n\nTotal 76\n30 228\n3 .2\n99"
  },
  {
    "objectID": "03-conjugate.html#baseball",
    "href": "03-conjugate.html#baseball",
    "title": "Bayes AI",
    "section": "Baseball",
    "text": "Baseball\nKenny Lofton\nKenny Lofton (career \\(.299\\) average, and current \\(.308\\) average for \\(2006\\) season) was facing the pitcher Milton (current record \\(1\\) for \\(19\\))\n.\n\nIs putting in a weaker player really a better bet?\nOver-reaction to bad luck?\n\n\\(\\mathbb{P}\\left ( \\leq 1 \\; {\\rm hit \\; in \\; } 19 \\; {\\rm attempts} | p = 0.3 \\right ) = 0.01\\)\nAn unlikely \\(1\\)-in-\\(100\\) event."
  },
  {
    "objectID": "03-conjugate.html#baseball-1",
    "href": "03-conjugate.html#baseball-1",
    "title": "Bayes AI",
    "section": "Baseball",
    "text": "Baseball\nKenny Lofton\nBayes solution: shrinkage. Borrow strength across pitchers\nBayes estimate: use the posterior mean\nLofton’s batting estimates that vary from \\(.265\\) to \\(.340\\).\nThe lowest being against Milton.\n\\(.265 &lt; .275\\)\nConclusion: resting Lofton against Milton was justified!!"
  },
  {
    "objectID": "03-conjugate.html#bayes-batter-pitcher-match-up",
    "href": "03-conjugate.html#bayes-batter-pitcher-match-up",
    "title": "Bayes AI",
    "section": "Bayes Batter-pitcher match-up",
    "text": "Bayes Batter-pitcher match-up\nHere’s our model again ...\n\nSmall sample sizes and pitcher variation.\nLet \\(p_i\\) denote Lofton’s ability. Observed number of hits \\(y_i\\)\n\n\\[\n(y_i | p_i ) \\sim Bin ( T_i , p_i ) \\; \\; {\\rm with} \\; \\; p_i \\sim Be ( \\alpha , \\beta )\n\\] where \\(T_i\\) is the number of at-bats against pitcher \\(i\\).\nEstimate \\(( \\alpha , \\beta )\\)"
  },
  {
    "objectID": "03-conjugate.html#example-derek-jeter",
    "href": "03-conjugate.html#example-derek-jeter",
    "title": "Bayes AI",
    "section": "Example: Derek Jeter",
    "text": "Example: Derek Jeter\nDerek Jeter 2006 season versus individual pitchers.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nR. Mendoza\n6\n5\n.833\n.322\n(.282, .394)\n\n\nH. Nomo\n20\n12\n.600\n.326\n(.289, .407)\n\n\nA.J.Burnett\n5\n3\n.600\n.320\n(.275, .381)\n\n\nE. Milton\n28\n14\n.500\n.324\n(.291, .397)\n\n\nD. Cone\n8\n4\n.500\n.320\n(.218, .381)\n\n\nR. Lopez\n45\n21\n.467\n.326\n(.291, .401)\n\n\nK. Escobar\n39\n16\n.410\n.322\n(.281, .386)\n\n\nJ. Wettland\n5\n2\n.400\n.318\n(.275, .375)\n\n\nT. Wakefield\n81\n26\n.321\n.318\n(.279, .364)\n\n\nP. Martinez\n83\n21\n.253\n.312\n(.254, .347)\n\n\nK. Benson\n8\n2\n.250\n.317\n(.264, .368)\n\n\nT. Hudson\n24\n6\n.250\n.315\n(.260, .362)\n\n\nJ. Smoltz\n5\n1\n.200\n.314\n(.253, .355)\n\n\nF. Garcia\n25\n5\n.200\n.314\n(.253, .355)\n\n\nB. Radke\n41\n8\n.195\n.311\n(.247, .347)\n\n\nD. Kolb\n5\n0\n.000\n.316\n(.258, .363)\n\n\nJ. Julio\n13\n0\n.000\n.312\n(.243, .350 )\n\n\nTotal\n6530\n2061\n.316\n\n\n\n\n\nTotal 6530 2061 .316"
  },
  {
    "objectID": "03-conjugate.html#bayes-estimates",
    "href": "03-conjugate.html#bayes-estimates",
    "title": "Bayes AI",
    "section": "Bayes Estimates",
    "text": "Bayes Estimates\nStern stimates \\(\\hat{\\phi} = ( \\alpha + \\beta + 1 )^{-1} = 0.002\\) for Jeter\nDoesn’t vary much across the population of pitchers.\nThe extremes are shrunk the most also matchups with the smallest sample sizes.\nJeter had a season \\(.308\\) average.\nBayes estimates vary from \\(.311\\) to \\(.327\\)–he’s very consistent.\nIf all players had a similar record then a constant batting average would make sense."
  },
  {
    "objectID": "03-conjugate.html#bayes-elections-nate-silver",
    "href": "03-conjugate.html#bayes-elections-nate-silver",
    "title": "Bayes AI",
    "section": "Bayes Elections: Nate Silver",
    "text": "Bayes Elections: Nate Silver\nMultinomial-Dirichlet\nPredicting the Electoral Vote (EV)\n\nMultinomial-Dirichlet: \\((\\hat{p} | p) \\sim Multi (p), ( p | \\alpha ) \\sim Dir (\\alpha)\\)\n\n\\[\np_{Obama} = ( p_{1}, \\ldots ,p_{51} | \\hat{p}) \\sim Dir \\left ( \\alpha + \\hat{p} \\right )\n\\]\n\nFlat uninformative prior \\(\\alpha\\equiv 1\\).\n\nhttp://www.electoral-vote.com/evp2012/Pres/prespolls.csv"
  },
  {
    "objectID": "03-conjugate.html#bayes-elections-nate-silver-1",
    "href": "03-conjugate.html#bayes-elections-nate-silver-1",
    "title": "Bayes AI",
    "section": "Bayes Elections: Nate Silver",
    "text": "Bayes Elections: Nate Silver\nSimulation\nCalculate probabilities via simulation: rdirichlet \\[\np \\left ( p_{j,O} | {\\rm data} \\right )  \\;\\; {\\rm and} \\;  \\;\np \\left ( EV &gt;270 | {\\rm data} \\right )\n\\]\nThe election vote prediction is given by the sum \\[\nEV =\\sum_{j=1}^{51} EV(j) \\mathbb{E} \\left ( p_{j} | {\\rm data} \\right )\n\\] where \\(EV(j)\\) are for individual states"
  },
  {
    "objectID": "03-conjugate.html#polling-data-electoral-vote.com",
    "href": "03-conjugate.html#polling-data-electoral-vote.com",
    "title": "Bayes AI",
    "section": "Polling Data: electoral-vote.com",
    "text": "Polling Data: electoral-vote.com\nElectoral Vote (EV), Polling Data: Mitt and Obama percentages\n\n\n\nState\nM.pct\nO.pct\nEV\n\n\n\n\nAlabama\n58\n36\n9\n\n\nAlaska\n55\n37\n3\n\n\nArizona\n50\n46\n10\n\n\nArkansas\n51\n44\n6\n\n\nCalifornia\n33\n55\n55\n\n\nColorado\n45\n52\n9\n\n\nConnecticut\n31\n56\n7\n\n\nDelaware\n38\n56\n3\n\n\nD.C.\n13\n82\n3\n\n\nFlorida\n46\n50\n27\n\n\nGeorgia\n52\n47\n15\n\n\nHawaii\n32\n63\n4\n\n\nIdaho\n68\n26\n4\n\n\nIllinois\n35\n59\n21\n\n\nIndiana\n48\n48\n11\n\n\nIowa\n37\n54\n7\n\n\nKansas\n63\n31\n6\n\n\nKentucky\n51\n42\n8\n\n\nLouisiana\n50\n43\n9\n\n\nMaine\n35\n56\n4\n\n\nMaryland\n39\n54\n10\n\n\nMassachusetts\n34\n53\n12\n\n\nMichigan\n37\n53\n17\n\n\nMinnesota\n42\n53\n10\n\n\nMississippi\n46\n33\n6"
  },
  {
    "objectID": "03-conjugate.html#polling-data",
    "href": "03-conjugate.html#polling-data",
    "title": "Bayes AI",
    "section": "Polling Data:",
    "text": "Polling Data:\n\n\n\n\n\n\n\n\n\nElection 2008 Prediction. Obama \\(370\\)\n\n\n\n\n\n\n\nElection 2012 Prediction. Obama \\(332\\)."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears-2014-2015-season",
    "href": "03-conjugate.html#chicago-bears-2014-2015-season",
    "title": "Bayes AI",
    "section": "Chicago Bears 2014-2015 Season",
    "text": "Chicago Bears 2014-2015 Season\nBayes Learning: Update our beliefs in light of new information\n\nIn the 2014-2015 season.\n\nThe Bears suffered back-to-back \\(50\\)-points defeats.\nPartiots-Bears \\(51-23\\)\nPackers-Bears \\(55-14\\)\n\nTheir next game was at home against the Minnesota Vikings.\n\nCurrent line against the Vikings was \\(-3.5\\) points.\nSlightly over a field goal\nWhat’s the Bayes approach to learning the line?"
  },
  {
    "objectID": "03-conjugate.html#hierarchical-model-1",
    "href": "03-conjugate.html#hierarchical-model-1",
    "title": "Bayes AI",
    "section": "Hierarchical Model",
    "text": "Hierarchical Model\nHierarchical model for the current average win/lose this year \\[\n\\begin{aligned}\n\\bar{y} | \\theta & \\sim N \\left ( \\theta , \\frac{\\sigma^2}{n} \\right ) \\sim N \\left ( \\theta , \\frac{18.34^2}{9} \\right )\\\\\n\\theta & \\sim N( 0 , \\tau^2 )\n\\end{aligned}\n\\] Here \\(n =9\\) games so far. With \\(s = 18.34\\) points\nPre-season prior mean \\(\\mu_0 = 0\\), standard deviation \\(\\tau = 4\\).\nRecord so-far. Data \\(\\bar{y} = -9.22\\)."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears",
    "href": "03-conjugate.html#chicago-bears",
    "title": "Bayes AI",
    "section": "Chicago Bears",
    "text": "Chicago Bears\nBayes Shrinkage estimator \\[\n\\mathbb{E} \\left ( \\theta | \\bar{y} , \\tau \\right ) = \\frac{ \\tau^2 }{ \\tau^2 + \\frac{\\sigma^2}{n} } \\bar{y}\n\\]\nThe Shrinkage factor is \\(0.3\\)!!\nThat’s quite a bit of shrinkage. Why?\n\nOur updated estimator is\n\n\\[\n\\mathbb{E} \\left ( \\theta | \\bar{y} , \\tau \\right ) = - 2.75 &gt; -.3.5\n\\] where current line is \\(-3.5\\).\n\nBased on our hierarchical model this is an over-reaction.\n\nOne point change on the line is about \\(3\\)% on a probability scale.\nAlternatively, calculate a market-based \\(\\tau\\) given line \\(=-3.5\\)."
  },
  {
    "objectID": "03-conjugate.html#chicago-bears-1",
    "href": "03-conjugate.html#chicago-bears-1",
    "title": "Bayes AI",
    "section": "Chicago Bears",
    "text": "Chicago Bears\nLast two defeats were \\(50\\) points scored by opponent (2014-15)\n\n\nCode\nbears=c(-3,8,8,-21,-7,14,-13,-28,-41)\nmean(bears)\n\n\n[1] -9.222222\n\n\nCode\nsd(bears)\n\n\n[1] 18.34242\n\n\nCode\ntau=4\nsig2=sd(bears)*sd(bears)/9\ntau^2/(sig2+tau^2)\n\n\n[1] 0.2997225\n\n\nCode\npnorm(-2.76/18)\n\n\n[1] 0.4390677\n\n\nHome advantage is worth \\(3\\) points. Vikings an average record.\nResult: Bears 21, Vikings 13"
  },
  {
    "objectID": "03-conjugate.html#steins-paradox",
    "href": "03-conjugate.html#steins-paradox",
    "title": "Bayes AI",
    "section": "Stein’s Paradox",
    "text": "Stein’s Paradox\nStein paradox: possible to make a uniform improvement on the MLE in terms of MSE.\n\nMistrust of the statistical interpretation of Stein’s result.\n\nIn particular, the loss function.\n\nDifficulties in adapting the procedure to special cases\nLong familiarity with good properties for the MLE\n\nAny gains from a “complicated” procedure could not be worth the extra trouble (Tukey, savings not more than 10 % in practice)\nFor \\(k\\ge 3\\), we have the remarkable inequality \\[\nMSE(\\hat \\theta_{JS},\\theta) &lt; MSE(\\bar y,\\theta) \\; \\forall \\theta\n\\] Bias-variance explanation! Inadmissability of the classical stats."
  },
  {
    "objectID": "03-conjugate.html#baseball-batting-averages",
    "href": "03-conjugate.html#baseball-batting-averages",
    "title": "Bayes AI",
    "section": "Baseball Batting Averages",
    "text": "Baseball Batting Averages\nData: 18 major-league players after 45 at bats (1970 season)\n\n\n\nPlayer\n\\(\\bar{y}_i\\)\n\\(E ( p_i | D )\\)\naverage season\n\n\n\n\nClemente\n0.400\n0.290\n0.346\n\n\nRobinson\n0.378\n0.286\n0.298\n\n\nHoward\n0.356\n0.281\n0.276\n\n\nJohnstone\n0.333\n0.277\n0.222\n\n\nBerry\n0.311\n0.273\n0.273\n\n\nSpencer\n0.311\n0.273\n0.270\n\n\nKessinger\n0.311\n0.268\n0.263\n\n\nAlvarado\n0.267\n0.264\n0.210\n\n\nSanto\n0.244\n0.259\n0.269\n\n\nSwoboda\n0.244\n0.259\n0.230\n\n\nUnser\n0.222\n0.254\n0.264\n\n\nWilliams\n0.222\n0.254\n0.256\n\n\nScott\n0.222\n0.254\n0.303\n\n\nPetrocelli\n0.222\n0.254\n0.264\n\n\nRodriguez\n0.222\n0.254\n0.226\n\n\nCampanens\n0.200\n0.259\n0.285\n\n\nMunson\n0.178\n0.244\n0.316\n\n\nAlvis\n0.156\n0.239\n0.200"
  },
  {
    "objectID": "03-conjugate.html#baseball-data",
    "href": "03-conjugate.html#baseball-data",
    "title": "Bayes AI",
    "section": "Baseball Data",
    "text": "Baseball Data\nFirst Shrinkage Estimator: Efron and Morris\n\nBaseball Shrinkage"
  },
  {
    "objectID": "03-conjugate.html#shrinkage",
    "href": "03-conjugate.html#shrinkage",
    "title": "Bayes AI",
    "section": "Shrinkage",
    "text": "Shrinkage\nLet \\(\\theta_i\\) denote the end of season average\n\nLindley: shrink to the overall grand mean\n\n\\[\nc = 1 - \\frac{ ( k - 3 ) \\sigma^2 }{ \\sum ( \\bar{y}_i - \\bar{y} )^2 }\n\\] where \\(\\bar{y}\\) is the overall grand mean and\n\\[\n\\hat{\\theta} = c \\bar{y}_i + ( 1 - c ) \\bar{y}\n\\]\n\nBaseball data: \\(c = 0.212\\) and \\(\\bar{y} = 0.265\\).\n\nCompute \\(\\sum ( \\hat{\\theta}_i - \\bar{y}^{obs}_i )^2\\) and see which is lower: \\[\nMLE = 0.077 \\; \\; STEIN = 0.022\n\\] That’s a factor of \\(3.5\\) times better!"
  },
  {
    "objectID": "03-conjugate.html#batting-averages",
    "href": "03-conjugate.html#batting-averages",
    "title": "Bayes AI",
    "section": "Batting Averages",
    "text": "Batting Averages"
  },
  {
    "objectID": "03-conjugate.html#baseball-paradoxes",
    "href": "03-conjugate.html#baseball-paradoxes",
    "title": "Bayes AI",
    "section": "Baseball Paradoxes",
    "text": "Baseball Paradoxes\nShrinkage on Clemente too severe: \\(z_{Cl} = 0.265 + 0.212 ( 0.400 - 0.265) = 0.294\\).\nThe \\(0.212\\) seems a little severe\n\nLimited translation rules, maximum shrinkage eg. 80%\nNot enough shrinkage eg O’Connor ( \\(y = 1 , n = 2\\)). \\(z_{O'C} = 0.265 + 0.212 ( 0.5  - 0.265 ) = 0.421\\).\n\nStill better than Ted Williams \\(0.406\\) in 1941.\n\nForeign car sales (\\(k = 19\\)) will further improve MSE performance! It will change the shrinkage factors.\nClearly an improvement over the Stein estimator is\n\n\\[\n\\hat{\\theta}_{S+} =\n\\max \\left ( \\left ( 1 - \\frac{k-2}{ \\sum \\bar{Y}_i^2 } \\right ) , 0 \\right ) \\bar{Y}_i\n\\]"
  },
  {
    "objectID": "03-conjugate.html#baseball-prior",
    "href": "03-conjugate.html#baseball-prior",
    "title": "Bayes AI",
    "section": "Baseball Prior",
    "text": "Baseball Prior\nInclude extra prior knowledge\nEmpirical distribution of all major league players \\[\n\\theta_i \\sim N ( 0.270 , 0.015 )\n\\] The \\(0.270\\) provides another origin to shrink to and the prior variance \\(0.015\\) would give a different shrinkage factor.\nTo fully understand maybe we should build a probabilistic model and see what the posterior mean is as our estimator for the unknown parameters."
  },
  {
    "objectID": "03-conjugate.html#shrinkage-unequal-variances",
    "href": "03-conjugate.html#shrinkage-unequal-variances",
    "title": "Bayes AI",
    "section": "Shrinkage: Unequal Variances",
    "text": "Shrinkage: Unequal Variances\nModel \\(Y_i | \\theta_i  \\sim N ( \\theta_i , D_i )\\) where \\(\\theta_i \\sim N ( \\theta_0 , A ) \\sim N ( 0.270 , 0.015 )\\).\n\nThe \\(D_i\\) can be different – unequal variances\nBayes posterior means are given by\n\n\\[\nE ( \\theta_i | Y ) = ( 1 - B_i ) Y_i \\; \\; {\\rm where} \\; \\;\nB_i = \\frac{ D_i }{ D_i + A }\n\\] where \\(\\hat{A}\\) is estimated from the data, see Efron and Morris (1975).\n\nDifferent shrinkage factors as different variances \\(D_i\\).\n\n\\(D_i \\propto n_i^{-1}\\) and so smaller sample sizes are shrunk more.\nMakes sense."
  },
  {
    "objectID": "03-conjugate.html#example-toxoplasmosis-data",
    "href": "03-conjugate.html#example-toxoplasmosis-data",
    "title": "Bayes AI",
    "section": "Example: Toxoplasmosis Data",
    "text": "Example: Toxoplasmosis Data\nDisease of Blood that is endemic in tropical regions.\nData: \\(5000\\) people in El Salvador (varying sample sizes) from \\(36\\) cities.\n\nEstimate “true” prevalences \\(\\theta_i\\) for \\(1 \\leq i \\leq 36\\)\nAllocation of Resources: should we spend funds on the city with the highest observed occurrence of the disease? Same shrinkage factors?\nShrinkage Procedure (Efron and Morris, p315) \\[\nz_i = c_i y_i\n\\] where \\(y_i\\) are the observed relative rates (normalized so \\(\\bar{y} = 0\\) The smaller sample sizes will get shrunk more.\n\nThe most gentle are in the range \\(0.6 \\rightarrow 0.9\\) but some are \\(0.1 \\rightarrow 0.3\\)."
  },
  {
    "objectID": "03-conjugate.html#bayes-portfolio-selection",
    "href": "03-conjugate.html#bayes-portfolio-selection",
    "title": "Bayes AI",
    "section": "Bayes Portfolio Selection",
    "text": "Bayes Portfolio Selection\nde Finetti and Markowitz: Mean-variance portfolio shrinkage: \\(\\frac{1}{\\gamma} \\Sigma^{-1} \\mu\\)\nDifferent shrinkage factors for different history lengths.\nPortfolio Allocation in the SP500 index\nEntry/exit; splits; spin-offs etc. For example, 73 replacements to the SP500 index in period 1/1/94 to 12/31/96.\nAdvantage: \\(E ( \\alpha | D_t ) = 0.39\\), that is 39 bps per month which on an annual basis is \\(\\alpha = 468\\) bps.\nThe posterior mean for \\(\\beta\\) is \\(p ( \\beta | D_t ) = 0.745\\)\n\\(\\bar{x}_{M} = 12.25 \\%\\) and \\(\\bar{x}_{PT} = 14.05 \\%\\)."
  },
  {
    "objectID": "03-conjugate.html#sp-composition",
    "href": "03-conjugate.html#sp-composition",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nGeneral Electric\nGE\n2.800\n2.485\n1.640\n1.569\n\n\nCoca Cola\nKO\n2.342\n1.126\n0.606\n1.051\n\n\nExxon\nXON\n2.142\n2.672\n3.439\n2.957\n\n\nATT\nT\n2.030\n2.090\n5.197\n5.948\n\n\nPhilip Morris\nMO\n1.678\n1.649\n0.637\n*****\n\n\nRoyal Dutch\nRD\n1.636\n1.774\n1.191\n*****\n\n\nMerck\nMRK\n1.615\n1.308\n0.773\n0.906\n\n\nMicrosoft\nMSFT\n1.436\n*****\n*****\n*****\n\n\nJohnson/Johnson\nJNJ\n1.320\n0.845\n0.689\n*****\n\n\nIntel\nINTC\n1.262\n*****\n*****\n*****\n\n\nProcter and Gamble\nPG\n1.228\n1.040\n0.871\n0.993\n\n\nWalmart\nWMT\n1.208\n1.084\n*****\n*****\n\n\nIBM\nIBM\n1.181\n2.327\n5.341\n9.231\n\n\nHewlett Packard\nHWP\n1.105\n0.477\n0.497\n*****\n\n\nPepsi\nPEP\n1.061\n0.719\n*****\n*****"
  },
  {
    "objectID": "03-conjugate.html#sp-composition-1",
    "href": "03-conjugate.html#sp-composition-1",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nPfizer\nPFE\n0.918\n0.491\n0.408\n0.486\n\n\nDupont\nDD\n0.910\n1.229\n0.837\n1.101\n\n\nAIG\nAIG\n0.910\n0.723\n*****\n*****\n\n\nMobil\nMOB\n0.906\n1.093\n1.659\n1.040\n\n\nBristol Myers Squibb\nBMY\n0.878\n1.247\n*****\n0.484\n\n\nGTE\nGTE\n0.849\n0.975\n0.593\n0.705\n\n\nGeneral Motors\nGM\n0.848\n1.086\n2.079\n4.399\n\n\nDisney\nDIS\n0.839\n0.644\n*****\n*****\n\n\nCiticorp\nCCI\n0.831\n0.400\n0.418\n*****\n\n\nBellSouth\nBLS\n0.822\n1.190\n*****\n*****\n\n\nMotorola\nMOT\n0.804\n*****\n*****\n*****\n\n\nFord\nF\n0.798\n0.883\n0.485\n0.640\n\n\nChervon\nCHV\n0.794\n0.990\n1.370\n0.966\n\n\nAmoco\nAN\n0.733\n1.198\n1.673\n0.758\n\n\nEli Lilly\nLLY\n0.720\n0.814\n*****\n*****"
  },
  {
    "objectID": "03-conjugate.html#sp-composition-2",
    "href": "03-conjugate.html#sp-composition-2",
    "title": "Bayes AI",
    "section": "SP Composition",
    "text": "SP Composition\n\n\n\nDate\nSymbol\n6/96\n12/89\n12/79\n12/69\n\n\n\n\nAbbott Labs\nABT\n0.690\n0.654\n*****\n*****\n\n\nAmerHome Products\nAHP\n0.686\n0.716\n0.606\n0.793\n\n\nFedNatlMortgage\nFNM\n0.686\n*****\n*****\n*****\n\n\nMcDonald’s\nMCD\n0.686\n0.545\n*****\n*****\n\n\nAmeritech\nAIT\n0.639\n0.782\n*****\n*****\n\n\nCisco Systems\nCSCO\n0.633\n*****\n*****\n*****\n\n\nCMB\nCMB\n0.621\n*****\n*****\n*****\n\n\nSBC\nSBC\n0.612\n0.819\n*****\n*****\n\n\nBoeing\nBA\n0.598\n0.584\n0.462\n*****\n\n\nMMM\nMMM\n0.581\n0.762\n0.838\n1.331\n\n\nBankAmerica\nBAC\n0.560\n*****\n0.577\n*****\n\n\nBell Atlantic\nBEL\n0.556\n0.946\n*****\n*****\n\n\nGillette\nG\n0.535\n*****\n*****\n*****\n\n\nKodak\nEK\n0.524\n0.570\n1.106\n*****\n\n\nChrysler\nC\n0.507\n*****\n*****\n0.367\n\n\nHome Depot\nHD\n0.497\n*****\n*****\n*****\n\n\nColgate\nCOL\n0.489\n0.499\n*****\n*****\n\n\nWells Fargo\nWFC\n0.478\n*****\n*****\n*****\n\n\nNations Bank\nNB\n0.453\n*****\n*****\n*****\n\n\nAmer Express\nAXP\n0.450\n0.621\n*****\n*****"
  },
  {
    "objectID": "03-conjugate.html#keynes-versus-buffett-capm",
    "href": "03-conjugate.html#keynes-versus-buffett-capm",
    "title": "Bayes AI",
    "section": "Keynes versus Buffett: CAPM",
    "text": "Keynes versus Buffett: CAPM\nkeynes = 15.08 + 1.83 market\nbuffett = 18.06 + 0.486 market\n\n\n\nYear\nKeynes\nMarket\n\n\n\n\n1928\n-3.4\n7.9\n\n\n1929\n0.8\n6.6\n\n\n1930\n-32.4\n-20.3\n\n\n1931\n-24.6\n-25.0\n\n\n1932\n44.8\n-5.8\n\n\n1933\n35.1\n21.5\n\n\n1934\n33.1\n-0.7\n\n\n1935\n44.3\n5.3\n\n\n1936\n56.0\n10.2\n\n\n1937\n8.5\n-0.5\n\n\n1938\n-40.1\n-16.1\n\n\n1939\n12.9\n-7.2\n\n\n1940\n-15.6\n-12.9\n\n\n1941\n33.5\n12.5\n\n\n1942\n-0.9\n0.8\n\n\n1943\n53.9\n15.6\n\n\n1944\n14.5\n5.4\n\n\n1945\n14.6\n0.8\n\n\n\nKing’s College Cambridge\n\nKeynes vs Cash"
  },
  {
    "objectID": "03-conjugate.html#superbowl-xlvii-ravens-vs-49ers",
    "href": "03-conjugate.html#superbowl-xlvii-ravens-vs-49ers",
    "title": "Bayes AI",
    "section": "SuperBowl XLVII: Ravens vs 49ers",
    "text": "SuperBowl XLVII: Ravens vs 49ers\nTradeSports.com\n\nSuperBowl XLVII"
  },
  {
    "objectID": "03-conjugate.html#super-bowl-xlvii-ravens-vs-49ers",
    "href": "03-conjugate.html#super-bowl-xlvii-ravens-vs-49ers",
    "title": "Bayes AI",
    "section": "Super Bowl XLVII: Ravens vs 49ers",
    "text": "Super Bowl XLVII: Ravens vs 49ers\n\nSuper Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013.\nWe will track \\(X(t)\\) which corresponds to the Raven’s lead over the 49ers at each point in time. Table 3 provides the score at the end of each quarter.\n\n\\[\n\\begin{array}{c|ccccc}\nt & 0 & \\frac{1}{4} & \\frac{1}{2} & \\frac{3}{4} & 1  \\\\\\hline\nRavens & 0 & 7  & 21 & 28 & 34 \\\\\n49ers & 0 & 3 & 6 & 23 & 31\\\\\\hline\nX(t) & 0 & 4 & 15 & 5 & 3\\\\\n\\end{array}\n\\] SuperBowl XLVII by Quarter"
  },
  {
    "objectID": "03-conjugate.html#initial-market",
    "href": "03-conjugate.html#initial-market",
    "title": "Bayes AI",
    "section": "Initial Market",
    "text": "Initial Market\nInitial point spread Ravens being a four point underdog, \\(\\mu=-4\\). \\[\n\\mu = \\mathbb{E} \\left (X(1) \\right )=-4 .\n\\] The Ravens upset the 49ers by \\(34-31\\) and \\(X(1)= 34-31=3\\) with the point spread being beaten by \\(7\\) points.\nTo determine the markets’ assessment of the probability that the Ravens would win at the beginning of the game we use the money-line odds."
  },
  {
    "objectID": "03-conjugate.html#initial-market-1",
    "href": "03-conjugate.html#initial-market-1",
    "title": "Bayes AI",
    "section": "Initial Market",
    "text": "Initial Market\n\nSan Francisco \\(-175\\)\nBaltimore Ravens \\(+155\\).\n\nThis implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155.\nConvert both of these money-lines to implied probabilities of the each team winning\n\\[\np_{SF} = \\frac{175}{100+175} = 0.686 \\; \\; {\\rm and} \\; \\; p_{Bal} = \\frac{100}{100+155} = 0.392\n\\]"
  },
  {
    "objectID": "03-conjugate.html#probabilities-of-winning",
    "href": "03-conjugate.html#probabilities-of-winning",
    "title": "Bayes AI",
    "section": "Probabilities of Winning",
    "text": "Probabilities of Winning\nThe probabilities do not sum to one. This \"overound\" probability is talso known as the bookmaker’s edge. \\[\np_{SF} + p_{Bal} = 0.686+0.392 = 1.078\n\\] providing a \\(7.8\\)% edge for the bookmakers.\nthe \"market vig\" is the implied probability of the bookie making money on the bet.\n\n\nCode\no = 1.078-1\nv = o/(1+o)\nv\n\n\n[1] 0.07235622\n\n\nWe use the mid-point of the spread to determine \\(p\\) implying that\n\\[\np = \\frac{1}{2} p_{Bal} + \\frac{1}{2} (1 - p_{SF} ) = 0.353\n\\] From the Ravens perspective, we have \\(p = \\mathbb{P}(X(1)&gt;0) =0.353\\).\nBaltimore’s win probability started trading at \\(p^{mkt}_0 =0.38\\)"
  },
  {
    "objectID": "03-conjugate.html#half-time-analysis",
    "href": "03-conjugate.html#half-time-analysis",
    "title": "Bayes AI",
    "section": "Half Time Analysis",
    "text": "Half Time Analysis\nThe Ravens took a commanding \\(21-6\\) lead at half time. Market was trading at \\(p_{\\frac{1}{2}}^{mkt}= 0.90\\).\n\nDuring the \\(34\\) minute blackout \\(42760\\) contracts changed hands with Baltimore’s win probability ticking down from \\(95\\) to \\(94\\).\nThe win probability peak of \\(95\\)% occurred after a third-quarter kickoff return for a touchdown.\nAt the end of the four quarter, however, when the 49ers nearly went into the lead with a touchdown, Baltimore’s win probability had dropped to \\(30\\)%."
  },
  {
    "objectID": "03-conjugate.html#implied-volatility",
    "href": "03-conjugate.html#implied-volatility",
    "title": "Bayes AI",
    "section": "Implied Volatility",
    "text": "Implied Volatility\nTo calculate the implied volatility of the Superbowl we substitute the pair \\((\\mu,p) = (-4, .353)\\) into our definition and solve for \\(\\sigma_{IV}\\). \\[\n\\sigma = \\frac{\\mu}{\\Phi^{-1}(p)} \\, ,\n\\]\n\nWe obtain\n\n\\[\n\\sigma_{IV} = \\frac{\\mu}{\\Phi^{-1}(p)} = \\frac{-4}{-0.377}  = 10.60\n\\] where \\(\\Phi^{-1} ( p) = qnorm(0.353) = -0.377\\). The \\(4\\) point advantage assessed for the 49ers is under a \\(\\frac{1}{2} \\sigma\\) favorite.\n\nThe outcome \\(X(1)=3\\) was within one standard deviation of the pregame model which had an expectation of \\(\\mu=-4\\) and volatility of \\(\\sigma= 10.6\\)."
  },
  {
    "objectID": "03-conjugate.html#half-time-probabilities",
    "href": "03-conjugate.html#half-time-probabilities",
    "title": "Bayes AI",
    "section": "Half Time Probabilities",
    "text": "Half Time Probabilities\nWhat’s the probability of the Ravens winning given their lead at half time?\nAt half time Baltimore led by 15 points, \\(21\\) to \\(6\\).\nThe conditional mean for the final outcome is \\(15 +  0.5*(-4) = 13\\) and the conditional volatility is \\(10.6 \\sqrt{1-t}.\\)\nThese imply a probability of \\(.96\\) for Baltimore to win the game.\n\nA second estimate of the probability of winning given the half time lead can be obtained directly from the betting market.\n\nFrom the online betting market we also have traded contracts on TradeSports.com that yield a half time probability of \\(p_{\\frac{1}{2}} = 0.90\\)."
  },
  {
    "objectID": "03-conjugate.html#whats-the-implied-volatility-for-the-second-half",
    "href": "03-conjugate.html#whats-the-implied-volatility-for-the-second-half",
    "title": "Bayes AI",
    "section": "What’s the implied volatility for the second half?",
    "text": "What’s the implied volatility for the second half?\n\\(p_t^{mkt}\\) reflects all available information\n\nFor example, at half-time \\(t = \\frac{1}{2}\\) we would update\n\n\\[\n\\sigma_{IV,t=\\frac{1}{2}} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}} = \\frac{15-2}{ \\Phi^{-1}(0.9) / \\sqrt{2} } = 14\n\\] where \\(qnorm(0.9)=1.28\\).\n\nAs \\(14&gt; 10.6\\), the market was expecting a more volatile second half–possibly anticipating a comeback from the 49ers."
  },
  {
    "objectID": "03-conjugate.html#how-can-we-form-a-valid-betting-strategy",
    "href": "03-conjugate.html#how-can-we-form-a-valid-betting-strategy",
    "title": "Bayes AI",
    "section": "How can we form a valid betting strategy?",
    "text": "How can we form a valid betting strategy?\nGiven the initial implied volatility \\(\\sigma=10.6\\).\nAt half time with the Ravens having a \\(l +\\mu(1-t)=13\\) points edge\n\nWe would assess with \\(\\sigma = 10.6\\)\n\n\\[\np_{\\frac{1}{2}} = \\Phi \\left ( 13/ (10.6/\\sqrt{2}) \\right ) = 0.96\n\\] probability of winning versus the \\(p_{\\frac{1}{2}}^{mkt} = 0.90\\) rate.\n\nTo determine our optimal bet size, \\(\\omega_{bet}\\), on the Ravens we might appeal to the Kelly criterion (Kelly, 1956) which yields\n\n\\[\n\\omega_{bet} = p_{\\frac{1}{2}} - \\frac{q_{\\frac{1}{2}}}{O^{mkt}} = 0.96 - \\frac{0.1}{1/9} = 0.60\n\\]"
  },
  {
    "objectID": "03-conjugate.html#multivariate-normal",
    "href": "03-conjugate.html#multivariate-normal",
    "title": "Bayes AI",
    "section": "Multivariate Normal",
    "text": "Multivariate Normal\nIn the multivariate case, the normal-normal model is \\[\n\\theta \\sim N(\\mu_0,\\Sigma_0), \\quad y \\mid \\theta \\sim N(\\theta,\\Sigma).\n\\] The posterior distribution is \\[\n\\theta \\mid y \\sim N(\\mu_1,\\Sigma_1),\n\\] where \\[\n\\Sigma_1 = (\\Sigma_0^{-1} + \\Sigma^{-1})^{-1}, \\quad \\mu_1 = \\Sigma_1(\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y).\n\\] The predictive distribution is \\[\ny_{new} \\mid y \\sim N(\\mu_1,\\Sigma_1 + \\Sigma).\n\\]"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance",
    "href": "03-conjugate.html#normal-with-unknown-variance",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nConsider, another example, when mean \\(\\mu\\) is fixed and variance is a random variable which follows some distribution \\(\\sigma^2 \\sim p(\\sigma^2)\\). Given an observed sample \\(y\\), we can update the distribution over variance using the Bayes rule \\[\np(\\sigma^2 \\mid  y) = \\dfrac{p(y\\mid \\sigma^2 )p(\\sigma^2)}{p(y)}.\n\\] Now, the total probability in the denominator can be calculated as \\[\np(y) = \\int p(y\\mid \\sigma^2 )p(\\sigma^2) d\\sigma^2.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance-1",
    "href": "03-conjugate.html#normal-with-unknown-variance-1",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nA conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. Thus, if \\[\n\\sigma^2 \\mid  \\alpha,\\beta \\sim IG(\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\sigma^{2(-\\alpha-1)}\\exp\\left(-\\dfrac{\\beta}{\\sigma^2}\\right)\n\\] and \\[\ny \\mid \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\n\\] Then the posterior distribution is another inverse Gamma \\(IG(\\alpha_{\\mathrm{posterior}},\\beta_{\\mathrm{posterior}})\\), with \\[\n\\alpha_{\\mathrm{posterior}} = \\alpha + 1/2, ~~\\beta_{\\mathrm{posterior}} = \\beta + \\dfrac{y-\\mu}{2}.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#normal-with-unknown-variance-2",
    "href": "03-conjugate.html#normal-with-unknown-variance-2",
    "title": "Bayes AI",
    "section": "Normal With Unknown Variance",
    "text": "Normal With Unknown Variance\nNow, the predictive distribution over \\(y\\) can be calculated by \\[\np(y_{new}\\mid y) = \\int p(y_{new},\\sigma^2\\mid y)p(\\sigma^2\\mid y)d\\sigma^2.\n\\] Which happens to be a \\(t\\)-distribution with \\(2\\alpha_{\\mathrm{posterior}}\\) degrees of freedom, mean \\(\\mu\\) and variance \\(\\alpha_{\\mathrm{posterior}}/\\beta_{\\mathrm{posterior}}\\)."
  },
  {
    "objectID": "03-conjugate.html#black-litterman",
    "href": "03-conjugate.html#black-litterman",
    "title": "Bayes AI",
    "section": "Black-Litterman",
    "text": "Black-Litterman\n\nBlack and Litterman (1991, 1992) work for combining investor views with market equilibrium.\nIn a multivariate returns setting the optimal allocation rule is \\[\n\\omega^\\star = \\frac{1}{\\gamma} \\Sigma^{-1} \\mu\n\\] The question is how to specify \\((\\mu, \\Sigma)\\) pairs?\nFor example, given \\(\\hat{\\Sigma}\\), BL derive Bayesian inference for \\(\\mu\\) given market equilibrium model and a priori views on the returns of pre-specified portfolios which take the form \\[\n( \\hat{\\mu} | \\mu ) \\sim \\mathcal{N} \\left ( \\mu , \\tau \\hat{\\Sigma} \\right ) \\; {\\rm and} \\;\n( Q | \\mu ) \\sim \\mathcal{N} \\left ( P \\mu , \\hat{\\Omega} \\right ) \\; .\n\\]"
  },
  {
    "objectID": "03-conjugate.html#posterior-views",
    "href": "03-conjugate.html#posterior-views",
    "title": "Bayes AI",
    "section": "Posterior Views",
    "text": "Posterior Views\n\nCombining views, the implied posterior is \\[\n( \\mu | \\hat{\\mu} , Q ) \\sim  \\mathcal{N} \\left ( B b , B \\right )\n\\]\nThe mean and variance are specified by\n\n\\[\nB = ( \\tau \\hat{\\Sigma} )^{-1} + P^\\prime \\hat{\\Omega}^{-1} P \\; {\\rm and} \\;    b = ( \\tau \\hat{\\Sigma} )^{-1} \\hat{\\mu}\n+ P^\\prime \\Omega^{-1} Q\n\\] These posterior moments then define the optimal allocation rule."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nIn 2014, Satya Nadella became the CEO of Microsoft.\nThe stock price of Microsoft has been on a steady rise since then.\nSuppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.\nSuppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&P500 index and tracks overall market performance.\nWhat is the mean returns of the positions in our portfolio?"
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-1",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-1",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nAssume the prior for the mean returns is a bivariate normal distribution, let \\(\\mu_0 = (\\mu_{M}, \\mu_{S})\\) represent the prior mean returns for the stocks.\nThe covariance matrix \\(\\Sigma_0\\) captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. \\[\n\\Sigma_0 = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\]\n\nWe will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-2",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-2",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nThe likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. \\[\n\\Sigma = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the observed returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by \\[\n\\mu = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the observed returns of MSFT and SPY, respectively."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-3",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-3",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\nYou update your beliefs (prior) about the mean returns using the observed data (likelihood).\nThe posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution.\nThe mean \\(\\mu_{\\text{post}}\\) and covariance \\(\\Sigma_{\\text{post}}\\) of the posterior are calculated using Bayesian updating formulas, which involve \\(\\mu_0\\), \\(\\Sigma_0\\), \\(\\mu\\), and \\(\\Sigma\\).\nWe use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014."
  },
  {
    "objectID": "03-conjugate.html#satya-nadella-ceo-of-microsoft-4",
    "href": "03-conjugate.html#satya-nadella-ceo-of-microsoft-4",
    "title": "Bayes AI",
    "section": "Satya Nadella: CEO of Microsoft",
    "text": "Satya Nadella: CEO of Microsoft\n\n\nCode\nlibrary(quantmod)\ngetSymbols(c(\"MSFT\", \"SPY\"), from = \"2001-01-01\", to = \"2023-12-31\")\n\n\n[1] \"MSFT\" \"SPY\" \n\n\nCode\ns = 3666 # 2015-07-30\nprior = 1:s\nobs = s:nrow(MSFT) # post covid\n# obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23\na = as.numeric(dailyReturn(MSFT))\nc = as.numeric(dailyReturn(SPY))\n# Prior\nmu0 = c(mean(a[prior]), mean(c[prior]))\nSigma0 = cov(data.frame(a=a[prior],c=c[prior]))\n# Data\nmu = c(mean(a[obs]), mean(c[obs]))\nSigma = cov(data.frame(a=a,c=c))\n# Posterior\nSigmaPost = solve(solve(Sigma0) + solve(Sigma))\nmuPost = SigmaPost %*% (solve(Sigma0) %*% mu0 + solve(Sigma) %*% mu)\n# Plot\nplot(a[obs], c[obs], xlab=\"MSFT\", ylab=\"SPY\", xlim=c(-0.005,0.005), ylim=c(-0.005,0.005), pch=16, cex=0.5)\nabline(v=0, h=0, col=\"grey\")\nabline(v=mu0[1], h=mu0[2], col=\"blue\",lwd=3) #prior\nabline(v=mu[1], h=mu[2], col=\"red\",lwd=3) #data\nabline(v=muPost[1], h=muPost[2], col=\"green\",lwd=3) #posterior\nlegend(\"bottomright\", c(\"Prior\", \"Likelihood\", \"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")"
  },
  {
    "objectID": "03-conjugate.html#mixtures-of-conjugate-priors",
    "href": "03-conjugate.html#mixtures-of-conjugate-priors",
    "title": "Bayes AI",
    "section": "Mixtures of Conjugate Priors",
    "text": "Mixtures of Conjugate Priors\n\nThe mixture of conjugate priors is a powerful tool for modeling complex data. \\[\n\\theta \\sim p(\\theta) = \\sum_{k=1}^K \\pi_k p_k(\\theta).\n\\] Then the posterior is also a mixture of normal distributions, that is \\[\np(\\theta\\mid y) = p(y\\mid \\theta)\\sum_{k=1}^K \\pi_k p_k(\\theta)/C.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixtures-of-conjugate-priors-1",
    "href": "03-conjugate.html#mixtures-of-conjugate-priors-1",
    "title": "Bayes AI",
    "section": "Mixtures of Conjugate Priors",
    "text": "Mixtures of Conjugate Priors\nWe introduce a normalizing constant for each component \\[\nC_k = \\int p(y\\mid \\theta)p_k(\\theta)d\\theta.\n\\] then \\[\np_k(\\theta\\mid y) =  p_k(\\theta)p(y\\mid \\theta)/C_k\n\\] is a proper distribution and our posterior is a mixture of these distributions \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\pi_k C_k p_k(\\theta\\mid y)/C.\n\\] Meaning that we need to require \\[\n\\dfrac{\\sum_{k=1}^K \\pi_k C_k}{C} = 1.\n\\] or \\[\nC = \\sum_{k=1}^K \\pi_k C_k\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nThe prior distribution is a mixture of two normal distributions, that is \\[\n\\mu \\sim 0.5 N(0,1) + 0.5 N(5,1).\n\\] The likelihood is a normal distribution with mean \\(\\mu\\) and variance 1, that is \\[\ny \\mid \\mu \\sim N(\\mu,1).\n\\] The posterior distribution is a mixture of two normal distributions, that is \\[\np(\\mu \\mid y) \\propto \\phi(y\\mid \\mu,1) \\left(0.5 \\phi(\\mu\\mid 0,1) + 0.5 \\phi(\\mu\\mid 5,1)\\right),\n\\] where \\(\\phi(x\\mid \\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions-1",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions-1",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nWe can calculate it using property of a normal distribution \\[\n\\phi(x\\mid \\mu_1,\\sigma_1^2)\\phi(x\\mid \\mu_2,\\sigma_2^2) = \\phi(x\\mid \\mu_3,\\sigma_3^2)\\phi(\\mu_1-\\mu_2\\mid 0,\\sigma_1^2+\\sigma_2^2)\n\\] where \\[\n\\mu_3 = \\dfrac{\\mu_1/\\sigma_2^2 + \\mu_2/\\sigma_1^2}{1/\\sigma_1^2 + 1/\\sigma_2^2}, \\quad \\sigma_3^2 = \\dfrac{1}{1/\\sigma_1^2 + 1/\\sigma_2^2}.\n\\]"
  },
  {
    "objectID": "03-conjugate.html#mixture-of-two-normal-distributions-2",
    "href": "03-conjugate.html#mixture-of-two-normal-distributions-2",
    "title": "Bayes AI",
    "section": "Mixture of two normal distributions",
    "text": "Mixture of two normal distributions\nGiven, we observed \\(y = 2\\), we can calculate the posterior distribution for \\(\\mu\\)\n\n\nCode\nmu0 = c(0,5)\nsigma02 = c(1,1)\npi = c(0.5,0.5)\ny = 2\nmu3 = (mu0/sigma02 + y) / (1/sigma02 + 1)\nsigma3 = 1/(1/sigma02 + 1)\nC = dnorm(y-mu0,0,1+sigma02)*pi\nw = C/sum(C)\nsprintf(\"Component parameters:\\nMean = (%1.1f,%2.1f)\\nVar = (%1.1f,%1.1f)\\nweights = (%1.2f,%1.2f)\", mu3[1],mu3[2], sigma3[1],sigma3[2],w[1],w[2])\n\n\n[1] \"Component parameters:\\nMean = (1.0,3.5)\\nVar = (0.5,0.5)\\nweights = (0.65,0.35)\""
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model",
    "href": "03-conjugate.html#exponential-gamma-model",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\n\nWaiting times between events: consecutive arrivals of a Poisson process is exponentially distributed with mean \\(1/\\lambda\\). \\[\nf(x;\\lambda) =  \\lambda e^{-\\lambda x}, ~ x \\geq 0\n\\]\n\\(\\lambda\\) is the rate parameter, which is the inverse of the mean\nspecial case of the Gamma distribution with shape 1 and scale \\(1/\\lambda\\).\n\n\n\n\nExponential Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = 1/\\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = 1/\\lambda^2\\)"
  },
  {
    "objectID": "03-conjugate.html#exponential-model-examples",
    "href": "03-conjugate.html#exponential-model-examples",
    "title": "Bayes AI",
    "section": "Exponential Model: Examples",
    "text": "Exponential Model: Examples\n\nLifespan of Electronic Components: The exponential distribution can model the time until a component fails in systems where the failure rate is constant over time.\nTime Between Arrivals: In a process where events (like customers arriving at a store or calls arriving at a call center) occur continuously and independently, the time between these events can often be modeled with an exponential distribution.\nRadioactive Decay: The time until a radioactive atom decays is often modeled with an exponential distribution."
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-1",
    "href": "03-conjugate.html#exponential-gamma-model-1",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\nThe Exponential-Gamma model assumes that the data follows an exponential distribution (likelihood). - The Gamma distribution is a flexible two-parameter family of distributions and can model a wide range of shapes. \\[\\begin{align*}\n    \\lambda &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n    x_i &\\sim \\text{Exponential}(\\lambda)\n\\end{align*}\\]\nThe posterior distribution of the rate parameter \\(\\lambda\\) is given by: \\[\np(\\lambda\\mid x_1, \\ldots, x_n) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\prod_{i=1}^n \\lambda e^{-\\lambda x_i} = \\lambda^{\\alpha + n - 1} e^{-(\\beta + \\sum_{i=1}^n x_i)\\lambda}\n\\]"
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-2",
    "href": "03-conjugate.html#exponential-gamma-model-2",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\nPosterior is a Gamma distribution with shape parameter \\(\\alpha + n\\) and rate parameter \\(\\beta + \\sum_{i=1}^n x_i\\). The posterior mean and variance are given by: \\[\n\\mathbb{E}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{\\beta + \\sum_{i=1}^n x_i}, \\quad \\mathrm{Var}[\\lambda|x_1, \\ldots, x_n] = \\frac{\\alpha + n}{(\\beta + \\sum_{i=1}^n x_i)^2}.\n\\] Notice, that \\(\\sum x_i\\) is the sufficient statistic for inference about parameter \\(\\lambda\\)!"
  },
  {
    "objectID": "03-conjugate.html#exponential-gamma-model-3",
    "href": "03-conjugate.html#exponential-gamma-model-3",
    "title": "Bayes AI",
    "section": "Exponential-Gamma Model",
    "text": "Exponential-Gamma Model\n\nReliability Engineering: In situations where the failure rate of components or systems may not be constant and can vary, the Exponential-Gamma model can be used to estimate the time until failure, incorporating uncertainty in the failure rate.\nMedical Research: For modeling survival times of patients where the rate of mortality or disease progression is not constant and varies across a population. The variability in rates can be due to different factors like age, genetics, or environmental influences.\nEcology: In studying phenomena like the time between rare environmental events (e.g., extreme weather events), where the frequency of occurrence can vary due to changing climate conditions or other factors."
  },
  {
    "objectID": "03-conjugate.html#exploratory-data-analysis",
    "href": "03-conjugate.html#exploratory-data-analysis",
    "title": "Bayes AI",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore deciding on a parametric model for a dataset. There are several tools that we use to choose the appropriate model. These include\n\nTheoretical assumptions underlying the distribution (our prior knowledge about the data)\nExploratory data analysis\nFormal goodness-of-fit tests\n\nThe two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms."
  },
  {
    "objectID": "03-conjugate.html#q-q-plot",
    "href": "03-conjugate.html#q-q-plot",
    "title": "Bayes AI",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\nQ-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.).\nQuantile is the fraction (or percent) of points below the given value.\nThat is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\).\nOn a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight."
  },
  {
    "objectID": "03-conjugate.html#q-q-plot-1",
    "href": "03-conjugate.html#q-q-plot-1",
    "title": "Bayes AI",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\nIf the two data sets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\).\nIf \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\)."
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot",
    "href": "03-conjugate.html#noraml-q-q-plot",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nQ-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\n\nCode\nrequire(UsingR)\nrequire(dplyr) \ndata(babyboom) \nqqnorm(babyboom$wt)\nqqline(babyboom$wt)\n\n\n\n\nFigure 1\nAre Birth Weights Normally Distributed?"
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot-1",
    "href": "03-conjugate.html#noraml-q-q-plot-1",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nThe Q-Q plots look different if we split the data based on the gender\n\nCode\npar(mar=c(4,4,0,0),bty=\"n\")\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\")  %&gt;% pull(wt) \nqqnorm(g); qqline(g)\nqqnorm(b); qqline(b)\n\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender"
  },
  {
    "objectID": "03-conjugate.html#noraml-q-q-plot-2",
    "href": "03-conjugate.html#noraml-q-q-plot-2",
    "title": "Bayes AI",
    "section": "Noraml Q-Q plot",
    "text": "Noraml Q-Q plot\nHow about the times in hours between births of babies?\n\n\nCode\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))"
  },
  {
    "objectID": "03-conjugate.html#exponential-q-q-plot",
    "href": "03-conjugate.html#exponential-q-q-plot",
    "title": "Bayes AI",
    "section": "Exponential Q-Q plot",
    "text": "Exponential Q-Q plot\nWhat about the Q-Q plot?\n\n\nCode\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval)\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb) # Overlay a line\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector"
  },
  {
    "objectID": "03-conjugate.html#brief-list-of-conjugate-models",
    "href": "03-conjugate.html#brief-list-of-conjugate-models",
    "title": "Bayes AI",
    "section": "Brief List of Conjugate Models",
    "text": "Brief List of Conjugate Models\n\n\n\nLikelihood\nPrior\nPosterior\n\n\n\n\nBinomial\nBeta\nBeta\n\n\nNegative\nBinomial\nBeta\n\n\nPoisson\nGamma\nGamma\n\n\nGeometric\nBeta\nBeta\n\n\nExponential\nGamma\nGamma\n\n\nNormal (mean unknown)\nNormal\nNormal\n\n\nNormal (variance unknown)\nInverse Gamma\nInverse Gamma\n\n\nNormal (mean and variance unknown)\nNormal/Gamma\nNormal/Gamma\n\n\nMultinomial\nDirichlet\nDirichlet"
  },
  {
    "objectID": "07-regression.html#example-history-of-pandemics",
    "href": "07-regression.html#example-history-of-pandemics",
    "title": "Bayes AI",
    "section": "Example: History of Pandemics",
    "text": "Example: History of Pandemics\nBill Gates: 12/11/2009: “I’m most worried about a worldwide Pandemic”\n\n\n\n\n\n\n\n\n\nEarly-period Pandemics\nDates\nSize of Mortality\n\n\n\n\nPlague of Athens\n430 BC\n\\(25\\)% population.\n\n\nBlack Death\n1347\n\\(30\\)% Europe\n\n\nLondon Plague\n\\(1666\\)\n\\(20\\)% population\n\n\n\n\n\n\n\n\nRecent Flu Epidemics\nDates 1900-2010\nSize\n\n\n\n\nSpanish Flu\n\\(1918\\)-\\(19\\)\n\\(40\\)-\\(50\\) million\n\n\nAsian Flu\nH2N2, \\(1957\\)-\\(58\\)\n\\(2\\) million\n\n\nHong Kong Flu\nH3N2, \\(1968\\)-\\(69\\)\n\\(6\\) million\n\n\n\n\n\n\nSpanish Flu killed more than WW1\nH1N1 Flu 2009: \\(18,449\\) people killed World wide:"
  },
  {
    "objectID": "07-regression.html#seir-epidemic-models",
    "href": "07-regression.html#seir-epidemic-models",
    "title": "Bayes AI",
    "section": "SEIR Epidemic Models",
    "text": "SEIR Epidemic Models\nGrowth “self-reinforcing”: More likely if more infectants\n\nAn individual comes into contact with disease at rate \\(\\beta_1\\)\nThe susceptible individual contracts the disease with probability \\(\\beta_2\\)\nEach infectant becomes infectious with rate \\(\\alpha\\) per unit time\nEach infectant recovers with rate \\(\\gamma\\) per unit time\n\n\\(S_t + E_t + I_t + R_t = N\\)"
  },
  {
    "objectID": "07-regression.html#current-models-seir",
    "href": "07-regression.html#current-models-seir",
    "title": "Bayes AI",
    "section": "Current Models: SEIR",
    "text": "Current Models: SEIR\nsusceptible-exposed-infectious-recovered model\nDynamic models that extend earlier models to include exposure and recovery.\nThe coupled SEIR model:\n\\(\\dot{S} = -\\beta S I\\)\n\\(\\dot{E} = \\beta S I - \\alpha E\\)\n\\(\\dot{I} = \\alpha E -\\gamma I\\)\n\\(\\dot{R} = \\gamma I\\)"
  },
  {
    "objectID": "07-regression.html#infectious-disease-models",
    "href": "07-regression.html#infectious-disease-models",
    "title": "Bayes AI",
    "section": "Infectious disease models",
    "text": "Infectious disease models\nDaniel Bernoulli’s (1766) first model of disease transmission in smallpox:\n“I wish simply that, in matters which so closely concern the well being of the human race, no decision shall be made without all knowledge which a little analysis and calculation can provide”\n\nR.A. Ross, (Nobel Medicine winner, 1902) – math model of malaria transmission, which ultimately lead to malaria control.\n\nRoss-McDonald model\n\nKermack and McKendrick: susceptible-infectious-recovered (SIR)\n\nLondon Plague 1665-1666; Cholera: London 1865, Bombay, 1906."
  },
  {
    "objectID": "07-regression.html#example-london-plague-1666-village-eyam-nr.-sheffield",
    "href": "07-regression.html#example-london-plague-1666-village-eyam-nr.-sheffield",
    "title": "Bayes AI",
    "section": "Example: London Plague, 1666: Village Eyam nr. Sheffield",
    "text": "Example: London Plague, 1666: Village Eyam nr. Sheffield\nModel of transmission from Infectants, \\(I\\), to susceptibles, \\(S\\).\n\n\n\nDate \\(1666\\)\nSusceptibles\nInfectives\n\n\n\n\nInitial\n254\n7\n\n\nJuly \\(3\\)\n\\(235\\)\n\\(15\\)\n\n\nJuly \\(19\\)\n\\(201\\)\n\\(22\\)\n\n\nAug \\(3\\)\n\\(153\\)\n\\(29\\)\n\n\nAug \\(19\\)\n\\(121\\)\n\\(21\\)\n\n\nSept \\(3\\)\n\\(108\\)\n\\(8\\)\n\n\nSept \\(19\\)\n\\(97\\)\n\\(8\\)\n\n\nOct \\(3\\)\n–\n–\n\n\nOct \\(19\\)\n\\(83\\)\n0\n\n\n\nInitial Population \\(N=261=S_0\\); Final population \\(S_\\infty = 83\\)."
  },
  {
    "objectID": "07-regression.html#modeling-growth-si",
    "href": "07-regression.html#modeling-growth-si",
    "title": "Bayes AI",
    "section": "Modeling Growth: SI",
    "text": "Modeling Growth: SI\nCoupled Differential eqn \\(\\dot{S} = - \\beta SI , \\dot{I} = ( \\beta S - \\alpha ) I\\)\n\nEstimates \\(\\frac{\\beta}{\\alpha} = 6.54 \\times 10^{-3} , \\frac{\\alpha}{\\beta} = 1.53\\).\n\n\\[\n\\frac{ \\hat{\\beta} }{\\alpha} = \\frac{  \\ln ( S_0 / S_\\infty ) }{S_0 - S_\\infty}\n\\] Predicted maximum \\(30.4\\), very close to observed \\(29\\)\nKey: \\(S\\) and \\(I\\) are observed and \\(\\alpha , \\beta\\) are estimated in hindsight"
  },
  {
    "objectID": "07-regression.html#transmission-rates-r_0-for-1918-episode",
    "href": "07-regression.html#transmission-rates-r_0-for-1918-episode",
    "title": "Bayes AI",
    "section": "Transmission Rates \\(R_0\\) for 1918 Episode",
    "text": "Transmission Rates \\(R_0\\) for 1918 Episode\n\n1918-19 influenza pandemic:\n\n\n\n\n\nMills et al. 2004:\n45 US cities\n3 (2-4)\n\n\nViboud et al. 2006:\nEngland and Wales\n1.8\n\n\nMassad et al. 2007:\nSao Paulo Brazil\n2.7\n\n\nNishiura, 2007:\nPrussia, Germany\n3.41\n\n\nChowell et al., 2006:\nGeneva, Switzerland\n2.7-3.8\n\n\nChowell et al., 2007:\nSan Francisco\n2.7-3.5\n\n\n\nThe larger the \\(R_0\\) the more severe the epidemic.\nTransmission parameters vary substantially from epidemic to epidemic"
  },
  {
    "objectID": "07-regression.html#boat-localization-example",
    "href": "07-regression.html#boat-localization-example",
    "title": "Bayes AI",
    "section": "Boat Localization Example",
    "text": "Boat Localization Example\nLocalization with measurement update\n\nA boat sails from one island to another\nBoat is trying to identify its location \\(\\theta \\sim N(m_0, C_0)\\)\nUsing a sequence of measurements to one of the islands \\(x_1,\\ldots,x_n\\)\n\nMeasurements are noisy due to dilution of precision http://www.sailingmates.com/your-gps-can-kill-you/"
  },
  {
    "objectID": "07-regression.html#reckoning",
    "href": "07-regression.html#reckoning",
    "title": "Bayes AI",
    "section": "Reckoning",
    "text": "Reckoning\nLocalization with no measurement updates is called reckoning\n\nsource: http://www.hakaimagazine.com/article-short/traversing-seas"
  },
  {
    "objectID": "07-regression.html#kalman-filter",
    "href": "07-regression.html#kalman-filter",
    "title": "Bayes AI",
    "section": "Kalman Filter",
    "text": "Kalman Filter\n\\[\n\\theta \\sim N(m_0, C_0)\n\\] \\[\nx_t = \\theta + w_t,~~~w_t \\sim N(0,\\sigma^2)\n\\] \\[\nx_1,x_2,\\ldots \\mid \\theta \\sim N(\\theta,\\sigma^2)\n\\] The prior variance \\(C_0\\) might be quite large if you are very uncertain about your guess \\(m_0\\)\nGiven the measurements \\(x^n = (x_1,\\ldots,x_n)\\), you update your opinion about \\(\\theta\\) computing its posterior density, using the Bayes formula"
  },
  {
    "objectID": "07-regression.html#normal-model",
    "href": "07-regression.html#normal-model",
    "title": "Bayes AI",
    "section": "Normal Model",
    "text": "Normal Model\n\\[\nf(x) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp^{-\\dfrac{1}{2}\\dfrac{(x-\\mu)^2}{\\sigma^2}}\n\\] Or multivariate equivalent \\[\nf(x) = (2 \\pi)^{-k/2} |\\Sigma|^{-1/2}\\exp^{-0.5(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}\n\\]\n\n\nCode\ncurve(exp(-x*x),-2,2, bty=\"newdataframe &lt;- na.omit(dataframe)\")"
  },
  {
    "objectID": "07-regression.html#the-conjugate-prior-for-the-normal-distribution",
    "href": "07-regression.html#the-conjugate-prior-for-the-normal-distribution",
    "title": "Bayes AI",
    "section": "The Conjugate Prior for the Normal Distribution",
    "text": "The Conjugate Prior for the Normal Distribution\nWe will look at the Gaussian distribution from a Bayesian point of view. In the standard form, the likelihood has two parameters, the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) \\[\np(x^n | \\mu, \\sigma^2) \\propto \\dfrac{1}{\\sigma^n}\\exp\\left(-\\dfrac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#normal-prior",
    "href": "07-regression.html#normal-prior",
    "title": "Bayes AI",
    "section": "Normal Prior",
    "text": "Normal Prior\nIn case when we know the variance \\(\\sigma^2\\), but do not know mean \\(\\mu\\), we assume \\(\\mu\\) is random. To have conjugate prior we choose \\[\np(\\mu | \\mu_0, \\sigma_0) \\propto \\dfrac{1}{\\sigma_0}\\exp\\left(-\\dfrac{1}{2\\sigma_0^2}(\\mu-\\mu_0^2)\\right)\n\\] In practice, when little is known about \\(\\mu\\), it is common to set the location hyper-parameter to zero and the scale to some large value."
  },
  {
    "objectID": "07-regression.html#normal-model-with-unknown-mean-known-variance",
    "href": "07-regression.html#normal-model-with-unknown-mean-known-variance",
    "title": "Bayes AI",
    "section": "Normal Model with Unknown Mean, Known Variance",
    "text": "Normal Model with Unknown Mean, Known Variance\nSuppose we wish to estimate a model where the likelihood of the data is normal with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nOur parameter of interest is \\(\\mu\\). We can use a conjugate Normal prior on \\(\\mu\\), with mean \\(\\mu_0\\) and variance \\(\\sigma_0^2\\). \\[\n\\begin{aligned}\np(\\mu| x^n, \\sigma^2) & \\propto p(x^n | \\mu, \\sigma^2)p(\\mu) ~~~\\mbox{(Bayes rule)}\\\\\n%    N(\\mu_1,\\tau_1)     & = N(\\mu, \\sigma^2)\\times N(\\mu_0, \\sigma_0^2)\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#useful-identity",
    "href": "07-regression.html#useful-identity",
    "title": "Bayes AI",
    "section": "Useful Identity",
    "text": "Useful Identity\nOne of the most useful algebraic tricks for calculating posterior distribution is completing the square.\n\n\n\n\n\n\n\\[\n\\dfrac{(x-\\mu_1)^2}{\\sigma_1} + \\dfrac{(x-\\mu_2)^2}{\\sigma_2} = \\dfrac{(x - \\mu_3)^2}{\\sigma_3} + \\dfrac{(\\mu_1-\\mu_2)^2}{\\sigma_1 + \\sigma_2}\n\\] where \\[\n\\mu_3 = \\sigma_3 (\\mu_1/\\sigma_1 + \\mu_2/\\sigma_2)\n\\] and \\[\n\\sigma_3 = (1/\\sigma_1 + 1/\\sigma_2)^{-1}\n\\]\n\n\nPrior: \\[\n\\theta \\sim \\frac{e^{-\\frac{(\\theta-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi } \\sigma }\n\\] Likelihood: \\[\nx \\mid \\theta \\sim \\frac{e^{-\\frac{(\\theta-y )^2}{2 r^2}}}{\\sqrt{2 \\pi } r}\n\\] Posterior mean: \\[\n\\frac{x  \\sigma ^2+\\mu  r^2}{r^2+\\sigma ^2}\n\\] Posterior variance: \\[\n\\frac{1}{\\frac{1}{r^2}+\\frac{1}{\\sigma ^2}}\n\\]"
  },
  {
    "objectID": "07-regression.html#prior-likelihood-posterior",
    "href": "07-regression.html#prior-likelihood-posterior",
    "title": "Bayes AI",
    "section": "Prior, Likelihood, Posterior",
    "text": "Prior, Likelihood, Posterior"
  },
  {
    "objectID": "07-regression.html#after-n-steps",
    "href": "07-regression.html#after-n-steps",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\n\\[\n\\begin{aligned}\np(\\mu  | x^n) & \\propto \\prod_{i=1}^{n}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x_i - \\mu )^2}{2\\sigma^2}\\right)\\times\\dfrac{1}{\\sqrt{2\\pi \\sigma_0^2}}\\exp\\left(-\\frac{(\\mu  - \\mu_0)^2}{2\\sigma_0^2}\\right)\\\\\n& \\propto \\exp\\left(-\\sum_{i=1}^{n}\\frac{(x_i - \\mu )^2}{2\\sigma^2} - \\frac{(\\mu  - \\mu_0)^2}{2\\sigma_0^2}\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2}\\left[\\sum_{i=1}^{n}\\frac{(x_i - \\mu )^2}{\\sigma^2} + \\frac{(\\mu  - \\mu_0)^2}{\\sigma_0^2}\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i - \\mu )^2 + \\sigma^2 (\\mu  - \\mu_0)^2\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i^2 - 2\\mu x_i+ \\mu ^2) + \\sigma^2 (\\mu ^2 - 2\\mu \\mu_0 + \\mu_0^2)\\right]\\right)\\\\\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-1",
    "href": "07-regression.html#after-n-steps-1",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\nWe can multiply the \\(2\\mu x_i\\) term in the summation by \\(n/n\\) in order to get the equations in terms of the sufficient statistic \\(\\bar{x}^n\\) \\[\n\\begin{aligned}\np(\\mu  | x^n) & \\propto \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}(x_i^2 - \\dfrac{n}{n}2\\mu x_i+ \\mu ^2) + \\sigma^2 (\\mu ^2 - 2\\mu \\mu_0 + \\mu_0^2)\\right]\\right)\\\\\n& = \\exp\\left(-\\dfrac{1}{2\\sigma^2 \\sigma_0^2}\\left[\\sigma_0^2\\sum_{i=1}^{n}x_i^2 - \\sigma_0^22\\mu n\\bar{x}^n+ \\tau_n^0n\\mu ^2 + \\sigma^2 \\mu ^2 - 2\\mu \\mu_0\\sigma^2 + \\mu_0^2\\sigma^2\\right]\\right)\\end{aligned}\n\\] set \\(k = \\sigma_0^2\\sum_{i=1}^{n}x_i^2 + \\mu_0^2\\sigma^2\\) (they do not contain \\(\\mu\\)) \\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left[\\mu ^2\\left(\\dfrac{1}{\\sigma_0^2} + \\dfrac{n}{\\sigma^2}\\right) - 2\\mu\\left(\\dfrac{\\mu_0}{\\sigma_0^2} + \\dfrac{n\\bar{x}^n}{\\sigma^2}\\right) + k\\right]\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-2",
    "href": "07-regression.html#after-n-steps-2",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\nLet’s multiply by \\[\n\\dfrac{1/\\sigma_0^2 + n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\n\\] Now \\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left(1/\\sigma_0^2 + n/\\sigma^2\\right)\\left(\\mu  - \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\right)^2\\right)\n\\]\n\\[\np(\\mu  | x^n)   \\propto \\exp\\left(-\\dfrac{1}{2}\\left(1/\\sigma_0^2 + n/\\sigma^2\\right)\\left(\\mu  - \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\right)^2\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#after-n-steps-3",
    "href": "07-regression.html#after-n-steps-3",
    "title": "Bayes AI",
    "section": "After \\(n\\) steps",
    "text": "After \\(n\\) steps\n\nPosterior mean: \\(\\mu_n  =  \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\)\nPosterior variance: \\(\\sigma_n^2 = \\left(1/\\sigma_0^2 + n/\\sigma^2\\right)^{-1}\\)\nPosterior precision:: \\(\\tau_n^2 = 1/\\sigma_0^2 + n/\\sigma^2\\)\n\nPosterior Precision is just the sum of the prior precision and the data precision."
  },
  {
    "objectID": "07-regression.html#posterior-mean",
    "href": "07-regression.html#posterior-mean",
    "title": "Bayes AI",
    "section": "Posterior Mean",
    "text": "Posterior Mean\n\\[\n\\begin{aligned}\n\\mu_n  & =  \\dfrac{\\mu_0/\\sigma_0^2 + n\\bar{x}^n/\\sigma^2}{1/\\sigma_0^2 + n/\\sigma^2}\\\\\n& = \\dfrac{\\mu_0\\sigma^2}{\\sigma^2 + n\\sigma_0^2} + \\dfrac{\\sigma_0^2n\\bar{x}^n}{\\sigma^2 + n\\sigma_0^2}\\end{aligned}\n\\]\n\nAs \\(n\\) increases, data mean dominates prior mean.\nAs \\(\\sigma_0^2\\) decreases (less prior variance, greater prior precision), our prior mean becomes more important."
  },
  {
    "objectID": "07-regression.html#a-state-space-model",
    "href": "07-regression.html#a-state-space-model",
    "title": "Bayes AI",
    "section": "A state space model",
    "text": "A state space model\nA state space model consists of two equations: \\[\n\\begin{aligned}\nZ_t&=HS_t+w_t\\\\\nS_{t+1} &= FS_t + v_t\\end{aligned}\n\\] where \\(S_t\\) is a state vector of dimension \\(m\\), \\(Z_t\\) is the observed time series, \\(F\\), \\(G\\), \\(H\\) are matrices of parameters, \\(\\{w_t\\}\\) and \\(\\{v_t\\}\\) are \\(iid\\) random vectors satisfying \\[\n\\mbox{E}(w_t)=0, \\hspace{0.5cm} \\mbox{E}(v_t)=0, \\hspace{0.5cm}\\mathrm{cov}(v_t)=V, \\hspace{0.5cm} \\mathrm{cov}(w_t)=W\n\\] and \\(\\{w_t\\}\\) and \\(\\{v_t\\}\\) are independent."
  },
  {
    "objectID": "07-regression.html#state-space-models",
    "href": "07-regression.html#state-space-models",
    "title": "Bayes AI",
    "section": "State Space Models",
    "text": "State Space Models\n\nState space models consider a time series as the output of a dynamic system perturbed by random disturbances.\nNatural interpretation of a time series as the combination of several components, such as trend, seasonal or regressive components.\nComputations can be implemented by recursive algorithms."
  },
  {
    "objectID": "07-regression.html#types-of-inference",
    "href": "07-regression.html#types-of-inference",
    "title": "Bayes AI",
    "section": "Types of Inference",
    "text": "Types of Inference\n\nModel building versus inferring unknown variable. Assume a linear model \\(Z = HS + \\epsilon\\)\nModel building: know signal \\(S\\), observe \\(Z\\), infer \\(H\\) (a.k.a. model identification, learning)\nEstimation: know \\(H\\), observe \\(Z\\), estimate \\(S\\)\nHypothesis testing: unknown takes one of few possible values; aim at small probability of incorrect decision\nEstimation: aim at a small estimation error"
  },
  {
    "objectID": "07-regression.html#time-series-estimation-tasks",
    "href": "07-regression.html#time-series-estimation-tasks",
    "title": "Bayes AI",
    "section": "Time Series Estimation Tasks",
    "text": "Time Series Estimation Tasks\n\nFiltering: To recover the state vector \\(S_t\\) given \\(Z^t\\)\nPrediction: To predict \\(S_{t+h}\\) or \\(Z_{t+h}\\) for \\(h &gt; 0\\), given \\(Z^t\\)\nSmoothing: To estimate \\(S_t\\) given \\(Z^T\\) , where \\(T &gt; t\\)"
  },
  {
    "objectID": "07-regression.html#property-of-multivariate-normal",
    "href": "07-regression.html#property-of-multivariate-normal",
    "title": "Bayes AI",
    "section": "Property of Multivariate Normal",
    "text": "Property of Multivariate Normal\nUnder normality, we have\n\nthat normal prior plus normal likelihood results in a normal posterior,\nthat if the random vector \\((X, Y )\\) are jointly normal\n\n\\[\n\\begin{bmatrix}\nX\\\\\nY\n\\end{bmatrix}\n\\sim N\\left(\n\\begin{bmatrix}\n\\mu_x\\\\\n\\mu_y\n\\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{xx}&\\Sigma_{xy}\\\\\n\\Sigma_{yx}&\\Sigma_{yy}\\\\\n\\end{bmatrix}\\right),\n\\]\n\nthen the conditional distribution of \\(X\\) given \\(Y = y\\) is normal\n\n\\[\nX|Y = y\\sim N\\left[\\mu_x+\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y),\\Sigma_{xx}-\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}\\right].\n\\]"
  },
  {
    "objectID": "07-regression.html#from-state-space-model",
    "href": "07-regression.html#from-state-space-model",
    "title": "Bayes AI",
    "section": "From State Space Model",
    "text": "From State Space Model\n\\[\n\\begin{aligned}\nS_{t+1}^t &= FS_t\\\\\nZ_{t+1}^t &=HS_{t+1}^t\\\\\nP_{t+1}^t&=FP_tF^T+GQG^T\\\\\nV_{t+1}^t&=HP_{t+1}^tH^T+R\\\\\nC_{t+1}^t&=HP_{t+1}^t\n\\end{aligned}\n\\]\n\n\\(P_{t+j}^t\\) = conditional covariance matrix of \\(S_{t+j}\\) given \\(\\{Z_t , Z_{t-1} , \\cdots\\}\\) for \\(j \\geq 0\\)\n\\(S_{t+j}^t\\) = conditional mean of \\(S_{t+j}\\) given \\(\\{Z_t , Z_{t-1} , \\cdots\\}\\)\n\\(V_{t+1}^t\\) = conditional variance of \\(Z_{t+1}\\) given \\(Z^t = \\{Z_t , Z_{t-1} , \\cdots\\}\\)\n\\(C_{t+1}^t\\) = conditional covariance between \\(Z_{t+1}\\) and \\(S_{t+1}\\)"
  },
  {
    "objectID": "07-regression.html#joint-conditional-distribution-ps_t1-z_t1-zt",
    "href": "07-regression.html#joint-conditional-distribution-ps_t1-z_t1-zt",
    "title": "Bayes AI",
    "section": "Joint conditional distribution \\(P(S_{t+1}, Z_{t+1} | Z^t)\\)",
    "text": "Joint conditional distribution \\(P(S_{t+1}, Z_{t+1} | Z^t)\\)\n\\[\n\\begin{bmatrix}\nS_{t+1}\\\\\nZ_{t+1}\n\\end{bmatrix}_t\n\\sim N\n\\left( \\begin{bmatrix}\nS_{t+1}^t\\\\\nZ_{t+1}^t\n\\end{bmatrix},\n\\begin{bmatrix}\nP_{t+1}^t & P_{t+1}^tH'\\\\\nHP_{t+1}^t & HP_{t+1}^tH'+R\n\\end{bmatrix} \\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#ps_t1-z_t1",
    "href": "07-regression.html#ps_t1-z_t1",
    "title": "Bayes AI",
    "section": "\\(P(S_{t+1}| Z_{t+1})\\)",
    "text": "\\(P(S_{t+1}| Z_{t+1})\\)\nFinally, when \\(Z_{t+1}\\) becomes available, we may use the property of nromality to update the distribution of \\(S_{t+1}\\) . More specifically, \\[\nS_{t+1}=S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\n\\] and \\[\nP_{t+1}=P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH'+R]^{-1}HP_{t+1}^t.\n\\] Predictive residual: \\[\nR_{t+1}^t=Z_{t+1}-Z_{t+1}^t=Z_{t+1}-HS_{t+1}^t \\ne 0\n\\] means there is new information about the system so that the state vector should be modified. The contribution of \\(r_{t+1}^t\\) to the state vector, of course, needs to be weighted by the variance of \\(r_{t+1}^t\\) and the conditional covariance matrix of \\(S_{t+1}\\)."
  },
  {
    "objectID": "07-regression.html#kalman-filter-1",
    "href": "07-regression.html#kalman-filter-1",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\nPredict: \\[\n\\begin{aligned}\nS_{t+1}^t &= FS_t\\\\\nZ_{t+1}^t &=HS_{t+1}^t\\\\\nP_{t+1}^t&=FP_tF^T+GQG^T\\\\\nV_{t+1}^t&=HP_{t+1}^tH^T+R\n\\end{aligned}\n\\]\nUpdate: \\[\n\\begin{aligned}\nS_{t+1|t+1}=& S_{t+1}^t+P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}(Z_{t+1}-Z_{t+1}^t)\\\\\nP_{t+1|t+1}=& P_{t+1}^t-P_{t+1}^tH^T[HP_{t+1}^tH^T+R]^{-1}HP_{t+1}^t\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#kalman-filter-2",
    "href": "07-regression.html#kalman-filter-2",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\nstarts with initial prior information \\(S_{0}\\) and \\(P_{0}\\)\npredicts \\(Z_{1}^0\\) and \\(V_{1}^0\\)\nOnce the observation \\(Z_1\\) is available, uses the updating equations to compute \\(S_{1}\\) and \\(P_{1}\\)\n\n\\(S_{1|1}\\) and \\(P_{1|1}\\) is the prior for the next observation.\nThis is the Kalman recusion."
  },
  {
    "objectID": "07-regression.html#kalman-filter-3",
    "href": "07-regression.html#kalman-filter-3",
    "title": "Bayes AI",
    "section": "Kalman filter",
    "text": "Kalman filter\n\neffect of the initial values \\(S_{0}\\) and \\(P_{0}\\) is decresing as \\(t\\) increases\nfor a stationary time series, all eigenvalues of the coefficient matrix \\(F\\) are less than one in modulus\nKalman filter recursion ensures that the effect of the initial values indeed vanishes as \\(t\\) increases\nuncertainty about the state is always normal"
  },
  {
    "objectID": "07-regression.html#local-trend-model",
    "href": "07-regression.html#local-trend-model",
    "title": "Bayes AI",
    "section": "Local Trend Model",
    "text": "Local Trend Model\n\\[\n\\begin{aligned}\ny_t =& \\mu_t + e_t,~e_t \\sim N(0,\\sigma_e^2)\\\\\\\n\\mu_{t+1} =& \\mu_t + \\eta_t,~ \\eta_t \\sim N(0,\\sigma_{\\eta}^2 )\n\\end{aligned}\n\\]\n\n\\(\\{e_t\\}\\) and \\(\\{\\eta_t\\}\\) are iid Gaussian white noise\n\\(\\mu_0\\) is given (possible as a distributed value)\ntrend \\(\\mu_t\\) is not observable\nwe observe some noisy version of the trend \\(y_t\\)\nsuch a model can be used to analyze realized volatility: \\(\\mu_t\\) is the log volatility and \\(y_t\\) is constructed from high frequency transactions data"
  },
  {
    "objectID": "07-regression.html#local-trend-model-1",
    "href": "07-regression.html#local-trend-model-1",
    "title": "Bayes AI",
    "section": "Local Trend Model",
    "text": "Local Trend Model\n\\[\n\\begin{aligned}\ny_t =& \\mu_t + e_t,~e_t \\sim N(0,\\sigma_e^2)\\\\\\\n\\mu_{t+1} =& \\mu_t + \\eta_t,~ \\eta_t \\sim N(0,\\sigma_{\\eta}^2 )\n\\end{aligned}\n\\]\n\nif \\(\\sigma_e=0\\), then we have ARIMA(0,1,0) model\nif \\(\\sigma_e &gt; 0\\), then we have ARIMA(0,1,1) model, satisfying\n\n\\[\n(1-B)y_t = (1-\\theta B)a_t, ~ a_t \\sim N(0,\\sigma_a^2)\n\\] \\(\\sigma_a\\) and \\(\\theta\\) are determined by \\(\\sigma_e\\) and \\(\\sigma_{\\eta}\\) \\[\n(1-B)y_t = \\eta_{t-1} + e_t - e_{t-1}\n\\]"
  },
  {
    "objectID": "07-regression.html#liner-regression-time-dependent-parameters",
    "href": "07-regression.html#liner-regression-time-dependent-parameters",
    "title": "Bayes AI",
    "section": "Liner Regression (time dependent parameters)",
    "text": "Liner Regression (time dependent parameters)\n\\[\n\\begin{aligned}\ny_t  &= \\alpha_t + \\beta_t \\, x_t + \\epsilon_t   \\qquad & \\epsilon_t \\, \\sim N(0,\\sigma^2) \\\\\n\\alpha_t &= \\quad \\alpha_{t-1}  + \\epsilon_t^{\\alpha} \\qquad & \\epsilon_t^{\\alpha} \\sim N(0,\\sigma_{\\alpha}^2) \\\\\n\\beta_t  &= \\quad \\beta_{t-1}   + \\epsilon_t^{\\beta}   \\qquad & \\epsilon_t^{\\beta} \\sim N(0, \\sigma_{\\beta}^2) \\\\\n\\end{aligned}\n\\]\ndlm Package\n\ndlmModARMA: for an ARMA process, potentially multivariate\ndlmModPoly: for an \\(n^{th}\\) order polynomial\ndlmModReg : for Linear regression\ndlmModSeas: for periodic – Seasonal factors\ndlmModTrig: for periodic – Trigonometric form"
  },
  {
    "objectID": "07-regression.html#local-linear-trend",
    "href": "07-regression.html#local-linear-trend",
    "title": "Bayes AI",
    "section": "Local Linear Trend",
    "text": "Local Linear Trend\n\\[\n\\begin{aligned}\ny_t &= \\qquad \\quad \\mu_t  + \\upsilon_t  \\quad &\\upsilon_t \\sim N(0,V) \\\\\n\\mu_t &= \\mu_{t-1}  + \\delta_{t-1} + \\omega_t^{\\mu} \\quad & \\omega_t^{\\mu} \\sim N(0,W^{\\mu}) \\\\\n\\delta_t &= \\qquad \\,\\, \\, \\delta_{t-1} + \\omega_t^{\\delta} \\quad & \\omega_t^{\\delta} \\sim N(0,W^{\\delta}) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#simple-exponential-smoothing-with-additive-errors",
    "href": "07-regression.html#simple-exponential-smoothing-with-additive-errors",
    "title": "Bayes AI",
    "section": "Simple exponential smoothing with additive errors",
    "text": "Simple exponential smoothing with additive errors\n\\[\nx_t = \\ell_{t-1} + \\varepsilon_t\n\\] \\[\n\\ell_t=\\ell_{t-1}+\\alpha \\varepsilon_t.\n\\]"
  },
  {
    "objectID": "07-regression.html#holts-linear-method-with-additive-errors",
    "href": "07-regression.html#holts-linear-method-with-additive-errors",
    "title": "Bayes AI",
    "section": "Holt’s linear method with additive errors",
    "text": "Holt’s linear method with additive errors\n\\[\n\\begin{aligned}\ny_t&=\\ell_{t-1}+b_{t-1}+\\varepsilon_t\\\\\n\\ell_t&=\\ell_{t-1}+b_{t-1}+\\alpha \\varepsilon_t\\\\\nb_t&=b_{t-1}+\\beta \\varepsilon_t, \\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#relation-to-arma-models",
    "href": "07-regression.html#relation-to-arma-models",
    "title": "Bayes AI",
    "section": "Relation to ARMA models",
    "text": "Relation to ARMA models\nConsider relation with ARMA models. The basic relations are\n\nan ARMA model can be put into a state space form in “infinite\" many ways;\nfor a given state space model in, there is an ARMA model."
  },
  {
    "objectID": "07-regression.html#state-space-model-to-arma-model",
    "href": "07-regression.html#state-space-model-to-arma-model",
    "title": "Bayes AI",
    "section": "State space model to ARMA model",
    "text": "State space model to ARMA model\nThe second possibility is that there is an observational noise. Then, the same argument gives \\[\n(1+\\alpha_1B+\\cdots+\\alpha_mB^m)(Z_{t+m}-\\epsilon_{t+m})=(1-\\theta_1B-\\cdots -\\theta_{m-1}B^{m-1})a_{t+m}\n\\] By combining \\(\\epsilon_t\\) with \\(a_t\\) , the above equation is an ARMA\\((m, m)\\) model."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ar2",
    "href": "07-regression.html#arma-model-to-state-space-model-ar2",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: AR(2)",
    "text": "ARMA model to state space model: AR(2)\n\\[\nZ_t=\\phi_1Z_{t-1}+\\phi_2Z_{t-2}+a_t\n\\] For such an AR(2) process, to compute the forecasts, we need \\(Z_{t-1}\\) and \\(Z_{t-2}\\) . Therefore, it is easily seen that \\[\n\\begin{bmatrix}\nZ_{t+1}\\\\\nZ_t\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\phi_1 & \\phi_2\\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nZ_t\\\\\nZ_{t-1}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\ne_t,\n\\] where \\(e_t = a_{t+1}\\) and \\[\nZ_t=[1, 0]S_t\n\\] where \\(S_t = (Z_t , Z_{t-1})^T\\) and there is no observational noise."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ma2",
    "href": "07-regression.html#arma-model-to-state-space-model-ma2",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: MA(2)",
    "text": "ARMA model to state space model: MA(2)\n\\[\nZ_t=a_t-\\theta_1a_{t-1}-\\theta_2a_{t-2}\n\\] Method 1:\n\\[\n\\begin{bmatrix}\na_t\\\\\na_{t-1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 0\\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\na_{t-1}\\\\\na_{t-2}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\n\\end{bmatrix}\na_t\n\\]\n\\[\nZ_t=[-\\theta_1, -\\theta_2]S_{t-1} + a_t\n\\] Here the innovation \\(a_t\\) shows up in both the state transition equation and the observation equation. The state vector is of dimension 2."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-ma2-1",
    "href": "07-regression.html#arma-model-to-state-space-model-ma2-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: MA(2)",
    "text": "ARMA model to state space model: MA(2)\nMethod 2: For an MA(2) model, we have \\[\n\\begin{aligned}\nZ_{t}^t&=Z_t\\\\\nZ_{t+1}^t&=-\\theta_1a_t-\\theta_2a_{t-1}\\\\\nZ_{t+2}^t&= -\\theta_2a_t\n\\end{aligned}\n\\] Let \\(S_t = (Z_t , -\\theta_1 a_t - \\theta_2 a_{t-1} , -\\theta_2 a_t )^T\\) . Then, \\[\nS_{t+1}=\n\\begin{bmatrix}\n0 & 1& 0\\\\\n0& 0& 1\\\\\n0& 0& 0\n\\end{bmatrix}\nS_t+\n\\begin{bmatrix}\n1\\\\\n-\\theta_1\\\\\n-\\theta_2\n\\end{bmatrix}\na_{t+1}\n\\] and \\[\nZ_t=[1,0,0]S_t\n\\] Here the state vector is of dimension 3, but there is no observational noise."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-akaikes-approach",
    "href": "07-regression.html#arma-model-to-state-space-model-akaikes-approach",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Akaike’s approach",
    "text": "ARMA model to state space model: Akaike’s approach\nConsider ARMA\\((p, q)\\) process, let \\(m = max\\{p, q + 1\\}\\), \\(\\phi_i = 0\\) for \\(i &gt; p\\) and \\(\\theta_j = 0\\) for \\(j &gt; q\\). \\[\nS_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\\cdots , Z_{t+m-1}^t )^T\n\\] where \\(Z_{t+\\ell}^t\\) is the conditional expectation of \\(Z_{t+\\ell}\\) given \\(\\Psi_t = \\{Z_t , Z_{t-1} , \\cdots\\}\\). By using the updating equation \\(f\\) forecasts (recall what we discussed before) \\[\nZ_{t+1}(\\ell -1)=Z_t(\\ell)+\\psi_{\\ell-1}a_{t+1},\n\\]"
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-akaikes-approach-1",
    "href": "07-regression.html#arma-model-to-state-space-model-akaikes-approach-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Akaike’s approach",
    "text": "ARMA model to state space model: Akaike’s approach\n\\[\nS_t = (Z_t , Z_{t+1}^t , Z_{t+2}^t ,\\cdots , Z_{t+m-1}^t )^T\n\\]   \\[\nS_{t+1}=FS_t+Ga_{t+1}\n\\]\n\\[\nZ_t=[1,0, \\cdots ,0]S_t\n\\] where \\[\nF=\n\\left[\n\\begin{array}{c|cccc}\n0 &1& 0& \\cdots& 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & & \\\\\n\\phi_m & \\phi_{m-1} & \\cdots & \\phi_2 & \\phi_1\n\\end{array}\\right], G=\n\\begin{bmatrix}\n1\\\\\n\\psi_1\\\\\n\\psi_2\\\\\n\\vdots\\\\\n\\psi_{m-1}\n\\end{bmatrix}\n\\] The matrix \\(F\\) is call a companion matrix of the polynomial \\(1 - \\phi_1 B - \\cdots - \\phi_m B^m\\)."
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-aokis-method",
    "href": "07-regression.html#arma-model-to-state-space-model-aokis-method",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Aoki’s Method",
    "text": "ARMA model to state space model: Aoki’s Method\nTwo-step procedure: First, consider the MA\\((q)\\) part: \\[\nW_t = a_t - \\theta_1 a_{t-1} - \\cdots - \\theta_q a_{t-q}\n\\] \\[\n\\begin{bmatrix}\na_t\\\\\na_{t-1}\\\\\n\\vdots\\\\\na_{t-q+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0&0&\\cdots & 0&0\\\\\n1&0& \\cdots&0&0\\\\\n\\vdots & & & &\\\\\n0 & 0 & \\cdots & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\na_{t-1}\\\\\na_{t-2}\\\\\n\\vdots\\\\\na_{t-q}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{bmatrix}\na_t\n\\]\n\\[\nW_t=[-\\theta_1,-\\theta_2, \\cdots, -\\theta_q]S_t+a_t\n\\]"
  },
  {
    "objectID": "07-regression.html#arma-model-to-state-space-model-aokis-method-1",
    "href": "07-regression.html#arma-model-to-state-space-model-aokis-method-1",
    "title": "Bayes AI",
    "section": "ARMA model to state space model: Aoki’s Method",
    "text": "ARMA model to state space model: Aoki’s Method\nFirst, consider the AR\\((p)\\) part: \\[\nZ_t = \\phi_1 Z_{t-1} + ... + \\phi_p Z_{t-p} + W_t\n\\] Define state-space vector as \\[\nS_t=(Z_{t-1},Z_{t-2}, \\cdots , Z_{t-p},a_{t-1}, \\cdots, a_{t-q})'\n\\] Then, we have \\[\n\\begin{bmatrix}\nZ-t\\\\\nZ_{t-1}\\\\\n\\vdots\\\\\nZ_{t-p+1}\\\\\na_t\\\\\na_{t-1}\\\\\n\\vdots\\\\\na_{t-q+1}\n\\end{bmatrix}\n=\n\\left[\n\\begin{array}{cccc|cccc}\n\\phi_1&\\phi_2&\\cdots&\\phi_p&-\\theta_1&-\\theta_2&\\cdots&-\\theta_q\\\\\n1&0&\\cdots&0&0&0&\\cdots&0\\\\\n\\vdots &&&&\\vdots&&&\\\\\n0&\\cdots&1&0&0&0&\\cdots&0\\\\\n0&0&\\cdots&0&0&0&\\cdots&0\\\\\n0&0&\\cdots&0&1&0&\\cdots&0\\\\\n\\vdots &&&&0&&&\\\\\n0&0&\\cdots&0&0&\\cdots&1&0\n\\end{array}\\right]\n\\begin{bmatrix}\nZ_{t-1}\\\\\nZ_{t-2}\\\\\n\\vdots\\\\\nZ_{t-p}\\\\\na_{t-1}\\\\\na_{t-2}\\\\\n\\vdots\\\\\na_{t-q}\n\\end{bmatrix}+\n\\begin{bmatrix}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\\\\n1\\\\\n0\\\\\n\\vdots\\\\\n0\n\\end{bmatrix}a_t\n\\] and \\[\nZ_t=[\\phi_1,\\cdots,\\phi_p,-\\theta_1,\\cdots,-\\theta_q]S_t+a_t\n\\]"
  },
  {
    "objectID": "07-regression.html#mle-estimation",
    "href": "07-regression.html#mle-estimation",
    "title": "Bayes AI",
    "section": "MLE Estimation",
    "text": "MLE Estimation\nInnovations are given by \\[\n\\epsilon_t = Z_t - HS_t^{t-1}\n\\] can be shown that \\(\\mathrm{var}(\\epsilon_t) = \\Sigma_t\\), where \\[\n\\Sigma_t = HP_t^{t-1}H^T + R\n\\] Incomplete Data Likelihood: \\[\n-\\ln L(\\Theta) = \\dfrac{1}{2}\\sum_{t=1}^{n}\\log|\\Sigma_t(\\Theta)| + \\dfrac{1}{2}\\sum_{t=1}^{n}\\epsilon_t(\\Theta)^T\\Sigma(\\Theta)^{-1}\\epsilon_t(\\Theta)\n\\] Here \\(\\Theta = (F, Q, R)\\). Use BFGS to find a sequence of \\(\\Theta\\)’s and stop when stagnation happens."
  },
  {
    "objectID": "07-regression.html#kalman-smoother",
    "href": "07-regression.html#kalman-smoother",
    "title": "Bayes AI",
    "section": "Kalman Smoother",
    "text": "Kalman Smoother\n\nInput: initial distribution \\(X_0\\) and data \\(Z_1,...,Z_T\\)\nAlgorithm: forward-backward pass\nForward pass: Kalman filter: compute \\(S_{t+1}^t\\) and \\(S_{t+1}^{t+1}\\) for \\(0 \\le t &lt; T\\)\nBackward pass: Compute \\(S_t^T\\) for \\(0 \\le t &lt; T\\)"
  },
  {
    "objectID": "07-regression.html#backward-pass",
    "href": "07-regression.html#backward-pass",
    "title": "Bayes AI",
    "section": "Backward Pass",
    "text": "Backward Pass\n\nCompute \\(X_t^T\\) given \\(S_{t+1}^T \\sim N(m_{t+1}^T,C_{t+1}^T)\\)\nReverse arrow: \\(S_t^t \\leftarrow X_{t+1}^t\\)\nSame as incorporating measurement in filter\nCompute joint \\((S_t^t, S_{t+1}^t)\\)\nCompute conditional \\((S_t^t \\mid S_{t+1}^t )\\)\nNew: \\(S_{t+1}\\) is not “known”, we only know its distribution: \\(S_{t+1} \\sim S_{t+1}^T\\)\n“Uncondition” on \\(S_{t+1}\\) to compute \\(S_t^T\\) using laws of total expectation and variance"
  },
  {
    "objectID": "07-regression.html#kalman-smoother-1",
    "href": "07-regression.html#kalman-smoother-1",
    "title": "Bayes AI",
    "section": "Kalman Smoother",
    "text": "Kalman Smoother\nA smoothed version of data (an estimate, based on the entire data set) If \\(S_n\\) and \\(P_n\\) obtained via Kalman recursions, then for \\(t=n,..,1\\) \\[\n\\begin{aligned}\nS_{t-1}^t &= S_{t-1} + J_{t-1}(S_t^n - S_t^{t-1})\\\\\nP_{t-1}^n &= P^{t-1} + J_{t-1}(P_t^n - P_t^{t-1})J^T_{t-1}\\\\\nJ_{t-1} & = P_{t-1}F^T[P_t^{t-1}]^{-1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "07-regression.html#kalman-and-histogran-filter-shortciomings",
    "href": "07-regression.html#kalman-and-histogran-filter-shortciomings",
    "title": "Bayes AI",
    "section": "Kalman and Histogran Filter Shortciomings",
    "text": "Kalman and Histogran Filter Shortciomings\nKalman:\n\nlinear dynamics\nlinear measurement model\nnormal errors\nunimodal uncertainty\n\nHistogram:\n\ndiscrete states\napproximation\ninefficient in memory"
  },
  {
    "objectID": "07-regression.html#mcmc-financial-econometrics",
    "href": "07-regression.html#mcmc-financial-econometrics",
    "title": "Bayes AI",
    "section": "MCMC Financial Econometrics",
    "text": "MCMC Financial Econometrics\nSet of tools for inference and pricing in continuous-time models.\n\nSimulation-based and provides a unified approach to state and parameter inference. Can also be applied sequentially.\nCan handle Estimation and Model risk. Important implications for financial decision making\nBayesian inference. Uses conditional probability to solve an inverse problem and estimates expectations using Monte Carlo."
  },
  {
    "objectID": "07-regression.html#filtering-smoothing-learning-and-prediction",
    "href": "07-regression.html#filtering-smoothing-learning-and-prediction",
    "title": "Bayes AI",
    "section": "Filtering, Smoothing, Learning and Prediction",
    "text": "Filtering, Smoothing, Learning and Prediction\nData \\(y_{t}\\) depends on a , \\(x_{t}\\). \\[\n\\begin{aligned}\n\\text{Observation equation} &  \\text{:\\ }y_{t}=f\\left(  x_{t},\\varepsilon\n_{t}^{y}\\right)  \\\\\n\\text{State evolution} &  \\text{: }x_{t+1}=g\\left(  x_{t},\\varepsilon\n_{t+1}^{x}\\right)  ,\\end{aligned}\n\\]\n\nPosterior distribution of \\(p\\left(x_{t}|y^{t}\\right)\\) where \\(y^{t}=\\left(\ny_{1},...,y_{t}\\right)\\)\nPrediction and Bayesian updating.\n\n\\[\np\\left(  x_{t+1}|y^{t}\\right)  =\\int p\\left(  x_{t+1}|x_{t}\\right)  p\\left(\nx_{t}|y^{t}\\right)  dx_{t},\\label{Predict}%\n\\] updated by Bayes rule\n\\[\n\\underset{\\text{Posterior}}{\\underbrace{p\\left(  x_{t+1}|y^{t+1}\\right)  }%\n}\\propto\\underset{\\text{Likelihood}}{\\underbrace{p\\left(  y_{t+1}%\n|x_{t+1}\\right)  }}\\underset{\\text{Prior}}{\\underbrace{p\\left(  x_{t+1}%\n|y^{t}\\right)  }}.\\label{Update}%\n\\]"
  },
  {
    "objectID": "07-regression.html#nonlinear-model",
    "href": "07-regression.html#nonlinear-model",
    "title": "Bayes AI",
    "section": "Nonlinear Model",
    "text": "Nonlinear Model\n\nThe observation and evolution dynamics are \\[\n\\begin{aligned}\ny_t & = \\frac{x_t}{1+ x_t^2} + v_t \\; , {\\rm where} \\; v_t \\sim N(0,1) \\\\\nx_t & = x_{t-1} + w_t  \\; , {\\rm where} \\; w_t \\sim N(0,0.5)\\end{aligned}\n\\]\nInitial condition \\(x_0 \\sim N( 1 , 10 )\\)\n\nFundamental question:\nHow do the filtering distributions \\(p(x_t|y^t)\\) propagate in time?"
  },
  {
    "objectID": "07-regression.html#nonlinear-y_t-x_t-1-x_t2-v_t",
    "href": "07-regression.html#nonlinear-y_t-x_t-1-x_t2-v_t",
    "title": "Bayes AI",
    "section": "Nonlinear: \\(y_t = x_t / (1+ x_t^2) + v_t\\)",
    "text": "Nonlinear: \\(y_t = x_t / (1+ x_t^2) + v_t\\)"
  },
  {
    "objectID": "07-regression.html#simulate-data",
    "href": "07-regression.html#simulate-data",
    "title": "Bayes AI",
    "section": "Simulate Data",
    "text": "Simulate Data\n\n\nCode\nset.seed(8)\n# MC sample size\nN = 1000\n\n#Posterior at time t=0 p(x[0]|y[0])=N(1,10)\nx = rnorm(N,1,sqrt(10))\nhist(x,main=\"Pr(x[0]|y[0])\")\n\n\n\n\n\n\n\n\n\nCode\n#Obtain draws from prior p(x[1]|y[0])\nx1 = x + rnorm(N,0,sqrt(0.5))\nhist(x1,main=\"Pr(x[1]|y[0])\")\n\n\n\n\n\n\n\n\n\nCode\n#Likelihood function p(y[1]|x[1])\ny1  = 5\nths = seq(-30,30,length=1000)\nplot(ths,dnorm(y1,ths/(1+ths^2),1),type=\"l\",xlab=\"\",ylab=\"\")\ntitle(paste(\"p(y[1]=\",y1,\"|x[1])\",sep=\"\"))"
  },
  {
    "objectID": "07-regression.html#nonlinear-filtering",
    "href": "07-regression.html#nonlinear-filtering",
    "title": "Bayes AI",
    "section": "Nonlinear Filtering",
    "text": "Nonlinear Filtering"
  },
  {
    "objectID": "07-regression.html#resampling",
    "href": "07-regression.html#resampling",
    "title": "Bayes AI",
    "section": "Resampling",
    "text": "Resampling\nKey: resample and propagate particles\n\n\nCode\n# Computing resampling weights\nw = dnorm(y1,x1/(1+x1^2),1)\n# Resample to obtain draws from p(x[1]|y[1])\nk = sample(1:N,size=N,replace=TRUE,prob=w)\nx = x1[k]\nhist(x,main=\"Pr(x[1]|y[1])\")\n\n\n\n\n\n\n\n\n\nCode\n#Obtain draws from prior p(x[2]|y[1])\nx2 = x + rnorm(N,0,sqrt(0.5))\nhist(x2,main=\"Pr(x[2]|y[1])\")"
  },
  {
    "objectID": "07-regression.html#propagation-of-mc-error",
    "href": "07-regression.html#propagation-of-mc-error",
    "title": "Bayes AI",
    "section": "Propagation of MC error",
    "text": "Propagation of MC error"
  },
  {
    "objectID": "07-regression.html#dynamic-linear-model-dlm-kalman-filter",
    "href": "07-regression.html#dynamic-linear-model-dlm-kalman-filter",
    "title": "Bayes AI",
    "section": "Dynamic Linear Model (DLM): Kalman Filter",
    "text": "Dynamic Linear Model (DLM): Kalman Filter\nKalman filter for linear Gaussian systems\n\nFFBS (Filter Forward Backwards Sample)\n\nThis determines the posterior distribution of the states\n\\[\np( x_t | y^t ) \\; {\\rm and} \\; p( x_t | y^T )\n\\] Also the joint distribution \\(p( x^T | y^T )\\) of the hidden states.\n\nDiscrete Hidden Markov Model HMM (Baum-Welch, Viterbi)\nWith parameters known the Kalman filter gives the exact recursions."
  },
  {
    "objectID": "07-regression.html#simulate-dlm",
    "href": "07-regression.html#simulate-dlm",
    "title": "Bayes AI",
    "section": "Simulate DLM",
    "text": "Simulate DLM\nDynamic Linear Models \\[\ny_t = x_t + v_t \\; \\; {\\rm and} \\; \\; x_t = \\alpha + \\beta x_{t-1} + w_t\n\\] Simulate Data\n\n\nCode\nT1=50; alpha=-0.01; beta=0.98; sW=0.1; sV=0.25; W=sW^2; V=sV^2\n\ny = rep(0,2*T1)\nht = rep(0,2*T1)\n\nfor (t in 2:(2*T1)){\n    ht[t] = rnorm(1,alpha+beta*ht[t-1],sW)\n    y[t] = rnorm(1,ht[t],sV)\n}\nht = ht[(T1+1):(2*T1)]\ny = y[(T1+1):(2*T1)]"
  },
  {
    "objectID": "07-regression.html#exact-calculations",
    "href": "07-regression.html#exact-calculations",
    "title": "Bayes AI",
    "section": "Exact calculations",
    "text": "Exact calculations\nKalman Filter recursions\n\n\nCode\nm=rep(0,T1);C=rep(0,T1); m0=0; C0=100\n\nR = C0+W; Q = R+V; A = R/Q\nm[1] = m0+A*(y[1]-m0); C[1] = R-A^2*Q\n\nfor (t in 2:T1){\n    R    = C[t-1]+W\n    Q    = R+V\n    A    = R/Q\n    m[t] = m[t-1]+A*(y[t]-m[t-1])\n    C[t] = R-A^2*Q\n}"
  },
  {
    "objectID": "07-regression.html#dlm-data",
    "href": "07-regression.html#dlm-data",
    "title": "Bayes AI",
    "section": "DLM Data",
    "text": "DLM Data\n\n\nCode\npar(mar=c(4,4,0,0))\nplot(y,type=\"b\",col=\"blue\",xlab=\"Time\",ylab=\"y_t\", lwd=2, bty='n')\nlines(m,col=\"red\")\nlines(m+2*sqrt(C),col=\"grey\",lty=2)\nlines(m-2*sqrt(C),col=\"grey\",lty=2)"
  },
  {
    "objectID": "07-regression.html#bootstrap-filter",
    "href": "07-regression.html#bootstrap-filter",
    "title": "Bayes AI",
    "section": "Bootstrap Filter",
    "text": "Bootstrap Filter\n\n\nCode\nM = 1000\nh = rnorm(M,m0,sqrt(C0))\nhs = NULL\n\nfor (t in 1:T1){\n    h1 = rnorm(M,alpha+beta*h,sW)\n    w  = dnorm(y[t],h1,sV)\n    w  = w/sum(w)\n    h  = sample(h1,size=M,replace=T,prob=w)\n    hs = cbind(hs,h)\n}\n# Quantiles\nq025 = function(x){quantile(x,0.025)}\nq975 = function(x){quantile(x,0.975)}\nh025 = apply(hs,2,q025)\nh975 = apply(hs,2,q975)\nsd   = sqrt(apply(hs,2,var))\nm = colMeans(hs)\npar(mar=c(4,4,0,0))\nplot(y,type=\"b\",col=\"blue\",xlab=\"Time\",ylab=\"y_t\", lwd=2, bty='n')\nlines(m,col=\"red\")\nlines(h025,col=\"grey\",lty=2)\nlines(h975,col=\"grey\",lty=2)"
  },
  {
    "objectID": "07-regression.html#streaming-data-how-do-parameter-distributions-change-in-time",
    "href": "07-regression.html#streaming-data-how-do-parameter-distributions-change-in-time",
    "title": "Bayes AI",
    "section": "Streaming Data: How do Parameter Distributions change in Time?",
    "text": "Streaming Data: How do Parameter Distributions change in Time?\n\n\n\n\n\n\n Bayes theorem: \\[\np(\\theta \\mid y^t) \\propto p(y_t \\mid \\theta) \\,\np(\\theta \\mid y^{t-1})\n\\]\n\n\nOnline Dynamic Learning\n\nReal-time surveillance\nBayes means sequential updating of information\nUpdate posterior density \\(p(\\theta \\mid y_t)\\) with every new observation (\\(t = 1, \\ldots, T\\)) - “sequential learning”"
  },
  {
    "objectID": "07-regression.html#galton-1877-first-particle-filter",
    "href": "07-regression.html#galton-1877-first-particle-filter",
    "title": "Bayes AI",
    "section": "Galton 1877: First Particle Filter",
    "text": "Galton 1877: First Particle Filter"
  },
  {
    "objectID": "07-regression.html#streaming-data-online-learning",
    "href": "07-regression.html#streaming-data-online-learning",
    "title": "Bayes AI",
    "section": "Streaming Data: Online Learning",
    "text": "Streaming Data: Online Learning\nConstruct an essential state vector \\(Z_{t+1}\\). \\[\n\\begin{aligned}\np(Z_{t+1}|y^{t+1}) &= \\int p(Z_{t+1}|Z_t, y_{t+1}) \\;d\\mathbb{P}(Z_t|y^{t+1}) \\\\\n&\\propto  \\int \\underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \\overbrace{ \\underbrace{p(y_{t+1} | Z_t)}_{resample} \\;d\\mathbb{P}(Z_t|y^t)}\\end{aligned}\n\\]\n\nRe-sample with weights proportional to \\(p(y_{t+1} | Z_t^{(i)})\\) and generate \\(\\{Z_t^{\\zeta(i)}\\}_{i=1}^N\\)\nPropagate with \\(Z_{t+1}^{(i)} \\sim\np(Z_{t+1}|Z_t^{\\zeta(i)}, y_{t+1})\\) to obtain \\(\\{Z_{t+1}^{(i)}\\}_{i=1}^N\\)\n\nParameters: \\(p( \\theta | Z_{t+1} )\\) drawn “offline”"
  },
  {
    "objectID": "07-regression.html#sample-resample",
    "href": "07-regression.html#sample-resample",
    "title": "Bayes AI",
    "section": "Sample – Resample",
    "text": "Sample – Resample\n\nsample-resample"
  },
  {
    "objectID": "07-regression.html#resample-sample",
    "href": "07-regression.html#resample-sample",
    "title": "Bayes AI",
    "section": "Resample – Sample",
    "text": "Resample – Sample\n\nresample-sample"
  },
  {
    "objectID": "07-regression.html#particle-methods-blind-propagation",
    "href": "07-regression.html#particle-methods-blind-propagation",
    "title": "Bayes AI",
    "section": "Particle Methods: Blind Propagation",
    "text": "Particle Methods: Blind Propagation\n\nPropagate-Resample is replaced by Resample-Propagate"
  },
  {
    "objectID": "07-regression.html#traffic-problem",
    "href": "07-regression.html#traffic-problem",
    "title": "Bayes AI",
    "section": "Traffic Problem",
    "text": "Traffic Problem\n\nState-Space"
  },
  {
    "objectID": "07-regression.html#wave-speed-propagation-is-a-mixture-distribution",
    "href": "07-regression.html#wave-speed-propagation-is-a-mixture-distribution",
    "title": "Bayes AI",
    "section": "Wave Speed Propagation is a Mixture Distribution",
    "text": "Wave Speed Propagation is a Mixture Distribution\nShock wave propagation speed is a mixture, when calculated using Godunov scheme \\[\nw = \\frac{q(\\rho_l) - q(\\rho_r)}{\\rho_l-\\rho_r}  \\ \\left[\\frac{mi}{h}\\right] = \\left[\\frac{veh}{h}\\right]\\left[\\frac{mi}{veh}\\right].\n\\] Assume \\(\\rho_l \\sim TN(32, 16, 0, 320)\\) and \\(\\rho_r \\sim TN(48, 16, 0,320)\\)\n\\(q_c = 1600  \\ veh/h\\), \\(\\rho_c = 40 \\ veh/mi\\), and \\(\\rho_{jam} = 320 \\ veh/mi\\)"
  },
  {
    "objectID": "07-regression.html#traffic-flow-speed-forecast-is-a-mixtrue-dsitribution",
    "href": "07-regression.html#traffic-flow-speed-forecast-is-a-mixtrue-dsitribution",
    "title": "Bayes AI",
    "section": "Traffic Flow Speed Forecast is a Mixtrue Dsitribution",
    "text": "Traffic Flow Speed Forecast is a Mixtrue Dsitribution\nTheorem: The solution (including numerical) to the LWR model with stochastic initial conditions is a mixture distribution.\n\nA moment based filters such as Kalman Filter or Extended Kalman Filter would not capture the mixture."
  },
  {
    "objectID": "07-regression.html#problem-at-hand",
    "href": "07-regression.html#problem-at-hand",
    "title": "Bayes AI",
    "section": "Problem at Hand",
    "text": "Problem at Hand\nThe Parameter Learning and State Estimation Problem\n\nGoal: given sparse sensor measurements, find the distribution over traffic state and underlying traffic flow parameters \\(p(\\theta_t, \\phi|y_1, y_2,...,y_t); \\ \\phi=(q_c,\\rho_c)\\)\nParameters of the evolution equation (LWR) are stochastic\nDistribution over state is a mixture\nCan’t use moment based filters (KF, EKF,...)"
  },
  {
    "objectID": "07-regression.html#data-assimilation-state-space-representation",
    "href": "07-regression.html#data-assimilation-state-space-representation",
    "title": "Bayes AI",
    "section": "Data Assimilation: State Space Representation",
    "text": "Data Assimilation: State Space Representation\n\nState space formulation allows to combine knowledge from analytical model with the one from field measurements, while taking model and measurement errors into account"
  },
  {
    "objectID": "07-regression.html#state-space-representation",
    "href": "07-regression.html#state-space-representation",
    "title": "Bayes AI",
    "section": "State Space Representation",
    "text": "State Space Representation\n\nState vector \\(\\theta_t = ( \\rho_{1t} , \\ldots , \\rho_{nt} )\\)\nBoundary conditionals \\(\\rho_{0t}\\) and \\(\\rho_{(n+1)t}\\)\nUnderlying parameters \\(\\phi = (q_c, \\rho_c)\\) are stochastic\n\n\\[\\begin{align}\n\\mbox{Observation: }&y_{t+1} = H\\theta_{t+1}  + v; \\ v \\sim N(0,V) \\label{eqn-y}\\\\\n\\mbox{Evolution: }&\\theta_{t+1} = f_{\\phi}(\\theta_t) + w; \\ w \\sim N(0,W) \\label{(eqn-x)}\n\\end{align}\\]\n\\(H: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^k\\) in the measurement model. \\(\\phi  = (q_c,\\rho_c, \\rho_{max})\\).\nParameter priors: \\(q_c \\sim N(\\mu_q, \\sigma_c^2)\\), \\(\\rho_c = Uniform(\\rho_{min}, \\rho_{max})\\)"
  },
  {
    "objectID": "07-regression.html#particle-parameter-learning",
    "href": "07-regression.html#particle-parameter-learning",
    "title": "Bayes AI",
    "section": "Particle Parameter Learning",
    "text": "Particle Parameter Learning"
  },
  {
    "objectID": "07-regression.html#sample-based-pdf-representation",
    "href": "07-regression.html#sample-based-pdf-representation",
    "title": "Bayes AI",
    "section": "Sample-based PDF Representation",
    "text": "Sample-based PDF Representation\n\nRegions of high density: Many particles and Large weight of particles\nUneven partitioning\nDiscrete approximation for continuous pdf\n\n\\[\np^{N}\\left(  \\theta_{t+1}|y^{t+1}\\right) \\propto \\sum_{i=1}^{N}w_{t}^{\\left(  i\\right)  }p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right)\n\\]"
  },
  {
    "objectID": "07-regression.html#particle-filter",
    "href": "07-regression.html#particle-filter",
    "title": "Bayes AI",
    "section": "Particle Filter",
    "text": "Particle Filter\nBayes Rule: \\[\np(y_{t+1},\\theta_{t+1}|\\theta_t)=p(y_{t+1}|\\theta_t)\\,p(\\theta_{t+1}|%\n\\theta_t,y_{t+1}).\n\\]\n\nGiven a particle approximation to \\(p^{N}\\left(  \\theta_t|y^{t}\\right)\\) \\[\n\\begin{aligned}\np^{N}\\left(  \\theta_{t+1}|y^{t+1}\\right)   &  \\propto\\sum_{i=1}^{N}p\\left(\ny_{t+1}|\\theta_t^{\\left(  i\\right)  }\\right)  p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right) \\label{Mixture2}\\\\\n&  =\\sum_{i=1}^{N}w_{t}^{\\left(  i\\right)  }p\\left(  \\theta_{t+1}|\\theta_t^{\\left(\ni\\right)  },y_{t+1}\\right)  \\text{,}%\n\\end{aligned}\n\\] where\n\n\\[\nw_{t}^{\\left(  i\\right)  }=\\frac{p\\left(  y_{t+1}|\\theta_t^{\\left(  i\\right)\n}\\right)  }{\\sum_{i=1}^{N}p\\left(  y_{t+1}|\\theta_t^{\\left(  i\\right)  }\\right)\n}\\text{.}%\n\\]\n\nEssentially a mixture Kalman filter"
  },
  {
    "objectID": "07-regression.html#particle-parameter-learning-1",
    "href": "07-regression.html#particle-parameter-learning-1",
    "title": "Bayes AI",
    "section": "Particle Parameter Learning",
    "text": "Particle Parameter Learning\nGiven particles (a.k.a. random draws) \\((\\theta^{(i)}_t,\\phi^{(i)},s^{(i)}_t),\\) \\(i=1,\\ldots,N\\) \\[\np( \\theta_t | y_{1:t} ) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{ \\theta^{(i)} } \\; .\n\\]\n\nFirst resample \\((\\theta^{k(i)}_t,\\phi^{k(i)},s^{k(i)}_t)\\) with weights proportional to \\(p(y_{t+1}|\\theta^{k(i)}_t,\\phi^{k(i)})\\) and \\(s_t^{k(i)}=S(s^{(i)}_t,\\theta^{k(i)}_t,y_{t+1})\\) and then propogate to \\(p(\\theta_{t+1}|y_{1:t+1})\\) by drawing \\(\\theta^{(i)}_{t+1}\\,\\)from \\(p(\\theta_{t+1}|\\theta^{k(i)}_t,\\phi^{k(i)},y_{t+1}),\\,i=1,\\ldots,N\\).\nNext we update the sufficient statistic as\n\n\\[\ns_{t+1}=S(s_t^{k(i)},\\theta^{(i)}_{t+1},y_{t+1}),\n\\] for \\(i=1,\\ldots,N\\), which represents a deterministic propogation.\n\nFinally, parameter learning is completed by drawing \\(\\phi^{(i)}\\) using \\(p(\\phi|s^{(i)}_{t+1})\\) for \\(i=1,\\ldots,N\\)."
  },
  {
    "objectID": "07-regression.html#streaming-data",
    "href": "07-regression.html#streaming-data",
    "title": "Bayes AI",
    "section": "Streaming Data",
    "text": "Streaming Data\nOnline Learning\nConstruct an essential state vector \\(Z_{t+1}\\). \\[\n\\begin{aligned}\np(Z_{t+1}|y^{t+1}) &= \\int p(Z_{t+1}|Z_t, y_{t+1}) \\;d\\mathbb{P}(Z_t|y^{t+1}) \\\\\n&\\propto  \\int \\underbrace{p(Z_{t+1}|Z_t, y_{t+1})}_{propagate} \\overbrace{ \\underbrace{p(y_{t+1} | Z_t)}_{resample} \\;d\\mathbb{P}(Z_t|y^t)}\n\\end{aligned}\n\\]\n\nRe-sample with weights proportional to \\(p(y_{t+1} | Z_t^{(i)})\\) and generate \\(\\{Z_t^{\\zeta(i)}\\}_{i=1}^N\\)\nPropagate with \\(Z_{t+1}^{(i)} \\sim\np(Z_{t+1}|Z_t^{\\zeta(i)}, y_{t+1})\\) to obtain \\(\\{Z_{t+1}^{(i)}\\}_{i=1}^N\\)\n\nParameters: \\(p( \\theta | Z_{t+1} )\\) drawn “offline”"
  },
  {
    "objectID": "07-regression.html#resample-propagate",
    "href": "07-regression.html#resample-propagate",
    "title": "Bayes AI",
    "section": "Resample – Propagate",
    "text": "Resample – Propagate"
  },
  {
    "objectID": "07-regression.html#algorithm",
    "href": "07-regression.html#algorithm",
    "title": "Bayes AI",
    "section": "Algorithm",
    "text": "Algorithm\nThese ingredients then define a particle filtering and learning algorithm for the sequence of joint posterior distributions \\(p( \\theta_t , \\phi | y_{1:t} )\\): \\[\n\\begin{aligned}\n&  \\text{Step 1. (Resample) Draw an index } k_t \\left(  i\\right)  \\sim\nMult_{N}\\left(  w_{t}^{\\left(  1\\right)  },...,w_{t}^{\\left(  N\\right)\n}\\right), \\\\\n& \\mbox{where the weights are given by } w_t^{(i)} \\propto p(y_{t+1}|(\\theta_t,\\phi)^{(i)}), \\   \\text{ for }i=1,...,N\\\\\n\\text{ }  &  \\text{Step 2. (Propagate) Draw }\\theta_{t+1}^{\\left(  i\\right)  }\\sim\np\\left(  \\theta_{t+1}|\\theta_t^{k_t \\left(  i\\right)  },y_{t+1}\\right)  \\text{ for\n}i=1,...,N.\\\\\n\\text{ }  &  \\text{Step 3. (Update) } s_{t+1}^{(i)} =S(s_t^{k_t(i)},\\theta_{t+1}^{(i)},y_{t+1})\\\\\n\\text{ }  &  \\text{Step 4. (Replenish) } \\phi^{(i)} \\sim p( \\phi | s_{t+1}^{(i)} )\n\\end{aligned}\n\\] There are a number of efficiency gains from such an approach, e.g. it does not suffer from degeneracy problems associated with traditional propagate-resample algorithms when \\(y_{t+1}\\) is an outliers."
  },
  {
    "objectID": "07-regression.html#obtaining-state-estimates-from-particles",
    "href": "07-regression.html#obtaining-state-estimates-from-particles",
    "title": "Bayes AI",
    "section": "Obtaining state estimates from particles",
    "text": "Obtaining state estimates from particles\n\nAny estimate of a function \\(f(\\theta_t)\\) can be calculated by discrete-approximation\n\n\\[\n\\mbox{E}(f(\\theta_t)) = \\dfrac{1}{N}\\sum_{j=1}^{N}w_t^{(j)}f(\\theta_t^{(j)})\n\\]\n\nMean:\n\n\\[\n\\mbox{E}(\\theta_t) = \\dfrac{1}{N}\\sum_{j=1}^{N}w_t^{(j)}\\theta_t^{(j)}\n\\]\n\nMAP-estimate: particle with largest weight\nRobust mean: mean within window around MAP-estimate"
  },
  {
    "objectID": "07-regression.html#particle-filters-pluses",
    "href": "07-regression.html#particle-filters-pluses",
    "title": "Bayes AI",
    "section": "Particle Filters: Pluses",
    "text": "Particle Filters: Pluses\n\nEstimation of full PDFs\nNon-Gaussian distributions (multi-modal)\nNon-linear state and observation model\nParallelizable"
  },
  {
    "objectID": "07-regression.html#particle-filters-minuses",
    "href": "07-regression.html#particle-filters-minuses",
    "title": "Bayes AI",
    "section": "Particle Filters: Minuses",
    "text": "Particle Filters: Minuses\n\nDegeneracy problem\nHigh number of particles needed\nComputationally expensive\nLinear-Gaussian assumption is often sufficient"
  },
  {
    "objectID": "07-regression.html#applications-localization",
    "href": "07-regression.html#applications-localization",
    "title": "Bayes AI",
    "section": "Applications: Localization",
    "text": "Applications: Localization\n\nTrack car position in given road map\nTrack car position from radio frequency measurements\nTrack aircraft position from estimated terrain elevation\nCollision Avoidance (Prediction)"
  },
  {
    "objectID": "07-regression.html#applications-model-estimation",
    "href": "07-regression.html#applications-model-estimation",
    "title": "Bayes AI",
    "section": "Applications: Model Estimation",
    "text": "Applications: Model Estimation\n\nTracking with multiple motion-models\nRecovery of signal from noisy measurements\nNeural Network model selection (on-line classification)"
  },
  {
    "objectID": "07-regression.html#applications-other",
    "href": "07-regression.html#applications-other",
    "title": "Bayes AI",
    "section": "Applications: Other",
    "text": "Applications: Other\n\nVisual Tracking\nPrediction of (financial) time series\nQuality control in semiconductor industry\nMilitary applications: Target recognition from single or multiple images, Guidance of missiles\nReinforcement Learning"
  },
  {
    "objectID": "07-regression.html#mixture-kalman-filter-for-traffic",
    "href": "07-regression.html#mixture-kalman-filter-for-traffic",
    "title": "Bayes AI",
    "section": "Mixture Kalman Filter For Traffic",
    "text": "Mixture Kalman Filter For Traffic\n\\[\n\\begin{aligned}\n\\mbox{Observation: }&y_{t+1} = Hx_{t+1}  + \\gamma^Tz_{t+1} + v_{t+1} , \\ v_{t+1} \\sim N(0, V_{t+1})\\\\\n\\mbox{Evolution: }&x_{t+1} = F_{\\alpha_{t+1}}x_t + (1-F_{\\alpha_{t+1}})\\mu + \\alpha_t\\beta_{t} + \\omega_{1} \\\\\n&\\beta_{t+1} = \\max(0,\\beta_{t} + \\omega_{2} \\label{eqn-beta})\\\\\n\\mbox{Switching Evolution: }&\\alpha_{t+1} \\sim p(\\alpha_{t+1} |\\alpha_{t},Z_{t})\n\\end{aligned}\n\\] where \\(z_t\\) is an exogenous variable that effects the sensor model, \\(\\mu\\) is an average free flow speed \\[\n\\alpha_t \\in \\{0,1,-1\\}\n\\] \\[\n\\omega = (\\omega_{1}, \\omega_{2})^T \\sim N(0, W), \\ v \\sim N(0,V)\n\\] \\[\nF_{\\alpha_t} = \\left\\{\n\\begin{aligned}\n&1,  \\ \\alpha_t \\in \\{1,-1\\}\\\\\n&F, \\  \\alpha_t = 0\\end{aligned}\n\\right.\n\\] No boundary conditions estimation is needed. No capacity/critical density is needed."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes AI Slides",
    "section": "",
    "text": "Unit 1: Introduction: AI Today and in the Past. Probability and Bayes Rule\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 2: Utility and Decision Theory\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 3: Bayesian Inference with Conjugate Pairs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 4: Bayesian Hypothesis Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 5: Stochastic Processes\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 6: Markov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 7: Bayesian Regression: Linear and Bayesian Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 8: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\n\n\n\n\n\n\n\n\n\n\n\n\n\n9: Bayesian Double Descent and Model Selection: Modern Approach to Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 10: Bayesian Neural Networks and Deep Learning\n\n\n\n\n\n\n\nNo matching items"
  }
]