[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes AI Slides",
    "section": "",
    "text": "Unit 1: Introduction: AI Today and in the Past. Probability and Bayes Rule\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 2: Utility and Decision Theory\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 3: Bayesian Inference with Conjugate Pairs\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 4: Bayesian Hypothesis Tests\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 5: Stochastic Processes\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 6: Markov Chain Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 7: Bayesian Regression: Linear and Bayesian Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 8: Quantile Neural Networks for Reinforcement Learning and Uncertainty Quantification\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 9: Bayesian Double Descent and Model Selection: Modern Approach to Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 10: Bayesian Neural Networks and Deep Learning\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "01-intro.html#random-facts",
    "href": "01-intro.html#random-facts",
    "title": "Bayes AI",
    "section": "Random facts",
    "text": "Random facts\nOn this Day (January 27):\n\n1888: The National Geographic Society is founded in Washington, D.C.\n1945: The Red Army liberates the Auschwitz-Birkenau concentration camp\n1967: The United States, United Kingdom, and Soviet Union sign the Outer Space Treaty in Washington, D.C.\n1973: The Paris Peace Accords officially end the Vietnam War.\n2010: Apple Inc. unveils the iPad."
  },
  {
    "objectID": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "href": "01-intro.html#the-first-thoughts-about-artificial-intelligence",
    "title": "Bayes AI",
    "section": "The first thoughts about artificial intelligence",
    "text": "The first thoughts about artificial intelligence\n\n\n\n\n\n\n\nHephaestus created for himself Android robots, such as a giant human-like robot of Talos.\nPygmalion revived Galatea.\nJehovah and Allah - pieces of clay\nParticularly pious and learned rabbis rabbis could create golems.\nAlbert the Great made an artificial speaking head (which very upset Thomas Aquinas)."
  },
  {
    "objectID": "01-intro.html#mechanical-machines",
    "href": "01-intro.html#mechanical-machines",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nRobots and Automatic Machines Were Generally Very Inventive: Al-Jazari (XII Century)\n\nHesdin Castle (Robert II of Artois), Leonardo’s robot…"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-1",
    "href": "01-intro.html#mechanical-machines-1",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\nJaquet-Droz automata (XVIII century):"
  },
  {
    "objectID": "01-intro.html#mechanical-machines-2",
    "href": "01-intro.html#mechanical-machines-2",
    "title": "Bayes AI",
    "section": "Mechanical machines",
    "text": "Mechanical machines\n\nBut this is in mechanics, in mathematics/logic AI it was quite rudimentary for a long time\n\n\nLogic machine of Ramon Llull (XIII-XIV centuries)\nStarting with Dr. Frankenstein, further AI in the literature appears constantly …"
  },
  {
    "objectID": "01-intro.html#turing-test",
    "href": "01-intro.html#turing-test",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\n\nAI as a science begins with a Turing test (1950).\nThe ides of the Turing test is to check if a machine can imitate a human in a conversation.\nThe original formulation was more nuanced."
  },
  {
    "objectID": "01-intro.html#shennons-theseus",
    "href": "01-intro.html#shennons-theseus",
    "title": "Bayes AI",
    "section": "Shennon’s Theseus",
    "text": "Shennon’s Theseus\n\nYouTube Video\nEarly 1950s, Claude Shannon (The father of Information Theory) demonstrates Theseus\nA life-sized magnetic mouse controlled by relay circuits, learns its way around a maze."
  },
  {
    "objectID": "01-intro.html#stanford-cart",
    "href": "01-intro.html#stanford-cart",
    "title": "Bayes AI",
    "section": "Stanford Cart",
    "text": "Stanford Cart\n\n\nYouTube Video\nTakes 2.6-second for signal to travel from earth to the moon\nLatest iterations is automated with 3D vision capabilities\nPause after each meter of movement and take 10-15 minutes to reassess its surroundings and reevaluate its decided path.\nIn 1979, this cautious version of the cart successfully made its way 20 meters through a chair-strewn room in five hours without human intervention."
  },
  {
    "objectID": "01-intro.html#turing-test-1",
    "href": "01-intro.html#turing-test-1",
    "title": "Bayes AI",
    "section": "Turing Test",
    "text": "Turing Test\nIt takes a lot to create an AI system:\n\nProcessing of a natural language\nSensors and actuators\nRepresentation of knowledge\nInference from the existing knowledge\nTraining on experience (Machine Learning)."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop",
    "href": "01-intro.html#dartmouth-workshop",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n\nAI as a science appeared in 1956 at the Dartmouth workshop.\nIt was organized by John McCarthy, Marvin Minsky, Claude Shennon and Nathaniel Rochester.\nIt was probably the most ambitious grant proposal in the history of computer science."
  },
  {
    "objectID": "01-intro.html#dartmouth-workshop-1",
    "href": "01-intro.html#dartmouth-workshop-1",
    "title": "Bayes AI",
    "section": "Dartmouth workshop",
    "text": "Dartmouth workshop\n We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."
  },
  {
    "objectID": "01-intro.html#great-hopes",
    "href": "01-intro.html#great-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Great hopes",
    "text": "1956-1960: Great hopes\n\nOptimistic time. It seemed a that we were almost there…\nAllen Newell, Herbert A. Simon, and Cliff Shaw: Logic Theorist.\nAutomated reasoning.\nIt was able to prof most of the Principia Mathematica, in some places even more elegant than Russell and Whitehead."
  },
  {
    "objectID": "01-intro.html#big-hopes",
    "href": "01-intro.html#big-hopes",
    "title": "Bayes AI",
    "section": "1956-1960: Big Hopes",
    "text": "1956-1960: Big Hopes\n\nGeneral Problem Solver - a program that tried to think as a person\nA lot of programs that have been able to do some limited things (MicroWorlds):\n\nAnalogy (IQ tests with multiple choice questions)\nStudent (algebraic verbal tasks)\nBlocks World (rearranged 3D blocks)."
  },
  {
    "objectID": "01-intro.html#sknowledge-based-systems",
    "href": "01-intro.html#sknowledge-based-systems",
    "title": "Bayes AI",
    "section": "1970s:Knowledge Based Systems",
    "text": "1970s:Knowledge Based Systems\n\nThe bottom line: to accumulate a fairly large set of rules and knowledge about the subject area, then draw conclusions.\nFirst success: MYCIN - Diagnosis of blood infections:\n\nabout 450 rules\nThe results are like an experienced doctor and significantly better than beginner doctors."
  },
  {
    "objectID": "01-intro.html#commercial-applications-industry-ai",
    "href": "01-intro.html#commercial-applications-industry-ai",
    "title": "Bayes AI",
    "section": "1980-2010: Commercial applications Industry AI",
    "text": "1980-2010: Commercial applications Industry AI\n\nThe first AI department was at Dec (Digital Equipment Corporation). It is argued that by 1986 he saved the Dec about  $ 10 million per year.\nThe boom ended by the end of the 80s, when many companies could not live up to high expectations."
  },
  {
    "objectID": "01-intro.html#data-mining-machine-learning",
    "href": "01-intro.html#data-mining-machine-learning",
    "title": "Bayes AI",
    "section": "1990-2010: DATA MINING, MACHINE LEARNING",
    "text": "1990-2010: DATA MINING, MACHINE LEARNING\n\nIn recent decades, the main emphasis has shifted to machine training and search for patterns in the data.\nEspecially - with the development of the Internet.\nNot too many people remember the original AI ideas, but Machine Learning is now everywhere.\nBut Robotics flourishes and uses Machine Learning at every step."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes",
    "href": "01-intro.html#rule-based-system-vs-bayes",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations\ntraditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments.\nOn the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning.\nAll of those are data-driven approaches.\nThese approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability."
  },
  {
    "objectID": "01-intro.html#rule-based-system-vs-bayes-1",
    "href": "01-intro.html#rule-based-system-vs-bayes-1",
    "title": "Bayes AI",
    "section": "Rule-Based System vs Bayes",
    "text": "Rule-Based System vs Bayes\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n If rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n\n \n\n\n\n\n\nNew AI\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\n\n\nBayesian approach is a powerful statistical framework based on the work of Thomas Bayes and later Laplace.\nIt provides a probabilistic approach to reasoning and learning\nAllowing us to update our beliefs about the world as we gather new data.\nThis makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information."
  },
  {
    "objectID": "01-intro.html#definition",
    "href": "01-intro.html#definition",
    "title": "Bayes AI",
    "section": "DEFINITION",
    "text": "DEFINITION\n\nHow to determine “learning”?\n\n\n\n\nDefinition:\n\n\nThe computer program learns as the data is accumulating relative to a certain problem class \\(T\\) and the target function of \\(P\\) if the quality of solving these problems (relative to \\(P\\)) improves with gaining new experience.\n\n\n\n\nThe definition is very (too?) General.\nWhat specific examples can be given?"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml",
    "href": "01-intro.html#tasks-and-concepts-of-ml",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML",
    "text": "Tasks and concepts of ML"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-supervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: Supervised Learning",
    "text": "Tasks and concepts of ML: Supervised Learning\n\ntraining sample – a set of examples, each of which consists of input features (attributes) and the correct “answers” - the response variable\nLearn a rule that maps input features to the response variable\nThen this rule is applied to new examples (deployment)\nThe main thing is to train a model that explains not only examples from the training set, but also new examples (generalizes)\nOtherwise - overfitting"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\nThere are no correct answers, only data, e.g. clustering:\n\nWe need to divide the data into pre -unknown classes to some extent similar:\n\nhighlight the family of genes from the sequences of nucleotides\ncluster users and personalize the application for them\ncluster the mass spectrometric image to parts with different composition"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "href": "01-intro.html#tasks-and-concepts-of-ml-unsupervised-learning-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: unsupervised learning",
    "text": "Tasks and concepts of ML: unsupervised learning\n\nDimensionality reduction: data have a high dimension, it is necessary to reduce it, select the most informative features so that all of the above algorithms can work\nMatrix Competition: There is a sparse matrix, we must predict what is in the missing positions.\nAnomaly detection: find anomalies in the data, e.g. fraud detection.\nOften the outputs answers are given for a small part of the data, then we call it semi -supervised Learning."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-reinforcement-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: reinforcement learning",
    "text": "Tasks and concepts of ML: reinforcement learning\n\nMulti-armed bandits: there is a certain set of actions, each of which leads to random results, you need to get as much rewards possible\nExploration vs.Exploitation: how and when to proceed from the study of the new to use what has already studied\nCredit Assignment: You get rewarded at the very end (won the game), and we must somehow distribute this reward on all the moves that led to victory."
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "href": "01-intro.html#tasks-and-concepts-of-ml-active-learning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of ML: active learning",
    "text": "Tasks and concepts of ML: active learning\n\nActive Learning - how to choose the following (relatively expensive) test\nBoosting - how to combine several weak classifiers so that it turns out good\nModel Selection - where to draw a line between models with many parameters and with a few.\nRanking: response list is ordered (internet search)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai",
    "href": "01-intro.html#tasks-and-concepts-of-ai",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI",
    "text": "Tasks and concepts of AI"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "href": "01-intro.html#tasks-and-concepts-of-ai-reasoning",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Reasoning",
    "text": "Tasks and concepts of AI: Reasoning\n\nBayesian networks: given conditional probabilities, calculate the probability of the event\no1 by OpenAI: a family of AI models that are designed to perform complex reasoning tasks, such as math, coding, and science. o1 models placed among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME)\nGemini 2.0: model for the agentic era"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-representation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Representation",
    "text": "Tasks and concepts of AI: Representation\n\nKnowledge Graphs: a graph database that uses semantic relationships to represent knowledge\nEmbeddings: a way to represent data in a lower-dimensional space\nTransformers: a deep learning model that uses self-attention to process sequential data"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nIn shadows of data, uncertainty reigns,\nBayesian whispers, where knowledge remains.\nWith prior beliefs, we start our quest,\nUpdating with evidence, we strive for the best.\nA dance of the models, predictions unfold,\nInferences drawn, from the new and the old.\nThrough probabilities, we find our way,\nIn the world of AI, it’s the Bayesian sway.\nSo gather your data, let prior thoughts flow,\nIn the realm of the unknown, let your insights grow.\nFor in this approach, with each little clue,\nWe weave understanding, both rich and true.\nMusic"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-1",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\n\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\")\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"a hockey player trying to understand the Bayes rule\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n)\n\nprint(response.data[0].url)"
  },
  {
    "objectID": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "href": "01-intro.html#tasks-and-concepts-of-ai-generation-2",
    "title": "Bayes AI",
    "section": "Tasks and concepts of AI: Generation",
    "text": "Tasks and concepts of AI: Generation\nA humorous and illustrative scene of a hockey player sitting on a bench in full gear, holding a hockey stick in one hand and a whiteboard marker in th"
  },
  {
    "objectID": "01-intro.html#chess-and-ai",
    "href": "01-intro.html#chess-and-ai",
    "title": "Bayes AI",
    "section": "Chess and AI",
    "text": "Chess and AI\nOld AI: Deep Blue (1997) vs. Garry Kasparov\n\nKasparov vs IBM’s DeepBlue in 1997"
  },
  {
    "objectID": "01-intro.html#alphago-zero",
    "href": "01-intro.html#alphago-zero",
    "title": "Bayes AI",
    "section": "AlphaGo Zero",
    "text": "AlphaGo Zero\n\nRemove all human knowledge from training process - only uses self play,\nTakes raw board as input and neural network predicts the next move.\nUses Monte Carlo tree search to evaluate the position.\nThe algorithm was able to beat AlphaGo 100-0. The algorithm was then used to play chess and shogi and was able to beat the best human players in those games as well.\n\n\nAlpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two"
  },
  {
    "objectID": "01-intro.html#probability-in-machine-learning",
    "href": "01-intro.html#probability-in-machine-learning",
    "title": "Bayes AI",
    "section": "Probability in machine learning",
    "text": "Probability in machine learning\n\nIn all methods and approaches, it is useful not only generate an answer, but also evaluate how confident in this answer, how well the model describes the data, how these values ​​will change in further experiments, etc.\nTherefore, the central role in machine learning is played by the theory of probability - and we will also actively use it."
  },
  {
    "objectID": "01-intro.html#references",
    "href": "01-intro.html#references",
    "title": "Bayes AI",
    "section": "References",
    "text": "References\n\nChristopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2007.\nKevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press, 2013.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., Springer, 2009."
  },
  {
    "objectID": "01-intro.html#probability",
    "href": "01-intro.html#probability",
    "title": "Bayes AI",
    "section": "Probability",
    "text": "Probability\nSubjective Probability (de Finetti, Ramsey, Savage, von Neumann, ... )\nPrinciple of Coherence:\nA set of subjective probability beliefs must avoid sure loss\n\nIf an event \\(A\\) is certain to occur, it has probability 1\nEither an event \\(A\\) occurs or it does not. \\[\nP(A) = 1 - P(\\mbox{not }A)\n\\]\nIf two events are mutually exclusive (both cannot occur simultaneously) then \\[\nP(A \\mbox{ or } B) = P(A) + P(B)\n\\]\nJoint probability, when events are independent \\[\nP(A \\mbox{ and } B) = P( A) P(B)\n\\]"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nUse probability to describe outcomes involving more than one variable at a time. Need to be able to measure what we think will happen to one variable relative to another\nIn general the notation is ...\n\n\\(P(X=x, Y=y )\\) is the joint probability that \\(X =x\\) and \\(Y=y\\)\n\\(P(X=x  \\mid  Y=y )\\) is the conditional probability that \\(X\\) equals \\(x\\) given \\(Y=y\\)\n\\(P(X=x)\\) is the marginal probability of \\(X=x\\)"
  },
  {
    "objectID": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "href": "01-intro.html#conditional-joint-and-marginal-distributions-1",
    "title": "Bayes AI",
    "section": "Conditional, Joint and Marginal Distributions",
    "text": "Conditional, Joint and Marginal Distributions\nRelationship between the joint and conditional ... \\[\n\\begin{aligned}\nP(x,y) & = P(x) P(y \\mid x) \\\\\n& =  P(y) P(x \\mid y)\n\\end{aligned}\n\\]\nRelationship between the joint and marginal ... \\[\n\\begin{aligned}\nP(x) & = \\sum_y P(x,y) \\\\\nP(y) & =  \\sum_x P(x,y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-rule",
    "href": "01-intro.html#bayes-rule",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nThe computation of \\(P(x \\mid y)\\) from \\(P(x)\\) and \\(P(y \\mid x)\\) is called Bayes theorem ... \\[\nP(x \\mid y) = \\frac{P(y,x)}{P(y)} = \\frac{P(y,x)}{\\sum_x P(y,x)} = \\frac{P(y \\mid x)P(x)}{\\sum_x P(y \\mid x)P(x)}\n\\]\nThis shows now the conditional distribution is related to the joint and marginal distributions.\nYou’ll be given all the quantities on the r.h.s."
  },
  {
    "objectID": "01-intro.html#bayes-rule-1",
    "href": "01-intro.html#bayes-rule-1",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nKey fact: \\(P(x \\mid y)\\) is generally different from \\(P(y \\mid x)\\)!\nExample: Most people would agree \\[\n\\begin{aligned}\nPr  & \\left ( Practice \\; hard  \\mid  Play \\; in \\; NBA \\right ) \\approx  1\\\\\nPr  & \\left ( Play \\; in \\; NBA  \\mid  Practice \\; hard  \\right ) \\approx  0\n\\end{aligned}\n\\]\nThe main reason for the difference is that \\(P( Play \\; in \\; NBA ) \\approx 0\\)."
  },
  {
    "objectID": "01-intro.html#independence",
    "href": "01-intro.html#independence",
    "title": "Bayes AI",
    "section": "Independence",
    "text": "Independence\nTwo random variable \\(X\\) and \\(Y\\) are independent if \\[\nP(Y = y  \\mid X = x) = P (Y = y)\n\\] for all possible \\(x\\) and \\(y\\) values. Knowing \\(X=x\\) tells you nothing about \\(Y\\)!\nExample: Tossing a coin twice. What’s the probability of getting \\(H\\) in the second toss given we saw a \\(T\\) in the first one?"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nimageSource: The Secret Betting Strategy That Beats Online Bookmakers"
  },
  {
    "objectID": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "href": "01-intro.html#bookies-vs-betters-the-battle-of-probabilistic-models-1",
    "title": "Bayes AI",
    "section": "Bookies vs Betters: The Battle of Probabilistic Models",
    "text": "Bookies vs Betters: The Battle of Probabilistic Models\n\nBookies set odds that reflect their best guess on probabilities of a win, draw, or loss. Plus their own margin\nBookies have risk aversion bias. When many people bet for an underdog (more popular team)\nBookies hedge their bets by offering more favorable odds to the opposed team\nSimple algorithm: calculate average odds across many bookies and find outliers with large deviation from the mean"
  },
  {
    "objectID": "01-intro.html#odds-oddschecker",
    "href": "01-intro.html#odds-oddschecker",
    "title": "Bayes AI",
    "section": "Odds: Oddschecker",
    "text": "Odds: Oddschecker\nWe can express probabilities in terms of Odds via \\[\nO(A) = \\frac{ 1- P(A) }{ P(A) }\n\\; \\; {\\rm or} \\; \\; P(A) = \\frac{ 1 }{ 1 + O(A) }\n\\]\n\nFor example if \\(O(A) = 1\\) then for ever $1 bet you will payout $1. An event with probability \\(\\frac{1}{2}\\).\nIf \\(O(A) = 2\\) or \\(2:1\\), then for a $1 bet you’ll payback $3.\n\nIn terms of probability \\(P = \\frac{1}{3}\\)."
  },
  {
    "objectID": "01-intro.html#envelope-paradox",
    "href": "01-intro.html#envelope-paradox",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nThe following problem is known as the “exchange paradox”.\n\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent.\nThe amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope."
  },
  {
    "objectID": "01-intro.html#envelope-paradox-1",
    "href": "01-intro.html#envelope-paradox-1",
    "title": "Bayes AI",
    "section": "Envelope Paradox",
    "text": "Envelope Paradox\nYou know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch!"
  },
  {
    "objectID": "01-intro.html#bayes-rule-2",
    "href": "01-intro.html#bayes-rule-2",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nWhere’s the flaw in this argument? Use Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami.\nSuch an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by\n\n\\[\nE(y) =  p \\left ( y = \\frac{1}{2} x \\; \\vert \\;  x \\right ) \\cdot  \\frac{1}{2} x +\np \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E( y) &gt; x\\) becomes a decision rule."
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma",
    "href": "01-intro.html#prisoners-dilemma",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nThree prisoners \\(A , B , C\\).\nEach believe are equally likely to be set free.\nPrisoner \\(A\\) goes to the warden \\(W\\) and asks if s/he is getting axed.\n\nThe Warden can’t tell \\(A\\) anything about him.\nHe provides the new information: \\(WB\\) = “\\(B\\) is to be executed”"
  },
  {
    "objectID": "01-intro.html#prisoners-dilemma-1",
    "href": "01-intro.html#prisoners-dilemma-1",
    "title": "Bayes AI",
    "section": "Prisoner’s Dilemma",
    "text": "Prisoner’s Dilemma\nUniform Prior Probabilities: \\[\n\\begin{array}{c|ccc}\nPrior & A  & B  & C  \\\\\\hline\nP ( {\\rm Pardon} ) & 0.33 & 0.33 & 0.33\n\\end{array}\n\\]\nPosterior: Compute \\(P ( A | WB )\\)?\n What happens if \\(C\\) overhears the conversation?\n Compute \\(P ( C | WB )\\)?"
  },
  {
    "objectID": "01-intro.html#game-show-problem",
    "href": "01-intro.html#game-show-problem",
    "title": "Bayes AI",
    "section": "Game Show Problem",
    "text": "Game Show Problem\nNamed after the host of the long-running TV show, Let’s make a Deal.\n\nA contestant is given the choice of 3 doors.\n\nThere is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats.\n\nThe optimal strategy is counter-intuitive"
  },
  {
    "objectID": "01-intro.html#puzzle",
    "href": "01-intro.html#puzzle",
    "title": "Bayes AI",
    "section": "Puzzle",
    "text": "Puzzle\nThe game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat.\nYou have the choice of switching doors.\n\n Is it advantageous to switch?\n Assume you pick door \\(A\\) at random. Then \\(P(A) = ( 1 /3 )\\).\nYou need to figure out \\(P( A | MB )\\) after Monte reveals \\(B\\) is a goat."
  },
  {
    "objectID": "01-intro.html#bayes-rule-3",
    "href": "01-intro.html#bayes-rule-3",
    "title": "Bayes AI",
    "section": "Bayes Rule",
    "text": "Bayes Rule\nIn its simplest form.\n\nTwo events \\(A\\) and \\(B\\). Bayes rule \\[\nP ( A | B ) = \\frac{P (  A \\cap  B )}{ P ( B )}\n= \\frac{P ( B | A ) P ( A )}{ P ( B )}\n\\]\nLaw of Total Probability \\[\nP ( B ) = P ( B | A ) P ( A ) + P ( B | \\bar{A} ) P ( \\bar{A} )\n\\] Hence we can calculate the denominator of Bayes rule."
  },
  {
    "objectID": "01-intro.html#bayes-theorem",
    "href": "01-intro.html#bayes-theorem",
    "title": "Bayes AI",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nMany problems in decision making can be solved using Bayes rule.\n\nAI: Rule-based decision making.\nIt’s counterintuitive! But gives the “right” answer.\n\nBayes Rule: \\[\n\\mbox{P}(A|B) = \\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} = \\frac{  \\mbox{P}(B|A) \\mbox{P}(A)}{ \\mbox{P}(B)}\n\\] Law of Total Probability: \\[\n\\mbox{P}(B) =  \\mbox{P}(B|A) \\mbox{P}(A ) +  \\mbox{P}(B| \\bar{A} ) \\mbox{P}(\\bar{A} )\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch",
    "href": "01-intro.html#apple-watch",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\nThe Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives).\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4?\nBayes’ Theorem: \\[\nP(A|B)=\\frac{P(B|A)P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#apple-watch-1",
    "href": "01-intro.html#apple-watch-1",
    "title": "Bayes AI",
    "section": "Apple Watch",
    "text": "Apple Watch\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\n\n\n\n\natrial fibrillation\n1960\n980\n\n\nno atrial fibrillation\n40\n97020\n\n\n\n\\[\n0.6667\n=\n\\frac{0.98\\cdot 0.02}{\n0.0294}\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%."
  },
  {
    "objectID": "01-intro.html#abraham-wald",
    "href": "01-intro.html#abraham-wald",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nHow Abraham Wald improved aircraft survivability. Raw Reports from the Field\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n?\n\n\nCockpit\n36\n?\n\n\nFuselage\n105\n?\n\n\nNone\n146\n0\n\n\n\nThis fact would allow Wald to estimate: \\[\nP(\\text{damage on fuselage} \\mid \\text{returns safely}) = 105/316 \\approx 32\\%\n\\] You need the inverse probability : \\[\nP(\\text{returns safely} \\mid \\text{damage on fuselage})\n\\] Completely different!"
  },
  {
    "objectID": "01-intro.html#abraham-wald-1",
    "href": "01-intro.html#abraham-wald-1",
    "title": "Bayes AI",
    "section": "Abraham Wald",
    "text": "Abraham Wald\nImputation: fill-in missing data.\n\n\n\nType of damage suffered\nReturned (316 total)\nShot down (60 total)\n\n\n\n\nEngine\n29\n31\n\n\nCockpit\n36\n21\n\n\nFuselage\n105\n8\n\n\nNone\n146\n0\n\n\n\nThen Wald got: \\[\n\\begin{aligned}\nP(\\text{returns safely} \\mid \\text{damage on fuselage}) & =\\frac{105}{105+8}\\approx 93\\%\\\\\nP(\\text{returns safely} \\mid \\text{damage on engine}) & =\\frac{29}{29+31}\\approx 48\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "01-intro.html#personalization-conditional-probability",
    "href": "01-intro.html#personalization-conditional-probability",
    "title": "Bayes AI",
    "section": "“Personalization\" \\(=\\)”Conditional Probability\"",
    "text": "“Personalization\" \\(=\\)”Conditional Probability\"\n\nConditional probability is how AI systems express judgments in a way that reflects their partial knowledge.\nPersonalization runs on conditional probabilities, all of which must be estimated from massive data sets in which you are the conditioning event.\n\n Many Business Applications!! Suggestions vs Search…."
  },
  {
    "objectID": "01-intro.html#probability-as-evidence",
    "href": "01-intro.html#probability-as-evidence",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nevidence: known facts about criminal (e.g. blood type, DNA, ...)\nsuspect: matches a trait with evidence at scene of crime\nLet \\(G\\) denote the event that the suspect is the criminal.\nBayes computes the conditional probability of guilt\n\\[\nP ( G | {\\rm evidence} )\n\\] Evidence \\(E\\): suspect and criminal possess a common trait"
  },
  {
    "objectID": "01-intro.html#probability-as-evidence-1",
    "href": "01-intro.html#probability-as-evidence-1",
    "title": "Bayes AI",
    "section": "Probability as Evidence",
    "text": "Probability as Evidence\nBayes Theorem yields \\[\nP ( G | {\\rm evidence} )\n= \\frac{ P ( {\\rm evidence} | G ) P ( G ) }{ P ( {\\rm evidence} )}\n\\]\nIn terms of relative odds \\[\n\\frac{ P ( I | {\\rm evidence} ) }{ P ( G | {\\rm evidence} ) }\n= \\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\frac{ P ( I ) }{ P ( G ) }\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factors",
    "href": "01-intro.html#bayes-factors",
    "title": "Bayes AI",
    "section": "Bayes Factors",
    "text": "Bayes Factors\nThere are two terms:\n\nPrior Odds of Guilt \\(O ( G ) = P ( I ) / P ( G )\\) ?\n\nHow many people on the island?\nSensitivity “what if” analysis?\n\nThe Bayes factor \\[\n\\frac{ P ( {\\rm evidence} | I ) }{ P ( {\\rm evidence} | G ) }\n\\] is common to all observers and updates everyone’s initials odds"
  },
  {
    "objectID": "01-intro.html#prosecutors-fallacy",
    "href": "01-intro.html#prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "Prosecutor’s Fallacy",
    "text": "Prosecutor’s Fallacy\nThe most common fallacy is confusing \\[\nP ( {\\rm evidence} | G ) \\; \\; {\\rm with} \\; \\;\nP ( G | {\\rm evidence} )\n\\]\nBayes rule yields \\[\nP ( G | {\\rm evidence} ) = \\frac{ P ( {\\rm evidence} | G ) p( G )}{ P ( {\\rm evidence}  )}\n\\] Your assessment of \\(P( G )\\) will matter."
  },
  {
    "objectID": "01-intro.html#island-problem",
    "href": "01-intro.html#island-problem",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nSuppose there’s a criminal on a island of \\(N+1\\) people.\n\nLet \\(I\\) denote innocence and \\(G\\) guilt.\nEvidence \\(E\\): the suspect matches a trait with the criminal.\nThe probabilities are \\[\np(E|I)=p\\;\\;\\mathrm{and}\\;\\;p(E|G)=1\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-factor",
    "href": "01-intro.html#bayes-factor",
    "title": "Bayes AI",
    "section": "Bayes factor",
    "text": "Bayes factor\nBayes factors are likelihood ratios\n\nThe Bayes factor is given by \\[\n\\frac{p(E|I)}{p(E|G)}=p\n\\]\nIf we start with a uniform prior distribution we have\n\n\\[\np(I)=\\frac{1}{N+1}\\;\\;\\mathrm{and}\\;\\;odds(I)=N\n\\]\n\nPriors will matter!"
  },
  {
    "objectID": "01-intro.html#island-problem-1",
    "href": "01-intro.html#island-problem-1",
    "title": "Bayes AI",
    "section": "Island Problem",
    "text": "Island Problem\nPosterior Probability related to Odds \\[\np(I|y)=\\frac\n{1}{1+odds(I|y)}%\n\\]\n\nProsecutors’ fallacy\n\nThe posterior probability \\(p(I|y)\\neq p(y|I)=p\\).\n\nSuppose that \\(N=10^{3}\\) and \\(p=10^{-3}\\). Then\n\n\\[\np( I|y) = \\frac{1}{1 + 10^3 \\cdot 10^{-3}} = \\frac{1}{2}\n\\]\nThe odds on innocence are \\(odds(I|y)=1\\).\nThere’s a \\(50/50\\) chance that the criminal has been found."
  },
  {
    "objectID": "01-intro.html#sally-clark-case-independence-or-bayes",
    "href": "01-intro.html#sally-clark-case-independence-or-bayes",
    "title": "Bayes AI",
    "section": "Sally Clark Case: Independence or Bayes?",
    "text": "Sally Clark Case: Independence or Bayes?\nSally Clark was accused and convicted of killing her two children\nThey could have both died of SIDS.\n\nThe chance of a family which are non-smokers and over 25 having a SIDS death is around 1 in 8,500.\nThe chance of a family which has already had a SIDS death having a second is around 1 in 100.\nThe chance of a mother killing her two children is around 1 in 1,000,000."
  },
  {
    "objectID": "01-intro.html#bayes-or-independence",
    "href": "01-intro.html#bayes-or-independence",
    "title": "Bayes AI",
    "section": "Bayes or Independence",
    "text": "Bayes or Independence\n\nUnder Bayes \\[\n\\begin{aligned}\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)   &  = P \\left(\n\\mathrm{first} \\; \\mathrm{SIDS} \\right)  P \\left(  \\mathrm{Second} \\; \\;\n\\mathrm{SIDS} | \\mathrm{first} \\; \\mathrm{SIDS} \\right) \\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}\n\\end{aligned}\n\\] The \\(\\frac{1}{100}\\) comes from taking into account genetics.\nIndependence, as the court did, gets you\n\n\\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000)\n\\]\n\nBy Bayes rule\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{P( E \\cap I)}{P( E \\cap G)}\n\\] \\(P( E \\cap I) = P(E|I )P(I)\\) needs discussion of \\(p(I)\\)."
  },
  {
    "objectID": "01-intro.html#comparison",
    "href": "01-intro.html#comparison",
    "title": "Bayes AI",
    "section": "Comparison",
    "text": "Comparison\n\nHence putting these two together gives the odds of guilt as\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1/850,000}{1/1,000,000} = 1.15\n\\] In terms of posterior probabilities\n\\[\np( G|E) = \\frac{1}{1 + O(G|E)} = 0.465\n\\]\n\nIf you use independence\n\n\\[\n\\frac{p(I|E)}{p(G|E)} = \\frac{1}{73} \\; {\\rm and} \\; p( G|E) \\approx 0.99\n\\] The suspect looks guilty."
  },
  {
    "objectID": "01-intro.html#oj-simpson",
    "href": "01-intro.html#oj-simpson",
    "title": "Bayes AI",
    "section": "OJ Simpson",
    "text": "OJ Simpson\nThe O.J. Simpson trial was possibly the trail of the century\nThe murder of his wife Nicole Brown Simpson, and a friend, Ron Goldman, in June 1994 and the trial dominated the TV networks\n\nDNA evidence and probability: \\(p( E| G)\\)\nBayes Theorem: \\(p( G | E )\\)\nProsecutor’s Fallacy: \\(p( G|E ) \\neq p(E|G)\\)\n\nOdds ratio \\[\n\\frac{ p( I|E) }{ p ( G | E ) } = \\frac{ p( E|I )}{ p( E|G) } \\frac{ p(I) }{p(G ) }\n\\] Prior odds conditioned on background information."
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem",
    "href": "01-intro.html#oj-simpson-bayes-theorem",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nSuppose that you are a juror in a murder case of a husband who is accused of killing his wife.\nThe husband is known is have battered her in the past.\nConsider the three events:\n\n\\(G\\) “husband murders wife in a given year”\n\\(M\\) “wife is murdered in a given year”\n\\(B\\) “husband is known to batter his wife”"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-1",
    "href": "01-intro.html#oj-simpson-bayes-theorem-1",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\n\nOnly \\(1/10\\)th of one percent of husbands who batter their wife actually murder them.\n\nConditional on eventually murdering their wife, there a one in ten chance it happens in a given year.\nIn 1994, 5000 women were murdered, 1500 by their husband\nGiven a population of 100 million women at the time \\[\np( M | I ) = \\frac{ 3500 }{ 10^8 } \\approx \\frac{1}{30,000} .\n\\] We’ll also need \\(p( M | I , B )  = p( M | I )\\)"
  },
  {
    "objectID": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "href": "01-intro.html#oj-simpson-prosecutors-fallacy",
    "title": "Bayes AI",
    "section": "OJ Simpson: Prosecutor’s Fallacy",
    "text": "OJ Simpson: Prosecutor’s Fallacy\n\nLet \\(G =\\) Guilt and \\(E=\\) Evidence\nProsecutor’s Fallacy: \\(P(G|E) \\neq P(E|G)\\).\nDNA evidence gives \\(P( E | I )\\) – the \\(p\\)-value.\n\nWhat’s the “match probability” for a rare event?\nBayes theorem in Odds \\[\n\\frac{p(G|M,B)}{p(I|M,B)} = \\frac{p(M|G,B)}{p(M|I,B)} \\frac{p(G|B)}{p(I|B)}\n\\]"
  },
  {
    "objectID": "01-intro.html#oj-simpson-bayes-theorem-2",
    "href": "01-intro.html#oj-simpson-bayes-theorem-2",
    "title": "Bayes AI",
    "section": "OJ Simpson: Bayes Theorem",
    "text": "OJ Simpson: Bayes Theorem\nBy assumption,\n\n\\(p(M|G,B)=1\\)\n\\(p(M|I,B)= \\frac{1}{30,000}\\)\n\\(p( G|B) = \\frac{1}{1000}\\) and so\n\n\\[\n\\frac{p(G|B)}{p(I|B)} = \\frac{1}{999}\n\\]\nTherefore, \\[\n\\frac{p(G|M,B)}{p(I|M,B)} \\approx 30 \\; {\\rm and} \\; p(G|M,B) = \\frac{30}{31} \\approx 97\\%\n\\] More than a 50/50 chance that your spouse murdered you!"
  },
  {
    "objectID": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "href": "01-intro.html#fallacy-p-g-b-neq-p-g-b-m",
    "title": "Bayes AI",
    "section": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)",
    "text": "Fallacy \\(p ( G | B ) \\neq p( G | B , M )\\)\nThe defense stated to the press: in any given year\n“Fewer than 1 in 2000 of batterers go on to murder their wives”.\n\nNow estimate \\(p( M | \\bar{G} , B ) = p( M| \\bar{G} ) = \\frac{1}{20,000}\\).\nThe Bayes factor is then\n\n\\[\n\\frac{ p( G | M , B ) }{ p( \\bar{G} | M , B ) } = \\frac{ 1/999 }{1 /20,000} = 20\n\\] which implies posterior probabilities\n\\[\np( \\bar{G} | M , B ) = \\frac{1}{1+20} \\; {\\rm and} \\; p( G | M , B ) = \\frac{20}{21}\n\\] Hence its over 95% chance that O.J. is guilty based on this information!\nDefense intended this information to exonerate O.J."
  },
  {
    "objectID": "01-intro.html#base-rate-fallacies",
    "href": "01-intro.html#base-rate-fallacies",
    "title": "Bayes AI",
    "section": "Base Rate Fallacies",
    "text": "Base Rate Fallacies\n“Witness” 80 % certain saw a “checker” \\(C\\) taxi in the accident.\n\nWhat’s your \\(P ( C | E )\\) ?\nNeed \\(P ( C )\\). Say \\(P( C ) = 0.2\\) and \\(P( E  | C) = 0.8\\).\nThen your posterior is\n\n\\[\nP ( C | E ) = \\frac{0.8 \\cdot 0.2}{ 0.8 \\cdot 0.2 + 0.2 \\cdot 0.8 } = 0.5\n\\]\nTherefore \\(O ( C ) = 1\\) a 50/50 bet."
  },
  {
    "objectID": "01-intro.html#updating-fallacies",
    "href": "01-intro.html#updating-fallacies",
    "title": "Bayes AI",
    "section": "Updating Fallacies",
    "text": "Updating Fallacies\nMost people don’t update quickly enough in light of new data\nWards Edwards 1960s\nWhen you have a small sample size, Bayes rule still updates probabilities\n\nTwo players: either 70 % A or 30 % A\nObserve \\(A\\) beats \\(B\\) 3 times out of 4.\nWhat’s \\(P ( A = 70 \\% \\; {\\rm player} )\\) ?"
  },
  {
    "objectID": "01-intro.html#conditional-independence",
    "href": "01-intro.html#conditional-independence",
    "title": "Bayes AI",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\nConsider three variables a,b,c\nConditional distribution of a given b and c, is p(a|b,c)\nIf p(a|b,c) does not depend on value of b, we can write p(a|b,c) = p(a|c)\nWe say that a is conditionally independent of b given c\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\np(x_1, x_2, \\ldots, x_n) = p(x_1)p(x_2 \\mid x_1)p(x_3 \\mid x_1, x_2) \\ldots p(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\]\nright hand side can be simplified if some of the variables are conditionally independent"
  },
  {
    "objectID": "01-intro.html#graphical-representation",
    "href": "01-intro.html#graphical-representation",
    "title": "Bayes AI",
    "section": "Graphical Representation",
    "text": "Graphical Representation\nWhen two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\[\np(b\\mid c,a) = p(b\\mid c),~ p(a,b,c) = p(a)p(c\\mid a)p(b\\mid c)\n\\]\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\[\np(a\\mid b,c) = p(a\\mid c), ~ p(a,b,c) = p(a\\mid c)p(b\\mid c)p(c)\n\\]\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\[\np(a\\mid b) = p(a),~ p(a,b,c) = p(c\\mid a,b)p(a)p(b)\n\\]"
  },
  {
    "objectID": "01-intro.html#derived-assumptions",
    "href": "01-intro.html#derived-assumptions",
    "title": "Bayes AI",
    "section": "Derived Assumptions",
    "text": "Derived Assumptions\n\n\n\n\n\n\n\n\n\nLine Structure\n\n\n\n\n\\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nLambda Structure\n\n\n\n\n\\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent.\n\n\n\n\n\n\n\nV-structure\n\n\n\n\n\\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(p(a,b\\mid c)\\) and \\(p(a\\mid c)p(b\\mid c)\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics",
    "href": "01-intro.html#bayes-home-diagnostics",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nAlarm system sends me a text notification when some motion inside my house is detected. - - Prior: during an earthquake alarm is triggered in 10% of the cases.\nI get text message and assess \\(p(b\\mid a)\\) is high and I start driving back home\nWhile driving I hear on the radio about a small earthquake in our area, need to calculate \\(p(b \\mid a,r)\\)\n\n\\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  p(b,e,a,r) = p(r \\mid a,b,e)p(a \\mid b,e)p(b\\mid e)p(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\np(b,e,a,r) = p(r \\mid e)p(a \\mid b,e)p(b)p(e).\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-1",
    "href": "01-intro.html#bayes-home-diagnostics-1",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nThe joint distribution is defined by\n\n\n\n\\(p(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-2",
    "href": "01-intro.html#bayes-home-diagnostics-2",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as Bayesian network.\n\n\nCode\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\n\n\ngraph TB\n    b((b)) --&gt; a((a))\n    e((e)) --&gt; a\n    e --&gt; r((r))\n\n\n\n\nFigure 1: Bayesian network for alarm ."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-3",
    "href": "01-intro.html#bayes-home-diagnostics-3",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nCalculate \\(p(a=0 \\mid b,e)\\), from \\[\np(a=1 \\mid b,e) + p(a=0 \\mid b,e) = 1.\n\\] Also know \\(p(r=1 \\mid e=1) = 0.5\\) and \\(p(r=1 \\mid e=0) = 0\\), \\(p(b) = 2\\cdot10^{-4}\\) and \\(p(e) = 10^{-2}\\) (historical data)\n\nGraph allowed us to have a more compact representation of the joint probability distribution. The original naive representations requires specifying \\(2^4\\) parameters."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-4",
    "href": "01-intro.html#bayes-home-diagnostics-4",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nTo answer our original question, calculate \\[\np(b \\mid a) = \\dfrac{p(a \\mid b)p(b)}{p(a)},~~p(b) = p(a=1 \\mid b=1)p(b=1) + p(a=1 \\mid b=0)p(b=0).\n\\] We have everything but \\(p(a \\mid b)\\). This is obtained by marginalizing \\(p(a=1 \\mid b,e)\\), to yield \\[\np(a \\mid b) = p(a \\mid b,e=1)p(e=1) + p(a \\mid b,e=0)p(e=0).\n\\] We can calculate \\[\np(a=1 \\mid b=1) = 1, ~p(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(p(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\)."
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-5",
    "href": "01-intro.html#bayes-home-diagnostics-5",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\nResult is somewhat counterintuitive.\nWe get such a low probability of burglary because its prior is very low compared to prior probability of an earthquake.\nWhat will happen to posterior if we live in an area with higher crime rates, say \\(p(b) = 10^{-3}\\). \\[\np(a \\mid b) = \\dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}\n\\]"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-6",
    "href": "01-intro.html#bayes-home-diagnostics-6",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\n\n\nCode\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\nFigure 2: Relationship between the prior and posterior"
  },
  {
    "objectID": "01-intro.html#bayes-home-diagnostics-7",
    "href": "01-intro.html#bayes-home-diagnostics-7",
    "title": "Bayes AI",
    "section": "Bayes Home Diagnostics",
    "text": "Bayes Home Diagnostics\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\np(b=1 \\mid a=1,r=1) =  \\dfrac{p(a,r  \\mid  b)p(b)}{p(a,r)}\n\\] and \\[\np(a,r  \\mid  b)p(b) = \\dfrac{\\sum_e p(b=1,e,a=1,r=1)}{\\sum_b\\sum_ep(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_ep(r=1 \\mid e)p(a=1 \\mid b=1,e)p(b=1)p(e)}{\\sum_b\\sum_ep(r=1 \\mid e)p(a=1 \\mid b,e)p(b)p(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact."
  },
  {
    "objectID": "01-intro.html#a-random-image",
    "href": "01-intro.html#a-random-image",
    "title": "Bayes AI",
    "section": "A Random Image",
    "text": "A Random Image"
  }
]