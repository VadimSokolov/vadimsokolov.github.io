<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>22&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./23-cnn.html" rel="next">
<link href="./21-sgd.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="22&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="22-qnn_files/figure-html/quantile-regression-mtcars-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="22&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="22-qnn_files/figure-html/quantile-regression-mtcars-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./22-qnn.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-qnn-generative" id="toc-sec-qnn-generative" class="nav-link active" data-scroll-target="#sec-qnn-generative"><span class="header-section-number">22.1</span> From Densities to Quantiles: A Generative Approach</a></li>
  <li><a href="#sec-qnn-quantile-regression" id="toc-sec-qnn-quantile-regression" class="nav-link" data-scroll-target="#sec-qnn-quantile-regression"><span class="header-section-number">22.2</span> Quantile Regression</a></li>
  <li><a href="#sec-qnn-trend-filtering" id="toc-sec-qnn-trend-filtering" class="nav-link" data-scroll-target="#sec-qnn-trend-filtering"><span class="header-section-number">22.3</span> Nonlinear Quantile Regression via Trend Filtering</a>
  <ul class="collapse">
  <li><a href="#implementation-with-r" id="toc-implementation-with-r" class="nav-link" data-scroll-target="#implementation-with-r">Implementation with R</a></li>
  </ul></li>
  <li><a href="#sec-qnn-bayes-quantiles" id="toc-sec-qnn-bayes-quantiles" class="nav-link" data-scroll-target="#sec-qnn-bayes-quantiles"><span class="header-section-number">22.4</span> Bayes Rule for Quantiles</a>
  <ul class="collapse">
  <li><a href="#conditional-quantile-representation" id="toc-conditional-quantile-representation" class="nav-link" data-scroll-target="#conditional-quantile-representation">Conditional Quantile Representation</a></li>
  </ul></li>
  <li><a href="#sec-qnn-meu" id="toc-sec-qnn-meu" class="nav-link" data-scroll-target="#sec-qnn-meu"><span class="header-section-number">22.5</span> Maximum Expected Utility via Quantile Neural Networks</a>
  <ul class="collapse">
  <li><a href="#the-quantile-expectation-identity" id="toc-the-quantile-expectation-identity" class="nav-link" data-scroll-target="#the-quantile-expectation-identity">The Quantile-Expectation Identity</a></li>
  <li><a href="#implementation-strategy" id="toc-implementation-strategy" class="nav-link" data-scroll-target="#implementation-strategy">Implementation Strategy</a></li>
  <li><a href="#formal-framework" id="toc-formal-framework" class="nav-link" data-scroll-target="#formal-framework">Formal Framework</a></li>
  <li><a href="#example-normal-normal-model-and-wang-distortion" id="toc-example-normal-normal-model-and-wang-distortion" class="nav-link" data-scroll-target="#example-normal-normal-model-and-wang-distortion">Example: Normal-Normal Model and Wang Distortion</a></li>
  </ul></li>
  <li><a href="#sec-qnn-portfolio" id="toc-sec-qnn-portfolio" class="nav-link" data-scroll-target="#sec-qnn-portfolio"><span class="header-section-number">22.6</span> Portfolio Optimization with Quantile Neural Networks</a>
  <ul class="collapse">
  <li><a href="#learning-under-parameter-uncertainty" id="toc-learning-under-parameter-uncertainty" class="nav-link" data-scroll-target="#learning-under-parameter-uncertainty">Learning Under Parameter Uncertainty</a></li>
  <li><a href="#the-quantile-approach" id="toc-the-quantile-approach" class="nav-link" data-scroll-target="#the-quantile-approach">The Quantile Approach</a></li>
  <li><a href="#numerical-example" id="toc-numerical-example" class="nav-link" data-scroll-target="#numerical-example">Numerical Example</a></li>
  </ul></li>
  <li><a href="#sec-qnn-implementation" id="toc-sec-qnn-implementation" class="nav-link" data-scroll-target="#sec-qnn-implementation"><span class="header-section-number">22.7</span> Neural Network Implementation</a>
  <ul class="collapse">
  <li><a href="#wasserstein-distance-and-quantile-loss" id="toc-wasserstein-distance-and-quantile-loss" class="nav-link" data-scroll-target="#wasserstein-distance-and-quantile-loss">Wasserstein Distance and Quantile Loss</a></li>
  <li><a href="#combined-loss-function" id="toc-combined-loss-function" class="nav-link" data-scroll-target="#combined-loss-function">Combined Loss Function</a></li>
  <li><a href="#learning-multiple-quantiles-simultaneously" id="toc-learning-multiple-quantiles-simultaneously" class="nav-link" data-scroll-target="#learning-multiple-quantiles-simultaneously">Learning Multiple Quantiles Simultaneously</a></li>
  <li><a href="#non-crossing-constraints" id="toc-non-crossing-constraints" class="nav-link" data-scroll-target="#non-crossing-constraints">Non-Crossing Constraints</a></li>
  <li><a href="#cosine-embedding-for-tau" id="toc-cosine-embedding-for-tau" class="nav-link" data-scroll-target="#cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></a></li>
  <li><a href="#synthetic-data-example" id="toc-synthetic-data-example" class="nav-link" data-scroll-target="#synthetic-data-example">Synthetic Data Example</a></li>
  </ul></li>
  <li><a href="#sec-qnn-forecasting" id="toc-sec-qnn-forecasting" class="nav-link" data-scroll-target="#sec-qnn-forecasting"><span class="header-section-number">22.8</span> Supply Chain Forecasting at Scale</a>
  <ul class="collapse">
  <li><a href="#the-business-problem" id="toc-the-business-problem" class="nav-link" data-scroll-target="#the-business-problem">The Business Problem</a></li>
  <li><a href="#amazons-deepar-and-quantile-forecasting" id="toc-amazons-deepar-and-quantile-forecasting" class="nav-link" data-scroll-target="#amazons-deepar-and-quantile-forecasting">Amazon’s DeepAR and Quantile Forecasting</a></li>
  <li><a href="#demand-forecasting-setup" id="toc-demand-forecasting-setup" class="nav-link" data-scroll-target="#demand-forecasting-setup">Demand Forecasting Setup</a></li>
  <li><a href="#why-neural-networks" id="toc-why-neural-networks" class="nav-link" data-scroll-target="#why-neural-networks">Why Neural Networks?</a></li>
  <li><a href="#implementation-strategy-1" id="toc-implementation-strategy-1" class="nav-link" data-scroll-target="#implementation-strategy-1">Implementation Strategy</a></li>
  <li><a href="#r-implementation-demand-forecasting-with-quantile-regression" id="toc-r-implementation-demand-forecasting-with-quantile-regression" class="nav-link" data-scroll-target="#r-implementation-demand-forecasting-with-quantile-regression">R Implementation: Demand Forecasting with Quantile Regression</a></li>
  <li><a href="#inventory-optimization" id="toc-inventory-optimization" class="nav-link" data-scroll-target="#inventory-optimization">Inventory Optimization</a></li>
  </ul></li>
  <li><a href="#sec-qnn-rl" id="toc-sec-qnn-rl" class="nav-link" data-scroll-target="#sec-qnn-rl"><span class="header-section-number">22.9</span> Distributional Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#why-distributions-matter-in-rl" id="toc-why-distributions-matter-in-rl" class="nav-link" data-scroll-target="#why-distributions-matter-in-rl">Why Distributions Matter in RL</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  </ul></li>
  <li><a href="#sec-qnn-discussion" id="toc-sec-qnn-discussion" class="nav-link" data-scroll-target="#sec-qnn-discussion"><span class="header-section-number">22.10</span> Discussion and Summary</a>
  <ul class="collapse">
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#when-to-use-quantile-neural-networks" id="toc-when-to-use-quantile-neural-networks" class="nav-link" data-scroll-target="#when-to-use-quantile-neural-networks">When to Use Quantile Neural Networks</a></li>
  <li><a href="#limitations-and-open-questions" id="toc-limitations-and-open-questions" class="nav-link" data-scroll-target="#limitations-and-open-questions">Limitations and Open Questions</a></li>
  <li><a href="#connections-to-broader-ai" id="toc-connections-to-broader-ai" class="nav-link" data-scroll-target="#connections-to-broader-ai">Connections to Broader AI</a></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./22-qnn.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“<em>It is better to be roughly right than precisely wrong.</em>” – John Maynard Keynes</p>
</blockquote>
<p>In <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>, we explored how to learn posterior distributions <span class="math inline">\(p(\theta\mid y)\)</span> using Bayesian learning and to use it for predictions, hypothesis testing, and other tasks. In <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a>, we explored how rational agents make decisions under uncertainty by maximizing expected utility. Traditional Bayesian approaches to such problems require computing posterior distributions <span class="math inline">\(p(\theta\mid y)\)</span>, which in turn requires specifying likelihood functions <span class="math inline">\(p(y\mid \theta)\)</span> and often involves challenging density estimation. But what if we could bypass density estimation entirely and directly learn the quantities we need for decision-making and other tasks?</p>
<p>This chapter introduces <em>quantile neural networks</em>, a powerful approach that learns posterior distributions through their quantile functions rather than their densities. This shift from densities to quantiles has profound implications: it enables likelihood-free inference, provides natural connections to decision theory through the quantile-expectation identity, and scales to high-dimensional problems where density estimation becomes intractable.</p>
<p>The key insight is surprisingly simple. Any expectation—including the expected utility central to decision theory—can be represented as an integral over quantiles: <span class="math display">\[
E[f(\theta)] = \int_0^1 F^{-1}_{f(\theta) | y}(\tau) d\tau
\]</span> Rather than learning densities and then computing expectations via sampling, we can directly learn the quantile function <span class="math inline">\(F^{-1}_{f(\theta) | y}(\tau)\)</span> using neural networks. This approach is not only more efficient but also naturally handles simulation-based models where likelihoods are unavailable or computationally expensive.</p>
<p>Throughout this chapter, we explore four major application domains where quantile neural networks demonstrate their power and versatility. We begin with maximum expected utility problems (<a href="#sec-qnn-meu" class="quarto-xref"><span>Section 22.5</span></a>), showing how to extend generative Bayesian methods to decision problems by incorporating utility functions directly into the training process. This enables efficient computation of optimal decisions without requiring explicit density estimation. Next, we turn to supply chain forecasting (<a href="#sec-qnn-forecasting" class="quarto-xref"><span>Section 22.8</span></a>), examining how companies like Amazon use quantile regression to forecast demand under uncertainty. By predicting entire distributions rather than point estimates, these methods enable more sophisticated inventory optimization. We then demonstrate how quantile methods handle the classic problem of optimal portfolio allocation under parameter uncertainty (<a href="#sec-qnn-portfolio" class="quarto-xref"><span>Section 22.6</span></a>), extending beyond the limited cases where closed-form solutions exist. Finally, we connect to modern artificial intelligence through distributional reinforcement learning (<a href="#sec-qnn-rl" class="quarto-xref"><span>Section 22.9</span></a>), showing how quantile methods enable agents to learn entire distributions of returns rather than just expected values, leading to more robust decision-making in complex environments.</p>
<p>We begin by developing the theoretical foundations of quantile regression, deriving the loss functions from first principles, and then show how neural networks provide a flexible architecture for learning complex quantile functions in high dimensions.</p>
<section id="sec-qnn-generative" class="level2" data-number="22.1">
<h2 data-number="22.1" class="anchored" data-anchor-id="sec-qnn-generative"><span class="header-section-number">22.1</span> From Densities to Quantiles: A Generative Approach</h2>
<p>Let <span class="math inline">\((X,Y) \sim P_{X,Y}\)</span> be input-output pairs and <span class="math inline">\(P_{X,Y}\)</span> a joint measure from which we can simulate a training dataset <span class="math inline">\((x_i, y_i)_{i=1}^N \sim P_{X,Y}\)</span>. Standard prediction techniques focus on the conditional posterior mean <span class="math inline">\(\hat{X}(Y) = E(X|Y) = f(Y)\)</span> of the input given the output. To do this, consider the multivariate non-parametric regression <span class="math inline">\(X = f(Y) + \epsilon\)</span> and provide methods for estimating the conditional mean. Typical estimators, <span class="math inline">\(\hat{f}\)</span>, include KNN and kernel methods. Recently, deep learners have been proposed and the theoretical properties of superpositions of affine functions (a.k.a. ridge functions) have been provided <span class="citation" data-cites="polson2023generative">N. G. Polson and Sokolov (<a href="references.html#ref-polson2023generative" role="doc-biblioref">2023</a>)</span>.</p>
<p>Generative methods take this approach one step further. Let <span class="math inline">\(Z \sim P_Z\)</span> be a base measure for a latent variable, <span class="math inline">\(Z\)</span>, typically a standard multivariate normal or vector of uniforms. The goal of generative methods is to characterize the posterior measure <span class="math inline">\(P_{X|Y}\)</span> from the training data <span class="math inline">\((x_i, y_i)_{i=1}^N \sim P_{X,Y}\)</span> where <span class="math inline">\(N\)</span> is chosen to be suitably large. A deep learner is used to estimate <span class="math inline">\(\hat{f}\)</span> via the non-parametric regression <span class="math inline">\(X = f(Y, Z)\)</span>. In the case where <span class="math inline">\(Z\)</span> is uniform, this amounts to inverse CDF sampling, namely <span class="math inline">\(X = F_{X|Y}^{-1}(Z)\)</span>.</p>
<p>In general, we characterize the posterior map for <em>any</em> output <span class="math inline">\(Y\)</span>. Simply evaluate the network at any <span class="math inline">\(Y\)</span> via the transport map <span class="math display">\[
X = H(S(Y), \psi(Z))
\]</span> Here <span class="math inline">\(\psi\)</span> denotes the embedding function. The deep learners <span class="math inline">\(H\)</span> and <span class="math inline">\(S\)</span> are estimated from the triples <span class="math inline">\((X_i, Y_i, \psi(Z_i))_{i=1}^N \sim P_{X,Y} \times P_Z\)</span>. The ensuing estimator <span class="math inline">\(\hat{H}\)</span> can be thought of as a transport map from the base distribution to the posterior as required.</p>
<p>Specifically, the idea of generative methods is straightforward. Let <span class="math inline">\(y\)</span> denote data and <span class="math inline">\(\theta\)</span> a vector of parameters including any hidden states (a.k.a. latent variables) <span class="math inline">\(z\)</span>. First, we generate a “look-up” table of “fake” data <span class="math inline">\(\{y^{(i)}, \theta^{(i)}\}_{i=1}^N\)</span>. By simulating a training dataset of outputs and parameters, we can use deep learning to solve for the inverse map via a supervised learning problem. Generative methods have the advantage of being likelihood-free. For example, our model might be specified by a forward map <span class="math inline">\(y^{(i)} = f(\theta^{(i)})\)</span> rather than a traditional random draw from a likelihood function <span class="math inline">\(y^{(i)} \sim p(y^{(i)}|\theta^{(i)})\)</span>. The base distribution <span class="math inline">\(P_Z\)</span> is typically uniform (for univariate problems) or a very high-dimensional Gaussian vector (for multivariate problems).</p>
<p>The theoretical foundation for this approach is the <em>noise outsourcing theorem</em>, which guarantees that we can represent any posterior distribution through a deterministic function of the data and a base random variable.</p>
<p><strong>Noise Outsourcing Theorem</strong> <span class="citation" data-cites="kallenberg1997foundations">(<a href="references.html#ref-kallenberg1997foundations" role="doc-biblioref">Kallenberg 1997</a>)</span>: If <span class="math inline">\((Y, \Theta)\)</span> are random variables in a Borel space <span class="math inline">\((\mathcal{Y}, \Theta)\)</span>, then there exists a random variable <span class="math inline">\(\tau \sim U(0,1)\)</span> which is independent of <span class="math inline">\(Y\)</span> and a function <span class="math inline">\(H: [0,1] \times \mathcal{Y} \rightarrow \Theta\)</span> such that <span class="math display">\[
(Y, \Theta) \stackrel{a.s.}{=} (Y, H(Y, \tau))
\]</span> Moreover, if there is a sufficient statistic <span class="math inline">\(S(Y)\)</span> with <span class="math inline">\(Y\)</span> independent of <span class="math inline">\(\Theta | S(Y)\)</span>, then <span class="math display">\[
\Theta\mid Y \stackrel{a.s.}{=} H(S(Y), \tau).
\]</span></p>
<p>This result tells us that posterior uncertainty can be characterized via an inverse non-parametric regression problem where we predict <span class="math inline">\(\theta^{(i)}\)</span> from <span class="math inline">\(y^{(i)}\)</span> and <span class="math inline">\(\tau^{(i)}\)</span>, where <span class="math inline">\(\tau^{(i)}\)</span> is drawn from a base distribution <span class="math inline">\(p(\tau)\)</span>. The base distribution is typically uniform (for univariate problems) or a very high-dimensional Gaussian vector (for multivariate problems). We train a deep neural network <span class="math inline">\(H\)</span> on <span class="math display">\[
\theta^{(i)} = H(S(y^{(i)}), \tau^{(i)}).
\]</span> Here <span class="math inline">\(S(y)\)</span> is a statistic used to perform dimension reduction with respect to the signal distribution—analogous to sufficient statistics in traditional Bayesian inference. A remarkable result due to <span class="citation" data-cites="brillinger2012generalized">Brillinger (<a href="references.html#ref-brillinger2012generalized" role="doc-biblioref">2012</a>)</span> shows that we can learn <span class="math inline">\(S\)</span> independently of <span class="math inline">\(H\)</span> via ordinary least squares, simplifying the overall estimation problem.</p>
<p>Specifying the architecture of <span class="math inline">\(H\)</span> is key to the efficiency of the approach. <span class="citation" data-cites="polson2024generative">N. Polson, Ruggeri, and Sokolov (<a href="references.html#ref-polson2024generative" role="doc-biblioref">2024</a>)</span> propose using quantile neural networks implemented with ReLU activation functions, which we detail in <a href="#sec-qnn-implementation" class="quarto-xref"><span>Section 22.7</span></a>.</p>
</section>
<section id="sec-qnn-quantile-regression" class="level2" data-number="22.2">
<h2 data-number="22.2" class="anchored" data-anchor-id="sec-qnn-quantile-regression"><span class="header-section-number">22.2</span> Quantile Regression</h2>
<p>Before diving into neural network implementations, we present the foundational concepts of quantile regression. This section derives the quantile loss function from first principles and discusses applications across multiple fields, setting the stage.</p>
<p>It is easy to show that, given observed values <span class="math inline">\(y_1, \ldots, y_n\)</span>, the median is the value that minimizes the expected absolute deviation: <span class="math display">\[
m = \arg\min_q \frac{1}{n} \sum_{i=1}^n |y_i - q|   = \arg\min_q E[|Y - q|]
\]</span> Intuitively, the sum of absolute deviations is minimized when the median is the value that is closest to half of the observations.</p>
<p>What if we want to find quantile <span class="math inline">\(\tau \in (0,1)\)</span>? We can use the generalization of the absolute value function to find the minimizer of the expected absolute deviation: <span class="math display">\[
q_\tau = \arg\min_q \frac{1}{n} \sum_{i=1}^n \rho_\tau(y_i - q).
\]</span> Here <span class="math inline">\(\rho_\tau(u)\)</span> is the <em>check loss</em> or <em>pinball loss</em> and is defined as: <span class="math display">\[
\rho_\tau(u) = u(\tau - I(u &lt; 0)) = \begin{cases}
\tau u &amp; \text{if } u \geq 0 \\
(\tau - 1) u &amp; \text{if } u &lt; 0
\end{cases}
\]</span> This can also be written in the more computationally convenient form: <span class="math display">\[
\rho_\tau(u) = \max(u\tau, u(\tau-1))
\]</span></p>
<div id="exm-quantile-regression-r" class="theorem example">
<p><span class="theorem-title"><strong>Example 22.1 (Linear Quantile Regression)</strong></span> To illustrate quantile regression in practice, we’ll analyze the relationship between engine displacement and fuel efficiency using the classic <code>mtcars</code> dataset. Rather than estimating the mean relationship (as ordinary least squares would), we’ll estimate conditional quantiles to understand how this relationship varies across the distribution of fuel efficiency.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load mtcars dataset</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(mtcars)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the check loss (pinball loss) function</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>check_loss <span class="ot">&lt;-</span> <span class="cf">function</span>(u, tau) {</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># rho_tau(u) = u * (tau - I(u &lt; 0))</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  u <span class="sc">*</span> (tau <span class="sc">-</span> (u <span class="sc">&lt;</span> <span class="dv">0</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Objective function: sum of check losses for quantile regression</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>quantile_objective <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, X, y, tau) {</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predicted values</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Residuals</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  residuals <span class="ot">&lt;-</span> y <span class="sc">-</span> y_pred</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sum of check losses</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fu">check_loss</span>(residuals, tau))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">log</span>(mtcars<span class="sc">$</span>disp))  <span class="co"># Design matrix with intercept</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">log</span>(mtcars<span class="sc">$</span>mpg)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit quantile regression models using optim()</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (tau <span class="cf">in</span> quantiles) {</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initial values (use OLS estimates as starting point)</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>  ols_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> <span class="fu">log</span>(disp), <span class="at">data =</span> mtcars)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  beta_init <span class="ot">&lt;-</span> <span class="fu">coef</span>(ols_fit)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Optimize using BFGS (quasi-Newton method)</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># We use BFGS because the check loss is non-differentiable at zero,</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># but BFGS can handle this with numerical approximations</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  optim_result <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> beta_init,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> quantile_objective,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> X,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y,</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="at">tau =</span> tau,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"BFGS"</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  results[[<span class="fu">as.character</span>(tau)]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="at">coefficients =</span> optim_result<span class="sc">$</span>par,</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> optim_result<span class="sc">$</span>value,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="at">convergence =</span> optim_result<span class="sc">$</span>convergence</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"tau = %.1f: Convergence = %d, Loss = %.4f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>              tau, optim_result<span class="sc">$</span>convergence, optim_result<span class="sc">$</span>value))</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## tau = 0.1: Convergence = 0, Loss = 0.5155
## tau = 0.5: Convergence = 0, Loss = 1.5182
## tau = 0.9: Convergence = 0, Loss = 0.6907</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Also fit OLS for comparison</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ols_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(mpg) <span class="sc">~</span> <span class="fu">log</span>(disp), <span class="at">data =</span> mtcars)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(mtcars<span class="sc">$</span>disp), <span class="fu">log</span>(mtcars<span class="sc">$</span>mpg), </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray30"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Displacement (cu.in.)"</span>, </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Miles per Gallon"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Quantile Regression: Fuel Efficiency vs. Engine Displacement"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add regression lines</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>disp_range <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(<span class="fu">log</span>(mtcars<span class="sc">$</span>disp)), <span class="fu">max</span>(<span class="fu">log</span>(mtcars<span class="sc">$</span>disp)), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># OLS line</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(ols_model, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile regression lines</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"darkgreen"</span>, <span class="st">"red"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(quantiles)) {</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  tau <span class="ot">&lt;-</span> quantiles[i]</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> results[[<span class="fu">as.character</span>(tau)]]<span class="sc">$</span>coefficients</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predicted values: y = beta_0 + beta_1 * x</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> beta[<span class="dv">1</span>] <span class="sc">+</span> beta[<span class="dv">2</span>] <span class="sc">*</span> disp_range</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(disp_range, pred, <span class="at">col =</span> colors[i], <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"OLS (Mean)"</span>, </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">expression</span>(tau <span class="sc">==</span> <span class="fl">0.1</span>),</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">expression</span>(tau <span class="sc">==</span> <span class="fl">0.5</span> <span class="sc">~</span> <span class="st">"(Median)"</span>),</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">expression</span>(tau <span class="sc">==</span> <span class="fl">0.9</span>)),</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, colors),</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>       <span class="at">bg =</span> <span class="st">"white"</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="22-qnn_files/figure-html/quantile-regression-mtcars-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption>Quantile regression on mtcars dataset. The relationship between engine displacement and fuel efficiency varies across quantiles, revealing heteroskedasticity in the data.</figcaption>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print model summaries</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n\n</span><span class="st">Quantile Regression Results (via optim):</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## 
## Quantile Regression Results (via optim):</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"=========================================</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## =========================================</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (tau <span class="cf">in</span> quantiles) {</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Quantile tau = %.1f:</span><span class="sc">\n</span><span class="st">"</span>, tau))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Intercept: %.4f</span><span class="sc">\n</span><span class="st">"</span>, results[[<span class="fu">as.character</span>(tau)]]<span class="sc">$</span>coefficients[<span class="dv">1</span>]))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Slope:     %.4f</span><span class="sc">\n</span><span class="st">"</span>, results[[<span class="fu">as.character</span>(tau)]]<span class="sc">$</span>coefficients[<span class="dv">2</span>]))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Quantile tau = 0.1:
##   Intercept: 5.6246
##   Slope:     -0.5340
## 
## Quantile tau = 0.5:
##   Intercept: 5.4783
##   Slope:     -0.4781
## 
## Quantile tau = 0.9:
##   Intercept: 5.0835
##   Slope:     -0.3660</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"OLS Regression:</span><span class="sc">\n</span><span class="st">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## OLS Regression:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Intercept: %.4f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(ols_model)[<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Intercept: 5.3810</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Slope:     %.4f</span><span class="sc">\n</span><span class="st">"</span>, <span class="fu">coef</span>(ols_model)[<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Slope:     -0.4586</code></pre>
</div>
</div>
<p>The quantile regression results reveal several important patterns in the relationship between engine displacement and fuel efficiency. First, the slopes differ substantially across quantiles, indicating heteroskedasticity in the conditional distribution. The 10th percentile slope is -0.5340, while the 90th percentile slope is -0.3660. This difference suggests that the negative relationship between displacement and fuel efficiency is stronger for more fuel-efficient cars.</p>
<p>Second, the widening gap between the 10th and 90th percentiles as displacement increases reveals increasing uncertainty in fuel efficiency for larger engines. This pattern likely reflects varying driving conditions, maintenance practices, and vehicle technology across different cars with similar engine sizes.</p>
<p>Third, the median regression (<span class="math inline">\(\tau\)</span> = 0.5) demonstrates robustness to outliers. Unlike OLS, which minimizes squared errors and thus heavily weights extreme values, quantile regression uses the asymmetric absolute loss that treats positive and negative residuals differently based on the quantile level. This asymmetry makes the estimator less sensitive to unusual observations.</p>
<p>Finally, the check loss <span class="math inline">\(\rho_\tau(u)\)</span> is piecewise linear, making it non-differentiable at zero. This property explains why we use <code>BFGS</code> rather than gradient-based methods that assume smoothness. The <code>BFGS</code> algorithm builds a quasi-Newton approximation that handles the kink effectively, converging reliably despite the non-smooth objective function.</p>
</div>
<p>Traditional quantile regression assumes <span class="math inline">\(f_\tau(x, \theta)\)</span> is linear in parameters. This limitation becomes severe in several important scenarios. First, many real-world relationships are inherently nonlinear—demand forecasting, for instance, involves complex interactions between time, seasonality, promotions, and product features that cannot be captured by linear models. Second, when working with high-dimensional inputs such as image or text data, we need feature learning capabilities that neural networks provide naturally. Third, when estimating multiple quantiles simultaneously, neural networks can learn shared representations across quantiles, substantially improving computational efficiency. Finally, as we demonstrate in <a href="#sec-qnn-meu" class="quarto-xref"><span>Section 22.5</span></a>, neural architectures enable us to incorporate utility functions directly into the learning process, seamlessly integrating prediction with decision-making.</p>
<p>Neural quantile regression addresses these limitations by combining the robustness and interpretability of quantile methods with the flexibility and scalability of deep learning. This synthesis proves particularly valuable in applications where both distributional uncertainty and complex feature interactions matter.</p>
</section>
<section id="sec-qnn-trend-filtering" class="level2" data-number="22.3">
<h2 data-number="22.3" class="anchored" data-anchor-id="sec-qnn-trend-filtering"><span class="header-section-number">22.3</span> Nonlinear Quantile Regression via Trend Filtering</h2>
<p>Before exploring the full power of neural networks for quantile regression, we examine an elegant middle ground: trend filtering combined with quantile loss. This approach, developed by <span class="citation" data-cites="polson2016mixtures">N. G. Polson and Scott (<a href="references.html#ref-polson2016mixtures" role="doc-biblioref">2016</a>)</span>, provides nonlinear function approximation while maintaining computational tractability through a hierarchical representation. Trend filtering estimates smooth, nonlinear functions by penalizing differences in derivatives rather than the function values themselves, making it particularly suitable for data with local smoothness but global complexity.</p>
<p>Consider the nonparametric regression problem where we observe pairs <span class="math inline">\((x_i, y_i)\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span> with <span class="math inline">\(x_1 &lt; x_2 &lt; \ldots &lt; x_n\)</span>. Traditional smoothing methods like cubic splines require choosing knot locations, while kernel smoothing requires bandwidth selection. Trend filtering offers an alternative: estimate a function <span class="math inline">\(f(x)\)</span> by solving</p>
<p><span class="math display">\[
\min_{f} \sum_{i=1}^n \rho_\tau(y_i - f(x_i)) + \lambda \sum_{i=1}^{n-k} |\Delta^k f_i|
\]</span></p>
<p>where <span class="math inline">\(\Delta^k\)</span> denotes the <span class="math inline">\(k\)</span>-th order discrete derivative operator and <span class="math inline">\(\rho_\tau\)</span> is the check loss for quantile <span class="math inline">\(\tau\)</span>. The penalty term controls smoothness: <span class="math inline">\(k=1\)</span> penalizes changes in slope (linear trend filtering), <span class="math inline">\(k=2\)</span> penalizes changes in curvature (quadratic trend filtering), and so on.</p>
<p>For <span class="math inline">\(k=2\)</span>, the penalty becomes <span class="math inline">\(\sum_{i=2}^{n-1} |f_{i+1} - 2f_i + f_{i-1}|\)</span>, which approximates the integrated squared second derivative <span class="math inline">\(\int (f''(x))^2 dx\)</span> used in smoothing splines. However, the <span class="math inline">\(\ell_1\)</span> penalty produces locally adaptive estimates—sharp changes are preserved while smooth regions remain smooth.</p>
<p><span class="citation" data-cites="polson2016mixtures">N. G. Polson and Scott (<a href="references.html#ref-polson2016mixtures" role="doc-biblioref">2016</a>)</span> show that trend filtering admits an elegant hierarchical representation through <em>envelope duality</em>. The key insight is that the <span class="math inline">\(\ell_1\)</span> penalty can be represented as an exponential prior in a hierarchical model. Specifically, for second-order trend filtering with quantile loss, we have the hierarchical model:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;\sim \text{AsymmetricLaplace}(f_i, \tau, \sigma) \\
\Delta^2 f_i &amp;\sim \text{Laplace}(0, 1/\lambda) \quad \text{for } i = 2, \ldots, n-1
\end{aligned}
\]</span></p>
<p>The asymmetric Laplace distribution naturally arises from the check loss—it is the distribution whose maximum likelihood estimator at quantile <span class="math inline">\(\tau\)</span> minimizes <span class="math inline">\(\rho_\tau\)</span>. This connection between optimization (minimizing penalized quantile loss) and probability (maximum a posteriori estimation in a hierarchical model) provides both computational and conceptual advantages.</p>
<p>The hierarchical formulation enables efficient computation through data augmentation schemes. Rather than directly optimizing the non-smooth objective, we introduce auxiliary variables that yield closed-form conditional distributions, leading to straightforward EM or Gibbs sampling algorithms.</p>
<section id="implementation-with-r" class="level3">
<h3 class="anchored" data-anchor-id="implementation-with-r">Implementation with R</h3>
<p>We now demonstrate trend filtering for nonlinear quantile regression using synthetic data with both smooth regions and sharp transitions:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate nonlinear synthetic data with heteroskedasticity</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># True function with different regimes</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>f_true <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(x <span class="sc">&lt;</span> <span class="dv">3</span>, <span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>                <span class="fu">ifelse</span>(x <span class="sc">&lt;</span> <span class="dv">5</span>, <span class="fl">2.5</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> (x <span class="sc">-</span> <span class="dv">3</span>),</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>                       <span class="fl">6.5</span> <span class="sc">+</span> <span class="fl">0.2</span> <span class="sc">*</span> (x <span class="sc">-</span> <span class="dv">5</span>)))</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Heteroskedastic noise that increases with x</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.3</span> <span class="sc">+</span> <span class="fl">0.1</span> <span class="sc">*</span> x</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> f_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Define second-order difference operator</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>D2 <span class="ot">&lt;-</span> <span class="cf">function</span>(n) {</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n <span class="sc">-</span> <span class="dv">2</span>, n)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n <span class="sc">-</span> <span class="dv">2</span>)) {</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    D[i, i<span class="sc">:</span>(i <span class="sc">+</span> <span class="dv">2</span>)] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(D)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Trend filtering objective function for quantile regression</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>trend_filter_obj <span class="ot">&lt;-</span> <span class="cf">function</span>(f, y, lambda, tau, D) {</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check loss</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>  residuals <span class="ot">&lt;-</span> y <span class="sc">-</span> f</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>  quantile_loss <span class="ot">&lt;-</span> <span class="fu">sum</span>(residuals <span class="sc">*</span> (tau <span class="sc">-</span> (residuals <span class="sc">&lt;</span> <span class="dv">0</span>)))</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># L1 penalty on second differences</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>  penalty <span class="ot">&lt;-</span> lambda <span class="sc">*</span> <span class="fu">sum</span>(<span class="fu">abs</span>(D <span class="sc">%*%</span> f))</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(quantile_loss <span class="sc">+</span> penalty)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit trend filtering for multiple quantiles using optim</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="co"># For better performance, we use BFGS with box constraints</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">2.0</span>  <span class="co"># Smoothing parameter</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">D2</span>(n)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>results_tf <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (tau <span class="cf">in</span> quantiles) {</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Initialize with linear quantile regression</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>  X_init <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>  init_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>  f_init <span class="ot">&lt;-</span> <span class="fu">predict</span>(init_fit)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Optimize using L-BFGS-B</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>  opt_result <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>    <span class="at">par =</span> f_init,</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>    <span class="at">fn =</span> trend_filter_obj,</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> y,</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    <span class="at">lambda =</span> lambda,</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>    <span class="at">tau =</span> tau,</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>    <span class="at">D =</span> D,</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">"L-BFGS-B"</span></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>  results_tf[[<span class="fu">as.character</span>(tau)]] <span class="ot">&lt;-</span> opt_result<span class="sc">$</span>par</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Left panel: Data and trend filtering estimates</span></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray50"</span>, <span class="at">cex =</span> <span class="fl">0.8</span>,</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">ylab =</span> <span class="st">"y"</span>,</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Trend Filtering Quantile Regression"</span>)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a><span class="co"># True function</span></span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, f_true, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile estimates</span></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"darkgreen"</span>, <span class="st">"red"</span>)</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(quantiles)) {</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>  tau <span class="ot">&lt;-</span> quantiles[i]</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, results_tf[[<span class="fu">as.character</span>(tau)]], </span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> colors[i], <span class="at">lwd =</span> <span class="fl">2.5</span>)</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"True mean"</span>, <span class="st">"tau = 0.1"</span>, <span class="st">"tau = 0.5"</span>, <span class="st">"tau = 0.9"</span>),</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, colors),</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">2.5</span>, <span class="fl">2.5</span>, <span class="fl">2.5</span>),</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>       <span class="at">bg =</span> <span class="st">"white"</span>)</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Right panel: Uncertainty quantification</span></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray50"</span>, <span class="at">cex =</span> <span class="fl">0.8</span>,</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">ylab =</span> <span class="st">"y"</span>,</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Conditional Quantiles and Prediction Intervals"</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill prediction intervals</span></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(x, <span class="fu">rev</span>(x)),</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(results_tf[[<span class="st">"0.1"</span>]], <span class="fu">rev</span>(results_tf[[<span class="st">"0.9"</span>]])),</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">0.3</span>), <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile curves</span></span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, results_tf[[<span class="st">"0.1"</span>]], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, results_tf[[<span class="st">"0.5"</span>]], <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>)</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, results_tf[[<span class="st">"0.9"</span>]], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"P50 (Median)"</span>, <span class="st">"P10-P90 (80% interval)"</span>),</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"darkgreen"</span>, <span class="fu">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)),</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="dv">8</span>),</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>       <span class="at">bg =</span> <span class="st">"white"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="22-qnn_files/figure-html/trend-filtering-quantile-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Nonlinear quantile regression via trend filtering. The method adapts to both smooth regions and sharp transitions while estimating conditional quantiles.</figcaption>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate empirical coverage</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>coverage <span class="ot">&lt;-</span> <span class="fu">mean</span>(y <span class="sc">&gt;=</span> results_tf[[<span class="st">"0.1"</span>]] <span class="sc">&amp;</span> y <span class="sc">&lt;=</span> results_tf[[<span class="st">"0.9"</span>]])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Empirical 80%% interval coverage: %.1f%%</span><span class="sc">\n</span><span class="st">"</span>, coverage <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Empirical 80% interval coverage: 69.0%</code></pre>
</div>
</div>
<p>Trend filtering for quantile regression exhibits several desirable properties that make it particularly attractive for practical applications, such as computational efficient and flexibility in matching complex patterns.</p>
<p>Trend filtering provides an important conceptual bridge to neural quantile networks. Both approaches learn nonlinear functions through composition: trend filtering composes piecewise polynomials, while neural networks compose nonlinear activation functions. The key difference lies in how they handle high-dimensional inputs. Trend filtering is most effective for univariate or low-dimensional problems with ordered inputs, while neural networks excel when inputs are high-dimensional or lack natural ordering.</p>
<p>For problems with structured, low-dimensional inputs—time series, spatial data along a transect, dose-response curves—trend filtering often provides better interpretability and requires less data than neural networks. For high-dimensional problems—images, text, complex multivariate relationships—neural networks become essential. Understanding both approaches allows practitioners to choose the right tool for their specific problem structure.</p>
</section>
</section>
<section id="sec-qnn-bayes-quantiles" class="level2" data-number="22.4">
<h2 data-number="22.4" class="anchored" data-anchor-id="sec-qnn-bayes-quantiles"><span class="header-section-number">22.4</span> Bayes Rule for Quantiles</h2>
<p>Having established the fundamentals of quantile regression, we now develop the connection to Bayesian inference. <span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> showed that quantile methods provide direct alternatives to density-based Bayesian computations. This section establishes the theoretical foundation for using quantiles to perform Bayesian updating.</p>
<p>Given a cumulative distribution function <span class="math inline">\(F_{\theta|y}(u)\)</span> (non-decreasing, right-continuous), we define the quantile function as: <span class="math display">\[Q_{\theta| y} (u) \defeq  F^{-1}_{\theta|y}  ( u ) = \inf \left \{ \theta : F_{\theta|y} (\theta) \geq u \right \}\]</span></p>
<p>The quantile function is non-decreasing and left-continuous. <span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> established the fundamental probabilistic property: <span class="math display">\[
\theta \stackrel{P}{=} Q_\theta ( F_\theta (\theta ) )
\]</span></p>
<p>This identity enables efficient implementation: we can increase computational efficiency by ordering the samples of <span class="math inline">\(\theta\)</span> and the baseline uniform draws <span class="math inline">\(\tau\)</span>, exploiting the monotonicity of the inverse CDF map.</p>
<p>A crucial property for understanding why quantiles naturally compose (and thus suit deep learning) is the following. Let <span class="math inline">\(g(y)\)</span> be non-decreasing and left-continuous with <span class="math inline">\(g^{-1}(z) = \sup \{ y : g(y) \leq z \}\)</span>. Then the transformed quantile has a compositional nature: <span class="math display">\[Q_{g(Y)}(u) = g(Q(u))\]</span></p>
<p>This composition property shows that quantiles act as superpositions—exactly the structure that deep neural networks learn through their layered architecture.</p>
<section id="conditional-quantile-representation" class="level3">
<h3 class="anchored" data-anchor-id="conditional-quantile-representation">Conditional Quantile Representation</h3>
<p>The connection to Bayesian learning is made explicit through the <em>conditional quantile representation</em>. For the Bayesian learning problem, we have the following result for updating prior to posterior quantiles: <span class="math display">\[Q_{\theta | Y=y}(u) = Q_\theta(s) \quad \text{where} \quad s = Q_{F(\theta) | Y=y}(u)\]</span></p>
<p>To compute <span class="math inline">\(s\)</span>, note that by definition: <span class="math display">\[
u = F_{F(\theta) | Y=y}(s) = P(F(\theta) \leq s | Y=y) = P(\theta \leq Q_\theta(s) | Y=y) = F_{\theta | Y=y}(Q_\theta(s))
\]</span></p>
<p>This result shows that Bayesian updating can be performed entirely in terms of quantile functions, without ever computing or manipulating density functions. The posterior quantile function is obtained by composing the prior quantile function with a learned transformation.</p>
</section>
</section>
<section id="sec-qnn-meu" class="level2" data-number="22.5">
<h2 data-number="22.5" class="anchored" data-anchor-id="sec-qnn-meu"><span class="header-section-number">22.5</span> Maximum Expected Utility via Quantile Neural Networks</h2>
<p>Having established how to learn posterior distributions via quantile neural networks, we now show how to extend this framework to decision problems—the central application where quantile methods truly shine. Recall from <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a> that optimal Bayesian decisions maximize expected utility: <span class="math display">\[
d^\star(y) = \arg \max_d E_{\theta|y}[U(d, \theta)] = \arg \max_d \int U(d, \theta) p(\theta | y) d\theta
\]</span></p>
<p>The naive approach would be to first learn the posterior <span class="math inline">\(p(\theta|y)\)</span>, then use Monte Carlo to approximate the expected utility for each decision <span class="math inline">\(d\)</span>, and finally optimize over <span class="math inline">\(d\)</span>. However, this approach is inefficient for several reasons:</p>
<ol type="1">
<li><p><strong>Computational waste</strong>: Monte Carlo requires many samples in regions of high posterior probability, but utility functions often place high weight on tail events (risk scenarios) that have low posterior probability.</p></li>
<li><p><strong>Density estimation</strong>: We must first estimate the potentially high-dimensional posterior density before we can compute expectations.</p></li>
<li><p><strong>Optimization difficulty</strong>: The expectation must be recomputed for each candidate decision during optimization.</p></li>
</ol>
<p>Quantile neural networks provide a more direct path. The key insight is that we can incorporate the utility function directly into the training process rather than as a post-processing step.</p>
<section id="the-quantile-expectation-identity" class="level3">
<h3 class="anchored" data-anchor-id="the-quantile-expectation-identity">The Quantile-Expectation Identity</h3>
<p>The foundation of our approach is a classical result relating expectations to quantiles. Given any random variable <span class="math inline">\(U\)</span>, its expectation can be computed as an integral over its quantile function: <span class="math display">\[
E[U] = \int_0^1 F^{-1}_{U}(\tau) d\tau
\]</span> This is sometimes called the <em>quantile representation of expectations</em> or the <em>Lorenz curve</em> identity. For decision problems, this means: <span class="math display">\[
E_{\theta|y}[U(d, \theta)] = \int_0^1 F^{-1}_{U|d,y}(\tau) d\tau
\]</span></p>
<p>Rather than learning <span class="math inline">\(p(\theta|y)\)</span> and then computing the expectation, we directly learn the quantile function <span class="math inline">\(F^{-1}_{U|d,y}(\tau)\)</span> of the utility distribution.</p>
</section>
<section id="implementation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="implementation-strategy">Implementation Strategy</h3>
<p>To extend our generative method to MEU problems, we assume that the utility function <span class="math inline">\(U(d, \theta)\)</span> is given (a standard assumption in decision theory). The training procedure is as follows:</p>
<ol type="1">
<li><p><strong>Generate synthetic dataset</strong>: Simulate triples <span class="math inline">\(\{y^{(i)}, \theta^{(i)}, \tau^{(i)}\}_{i=1}^N\)</span> where <span class="math inline">\(y^{(i)} \sim p(y|\theta^{(i)})\)</span>, <span class="math inline">\(\theta^{(i)} \sim p(\theta)\)</span>, and <span class="math inline">\(\tau^{(i)} \sim U(0,1)\)</span>.</p></li>
<li><p><strong>Compute utilities</strong>: For each decision <span class="math inline">\(d\)</span> of interest, compute <span class="math inline">\(U^{(i)}_d \defeq U(d,\theta^{(i)})\)</span>.</p></li>
<li><p><strong>Augment training data</strong>: Create the augmented dataset <span class="math display">\[
\{U_d^{(i)}, S(y^{(i)}), \tau^{(i)}, d\}_{i=1}^N.
\]</span></p></li>
<li><p><strong>Train quantile network</strong>: Learn a neural network <span class="math inline">\(H\)</span> that predicts: <span class="math display">\[
U_d^{(i)} = H(S(y^{(i)}), \tau^{(i)}, d)
\]</span></p></li>
</ol>
<p>Once trained, the network <span class="math inline">\(H\)</span> represents the quantile function <span class="math inline">\(F^{-1}_{U|d,y}(\tau)\)</span>. For any observed data <span class="math inline">\(y\)</span> and candidate decision <span class="math inline">\(d\)</span>, we can:</p>
<ul>
<li><strong>Compute expected utility</strong>: Numerically integrate <span class="math inline">\(\int_0^1 H(S(y), \tau, d) d\tau\)</span></li>
<li><strong>Find optimal decision</strong>: <span class="math display">\[
d^\star(y) = \arg \max_d \int_0^1 H(S(y), \tau, d) d\tau
\]</span></li>
</ul>
<p>This approach has several advantages over naive Monte Carlo:</p>
<ol type="1">
<li>The network learns to focus on regions of the <span class="math inline">\((\theta, \tau)\)</span> space that matter for utility computation.</li>
<li>We avoid explicit density estimation of <span class="math inline">\(p(\theta|y)\)</span>.</li>
<li>The same network handles all decisions <span class="math inline">\(d\)</span> simultaneously if <span class="math inline">\(d\)</span> is included as an input.</li>
<li>The approach naturally handles likelihood-free models where <span class="math inline">\(p(y|\theta)\)</span> is unavailable but we can simulate from the forward model.</li>
</ol>
</section>
<section id="formal-framework" class="level3">
<h3 class="anchored" data-anchor-id="formal-framework">Formal Framework</h3>
<p>For completeness, we provide the formal measure-theoretic framework. Let <span class="math inline">\(\mathcal{Y}\)</span> denote a locally compact metric space of signals <span class="math inline">\(y\)</span> and <span class="math inline">\(\Theta\)</span> a space of parameters <span class="math inline">\(\theta\)</span> (including any latent variables). Let <span class="math inline">\(P(dy|\theta)\)</span> denote the conditional distribution of signals given parameters. Let <span class="math inline">\(\Pi(d\theta|y)\)</span> denote the posterior distribution. In many cases, <span class="math inline">\(\Pi\)</span> is absolutely continuous with density <span class="math inline">\(\pi\)</span>: <span class="math display">\[
\Pi(d\theta|y) = \pi(\theta|y) \mu(d\theta).
\]</span></p>
<p>The framework handles both traditional likelihood-based models where <span class="math inline">\(P(dy|\theta) = p(y|\theta) \lambda(dy)\)</span> and likelihood-free models specified by forward simulators <span class="math inline">\(y = f(\theta)\)</span>. This generality is crucial for modern applications in economics, epidemiology, and climate science where complex simulation models replace closed-form likelihoods.</p>
<p>For multivariate parameters <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>, we can use autoregressive structures to model the sequence of conditional quantiles: <span class="math display">\[
(F^{-1}_{\theta_1}(\tau_1), F^{-1}_{\theta_2|\theta_1}(\tau_2), \ldots, F^{-1}_{\theta_p|\theta_{1:p-1}}(\tau_p))
\]</span> This factorization is analogous to autoregressive density models but operates directly on quantiles, avoiding normalization constraints.</p>
<p>An important architectural choice distinguishes our approach from standard posterior learning followed by Monte Carlo integration: we incorporate the utility function <span class="math inline">\(U(d, \theta)\)</span> directly into the first layer of the network. This allows the network to learn representations optimized for utility computation rather than pure posterior approximation. As utility functions often place high weight on tail events (representing rare but consequential outcomes), this direct incorporation significantly improves efficiency compared to the naive two-step approach.</p>
</section>
<section id="example-normal-normal-model-and-wang-distortion" class="level3">
<h3 class="anchored" data-anchor-id="example-normal-normal-model-and-wang-distortion">Example: Normal-Normal Model and Wang Distortion</h3>
<p>For illustration, we consider the normal-normal conjugate learning model—a case where the quantile updating rule can be derived analytically. This example connects quantile methods to Wang’s risk distortion measure from finance, showing that the distortion function is precisely the transformation that needs to be learned.</p>
<p>Consider the model: <span class="math display">\[
y_1, \ldots, y_n \mid \theta \sim N(\theta, \sigma^2)
\]</span> <span class="math display">\[
\theta \sim N(\mu,\alpha^2)
\]</span></p>
<p>The sufficient statistic is <span class="math inline">\(S(y) = \bar y = \frac{1}{n} \sum_{i=1}^n y_i\)</span>. The posterior is <span class="math inline">\(\theta \mid y \sim N(\mu_*, \sigma_*^2)\)</span> with <span class="math display">\[
\mu_* = \frac{\sigma^2 \mu + n\alpha^2\bar{y}}{t}, \quad \sigma^2_* = \frac{\alpha^2 \sigma^2}{t}
\]</span> where <span class="math inline">\(t = \sigma^2 + n\alpha^2\)</span>.</p>
<p>The remarkable result is that the posterior and prior CDFs are related via a Wang distortion function: <span class="math display">\[
1-\Phi(\theta; \mu_*,\sigma_*) = g(1 - \Phi(\theta; \mu, \alpha^2))
\]</span> where <span class="math inline">\(\Phi(\cdot; \mu, \sigma^2)\)</span> denotes the normal CDF. The Wang distortion is: <span class="math display">\[
g(p) = \Phi\left(\lambda_1 \Phi^{-1}(p) + \lambda\right)
\]</span> with distortion parameters: <span class="math display">\[
\lambda_1 = \frac{\alpha}{\sigma_*}, \quad \lambda = \frac{\alpha\lambda_1(n\bar{y}-n\mu)}{t}
\]</span></p>
<p><strong>Proof sketch</strong>: The key is to rewrite the tail probabilities: <span class="math display">\[
g(1 - \Phi(\theta; \mu, \alpha^2)) = g\left(\Phi\left(-\frac{\theta - \mu}{\alpha}\right)\right) = \Phi\left(\lambda_1 \left(-\frac{\theta - \mu}{\alpha}\right) + \lambda\right)
\]</span> <span class="math display">\[
= 1 - \Phi\left(\frac{\theta - (\mu+ \alpha\lambda/\lambda_1)}{\alpha/\lambda_1}\right) = 1-\Phi(\theta; \mu_*,\sigma_*)
\]</span></p>
<p>This analytical result has several implications:</p>
<ol type="1">
<li><p><strong>Wang distortions</strong> are natural: The distortion functions used in risk management <span class="citation" data-cites="wang1996premium">(<a href="references.html#ref-wang1996premium" role="doc-biblioref">Wang 1996</a>)</span> arise naturally from Bayesian updating in the normal case.</p></li>
<li><p><strong>Learning the distortion</strong>: In more complex models, a neural network can learn this distortion function <span class="math inline">\(g\)</span> directly from data, without requiring conjugacy.</p></li>
<li><p><strong>Computational efficiency</strong>: When the distortion is smooth and well-behaved, neural networks with relatively few parameters can accurately represent it.</p></li>
</ol>
<p><strong>Numerical Example</strong>: Consider prior <span class="math inline">\(\theta \sim N(0,5)\)</span> and data from <span class="math inline">\(y_i \sim N(3,10)\)</span> with <span class="math inline">\(n=100\)</span> observations. The posterior is <span class="math inline">\(\theta \mid y \sim N(3.28, 0.98)\)</span>.</p>
<div id="fig-wang" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Model for simulated data
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang2.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Distortion Function <span class="math inline">\(g\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang1.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Survival Functions 1 - <span class="math inline">\(\Phi\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22.1: The Normal-Normal learning model. Left: Prior, likelihood, and posterior densities. Center: The Wang distortion function <span class="math inline">\(g\)</span> that transforms the prior CDF to the posterior CDF. Right: Survival functions showing how <span class="math inline">\(g\)</span> maps the prior tail probabilities to posterior tail probabilities.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-qnn-portfolio" class="level2" data-number="22.6">
<h2 data-number="22.6" class="anchored" data-anchor-id="sec-qnn-portfolio"><span class="header-section-number">22.6</span> Portfolio Optimization with Quantile Neural Networks</h2>
<p>A classic application of decision theory in finance is portfolio optimization. Consider an investor allocating wealth between a risk-free asset with return <span class="math inline">\(r_f\)</span> and a risky asset with uncertain return <span class="math inline">\(R\)</span>. Let <span class="math inline">\(\omega \in (0,1)\)</span> denote the fraction allocated to the risky asset. The portfolio return is: <span class="math display">\[
W = (1-\omega)r_f + \omega R
\]</span></p>
<p>With exponential utility <span class="math inline">\(U(W) = -e^{-\gamma W}\)</span> (measuring risk aversion <span class="math inline">\(\gamma &gt; 0\)</span>) and normally distributed returns <span class="math inline">\(R \sim N(\mu, \sigma^2)\)</span>, the expected utility has a closed form due to the moment-generating function of the normal distribution: <span class="math display">\[
U(\omega) = E[-e^{-\gamma W}] = -\exp\left\{-\gamma \left[(1-\omega)r_f + \omega\mu\right] + \frac{1}{2}\gamma^2\omega^2\sigma^2 \right\}
\]</span></p>
<p>Maximizing expected utility yields the celebrated <em>Kelly-Merton</em> optimal allocation: <span class="math display">\[
\omega^* = \frac{\mu - r_f}{\gamma\sigma^2}
\]</span></p>
<p>This formula has a simple interpretation: invest more when expected excess return <span class="math inline">\((\mu - r_f)\)</span> is high, less when risk <span class="math inline">\((\sigma^2)\)</span> or risk aversion <span class="math inline">\((\gamma)\)</span> is high.</p>
<section id="learning-under-parameter-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="learning-under-parameter-uncertainty">Learning Under Parameter Uncertainty</h3>
<p>The closed-form solution assumes <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are known. In practice, these parameters must be estimated from historical data, introducing parameter uncertainty. Traditional approaches use plug-in estimates <span class="math inline">\(\hat{\mu}\)</span>, <span class="math inline">\(\hat{\sigma}^2\)</span>, ignoring estimation error. A Bayesian approach would specify priors <span class="math inline">\(p(\mu, \sigma^2)\)</span>, observe returns <span class="math inline">\(\{R_t\}_{t=1}^T\)</span>, and compute the posterior <span class="math inline">\(p(\mu, \sigma^2 | \{R_t\})\)</span>.</p>
<p>The optimal decision under parameter uncertainty integrates over the posterior: <span class="math display">\[
\omega^* = \arg\max_\omega E_{\mu, \sigma^2 | \{R_t\}}[U(\omega, \mu, \sigma^2)]
\]</span></p>
<p>For conjugate priors (normal-inverse-gamma), this can be computed analytically. For more complex models—non-normal returns, time-varying volatility, multi-asset portfolios—analytical solutions are unavailable. This is where quantile neural networks excel.</p>
</section>
<section id="the-quantile-approach" class="level3">
<h3 class="anchored" data-anchor-id="the-quantile-approach">The Quantile Approach</h3>
<p>Rather than integrating expected utility via Monte Carlo sampling from <span class="math inline">\(p(\mu, \sigma^2 | \{R_t\})\)</span>, we use the quantile-expectation identity. The utility distribution <span class="math inline">\(U(\omega) = -e^{-\gamma W}\)</span> can be integrated via its quantile function: <span class="math display">\[
E[U(W)] = \int_{0}^{1}F_{U(W)}^{-1}(\tau)d\tau
\]</span></p>
<p>The quantile neural network learns <span class="math inline">\(F_{U(W)}^{-1}(\tau, \omega, \{R_t\})\)</span> directly from simulated data. Once trained, we can:</p>
<ol type="1">
<li>Compute expected utility for any <span class="math inline">\(\omega\)</span> by numerical integration over <span class="math inline">\(\tau \in (0,1)\)</span></li>
<li>Optimize <span class="math inline">\(\omega\)</span> by evaluating the network on a grid or using gradient-based optimization</li>
<li>Handle non-normal returns, transaction costs, and constraints naturally</li>
</ol>
</section>
<section id="numerical-example" class="level3">
<h3 class="anchored" data-anchor-id="numerical-example">Numerical Example</h3>
<p>Consider the parameter values: - Risk-free rate: <span class="math inline">\(r_f = 0.05\)</span> - Expected return: <span class="math inline">\(\mu = 0.15\)</span> - Volatility: <span class="math inline">\(\sigma = 0.25\)</span> - Risk aversion: <span class="math inline">\(\gamma = 2\)</span></p>
<p>The closed-form optimal allocation is: <span class="math display">\[
\omega^* = \frac{0.15 - 0.05}{2 \times 0.25^2} = \frac{0.10}{0.125} = 0.80
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fl">0.15</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Portfolio utility for different weights</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>utility_quantile <span class="ot">&lt;-</span> <span class="cf">function</span>(tau, omega, rf, mu, sigma, gamma) {</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  W_quantile <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> omega) <span class="sc">*</span> rf <span class="sc">+</span> omega <span class="sc">*</span> (mu <span class="sc">+</span> sigma <span class="sc">*</span> <span class="fu">qnorm</span>(tau))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">exp</span>(<span class="sc">-</span>gamma <span class="sc">*</span> W_quantile)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(U)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected utility via quantile integration</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>expected_utility <span class="ot">&lt;-</span> <span class="cf">function</span>(omega, rf, mu, sigma, gamma, <span class="at">n_quantiles =</span> <span class="dv">1000</span>) {</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>  tau_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.999</span>, <span class="at">length.out =</span> n_quantiles)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>  U_quantiles <span class="ot">&lt;-</span> <span class="fu">sapply</span>(tau_grid, utility_quantile, <span class="at">omega =</span> omega, </span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>                       <span class="at">rf =</span> rf, <span class="at">mu =</span> mu, <span class="at">sigma =</span> sigma, <span class="at">gamma =</span> gamma)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Integrate using trapezoidal rule</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">mean</span>(U_quantiles))</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute expected utility for a grid of portfolio weights</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>omega_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>EU_grid <span class="ot">&lt;-</span> <span class="fu">sapply</span>(omega_grid, expected_utility, <span class="at">rf =</span> rf, <span class="at">mu =</span> mu, </span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>                 <span class="at">sigma =</span> sigma, <span class="at">gamma =</span> gamma)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal weight</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>omega_opt <span class="ot">&lt;-</span> omega_grid[<span class="fu">which.max</span>(EU_grid)]</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Quantile functions for different weights</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>tau_plot <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tau_plot, <span class="fu">utility_quantile</span>(tau_plot, <span class="fl">0.2</span>, rf, mu, sigma, gamma), </span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">col =</span> <span class="st">'blue'</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(tau), <span class="at">ylab =</span> <span class="fu">expression</span>(F[U]<span class="sc">^</span>{<span class="sc">-</span><span class="dv">1</span>}(tau)),</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Utility Quantile Functions"</span>)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(tau_plot, <span class="fu">utility_quantile</span>(tau_plot, <span class="fl">0.5</span>, rf, mu, sigma, gamma), </span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">'green'</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(tau_plot, <span class="fu">utility_quantile</span>(tau_plot, <span class="fl">0.8</span>, rf, mu, sigma, gamma), </span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"omega = 0.2"</span>, <span class="st">"omega = 0.5"</span>, <span class="st">"omega = 0.8"</span>),</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"green"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Expected utility as function of portfolio weight</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(omega_grid, EU_grid, <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">'darkblue'</span>,</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(omega), <span class="at">ylab =</span> <span class="st">"Expected Utility"</span>,</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Portfolio Optimization"</span>)</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> omega_opt, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(omega_opt <span class="sc">+</span> <span class="fl">0.1</span>, <span class="fu">max</span>(EU_grid) <span class="sc">*</span> <span class="fl">0.95</span>, </span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>     <span class="fu">paste0</span>(<span class="st">"omega* = "</span>, <span class="fu">round</span>(omega_opt, <span class="dv">2</span>)), <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="22-qnn_files/figure-html/portfolio-quantile-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Portfolio optimization using quantile representation. Left: Quantile function of utility for different portfolio weights. Right: Expected utility as a function of portfolio weight, with maximum at omega* = 0.80.</figcaption>
</figure>
</div>
</div>
</div>
<p>The quantile approach recovers the analytical optimum <span class="math inline">\(\omega^* = 0.80\)</span>. The left panel shows how utility quantiles shift with portfolio weight: more aggressive allocations (<span class="math inline">\(\omega\)</span> closer to 1) increase both upside and downside risk. The right panel shows expected utility as a function of <span class="math inline">\(\omega\)</span>, maximized at the Kelly-Merton allocation.</p>
<p>In the generative framework with parameter uncertainty <span class="math inline">\(p(\mu, \sigma^2 | \{R_t\})\)</span>, we would train a quantile NN on triples <span class="math inline">\((\omega, \tau, \{R_t\}, U)\)</span> where <span class="math inline">\((\mu, \sigma^2)\)</span> are drawn from the posterior and <span class="math inline">\(U\)</span> is the resulting utility. The trained network then provides expected utilities for any observed return history, enabling real-time portfolio rebalancing.</p>
</section>
</section>
<section id="sec-qnn-implementation" class="level2" data-number="22.7">
<h2 data-number="22.7" class="anchored" data-anchor-id="sec-qnn-implementation"><span class="header-section-number">22.7</span> Neural Network Implementation</h2>
<p>Having established the theory, we now detail how to implement quantile neural networks in practice. The key components are: (1) an appropriate loss function, (2) a neural architecture that handles the quantile input <span class="math inline">\(\tau\)</span>, and (3) training strategies for learning multiple quantiles simultaneously.</p>
<section id="wasserstein-distance-and-quantile-loss" class="level3">
<h3 class="anchored" data-anchor-id="wasserstein-distance-and-quantile-loss">Wasserstein Distance and Quantile Loss</h3>
<p>The 1-Wasserstein distance (also known as <em>earth mover’s distance</em>) provides theoretical justification for quantile methods. For two distributions with quantile functions <span class="math inline">\(F^{-1}_U\)</span> and <span class="math inline">\(F^{-1}_V\)</span>, the 1-Wasserstein distance is: <span class="math display">\[
W_1(F^{-1}_U, F^{-1}_V) = \int_0^1 |F^{-1}_U(\tau) - F^{-1}_V(\tau)| d\tau
\]</span></p>
<p>This distance can be computed efficiently using order statistics <span class="citation" data-cites="levina2001earth">(<a href="references.html#ref-levina2001earth" role="doc-biblioref">Levina and Bickel 2001</a>)</span>. The success of Wasserstein GANs <span class="citation" data-cites="arjovsky2017wasserstein">(<a href="references.html#ref-arjovsky2017wasserstein" role="doc-biblioref">Arjovsky, Chintala, and Bottou 2017</a>)</span> over vanilla GANs stems partly from this improved metric—Wasserstein distance provides meaningful gradients even when distributions have non-overlapping supports.</p>
<p>The key insight is that minimizing the quantile loss is equivalent to minimizing the 1-Wasserstein distance. For a target quantile <span class="math inline">\(q_\tau = F^{-1}_U(\tau)\)</span>, the check loss <span class="math inline">\(\rho_\tau\)</span> we derived in <a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 22.2</span></a> minimizes the expected prediction error: <span class="math display">\[
q_\tau = \arg\min_q E_U[\rho_{\tau}(U-q)]
\]</span></p>
</section>
<section id="combined-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="combined-loss-function">Combined Loss Function</h3>
<p>For training neural networks, we use a combination of quantile loss and mean-squared error (MSE). Given training data <span class="math inline">\(\{x_i, y_i\}_{i=1}^N\)</span> and a quantile <span class="math inline">\(\tau\)</span>, the loss is: <span class="math display">\[
L_{\tau}(\theta) = \sum_{i=1}^N \rho_{\tau}(y_i - f(\tau, x_i, \theta))
\]</span></p>
<p>Empirically, adding an MSE term improves stability and predictive accuracy: <span class="math display">\[
L(\theta) = \alpha L_{\tau}(\theta) + (1-\alpha) \cdot \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i, \theta))^2
\]</span></p>
<p>The weighting parameter <span class="math inline">\(\alpha \in [0,1]\)</span> balances quantile accuracy against overall fit. Typical values are <span class="math inline">\(\alpha \in [0.7, 0.9]\)</span>. The MSE term encourages the median prediction (<span class="math inline">\(\tau = 0.5\)</span>) to align with the conditional mean, which often improves generalization.</p>
</section>
<section id="learning-multiple-quantiles-simultaneously" class="level3">
<h3 class="anchored" data-anchor-id="learning-multiple-quantiles-simultaneously">Learning Multiple Quantiles Simultaneously</h3>
<p>Rather than training separate networks for each quantile <span class="math inline">\(\tau_k\)</span>, it is more efficient to learn all quantiles with a single network that takes <span class="math inline">\(\tau\)</span> as an input. Given quantiles <span class="math inline">\(0 &lt; \tau_1 &lt; \tau_2 &lt; \ldots &lt; \tau_K &lt; 1\)</span>, we minimize: <span class="math display">\[
L(\theta) = \frac{1}{NK} \sum_{i=1}^N \sum_{k=1}^K \rho_{\tau_k}(y_i - f_{\tau_k}(x_i, \theta))
\]</span></p>
<p>This approach has several advantages:</p>
<ol type="1">
<li><strong>Shared representations</strong>: The network learns features useful across all quantiles, improving sample efficiency.</li>
<li><strong>Enforcing monotonicity</strong>: A single network makes it easier to ensure quantiles don’t cross.</li>
<li><strong>Smooth quantile function</strong>: Interpolation between trained quantiles is more reliable.</li>
</ol>
</section>
<section id="non-crossing-constraints" class="level3">
<h3 class="anchored" data-anchor-id="non-crossing-constraints">Non-Crossing Constraints</h3>
<p>A valid distribution function must satisfy <span class="math inline">\(F^{-1}(\tau_i) \leq F^{-1}(\tau_j)\)</span> for <span class="math inline">\(\tau_i &lt; \tau_j\)</span>. Without explicit constraints, neural networks may learn quantile functions that cross: <span class="math display">\[
f_{\tau_i}(x, \theta) &gt; f_{\tau_j}(x, \theta) \quad \text{for some } x, \text{ despite } \tau_i &lt; \tau_j
\]</span></p>
<p>Several approaches address this <span class="citation" data-cites="chernozhukov2010quantile cannon2018noncrossing">(<a href="references.html#ref-chernozhukov2010quantile" role="doc-biblioref">Chernozhukov, Fernández-Val, and Galichon 2010</a>; <a href="references.html#ref-cannon2018noncrossing" role="doc-biblioref">Cannon 2018</a>)</span>:</p>
<ol type="1">
<li><strong>Soft penalties</strong>: Add a term to the loss penalizing violations.</li>
<li><strong>Monotonic networks</strong>: Design architectures that guarantee monotonicity in <span class="math inline">\(\tau\)</span> (e.g., using monotonic activation functions or cumulative link structures).</li>
<li><strong>Post-processing</strong>: After training, rearrange predictions to enforce monotonicity.</li>
</ol>
<p>For the implementations in this chapter, we use soft penalties during training combined with post-processing for final predictions.</p>
</section>
<section id="cosine-embedding-for-tau" class="level3">
<h3 class="anchored" data-anchor-id="cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></h3>
<p>A key architectural choice is how to incorporate the quantile level <span class="math inline">\(\tau \in (0,1)\)</span> as an input to the network. Simply concatenating <span class="math inline">\(\tau\)</span> as an additional feature works but is inefficient—the network must learn the entire relationship between <span class="math inline">\(\tau\)</span> and the output from scratch.</p>
<p>A more effective approach uses a <em>cosine embedding</em> to represent <span class="math inline">\(\tau\)</span> in a higher-dimensional feature space. This leverages Fourier analysis: smooth functions can be well-approximated by cosine bases. The quantile function <span class="math inline">\(F^{-1}(\tau, x)\)</span> is typically smooth in <span class="math inline">\(\tau\)</span>, making Fourier representations natural.</p>
<p>We represent the quantile network as: <span class="math display">\[
F^{-1}(\tau, x) = f_\theta(\tau, x) = g(\psi(x) \circ \phi(\tau))
\]</span> where <span class="math inline">\(\circ\)</span> denotes element-wise multiplication (Hadamard product), <span class="math inline">\(g\)</span> and <span class="math inline">\(\psi\)</span> are feed-forward networks, and <span class="math inline">\(\phi\)</span> is the cosine embedding: <span class="math display">\[
\phi_j(\tau) = \mathrm{ReLU}\left(\sum_{i=0}^{n-1} \cos(\pi i \tau) w_{ij} + b_j\right)
\]</span></p>
<p>The cosine embedding <span class="math inline">\(\phi(\tau)\)</span> transforms the scalar <span class="math inline">\(\tau\)</span> into a vector of dimension <span class="math inline">\(m\)</span>, where <span class="math inline">\(n\)</span> controls the frequency resolution. This embedding has several advantages:</p>
<ol type="1">
<li><strong>Smooth interpolation</strong>: The cosine basis ensures smooth quantile functions.</li>
<li><strong>Universal approximation</strong>: <span class="citation" data-cites="barron1993universal">Barron (<a href="references.html#ref-barron1993universal" role="doc-biblioref">1993</a>)</span> showed that cosine-embedded networks achieve approximation rates of <span class="math inline">\(O(N^{-1/2})\)</span> for sufficiently smooth functions.</li>
<li><strong>Parameter efficiency</strong>: The embedding significantly reduces the number of parameters needed compared to learning the <span class="math inline">\(\tau\)</span> dependence from scratch.</li>
</ol>
<p>This architecture was successfully applied to distributional reinforcement learning by <span class="citation" data-cites="dabney2018implicit">Dabney et al. (<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">2018</a>)</span>, where it enabled agents to learn entire distributions of returns rather than just expectations. We use the same principle here for Bayesian posterior quantiles.</p>
</section>
<section id="synthetic-data-example" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-data-example">Synthetic Data Example</h3>
<p>To validate our approach before applying it to real data, we first test on synthetic data where the true quantile function is known. This allows us to assess both accuracy and the quality of uncertainty quantification.</p>
<p>Consider synthetic data generated from the model: <span class="math display">\[
x \sim U(-1, 1), \quad y | x \sim N\left(\frac{\sin(\pi x)}{\pi x}, \frac{\exp(1-x)}{10}\right)
\]</span></p>
<p>This model has heteroskedastic noise—the variance increases as <span class="math inline">\(x\)</span> decreases. The conditional mean is the sinc function <span class="math inline">\(\text{sinc}(x) = \sin(\pi x)/(\pi x)\)</span>, which has interesting non-linear behavior near zero.</p>
<p>The true conditional <span class="math inline">\(\tau\)</span>-quantile function is: <span class="math display">\[
f_{\tau}(x) = \frac{\sin(\pi x)}{\pi x} + \Phi^{-1}(\tau) \sqrt{\frac{\exp(1-x)}{10}}
\]</span></p>
<p>We train two types of quantile networks:</p>
<ol type="1">
<li><strong>Implicit network</strong>: Uses cosine embedding for <span class="math inline">\(\tau\)</span>, trained on random <span class="math inline">\((\tau, x, y)\)</span> triples</li>
<li><strong>Explicit network</strong>: Trains separate outputs for fixed quantiles <span class="math inline">\(\tau \in \{0.05, 0.5, 0.95\}\)</span></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/synthetic.svg" class="img-fluid figure-img"></p>
<figcaption>Quantile neural network predictions on synthetic data. Both implicit and explicit architectures recover the true quantile functions accurately. The shaded region shows the 90% prediction interval (<span class="math inline">\(\tau = 0.05\)</span> to <span class="math inline">\(\tau = 0.95\)</span>).</figcaption>
</figure>
</div>
<p>The figure shows both networks recover the true quantiles accurately. The implicit network provides smooth interpolation across all <span class="math inline">\(\tau\)</span> values, while the explicit network gives predictions only at the three trained quantiles. For applications requiring full distributional predictions, the implicit approach is preferable despite slightly higher computational cost during training.</p>
</section>
</section>
<section id="sec-qnn-forecasting" class="level2" data-number="22.8">
<h2 data-number="22.8" class="anchored" data-anchor-id="sec-qnn-forecasting"><span class="header-section-number">22.8</span> Supply Chain Forecasting at Scale</h2>
<p>Having developed the theory and core implementations, we now turn to a major industrial application: probabilistic forecasting for inventory management. This case study illustrates how quantile neural networks scale to millions of products while providing the uncertainty quantification essential for optimal decision-making.</p>
<section id="the-business-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-business-problem">The Business Problem</h3>
<p>Amazon, Walmart, and other large retailers face a fundamental challenge: for millions of products across thousands of warehouses, how much inventory should be kept in stock? Too little inventory leads to stockouts and lost sales; too much ties up capital and risks obsolescence. The optimal inventory level depends critically on future demand, which is inherently uncertain.</p>
<p>Traditional forecasting provides point estimates—a single expected demand value. But optimal inventory decisions require understanding the entire demand distribution:</p>
<ul>
<li><strong>Safety stock</strong> (<span class="math inline">\(P_{10}\)</span>): Conservative estimate—ensures 90% of demand scenarios are covered, preventing stockouts</li>
<li><strong>Base stock</strong> (<span class="math inline">\(P_{50}\)</span>): Median demand—balances inventory holding costs against stockout risk</li>
<li><strong>Capacity planning</strong> (<span class="math inline">\(P_{90}\)</span>): Optimistic scenario—ensures warehouse and logistics capacity for high-demand periods</li>
</ul>
<p>The asymmetry matters enormously. For a highly profitable product with long lead times, the cost of stockouts (lost sales, customer dissatisfaction) far exceeds the cost of overstock. The optimal policy may target <span class="math inline">\(P_{70}\)</span> or <span class="math inline">\(P_{80}\)</span>. For perishable goods or fast-moving consumer products with low margins, the reverse holds—target <span class="math inline">\(P_{30}\)</span> or <span class="math inline">\(P_{40}\)</span> to minimize waste.</p>
</section>
<section id="amazons-deepar-and-quantile-forecasting" class="level3">
<h3 class="anchored" data-anchor-id="amazons-deepar-and-quantile-forecasting">Amazon’s DeepAR and Quantile Forecasting</h3>
<p>Amazon developed DeepAR <span class="citation" data-cites="salinas2019deepar">(<a href="references.html#ref-salinas2019deepar" role="doc-biblioref">Salinas, Flunkert, and Gasthaus 2019</a>)</span>, a deep learning approach for probabilistic time series forecasting. DeepAR uses recurrent neural networks (specifically LSTMs) to model temporal dependencies, but its key innovation is forecasting entire probability distributions rather than point estimates.</p>
<p>The model predicts parameters of a parametric distribution (e.g., mean and variance of a Gaussian or negative binomial). To obtain quantiles, one samples from this learned distribution. However, parametric assumptions can be restrictive. Amazon later extended the approach to directly forecast quantiles using the check loss functions we developed in <a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 22.2</span></a>.</p>
<p>The quantile approach has several advantages for demand forecasting:</p>
<ol type="1">
<li><p><strong>Robustness</strong>: Demand data often contains outliers (promotional events, viral products). Quantile regression is robust to these.</p></li>
<li><p><strong>Intermittent demand</strong>: Many products have sparse, intermittent demand (many zeros). Parametric distributions struggle; quantile methods handle this naturally.</p></li>
<li><p><strong>Asymmetric costs</strong>: Different quantiles inform different decisions. The <span class="math inline">\(P_{10}\)</span> quantile is more relevant for safety stock than the mean.</p></li>
<li><p><strong>Model flexibility</strong>: Neural quantile regression makes no distributional assumptions beyond smoothness.</p></li>
</ol>
</section>
<section id="demand-forecasting-setup" class="level3">
<h3 class="anchored" data-anchor-id="demand-forecasting-setup">Demand Forecasting Setup</h3>
<p>Consider forecasting demand <span class="math inline">\(y_t\)</span> for a product given:</p>
<ul>
<li><strong>Temporal features</strong>: Day of week, month, holiday indicators, days since launch</li>
<li><strong>Lagged demand</strong>: <span class="math inline">\(y_{t-1}, y_{t-7}, y_{t-28}\)</span> (yesterday, last week, last month)</li>
<li><strong>Product features</strong>: Category, price, promotional flags</li>
<li><strong>External covariates</strong>: Weather, events, competitor prices</li>
</ul>
<p>The model predicts conditional quantiles: <span class="math display">\[
q_\tau(t) = F^{-1}_{Y_t | X_t}(\tau)
\]</span></p>
<p>where <span class="math inline">\(X_t\)</span> represents all available features at time <span class="math inline">\(t\)</span>. Training data consists of historical demand <span class="math inline">\((y_t, x_t)\)</span> for <span class="math inline">\(t = 1, \ldots, T\)</span> across many products.</p>
</section>
<section id="why-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="why-neural-networks">Why Neural Networks?</h3>
<p>Traditional quantile regression assumes linear relationships. For demand forecasting, this is inadequate:</p>
<ul>
<li><strong>Non-linear seasonality</strong>: Holiday effects interact with day-of-week patterns non-linearly</li>
<li><strong>Product interactions</strong>: Substitution and complementarity effects between products</li>
<li><strong>Promotional dynamics</strong>: Price elasticity varies by product category, time, and competitive environment</li>
<li><strong>Cross-product learning</strong>: Products with similar characteristics exhibit similar demand patterns; neural networks can learn shared representations</li>
</ul>
<p>A quantile neural network can:</p>
<ul>
<li>Learn shared embeddings for product categories</li>
<li>Capture complex temporal patterns through recurrent or attention layers</li>
<li>Model interactions between features automatically</li>
<li>Transfer learning from data-rich products to new products with limited history</li>
</ul>
</section>
<section id="implementation-strategy-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-strategy-1">Implementation Strategy</h3>
<p>For a retailer with millions of SKUs (stock-keeping units), computational efficiency is critical. The architecture typically involves:</p>
<ol type="1">
<li><strong>Embedding layers</strong>: Map categorical variables (product ID, category, location) to dense vectors</li>
<li><strong>Temporal encoder</strong>: LSTM or Transformer to process time series history</li>
<li><strong>Feature fusion</strong>: Combine embeddings with numerical features</li>
<li><strong>Quantile head</strong>: Final layer produces <span class="math inline">\(\hat{q}_\tau(t)\)</span> for input <span class="math inline">\(\tau\)</span>, using cosine embedding</li>
</ol>
<p>Training uses mini-batches sampling randomly across products and time periods, with the combined quantile + MSE loss we discussed. The model is trained to predict multiple quantiles simultaneously (<span class="math inline">\(\tau \in \{0.1, 0.2, \ldots, 0.9\}\)</span>).</p>
<p>At inference time, for a given product and time period, the model outputs all quantiles instantly, enabling inventory optimization algorithms to compute optimal stock levels.</p>
</section>
<section id="r-implementation-demand-forecasting-with-quantile-regression" class="level3">
<h3 class="anchored" data-anchor-id="r-implementation-demand-forecasting-with-quantile-regression">R Implementation: Demand Forecasting with Quantile Regression</h3>
<p>We now provide a complete R implementation for demand forecasting using quantile neural networks. This example simulates realistic demand data with seasonality, trends, and promotional effects, then trains a quantile model to forecast the entire demand distribution.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quantreg)  <span class="co"># For quantile regression</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic demand data for multiple products</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>generate_demand_data <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n_days =</span> <span class="dv">365</span> <span class="sc">*</span> <span class="dv">2</span>, <span class="at">n_products =</span> <span class="dv">5</span>) {</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  days <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>n_days</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create data frame</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>  data_list <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n_products, <span class="cf">function</span>(p) {</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Base parameters varying by product</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    base_demand <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="sc">+</span> p <span class="sc">*</span> <span class="dv">20</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    trend <span class="ot">&lt;-</span> <span class="fl">0.05</span> <span class="sc">*</span> p</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Seasonal components</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    weekly_pattern <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> days <span class="sc">/</span> <span class="dv">7</span> <span class="sc">+</span> p)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    monthly_pattern <span class="ot">&lt;-</span> <span class="dv">15</span> <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> days <span class="sc">/</span> <span class="fl">30.5</span> <span class="sc">+</span> p <span class="sc">*</span> <span class="fl">0.5</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Promotional effect (random promotions)</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    promo <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_days, <span class="dv">1</span>, <span class="fl">0.1</span>)  <span class="co"># 10% of days have promotions</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    promo_effect <span class="ot">&lt;-</span> promo <span class="sc">*</span> (<span class="dv">30</span> <span class="sc">+</span> <span class="dv">20</span> <span class="sc">*</span> p)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Trend + seasonality + promotions + noise</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> base_demand <span class="sc">+</span> trend <span class="sc">*</span> days <span class="sc">+</span> weekly_pattern <span class="sc">+</span> monthly_pattern <span class="sc">+</span> promo_effect</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Heteroskedastic noise (variance increases with demand)</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.1</span> <span class="sc">*</span> mu</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    demand <span class="ot">&lt;-</span> <span class="fu">pmax</span>(<span class="dv">0</span>, <span class="fu">rnorm</span>(n_days, mu, sigma))  <span class="co"># Demand can't be negative</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>      <span class="at">product =</span> <span class="fu">paste0</span>(<span class="st">"Product_"</span>, p),</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>      <span class="at">day =</span> days,</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>      <span class="at">demand =</span> demand,</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>      <span class="at">promo =</span> promo,</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>      <span class="at">day_of_week =</span> (days <span class="sc">%%</span> <span class="dv">7</span>) <span class="sc">+</span> <span class="dv">1</span>,</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>      <span class="at">day_of_month =</span> ((days <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%%</span> <span class="dv">30</span>) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>  <span class="fu">do.call</span>(rbind, data_list)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>demand_data <span class="ot">&lt;-</span> <span class="fu">generate_demand_data</span>(<span class="at">n_days =</span> <span class="dv">365</span> <span class="sc">*</span> <span class="dv">2</span>, <span class="at">n_products =</span> <span class="dv">3</span>)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Create lagged features</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>demand_data <span class="ot">&lt;-</span> demand_data <span class="sc">%&gt;%</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(product) <span class="sc">%&gt;%</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(day) <span class="sc">%&gt;%</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    <span class="at">lag_1 =</span> <span class="fu">lag</span>(demand, <span class="dv">1</span>, <span class="at">default =</span> <span class="cn">NA</span>),</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>    <span class="at">lag_7 =</span> <span class="fu">lag</span>(demand, <span class="dv">7</span>, <span class="at">default =</span> <span class="cn">NA</span>),</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    <span class="at">lag_30 =</span> <span class="fu">lag</span>(demand, <span class="dv">30</span>, <span class="at">default =</span> <span class="cn">NA</span>)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(lag_30))  <span class="co"># Remove rows with NA lags</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Train/test split</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>train_days <span class="ot">&lt;-</span> <span class="fu">max</span>(demand_data<span class="sc">$</span>day) <span class="sc">-</span> <span class="dv">90</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> demand_data <span class="sc">%&gt;%</span> <span class="fu">filter</span>(day <span class="sc">&lt;=</span> train_days)</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> demand_data <span class="sc">%&gt;%</span> <span class="fu">filter</span>(day <span class="sc">&gt;</span> train_days)</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile regression for multiple quantiles</span></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>)</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Training quantile regression models...</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Training quantile regression models...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> quantiles) {</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Quantile %.2f...</span><span class="sc">\n</span><span class="st">"</span>, q))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  models[[<span class="fu">as.character</span>(q)]] <span class="ot">&lt;-</span> <span class="fu">rq</span>(</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    demand <span class="sc">~</span> product <span class="sc">+</span> day <span class="sc">+</span> promo <span class="sc">+</span> day_of_week <span class="sc">+</span> day_of_month <span class="sc">+</span> </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>             lag_1 <span class="sc">+</span> lag_7 <span class="sc">+</span> lag_30,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">tau =</span> q,</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> train_data</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Quantile 0.10...
##   Quantile 0.30...
##   Quantile 0.50...
##   Quantile 0.70...
##   Quantile 0.90...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predictions for test set</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> test_data</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (q <span class="cf">in</span> quantiles) {</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  col_name <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"q_"</span>, <span class="fu">gsub</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">."</span>, <span class="st">""</span>, <span class="fu">sprintf</span>(<span class="st">"%.2f"</span>, q)))</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  predictions[[col_name]] <span class="ot">&lt;-</span> <span class="fu">predict</span>(models[[<span class="fu">as.character</span>(q)]], <span class="at">newdata =</span> test_data)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction intervals and coverage</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> predictions <span class="sc">%&gt;%</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">in_80_interval =</span> (demand <span class="sc">&gt;=</span> q_010 <span class="sc">&amp;</span> demand <span class="sc">&lt;=</span> q_090),</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">in_60_interval =</span> (demand <span class="sc">&gt;=</span> q_030 <span class="sc">&amp;</span> demand <span class="sc">&lt;=</span> q_070)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>coverage_80 <span class="ot">&lt;-</span> <span class="fu">mean</span>(predictions<span class="sc">$</span>in_80_interval, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>coverage_60 <span class="ot">&lt;-</span> <span class="fu">mean</span>(predictions<span class="sc">$</span>in_60_interval, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>mae_median <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(predictions<span class="sc">$</span>demand <span class="sc">-</span> predictions<span class="sc">$</span>q_050), <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Forecast Performance:</span><span class="sc">\n</span><span class="st">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Forecast Performance:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  80%% interval coverage: %.1f%% (target: 80%%)</span><span class="sc">\n</span><span class="st">"</span>, coverage_80 <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   80% interval coverage: 79.6% (target: 80%)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  60%% interval coverage: %.1f%% (target: 60%%)</span><span class="sc">\n</span><span class="st">"</span>, coverage_60 <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   60% interval coverage: 33.7% (target: 60%)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Median absolute error: %.2f units</span><span class="sc">\n</span><span class="st">"</span>, mae_median))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Median absolute error: 21.94 units</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 1: Training data for one product</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>product_1_train <span class="ot">&lt;-</span> train_data <span class="sc">%&gt;%</span> <span class="fu">filter</span>(product <span class="sc">==</span> <span class="st">"Product_1"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(product_1_train<span class="sc">$</span>day, product_1_train<span class="sc">$</span>demand, </span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>, <span class="at">col =</span> <span class="st">'darkblue'</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Day"</span>, <span class="at">ylab =</span> <span class="st">"Demand"</span>,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Historical Demand (Product 1)"</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(product_1_train<span class="sc">$</span>day[product_1_train<span class="sc">$</span>promo <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>       product_1_train<span class="sc">$</span>demand[product_1_train<span class="sc">$</span>promo <span class="sc">==</span> <span class="dv">1</span>],</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Demand"</span>, <span class="st">"Promotion"</span>),</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"darkblue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="cn">NA</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">16</span>))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot 2: Forecasts with quantiles (fan chart)</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>product_1_test <span class="ot">&lt;-</span> predictions <span class="sc">%&gt;%</span> <span class="fu">filter</span>(product <span class="sc">==</span> <span class="st">"Product_1"</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(product_1_test<span class="sc">$</span>day, product_1_test<span class="sc">$</span>demand,</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">'l'</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">'black'</span>,</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Day"</span>, <span class="at">ylab =</span> <span class="st">"Demand"</span>,</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Demand Forecasts with Quantiles (Product 1)"</span>,</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">c</span>(product_1_test<span class="sc">$</span>q_010, product_1_test<span class="sc">$</span>q_090, product_1_test<span class="sc">$</span>demand)))</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Add prediction intervals</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(product_1_test<span class="sc">$</span>day, <span class="fu">rev</span>(product_1_test<span class="sc">$</span>day)),</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(product_1_test<span class="sc">$</span>q_010, <span class="fu">rev</span>(product_1_test<span class="sc">$</span>q_090)),</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="fl">0.7</span>, <span class="fl">0.7</span>, <span class="dv">1</span>, <span class="fl">0.3</span>), <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(product_1_test<span class="sc">$</span>day, <span class="fu">rev</span>(product_1_test<span class="sc">$</span>day)),</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        <span class="fu">c</span>(product_1_test<span class="sc">$</span>q_030, <span class="fu">rev</span>(product_1_test<span class="sc">$</span>q_070)),</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">0.4</span>), <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Add quantile lines</span></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(product_1_test<span class="sc">$</span>day, product_1_test<span class="sc">$</span>q_010, <span class="at">col =</span> <span class="st">'blue'</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(product_1_test<span class="sc">$</span>day, product_1_test<span class="sc">$</span>q_050, <span class="at">col =</span> <span class="st">'darkblue'</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(product_1_test<span class="sc">$</span>day, product_1_test<span class="sc">$</span>q_090, <span class="at">col =</span> <span class="st">'blue'</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Actual demand</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(product_1_test<span class="sc">$</span>day, product_1_test<span class="sc">$</span>demand, <span class="at">col =</span> <span class="st">'black'</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, </span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Actual"</span>, <span class="st">"P50 (Median)"</span>, <span class="st">"P10-P90 (80%)"</span>, <span class="st">"P30-P70 (60%)"</span>),</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"darkblue"</span>, <span class="fu">rgb</span>(<span class="fl">0.7</span>, <span class="fl">0.7</span>, <span class="dv">1</span>), <span class="fu">rgb</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)),</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">8</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="22-qnn_files/figure-html/amazon-demand-forecast-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Demand forecasting with quantile neural networks. Top: Historical demand with trend and seasonality. Bottom: Out-of-sample forecasts showing P10, P50, P90 quantiles (fan chart).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="inventory-optimization" class="level3">
<h3 class="anchored" data-anchor-id="inventory-optimization">Inventory Optimization</h3>
<p>Given the quantile forecasts, we can compute optimal inventory levels. For a newsvendor problem with underage cost <span class="math inline">\(c_u\)</span> (cost of stockout) and overage cost <span class="math inline">\(c_o\)</span> (cost of excess inventory), the optimal quantile to stock is: <span class="math display">\[
\tau^* = \frac{c_u}{c_u + c_o}
\]</span></p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: High-margin product with stockout cost &gt;&gt; overstock cost</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>c_u <span class="ot">&lt;-</span> <span class="dv">50</span>  <span class="co"># Lost profit from stockout</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>c_o <span class="ot">&lt;-</span> <span class="dv">5</span>   <span class="co"># Cost of excess unit (storage, markdown, disposal)</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>optimal_tau <span class="ot">&lt;-</span> c_u <span class="sc">/</span> (c_u <span class="sc">+</span> c_o)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Inventory Optimization Example:</span><span class="sc">\n</span><span class="st">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Inventory Optimization Example:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Underage cost (stockout): $%.2f</span><span class="sc">\n</span><span class="st">"</span>, c_u))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Underage cost (stockout): $50.00</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Overage cost (overstock): $%.2f</span><span class="sc">\n</span><span class="st">"</span>, c_o))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Overage cost (overstock): $5.00</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Optimal service level (tau): %.1f%%</span><span class="sc">\n</span><span class="st">"</span>, optimal_tau <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Optimal service level (tau): 90.9%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For low-margin perishable product</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>c_u_perishable <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>c_o_perishable <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>optimal_tau_perishable <span class="ot">&lt;-</span> c_u_perishable <span class="sc">/</span> (c_u_perishable <span class="sc">+</span> c_o_perishable)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Perishable Product:</span><span class="sc">\n</span><span class="st">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Perishable Product:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Optimal service level (tau): %.1f%%</span><span class="sc">\n</span><span class="st">"</span>, optimal_tau_perishable <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Optimal service level (tau): 20.0%</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Stock at lower quantile to minimize waste</span><span class="sc">\n</span><span class="st">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>##   Stock at lower quantile to minimize waste</code></pre>
</div>
</div>
<p>This implementation demonstrates the key principles:</p>
<ol type="1">
<li><strong>Quantile regression</strong> captures the full demand distribution, not just the mean</li>
<li><strong>Multiple quantiles</strong> provide prediction intervals essential for risk management</li>
<li><strong>Lagged features</strong> and <strong>seasonality</strong> are naturally incorporated</li>
<li><strong>Decision-making</strong> (optimal inventory levels) follows directly from quantile forecasts</li>
</ol>
<p>For production systems handling millions of SKUs, this approach scales by using neural networks (e.g., via <code>keras</code> or <code>torch</code> in R) with embeddings for categorical features and recurrent layers for temporal dependencies.</p>
</section>
</section>
<section id="sec-qnn-rl" class="level2" data-number="22.9">
<h2 data-number="22.9" class="anchored" data-anchor-id="sec-qnn-rl"><span class="header-section-number">22.9</span> Distributional Reinforcement Learning</h2>
<p>Quantile neural networks have also revolutionized reinforcement learning (RL). Recall from <a href="09-rl.html" class="quarto-xref"><span>Chapter 9</span></a> that RL agents learn policies <span class="math inline">\(\pi\)</span> that maximize expected cumulative reward. Traditional RL algorithms like Q-learning estimate the expected value function <span class="math inline">\(Q^\pi(s,a) = E[R | s, a, \pi]\)</span>—the expected return from taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span> thereafter.</p>
<p><em>Distributional reinforcement learning</em> <span class="citation" data-cites="bellemare2017distributional">(<a href="references.html#ref-bellemare2017distributional" role="doc-biblioref">Bellemare, Dabney, and Munos 2017</a>)</span> extends this by learning the entire distribution of returns rather than just the expectation. Using quantile neural networks, agents learn <span class="math inline">\(F^{-1}_{R|s,a}(\tau)\)</span>, the quantile function of returns.</p>
<section id="why-distributions-matter-in-rl" class="level3">
<h3 class="anchored" data-anchor-id="why-distributions-matter-in-rl">Why Distributions Matter in RL</h3>
<p>Learning return distributions provides several advantages:</p>
<ol type="1">
<li><p><strong>Risk-sensitive policies</strong>: Different quantiles inform different behaviors. Conservative agents might maximize <span class="math inline">\(P_{10}\)</span> (avoid worst-case scenarios), while risk-seeking agents target <span class="math inline">\(P_{90}\)</span> (optimize for best-case outcomes).</p></li>
<li><p><strong>Better learning signals</strong>: The Bellman operator naturally contracts in Wasserstein distance when formulated for quantiles, leading to more stable learning <span class="citation" data-cites="dabney2018implicit">(<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">Dabney et al. 2018</a>)</span>.</p></li>
<li><p><strong>Uncertainty quantification</strong>: Understanding return variance helps agents explore intelligently—high uncertainty indicates potential for learning.</p></li>
</ol>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p><span class="citation" data-cites="dabney2018implicit">Dabney et al. (<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">2018</a>)</span> use quantile neural networks for distributional Q-learning. The key insight is that expectations can be computed as integrals over quantiles (the Lorenz curve identity): <span class="math display">\[
E[R] = \int_{-\infty}^{\infty} r dF(r) = \int_0^1 F^{-1}(u) du
\]</span></p>
<p>The distributional RL algorithm finds the optimal policy: <span class="math display">\[
\pi^\star(s) = \arg\max_a \int_0^1 F^{-1}_{R|s,a}(\tau) d\tau = \arg\max_a E_{Z \sim z(s, a)}[Z]
\]</span></p>
<p>The network is trained using the quantile loss <span class="math inline">\(\rho_\tau\)</span> we developed, and Q-learning updates can be applied since the quantile projection operator preserves the contraction property of the Bellman operator. This approach has achieved state-of-the-art performance on Atari games and robotic control tasks.</p>
<p>Connections to dual utility theory <span class="citation" data-cites="yaari1987dual">(<a href="references.html#ref-yaari1987dual" role="doc-biblioref">Yaari 1987</a>)</span> suggest that distributional RL naturally incorporates risk preferences—agents can be trained to maximize any utility functional, not just expectations.</p>
</section>
</section>
<section id="sec-qnn-discussion" class="level2" data-number="22.10">
<h2 data-number="22.10" class="anchored" data-anchor-id="sec-qnn-discussion"><span class="header-section-number">22.10</span> Discussion and Summary</h2>
<p>This chapter developed quantile neural networks as a powerful framework for decision-making under uncertainty. We have shown how learning posterior distributions through their quantile functions—rather than densities—provides computational and conceptual advantages across diverse applications.</p>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li><p><strong>Quantile-expectation identity</strong>: The fundamental insight <span class="math inline">\(E[U] = \int_0^1 F^{-1}_U(\tau) d\tau\)</span> enables direct learning of expected utilities without intermediate density estimation.</p></li>
<li><p><strong>Loss function derivation</strong>: The check loss <span class="math inline">\(\rho_\tau(u) = u(\tau - I(u &lt; 0))\)</span> minimizes Wasserstein distance and naturally handles asymmetric costs.</p></li>
<li><p><strong>Neural architectures</strong>: Cosine embeddings for <span class="math inline">\(\tau\)</span> leverage Fourier approximation theory, providing efficient universal approximators with <span class="math inline">\(O(N^{-1/2})\)</span> convergence rates.</p></li>
<li><p><strong>Applications span domains</strong>:</p>
<ul>
<li><strong>Finance</strong>: Portfolio optimization under parameter uncertainty (<a href="#sec-qnn-portfolio" class="quarto-xref"><span>Section 22.6</span></a>)</li>
<li><strong>Supply chain</strong>: Demand forecasting for inventory management (<a href="#sec-qnn-forecasting" class="quarto-xref"><span>Section 22.8</span></a>)</li>
<li><strong>Bayesian inference</strong>: Posterior quantile learning via Wang distortions (<a href="#sec-qnn-bayes-quantiles" class="quarto-xref"><span>Section 22.4</span></a>)</li>
<li><strong>Reinforcement learning</strong>: Distributional Q-learning for risk-sensitive policies (<a href="#sec-qnn-rl" class="quarto-xref"><span>Section 22.9</span></a>)</li>
</ul></li>
</ol>
</section>
<section id="when-to-use-quantile-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-quantile-neural-networks">When to Use Quantile Neural Networks</h3>
<p>Quantile NNs are particularly well-suited when:</p>
<ul>
<li><strong>Decision problems</strong>: You need expectations or quantiles for optimization, not full densities</li>
<li><strong>Likelihood-free models</strong>: Complex simulators where <span class="math inline">\(p(y|\theta)\)</span> is unavailable</li>
<li><strong>Robustness</strong>: Data contains outliers or heavy tails</li>
<li><strong>Asymmetric costs</strong>: Different quantiles drive different decisions (e.g., stockouts vs.&nbsp;overstock)</li>
<li><strong>High dimensions</strong>: Density estimation becomes intractable, but quantile regression remains feasible</li>
<li><strong>Real-time requirements</strong>: Inference must be fast; networks provide instant quantile predictions</li>
</ul>
</section>
<section id="limitations-and-open-questions" class="level3">
<h3 class="anchored" data-anchor-id="limitations-and-open-questions">Limitations and Open Questions</h3>
<p>Despite their power, quantile methods have limitations:</p>
<ol type="1">
<li><p><strong>Multivariate outputs</strong>: Extension to high-dimensional outputs <span class="math inline">\(\mathbf{X} \in \mathbb{R}^p\)</span> requires careful handling of dependence structure. Autoregressive factorizations help but lose some joint information.</p></li>
<li><p><strong>Extreme quantiles</strong>: Estimating <span class="math inline">\(P_{0.01}\)</span> or <span class="math inline">\(P_{99.99}\)</span> requires substantial data in tails. Extreme value theory may help regularize these estimates.</p></li>
<li><p><strong>Non-crossing enforcement</strong>: Ensuring monotonicity in <span class="math inline">\(\tau\)</span> adds computational overhead. Recent work on monotonic neural networks <span class="citation" data-cites="wehenkel2019unconstrained">(<a href="references.html#ref-wehenkel2019unconstrained" role="doc-biblioref">Wehenkel and Louppe 2019</a>)</span> offers promise.</p></li>
<li><p><strong>Interpretability vs.&nbsp;flexibility</strong>: Neural networks are black boxes. For high-stakes decisions (medical, legal), transparency may favor simpler quantile methods.</p></li>
<li><p><strong>Calibration</strong>: Like all predictive models, quantile NNs can be miscalibrated. Post-hoc calibration techniques <span class="citation" data-cites="kuleshov2018accurate">(<a href="references.html#ref-kuleshov2018accurate" role="doc-biblioref">Kuleshov, Fenner, and Ermon 2018</a>)</span> should be applied, especially for critical applications.</p></li>
</ol>
</section>
<section id="connections-to-broader-ai" class="level3">
<h3 class="anchored" data-anchor-id="connections-to-broader-ai">Connections to Broader AI</h3>
<p>Quantile methods connect to several contemporary themes in AI and machine learning:</p>
<ul>
<li><strong>Generative modeling</strong>: Quantile networks are generative models—sample from uniform, pass through <span class="math inline">\(F^{-1}\)</span>, obtain samples from target distribution</li>
<li><strong>Implicit distributions</strong>: Like GANs, quantile NNs avoid explicit likelihood computation</li>
<li><strong>Conformal prediction</strong>: Quantile regression provides natural prediction sets with finite-sample coverage guarantees <span class="citation" data-cites="romano2019conformalized">(<a href="references.html#ref-romano2019conformalized" role="doc-biblioref">Romano, Patterson, and Candes 2019</a>)</span></li>
<li><strong>Bayesian deep learning</strong>: Quantile NNs offer an alternative to variational inference and MC dropout for uncertainty quantification</li>
<li><strong>Causal inference</strong>: Quantile treatment effects <span class="citation" data-cites="firpo2009unconditional">(<a href="references.html#ref-firpo2009unconditional" role="doc-biblioref">Firpo, Fortin, and Lemieux 2009</a>)</span> extend average treatment effects, capturing heterogeneous impacts across the outcome distribution</li>
</ul>
</section>
<section id="future-directions" class="level3">
<h3 class="anchored" data-anchor-id="future-directions">Future Directions</h3>
<p>Active research directions include:</p>
<ul>
<li><strong>Theoretical understanding</strong>: Tighter approximation bounds for quantile NNs in high dimensions</li>
<li><strong>Efficient architectures</strong>: Attention mechanisms and transformers for temporal quantile modeling</li>
<li><strong>Multi-task learning</strong>: Sharing representations across related forecasting problems</li>
<li><strong>Online learning</strong>: Adapting quantile models in non-stationary environments</li>
<li><strong>Integration with causal methods</strong>: Combining quantile regression with instrumental variables and difference-in-differences</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Quantile neural networks represent a convergence of classical statistical theory (quantile regression, robust statistics) with modern machine learning (deep learning, representation learning). By focusing on the quantities we actually need—quantiles for decision-making—rather than intermediate densities, these methods offer a pragmatic and powerful approach to uncertainty quantification.</p>
<p>As Keynes observed, it is better to be roughly right than precisely wrong. Quantile methods embrace this philosophy: they provide the distributional information needed for sound decisions without claiming to know the complete probability model. In an era of increasingly complex data and high-stakes applications, this combination of flexibility, robustness, and decision-focus makes quantile neural networks an essential tool for the modern data scientist.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-arjovsky2017wasserstein" class="csl-entry" role="listitem">
Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. <span>“Wasserstein Generative Adversarial Networks.”</span> <em>Proceedings of the 34th International Conference on Machine Learning</em>, 214–23.
</div>
<div id="ref-barron1993universal" class="csl-entry" role="listitem">
Barron, Andrew R. 1993. <span>“Universal Approximation Bounds for Superpositions of a Sigmoidal Function.”</span> <em>IEEE Transactions on Information Theory</em> 39 (3): 930–45.
</div>
<div id="ref-bellemare2017distributional" class="csl-entry" role="listitem">
Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017. <span>“A Distributional Perspective on Reinforcement Learning.”</span> <em>Proceedings of the 34th International Conference on Machine Learning</em>, 449–58.
</div>
<div id="ref-brillinger2012generalized" class="csl-entry" role="listitem">
Brillinger, David R. 2012. <span>“A <span>Generalized Linear Model With</span> <span>‘<span>Gaussian</span>’</span> <span>Regressor Variables</span>.”</span> In <em>Selected <span>Works</span> of <span>David Brillinger</span></em>, edited by Peter Guttorp and David Brillinger, 589–606. Selected <span>Works</span> in <span>Probability</span> and <span>Statistics</span>. New York, NY: Springer.
</div>
<div id="ref-cannon2018noncrossing" class="csl-entry" role="listitem">
Cannon, Alex J. 2018. <span>“Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes.”</span> <em>Stochastic Environmental Research and Risk Assessment</em> 32 (11): 3207–25.
</div>
<div id="ref-chernozhukov2010quantile" class="csl-entry" role="listitem">
Chernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010. <span>“Quantile and <span>Probability Curves Without Crossing</span>.”</span> <em>Econometrica</em> 78 (3): 1093–1125. <a href="https://www.jstor.org/stable/40664520">https://www.jstor.org/stable/40664520</a>.
</div>
<div id="ref-dabney2018implicit" class="csl-entry" role="listitem">
Dabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018. <span>“Implicit <span>Quantile Networks</span> for <span>Distributional Reinforcement Learning</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1806.06923">https://arxiv.org/abs/1806.06923</a>.
</div>
<div id="ref-firpo2009unconditional" class="csl-entry" role="listitem">
Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. 2009. <span>“Unconditional Quantile Regressions.”</span> <em>Econometrica : Journal of the Econometric Society</em> 77 (3): 953–73.
</div>
<div id="ref-kallenberg1997foundations" class="csl-entry" role="listitem">
Kallenberg, Olav. 1997. <em>Foundations of <span>Modern Probability</span></em>. 2nd ed. edition. Springer.
</div>
<div id="ref-kuleshov2018accurate" class="csl-entry" role="listitem">
Kuleshov, Volodymyr, Nathan Fenner, and Stefano Ermon. 2018. <span>“Accurate Uncertainties for Deep Learning Using Calibrated Regression.”</span> <em>Proceedings of the 35th International Conference on Machine Learning</em>, 2796–2804.
</div>
<div id="ref-levina2001earth" class="csl-entry" role="listitem">
Levina, Elizaveta, and Peter Bickel. 2001. <span>“The Earth Mover’s Distance Is the Mallows Distance: <span>Some</span> Insights from Statistics.”</span> In <em>Proceedings Eighth <span>IEEE</span> International Conference on Computer Vision. <span>ICCV</span> 2001</em>, 2:251–56. IEEE.
</div>
<div id="ref-parzen2004quantile" class="csl-entry" role="listitem">
Parzen, Emanuel. 2004. <span>“Quantile <span>Probability</span> and <span>Statistical Data Modeling</span>.”</span> <em>Statistical Science</em> 19 (4): 652–62. <a href="https://www.jstor.org/stable/4144436">https://www.jstor.org/stable/4144436</a>.
</div>
<div id="ref-polson2016mixtures" class="csl-entry" role="listitem">
Polson, Nicholas G., and James G. Scott. 2016. <span>“Mixtures, <span>Envelopes</span> and <span>Hierarchical Duality</span>.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 78 (4): 701–27.
</div>
<div id="ref-polson2023generative" class="csl-entry" role="listitem">
Polson, Nicholas G., and Vadim Sokolov. 2023. <span>“Generative <span>AI</span> for <span>Bayesian Computation</span>.”</span> <a href="https://arxiv.org/abs/2305.14972">https://arxiv.org/abs/2305.14972</a>.
</div>
<div id="ref-polson2024generative" class="csl-entry" role="listitem">
Polson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024. <span>“Generative <span>Bayesian Computation</span> for <span>Maximum Expected Utility</span>.”</span> <em>Entropy</em> 26 (12): 1076.
</div>
<div id="ref-romano2019conformalized" class="csl-entry" role="listitem">
Romano, Yaniv, Evan Patterson, and Emmanuel Candes. 2019. <span>“Conformalized <span>Quantile Regression</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 32. Curran Associates, Inc.
</div>
<div id="ref-salinas2019deepar" class="csl-entry" role="listitem">
Salinas, David, Valentin Flunkert, and Jan Gasthaus. 2019. <span>“<span>DeepAR</span>: <span>Probabilistic Forecasting</span> with <span>Autoregressive Recurrent Networks</span>.”</span> <em>arXiv:1704.04110 [Cs, Stat]</em>, February. <a href="https://arxiv.org/abs/1704.04110">https://arxiv.org/abs/1704.04110</a>.
</div>
<div id="ref-wang1996premium" class="csl-entry" role="listitem">
Wang, Shaun. 1996. <span>“Premium Calculation by Transforming the Layer Premium Density.”</span> <em>ASTIN Bulletin</em> 26 (1): 71–92.
</div>
<div id="ref-wehenkel2019unconstrained" class="csl-entry" role="listitem">
Wehenkel, Antoine, and Gilles Louppe. 2019. <span>“Unconstrained Monotonic Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 32: 1545–55.
</div>
<div id="ref-yaari1987dual" class="csl-entry" role="listitem">
Yaari, Menahem E. 1987. <span>“The <span>Dual Theory</span> of <span>Choice</span> Under <span>Risk</span>.”</span> <em>Econometrica</em> 55 (1): 95–115. <a href="https://www.jstor.org/stable/1911158">https://www.jstor.org/stable/1911158</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./21-sgd.html" class="pagination-link" aria-label="Gradient Descent">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./23-cnn.html" class="pagination-link" aria-label="Convolutional Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>