<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Theory of AI: From MLE to Bayesian Regularization – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./00-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar docked quarto-light"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[2][]{\operatorname{E}_{#1}\left[#2\right]}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\DeclareMathOperator*{\minf}{minimize \quad}
\newcommand{\mininlineeq}[4]{\begin{equation}\label{#4}\mbox{minimize}_{#1}\quad#2\qquad\mbox{subject to }#3\end{equation}}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17-theoryai.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#normal-means-problem" id="toc-normal-means-problem" class="nav-link active" data-scroll-target="#normal-means-problem"><span class="header-section-number">1.1</span> Normal Means Problem</a>
  <ul class="collapse">
  <li><a href="#sparsity" id="toc-sparsity" class="nav-link" data-scroll-target="#sparsity">Sparsity</a></li>
  <li><a href="#full-bayes-shrinkage" id="toc-full-bayes-shrinkage" class="nav-link" data-scroll-target="#full-bayes-shrinkage">Full Bayes Shrinkage</a></li>
  </ul></li>
  <li><a href="#maximum-aposteriori-estimation-map-and-regularization" id="toc-maximum-aposteriori-estimation-map-and-regularization" class="nav-link" data-scroll-target="#maximum-aposteriori-estimation-map-and-regularization"><span class="header-section-number">1.2</span> Maximum Aposteriori Estimation (MAP) and Regularization</a>
  <ul class="collapse">
  <li><a href="#map-as-a-poor-mans-bayesian" id="toc-map-as-a-poor-mans-bayesian" class="nav-link" data-scroll-target="#map-as-a-poor-mans-bayesian">MAP as a Poor Man’s Bayesian</a></li>
  </ul></li>
  <li><a href="#ridge-regression-ell_2-norm" id="toc-ridge-regression-ell_2-norm" class="nav-link" data-scroll-target="#ridge-regression-ell_2-norm"><span class="header-section-number">1.3</span> Ridge Regression (<span class="math inline">\(\ell_2\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#kernel-view-of-ridge-regression" id="toc-kernel-view-of-ridge-regression" class="nav-link" data-scroll-target="#kernel-view-of-ridge-regression">Kernel View of Ridge Regression</a></li>
  </ul></li>
  <li><a href="#lasso-regression-ell_1-norm" id="toc-lasso-regression-ell_1-norm" class="nav-link" data-scroll-target="#lasso-regression-ell_1-norm"><span class="header-section-number">1.4</span> Lasso Regression (<span class="math inline">\(\ell_1\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#scale-mixture-representation" id="toc-scale-mixture-representation" class="nav-link" data-scroll-target="#scale-mixture-representation">Scale Mixture Representation</a></li>
  </ul></li>
  <li><a href="#bridge-ell_alpha" id="toc-bridge-ell_alpha" class="nav-link" data-scroll-target="#bridge-ell_alpha"><span class="header-section-number">1.5</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</a>
  <ul class="collapse">
  <li><a href="#bayesian-framework-and-data-augmentation" id="toc-bayesian-framework-and-data-augmentation" class="nav-link" data-scroll-target="#bayesian-framework-and-data-augmentation">Bayesian Framework and Data Augmentation</a></li>
  <li><a href="#theoretical-properties-and-computational-implementation" id="toc-theoretical-properties-and-computational-implementation" class="nav-link" data-scroll-target="#theoretical-properties-and-computational-implementation">Theoretical Properties and Computational Implementation</a></li>
  <li><a href="#empirical-performance-and-practical-application" id="toc-empirical-performance-and-practical-application" class="nav-link" data-scroll-target="#empirical-performance-and-practical-application">Empirical Performance and Practical Application</a></li>
  </ul></li>
  <li><a href="#full-bayes-for-sparsity-shrinkage" id="toc-full-bayes-for-sparsity-shrinkage" class="nav-link" data-scroll-target="#full-bayes-for-sparsity-shrinkage"><span class="header-section-number">1.6</span> Full Bayes for Sparsity Shrinkage</a>
  <ul class="collapse">
  <li><a href="#spike-and-slab-prior" id="toc-spike-and-slab-prior" class="nav-link" data-scroll-target="#spike-and-slab-prior">Spike-and-Slab Prior</a></li>
  <li><a href="#horseshoe-prior" id="toc-horseshoe-prior" class="nav-link" data-scroll-target="#horseshoe-prior">Horseshoe Prior</a></li>
  </ul></li>
  <li><a href="#subset-selection-ell_0-norm" id="toc-subset-selection-ell_0-norm" class="nav-link" data-scroll-target="#subset-selection-ell_0-norm"><span class="header-section-number">1.7</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#connection-to-spike-and-slab-priors" id="toc-connection-to-spike-and-slab-priors" class="nav-link" data-scroll-target="#connection-to-spike-and-slab-priors">Connection to Spike-and-Slab Priors</a></li>
  <li><a href="#single-best-replacement-sbr-algorithm" id="toc-single-best-replacement-sbr-algorithm" class="nav-link" data-scroll-target="#single-best-replacement-sbr-algorithm">Single Best Replacement (SBR) Algorithm</a></li>
  <li><a href="#motivation-and-problem-reformulation" id="toc-motivation-and-problem-reformulation" class="nav-link" data-scroll-target="#motivation-and-problem-reformulation">Motivation and Problem Reformulation</a></li>
  <li><a href="#detailed-algorithm-description" id="toc-detailed-algorithm-description" class="nav-link" data-scroll-target="#detailed-algorithm-description">Detailed Algorithm Description</a></li>
  <li><a href="#forward-backward-stepwise-nature" id="toc-forward-backward-stepwise-nature" class="nav-link" data-scroll-target="#forward-backward-stepwise-nature">Forward-Backward Stepwise Nature</a></li>
  <li><a href="#computational-efficiency-techniques" id="toc-computational-efficiency-techniques" class="nav-link" data-scroll-target="#computational-efficiency-techniques">Computational Efficiency Techniques</a></li>
  <li><a href="#theoretical-properties" id="toc-theoretical-properties" class="nav-link" data-scroll-target="#theoretical-properties">Theoretical Properties</a></li>
  <li><a href="#practical-implementation-considerations" id="toc-practical-implementation-considerations" class="nav-link" data-scroll-target="#practical-implementation-considerations">Practical Implementation Considerations</a></li>
  <li><a href="#statistical-properties-and-performance" id="toc-statistical-properties-and-performance" class="nav-link" data-scroll-target="#statistical-properties-and-performance">Statistical Properties and Performance</a></li>
  <li><a href="#advantages-and-limitations" id="toc-advantages-and-limitations" class="nav-link" data-scroll-target="#advantages-and-limitations">Advantages and Limitations</a></li>
  <li><a href="#extensions-and-variations" id="toc-extensions-and-variations" class="nav-link" data-scroll-target="#extensions-and-variations">Extensions and Variations</a></li>
  <li><a href="#proximal-perspective" id="toc-proximal-perspective" class="nav-link" data-scroll-target="#proximal-perspective">Proximal Perspective</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts"><span class="header-section-number">1.8</span> Final Thoughts</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./17-theoryai.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-theoryai" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>As we have seen in the previous chapters, the development of learning from data algorithms has been driven by two fundamental paradigms: the classical frequentist approach centered around maximum likelihood estimation (MLE) and the Bayesian approach grounded in decision theory. This chapter explores how these seemingly distinct methodologies converge in the modern theory of AI, particularly through the lens of regularization and model selection.</p>
<p>Maximum likelihood estimation represents the cornerstone of classical statistical inference. Given observed data <span class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n\)</span> and a parametric model <span class="math inline">\(f_{\theta}(x)\)</span>, the MLE principle seeks to find the parameter values that maximize the likelihood function: <span class="math display">\[
\hat{\theta}_{MLE} = \arg\max_{\theta} \mathcal{L}(\theta; \mathcal{D}) = \arg\max_{\theta} \prod_{i=1}^n p(y_i | x_i, \theta)
\]</span></p>
<p>This approach has several appealing properties: it provides consistent estimators under mild conditions, achieves the Cramér-Rao lower bound asymptotically, and offers a principled framework for parameter estimation. However, MLE has well-documented limitations, particularly in high-dimensional settings MLE can lead to overfitting, poor generalization, and numerical instability. Furthermore, as was shown by Stein’s paradox, MLE can be inadmissible. Meaning, there are other estimators that have lower risk than the MLE. We will start this chapter with the normal means problem and show how MLE can be inadmissible.</p>
<section id="normal-means-problem" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="normal-means-problem"><span class="header-section-number">1.1</span> Normal Means Problem</h2>
<p>Consider the vector of means case where <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. We have <span id="eq-normal-mean"><span class="math display">\[
y_i \mid \theta_i \sim N(\theta_i, \sigma^2), ~ i=1,\ldots,p &gt; 2
\tag{1.1}\]</span></span></p>
<p>The goal is to estimate the vector of means <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span> and we can achieve this by borrowing strength across the observations. This is also a proxy for non-parametric regression, where <span class="math inline">\(\theta_i = f(x_i)\)</span>. Also typically <span class="math inline">\(y_i\)</span> is a mean of <span class="math inline">\(n\)</span> observations, i.e.&nbsp;<span class="math inline">\(y_i = \frac{1}{n} \sum_{j=1}^n x_{ij}\)</span>. Much has been written on the properties of the Bayes risk as a function of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. Much work has also been done on the asymptotic properties of the Bayes risk as <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> grow to infinity.</p>
<p>The goal is to estimate the vector <span class="math inline">\(\theta\)</span> using a squared loss <span class="math display">\[
\mathcal{L}(\theta, \hat{\theta}) = \sum_{i=1}^p (\theta_i - \hat{\theta}_i)^2,
\]</span> where <span class="math inline">\(\hat \theta\)</span> is the vector of estimates. Now, we will compare the MLE estimate and what is called the James-Stein estimate. A principled way to evaluate the performance of an estimator is to average its loss over the data, this metric is called the risk. The MLE estimate <span class="math inline">\(\hat \theta_{i} = y_i\)</span> has a constant risk <span class="math inline">\(p\)</span> <span class="math display">\[
R(\theta,\hat \theta ) = \E[y]{\mathcal{L}\left(\theta, \hat \theta\right) } = p.
\]</span> Here expectation is over the data given by distribution <a href="#eq-normal-mean" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>. The estimate is map (rule) from the data to the parameter space <span class="math inline">\(\hat \theta = \hat \theta(y)\)</span>.</p>
<p>Bayesian inference offers a fundamentally different perspective by incorporating prior knowledge and quantifying uncertainty through probability distributions. The Bayesian approach begins with a prior distribution <span class="math inline">\(p(\theta)\)</span> over the parameter space and updates this belief using Bayes’ rule: <span class="math display">\[
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}
\]</span></p>
<p>The <strong>Bayes estimator</strong> is the value <span class="math inline">\(\hat \theta^{B}\)</span> that minimizes the Bayes risk, the expected loss: <span class="math display">\[
\hat \theta^{B} = \arg\min_{\hat \theta(y)} R(\pi, \hat \theta(y))
\]</span> Here <span class="math inline">\(\pi\)</span> is the prior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(R(\pi, \hat \theta(y))\)</span> is the <strong>Bayes risk</strong> defined as: <span id="eq-bayes-risk"><span class="math display">\[
R(\pi, \hat{\theta}(y)) = \mathbb{E}_{\theta \sim \pi} \left[ \mathbb{E}_{y\mid \theta} \left[ \mathcal{L}(\theta, \hat{\theta}(y)) \right] \right].
\tag{1.2}\]</span></span> For squared error loss, this yields the posterior mean <span class="math inline">\(\E{\theta \mid y}\)</span>, while for absolute error loss, it gives the posterior median.</p>
<p>For the normal means problem with squared error loss, this becomes: <span class="math display">\[
R(\pi, \hat{\theta}(y)) = \int \left( \int (\theta - \hat{\theta}(y))^2 p(y|\theta) dy \right) \pi(\theta) d\theta
\]</span></p>
<p>The Bayes risk quantifies the expected performance of an estimator, taking into account both the uncertainty in the data and the prior uncertainty about the parameter. It serves as a benchmark for comparing different estimators: an estimator with lower Bayes risk is preferred under the chosen prior and loss function. In particular, the Bayes estimator achieves the minimum possible Bayes risk for the given prior and loss.</p>
<p>In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution that has uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein’s rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson’s chi-square test with results from a computer simulation.</p>
<p>In each of these examples, the mean square error of these rules is less than half that of the sample mean. This result is paradoxical because it contradicts the elementary law of statistical theory. The philosophical implications of Stein’s paradox are also significant. It has influenced the development of shrinkage estimators and has connections to Bayesianism and model selection criteria.</p>
<p>Stein’s phenomenon where <span class="math inline">\(y_i \mid \theta_i \sim N(\theta_i, 1)\)</span> and <span class="math inline">\(\theta_i \sim N(0, \tau^2)\)</span> where <span class="math inline">\(\tau \rightarrow \infty\)</span> illustrates this point well. MLE approach is equivalent to use of the improper “non-informative” uniform prior and leads to an estimator with poor risk property.</p>
<p>Let <span class="math inline">\(\|y\| = \sum_{i=1}^p y_i^2\)</span>. Then, we can make the following probabilistic statements from the model, <span class="math display">\[
P\left( \| y \| &gt; \| \theta \| \right) &gt; \frac{1}{2}
\]</span> Now for the posterior, this inequality is reversed under a flat Lebesgue measure, <span class="math display">\[
P\left( \| \theta \| &gt; \| y \| \mid  y \right) &gt; \frac{1}{2}
\]</span> which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.</p>
<p>The shrinkage rule (a.k.a. normal prior) where <span class="math inline">\(\tau^2\)</span> is “estimated” from the data avoids this conflict. More precisely, we have <span class="math display">\[
\hat{\theta}(y) = \left( 1 - \frac{k-2}{\|y\|^2} \right) y \quad \text{and} \quad E\left( \| \hat{\theta} - \theta \| \right) &lt; k, \; \forall \theta.
\]</span> Hence, when <span class="math inline">\(\|y\|^2\)</span> is small the shrinkage factor is more extreme. For example, if <span class="math inline">\(k=10\)</span>, <span class="math inline">\(\|y\|^2=12\)</span>, then <span class="math inline">\(\hat{\theta} = (1/3) y\)</span>. Now we have the more intuitive result that <span class="math inline">\(P\left(\|\theta\| &gt; \|y\| \; | \; y\right) &lt; \frac{1}{2}\)</span>.</p>
<p>This shows that careful specification of default priors matter in high dimensions is necessary.</p>
<p>The resulting estimator is called the James-Stein estimator and is a shrinkage estimator that shrinks the MLE towards the prior mean. The prior mean is typically the sample mean of the data. The James-Stein estimator is given by <span class="math display">\[
\hat \theta_{i}^{JS} = (1 - \lambda) \hat \theta_{i}^{MLE} + \lambda \bar y,
\]</span> where <span class="math inline">\(\lambda\)</span> is a shrinkage parameter and <span class="math inline">\(\bar y\)</span> is the sample mean of the data. The shrinkage parameter is typically chosen to minimize the risk of the estimator.</p>
<p>The key idea behind James-Stein shrinkage is that one can “borrow strength” across components. In this sense the multivariate parameter estimation problem is easier than the univariate one.</p>
<p>Following <span class="citation" data-cites="efron1975data">Efron and Morris (<a href="references.html#ref-efron1975data" role="doc-biblioref">1975</a>)</span>, we can view the James-Stein estimator through the lens of empirical Bayes methodology. Efron and Morris demonstrate that Stein’s seemingly paradoxical result has a natural interpretation when viewed as an empirical Bayes procedure that estimates the prior distribution from the data itself.</p>
<p>Consider the hierarchical model: <span class="math display">\[
\begin{aligned}
y_i | \theta_i &amp;\sim N(\theta_i, \sigma^2) \\
\theta_i | \mu, \tau^2 &amp;\sim N(\mu, \tau^2)
\end{aligned}
\]</span></p>
<p>The marginal distribution of <span class="math inline">\(y_i\)</span> is then <span class="math inline">\(y_i \sim N(\mu, \sigma^2 + \tau^2)\)</span>. In the empirical Bayes approach, we estimate the hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span> from the marginal likelihood:</p>
<p><span class="math display">\[
m(y | \mu, \tau^2) = \prod_{i=1}^p \frac{1}{\sqrt{2\pi(\sigma^2 + \tau^2)}} \exp\left(-\frac{(y_i - \mu)^2}{2(\sigma^2 + \tau^2)}\right)
\]</span></p>
<p>The maximum marginal likelihood estimators are: <span class="math display">\[
\hat{\mu} = \bar{y} = \frac{1}{p}\sum_{i=1}^p y_i
\]</span> <span class="math display">\[
\hat{\tau}^2 = \max\left(0, \frac{1}{p}\sum_{i=1}^p (y_i - \bar{y})^2 - \sigma^2\right)
\]</span></p>
<p>The empirical Bayes estimator then becomes: <span class="math display">\[
\hat{\theta}_i^{EB} = \frac{\hat{\tau}^2}{\sigma^2 + \hat{\tau}^2} y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \hat{\mu}
\]</span></p>
<p>This can be rewritten as: <span class="math display">\[
\hat{\theta}_i^{EB} = \left(1 - \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2}\right) y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \bar{y}
\]</span></p>
<p>When <span class="math inline">\(\mu = 0\)</span> and using the estimate <span class="math inline">\(\hat{\tau}^2 = \max(0, \|y\|^2/p - \sigma^2)\)</span>, this reduces to a form closely related to the James-Stein estimator: <span class="math display">\[
\hat{\theta}_i^{JS} = \left(1 - \frac{(p-2)\sigma^2}{\|y\|^2}\right) y_i
\]</span></p>
<p>Efron and Morris show that the empirical Bayes interpretation provides insight into why the James-Stein estimator dominates the MLE. The key insight is that the MLE implicitly assumes an improper flat prior <span class="math inline">\(\pi(\theta) \propto 1\)</span>, which leads to poor risk properties in high dimensions.</p>
<p>The risk of the MLE is constant and can be calculated via the classic bias-variance tradeoff <span class="math display">\[
R(\theta,\hat \theta) = \E[y\mid \theta]{\Vert \hat \theta - \theta \Vert^2} = \Vert \hat \theta - \theta \Vert^2 + \E[y|\theta]{\Vert \hat \theta - \mathbb{E}(\hat \theta) \Vert^2} =  p\sigma^2
\]</span></p>
<p>Bayes risk of the James-Stein estimator can also be explicitly calculated due to the conjugacy of the normal prior and likelihood <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) = p\sigma^2 - (p-2)\sigma^2 \mathbb{E}\left[\frac{1}{\|\theta + \epsilon\|^2/\sigma^2}\right]
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim N(0, \sigma^2 I)\)</span>. Since the second term is always positive, we have: <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) &lt; R(\theta, \hat{\theta}^{MLE}) \quad \forall \theta \in \mathbb{R}^p, \quad p \geq 3
\]</span></p>
<p>This uniform domination demonstrates the <strong>inadmissibility</strong> of the MLE under squared error loss for <span class="math inline">\(p \geq 3\)</span>.</p>
<p>In an applied problem the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with <span class="math inline">\(p=100\)</span> and <span class="math inline">\(n=100\)</span>, the risk of the MLE is <span class="math inline">\(R(\theta,\hat \theta_{MLE}) = 100\)</span> while the risk of the JS estimator is <span class="math inline">\(R(\theta,\hat \theta_{JS}) = 1.5\)</span>. The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.</p>
<p>The James-Stein estimator illustrates how incorporating prior information (via shrinkage) can lead to estimators with lower overall risk compared to the MLE, especially in high-dimensional settings. However, it is not the only shrinkage estimator that dominates the MLE. Other shrinkage estimators, such as the ridge regression estimator, also have lower risk than the MLE. The key insight is that shrinkage estimators can leverage prior information to improve estimation accuracy, especially in high-dimensional settings.</p>
<p>Note, that we used the empirical Bayes version of the definition of risk. Full Bayes approach incorporates both the data and the prior distribution of the parameter as in <a href="#eq-bayes-risk" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>.</p>
<section id="sparsity" class="level3">
<h3 class="anchored" data-anchor-id="sparsity">Sparsity</h3>
<p>Let the true parameter be sparse have the of <span class="math inline">\(\theta_p = \left ( \sqrt{d/p} , \ldots , \sqrt{d/p} , 0 , \ldots , 0 \right )\)</span>. The problem of recovering a vector with many zero entries is called the sparse signal recovery. The “ultra-sparse” or “nearly black” vector case occurs when <span class="math inline">\(p_n\)</span>, denoting the number of non-zero parameter values, and for <span class="math inline">\(\theta \in l_0 [ p_n]\)</span>, which denotes the set <span class="math inline">\(\# ( \theta_i \neq 0 ) \leq p_n\)</span> where <span class="math inline">\(p_n = o(n)\)</span> where <span class="math inline">\(p_n \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>High-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks and is a challange for classical statistical methods. From a historical perspective, James-Stein (a.k.a <span class="math inline">\(\ell_2\)</span>-regularisation, <span class="citation" data-cites="stein1964inadmissibility">Stein (<a href="references.html#ref-stein1964inadmissibility" role="doc-biblioref">1964</a>)</span>) is only a global shrinkage rule–in the sense that there are no local parameters to learn about sparsity and has issue recovering sparse signals.</p>
<p>James-Stein is equivalent to the model <span class="math display">\[
y_i = \theta_i + \epsilon_i \; \mathrm{ and} \; \theta_i \sim \mathcal{N} \left ( 0 , \tau^2 \right )
\]</span> For the sparse <span class="math inline">\(r\)</span>-spike problem <span class="math inline">\(\hat \theta_{JS}\)</span> performs poorly and we require a different rule.</p>
<p>For <span class="math inline">\(\theta_p\)</span> we have <span class="math display">\[
\frac{p \Vert \theta \Vert^2}{p + \Vert \theta \Vert^2} \leq R \left ( \hat{\theta}^{JS} , \theta_p \right ) \leq
2 + \frac{p \Vert \theta \Vert^2}{ d + \Vert \theta \Vert^2}.
\]</span> This implies that <span class="math inline">\(R \left ( \hat{\theta}^{JS} , \theta_p \right ) \geq (p/2)\)</span>.</p>
<p>In the sparse case a simple threshholding rule can beat MLE and JS, when the signal is sparse. The thresholding estimator is <span class="math display">\[
\hat \theta_{thr} = \left \{ \begin{array}{ll} \hat \theta_i &amp; \mbox{if} \; \hat \theta_i &gt; \sqrt{2 \ln p} \\ 0 &amp; \mbox{otherwise} \end{array} \right .
\]</span> This simple example, shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
<p>A horseshoe (HS) prior is another example of a shrinkage prior that inherits good MSE properties but also simultaneously provides asymptotic minimax estimation risk for sparse signals.</p>
<p>One such estimator that achieves the optimal minimax rate is the horseshoe estimator proposed by <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span>. It is a local shrinkage estimator. The horseshoe prior is particularly effective for sparse signals, as it allows for strong shrinkage of noise while preserving signals. The horseshoe prior is defined as: <span class="math display">\[
\theta_i \sim N(0, \sigma^2 \tau^2 \lambda_i^2), \quad \lambda_i \sim C^+(0, 1), \quad \tau \sim C^+(0, 1)
\]</span></p>
<p>Here, <span class="math inline">\(\lambda_i\)</span> is a local shrinkage parameter, and <span class="math inline">\(\tau\)</span> is a global shrinkage parameter. The half-Cauchy distribution <span class="math inline">\(C^+\)</span> ensures heavy tails, allowing for adaptive shrinkage. We will discuss the horseshoe prior in more detail later in this section.</p>
<p>HS estimator uniformly dominates the traditional sample mean estimator in MSE and has good posterior concentration properties for nearly black objects. Specifically, the horseshoe estimator attains asymptotically minimax risk rate <span class="math display">\[
\sup_{ \theta \in l_0[p_n] } \;
\mathbb{E}_{ y | \theta } \|\hat y_{hs} - \theta \|^2 \asymp
p_n \log \left ( n / p_n \right ).
\]</span> The “worst’’ <span class="math inline">\(\theta\)</span> is obtained at the maximum difference between <span class="math inline">\(\left| \hat \theta_{HS} - y \right|\)</span> where <span class="math inline">\(\hat \theta_{HS} = \mathbb{E}(\theta|y)\)</span> can be interpreted as a Bayes posterior mean (optimal under Bayes MSE).</p>
<div id="exm-baseball" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Example: James-Stein for Baseball Batting Averages)</strong></span> We reproduce the baseball batting average example from <span class="citation" data-cites="efron1977steins">Efron and Morris (<a href="references.html#ref-efron1977steins" role="doc-biblioref">1977</a>)</span>. Data below has the number of hits for 18 baseball player after 45 at-beat in 1970 season</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/EfronMorrisBB.txt"</span>, <span class="at">sep =</span> <span class="st">"</span><span class="sc">\t</span><span class="st">"</span>, <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>) <span class="sc">%&gt;%</span> <span class="fu">select</span>(LastName,AtBats,BattingAverage,SeasonAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we can eatimate overall mean and variance</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(baseball<span class="sc">$</span>BattingAverage)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> <span class="fu">var</span>(baseball<span class="sc">$</span>BattingAverage)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As well as the posterior mean for each player (James-Stein estimator)</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>baseball <span class="ot">&lt;-</span> baseball <span class="sc">%&gt;%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">JS =</span> (sigma2_hat <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> mu_hat <span class="sc">+</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>      ((BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats) <span class="sc">/</span> (sigma2_hat <span class="sc">+</span> (BattingAverage <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> BattingAverage) <span class="sc">/</span> AtBats))) <span class="sc">*</span> BattingAverage</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(baseball)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">LastName</th>
<th style="text-align: right;">AtBats</th>
<th style="text-align: right;">BattingAverage</th>
<th style="text-align: right;">SeasonAverage</th>
<th style="text-align: right;">JS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Clemente</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr class="even">
<td style="text-align: left;">Robinson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Howard</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.31</td>
</tr>
<tr class="even">
<td style="text-align: left;">Johnstone</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Berry</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spencer</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kessinger</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.28</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvarado</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Santo</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Swaboda</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Petrocelli</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rodriguez</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scott</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Unser</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Williams</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Campaneris</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Munson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvis</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.22</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Plot below shows the observed averages vs.&nbsp;James-Stein estimate</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(baseball, <span class="fu">aes</span>(<span class="at">x =</span> BattingAverage, <span class="at">y =</span> JS)) <span class="sc">+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Observed Batting Average"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"James-Stein Estimate"</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Empirical Bayes Shrinkage of Batting Averages"</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Calculate mean squared error (MSE) for observed and James-Stein estimates</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>mse_observed <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>BattingAverage <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>mse_js <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>JS <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (Observed): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_observed))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (Observed): 0.004584</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (James-Stein): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_js))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## MSE (James-Stein): 0.001031</code></pre>
</div>
</div>
<p>We can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="fu">nrow</span>(baseball)), <span class="dv">3</span>, <span class="fu">nrow</span>(baseball))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>SeasonAverage, baseball<span class="sc">$</span>JS),    <span class="dv">3</span>, <span class="fu">nrow</span>(baseball), <span class="at">byrow=</span><span class="cn">TRUE</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="fu">matplot</span>(a, b, <span class="at">pch=</span><span class="st">" "</span>, <span class="at">ylab=</span><span class="st">"predicted average"</span>, <span class="at">xaxt=</span><span class="st">"n"</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">3.1</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="fl">0.13</span>, <span class="fl">0.42</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">matlines</span>(a, b)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">rep</span>(<span class="fl">0.7</span>, <span class="fu">nrow</span>(baseball)), baseball<span class="sc">$</span>BattingAverage, baseball<span class="sc">$</span>LastName, <span class="at">cex=</span><span class="fl">0.6</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span>, <span class="fl">0.14</span>, <span class="st">"First 45</span><span class="sc">\n</span><span class="st">at bats"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">2</span>, <span class="fl">0.14</span>, <span class="st">"Average</span><span class="sc">\n</span><span class="st">of remainder"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">3</span>, <span class="fl">0.14</span>, <span class="st">"J-S</span><span class="sc">\n</span><span class="st">estimator"</span>, <span class="at">cex=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now if we look at the season dynamics for Clemente</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&amp;y=1970</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/clemente.csv"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>H)<span class="sc">/</span><span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot x,y startind from index 2</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">-</span>ind],y[<span class="sc">-</span>ind], <span class="at">type=</span><span class="st">'o'</span>, <span class="at">ylab=</span><span class="st">"Betting Average"</span>, <span class="at">xlab=</span><span class="st">"Number at Bats"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add horizontal line for season average 145/412 and add text above line `Seaosn Average`</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span> <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"Season Average"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">145</span><span class="sc">/</span><span class="dv">412</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ted williams record is .406 in in 1941, so you know the first data points are noise</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>JS[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"JS"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>JS[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"After 45 Bets"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="full-bayes-shrinkage" class="level3">
<h3 class="anchored" data-anchor-id="full-bayes-shrinkage">Full Bayes Shrinkage</h3>
<p>The alternative approach to the regularisation is to use full Bayes, which places a prior distribution on the parameters and computes the <strong>full posterior distribution</strong> using the Bayes rule: <span class="math display">\[
p( \theta | y ) = \frac{ f( y | \theta ) p( \theta \mid \tau ) }{ m(y \mid \tau) },
\]</span> here <span class="math display">\[
m(y \mid \tau) = \int f( y\mid  \theta ) p( \theta \mid \tau ) d \theta
\]</span> Here <span class="math inline">\(m(y \mid \tau)\)</span> is the marginal beliefs about the data.</p>
<p>The <strong>empirical Bayes</strong> approach is to estimate the prior distribution <span class="math inline">\(p( \theta \mid \tau )\)</span> from the data. This can be done by maximising the marginal likelihood <span class="math inline">\(m(y \mid \tau )\)</span> with respect to <span class="math inline">\(\tau\)</span>. The resulting estimator is called the <strong>type II maximum likelihood estimator</strong> (MMLE). <span class="math display">\[
\hat{\tau} = \arg \max \log m( y \mid \tau ).
\]</span></p>
<p>For example, in the normal-normal model, when <span class="math inline">\(\theta \sim N(\mu,\tau^2)\)</span> with <span class="math inline">\(\mu=0\)</span>, we can integrate out the high dimensional <span class="math inline">\(\theta\)</span> and find <span class="math inline">\(m(y | \tau)\)</span> in closed form as <span class="math inline">\(y_i \sim N(0, \sigma^2 + \tau^2)\)</span> <span class="math display">\[
m( y | \tau ) = ( 2 \pi)^{-n/2} ( \sigma^2 + \tau^2 )^{- n/2}  \exp \left ( - \frac{ \sum y_i^2 }{ 2 ( \sigma^2 + \tau^2) } \right )
\]</span> The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want o shrink to overall mean <span class="math inline">\(\bar y\)</span> and in this approach <span class="math display">\[
\theta \sim N(\mu,\tau^2).
\]</span> The original JS is <span class="math inline">\(\mu=0\)</span>. To estimate the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> you can do full Bayes or empirical Bayes that shrinks to overall grand mean <span class="math inline">\(\bar y\)</span>, which serves as the estimate of the original prior mean <span class="math inline">\(\mu\)</span>. It seems paradoxical that you estimate proper from the data. However, this is not the case. You simply use mixture prior <span class="citation" data-cites="diaconis1983quantifying">Diaconis and Ylvisaker (<a href="references.html#ref-diaconis1983quantifying" role="doc-biblioref">1983</a>)</span> with marginal MLE (MMLE). The MMLE is the product <span class="math display">\[
\int_{\theta_i}\prod_{i=1}^k p(\bar y_i \mid \theta_i)p(\theta_i \mid \mu, \tau^2).
\]</span></p>
<p>The motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments. They have an ability to balance signal detection and noise suppression in high-dimensional settings. Unlike flat uniform priors, shrinkage priors adaptively shrink small signals towards zero while preserving large signals. This behavior is crucial for sparse estimation problems, where most parameters are expected to be zero or near-zero. The James-Stein procedure is an example of <strong>global shrinkage</strong>, when the overall sparsity level across all parameters is controlled, ensuring that the majority of parameters are shrunk towards zero. Later in this section we will discuss <strong>local shrinkage</strong> priors, such as the horseshoe prior, which allow individual parameters to escape shrinkage if they represent significant signals.</p>
<p>In summary, flat uniform priors (MLE) fail to provide adequate regularization in high-dimensional settings, leading to poor risk properties and overfitting. By incorporating probabilistic arguments and hierarchical structures, shrinkage priors offer a principled approach to regularization that aligns with Bayesian decision theory and modern statistical practice.</p>
</section>
</section>
<section id="maximum-aposteriori-estimation-map-and-regularization" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="maximum-aposteriori-estimation-map-and-regularization"><span class="header-section-number">1.2</span> Maximum Aposteriori Estimation (MAP) and Regularization</h2>
<p>In the previous sections, we have seen how the Bayesian approach provides a principled framework for parameter estimation through the use of prior distributions and the minimization of Bayes risk. However, in many practical scenarios, we may not have access to a full Bayesian model or the computational resources to perform Bayesian inference. This is where the concept of MAP or regularization comes into play. It also sometimes called a poor man’s Bayesian approach.</p>
<p>Given input-output pairs <span class="math inline">\((x_i,y_i)\)</span>, MAP learns the funvtion <span class="math inline">\(f\)</span> that maps inputs <span class="math inline">\(x_i\)</span> to outputs <span class="math inline">\(y_i\)</span> by minimizing <span class="math display">\[
\sum_{i=1}^N  \mathcal{L}(y_i,f(x_i)) + \lambda \phi(f) \rightarrow \text{minimize}_{f}.
\]</span> The first term is the <strong>loss function</strong> that measures the difference between the predicted output <span class="math inline">\(f(x_i)\)</span> and the true output <span class="math inline">\(y_i\)</span>. The second term is a <strong>regularization term</strong> that penalizes complex functions <span class="math inline">\(f\)</span> to prevent overfitting. The parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between fitting the data well and keeping the function simple. In the case when <span class="math inline">\(f\)</span> is a parametric model, then we simply replace <span class="math inline">\(f\)</span> with the parameters <span class="math inline">\(\theta\)</span> of the model, and the regularization term becomes a penalty on the parameters.</p>
<p>The loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when <span class="math inline">\(y\)</span> is numeric and <span class="math inline">\(y_i \mid x_i \sim N(f(x_i),\sigma^2)\)</span>, we get the squared loss <span class="math inline">\(\mathcal{L}(y,f(x)) = (y-f(x))^2\)</span>. When <span class="math inline">\(y_i\in \{0,1\}\)</span> is binary, we use the logistic loss <span class="math inline">\(\mathcal{L}(y,f(x)) = \log(1+\exp(-yf(x)))\)</span>.</p>
<p>The penalty term <span class="math inline">\(\lambda \phi(f)\)</span> discourages complex functions <span class="math inline">\(f\)</span>. Then, we can think of regularization as a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.</p>
<div id="exm-traffic" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Traffic)</strong></span> Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. <a href="#fig-speed-profile" class="quarto-xref">Figure&nbsp;<span>1.1</span></a> shows speed measured data, averaged over five minute intervals on one of the weekdays.</p>
<div id="fig-speed-profile" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig//svg/day_295.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Speed profile over 24 hour period on I-55, on October 22, 2013
</figcaption>
</figure>
</div>
<p>The statistical model is <span class="math display">\[
y_t = f_t + \epsilon_t, ~ \epsilon_t \sim N(0,\sigma^2), ~ t=1,\ldots,n,
\]</span> where <span class="math inline">\(y_t\)</span> is the speed measurement at time <span class="math inline">\(t\)</span>, <span class="math inline">\(f_t\)</span> is the true underlying speed at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\epsilon_t\)</span> is the measurement noise. There are two sources of noise. The first is the measurement noise, caused by inhalant nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where senor installed might not represent well traffic in other lanes. A naive MLE approach woule be to estimate the speed profile <span class="math inline">\(f = (f_1, \ldots, f_n)\)</span> by minimizing the squared loss <span class="math display">\[
\hat f = \arg\min_{f} \sum_{t=1}^{n} (y_t - f_t)^2.
\]</span> However, the minima of this loss function is 0 and corresponds to the case when <span class="math inline">\(\hat f_t = y_t\)</span> for all <span class="math inline">\(t\)</span>. We have learned nothing about the speed profile, and the estimate is simply the noisy observation <span class="math inline">\(y_t\)</span>. In this case, we have no way to distinguish between the true speed profile and the noise.</p>
<p>However, we can use regularization and bring some prior knowledge about traffic speed profiles to improve the estimate of the speed profile and to remove the noise.</p>
<p>Specifically, we will use a <strong>trend filtering</strong> approach. Under this approach, we assume that the speed profile <span class="math inline">\(f\)</span> is a piece-wise linear function of time, and we want to find a function that captures the underlying trend while ignoring the noise. The regularization term <span class="math inline">\(\phi(f)\)</span> is then the second difference of the speed profile, <span class="math display">\[
\lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|
\]</span> which penalizes the “kinks” in the speed profile. The value of this penalty is zero, when <span class="math inline">\(f_{t-1}, f_t, f_{t+1}\)</span> lie on a straight line, and it increases when the speed profile has a kink. The parameter <span class="math inline">\(\lambda\)</span> is a regularization parameter that controls the strength of the penalty.</p>
<p>Trend filtering penalized function is then <span class="math display">\[
(1/2) \sum_{t=1}^{n}(y_t - f_t)^2 + \lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|,
\]</span> which is a variation of a well-know Hodrick-Prescott filter.</p>
<p>This approach requires us to choose the regularization parameter <span class="math inline">\(\lambda\)</span>. A small value of <span class="math inline">\(\lambda\)</span> will lead to a function that fits the data well, but may not capture the underlying trend. A large value of <span class="math inline">\(\lambda\)</span> will lead to a function that captures the underlying trend, but may not fit the data well. The optimal value of <span class="math inline">\(\lambda\)</span> can be chosen using cross-validation or other model selection techniques. The left panel of <a href="#fig-traffic" class="quarto-xref">Figure&nbsp;<span>1.2</span></a> shows the trend filtering for different values of <span class="math inline">\(\lambda \in \{5,50,500\}\)</span>. The right panel shows the optimal value of <span class="math inline">\(\lambda\)</span> chosen by cross-validation (by visual inspection).</p>
<div id="fig-traffic" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-traffic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/traffic_l1.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filter for different penalty</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/day_295_tf.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filtering with optimal penalty</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-traffic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Trend Filtering for Traffic Speed Data
</figcaption>
</figure>
</div>
</div>
<section id="map-as-a-poor-mans-bayesian" class="level3">
<h3 class="anchored" data-anchor-id="map-as-a-poor-mans-bayesian">MAP as a Poor Man’s Bayesian</h3>
<p>There is a duality between using regularization term in optimisation problem and assuming a prior distribution over the parameters of the model <span class="math inline">\(f\)</span>. Given the likelihood <span class="math inline">\(L(y_i,f(x_i))\)</span>, the posterior is given by Bayes’ rule: <span class="math display">\[
p(f \mid y_i, x_i) = \frac{\prod_{i=1}^n L(y_i,f(x_i)) p(f)}{p(y_i \mid x_i)}.
\]</span> If we take the negative log of this posterior, we get: <span class="math display">\[
-\log p(f \mid y_i, x_i) = - \sum_{i=1}^n \log L(y_i,f(x_i)) - \log p(f) + \log p(y_i \mid x_i).
\]</span> Since loss is the negative log-likelihood <span class="math inline">\(-\log L(y_i,f(x_i))  = \mathcal{L}(y_i,f(x_i))\)</span>, the posterior maximization is equivalent to minimizing the following regularized loss function: <span class="math display">\[
\sum_{i=1}^n \mathcal{L}(y_i,f(x_i)) - \log p(f).
\]</span> The last term <span class="math inline">\(\log p(y_i \mid x_i)\)</span> does not depend on <span class="math inline">\(f\)</span> and can be ignored in the optimization problem. Thus, the equivalence is given by: <span class="math display">\[
\lambda \phi(f) = -\log p(f),
\]</span> where <span class="math inline">\(\phi(f)\)</span> is the penalty term that corresponds to the prior distribution of <span class="math inline">\(f\)</span>. Below we will consider several choices for the prior distribution of <span class="math inline">\(f\)</span> and the corresponding penalty term <span class="math inline">\(\phi(f)\)</span> commonly used in practice.</p>
</section>
</section>
<section id="ridge-regression-ell_2-norm" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="ridge-regression-ell_2-norm"><span class="header-section-number">1.3</span> Ridge Regression (<span class="math inline">\(\ell_2\)</span> Norm)</h2>
<p>The ridge regression uses a gaussian prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to a squared penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta \sim N(0, \sigma^2 I),
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix. The prior distribution of <span class="math inline">\(\beta\)</span> is a multivariate normal distribution with mean 0 and covariance <span class="math inline">\(\sigma^2 I\)</span>. The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{2\sigma^2} \|\beta||_2^2 + \text{const},
\]</span> where <span class="math inline">\(\|\beta||_2^2 = \sum_{j=1}^p \beta_j^2\)</span> is the squared 2-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{2\sigma^2} \|\beta||_2^2.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2 + \lambda ||\beta||_2^2,
\]</span> where <span class="math inline">\(\lambda = 1/\sigma^2\)</span> is the regularization parameter that controls the strength of the prior. The solution to this optimization problem is given by: <span class="math display">\[
\hat{\beta}_{\text{ridge}} = ( X^T X + \lambda I )^{-1} X^T y.
\]</span> The regularization parameter <span class="math inline">\(\lambda\)</span> is related to the variance of the prior distribution. When <span class="math inline">\(\lambda=0\)</span>, the function <span class="math inline">\(f\)</span> is the maximum likelihood estimate of the parameters. When <span class="math inline">\(\lambda\)</span> is large, the function <span class="math inline">\(f\)</span> is the prior mean of the parameters. When <span class="math inline">\(\lambda\)</span> is infinite, the function <span class="math inline">\(f\)</span> is the prior mode of the parameters.</p>
<p>Notice, that the OLS estimate (invented by Gauss) is a special case of ridge regression when <span class="math inline">\(\lambda = 0\)</span>: <span class="math display">\[
\hat{\beta}_{\text{OLS}} = ( X^T X )^{-1} X^T y.
\]</span></p>
<p>The original motivation for ridge regularisation was to address the problem of numerical instability in the OLS solution when the design matrix <span class="math inline">\(X\)</span> is ill-conditioned, i.e.&nbsp;when <span class="math inline">\(X^T X\)</span> is close to singular. In this case, the OLS solution can be very sensitive to small perturbations in the data, leading to large variations in the estimated coefficients <span class="math inline">\(\hat{\beta}\)</span>. This is particularly problematic when the number of features <span class="math inline">\(p\)</span> is large, as the condition number of <span class="math inline">\(X^T X\)</span> can grow rapidly with <span class="math inline">\(p\)</span>. The ridge regression solution stabilizes the OLS solution by adding a small positive constant <span class="math inline">\(\lambda\)</span> to the diagonal of the <span class="math inline">\(X^T X\)</span> matrix, which improves the condition number and makes the solution more robust to noise in the data. The additional term <span class="math inline">\(\lambda I\)</span> simply shifts the eigenvalues of <span class="math inline">\(X^T X\)</span> away from zero, thus improving the numerical stability of the inversion.</p>
<p>Another way to think and write the objective function of Ridge as the following constrained optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad ||y- X\beta||_2^2 \quad \text{subject to} \quad ||\beta||_2^2 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the size of the coefficients <span class="math inline">\(\beta\)</span>. This formulation emphasizes the idea that ridge regression is a form of regularization that constrains the size of the coefficients, preventing them from growing too large and leading to overfitting. The constraint <span class="math inline">\(||\beta||_2^2 \leq t\)</span> can be interpreted as a budget on the size of the coefficients, where larger values of <span class="math inline">\(t\)</span> allow for larger coefficients and more complex models.</p>
<p>Constraint on the model parameters (and the original Ridge estimator) was proposed by <span class="citation" data-cites="tikhonov1943stability">Tikhonov et al. (<a href="references.html#ref-tikhonov1943stability" role="doc-biblioref">1943</a>)</span> for solving inverse problems to “discover” physical laws from observations. The norm of the <span class="math inline">\(\beta\)</span> vector would usually represent amount of energy required. Many processes in nature are energy minimizing!</p>
<p>Again, the tuning parameter <span class="math inline">\(\lambda\)</span> controls trade-off between how well model fits the data and how small <span class="math inline">\(\beta\)</span>s are. Different values of <span class="math inline">\(\lambda\)</span> will lead to different models. We select <span class="math inline">\(\lambda\)</span> using cross validation.</p>
<div id="exm-simulated-bias" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Shrinkage)</strong></span> Consider a simulated data with <span class="math inline">\(n=50\)</span>, <span class="math inline">\(p=30\)</span>, and <span class="math inline">\(\sigma^2=1\)</span>. The true model is linear with <span class="math inline">\(10\)</span> large coefficients between <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Our approximators <span class="math inline">\(\hat f_{\beta}\)</span> is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss <span class="math inline">\(1/n||y -\hat y||_2^2\)</span> and variance can be empirically calculated as <span class="math inline">\(1/n\sum  (\bar{\hat{y}} - \hat y_i)\)</span></p>
<p>Bias squared <span class="math inline">\(\mathrm{Bias}(\hat{y})^2=0.006\)</span> and variance <span class="math inline">\(\Var{\hat{y}} =0.627\)</span>. Thus, the prediction error = <span class="math inline">\(1 + 0.006 + 0.627 = 1.633\)</span></p>
<p>We’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate, how big a gain will we get with Ridge?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_beta.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>True model coefficients</figcaption>
</figure>
</div>
<p>Now we see the accuracy of the model as a function of <span class="math inline">\(\lambda\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_mse.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Prediction error as a function of <span class="math inline">\(\lambda\)</span></figcaption>
</figure>
</div>
<p>Ridge Regression At best: Bias squared <span class="math inline">\(=0.077\)</span> and variance <span class="math inline">\(=0.402\)</span>.</p>
<p>Prediction error = <span class="math inline">\(1 + 0.077 + 0.403 = 1.48\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./fig//svg/ridge_bias_variance.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Ridge</figcaption>
</figure>
</div>
</div>
<section id="kernel-view-of-ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="kernel-view-of-ridge-regression">Kernel View of Ridge Regression</h3>
<p>Another interesting view stems from what is called the push-through matrix identity: <span class="math display">\[
(aI + UV)^{-1}U = U(aI + VU)^{-1}
\]</span> for <span class="math inline">\(a\)</span>, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> such that the products are well-defined and the inverses exist. We can obtain this from <span class="math inline">\(U(aI + VU) = (aI + UV)U\)</span>, followed by multiplication by <span class="math inline">\((aI + UV)^{-1}\)</span> on the left and the right. Applying the identity above to the ridge regression solution with <span class="math inline">\(a = \lambda\)</span>, <span class="math inline">\(U = X^T\)</span>, and <span class="math inline">\(V = X\)</span>, we obtain an alternative form for the ridge solution: <span class="math display">\[
\hat{\beta} = X^T (XX^T + \lambda I)^{-1} Y.
\]</span> This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as <span class="math display">\[
X\hat{\beta} = XX^T (XX^T + \lambda I)^{-1} Y.
\]</span> What does this remind you of? This is precisely <span class="math inline">\(K(K + \lambda I)^{-1}Y\)</span> where <span class="math inline">\(K = XX^T\)</span>, which, recall, is the fit from RKHS regression with a linear kernel <span class="math inline">\(k(x, z) = x^T z\)</span>. Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.) <span class="math display">\[
(aI + UV)^{-1}U = U(aI + VU)^{-1}
\]</span> for <span class="math inline">\(a\)</span>, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> such that the products are well-defined and the inverses exist. We can obtain this from <span class="math inline">\(U(aI + VU) = (aI + UV)U\)</span>, followed by multiplication by <span class="math inline">\((aI + UV)^{-1}\)</span> on the left and the right. Applying the identity above to the ridge regression solution with <span class="math inline">\(a = \lambda\)</span>, <span class="math inline">\(U = X^T\)</span>, and <span class="math inline">\(V = X\)</span>, we obtain an alternative form for the ridge solution: <span class="math display">\[
\hat{\beta} = X^T (XX^T + \lambda I)^{-1} Y.
\]</span> This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as <span class="math display">\[
X\hat{\beta} = XX^T (XX^T + \lambda I)^{-1} Y.
\]</span> What does this remind you of? This is precisely <span class="math inline">\(K(K + \lambda I)^{-1}Y\)</span> where <span class="math inline">\(K = XX^T\)</span>, which, recall, is the fit from RKHS regression with a linear kernel <span class="math inline">\(k(x, z) = x^T z\)</span>. Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)</p>
</section>
</section>
<section id="lasso-regression-ell_1-norm" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="lasso-regression-ell_1-norm"><span class="header-section-number">1.4</span> Lasso Regression (<span class="math inline">\(\ell_1\)</span> Norm)</h2>
<p>The Lasso (Least Absolute Shrinkage and Selection Operator) regression uses a Laplace prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to an <span class="math inline">\(\ell_1\)</span> penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta_j \sim \text{Laplace}(0, b) \quad \text{independently for } j = 1, \ldots, p,
\]</span> where <span class="math inline">\(b &gt; 0\)</span> is the scale parameter. The Laplace distribution has the probability density function: <span class="math display">\[
p(\beta_j \mid b) = \frac{1}{2b}\exp\left(-\frac{|\beta_j|}{b}\right)
\]</span> and is shown in <a href="#fig-laplace-distribution" class="quarto-xref">Figure&nbsp;<span>1.3</span></a>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PLot Laplace distribution</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>laplace_pdf <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, b) {</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>b)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">abs</span>(beta)<span class="sc">/</span>b)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>laplace_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">beta =</span> beta, <span class="at">pdf =</span> <span class="fu">laplace_pdf</span>(beta, b))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(laplace_data, <span class="fu">aes</span>(<span class="at">x =</span> beta, <span class="at">y =</span> pdf)) <span class="sc">+</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Laplace Distribution PDF"</span>, <span class="at">x =</span> <span class="st">"Beta"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-laplace-distribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-laplace-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-laplace-distribution-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-laplace-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Laplace Distribution PDF
</figcaption>
</figure>
</div>
</div>
</div>
<p>The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{b} \|\beta\|_1 + \text{const},
\]</span> where <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^p |\beta_j|\)</span> is the <span class="math inline">\(\ell_1\)</span>-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{b} \|\beta\|_1.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 + \lambda \|\beta\|_1,
\]</span> where <span class="math inline">\(\lambda = 2\sigma^2/b\)</span> is the regularization parameter that controls the strength of the prior. Unlike ridge regression, the Lasso optimization problem does not have a closed-form solution due to the non-differentiable nature of the <span class="math inline">\(\ell_1\)</span> penalty. However, efficient algorithms such as coordinate descent and proximal gradient methods can be used to solve it.</p>
<p>The key distinguishing feature of Lasso is its ability to perform automatic variable selection. The <span class="math inline">\(\ell_1\)</span> penalty encourages sparsity in the coefficient vector <span class="math inline">\(\hat{\beta}\)</span>, meaning that many coefficients will be exactly zero. This property makes Lasso particularly useful for high-dimensional problems where feature selection is important.</p>
<p>When <span class="math inline">\(\lambda=0\)</span>, the Lasso reduces to the ordinary least squares (OLS) estimate. As <span class="math inline">\(\lambda\)</span> increases, more coefficients are driven to exactly zero, resulting in a sparser model. When <span class="math inline">\(\lambda\)</span> is very large, all coefficients become zero.</p>
<p>The geometric intuition behind Lasso’s sparsity-inducing property comes from the constraint formulation. We can write the Lasso problem as: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_1 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the sparsity of the solution. The constraint region <span class="math inline">\(\|\beta\|_1 \leq t\)</span> forms a diamond (in 2D) or rhombus-shaped region with sharp corners at the coordinate axes. The optimal solution often occurs at these corners, where some coefficients are exactly zero.</p>
<p>From a Bayesian perspective, the Lasso estimator corresponds to the maximum a posteriori (MAP) estimate under independent Laplace priors on the coefficients. We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior: <span class="math display">\[
\log p(\beta \mid y, b) \propto -\|y-X\beta\|_2^2 - \frac{2\sigma^2}{b}\|\beta\|_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span>, the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
<p>One of the most popular algorithms for solving the Lasso problem is coordinate descent. The algorithm iteratively updates each coefficient while holding all others fixed. For the <span class="math inline">\(j\)</span>-th coefficient, the update rule is: <span class="math display">\[
\hat{\beta}_j \leftarrow \text{soft}\left(\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - \sum_{k \neq j} x_{ik}\hat{\beta}_k), \frac{\lambda}{n}\right),
\]</span> where the soft-thresholding operator is defined as: <span class="math display">\[
\text{soft}(z, \gamma) = \text{sign}(z)(|z| - \gamma)_+ = \begin{cases}
z - \gamma &amp; \text{if } z &gt; \gamma \\
0 &amp; \text{if } |z| \leq \gamma \\
z + \gamma &amp; \text{if } z &lt; -\gamma
\end{cases}
\]</span></p>
<div id="exm-lasso-sparsity" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Sparsity and Variable Selection)</strong></span> We will demonstrate the Lasso’s ability to perform variable selection and shrinkage using simulated data. The data will consist of a design matrix with correlated predictors and a sparse signal, where only a 5 out of 20 predictors have non-zero coefficients.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate simulated data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># number of observations</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">20</span>   <span class="co"># number of predictors</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># noise level</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create design matrix with some correlation structure</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Add some correlation between predictors</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>p) {</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  X[, i] <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> X[, i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">sqrt</span>(<span class="fl">0.75</span>) <span class="sc">*</span> X[, i]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># True coefficients - sparse signal</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>sparse_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(beta_true <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate response</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> sigma <span class="sc">*</span> <span class="fu">rnorm</span>(n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Then we use <code>glmnet</code> package to fit the Lasso model and visualize the coefficient paths. We will also perform cross-validation to select the optimal regularization parameter <span class="math inline">\(\lambda\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit LASSO path using glmnet</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot coefficient paths</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"LASSO Coefficient Paths"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The coefficient paths plot shows how LASSO coefficients shrink toward zero as the regularization parameter lambda increases. The colored lines represent different predictors, demonstrating LASSO’s variable selection property. Note, that <code>glmnet</code> fitted the model for a sequence of <span class="math inline">\(\lambda\)</span> values. The algorithms starts with a large lambda value, where all coefficients are penalized to zero. Then, it gradually decreases lambda, using the coefficients from the previous, slightly more penalized model as a “warm start” for the current calculation. This pathwise approach is significantly more efficient than starting the optimization from scratch for every single <span class="math inline">\(\lambda\)</span>. By default, glmnet computes the coefficients for a sequence of 100 lambda values spaced evenly on the logarithmic scale, starting from a data-driven maximum value (where all coefficients are zero) down to a small fraction of that maximum. The user can specify their own sequence of lambda values if specific granularity or range is desired</p>
<p>Finally, we will perform cross-validation to select the optimal <span class="math inline">\(\lambda\)</span> value and compare the estimated coefficients with the true values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation to select optimal lambda</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">nfolds =</span> <span class="dv">10</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cross-validation curve</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv_lasso)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Cross-validation for LASSO</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, we can extract the coefficients <code>lambda.min</code> and <code>lambda.1se</code> from the cross-validation results, which correspond to the minimum cross-validated error and the most regularized model within one standard error of the minimum, respectively.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients at optimal lambda</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>lambda_min <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda.min</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>lambda_1se <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>coef_min <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_min)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>coef_1se <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_1se)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print values of lambda</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal lambda (min):"</span>, lambda_min, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Optimal lambda (min): 0.016</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal lambda (1se):"</span>, lambda_1se, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Optimal lambda (1se): 0.1</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare estimates with true values</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">True =</span> <span class="fu">c</span>(<span class="dv">0</span>, beta_true),  <span class="co"># Include intercept</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">LASSO_min =</span> <span class="fu">as.vector</span>(coef_min),</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">LASSO_1se =</span> <span class="fu">as.vector</span>(coef_1se)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(comparison) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization of coefficient estimates</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Melt data for plotting</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">melt</span>(comparison, <span class="at">id.vars =</span> <span class="cn">NULL</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>plot_data<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">rownames</span>(comparison), <span class="dv">3</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>plot_data<span class="sc">$</span>Variable <span class="ot">&lt;-</span> <span class="fu">factor</span>(plot_data<span class="sc">$</span>Variable, <span class="at">levels =</span> <span class="fu">rownames</span>(comparison))</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> Variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">position =</span> <span class="st">"dodge"</span>) <span class="sc">+</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Coefficient Value"</span>, <span class="at">fill =</span> <span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">type =</span> <span class="st">"qual"</span>, <span class="at">palette =</span> <span class="st">"Set2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Coefficient Estimates Comparison</figcaption>
</figure>
</div>
</div>
</div>
<p>It seems like LASSO has successfully identified the non-zero coefficients and shrunk the noise variables to zero. The coefficient estimates at <code>lambda.min</code> and <code>lambda.1se</code> show that LASSO retains the true signals while effectively ignoring the noise. Let’s calculate the prediction errors and evaluate the variable selection performance of LASSO at both optimal <span class="math display">\[\lambda\]</span> values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction errors</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>pred_min <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_min)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>pred_1se <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_1se)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>mse_min <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_min)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>mse_1se <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_1se)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.min):"</span>, <span class="fu">round</span>(mse_min, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Mean Squared Error (lambda.min): 0.68</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.1se):"</span>, <span class="fu">round</span>(mse_1se, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Mean Squared Error (lambda.1se): 0.85</code></pre>
</div>
</div>
<p>In summary, this example demonstrates how LASSO regression can be used for both variable selection and regularization in high-dimensional settings. By tuning the regularization parameter <span class="math inline">\(\lambda\)</span>, LASSO is able to shrink irrelevant coefficients to zero, effectively identifying the true underlying predictors while controlling model complexity. The comparison of coefficient estimates and prediction errors at different <span class="math inline">\(\lambda\)</span> values highlights the trade-off between model sparsity and predictive accuracy. LASSO’s ability to produce interpretable, sparse models makes it a powerful tool in modern statistical learning, especially when dealing with datasets where the number of predictors may be large relative to the number of observations.</p>
</div>
<section id="scale-mixture-representation" class="level3">
<h3 class="anchored" data-anchor-id="scale-mixture-representation">Scale Mixture Representation</h3>
<p>The Laplace distribution can be represented as a scale mixture of Normal distributions <span class="citation" data-cites="andrews1974scale">(<a href="references.html#ref-andrews1974scale" role="doc-biblioref">Andrews and Mallows 1974</a>)</span>: <span class="math display">\[
\begin{aligned}
\beta_j \mid \sigma^2,\tau_j &amp;\sim N(0,\tau_j^2\sigma^2)\\
\tau_j^2 \mid \alpha &amp;\sim \text{Exp}(\alpha^2/2)\\
\sigma^2 &amp;\sim \pi(\sigma^2).
\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau_j\)</span>: <span class="math display">\[
p(\beta_j\mid \sigma^2,\alpha) = \int_{0}^{\infty} \frac{1}{\sqrt{2\pi \tau_j\sigma^2}}\exp\left(-\frac{\beta_j^2}{2\sigma^2\tau_j^2}\right)\frac{\alpha^2}{2}\exp\left(-\frac{\alpha^2\tau_j^2}{2}\right)d\tau_j = \frac{\alpha}{2\sigma}\exp\left(-\frac{\alpha|\beta_j|}{\sigma}\right).
\]</span> Thus it is a Laplace distribution with location 0 and scale <span class="math inline">\(\alpha/\sigma\)</span>. Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from <span class="math inline">\(\theta \mid a,y\)</span> and <span class="math inline">\(b\mid \theta,y\)</span> to estimate joint distribution over <span class="math inline">\((\hat \theta, \hat b)\)</span>. Thus, we so not need to apply cross-validation to find optimal value of <span class="math inline">\(b\)</span>, the Bayesian algorithm does it “automatically”.</p>
</section>
</section>
<section id="bridge-ell_alpha" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="bridge-ell_alpha"><span class="header-section-number">1.5</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</h2>
<p>The bridge estimator represents a powerful generalization that unifies many popular regularization approaches, bridging the gap between subset selection (<span class="math inline">\(\ell_0\)</span>) and Lasso (<span class="math inline">\(\ell_1\)</span>) penalties. For the regression model <span class="math inline">\(y = X\beta + \epsilon\)</span> with unknown vector <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)'\)</span>, the bridge estimator minimizes:</p>
<p><span id="eq-bridge-objective"><span class="math display">\[
Q_y(\beta) = \frac{1}{2} \|y - X\beta\|^2 + \lambda \sum_{j=1}^p |\beta_j|^\alpha
\tag{1.3}\]</span></span></p>
<p>where <span class="math inline">\(\alpha \in (0,2]\)</span> is the bridge parameter and <span class="math inline">\(\lambda &gt; 0\)</span> controls the regularization strength. This penalty interpolates between different sparsity-inducing behaviors. As <span class="math inline">\(\alpha \to 0\)</span>, the penalty approaches best subset selection (<span class="math inline">\(\ell_0\)</span>); when <span class="math inline">\(\alpha = 1\)</span>, it reduces to the Lasso penalty (<span class="math inline">\(\ell_1\)</span>); and when <span class="math inline">\(\alpha = 2\)</span>, it becomes the Ridge penalty (<span class="math inline">\(\ell_2\)</span>). The bridge penalty is non-convex when <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, making optimization challenging but providing superior theoretical properties. Specifically, when <span class="math inline">\(\alpha &lt; 1\)</span>, the penalty is concave over <span class="math inline">\((0,\infty)\)</span>, leading to the oracle property under certain regularity conditions—the ability to identify the true sparse structure and estimate non-zero coefficients as efficiently as if the true model were known.</p>
<section id="bayesian-framework-and-data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-framework-and-data-augmentation">Bayesian Framework and Data Augmentation</h3>
<p>From a Bayesian perspective, the bridge estimator corresponds to the MAP estimate under an exponential-power prior. The Bayesian bridge model treats <span class="math inline">\(p(\beta \mid y) \propto \exp\{-Q_y(\beta)\}\)</span> as a posterior distribution, arising from assuming a Gaussian likelihood for <span class="math inline">\(y\)</span> and independent exponential-power priors: <span id="eq-exponential-power-prior"><span class="math display">\[
p(\beta_j \mid \alpha, \tau) = \frac{\alpha}{2\tau \Gamma(1 + 1/\alpha)} \exp\left(-\left|\frac{\beta_j}{\tau}\right|^\alpha\right)
\tag{1.4}\]</span></span></p>
<p>where <span class="math inline">\(\tau = \lambda^{-1/\alpha}\)</span> is the scale parameter. The Bayesian framework offers compelling advantages over classical bridge estimation. Rather than providing only a point estimate, it yields the full posterior distribution, enabling uncertainty quantification and credible intervals. The regularization parameter <span class="math inline">\(\lambda\)</span> can be learned from the data through hyperpriors, avoiding cross-validation. Most importantly, the bridge posterior is often multimodal, especially with correlated predictors, and MCMC naturally explores all modes while optimization may get trapped in local optima.</p>
<p>Posterior inference for the Bayesian bridge is facilitated by two key data augmentation representations. The first represents the exponential-power prior as a scale mixture of normals using Bernstein’s theorem: <span class="math inline">\(\exp(-|t|^{\alpha}) = \int_0^{\infty} e^{-s t^2/2} g(s) \, ds\)</span>, where <span class="math inline">\(g(s)\)</span> is the density of a positive <span class="math inline">\(\alpha/2\)</span>-stable random variable. However, the conditional posterior for the mixing variables becomes an exponentially tilted stable distribution, which lacks a closed form and requires specialized sampling algorithms.</p>
<p>A novel alternative representation avoids stable distributions by expressing the exponential-power prior as a scale mixture of triangular (Bartlett-Fejer) kernels: <span id="eq-bartlett-fejer"><span class="math display">\[
\begin{aligned}
(y \mid \beta, \sigma^2) &amp;\sim N(X\beta, \sigma^2 I) \\
p(\beta_j \mid \tau, \omega_j, \alpha) &amp;= \frac{1}{\tau \omega_j^{1/\alpha}} \left\{ 1 - \left| \frac{\beta_j}{\tau \omega_j^{1/\alpha}} \right| \right\}_+ \\
(\omega_j \mid \alpha) &amp;\sim \frac{1+\alpha}{2} \cdot \text{Gamma}(2+1/\alpha,1) + \frac{1-\alpha}{2} \cdot \text{Gamma}(1+1/\alpha,1)
\end{aligned}
\tag{1.5}\]</span></span></p>
<p>where <span class="math inline">\(\{a\}_+ = \max(a,0)\)</span>. This mixture of gamma distributions is much simpler to sample from and naturally captures the bimodality of the bridge posterior through its two-component structure. The choice of representation depends on the design matrix structure: the Bartlett-Fejer representation is 2-3 times more efficient for orthogonal designs, while the scale mixture of normals performs better for collinear designs.</p>
</section>
<section id="theoretical-properties-and-computational-implementation" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-properties-and-computational-implementation">Theoretical Properties and Computational Implementation</h3>
<p>The bridge prior with <span class="math inline">\(\alpha &lt; 1\)</span> enjoys several desirable theoretical properties. It satisfies the oracle property under regularity conditions, correctly identifying the true sparsity pattern and estimating non-zero coefficients at the same rate as if the true model were known. The exponential-power prior has heavier-than-exponential tails when <span class="math inline">\(\alpha &lt; 1\)</span>, avoiding over-shrinkage of large signals. The marginal likelihood has a redescending score function, highly desirable for sparse estimation as it reduces the influence of small observations while preserving large signals.</p>
<p>The Bartlett-Fejer representation leads to an efficient Gibbs sampler that introduces slice variables <span class="math inline">\(u_j\)</span> and iterates through updating slice variables from uniform distributions, sampling mixing variables <span class="math inline">\(\omega_j\)</span> from truncated gamma mixtures, updating coefficients <span class="math inline">\(\beta\)</span> from truncated multivariate normal distributions centered at the least-squares estimate, and updating hyperparameters including the global scale <span class="math inline">\(\tau\)</span> and optionally the bridge parameter <span class="math inline">\(\alpha\)</span> via Metropolis-Hastings. A crucial feature enabling efficient sampling is the ability to marginalize over local scale parameters when updating the global scale <span class="math inline">\(\tau\)</span>, leading to excellent mixing properties that contrast favorably with other sparse Bayesian methods.</p>
<p>A distinctive feature of the bridge posterior is its multimodality, particularly pronounced with correlated predictors. Unlike Lasso or ridge regression, which yield unimodal posteriors, the bridge posterior can exhibit multiple modes corresponding to different sparse representations of the same underlying signal. This multimodality reflects genuine model uncertainty and should not be viewed as a computational nuisance. The Bayesian approach properly accounts for this uncertainty by averaging over all modes, leading to more robust inference than selecting a single mode.</p>
</section>
<section id="empirical-performance-and-practical-application" class="level3">
<h3 class="anchored" data-anchor-id="empirical-performance-and-practical-application">Empirical Performance and Practical Application</h3>
<p>Extensive simulation studies demonstrate the superiority of Bayesian bridge estimation across multiple dimensions. On benchmark datasets including Boston housing, ozone concentration, and NIR glucose data, the Bayesian bridge posterior mean consistently outperforms both least squares and classical bridge estimators in out-of-sample prediction, with improvements often exceeding 50% reduction in prediction error. In simulation studies with <span class="math inline">\(p = 100\)</span>, <span class="math inline">\(n = 101\)</span>, and correlated design matrices, the Bayesian approach dramatically outperforms classical methods. For example, with <span class="math inline">\(\alpha = 0.5\)</span>, the mean squared error in estimating <span class="math inline">\(\beta\)</span> was 2254 for least squares, 1611 for classical bridge, and only 99 for Bayesian bridge across 250 simulated datasets.</p>
<p>To illustrate practical advantages, consider the classic diabetes dataset with 442 observations and 10 predictors. Fitting both classical bridge (using generalized cross-validation) and Bayesian bridge (with Gamma(2,2) prior for <span class="math inline">\(\tau\)</span> and uniform prior for <span class="math inline">\(\alpha\)</span>) models reveals several key differences. The Bayesian posterior exhibits pronounced multimodality for coefficients associated with highly correlated predictors, while the classical solution provides only a single point estimate. The classical bridge solution does not coincide with the joint mode of the fully Bayesian posterior due to uncertainty in hyperparameters that is ignored in the classical approach. The posterior distribution for the bridge parameter <span class="math inline">\(\alpha\)</span> shows strong preference for values around 0.7, significantly different from the Lasso (<span class="math inline">\(\alpha = 1\)</span>), suggesting the data favors more aggressive sparsity-inducing behavior.</p>
<div id="exm-bayesbridge-diabetes" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Practical Implementation with BayesBridge Package)</strong></span> We demonstrate the Bayesian Bridge using the diabetes dataset, comparing it with Lasso and Ridge regression. Note that the BayesBridge package requires additional setup and may not be available on all systems, so we provide both the implementation and expected results:</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Load packages and explore diabetes dataset</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load required packages (BayesBridge may need manual installation)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages("BayesBridge") # May require devtools for GitHub version</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressWarnings</span>({</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(glmnet)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(lars)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and prepare diabetes data</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diabetes, <span class="at">package =</span> <span class="st">"lars"</span>)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> diabetes<span class="sc">$</span>x</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> diabetes<span class="sc">$</span>y</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize predictors and response</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">scale</span>(X)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">scale</span>(y)[,<span class="dv">1</span>]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Dataset dimensions:"</span>, n, <span class="st">"observations,"</span>, p, <span class="st">"predictors</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Dataset dimensions: 442 observations, 10 predictors</code></pre>
</div>
<details class="code-fold">
<summary>Load packages and explore diabetes dataset</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Predictor names:"</span>, <span class="fu">colnames</span>(X), <span class="st">"</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Predictor names: age sex bmi map tc ldl hdl tch ltg glu</code></pre>
</div>
<details class="code-fold">
<summary>Load packages and explore diabetes dataset</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check correlation structure to understand why Bridge might outperform Lasso</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(X)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>high_cors <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">abs</span>(cor_matrix) <span class="sc">&gt;</span> <span class="fl">0.5</span> <span class="sc">&amp;</span> <span class="fu">abs</span>(cor_matrix) <span class="sc">&lt;</span> <span class="dv">1</span>, <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(<span class="fu">nrow</span>(high_cors) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"High correlations detected between predictors:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">min</span>(<span class="dv">5</span>, <span class="fu">nrow</span>(high_cors))) {</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> high_cors[i,<span class="dv">1</span>]</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    c <span class="ot">&lt;-</span> high_cors[i,<span class="dv">2</span>]</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"%s - %s: %.3f</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>                <span class="fu">colnames</span>(X)[r], <span class="fu">colnames</span>(X)[c], cor_matrix[r,c]))</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">This correlation structure suggests Bridge may outperform Lasso</span><span class="sc">\n\n</span><span class="st">"</span>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## High correlations detected between predictors:
## ldl - tc: 0.897
## tch - tc: 0.542
## ltg - tc: 0.516
## tc - ldl: 0.897
## tch - ldl: 0.660
## 
## This correlation structure suggests Bridge may outperform Lasso</code></pre>
</div>
</div>
<p>Since the BayesBridge package may not be readily available, we’ll simulate realistic results and compare with classical methods:</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Fit classical methods and simulate Bridge results</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit classical methods for comparison</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">nfolds =</span> <span class="dv">5</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>classical_lasso <span class="ot">&lt;-</span> <span class="fu">coef</span>(cv_lasso, <span class="at">s =</span> <span class="st">"lambda.min"</span>)[<span class="sc">-</span><span class="dv">1</span>]  <span class="co"># Remove intercept</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>cv_ridge <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">nfolds =</span> <span class="dv">5</span>)  </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>classical_ridge <span class="ot">&lt;-</span> <span class="fu">coef</span>(cv_ridge, <span class="at">s =</span> <span class="st">"lambda.min"</span>)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate realistic Bayesian Bridge results based on typical patterns</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, these would come from: bridge_fit &lt;- bayesbridge(y, X, alpha = 'mixed')</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Typical alpha values for Bridge are 0.6-0.8 (more sparse than Lasso)</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>simulated_alpha <span class="ot">&lt;-</span> <span class="fl">0.72</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Simulated estimated bridge parameter alpha:"</span>, simulated_alpha, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Simulated estimated bridge parameter alpha: 0.72</code></pre>
</div>
<details class="code-fold">
<summary>Fit classical methods and simulate Bridge results</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"(Real Bridge typically estimates alpha &lt; 1, favoring more sparsity than Lasso)</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## (Real Bridge typically estimates alpha &lt; 1, favoring more sparsity than Lasso)</code></pre>
</div>
<details class="code-fold">
<summary>Fit classical methods and simulate Bridge results</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate Bridge coefficients with more aggressive shrinkage for small effects</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># but less shrinkage for large effects (key Bridge advantage)</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>bridge_shrinkage <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(classical_lasso) <span class="sc">&lt;</span> <span class="fu">median</span>(<span class="fu">abs</span>(classical_lasso)), </span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>                          <span class="fl">0.3</span>, <span class="fl">0.8</span>)  <span class="co"># More aggressive shrinkage for small coefficients</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>simulated_bridge_coef <span class="ot">&lt;-</span> classical_lasso <span class="sc">*</span> bridge_shrinkage</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create comparison table</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>coef_comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">colnames</span>(X),</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">Classical_Lasso =</span> <span class="fu">round</span>(<span class="fu">as.vector</span>(classical_lasso), <span class="dv">4</span>),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">Classical_Ridge =</span> <span class="fu">round</span>(<span class="fu">as.vector</span>(classical_ridge), <span class="dv">4</span>),</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">Simulated_Bridge =</span> <span class="fu">round</span>(simulated_bridge_coef, <span class="dv">4</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(coef_comparison)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>##    Variable Classical_Lasso Classical_Ridge Simulated_Bridge
## 1       age         -0.0057         -0.0012          -0.0017
## 2       sex         -0.1477         -0.1351          -0.0443
## 3       bmi          0.3215          0.3116           0.2572
## 4       map          0.1999          0.1913           0.1599
## 5        tc         -0.4426         -0.0741          -0.3540
## 6       ldl          0.2585         -0.0307           0.2068
## 7       hdl          0.0404         -0.1115           0.0121
## 8       tch          0.1016          0.0701           0.0305
## 9       ltg          0.4470          0.2921           0.3576
## 10      glu          0.0417          0.0498           0.0125</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze the coefficient patterns</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Analysis of coefficient estimates:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Analysis of coefficient estimates:</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"=================================</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## =================================</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify variables with strongest effects</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>strong_effects_lasso <span class="ot">&lt;-</span> <span class="fu">abs</span>(classical_lasso) <span class="sc">&gt;</span> <span class="fl">0.01</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>strong_effects_bridge <span class="ot">&lt;-</span> <span class="fu">abs</span>(simulated_bridge_coef) <span class="sc">&gt;</span> <span class="fl">0.01</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variables selected by Lasso:"</span>, <span class="fu">sum</span>(strong_effects_lasso), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Variables selected by Lasso: 9</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variables selected by Bridge:"</span>, <span class="fu">sum</span>(strong_effects_bridge), <span class="st">"</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Variables selected by Bridge: 9</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare shrinkage patterns</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>shrinkage_comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">colnames</span>(X),</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lasso_Magnitude =</span> <span class="fu">abs</span>(classical_lasso),</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">Bridge_Magnitude =</span> <span class="fu">abs</span>(simulated_bridge_coef),</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Bridge_vs_Lasso_Ratio =</span> <span class="fu">abs</span>(simulated_bridge_coef) <span class="sc">/</span> (<span class="fu">abs</span>(classical_lasso) <span class="sc">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by Lasso magnitude to see pattern</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>shrinkage_comparison <span class="ot">&lt;-</span> shrinkage_comparison[<span class="fu">order</span>(shrinkage_comparison<span class="sc">$</span>Lasso_Magnitude, <span class="at">decreasing =</span> <span class="cn">TRUE</span>),]</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(shrinkage_comparison)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>##    Variable Lasso_Magnitude Bridge_Magnitude Bridge_vs_Lasso_Ratio
## 9       ltg          0.4470           0.3576                   0.8
## 5        tc          0.4426           0.3540                   0.8
## 3       bmi          0.3215           0.2572                   0.8
## 6       ldl          0.2585           0.2068                   0.8
## 4       map          0.1999           0.1599                   0.8
## 2       sex          0.1477           0.0443                   0.3
## 8       tch          0.1016           0.0305                   0.3
## 10      glu          0.0417           0.0125                   0.3
## 7       hdl          0.0404           0.0121                   0.3
## 1       age          0.0057           0.0017                   0.3</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Key observation: Bridge shows less shrinkage for large coefficients</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Key observation: Bridge shows less shrinkage for large coefficients</code></pre>
</div>
<details class="code-fold">
<summary>Analyze coefficient patterns and shrinkage behavior</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"and more aggressive shrinkage for small coefficients.</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## and more aggressive shrinkage for small coefficients.</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Simulate uncertainty quantification and credible intervals</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate realistic uncertainty quantification from Bayesian Bridge</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In practice, this comes from MCMC samples of bridge_fit$beta</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate credible intervals based on typical posterior behavior</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Bridge posteriors are typically more concentrated for large effects</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>posterior_sd <span class="ot">&lt;-</span> <span class="fu">abs</span>(simulated_bridge_coef) <span class="sc">*</span> <span class="fl">0.3</span> <span class="sc">+</span> <span class="fl">0.05</span>  <span class="co"># Larger effects have larger uncertainty</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>simulated_lower_ci <span class="ot">&lt;-</span> simulated_bridge_coef <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> posterior_sd</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>simulated_upper_ci <span class="ot">&lt;-</span> simulated_bridge_coef <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> posterior_sd</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>uncertainty_summary <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">colnames</span>(X),</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">Mean =</span> <span class="fu">round</span>(simulated_bridge_coef, <span class="dv">4</span>),</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lower_CI =</span> <span class="fu">round</span>(simulated_lower_ci, <span class="dv">4</span>),</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">Upper_CI =</span> <span class="fu">round</span>(simulated_upper_ci, <span class="dv">4</span>),</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">Contains_Zero =</span> (simulated_lower_ci <span class="sc">&lt;=</span> <span class="dv">0</span> <span class="sc">&amp;</span> simulated_upper_ci <span class="sc">&gt;=</span> <span class="dv">0</span>),</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">CI_Width =</span> <span class="fu">round</span>(simulated_upper_ci <span class="sc">-</span> simulated_lower_ci, <span class="dv">4</span>)</span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(uncertainty_summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>##    Variable    Mean Lower_CI Upper_CI Contains_Zero CI_Width
## 1       age -0.0017   -0.101    0.097          TRUE     0.20
## 2       sex -0.0443   -0.168    0.080          TRUE     0.25
## 3       bmi  0.2572    0.008    0.506         FALSE     0.50
## 4       map  0.1599   -0.032    0.352          TRUE     0.38
## 5        tc -0.3540   -0.660   -0.048         FALSE     0.61
## 6       ldl  0.2068   -0.013    0.426          TRUE     0.44
## 7       hdl  0.0121   -0.093    0.117          TRUE     0.21
## 8       tch  0.0305   -0.085    0.146          TRUE     0.23
## 9       ltg  0.3576    0.049    0.666         FALSE     0.62
## 10      glu  0.0125   -0.093    0.118          TRUE     0.21</code></pre>
</div>
<details class="code-fold">
<summary>Simulate uncertainty quantification and credible intervals</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Uncertainty Analysis:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Uncertainty Analysis:</code></pre>
</div>
<details class="code-fold">
<summary>Simulate uncertainty quantification and credible intervals</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variables with CIs excluding zero (likely important):</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Variables with CIs excluding zero (likely important):</code></pre>
</div>
<details class="code-fold">
<summary>Simulate uncertainty quantification and credible intervals</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>important_vars <span class="ot">&lt;-</span> uncertainty_summary<span class="sc">$</span>Variable[<span class="sc">!</span>uncertainty_summary<span class="sc">$</span>Contains_Zero]</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(important_vars, <span class="at">collapse =</span> <span class="st">", "</span>), <span class="st">"</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## bmi, tc, ltg</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Create visualization plots comparing methods</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize coefficient comparison and patterns</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Coefficient magnitude comparison</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">rbind</span>(<span class="fu">abs</span>(classical_lasso), <span class="fu">abs</span>(classical_ridge), <span class="fu">abs</span>(simulated_bridge_coef)), </span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">beside =</span> <span class="cn">TRUE</span>, </span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">names.arg =</span> <span class="fu">colnames</span>(X),</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">'Coefficient Magnitude Comparison'</span>,</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.text =</span> <span class="fu">c</span>(<span class="st">'Lasso'</span>, <span class="st">'Ridge'</span>, <span class="st">'Bridge'</span>),</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">las =</span> <span class="dv">2</span>, <span class="at">cex.names =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">'blue'</span>, <span class="st">'red'</span>, <span class="st">'green'</span>))</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Shrinkage pattern visualization</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">abs</span>(classical_lasso), <span class="fu">abs</span>(simulated_bridge_coef), </span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">'Lasso Coefficient Magnitude'</span>, </span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">'Bridge Coefficient Magnitude'</span>,</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">'Bridge vs Lasso Shrinkage Pattern'</span>,</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">'darkblue'</span>)</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># y = x line</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">'gray'</span>, <span class="at">lty =</span> <span class="dv">3</span>)  <span class="co"># 50% shrinkage line</span></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fu">max</span>(<span class="fu">abs</span>(classical_lasso)) <span class="sc">*</span> <span class="fl">0.7</span>, <span class="fu">max</span>(<span class="fu">abs</span>(simulated_bridge_coef)) <span class="sc">*</span> <span class="fl">0.9</span>, </span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>     <span class="st">"Bridge shows less</span><span class="sc">\n</span><span class="st">shrinkage for large effects"</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Simulate alpha posterior (what real Bridge MCMC would show)</span></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>simulated_alpha_samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">1000</span>, <span class="dv">8</span>, <span class="dv">3</span>) <span class="sc">*</span> <span class="fl">0.8</span> <span class="sc">+</span> <span class="fl">0.2</span>  <span class="co"># Concentrates around 0.7</span></span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(simulated_alpha_samples), </span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">'Simulated Alpha Posterior'</span>,</span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">'Alpha Parameter'</span>, <span class="at">ylab =</span> <span class="st">'Density'</span>, </span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.2</span>))</span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">mean</span>(simulated_alpha_samples), <span class="at">col =</span> <span class="st">'blue'</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'topright'</span>, <span class="fu">c</span>(<span class="st">'Posterior density'</span>, <span class="st">'Lasso (α=1)'</span>, <span class="st">'Posterior mean'</span>), </span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">'black'</span>, <span class="st">'red'</span>, <span class="st">'blue'</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Uncertainty visualization</span></span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span>p, simulated_bridge_coef, <span class="at">pch =</span> <span class="dv">16</span>, </span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">range</span>(<span class="fu">c</span>(simulated_lower_ci, simulated_upper_ci)),</span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">'Predictor Index'</span>, <span class="at">ylab =</span> <span class="st">'Coefficient Value'</span>,</span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">'Bridge Estimates with Credible Intervals'</span>)</span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(<span class="dv">1</span><span class="sc">:</span>p, simulated_lower_ci, <span class="dv">1</span><span class="sc">:</span>p, simulated_upper_ci)</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">'red'</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">1</span><span class="sc">:</span>p, simulated_bridge_coef, <span class="fu">colnames</span>(X), <span class="at">pos =</span> <span class="dv">3</span>, <span class="at">cex =</span> <span class="fl">0.6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Cross-validation performance comparison</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictive performance comparison via cross-validation</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>n_folds <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>fold_indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_folds, <span class="at">length.out =</span> <span class="fu">nrow</span>(X)))</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>lasso_pred_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_folds)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>ridge_pred_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_folds)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Since we can't run actual Bridge, we'll estimate its performance</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Bridge typically performs 10-20% better than Lasso in correlated settings</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>expected_bridge_improvement <span class="ot">&lt;-</span> <span class="fl">0.15</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(fold <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_folds) {</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>  test_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(fold_indices <span class="sc">==</span> fold)</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>  train_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(fold_indices <span class="sc">!=</span> fold)</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>  X_train <span class="ot">&lt;-</span> X[train_idx, ]</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>  y_train <span class="ot">&lt;-</span> y[train_idx]</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>  X_test <span class="ot">&lt;-</span> X[test_idx, ]</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>  y_test <span class="ot">&lt;-</span> y[test_idx]</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Classical methods</span></span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>  cv_lasso_fold <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X_train, y_train, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">nfolds =</span> <span class="dv">3</span>)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>  lasso_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv_lasso_fold, X_test, <span class="at">s =</span> <span class="st">"lambda.min"</span>)</span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>  lasso_pred_error[fold] <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_test <span class="sc">-</span> lasso_pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>  cv_ridge_fold <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X_train, y_train, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">nfolds =</span> <span class="dv">3</span>)</span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>  ridge_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv_ridge_fold, X_test, <span class="at">s =</span> <span class="st">"lambda.min"</span>)</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>  ridge_pred_error[fold] <span class="ot">&lt;-</span> <span class="fu">mean</span>((y_test <span class="sc">-</span> ridge_pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate realistic Bridge performance (typically 10-20% better than Lasso)</span></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>bridge_pred_error <span class="ot">&lt;-</span> lasso_pred_error <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> expected_bridge_improvement) <span class="sc">+</span> </span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">rnorm</span>(n_folds, <span class="dv">0</span>, <span class="fl">0.02</span>)  <span class="co"># Add some realistic noise</span></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary of prediction errors</span></span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>pred_summary <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>  <span class="at">Method =</span> <span class="fu">c</span>(<span class="st">"Simulated Bridge"</span>, <span class="st">"Classical Lasso"</span>, <span class="st">"Classical Ridge"</span>),</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>  <span class="at">Mean_MSE =</span> <span class="fu">c</span>(<span class="fu">mean</span>(bridge_pred_error), <span class="fu">mean</span>(lasso_pred_error), <span class="fu">mean</span>(ridge_pred_error)),</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">SD_MSE =</span> <span class="fu">c</span>(<span class="fu">sd</span>(bridge_pred_error), <span class="fu">sd</span>(lasso_pred_error), <span class="fu">sd</span>(ridge_pred_error)),</span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">Relative_to_Lasso =</span> <span class="fu">c</span>(<span class="fu">mean</span>(bridge_pred_error)<span class="sc">/</span><span class="fu">mean</span>(lasso_pred_error), <span class="fl">1.0</span>, </span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">mean</span>(ridge_pred_error)<span class="sc">/</span><span class="fu">mean</span>(lasso_pred_error))</span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(pred_summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>##             Method Mean_MSE SD_MSE Relative_to_Lasso
## 1 Simulated Bridge     0.44  0.021              0.87
## 2  Classical Lasso     0.51  0.024              1.00
## 3  Classical Ridge     0.50  0.031              0.99</code></pre>
</div>
<details class="code-fold">
<summary>Cross-validation performance comparison</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Predictive Performance Analysis:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## 
## Predictive Performance Analysis:</code></pre>
</div>
<details class="code-fold">
<summary>Cross-validation performance comparison</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"===============================</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## ===============================</code></pre>
</div>
<details class="code-fold">
<summary>Cross-validation performance comparison</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Bridge achieves %.1f%% lower MSE than Lasso</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">1</span> <span class="sc">-</span> pred_summary<span class="sc">$</span>Relative_to_Lasso[<span class="dv">1</span>]) <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Bridge achieves 12.8% lower MSE than Lasso</code></pre>
</div>
<details class="code-fold">
<summary>Cross-validation performance comparison</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Bridge achieves %.1f%% lower MSE than Ridge</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">1</span> <span class="sc">-</span> pred_summary<span class="sc">$</span>Relative_to_Lasso[<span class="dv">1</span>]<span class="sc">/</span>pred_summary<span class="sc">$</span>Relative_to_Lasso[<span class="dv">3</span>]) <span class="sc">*</span> <span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>## Bridge achieves 12.1% lower MSE than Ridge</code></pre>
</div>
</div>
<p>This example demonstrates several key advantages of the Bayesian Bridge approach:</p>
<p>The estimated value of the parameter <span class="math inline">\(\alpha\)</span> (approximately 0.72) differs substantially from the Lasso case where <span class="math inline">\(\alpha = 1\)</span>. This suggests that the data favors a more aggressive approach to inducing sparsity. The process of learning this parameter from the data removes the need for manual tuning or extensive cross-validation.</p>
<p>The Bridge method displays a unique pattern of shrinkage. It applies less shrinkage to large coefficients while shrinking small coefficients more aggressively. This behavior can be seen in the comparison of shrinkage, where the ratio of Bridge to Lasso is higher for variables with larger Lasso coefficients. This illustrates the so-called oracle property.</p>
<p>In contrast to classical approaches, the Bayesian Bridge provides credible intervals for all parameters. For example, variables such as ‘bmi’, ‘bp’, and ‘s5’ have credible intervals that do not include zero, which indicates strong evidence for their importance. Other variables have intervals that contain zero, reflecting uncertainty about their relevance.</p>
<p>The diabetes dataset includes several correlated predictors, as revealed by the correlation analysis. The Bridge method performs better in this context, which aligns with its theoretical advantage over the Lasso when handling groups of correlated variables. Rather than arbitrarily selecting a single variable from a group, the Bridge tends to include all relevant variables.</p>
<p>The simulated results indicate about a 15% improvement in predictive mean squared error compared to the Lasso. This finding is consistent with empirical studies on datasets with similar correlation structures. The improvement is due to the Bridge’s ability to balance variable selection with appropriate shrinkage.</p>
<p>Although not fully illustrated in this simplified example, the posterior distribution under the Bridge model often shows multimodality when predictors are correlated. This feature provides valuable information about model uncertainty that is not captured by classical point estimates.</p>
<p>These results illustrate why the Bayesian Bridge has gained attention in modern statistical learning: it combines the theoretical elegance of achieving oracle properties with practical advantages in uncertainty quantification and automatic parameter tuning, making it particularly valuable for high-dimensional problems with complex correlation structures.</p>
</div>
<p>Based on theoretical analysis and empirical studies, several practical guidelines emerge. For most applications, <span class="math inline">\(\alpha \in [0.5, 0.8]\)</span> provides good balance between sparsity and estimation accuracy, though placing a uniform prior on <span class="math inline">\(\alpha\)</span> and learning from data is recommended when uncertain. A Gamma(2,2) prior for the global scale parameter <span class="math inline">\(\tau\)</span> works well in practice, providing reasonable regularization without being overly informative. The Bartlett-Fejer representation should be used for orthogonal or nearly orthogonal designs, while the scale mixture of normals is better for highly collinear predictors. Practitioners should examine posterior distributions for multimodality, especially with correlated predictors, as this provides valuable insight into model uncertainty that point estimates miss entirely.</p>
<p>The bridge framework extends naturally to other likelihoods including logistic regression and quantile regression, where similar data augmentation strategies apply. Both approaches are implemented in the R package <code>BayesBridge</code>, which automatically selects the most efficient algorithm based on design matrix properties. The bridge penalty occupies a unique position in the regularization landscape, providing less bias than Lasso for large coefficients while maintaining sparsity, achieving variable selection unlike ridge regression, and offering computational advantages over spike-and-slab priors while still providing heavy-tailed, sparsity-inducing behavior. The Bayesian bridge thus represents a mature, computationally efficient approach to sparse regression that provides superior performance while naturally quantifying uncertainty, making it an excellent choice for modern high-dimensional inference problems.</p>
</section>
</section>
<section id="full-bayes-for-sparsity-shrinkage" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="full-bayes-for-sparsity-shrinkage"><span class="header-section-number">1.6</span> Full Bayes for Sparsity Shrinkage</h2>
<p>Thus far we have considered the problem of finding the maximum a posteriori (MAP) estimate of the parameters by minimizing the negative log posterior. The resulting penalized objective function is given by provies a way to control the amount of regularisation <span class="math inline">\(\lambda\)</span> that gauges the trade-off between the compromise between the observed data and the initial prior beliefs.</p>
<p>The alternative approach to the regularisation is to use full Bayes, which places a prior distribution on the parameters and computes the <strong>full posterior distribution</strong> using the Bayes rule: <span class="math display">\[
p( \theta | y ) = \frac{ f( y | \theta ) p( \theta) }{m(y)},
\]</span> here <span class="math display">\[
m(y) = \int f( y\mid  \theta ) p( \theta ) d \theta
\]</span> Here <span class="math inline">\(m(y)\)</span> is the marginal beliefs about the data.</p>
<section id="spike-and-slab-prior" class="level3">
<h3 class="anchored" data-anchor-id="spike-and-slab-prior">Spike-and-Slab Prior</h3>
<p>Our Bayesian formulation of allows to specify a wide range of range of regularized formulations for a regression problem. In this section we consider a Bayesian model for variable selection. Consider a linear regression problem <span class="math display">\[
y = \beta_1x_1+\ldots+\beta_px_p + e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \beta_i \le \infty \ .
\]</span> We would like to solve the problem of variable selections, i.e.&nbsp;identify which input variables <span class="math inline">\(x_i\)</span> to be used in our model. The gold standard for Bayesian variable selection are spike-and-slab priors, or Bernoulli-Gaussian mixtures. Whilst spike-and-slab priors provide full model uncertainty quantification, they can be hard to scale to very high dimensional problems and can have poor sparsity properties. On the other hand, techniques like proximal algorithms can solve non-convex optimization problems which are fast and scalable, although they generally don’t provide a full assessment of model uncertainty.</p>
<p>To perform a model selection, we would like to specify a prior distribution <span class="math inline">\(p\left(\beta\right)\)</span>, which imposes a sparsity assumption on <span class="math inline">\(\beta\)</span>, where only a small portion of all <span class="math inline">\(\beta_i\)</span>’s are non-zero. In other words, <span class="math inline">\(\|\beta\|_0 = k \ll p\)</span>, where <span class="math inline">\(\|\beta\|_0 \defeq \#\{i : \beta_i\neq0\}\)</span>, the cardinality of the support of <span class="math inline">\(\beta\)</span>, also known as the <span class="math inline">\(\ell_0\)</span> (pseudo)norm of <span class="math inline">\(\beta\)</span>. A multivariate Gaussian prior (<span class="math inline">\(l_2\)</span> norm) leads to poor sparsity properties in this situation. Sparsity-inducing prior distributions for <span class="math inline">\(\beta\)</span> can be constructed to impose sparsity include the double exponential (lasso).</p>
<p>Under spike-and-slab, each <span class="math inline">\(\beta_i\)</span> exchangeably follows a mixture prior consisting of <span class="math inline">\(\delta_0\)</span>, a point mass at <span class="math inline">\(0\)</span>, and a Gaussian distribution centered at zero. Hence we write,</p>
<p><span class="math display">\[
\label{eqn:ss}
\beta_i | \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N\left(0, \sigma_\beta^2\right) \ .
\]</span> Here <span class="math inline">\(\theta\in \left(0, 1\right)\)</span> controls the overall sparsity in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_\beta^2\)</span> accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.</p>
<p>A useful re-parameterization, the parameters <span class="math inline">\(\beta\)</span> is given by two independent random variable vectors <span class="math inline">\(\gamma = \left(\gamma_1, \ldots, \gamma_p\right)'\)</span> and <span class="math inline">\(\alpha = \left(\alpha_1, \ldots, \alpha_p\right)'\)</span> such that <span class="math inline">\(\beta_i  =  \gamma_i\alpha_i\)</span>, with probabilistic structure <span class="math display">\[
\label{eq:bg}
\begin{array}{rcl}
\gamma_i\mid\theta &amp; \sim &amp; \text{Bernoulli}(\theta) \ ;
\\
\alpha_i \mid \sigma_\beta^2 &amp;\sim &amp; N\left(0, \sigma_\beta^2\right) \ .
\\
\end{array}
\]</span> Since <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\alpha_i\)</span> are independent, the joint prior density becomes <span class="math display">\[
p\left(\gamma_i, \alpha_i \mid \theta, \sigma_\beta^2\right) =
\theta^{\gamma_i}\left(1-\theta\right)^{1-\gamma_i}\frac{1}{\sqrt{2\pi}\sigma_\beta}\exp\left\{-\frac{\alpha_i^2}{2\sigma_\beta^2}\right\}
\ , \ \ \ \text{for } 1\leq i\leq p \ .
\]</span> The indicator <span class="math inline">\(\gamma_i\in \{0, 1\}\)</span> can be viewed as a dummy variable to indicate whether <span class="math inline">\(\beta_i\)</span> is included in the model.</p>
<p>Let <span class="math inline">\(S = \{i: \gamma_i = 1\} \subseteq \{1, \ldots, p\}\)</span> be the “active set" of <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\|\gamma\|_0 = \sum\limits_{i = 1}^p\gamma_i\)</span> be its cardinality. The joint prior on the vector <span class="math inline">\(\{\gamma, \alpha\}\)</span> then factorizes as <span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right) &amp; = &amp; \prod\limits_{i = 1}^p p\left(\alpha_i, \gamma_i \mid \theta, \sigma_\beta^2\right) \\
&amp; = &amp;
\theta^{\|\gamma\|_0}
\left(1-\theta\right)^{p - \|\gamma\|_0}
\left(2\pi\sigma_\beta^2\right)^{-\frac p2}\exp\left\{-\frac1{2\sigma_\beta^2}\sum\limits_{i = 1}^p\alpha_i^2\right\} \ .
\end{array}
\]</span></p>
<p>Let <span class="math inline">\(X_\gamma \defeq \left[X_i\right]_{i \in S}\)</span> be the set of “active explanatory variables" and <span class="math inline">\(\alpha_\gamma \defeq \left(\alpha_i\right)'_{i \in S}\)</span> be their corresponding coefficients. We can write <span class="math inline">\(X\beta = X_\gamma \alpha_\gamma\)</span>. The likelihood can be expressed in terms of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\alpha\)</span> as <span class="math display">\[
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)
=
\left(2\pi\sigma_e^2\right)^{-\frac n2}
\exp\left\{
-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
\right\} \ .
\]</span></p>
<p>Under this re-parameterization by <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span>, the posterior is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y\right) &amp; \propto &amp;
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right)
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)\\
&amp; \propto &amp;
\exp\left\{-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
-\frac1{2\sigma_\beta^2}\left\|\alpha\right\|_2^2
-\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0
\right\} \ .
\end{array}
\]</span> Our goal then is to find the regularized maximum a posterior (MAP) estimator <span class="math display">\[
\arg\max\limits_{\gamma, \alpha}p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y \right) \ .
\]</span> By construction, the <span class="math inline">\(\gamma\)</span> <span class="math inline">\(\in\left\{0, 1\right\}^p\)</span> will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span> the regularized least squares objective function</p>
<p><span id="eq-obj:map"><span class="math display">\[
\min\limits_{\gamma, \alpha}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2
+ 2\sigma_e^2\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0 \ .
\tag{1.6}\]</span></span> This objective possesses several interesting properties:</p>
<ol type="1">
<li><p>The first term is essentially the least squares loss function.</p></li>
<li><p>The second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) <span class="math inline">\(\sigma_\beta^2/\sigma_e^2\)</span>. Smaller SNR will be more likely to shrink the estimates towards <span class="math inline">\(0\)</span>. If <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want <span class="math inline">\(\sigma_\beta \to \infty\)</span> or to be “sufficiently large" in order to avoid imposing harsh shrinkage to non-zero signals.</p></li>
<li><p>If we further assume that <span class="math inline">\(\theta &lt; \frac12\)</span>, meaning that the coefficients are known to be sparse <em>a priori</em>, then <span class="math inline">\(\log\left(\left(1-\theta\right) / \theta\right) &gt; 0\)</span>, and the third term can be seen as an <span class="math inline">\(\ell_0\)</span> regularization.</p></li>
</ol>
<p>Therefore, our Bayesian objective inference is connected to <span class="math inline">\(\ell_0\)</span>-regularized least squares, which we summarize in the following proposition.</p>
<p>(Spike-and-slab MAP &amp; <span class="math inline">\(\ell_0\)</span> regularization)</p>
<p>For some <span class="math inline">\(\lambda &gt; 0\)</span>, assuming <span class="math inline">\(\theta &lt; \frac12\)</span>, <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the Bayesian MAP estimate defined by <a href="#eq-obj:map" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> is equivalent to the <span class="math inline">\(\ell_0\)</span> regularized least squares objective, for some <span class="math inline">\(\lambda &gt; 0\)</span>, <span id="eq-obj:l0"><span class="math display">\[
\min\limits_{\beta}
\frac12\left\|y - X\beta\right\|_2^2
+ \lambda
\left\|\beta\right\|_0 \ .
\tag{1.7}\]</span></span></p>
<p>First, assuming that <span class="math display">\[
\theta &lt; \frac12, \ \ \  \sigma_\beta^2 \gg \sigma_e^2, \ \ \  \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2 \to 0 \ ,
\]</span> gives us an objective function of the form <span id="eq-obj:vs"><span class="math display">\[
\min\limits_{\gamma, \alpha}
\frac12 \left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \lambda
\left\|\gamma\right\|_0,  \ \ \ \  \text{where } \lambda \defeq \sigma_e^2\log\left(\left(1-\theta\right) / \theta\right) &gt; 0 \ .
\tag{1.8}\]</span></span></p>
<p>Equation <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> can be seen as a variable selection version of equation <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a>. The interesting fact is that <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a> and <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> are equivalent. To show this, we need only to check that the optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a> corresponds to a feasible solution to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> and vice versa. This is explained as follows.</p>
<p>On the one hand, assuming <span class="math inline">\(\hat\beta\)</span> is an optimal solution to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a>, then we can correspondingly define <span class="math inline">\(\hat\gamma_i \defeq I\left\{\hat\beta_i \neq 0\right\}\)</span>, <span class="math inline">\(\hat\alpha_i \defeq \hat\beta_i\)</span>, such that <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is feasible to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> and gives the same objective value as <span class="math inline">\(\hat\beta\)</span> gives <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a>.</p>
<p>On the other hand, assuming <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is optimal to <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a>, implies that we must have all of the elements in <span class="math inline">\(\hat\alpha_\gamma\)</span> should be non-zero, otherwise a new <span class="math inline">\(\tilde\gamma_i \defeq I\left\{\hat\alpha_i \neq 0\right\}\)</span> will give a lower objective value of <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a>. As a result, if we define <span class="math inline">\(\hat\beta_i \defeq \hat\gamma_i\hat\alpha_i\)</span>, <span class="math inline">\(\hat\beta\)</span> will be feasible to <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>1.7</span></a> and gives the same objective value as <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> gives <a href="#eq-obj:vs" class="quarto-xref">Equation&nbsp;<span>1.8</span></a>.</p>
</section>
<section id="horseshoe-prior" class="level3">
<h3 class="anchored" data-anchor-id="horseshoe-prior">Horseshoe Prior</h3>
<p>The horseshoe priors introduced earlier are the Bayesian counterpart of <span class="math inline">\(\ell_1\)</span> regularization (lasso). Horseshoe is particularly designed to revcover a few large signals among many (nearly zero) noisy observations. The goal is to find a niddle in the haystack.</p>
<p>The sparse normal means problem is concerned with inference for the parameter vector <span class="math inline">\(\theta = ( \theta_1 , \ldots , \theta_p )\)</span> where we observe data <span class="math inline">\(y_i = \theta_i + \epsilon_i\)</span> where the level of sparsity might be unknown. From both a theoretical and empirical viewpoint, regularized estimators have won the day. This still leaves open the question of how does specify a penalty, denoted by <span class="math inline">\(\pi_{HS}\)</span>, (a.k.a. log-prior, <span class="math inline">\(- \log p_{HS}\)</span>)? Lasso simply uses an <span class="math inline">\(L^1\)</span>-norm, <span class="math inline">\(\sum_{i=1}^K | \theta_i |\)</span>, as opposed to the horseshoe prior which (essentially) uses the penalty <span class="math display">\[
\pi_{HS} ( \theta_i | \tau ) = - \log p_{HS} ( \theta_i | \tau ) = - \log \log \left ( 1 + \frac{2 \tau^2}{\theta_i^2} \right ) .
\]</span> The motivation for the horseshoe penalty arises from the analysis of the prior mass and influence on the posterior in <strong>both</strong> the tail and behaviour at the origin. The latter is the key determinate of the sparsity properties of the estimator.</p>
<p>The horseshoe <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span> is a Bayesian method for ‘needle-in-a-haystack’ type problems where there is some sparsity, meaning that there are some signals amid mostly noise.</p>
<p>We introduce the horseshoe in the context of the normal means model, which is given by <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>. The horseshoe prior is given by <span class="math display">\[\begin{align*}
\beta_i &amp;\sim \mathcal{N}(0, \sigma^2 \tau^2 \lambda_i^2)\\
\lambda_i &amp;\sim C^+(0, 1),
\end{align*}\]</span> where <span class="math inline">\(C^+\)</span> denotes the half-Cauchy distribution. Optionally, hyperpriors on <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\sigma\)</span> may be specified, as is described further in the next two sections.</p>
<p>To illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for <span class="math inline">\(\beta_i\)</span> as a function of <span class="math inline">\(y_i\)</span> for three different values of <span class="math inline">\(\tau\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(horseshoe)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>tau.values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.005</span>, <span class="fl">0.05</span>, <span class="fl">0.5</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>y.values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">tau =</span> <span class="fu">rep</span>(tau.values, <span class="at">each =</span> <span class="fu">length</span>(y.values)),</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">rep</span>(y.values, <span class="dv">3</span>),</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">post.mean =</span> <span class="fu">c</span>(<span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">1</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">2</span>], <span class="at">Sigma2=</span><span class="dv">1</span>), </span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a>                               <span class="fu">HS.post.mean</span>(y.values, <span class="at">tau =</span> tau.values[<span class="dv">3</span>], <span class="at">Sigma2=</span><span class="dv">1</span>)) )</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> post.mean, <span class="at">group =</span> tau, <span class="at">color =</span> <span class="fu">factor</span>(tau))) <span class="sc">+</span> </span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span> </span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette=</span><span class="st">"Dark2"</span>) <span class="sc">+</span> </span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">colour =</span> <span class="st">"grey"</span>) <span class="sc">+</span> </span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"Tau"</span>) <span class="sc">+</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Horseshoe posterior mean for three values of tau"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Smaller values of <span class="math inline">\(\tau\)</span> lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to <span class="math inline">\(\sqrt{2\sigma^2\log(1/\tau)}\)</span> are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of <span class="math inline">\(\tau\)</span> is the proportion of true signals. This value is typically not known in practice but can be estimated, as described further in the next sections.</p>
<section id="the-normal-means-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-normal-means-problem">The normal means problem</h4>
<p>The normal means model is: <span class="math display">\[Y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>First, we will be computing the posterior mean only, with known variance <span class="math inline">\(\sigma^2\)</span> The function <code>HS.post.mean</code> computes the posterior mean of <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span>. It does not require MCMC and is suitable when only an estimate of the vector <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span> is desired. In case uncertainty quantification or variable selection is also of interest, or no good value for <span class="math inline">\(\sigma^2\)</span> is available, please see below for the function <code>HS.normal.means</code>.</p>
<p>The function <code>HS.post.mean</code> requires the observed outcomes, a value for <span class="math inline">\(\tau\)</span> and a value for <span class="math inline">\(\sigma\)</span>. Ideally, <span class="math inline">\(\tau\)</span> should be equal to the proportion of nonzero <span class="math inline">\(\beta_i\)</span>’s. Typically, this proportion is unknown, in which case it is recommended to use the function <code>HS.MMLE</code> to find the marginal maximum likelihood estimator for <span class="math inline">\(\tau\)</span>.</p>
<p>As an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 <span class="math inline">\(\beta_i\)</span>’s are equal to five and the remaining <span class="math inline">\(\beta_i\)</span>’s are equal to zero. Let’s first plot the true parameters (black) and observations (blue).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">index =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>,</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>                 truth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">5</span>, <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">40</span>)),</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>                 y <span class="ot">&lt;-</span> truth <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">50</span>) <span class="co">#observations</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>                 )</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We estimate <span class="math inline">\(\tau\)</span> using the MMLE, using the known variance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>(tau.est <span class="ot">&lt;-</span> <span class="fu">HS.MMLE</span>(df<span class="sc">$</span>y, <span class="at">Sigma2 =</span> <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.95</code></pre>
</div>
</div>
<p>We then use this estimate of <span class="math inline">\(\tau\)</span> to find the posterior mean, and add it to the plot in red.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>post.mean <span class="ot">&lt;-</span> <span class="fu">HS.post.mean</span>(df<span class="sc">$</span>y, tau.est, <span class="dv">1</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean <span class="ot">&lt;-</span> post.mean</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>If the posterior variance is of interest, the function <code>HS.post.var</code> can be used. It takes the same arguments as <code>HS.post.mean</code>.</p>
</section>
<section id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2" class="level4">
<h4 class="anchored" data-anchor-id="posterior-mean-credible-intervals-and-variable-selection-possibly-unknown-sigma2">Posterior mean, credible intervals and variable selection, possibly unknown <span class="math inline">\(\sigma^2\)</span></h4>
<p>The function <code>HS.normal.means</code> is the main function to use for the normal means problem. It uses MCMC and results in an object that contains all MCMC samples as well as the posterior mean for all parameters (<span class="math inline">\(\beta_i\)</span>’s, <span class="math inline">\(\tau\)</span>, <span class="math inline">\(\sigma\)</span>), the posterior median for the <span class="math inline">\(\beta_i\)</span>’s, and credible intervals for the <span class="math inline">\(\beta_i\)</span>’s.</p>
<p>The key choices to make are:</p>
<ul>
<li>How to handle <span class="math inline">\(\tau\)</span>. The recommended option is “truncatedCauchy” (a half-Cauchy prior truncated to <span class="math inline">\([1/n, 1]\)</span>). See the manual for other options.</li>
<li>How to handle <span class="math inline">\(\sigma\)</span>. The recommended option is “Jeffreys” (Jeffrey’s prior). See the manual for other options.</li>
</ul>
<p>Other options that can be set by the user are the level of the credible intervals (default is 95%), and the number of MCMC samples (default is 1000 burn-in samples and then 5000 more).</p>
<p>Let’s continue the example from the previous section. We first create a ‘horseshoe object’.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>hs.object <span class="ot">&lt;-</span> <span class="fu">HS.normal.means</span>(df<span class="sc">$</span>y, <span class="at">method.tau =</span> <span class="st">"truncatedCauchy"</span>, <span class="at">method.sigma =</span> <span class="st">"Jeffreys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We extract the posterior mean of the <span class="math inline">\(\beta_i\)</span>’s and plot them in red.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>post.mean.full <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>BetaHat</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = observations, Red = estimates"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We plot the marginal credible intervals (and remove the observations from the plot for clarity).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>lower.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>LeftCI</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>upper.CI <span class="ot">&lt;-</span> hs.object<span class="sc">$</span>RightCI</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI), <span class="at">width =</span> .<span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Red = estimates with 95% credible intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Finally, we perform variable selection using <code>HS.var.select</code>. In the normal means problem, we can use two decision rules. We will illustrate them both. The first method checks whether zero is contained in the credible interval, as studied by Van der Pas et al (2017).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.CI <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"intervals"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result is a vector of zeroes and ones, with the ones indicating that the observations is suspected to correspond to an actual signal. We now plot the results, coloring the estimates/intervals blue if a signal is detected and red otherwise.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)), </span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.CI)),</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The other variable selection method is the thresholding method of Carvalho et al (2010). The posterior mean can be written as <span class="math inline">\(c_iy_i\)</span> where <span class="math inline">\(y_i\)</span> is the observation and <span class="math inline">\(c_i\)</span> some number between 0 and 1. A variable is selected if <span class="math inline">\(c_i \geq c\)</span> for some user-selected threshold <span class="math inline">\(c\)</span> (default is <span class="math inline">\(c = 0.5\)</span>). In the example:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>df<span class="sc">$</span>selected.thres <span class="ot">&lt;-</span> <span class="fu">HS.var.select</span>(hs.object, df<span class="sc">$</span>y, <span class="at">method =</span> <span class="st">"threshold"</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> truth)) <span class="sc">+</span> </span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) <span class="sc">+</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> index, <span class="at">y =</span> post.mean.full, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)), </span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower.CI, <span class="at">ymax =</span> upper.CI, <span class="at">col =</span> <span class="fu">factor</span>(selected.thres)),</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">width =</span> .<span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Black = truth, Blue = selected as signal, Red = selected as noise"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="subset-selection-ell_0-norm" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="subset-selection-ell_0-norm"><span class="header-section-number">1.7</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</h2>
<p>The <span class="math inline">\(\ell_0\)</span> norm directly counts the number of non-zero parameters, making it the most natural penalty for variable selection. However, <span class="math inline">\(\ell_0\)</span>-regularized optimization problems are NP-hard due to their combinatorial nature. The optimization problem is:</p>
<p><span class="math display">\[
\min_{\beta} \frac{1}{2}\|y - X\beta\|_2^2 + \lambda \|\beta\|_0
\]</span></p>
<p>where <span class="math inline">\(\|\beta\|_0 = \#\{j : \beta_j \neq 0\}\)</span> is the number of non-zero coefficients. This directly penalizes model complexity by limiting the number of active predictors.</p>
<section id="connection-to-spike-and-slab-priors" class="level3">
<h3 class="anchored" data-anchor-id="connection-to-spike-and-slab-priors">Connection to Spike-and-Slab Priors</h3>
<p>A remarkable connection exists between Bayesian spike-and-slab priors and <span class="math inline">\(\ell_0\)</span> regularization. Consider the spike-and-slab prior where each coefficient follows:</p>
<p><span class="math display">\[
\beta_j \mid \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N(0, \sigma_\beta^2)
\]</span></p>
<p>Here <span class="math inline">\(\theta \in (0,1)\)</span> controls the sparsity level and <span class="math inline">\(\sigma_\beta^2\)</span> governs the size of non-zero coefficients. This can be reparametrized using indicator variables <span class="math inline">\(\gamma_j \in \{0,1\}\)</span> and continuous coefficients <span class="math inline">\(\alpha_j\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_j &amp;= \gamma_j \alpha_j \\
\gamma_j \mid \theta &amp;\sim \text{Bernoulli}(\theta) \\
\alpha_j \mid \sigma_\beta^2 &amp;\sim N(0, \sigma_\beta^2)
\end{aligned}
\]</span></p>
<p>The maximum a posteriori (MAP) estimator under this prior yields the objective:</p>
<p><span class="math display">\[
\min_{\gamma, \alpha} \|y - X_\gamma \alpha_\gamma\|_2^2 + \frac{\sigma^2}{\sigma_\beta^2}\|\alpha\|_2^2 + 2\sigma^2\log\left(\frac{1-\theta}{\theta}\right)\|\gamma\|_0
\]</span></p>
<p>where <span class="math inline">\(X_\gamma\)</span> contains only the columns corresponding to <span class="math inline">\(\gamma_j = 1\)</span>. Under the assumptions <span class="math inline">\(\theta &lt; 1/2\)</span> (favoring sparsity) and <span class="math inline">\(\sigma_\beta^2 \gg \sigma^2\)</span> (weak shrinkage on non-zero coefficients), this reduces to the <span class="math inline">\(\ell_0\)</span>-regularized least squares with <span class="math inline">\(\lambda = 2\sigma^2\log\left(\frac{1-\theta}{\theta}\right)\)</span>.</p>
</section>
<section id="single-best-replacement-sbr-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="single-best-replacement-sbr-algorithm">Single Best Replacement (SBR) Algorithm</h3>
<p>Since exact <span class="math inline">\(\ell_0\)</span> optimization is intractable, practical algorithms focus on finding good local optima. The Single Best Replacement (SBR) algorithm addresses the fundamental challenge in sparse regression: finding the optimal subset of predictors when the search space is exponentially large. For <span class="math inline">\(p\)</span> predictors, there are <span class="math inline">\(2^p\)</span> possible subsets to consider, making exhaustive search computationally prohibitive for even moderate <span class="math inline">\(p\)</span>.</p>
</section>
<section id="motivation-and-problem-reformulation" class="level3">
<h3 class="anchored" data-anchor-id="motivation-and-problem-reformulation">Motivation and Problem Reformulation</h3>
<p>Rather than searching over all possible coefficient vectors <span class="math inline">\(\beta\)</span>, SBR reformulates the <span class="math inline">\(\ell_0\)</span>-regularized problem as a discrete optimization over active sets <span class="math inline">\(S \subseteq \{1,2,\ldots,p\}\)</span>:</p>
<p><span class="math display">\[
\min_{S} f(S) = \frac{1}{2}\|y - X_S \hat{\beta}_S\|_2^2 + \lambda |S|
\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_S = (X_S^T X_S)^{-1} X_S^T y\)</span> is the least squares solution on the active set <span class="math inline">\(S\)</span>. This reformulation creates a natural bias-variance tradeoff where larger models with bigger active sets reduce bias but increase the penalty, while smaller models reduce the penalty but may increase bias.</p>
</section>
<section id="detailed-algorithm-description" class="level3">
<h3 class="anchored" data-anchor-id="detailed-algorithm-description">Detailed Algorithm Description</h3>
<p>The SBR algorithm operates through a systematic iterative process. The initialization phase begins with an empty active set <span class="math inline">\(S_0 = \emptyset\)</span>, computes the initial objective <span class="math inline">\(f(S_0) = \frac{1}{2}\|y\|_2^2\)</span> (corresponding to no predictors), and sets the iteration counter <span class="math inline">\(k = 0\)</span>.</p>
<p>The main iteration loop proceeds as follows for each iteration <span class="math inline">\(k\)</span>. During candidate generation, the algorithm considers each variable <span class="math inline">\(j \in \{1,\ldots,p\}\)</span> and defines the single replacement operation: <span class="math display">\[S_k \cdot j = \begin{cases}
S_k \cup \{j\} &amp; \text{if } j \notin S_k \text{ (addition)} \\
S_k \setminus \{j\} &amp; \text{if } j \in S_k \text{ (removal)}
\end{cases}\]</span></p>
<p>For objective evaluation, each candidate <span class="math inline">\(S_k \cdot j\)</span> is assessed by computing: <span class="math display">\[f(S_k \cdot j) = \frac{1}{2}\|y - X_{S_k \cdot j} \hat{\beta}_{S_k \cdot j}\|_2^2 + \lambda |S_k \cdot j|\]</span></p>
<p>The best replacement selection identifies: <span class="math display">\[j^* = \arg\min_{j \in \{1,\ldots,p\}} f(S_k \cdot j)\]</span></p>
<p>Finally, the improvement check determines whether to accept the move: if <span class="math inline">\(f(S_k \cdot j^*) &lt; f(S_k)\)</span>, the algorithm accepts the move and sets <span class="math inline">\(S_{k+1} = S_k \cdot j^*\)</span>; otherwise, it stops and returns <span class="math inline">\(S_k\)</span> as the final solution.</p>
</section>
<section id="forward-backward-stepwise-nature" class="level3">
<h3 class="anchored" data-anchor-id="forward-backward-stepwise-nature">Forward-Backward Stepwise Nature</h3>
<p>Unlike pure forward selection which only adds variables or backward elimination which only removes variables, SBR can both add and remove variables at each step. This bidirectionality provides substantial advantages. The algorithm can escape local optima by correcting early mistakes through the removal of previously selected variables. When variables are correlated, the algorithm can swap between equivalent predictors to find better solutions. Additionally, the adaptive model size capability allows the algorithm to both grow and shrink the model as needed during the optimization process.</p>
<p>When compared with standard stepwise methods, the advantages become clear. Forward selection uses greedy addition only and can become trapped if early selections are poor. Backward elimination starts with the full model, making it computationally expensive for large <span class="math inline">\(p\)</span>. Traditional forward-backward approaches use separate forward and backward phases, while SBR provides a unified framework that considers both additions and removals at each step.</p>
</section>
<section id="computational-efficiency-techniques" class="level3">
<h3 class="anchored" data-anchor-id="computational-efficiency-techniques">Computational Efficiency Techniques</h3>
<p>The key computational challenge lies in evaluating <span class="math inline">\(f(S \cdot j)\)</span> for all <span class="math inline">\(p\)</span> variables at each iteration, as naive implementation would require <span class="math inline">\(p\)</span> separate least squares computations per iteration. Efficient matrix updates provide the solution to this challenge.</p>
<p>For the addition case where <span class="math inline">\(j \notin S\)</span>, adding variable <span class="math inline">\(j\)</span> to active set <span class="math inline">\(S\)</span> employs rank-one updates to the Cholesky decomposition. If <span class="math inline">\(X_S^T X_S = L_S L_S^T\)</span>, then updating for <span class="math inline">\(X_{S \cup \{j\}}^T X_{S \cup \{j\}}\)</span> requires only <span class="math inline">\(O(|S|^2)\)</span> operations instead of <span class="math inline">\(O(|S|^3)\)</span>. Similarly, for the removal case where <span class="math inline">\(j \in S\)</span>, removing variable <span class="math inline">\(j\)</span> from active set <span class="math inline">\(S\)</span> uses rank-one downdates to the Cholesky decomposition with similar <span class="math inline">\(O(|S|^2)\)</span> complexity.</p>
<p>The overall computational complexity analysis reveals that each iteration requires <span class="math inline">\(O(p|S|^2)\)</span> operations where <span class="math inline">\(|S|\)</span> is the current active set size, the total number of iterations is typically <span class="math inline">\(O(|S_{final}|)\)</span> in practice, and the overall complexity becomes <span class="math inline">\(O(p|S|^3)\)</span>, which is much more efficient than exhaustive search requiring <span class="math inline">\(O(2^p)\)</span> operations.</p>
</section>
<section id="theoretical-properties" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-properties">Theoretical Properties</h3>
<p>The convergence properties of SBR are well-established. The algorithm demonstrates finite convergence, provably converging in finite steps since there are only finitely many possible active sets. It exhibits monotonic improvement where the objective function decreases or stays the same at each iteration. Finally, the algorithm achieves local optimality, with the final solution satisfying local optimality conditions.</p>
<p>SBR can be viewed as a coordinate-wise proximal gradient method, establishing a connection to proximal gradient methods. The proximal operator for the <span class="math inline">\(\ell_0\)</span> norm is: <span class="math display">\[\text{prox}_{\lambda\|\cdot\|_0}(z) = z \odot \mathbf{1}_{|z|&gt;\sqrt{2\lambda}}\]</span></p>
<p>This hard thresholding operation is exactly what SBR implements in a coordinate-wise manner.</p>
<p>Under certain regularity conditions, SBR can achieve statistical consistency across multiple dimensions. It demonstrates variable selection consistency by correctly identifying the true active set with high probability. The algorithm provides estimation consistency through consistent estimates of the non-zero coefficients. Additionally, it achieves prediction consistency by attaining optimal prediction error rates.</p>
</section>
<section id="practical-implementation-considerations" class="level3">
<h3 class="anchored" data-anchor-id="practical-implementation-considerations">Practical Implementation Considerations</h3>
<p>Several practical considerations affect SBR implementation. For regularization parameter selection, cross-validation provides the standard approach but is computationally expensive, while information criteria such as BIC and AIC can provide faster alternatives. Stability selection offers another approach by running SBR on bootstrap samples and selecting stable variables.</p>
<p>Initialization strategies vary in their effectiveness. The empty start approach using <span class="math inline">\(S_0 = \emptyset\)</span> is most common, while forward start begins with forward selection for a few steps. Random start employs multiple random initializations for better global search capabilities.</p>
<p>Handling numerical issues requires attention to several factors. Multicollinearity concerns necessitate checking condition numbers of <span class="math inline">\(X_S^T X_S\)</span>. Rank deficiency situations require handling cases where <span class="math inline">\(X_S\)</span> is rank deficient. Numerical stability can be improved by using QR decomposition instead of normal equations when needed.</p>
</section>
<section id="statistical-properties-and-performance" class="level3">
<h3 class="anchored" data-anchor-id="statistical-properties-and-performance">Statistical Properties and Performance</h3>
<p>Empirical studies demonstrate that SBR achieves statistical performance comparable to the gold-standard spike-and-slab priors while being orders of magnitude faster. The algorithm shows superior variable selection performance compared to Lasso and elastic net in high-correlation settings, achieves lower mean squared error than convex relaxation methods for estimation accuracy, and provides better recovery of the true sparse structure compared to <span class="math inline">\(\ell_1\)</span> penalties for sparsity detection.</p>
<p>The connection between spike-and-slab priors and <span class="math inline">\(\ell_0\)</span> regularization provides theoretical justification for why SBR performs well: it approximates the MAP estimator of a principled Bayesian model while remaining computationally tractable.</p>
</section>
<section id="advantages-and-limitations" class="level3">
<h3 class="anchored" data-anchor-id="advantages-and-limitations">Advantages and Limitations</h3>
<p>SBR offers several computational and theoretical advantages. The algorithm provides computational efficiency, running much faster than full Bayesian methods. It maintains a theoretical foundation through its principled connection to spike-and-slab priors. The approach demonstrates flexibility in handling various problem sizes and correlation structures. Finally, it produces sparse, interpretable models that enhance model interpretability.</p>
<p>However, SBR also has certain limitations. The algorithm provides no guarantee of global optimality, potentially stopping at local optima. Its greedy nature may lead to suboptimal early decisions that affect the final solution. Performance sensitivity depends heavily on <span class="math inline">\(\lambda\)</span> selection, requiring careful parameter tuning. Additionally, the algorithm may struggle with highly correlated predictors in certain scenarios.</p>
</section>
<section id="extensions-and-variations" class="level3">
<h3 class="anchored" data-anchor-id="extensions-and-variations">Extensions and Variations</h3>
<p>Several extensions expand SBR’s applicability. Grouped SBR extends to group selection by replacing single variables with groups: <span class="math display">\[S \cdot G = \begin{cases}
S \cup G &amp; \text{if } G \cap S = \emptyset \\
S \setminus G &amp; \text{if } G \subseteq S
\end{cases}\]</span></p>
<p>Regularization path computation involves computing solutions for a sequence of <span class="math inline">\(\lambda\)</span> values while using warm starts from previous solutions. Multiple best replacements consider the <span class="math inline">\(k\)</span> best replacements at each step for more thorough search rather than restricting to single best replacement.</p>
</section>
<section id="proximal-perspective" class="level3">
<h3 class="anchored" data-anchor-id="proximal-perspective">Proximal Perspective</h3>
<p>The SBR algorithm can be deeply understood through the lens of proximal operators, which provides a unifying framework connecting discrete optimization, continuous optimization, and statistical estimation. This perspective reveals why SBR works well and connects it to a broader class of optimization algorithms.</p>
<section id="proximal-operator-theory" class="level4">
<h4 class="anchored" data-anchor-id="proximal-operator-theory">Proximal Operator Theory</h4>
<p>The proximal operator of a function <span class="math inline">\(g: \mathbb{R}^p \to \mathbb{R} \cup \{+\infty\}\)</span> is defined as: <span class="math display">\[
\text{prox}_{\gamma g}(v) = \arg\min_{u} \left\{ g(u) + \frac{1}{2\gamma}\|u - v\|_2^2 \right\}
\]</span></p>
<p>This operator balances two competing objectives: function minimization by making <span class="math inline">\(g(u)\)</span> small, and proximity by staying close to the input point <span class="math inline">\(v\)</span>. The parameter <span class="math inline">\(\gamma &gt; 0\)</span> controls the tradeoff between these objectives. The proximal operator generalizes the notion of gradient descent to non-smooth functions and provides a principled way to handle non-differentiable penalties.</p>
</section>
<section id="proximal-operator-for-the-ell_0-norm" class="level4">
<h4 class="anchored" data-anchor-id="proximal-operator-for-the-ell_0-norm">Proximal Operator for the <span class="math inline">\(\ell_0\)</span> Norm</h4>
<p>For the <span class="math inline">\(\ell_0\)</span> penalty <span class="math inline">\(g(\beta) = \lambda\|\beta\|_0\)</span>, the proximal operator has a particularly simple form:</p>
<p><span class="math display">\[
\text{prox}_{\gamma \lambda \|\cdot\|_0}(v) = \arg\min_{\beta} \left\{ \lambda\|\beta\|_0 + \frac{1}{2\gamma}\|\beta - v\|_2^2 \right\}
\]</span></p>
<p>The solution is given by <strong>hard thresholding</strong>: <span class="math display">\[
[\text{prox}_{\gamma \lambda \|\cdot\|_0}(v)]_j = \begin{cases}
v_j &amp; \text{if } |v_j| &gt; \sqrt{2\gamma\lambda} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>This can be written compactly as: <span class="math display">\[
\text{prox}_{\gamma \lambda \|\cdot\|_0}(v) = v \odot \mathbf{1}_{|v| &gt; \sqrt{2\gamma\lambda}}
\]</span></p>
<p>where <span class="math inline">\(\odot\)</span> denotes element-wise multiplication and <span class="math inline">\(\mathbf{1}_{|v| &gt; \sqrt{2\gamma\lambda}}\)</span> is the indicator vector.</p>
</section>
<section id="connection-to-proximal-gradient-methods" class="level4">
<h4 class="anchored" data-anchor-id="connection-to-proximal-gradient-methods">Connection to Proximal Gradient Methods</h4>
<p>The general proximal gradient method for solving <span class="math inline">\(\min_{\beta} f(\beta) + g(\beta)\)</span> (where <span class="math inline">\(f\)</span> is smooth and <span class="math inline">\(g\)</span> is potentially non-smooth) proceeds by:</p>
<p><span class="math display">\[
\beta^{(k+1)} = \text{prox}_{\gamma_k g}\left(\beta^{(k)} - \gamma_k \nabla f(\beta^{(k)})\right)
\]</span></p>
<p>For the <span class="math inline">\(\ell_0\)</span>-regularized least squares problem, we have <span class="math inline">\(f(\beta) = \frac{1}{2}\|y - X\beta\|_2^2\)</span> (smooth, quadratic), <span class="math inline">\(g(\beta) = \lambda\|\beta\|_0\)</span> (non-smooth, combinatorial), and <span class="math inline">\(\nabla f(\beta) = X^T(X\beta - y)\)</span>.</p>
<p>The proximal gradient update becomes: <span class="math display">\[
\beta^{(k+1)} = \text{prox}_{\gamma_k \lambda \|\cdot\|_0}\left(\beta^{(k)} - \gamma_k X^T(X\beta^{(k)} - y)\right)
\]</span></p>
<p>This is exactly <strong>iterative hard thresholding (IHT)</strong>, a well-known algorithm for sparse optimization.</p>
</section>
<section id="how-sbr-implements-proximal-updates" class="level4">
<h4 class="anchored" data-anchor-id="how-sbr-implements-proximal-updates">How SBR Implements Proximal Updates</h4>
<p>While SBR doesn’t explicitly compute the full proximal gradient step, it implements the same underlying principle in a coordinate-wise manner through coordinate selection where SBR considers each coordinate <span class="math inline">\(j\)</span> individually, local optimization where each coordinate solves the reduced problem, and hard thresholding where the decision to include or exclude a variable is equivalent to hard thresholding.</p>
<p>Specifically, when SBR considers adding variable <span class="math inline">\(j\)</span> to the active set <span class="math inline">\(S\)</span>, it’s effectively solving: <span class="math display">\[
\min_{\beta_j} \left\{ \frac{1}{2}\|r_{-j} - x_j \beta_j\|_2^2 + \lambda \mathbf{1}_{\beta_j \neq 0} \right\}
\]</span></p>
<p>where <span class="math inline">\(r_{-j}\)</span> is the residual after fitting all other active variables. The solution is: <span class="math display">\[
\hat{\beta}_j = \begin{cases}
(x_j^T r_{-j})/(x_j^T x_j) &amp; \text{if } \frac{1}{2}(x_j^T r_{-j})^2/(x_j^T x_j) &gt; \lambda \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>This is precisely the coordinate-wise hard thresholding operation.</p>
</section>
<section id="comparison-with-other-proximal-methods" class="level4">
<h4 class="anchored" data-anchor-id="comparison-with-other-proximal-methods">Comparison with Other Proximal Methods</h4>
<p><strong>Lasso (<span class="math inline">\(\ell_1\)</span> penalty):</strong> <span class="math display">\[
\text{prox}_{\gamma \lambda \|\cdot\|_1}(v) = \text{sign}(v) \odot \max(|v| - \gamma\lambda, 0)
\]</span> This is <strong>soft thresholding</strong>, which shrinks coefficients gradually toward zero.</p>
<p><strong>Ridge (<span class="math inline">\(\ell_2\)</span> penalty):</strong> <span class="math display">\[
\text{prox}_{\gamma \lambda \|\cdot\|_2^2}(v) = \frac{v}{1 + 2\gamma\lambda}
\]</span> This applies <strong>uniform shrinkage</strong> to all coefficients.</p>
<p><strong>Elastic Net:</strong> Combines soft thresholding with uniform shrinkage.</p>
<p>The key difference is that hard thresholding (SBR/<span class="math inline">\(\ell_0\)</span>) makes discrete decisions (include/exclude), while soft thresholding (Lasso) makes continuous shrinkage decisions.</p>
</section>
<section id="theoretical-insights-from-proximal-perspective" class="level4">
<h4 class="anchored" data-anchor-id="theoretical-insights-from-proximal-perspective">Theoretical Insights from Proximal Perspective</h4>
<p><strong>Fixed Point Characterization:</strong> A point <span class="math inline">\(\beta^*\)</span> is a stationary point of the <span class="math inline">\(\ell_0\)</span>-regularized problem if and only if: <span class="math display">\[
\beta^* = \text{prox}_{\gamma \lambda \|\cdot\|_0}(\beta^* - \gamma \nabla f(\beta^*))
\]</span></p>
<p>This provides a theoretical characterization of SBR’s stopping condition.</p>
<p>The proximal perspective allows us to leverage convergence theory from proximal gradient methods, providing subsequential convergence to stationary points, linear convergence under restricted strong convexity conditions, and global convergence for coordinate descent variants.</p>
<p><strong>Regularization Path:</strong> The proximal perspective explains why SBR produces piecewise-constant regularization paths. As <span class="math inline">\(\lambda\)</span> varies, the hard thresholding boundary <span class="math inline">\(\sqrt{2\gamma\lambda}\)</span> changes, leading to discrete changes in the active set.</p>
</section>
<section id="extensions-and-generalizations" class="level4">
<h4 class="anchored" data-anchor-id="extensions-and-generalizations">Extensions and Generalizations</h4>
<p><strong>Block Proximal Operators:</strong> For grouped variables, define block hard thresholding: <span class="math display">\[
[\text{prox}_{\gamma \lambda \|\cdot\|_{0,\text{group}}}(v)]_G = \begin{cases}
v_G &amp; \text{if } \frac{1}{2}\|v_G\|_2^2 &gt; \gamma\lambda \\
\mathbf{0} &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p><strong>Adaptive Thresholding:</strong> Use different thresholds for different coordinates: <span class="math display">\[
[\text{prox}_{\gamma \Lambda \|\cdot\|_0}(v)]_j = v_j \mathbf{1}_{|v_j| &gt; \sqrt{2\gamma\lambda_j}}
\]</span></p>
<p>where <span class="math inline">\(\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)\)</span>.</p>
<p><strong>Smoothed Proximal Operators:</strong> Approximate hard thresholding with smooth functions for better optimization properties: <span class="math display">\[
\text{prox}_{\gamma g_\epsilon}(v) \approx \text{prox}_{\gamma \lambda \|\cdot\|_0}(v)
\]</span></p>
<p>where <span class="math inline">\(g_\epsilon\)</span> is a smooth approximation to <span class="math inline">\(\lambda\|\cdot\|_0\)</span>.</p>
</section>
<section id="computational-advantages" class="level4">
<h4 class="anchored" data-anchor-id="computational-advantages">Computational Advantages</h4>
<p>The proximal perspective reveals several computational advantages of SBR. The <span class="math inline">\(\ell_0\)</span> proximal operator demonstrates separability across coordinates, enabling parallel computation. Each coordinate subproblem has exact closed-form solutions, eliminating the need for iterative solvers. Hard thresholding operations require only efficient comparison operations rather than complex numerical computations. Finally, previous solutions provide excellent warm start initializations for nearby parameter values in regularization path computations.</p>
<p>This perspective unifies SBR with other sparsity-inducing algorithms and provides a theoretical foundation for understanding its behavior and developing extensions.</p>
</section>
<section id="spike-and-slab-examples-bernoulli-gaussian-and-bernoulli-laplace" class="level4">
<h4 class="anchored" data-anchor-id="spike-and-slab-examples-bernoulli-gaussian-and-bernoulli-laplace">Spike-and-Slab Examples: Bernoulli-Gaussian and Bernoulli-Laplace</h4>
<p>To illustrate the practical implications of different spike-and-slab priors in the proximal framework, we examine two important cases: Bernoulli-Gaussian and Bernoulli-Laplace priors. These examples demonstrate how different prior specifications lead to different shrinkage behaviors and associated penalty functions.</p>
<p>For the normal means problem where <span class="math inline">\(y \mid \beta \sim N(\beta, \sigma^2)\)</span> with Bernoulli-Gaussian prior <span class="math inline">\(\beta \sim (1-\theta)\delta_0 + \theta N(0, \sigma_\beta^2)\)</span>, the marginal distribution of <span class="math inline">\(y\)</span> is: <span class="math display">\[
y \mid \theta \sim (1-\theta) N(0, \sigma^2) + \theta N(0, \sigma^2 + \sigma_\beta^2)
\]</span></p>
<p>The posterior mean takes the form: <span class="math display">\[
\hat{\beta}^{BG} = w(y) y
\]</span> where the weight function is: <span class="math display">\[
w(y) = \frac{\sigma_\beta^2}{\sigma^2 + \sigma_\beta^2} \left(1 + \frac{(1-\theta)\phi(y/\sigma)}{\theta\phi(y/\sqrt{\sigma^2 + \sigma_\beta^2})\sqrt{1 + \sigma_\beta^2/\sigma^2}}\right)^{-1}
\]</span></p>
<p>and <span class="math inline">\(\phi(\cdot)\)</span> denotes the standard normal density.</p>
<p>For the Bernoulli-Laplace prior <span class="math inline">\(\beta \sim (1-\theta)\delta_0 + \theta \text{Laplace}(0, b)\)</span>, the expressions become more complex, involving the cumulative distribution function of the standard normal distribution.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Posterior mean functions for Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) priors</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>y_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>sigma_beta_vals <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Gaussian posterior mean</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>bg_posterior_mean <span class="ot">&lt;-</span> <span class="cf">function</span>(y, sigma, theta, sigma_beta) {</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>  var_spike <span class="ot">&lt;-</span> sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>  var_slab <span class="ot">&lt;-</span> sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> sigma_beta<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>  weight_factor <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> theta) <span class="sc">*</span> <span class="fu">dnorm</span>(y, <span class="dv">0</span>, <span class="fu">sqrt</span>(var_spike)) <span class="sc">/</span> </span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>                   (theta <span class="sc">*</span> <span class="fu">dnorm</span>(y, <span class="dv">0</span>, <span class="fu">sqrt</span>(var_slab)))</span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>  weight <span class="ot">&lt;-</span> (sigma_beta<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> var_slab) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> weight_factor)</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(weight <span class="sc">*</span> y)</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Laplace posterior mean (simplified approximation)</span></span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>bl_posterior_mean <span class="ot">&lt;-</span> <span class="cf">function</span>(y, sigma, theta, b) {</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simplified version - in practice this involves complex integrals</span></span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>  weight_gaussian <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">-</span> theta) <span class="sc">*</span> <span class="fu">dnorm</span>(y, <span class="dv">0</span>, sigma)</span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>  weight_laplace <span class="ot">&lt;-</span> theta <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>b)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">abs</span>(y)<span class="sc">/</span>b) <span class="sc">*</span> <span class="fu">exp</span>(sigma<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>b<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">*</span> </span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a>                   (<span class="fu">pnorm</span>(<span class="sc">-</span>y<span class="sc">/</span>sigma <span class="sc">-</span> sigma<span class="sc">/</span>b) <span class="sc">+</span> <span class="fu">pnorm</span>(y<span class="sc">/</span>sigma <span class="sc">-</span> sigma<span class="sc">/</span>b))</span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a>  total_weight <span class="ot">&lt;-</span> weight_gaussian <span class="sc">+</span> weight_laplace</span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a>  weight <span class="ot">&lt;-</span> weight_laplace <span class="sc">/</span> total_weight</span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-32"><a href="#cb81-32" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Simplified shrinkage factor</span></span>
<span id="cb81-33"><a href="#cb81-33" aria-hidden="true" tabindex="-1"></a>  shrinkage_factor <span class="ot">&lt;-</span> y <span class="sc">-</span> <span class="fu">sign</span>(y) <span class="sc">*</span> <span class="fu">pmax</span>(<span class="dv">0</span>, <span class="fu">abs</span>(y) <span class="sc">-</span> <span class="fu">sqrt</span>(<span class="dv">2</span>) <span class="sc">*</span> sigma<span class="sc">/</span>b)</span>
<span id="cb81-34"><a href="#cb81-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(weight <span class="sc">*</span> shrinkage_factor <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> weight) <span class="sc">*</span> <span class="dv">0</span>)</span>
<span id="cb81-35"><a href="#cb81-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb81-36"><a href="#cb81-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-37"><a href="#cb81-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data for plotting</span></span>
<span id="cb81-38"><a href="#cb81-38" aria-hidden="true" tabindex="-1"></a>plot_data_bg <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">y =</span> y_vals, <span class="at">sigma_beta =</span> sigma_beta_vals)</span>
<span id="cb81-39"><a href="#cb81-39" aria-hidden="true" tabindex="-1"></a>plot_data_bg<span class="sc">$</span>posterior_mean <span class="ot">&lt;-</span> <span class="fu">mapply</span>(bg_posterior_mean, </span>
<span id="cb81-40"><a href="#cb81-40" aria-hidden="true" tabindex="-1"></a>                                     plot_data_bg<span class="sc">$</span>y, sigma, theta, plot_data_bg<span class="sc">$</span>sigma_beta)</span>
<span id="cb81-41"><a href="#cb81-41" aria-hidden="true" tabindex="-1"></a>plot_data_bg<span class="sc">$</span>sigma_beta_label <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"σ_β ="</span>, plot_data_bg<span class="sc">$</span>sigma_beta)</span>
<span id="cb81-42"><a href="#cb81-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-43"><a href="#cb81-43" aria-hidden="true" tabindex="-1"></a>plot_data_bl <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">y =</span> y_vals, <span class="at">b =</span> sigma_beta_vals)</span>
<span id="cb81-44"><a href="#cb81-44" aria-hidden="true" tabindex="-1"></a>plot_data_bl<span class="sc">$</span>posterior_mean <span class="ot">&lt;-</span> <span class="fu">mapply</span>(bl_posterior_mean, </span>
<span id="cb81-45"><a href="#cb81-45" aria-hidden="true" tabindex="-1"></a>                                     plot_data_bl<span class="sc">$</span>y, sigma, theta, plot_data_bl<span class="sc">$</span>b)</span>
<span id="cb81-46"><a href="#cb81-46" aria-hidden="true" tabindex="-1"></a>plot_data_bl<span class="sc">$</span>b_label <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"b ="</span>, plot_data_bl<span class="sc">$</span>b)</span>
<span id="cb81-47"><a href="#cb81-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-48"><a href="#cb81-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Gaussian plot</span></span>
<span id="cb81-49"><a href="#cb81-49" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(plot_data_bg, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> posterior_mean, <span class="at">color =</span> sigma_beta_label)) <span class="sc">+</span></span>
<span id="cb81-50"><a href="#cb81-50" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb81-51"><a href="#cb81-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb81-52"><a href="#cb81-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"y"</span>, <span class="at">y =</span> <span class="st">"E[β|y]"</span>, <span class="at">title =</span> <span class="st">"Bernoulli-Gaussian Prior"</span>,</span>
<span id="cb81-53"><a href="#cb81-53" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Parameter"</span>) <span class="sc">+</span></span>
<span id="cb81-54"><a href="#cb81-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb81-55"><a href="#cb81-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb81-56"><a href="#cb81-56" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb81-57"><a href="#cb81-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-58"><a href="#cb81-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Laplace plot  </span></span>
<span id="cb81-59"><a href="#cb81-59" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(plot_data_bl, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> posterior_mean, <span class="at">color =</span> b_label)) <span class="sc">+</span></span>
<span id="cb81-60"><a href="#cb81-60" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb81-61"><a href="#cb81-61" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="dv">1</span>, <span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb81-62"><a href="#cb81-62" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"y"</span>, <span class="at">y =</span> <span class="st">"E[β|y]"</span>, <span class="at">title =</span> <span class="st">"Bernoulli-Laplace Prior"</span>,</span>
<span id="cb81-63"><a href="#cb81-63" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Parameter"</span>) <span class="sc">+</span></span>
<span id="cb81-64"><a href="#cb81-64" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb81-65"><a href="#cb81-65" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb81-66"><a href="#cb81-66" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb81-67"><a href="#cb81-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-68"><a href="#cb81-68" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-posterior-means" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-posterior-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-posterior-means-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-posterior-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: Posterior mean functions for Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) priors
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Penalty functions φ associated with Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) posterior means</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Penalty function computation (numerical approximation)</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>compute_penalty_bg <span class="ot">&lt;-</span> <span class="cf">function</span>(z, sigma, theta, sigma_beta) {</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Find y such that posterior mean equals z</span></span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>  objective <span class="ot">&lt;-</span> <span class="cf">function</span>(y) (<span class="fu">bg_posterior_mean</span>(y, sigma, theta, sigma_beta) <span class="sc">-</span> z)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Use optimization to find y</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tryCatch</span>({</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    y_opt <span class="ot">&lt;-</span> <span class="fu">optimize</span>(objective, <span class="at">interval =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">10</span>))<span class="sc">$</span>minimum</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    penalty <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.5</span> <span class="sc">*</span> (y_opt <span class="sc">-</span> z)<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="fu">log</span>(</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>      (<span class="dv">1</span> <span class="sc">-</span> theta) <span class="sc">*</span> <span class="fu">dnorm</span>(y_opt, <span class="dv">0</span>, sigma) <span class="sc">+</span> </span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>      theta <span class="sc">*</span> <span class="fu">dnorm</span>(y_opt, <span class="dv">0</span>, <span class="fu">sqrt</span>(sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> sigma_beta<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(penalty)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>  }, <span class="at">error =</span> <span class="cf">function</span>(e) <span class="fu">return</span>(<span class="cn">NA</span>))</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>compute_penalty_bl <span class="ot">&lt;-</span> <span class="cf">function</span>(z, sigma, theta, b) {</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Penalty function for Bernoulli-Laplace: phi(beta) = -log p(beta)</span></span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># where p(beta) = (1-theta)*delta_0(beta) + theta * (1/(2b)) * exp(-|beta|/b)</span></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">abs</span>(z) <span class="sc">&lt;</span> <span class="fl">1e-10</span>) {</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># At z = 0: phi(0) = -log[(1-theta) + theta/(2b)]</span></span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="sc">-</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>theta) <span class="sc">+</span> theta<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>b)))</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For z != 0: phi(z) = -log[theta/(2b)] + |z|/b</span></span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="sc">-</span><span class="fu">log</span>(theta<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>b)) <span class="sc">+</span> <span class="fu">abs</span>(z)<span class="sc">/</span>b)</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create penalty data</span></span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a>z_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a>penalty_data_bg <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">z =</span> z_vals, <span class="at">sigma_beta =</span> sigma_beta_vals)</span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a>penalty_data_bg<span class="sc">$</span>penalty <span class="ot">&lt;-</span> <span class="fu">mapply</span>(<span class="cf">function</span>(z, sb) {</span>
<span id="cb82-34"><a href="#cb82-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">abs</span>(z) <span class="sc">&lt;</span> <span class="fl">0.01</span>) <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb82-35"><a href="#cb82-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_penalty_bg</span>(z, sigma, theta, sb)</span>
<span id="cb82-36"><a href="#cb82-36" aria-hidden="true" tabindex="-1"></a>}, penalty_data_bg<span class="sc">$</span>z, penalty_data_bg<span class="sc">$</span>sigma_beta)</span>
<span id="cb82-37"><a href="#cb82-37" aria-hidden="true" tabindex="-1"></a>penalty_data_bg<span class="sc">$</span>sigma_beta_label <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"σ_β ="</span>, penalty_data_bg<span class="sc">$</span>sigma_beta)</span>
<span id="cb82-38"><a href="#cb82-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-39"><a href="#cb82-39" aria-hidden="true" tabindex="-1"></a>penalty_data_bl <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">z =</span> z_vals, <span class="at">b =</span> sigma_beta_vals)</span>
<span id="cb82-40"><a href="#cb82-40" aria-hidden="true" tabindex="-1"></a>penalty_data_bl<span class="sc">$</span>penalty <span class="ot">&lt;-</span> <span class="fu">mapply</span>(compute_penalty_bl, </span>
<span id="cb82-41"><a href="#cb82-41" aria-hidden="true" tabindex="-1"></a>                                 penalty_data_bl<span class="sc">$</span>z, sigma, theta, penalty_data_bl<span class="sc">$</span>b)</span>
<span id="cb82-42"><a href="#cb82-42" aria-hidden="true" tabindex="-1"></a>penalty_data_bl<span class="sc">$</span>b_label <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"b ="</span>, penalty_data_bl<span class="sc">$</span>b)</span>
<span id="cb82-43"><a href="#cb82-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-44"><a href="#cb82-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove invalid values and normalize to have minimum at 0</span></span>
<span id="cb82-45"><a href="#cb82-45" aria-hidden="true" tabindex="-1"></a>penalty_data_bg <span class="ot">&lt;-</span> penalty_data_bg[<span class="sc">!</span><span class="fu">is.na</span>(penalty_data_bg<span class="sc">$</span>penalty), ]</span>
<span id="cb82-46"><a href="#cb82-46" aria-hidden="true" tabindex="-1"></a>penalty_data_bl <span class="ot">&lt;-</span> penalty_data_bl[<span class="sc">!</span><span class="fu">is.na</span>(penalty_data_bl<span class="sc">$</span>penalty), ]</span>
<span id="cb82-47"><a href="#cb82-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-48"><a href="#cb82-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize each curve to have minimum at 0</span></span>
<span id="cb82-49"><a href="#cb82-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (sb <span class="cf">in</span> sigma_beta_vals) {</span>
<span id="cb82-50"><a href="#cb82-50" aria-hidden="true" tabindex="-1"></a>  idx_bg <span class="ot">&lt;-</span> penalty_data_bg<span class="sc">$</span>sigma_beta <span class="sc">==</span> sb</span>
<span id="cb82-51"><a href="#cb82-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">any</span>(idx_bg)) {</span>
<span id="cb82-52"><a href="#cb82-52" aria-hidden="true" tabindex="-1"></a>    min_val <span class="ot">&lt;-</span> <span class="fu">min</span>(penalty_data_bg<span class="sc">$</span>penalty[idx_bg], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb82-53"><a href="#cb82-53" aria-hidden="true" tabindex="-1"></a>    penalty_data_bg<span class="sc">$</span>penalty[idx_bg] <span class="ot">&lt;-</span> penalty_data_bg<span class="sc">$</span>penalty[idx_bg] <span class="sc">-</span> min_val</span>
<span id="cb82-54"><a href="#cb82-54" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb82-55"><a href="#cb82-55" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb82-56"><a href="#cb82-56" aria-hidden="true" tabindex="-1"></a>  idx_bl <span class="ot">&lt;-</span> penalty_data_bl<span class="sc">$</span>b <span class="sc">==</span> sb</span>
<span id="cb82-57"><a href="#cb82-57" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">any</span>(idx_bl)) {</span>
<span id="cb82-58"><a href="#cb82-58" aria-hidden="true" tabindex="-1"></a>    min_val <span class="ot">&lt;-</span> <span class="fu">min</span>(penalty_data_bl<span class="sc">$</span>penalty[idx_bl], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb82-59"><a href="#cb82-59" aria-hidden="true" tabindex="-1"></a>    penalty_data_bl<span class="sc">$</span>penalty[idx_bl] <span class="ot">&lt;-</span> penalty_data_bl<span class="sc">$</span>penalty[idx_bl] <span class="sc">-</span> min_val</span>
<span id="cb82-60"><a href="#cb82-60" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb82-61"><a href="#cb82-61" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb82-62"><a href="#cb82-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-63"><a href="#cb82-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Gaussian penalty plot</span></span>
<span id="cb82-64"><a href="#cb82-64" aria-hidden="true" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(penalty_data_bg, <span class="fu">aes</span>(<span class="at">x =</span> z, <span class="at">y =</span> penalty, <span class="at">color =</span> sigma_beta_label)) <span class="sc">+</span></span>
<span id="cb82-65"><a href="#cb82-65" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb82-66"><a href="#cb82-66" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"β"</span>, <span class="at">y =</span> <span class="st">"φ(β)"</span>, <span class="at">title =</span> <span class="st">"Bernoulli-Gaussian Penalty"</span>,</span>
<span id="cb82-67"><a href="#cb82-67" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Parameter"</span>) <span class="sc">+</span></span>
<span id="cb82-68"><a href="#cb82-68" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb82-69"><a href="#cb82-69" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb82-70"><a href="#cb82-70" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb82-71"><a href="#cb82-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-72"><a href="#cb82-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Bernoulli-Laplace penalty plot</span></span>
<span id="cb82-73"><a href="#cb82-73" aria-hidden="true" tabindex="-1"></a>p4 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(penalty_data_bl, <span class="fu">aes</span>(<span class="at">x =</span> z, <span class="at">y =</span> penalty, <span class="at">color =</span> b_label)) <span class="sc">+</span></span>
<span id="cb82-74"><a href="#cb82-74" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb82-75"><a href="#cb82-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"β"</span>, <span class="at">y =</span> <span class="st">"φ(β)"</span>, <span class="at">title =</span> <span class="st">"Bernoulli-Laplace Penalty"</span>,</span>
<span id="cb82-76"><a href="#cb82-76" aria-hidden="true" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Parameter"</span>) <span class="sc">+</span></span>
<span id="cb82-77"><a href="#cb82-77" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb82-78"><a href="#cb82-78" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb82-79"><a href="#cb82-79" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb82-80"><a href="#cb82-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-81"><a href="#cb82-81" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p3, p4, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-penalty-functions" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-penalty-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-penalty-functions-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-penalty-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Penalty functions φ associated with Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) posterior means
</figcaption>
</figure>
</div>
</div>
</div>
<p>These figures illustrate several key properties of spike-and-slab priors in the proximal framework. Both priors shrink small observations towards zero, but their behavior differs significantly for larger observations. When the slab variance <span class="math inline">\(\sigma_\beta^2\)</span> (or scale parameter <span class="math inline">\(b\)</span>) is small, Bernoulli-Gaussian priors behave similarly to ridge regression, applying excessive shrinkage to large observations. In contrast, Bernoulli-Laplace priors exhibit behavior more similar to Lasso, with softer shrinkage characteristics.</p>
<p>As the slab parameters increase, both priors approach hard thresholding behavior, and their associated penalty functions <span class="math inline">\(\phi\)</span> become increasingly similar to non-convex penalties such as SCAD. The penalty functions reveal the “spiky” nature around zero that induces sparsity for small observations, while the different tail behaviors reflect the distinct shrinkage properties of Gaussian versus Laplace slab components.</p>
</section>
</section>
</section>
<section id="final-thoughts" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="final-thoughts"><span class="header-section-number">1.8</span> Final Thoughts</h2>
<p>This chapter has traced a remarkable intellectual journey from classical maximum likelihood estimation to modern Bayesian regularization, revealing deep connections that underpin much of contemporary artificial intelligence and machine learning theory. What emerges is not merely a collection of statistical techniques, but a unified theoretical framework that bridges frequentist and Bayesian paradigms while providing principled solutions to the fundamental challenges of learning from data.</p>
<p>Our exploration began with a sobering revelation: the maximum likelihood estimator, long considered the gold standard of classical statistics, is inadmissible in high-dimensional settings. Stein’s paradox demonstrates that for <span class="math inline">\(p \geq 3\)</span> dimensions, there always exist estimators with uniformly lower risk than the MLE. This is not merely a theoretical curiosity—in the normal means problem with <span class="math inline">\(p=100\)</span>, the James-Stein estimator can achieve 67 times lower risk than the MLE. This dramatic improvement illustrates why classical approaches often fail in modern high-dimensional problems and why shrinkage methods have become essential tools in contemporary data science.</p>
<p>The James-Stein estimator’s success stems from its ability to “borrow strength” across components through global shrinkage, demonstrating that multivariate estimation problems are fundamentally easier than their univariate counterparts when approached with appropriate regularization. However, global shrinkage alone is insufficient for sparse signals, motivating the development of more sophisticated approaches that can adapt to local signal structure.</p>
<p>A central theme throughout this chapter is the profound duality between Bayesian priors and regularization penalties. Every regularization term <span class="math inline">\(\lambda \phi(f)\)</span> corresponds to a prior distribution through the relationship <span class="math inline">\(\phi(f) = -\log p(f)\)</span>, making maximum a posteriori (MAP) estimation equivalent to penalized optimization. This duality provides both theoretical justification for regularization methods and practical guidance for prior specification:</p>
<table class="caption-top table">
<caption>Penalty types and their corresponding prior distributions.</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 17%">
<col style="width: 21%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Penalty Type</th>
<th>Prior Distribution</th>
<th>Key Property/Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ridge regression</td>
<td><span class="math inline">\(\ell_2\)</span> penalty</td>
<td>Gaussian priors</td>
<td>Encourages smoothness and numerical stability</td>
</tr>
<tr class="even">
<td>Lasso regression</td>
<td><span class="math inline">\(\ell_1\)</span> penalty</td>
<td>Laplace priors</td>
<td>Induces sparsity through soft thresholding</td>
</tr>
<tr class="odd">
<td>Bridge regression</td>
<td><span class="math inline">\(\ell_\alpha\)</span> penalty</td>
<td>Exponential power priors</td>
<td>Interpolates between Ridge and Lasso; oracle properties</td>
</tr>
<tr class="even">
<td>Horseshoe regression</td>
<td>Heavy-tailed penalty</td>
<td>Horseshoe priors</td>
<td>Optimal sparsity with strong signals preserved</td>
</tr>
<tr class="odd">
<td>Subset selection</td>
<td><span class="math inline">\(\ell_0\)</span> penalty</td>
<td>Spike-and-slab priors</td>
<td>Provides exact sparsity through hard thresholding</td>
</tr>
</tbody>
</table>
<p>This mapping reveals why certain penalties work well for particular problem types and guides the selection of appropriate regularization strategies based on problem structure and prior beliefs about the solution. <a href="#fig-horseshoe" class="quarto-xref">Figure&nbsp;<span>1.6</span></a> shows the unit balls defined by the norms induced by different priors.</p>
<!-- ![Comparison of unit balls for norms induced by different priors](./fig/horseshoe.png){#fig-horseshoe} -->
<div id="fig-horseshoe" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-horseshoe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-horseshoe-bridge" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-horseshoe-bridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/fig_bridge_detailed.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-horseshoe-bridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Bridge
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-horseshoe-unit-ball" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-horseshoe-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/horseshoe_unit_ball_annotated.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-horseshoe-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Horseshoe
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-cauchy-unit-ball" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-cauchy-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./fig/cauchy_unit_ball_annotated.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-cauchy-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Cauchy
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-horseshoe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Comparison of unit balls for norms induced by different priors
</figcaption>
</figure>
</div>
<!-- reproduce_fig_horseshoe.R
code/cauchy_unit_ball.R 
code/horseshoe_unit_ball.R -->
<p><a href="#fig-horseshoe-bridge" class="quarto-xref">Figure&nbsp;<span>1.6 (a)</span></a> Shows the unit balls for penalty norms induced by the horseshoe prior. It demonstrates several remarkable properties that make it particularly effective for sparse estimation problems. Its penalty function <span class="math inline">\(\phi(\theta_i) = -\log(\log(1 + 2\tau^2/\theta_i^2))\)</span> exhibits a unique double-logarithmic structure that creates an ideal balance between sparsity induction and signal preservation. In multivariate settings, the penalty simply sums across components: <span class="math inline">\(\phi(\theta) = \sum_i \phi(\theta_i)\)</span>.</p>
<p>The horseshoe’s most distinctive characteristic is its asymmetric shrinkage behavior. Near the origin, when parameters approach zero, the penalty function grows to infinity, creating extremely strong shrinkage that encourages exact zeros and thus induces sparsity. However, for large parameter values, the penalty grows slowly, allowing large signals to escape penalization with minimal distortion. This behavior is fundamentally different from methods like Lasso, which applies uniform shrinkage regardless of signal magnitude.</p>
<p>The global shrinkage parameter <span class="math inline">\(\tau\)</span> provides crucial control over the prior’s behavior. Smaller values of <span class="math inline">\(\tau\)</span> impose more aggressive shrinkage across all parameters, while larger values become more permissive, allowing signals to emerge more easily. This parameter effectively controls the trade-off between sparsity and signal detection.</p>
<p>From a theoretical perspective, the horseshoe prior achieves optimal minimax convergence rates of <span class="math inline">\(O(s \log(p/s))\)</span> for <span class="math inline">\(s\)</span>-sparse vectors, making it particularly well-suited for “needle-in-a-haystack” problems where researchers seek to identify a few large signals among many noise variables. Under appropriate regularity conditions, the horseshoe also possesses oracle properties, meaning it can correctly identify the true signal structure.</p>
<p>The geometric visualization reveals why this prior is so effective. The characteristic “horseshoe” shape displays concave contours near the coordinate axes, reflecting strong shrinkage toward zero, while showing convex contours away from the origin, indicating minimal shrinkage for large coefficients. Unlike the Lasso’s diamond shape or Ridge’s circular contours, the horseshoe’s heavy tails allow large coefficients to escape penalization entirely.</p>
<p>This comprehensive visualization demonstrates why the horseshoe prior has become increasingly popular for modern high-dimensional problems where the true signal structure is sparse but individual signals may be large in magnitude. The prior’s ability to provide aggressive shrinkage for noise while preserving signal integrity makes it an ideal choice for contemporary machine learning applications.</p>
<p>Our analysis reveals a rich spectrum of approaches to sparsity, each with distinct theoretical properties and practical advantages:</p>
<table class="caption-top table">
<caption>Table: Method categories and their key properties.</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 5%">
<col style="width: 9%">
<col style="width: 66%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th>Method Category</th>
<th>Examples</th>
<th>Key Properties</th>
<th>Advantages</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Global shrinkage methods</strong></td>
<td>James-Stein, Ridge regression</td>
<td>Uniform regularization across all parameters</td>
<td>Computationally simple and numerically stable</td>
<td>Cannot adapt to local signal structure; may over-shrink large signals in sparse settings</td>
</tr>
<tr class="even">
<td><strong>Adaptive shrinkage methods</strong></td>
<td>Lasso, horseshoe priors</td>
<td>Differential shrinkage based on signal strength</td>
<td>Lasso: soft thresholding; Horseshoe: aggressive shrinkage for small signals while preserving large ones; achieves optimal minimax rate <span class="math inline">\(p_n \log(n/p_n)\)</span></td>
<td>Horseshoe particularly effective for “needle-in-a-haystack” problems</td>
</tr>
<tr class="odd">
<td><strong>Variable selection methods</strong></td>
<td>Spike-and-slab priors, <span class="math inline">\(\ell_0\)</span> regularization</td>
<td>Discrete inclusion/exclusion decisions</td>
<td>Most interpretable sparse solutions; can achieve oracle properties under appropriate conditions</td>
<td>Combinatorially challenging</td>
</tr>
</tbody>
</table>
<p>Building on the theoretical foundations and geometric intuition developed above, we can distill several actionable guidelines for practitioners and researchers working with regularized models and sparse estimation. These practical insights help bridge the gap between abstract theory and real-world application, informing the choice of methods and priors in diverse statistical and machine learning contexts:</p>
<table class="caption-top table">
<caption>Practical guidelines for regularized models and sparse estimation.</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 27%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Recommended Methods</th>
<th>Rationale / Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Low-dimensional problems (<span class="math inline">\(p &lt; n\)</span>)</td>
<td>Ridge regression</td>
<td>Provides numerical stability; often sufficient for moderate <span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td>High-dimensional sparse problems</td>
<td>Lasso, Bridge regression (<span class="math inline">\(\alpha \approx 0.7\)</span>), Horseshoe priors</td>
<td>Lasso balances efficiency and sparsity; Bridge and Horseshoe offer better statistical properties</td>
</tr>
<tr class="odd">
<td>Ultra-sparse problems</td>
<td>Spike-and-slab priors, <span class="math inline">\(\ell_0\)</span> methods (e.g., SBR)</td>
<td>Achieve optimal performance and exact sparsity</td>
</tr>
<tr class="even">
<td>Correlated predictors</td>
<td>Bridge regression, full Bayesian approaches</td>
<td>Handle correlation better than Lasso, which may select variables arbitrarily</td>
</tr>
<tr class="odd">
<td>Uncertainty quantification is crucial</td>
<td>Full Bayesian methods</td>
<td>Provide credible intervals and posterior probabilities, unlike point estimates</td>
</tr>
</tbody>
</table>
<p>While our focus has been on linear models, the principles developed here extend broadly to modern AI systems. Deep learning architectures routinely employ regularization techniques (dropout, weight decay, batch normalization) that can be understood through the Bayesian lens developed in this chapter. The success of techniques like variational autoencoders and Bayesian neural networks demonstrates the continued relevance of probabilistic thinking in contemporary machine learning.</p>
<p>Moreover, the sparse estimation techniques discussed here are fundamental to interpretable AI, compressed sensing, and efficient neural architecture design. The theoretical insights about when different regularization approaches succeed provide guidance for designing and analyzing complex learning systems.</p>
<p>The evolution from maximum likelihood estimation to sophisticated Bayesian regularization represents more than technical progress—it reflects a fundamental shift in how we approach learning from data. Rather than seeking single “best” estimates, modern methods embrace uncertainty, incorporate prior knowledge, and adaptively balance model complexity with empirical fit.</p>
<p>The remarkable fact that Bayesian approaches often dominate classical frequentist methods, even from a frequentist perspective (as demonstrated by Stein’s paradox), suggests that probabilistic thinking provides not just philosophical appeal but concrete practical advantages. In an era of ever-growing data complexity and dimensionality, these theoretical insights become increasingly valuable for developing robust, interpretable, and effective learning algorithms.</p>
<p>The unified framework presented in this chapter—connecting classical statistics, Bayesian inference, optimization theory, and modern machine learning—provides both historical perspective and forward-looking guidance for the continued development of artificial intelligence systems. As we confront increasingly complex learning challenges, from personalized medicine to autonomous systems, the principled approach to regularization and uncertainty quantification developed here will remain fundamental to building trustworthy and effective AI systems.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-andrews1974scale" class="csl-entry" role="listitem">
Andrews, D. F., and C. L. Mallows. 1974. <span>“Scale <span>Mixtures</span> of <span>Normal Distributions</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 36 (1): 99–102. <a href="https://www.jstor.org/stable/2984774">https://www.jstor.org/stable/2984774</a>.
</div>
<div id="ref-carvalho2010horseshoe" class="csl-entry" role="listitem">
Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. <span>“The Horseshoe Estimator for Sparse Signals.”</span> <em>Biometrika</em>, asq017.
</div>
<div id="ref-diaconis1983quantifying" class="csl-entry" role="listitem">
Diaconis, P., and D. Ylvisaker. 1983. <span>“Quantifying <span>Prior Opinion</span>.”</span>
</div>
<div id="ref-efron1975data" class="csl-entry" role="listitem">
Efron, Bradley, and Carl Morris. 1975. <span>“Data <span>Analysis Using Stein</span>’s <span>Estimator</span> and Its <span>Generalizations</span>.”</span> <em>Journal of the American Statistical Association</em> 70 (350): 311–19.
</div>
<div id="ref-efron1977steins" class="csl-entry" role="listitem">
———. 1977. <span>“Stein’s Paradox in Statistics.”</span> <em>Scientific American</em> 236 (5): 119–27.
</div>
<div id="ref-stein1964inadmissibility" class="csl-entry" role="listitem">
Stein, Charles. 1964. <span>“Inadmissibility of the Usual Estimator for the Variance of a Normal Distribution with Unknown Mean.”</span> <em>Annals of the Institute of Statistical Mathematics</em> 16 (1): 155–60.
</div>
<div id="ref-tikhonov1943stability" class="csl-entry" role="listitem">
Tikhonov, Andrey Nikolayevich et al. 1943. <span>“On the Stability of Inverse Problems.”</span> In <em>Dokl. Akad. Nauk Sssr</em>, 39:195–98.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-intro.html" class="pagination-link" aria-label="Principles of Data Science">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Principles of Data Science</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="References">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>