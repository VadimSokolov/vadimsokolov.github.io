<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>17&nbsp; Statistical Learning Theory and Regularization – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./18-nn.html" rel="next">
<link href="./16-select.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="17&nbsp; Statistical Learning Theory and Regularization – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="17-theoryai_files/figure-html/unnamed-chunk-4-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="17&nbsp; Statistical Learning Theory and Regularization – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="17-theoryai_files/figure-html/unnamed-chunk-4-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#normal-means-problem" id="toc-normal-means-problem" class="nav-link active" data-scroll-target="#normal-means-problem"><span class="header-section-number">17.1</span> Normal Means Problem</a></li>
  <li><a href="#sec-unbiasedness-optimality" id="toc-sec-unbiasedness-optimality" class="nav-link" data-scroll-target="#sec-unbiasedness-optimality"><span class="header-section-number">17.2</span> Unbiasedness and the Optimality of Bayes Rules</a>
  <ul class="collapse">
  <li><a href="#full-bayes-shrinkage" id="toc-full-bayes-shrinkage" class="nav-link" data-scroll-target="#full-bayes-shrinkage">Full Bayes Shrinkage</a></li>
  </ul></li>
  <li><a href="#bias-variance-decomposition" id="toc-bias-variance-decomposition" class="nav-link" data-scroll-target="#bias-variance-decomposition"><span class="header-section-number">17.3</span> Bias-Variance Decomposition</a>
  <ul class="collapse">
  <li><a href="#risk-decomposition" id="toc-risk-decomposition" class="nav-link" data-scroll-target="#risk-decomposition">Risk Decomposition</a></li>
  </ul></li>
  <li><a href="#sparsity" id="toc-sparsity" class="nav-link" data-scroll-target="#sparsity"><span class="header-section-number">17.4</span> Sparsity</a></li>
  <li><a href="#maximum-a-posteriori-estimation-map-and-regularization" id="toc-maximum-a-posteriori-estimation-map-and-regularization" class="nav-link" data-scroll-target="#maximum-a-posteriori-estimation-map-and-regularization"><span class="header-section-number">17.5</span> Maximum A posteriori Estimation (MAP) and Regularization</a></li>
  <li><a href="#the-duality-between-regularization-and-priors" id="toc-the-duality-between-regularization-and-priors" class="nav-link" data-scroll-target="#the-duality-between-regularization-and-priors"><span class="header-section-number">17.6</span> The Duality Between Regularization and Priors</a>
  <ul class="collapse">
  <li><a href="#map-as-a-poor-mans-bayesian" id="toc-map-as-a-poor-mans-bayesian" class="nav-link" data-scroll-target="#map-as-a-poor-mans-bayesian">MAP as a Poor Man’s Bayesian</a></li>
  </ul></li>
  <li><a href="#ridge-regression-ell_2-norm" id="toc-ridge-regression-ell_2-norm" class="nav-link" data-scroll-target="#ridge-regression-ell_2-norm"><span class="header-section-number">17.7</span> Ridge Regression (<span class="math inline">\(\ell_2\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#tikhonov-regularization-framework" id="toc-tikhonov-regularization-framework" class="nav-link" data-scroll-target="#tikhonov-regularization-framework">Tikhonov Regularization Framework</a></li>
  <li><a href="#kernel-view-of-ridge-regression" id="toc-kernel-view-of-ridge-regression" class="nav-link" data-scroll-target="#kernel-view-of-ridge-regression">Kernel View of Ridge Regression</a></li>
  </ul></li>
  <li><a href="#scale-mixtures-representations" id="toc-scale-mixtures-representations" class="nav-link" data-scroll-target="#scale-mixtures-representations"><span class="header-section-number">17.8</span> Scale Mixtures Representations</a></li>
  <li><a href="#lasso-regression-ell_1-norm" id="toc-lasso-regression-ell_1-norm" class="nav-link" data-scroll-target="#lasso-regression-ell_1-norm"><span class="header-section-number">17.9</span> Lasso Regression (<span class="math inline">\(\ell_1\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#lasso-as-a-scale-mixture" id="toc-lasso-as-a-scale-mixture" class="nav-link" data-scroll-target="#lasso-as-a-scale-mixture">Lasso as a Scale Mixture</a></li>
  <li><a href="#more-mixture-representations" id="toc-more-mixture-representations" class="nav-link" data-scroll-target="#more-mixture-representations">More Mixture Representations</a></li>
  </ul></li>
  <li><a href="#sec-horseshoe" id="toc-sec-horseshoe" class="nav-link" data-scroll-target="#sec-horseshoe"><span class="header-section-number">17.10</span> Horseshoe</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation">Mathematical Formulation</a></li>
  <li><a href="#shrinkage-properties" id="toc-shrinkage-properties" class="nav-link" data-scroll-target="#shrinkage-properties">Shrinkage Properties</a></li>
  <li><a href="#comparison-with-other-priors" id="toc-comparison-with-other-priors" class="nav-link" data-scroll-target="#comparison-with-other-priors">Comparison with Other Priors</a></li>
  <li><a href="#computational-implementation" id="toc-computational-implementation" class="nav-link" data-scroll-target="#computational-implementation">Computational Implementation</a></li>
  <li><a href="#comparison-and-package-implementation" id="toc-comparison-and-package-implementation" class="nav-link" data-scroll-target="#comparison-and-package-implementation">Comparison and Package Implementation</a></li>
  </ul></li>
  <li><a href="#bridge-ell_alpha" id="toc-bridge-ell_alpha" class="nav-link" data-scroll-target="#bridge-ell_alpha"><span class="header-section-number">17.11</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</a>
  <ul class="collapse">
  <li><a href="#bayesian-framework-and-data-augmentation" id="toc-bayesian-framework-and-data-augmentation" class="nav-link" data-scroll-target="#bayesian-framework-and-data-augmentation">Bayesian Framework and Data Augmentation</a></li>
  </ul></li>
  <li><a href="#full-bayes-for-sparsity-shrinkage" id="toc-full-bayes-for-sparsity-shrinkage" class="nav-link" data-scroll-target="#full-bayes-for-sparsity-shrinkage"><span class="header-section-number">17.12</span> Full Bayes for Sparsity Shrinkage</a>
  <ul class="collapse">
  <li><a href="#spike-and-slab-prior" id="toc-spike-and-slab-prior" class="nav-link" data-scroll-target="#spike-and-slab-prior">Spike-and-Slab Prior</a></li>
  </ul></li>
  <li><a href="#subset-selection-ell_0-norm" id="toc-subset-selection-ell_0-norm" class="nav-link" data-scroll-target="#subset-selection-ell_0-norm"><span class="header-section-number">17.13</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</a>
  <ul class="collapse">
  <li><a href="#connection-to-spike-and-slab-priors" id="toc-connection-to-spike-and-slab-priors" class="nav-link" data-scroll-target="#connection-to-spike-and-slab-priors">Connection to Spike-and-Slab Priors</a></li>
  <li><a href="#single-best-replacement-sbr-algorithm" id="toc-single-best-replacement-sbr-algorithm" class="nav-link" data-scroll-target="#single-best-replacement-sbr-algorithm">Single Best Replacement (SBR) Algorithm</a></li>
  </ul></li>
  <li><a href="#advanced-topics-in-regularization" id="toc-advanced-topics-in-regularization" class="nav-link" data-scroll-target="#advanced-topics-in-regularization"><span class="header-section-number">17.14</span> Advanced Topics in Regularization</a>
  <ul class="collapse">
  <li><a href="#the-vertical-likelihood-duality" id="toc-the-vertical-likelihood-duality" class="nav-link" data-scroll-target="#the-vertical-likelihood-duality">The Vertical Likelihood Duality</a></li>
  <li><a href="#fundamental-integral-identities" id="toc-fundamental-integral-identities" class="nav-link" data-scroll-target="#fundamental-integral-identities">Fundamental Integral Identities</a></li>
  <li><a href="#quantile-regression-example" id="toc-quantile-regression-example" class="nav-link" data-scroll-target="#quantile-regression-example">Quantile Regression Example</a></li>
  <li><a href="#improper-limit-cases" id="toc-improper-limit-cases" class="nav-link" data-scroll-target="#improper-limit-cases">Improper Limit Cases</a></li>
  <li><a href="#computational-advantages-of-scale-mixtures" id="toc-computational-advantages-of-scale-mixtures" class="nav-link" data-scroll-target="#computational-advantages-of-scale-mixtures">Computational Advantages of Scale Mixtures</a></li>
  <li><a href="#generalized-ridge-regression-in-the-canonical-basis" id="toc-generalized-ridge-regression-in-the-canonical-basis" class="nav-link" data-scroll-target="#generalized-ridge-regression-in-the-canonical-basis">Generalized Ridge Regression in the Canonical Basis</a></li>
  <li><a href="#trend-filtering-and-composite-penalties" id="toc-trend-filtering-and-composite-penalties" class="nav-link" data-scroll-target="#trend-filtering-and-composite-penalties">Trend Filtering and Composite Penalties</a></li>
  </ul></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts"><span class="header-section-number">17.15</span> Final Thoughts</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./17-theoryai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-theoryai" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“In God we trust; all others must bring data.” — W. Edwards Deming</p>
</blockquote>
<p>When AlphaGo defeated world champion Lee Sedol in 2016, its neural networks contained millions of parameters—far more than the number of training positions it had observed. How did it avoid simply memorizing the training data? The answer lies in regularization: the systematic imposition of constraints that prevent models from over-fitting to noise. This chapter reveals that regularization is not merely a computational trick but emerges naturally from Bayesian reasoning about uncertainty.</p>
<p>The development of learning algorithms has been driven by two fundamental paradigms: the classical frequentist approach centered around maximum likelihood estimation (MLE) and the Bayesian approach grounded in decision theory. This chapter explores how these seemingly distinct methodologies converge in modern AI theory, particularly through the lens of regularization and model selection.</p>
<p>Maximum likelihood estimation represents the cornerstone of classical statistical inference. Given observed data <span class="math inline">\(\mathcal{D} = (x_i, y_i)_{i=1}^n\)</span> and a parametric model <span class="math inline">\(f_{\theta}(x)\)</span>, the MLE principle seeks to find the parameter values that maximize the likelihood function: <span class="math display">\[
\hat{\theta}_{\mathrm{MLE}} = \arg\max_{\theta} L(\theta; \mathcal{D}) = \arg\max_{\theta} \prod_{i=1}^n p(y_i \mid x_i, \theta)
\]</span></p>
<p>This approach has several appealing properties: it provides consistent estimators under mild conditions, achieves the Cramer-Rao lower bound asymptotically, and offers a principled framework for parameter estimation. These classical guarantees are typically stated under i.i.d. sampling assumptions; from a Bayesian perspective, exchangeability (<a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>) is a weaker condition that still supports learning from data in many settings. The Cramer-Rao bound is expressed in terms of Fisher information, and in exponential-family models Fisher information and sufficiency are closely related ways of formalizing how much information the data carry about a parameter.</p>
<p>However, MLE has well-documented limitations, particularly in high-dimensional settings. MLE can lead to overfitting, poor generalization, and numerical instability. Furthermore, as shown by Stein’s paradox, MLE can be inadmissible, meaning there are other estimators that have lower risk than the MLE. We will start this chapter with the normal means problem and demonstrate how MLE can be inadmissible.</p>
<section id="normal-means-problem" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="normal-means-problem"><span class="header-section-number">17.1</span> Normal Means Problem</h2>
<p>Consider the vector of means case where <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. We have <span id="eq-normal-mean"><span class="math display">\[
\bar y_i \mid \theta_i \sim N(\theta_i, \sigma^2/n_i), \quad i=1,\ldots,p, \quad p &gt; 2
\tag{17.1}\]</span></span> Here <span class="math inline">\(\bar y_i\)</span> is the mean of <span class="math inline">\(n_i\)</span> observations, i.e., <span class="math inline">\(\bar y_i = \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij}\)</span>.</p>
<p>The goal is to estimate the vector of means <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>, and we can achieve this by borrowing strength across the observations.</p>
<p>This is also a proxy for non-parametric regression, where <span class="math inline">\(y_i = f(x_i) + \varepsilon_i\)</span> and <span class="math inline">\(\theta_i = f(x_i)\)</span>. Much has been written on the properties of the Bayes risk as a function of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, and extensive work has been done on the asymptotic properties of the Bayes risk as <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> grow to infinity.</p>
<p>The classical inference is based on the CLT <span class="math display">\[
\hat \theta_i \mid \theta_i \sim N(\theta_i, \sigma^2/n_i),
\]</span> and the MLE estimate is given by <span class="math inline">\(\hat \theta_i = \bar y_i\)</span>. The MLE estimate is consistent and asymptotically normal, i.e., <span class="math display">\[
\hat \theta_i \to N(\theta_i, \sigma^2/n_i) \quad \text{as} \quad n_i \to \infty.
\]</span></p>
<p>On the other hand, the Bayes estimator is based on <span class="math inline">\(\theta_i \mid \hat \theta_i\)</span>, where <span class="math inline">\(\hat \theta_i = \bar y_i\)</span>. In other words, the classical approach is subject to the <em>prosecutor’s fallacy</em>—the logical error of confusing <span class="math inline">\(P(\text{data} \mid \text{hypothesis})\)</span> with <span class="math inline">\(P(\text{hypothesis} \mid \text{data})\)</span>. Classical estimators are unbiased, whereas a Bayes estimator is biased.</p>
<div id="exm-corporate-performance" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.1 (Screening Corporate Performance)</strong></span> To illustrate the practical importance of the normal means problem, consider the challenge faced by investment analysts screening thousands of publicly traded companies for sustained superior performance <span class="citation" data-cites="polson2012good">(<a href="references.html#ref-polson2012good" role="doc-biblioref">Polson and Scott 2012</a>)</span>. Suppose we have return on assets (ROA) data for <span class="math inline">\(p = 53,038\)</span> firms across 93 countries over 45 years. For each firm <span class="math inline">\(i\)</span>, we observe an average ROA performance <span class="math inline">\(\bar{y}_i\)</span> over some time period.</p>
<p>The fundamental question is: <em>Which firms have genuinely superior performance versus those that appear successful due to luck?</em> This is precisely a massive multiple testing problem in the normal means framework. Using MLE, we would estimate each firm’s performance as <span class="math inline">\(\hat{\theta}_i = \bar{y}_i\)</span> and declare any firm with <span class="math inline">\(\bar{y}_i &gt; 0\)</span> as superior. However, this approach ignores the massive multiplicity problem—with over 50,000 firms, many will appear successful purely by chance.</p>
<p>The Bayesian approach with appropriate shrinkage priors can distinguish between truly superior firms and those that are merely lucky. As we’ll see, this requires heavy-tailed priors that can accommodate the rare firms with genuine outperformance while shrinking the estimates of mediocre firms toward zero. This example demonstrates why the choice between MLE and Bayesian estimation has profound practical consequences in high-dimensional settings.</p>
<p>In practice, the normal means problem often requires careful preprocessing of the observed data. A common and crucial step is to normalize the observations by converting them to z-scores. This standardization serves multiple purposes and connects directly to the theoretical framework we’ve established.</p>
<p>Consider our corporate performance example where we observe ROA values <span class="math inline">\(\bar{y}_i\)</span> for different firms. These firms may operate in different industries, countries, or time periods, making direct comparison problematic. Raw ROA values might range from -20% to +30%, with different scales and baseline expectations across sectors.</p>
<p>The z-score transformation standardizes each observation relative to a reference distribution: <span class="math display">\[
z_i = \frac{\bar{y}_i - \mu_i}{\sigma_i}.
\]</span> where <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(\sigma_i\)</span> represent the mean and standard deviation estimated and predicted by an autoregressive model.</p>
<p>In the Polson &amp; Scott study <span class="citation" data-cites="polson2012good">(<a href="references.html#ref-polson2012good" role="doc-biblioref">Polson and Scott 2012</a>)</span>, standardization was essential for handling the massive heterogeneity across their dataset of 53,038 firms spanning 93 countries and 45 years. The authors faced three critical challenges that z-score normalization helped address:</p>
<p><strong>1. Cross-Country Comparisons</strong>: Raw ROA values varied dramatically across countries due to different accounting standards, economic conditions, and regulatory environments. A 5% ROA in Japan during the 1990s had very different implications than 5% ROA in Brazil during the same period.</p>
<p><strong>2. Temporal Adjustments</strong>: Economic cycles, inflation rates, and market conditions changed substantially over the 45-year study period. The authors needed to adjust for these time-varying factors to identify firms with genuinely superior performance rather than those that simply operated during favorable periods.</p>
<p><strong>3. Industry Heterogeneity</strong>: Different industries have fundamentally different ROA distributions. Technology firms typically show higher volatility and different baseline performance compared to utilities or manufacturing companies.</p>
<p>The authors implemented a sophisticated normalization procedure: <span class="math display">\[
z_{i,t} = \frac{\text{ROA}_{i,t} - \mu_{\text{peer}(i),t}}{\sigma_{\text{peer}(i),t}}
\]</span></p>
<p>where <span class="math inline">\(\text{peer}(i)\)</span> represents firm <span class="math inline">\(i\)</span>’s reference group (defined by industry, country, and size), and the subscript <span class="math inline">\(t\)</span> indicates time-varying adjustments. This created standardized performance measures where:</p>
<ul>
<li><span class="math inline">\(z_{i,t} = 0\)</span> indicates performance exactly at the peer group median</li>
<li><span class="math inline">\(z_{i,t} = 1\)</span> indicates performance one standard deviation above peers<br>
</li>
<li><span class="math inline">\(z_{i,t} = -1\)</span> indicates performance one standard deviation below peers</li>
</ul>
<p>After this rigorous standardization, the authors discovered a <em>critical finding</em>: sustained superior performance (<span class="math inline">\(\theta_i &gt; 0\)</span> consistently over time) was remarkably rare. Most firms showing high raw ROA were simply benefiting from favorable conditions rather than demonstrating genuine operational excellence. This finding emerged only after proper normalization—without standardization, hundreds of firms would have been incorrectly classified as superior performers.</p>
</div>
<p>The goal is to estimate the vector <span class="math inline">\(\theta\)</span> using squared loss: <span class="math display">\[
\mathcal{L}(\theta, \hat{\theta}) = \sum_{i=1}^p (\theta_i - \hat{\theta}_i)^2,
\]</span> where <span class="math inline">\(\hat{\theta}\)</span> is the vector of estimates. We will compare the MLE estimate with the James-Stein estimate. A principled way to evaluate the performance of an estimator is to average its loss over the data; this metric is called the risk. The MLE estimate <span class="math inline">\(\hat{\theta}_i = y_i\)</span> has a constant risk <span class="math inline">\(p\sigma^2\)</span>: <span class="math display">\[
R(\theta,\hat{\theta}) = \E[y]{\mathcal{L}(\theta, \hat{\theta})} = \sum_{i=1}^p \E[y_i]{(y_i - \theta_i )^2}.
\]</span></p>
<p>Here the expectation is over the data given by distribution <a href="#eq-normal-mean" class="quarto-xref">Equation&nbsp;<span>17.1</span></a> and <span class="math inline">\(y_i \sim N(\theta_i, \sigma^2)\)</span>, we have <span class="math inline">\(\E[y_i]{(\theta_i - y_i)^2} = \Var{y_i} = \sigma^2\)</span> for each <span class="math inline">\(i\)</span>. Therefore: <span class="math display">\[
R(\theta,\hat{\theta}) = \sum_{i=1}^p \sigma^2 = p\sigma^2.
\]</span></p>
<p>This shows that the MLE risk is constant and does not depend on the true parameter values <span class="math inline">\(\theta\)</span>, only on the dimension <span class="math inline">\(p\)</span> and the noise variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Given that MLE provides a natural baseline estimator with known risk properties, one might ask: can we do better? The Bayesian paradigm offers a fundamentally different perspective that often yields estimators with uniformly lower risk.</p>
<p>Bayesian inference offers a fundamentally different perspective by incorporating prior knowledge and quantifying uncertainty through probability distributions. The Bayesian approach begins with a prior distribution <span class="math inline">\(p(\theta)\)</span> over the parameter space and updates this belief using Bayes’ rule: <span class="math display">\[
p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}
\]</span></p>
<p>The <em>Bayes estimator</em> is the value <span class="math inline">\(\hat{\theta}^{B}\)</span> that minimizes the Bayes risk, the expected loss: <span class="math display">\[
\hat{\theta}^{B} = \arg\min_{\hat{\theta}(y)} R(\pi, \hat{\theta}(y))
\]</span> Here <span class="math inline">\(\pi\)</span> is the prior distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(R(\pi, \hat{\theta}(y))\)</span> is the <em>Bayes risk</em> defined as: <span id="eq-bayes-risk"><span class="math display">\[
R(\pi, \hat{\theta}(y)) = \mathbb{E}_{\theta \sim \pi} \left[ \mathbb{E}_{y\mid \theta} \left[ \mathcal{L}(\theta, \hat{\theta}(y)) \right] \right].
\tag{17.2}\]</span></span> For squared error loss, this yields the posterior mean <span class="math inline">\(\E{\theta \mid y}\)</span>, while for absolute error loss, it gives the posterior median.</p>
<p>For the normal means problem with squared error loss, this becomes: <span class="math display">\[
R(\pi, \hat{\theta}(y)) = \int_{\theta \in \Theta} \left( \int_{y \in \mathcal{Y}} (\theta - \hat{\theta}(y))^2 p(y|\theta) dy \right) \pi(\theta) d\theta
\]</span></p>
<p>The Bayes risk quantifies the expected performance of an estimator, taking into account both the uncertainty in the data and the prior uncertainty about the parameter. It serves as a benchmark for comparing different estimators: an estimator with lower Bayes risk is preferred under the chosen prior and loss function. In particular, the Bayes estimator achieves the minimum possible Bayes risk for the given prior and loss.</p>
<p>In 1961, Charles Stein and Willard James proved a startling result: they constructed an estimator for the mean of a multivariate normal distribution that uniformly dominates the sample mean under squared error loss—a finding that challenged what seemed like an elementary law of statistical theory. This result, now known as <em>Stein’s paradox</em>, demonstrates that for dimensions <span class="math inline">\(p \geq 3\)</span>, there always exists an estimator with strictly lower risk than the MLE for all parameter values.</p>
<p>The empirical evidence is striking: in applications ranging from baseball batting averages to toxoplasmosis prevalence rates to Pearson’s chi-square tests, the James-Stein estimator achieves mean squared errors less than half that of the sample mean. This result is paradoxical because it overturns the intuition that unbiased estimators should be optimal—despite introducing bias, the James-Stein estimator achieves strictly lower risk for all parameter values when <span class="math inline">\(p \geq 3\)</span>. The philosophical implications extend beyond estimation theory: by demonstrating that individual estimates can be improved by considering them jointly, Stein’s paradox provides deep connections to Bayesian thinking and the empirical Bayes framework.</p>
<p>Stein’s phenomenon where <span class="math inline">\(y_i \mid \theta_i \sim N(\theta_i, 1)\)</span> and <span class="math inline">\(\theta_i \sim N(0, \tau^2)\)</span> where <span class="math inline">\(\tau \rightarrow \infty\)</span> illustrates this point well. The MLE approach is equivalent to the use of the improper “non-informative” uniform prior and leads to an estimator with poor risk properties.</p>
<p>Let <span class="math inline">\(\|y\|^2 = \sum_{i=1}^p y_i^2\)</span> denote the squared Euclidean norm of <span class="math inline">\(y\)</span>. Then, we can make the following probabilistic statements from the model: <span class="math display">\[
P\left( \| y \| &gt; \| \theta \| \right) &gt; \frac{1}{2}
\]</span> Now for the posterior, this inequality is reversed under a flat Lebesgue measure: <span class="math display">\[
P\left( \| \theta \| &gt; \| y \| \mid y \right) &gt; \frac{1}{2}
\]</span> which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.</p>
<p>The shrinkage rule (a.k.a. normal prior) where <span class="math inline">\(\tau^2\)</span> is “estimated” from the data avoids this conflict. More precisely, we have: <span class="math display">\[
\hat{\theta}(y) = \left( 1 - \frac{k-2}{\|y\|^2} \right) y \quad \text{and} \quad \E{\| \hat{\theta} - \theta \|^2 } &lt; k, \; \forall \theta.
\]</span> Hence, when <span class="math inline">\(\|y\|^2\)</span> is small, the shrinkage factor is more extreme. For example, if <span class="math inline">\(k=10\)</span>, <span class="math inline">\(\|y\|^2=12\)</span>, then <span class="math inline">\(\hat{\theta} = (1/3) y\)</span>. Now we have the more intuitive result that: <span class="math display">\[
P\left(\|\theta\| &gt; \|y\| \mid y\right) &lt; \frac{1}{2}.
\]</span></p>
<p>This shows that careful specification of default priors matters in high dimensions.</p>
<p>The resulting estimator is called the James-Stein estimator and is a shrinkage estimator that shrinks the MLE towards the prior mean. The prior mean is typically the sample mean of the data. The James-Stein estimator is given by: <span class="math display">\[
\hat{\theta}_{i}^{JS} = (1 - \lambda) \hat{\theta}_{i}^{MLE} + \lambda \bar{y},
\]</span> where <span class="math inline">\(\lambda\)</span> is a shrinkage parameter and <span class="math inline">\(\bar{y}\)</span> is the sample mean of the data. The shrinkage parameter is typically chosen to minimize the risk of the estimator.</p>
<p>The key idea behind James-Stein shrinkage is that one can “borrow strength” across components. In this sense, the multivariate parameter estimation problem is easier than the univariate one.</p>
<p>Following <span class="citation" data-cites="efron1975data">Efron and Morris (<a href="references.html#ref-efron1975data" role="doc-biblioref">1975</a>)</span>, we can view the James-Stein estimator through the lens of empirical Bayes methodology. Efron and Morris demonstrate that Stein’s seemingly paradoxical result has a natural interpretation when viewed as an empirical Bayes procedure that estimates the prior distribution from the data itself.</p>
<p>Consider the hierarchical model: <span class="math display">\[
\begin{aligned}
y_i \mid \theta_i &amp;\sim N(\theta_i, \sigma^2) \\
\theta_i \mid \mu, \tau^2 &amp;\sim N(\mu, \tau^2)
\end{aligned}
\]</span></p>
<p>The marginal distribution of <span class="math inline">\(y_i\)</span> is then <span class="math inline">\(y_i \sim N(\mu, \sigma^2 + \tau^2)\)</span>. In the empirical Bayes approach, we estimate the hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau^2\)</span> from the marginal likelihood:</p>
<p><span class="math display">\[
m(y \mid \mu, \tau^2) = \prod_{i=1}^p \frac{1}{\sqrt{2\pi(\sigma^2 + \tau^2)}} \exp\left(-\frac{(y_i - \mu)^2}{2(\sigma^2 + \tau^2)}\right)
\]</span></p>
<p>The maximum marginal likelihood estimators are: <span class="math display">\[
\hat{\mu} = \bar{y} = \frac{1}{p}\sum_{i=1}^p y_i
\]</span> <span class="math display">\[
\hat{\tau}^2 = \max\left(0, \frac{1}{p}\sum_{i=1}^p (y_i - \bar{y})^2 - \sigma^2\right)
\]</span></p>
<p>The empirical Bayes estimator then becomes: <span class="math display">\[
\hat{\theta}_i^{EB} = \frac{\hat{\tau}^2}{\sigma^2 + \hat{\tau}^2} y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \hat{\mu}
\]</span></p>
<p>This can be rewritten as: <span class="math display">\[
\hat{\theta}_i^{EB} = \left(1 - \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2}\right) y_i + \frac{\sigma^2}{\sigma^2 + \hat{\tau}^2} \bar{y}
\]</span></p>
<p>When <span class="math inline">\(\mu = 0\)</span> and using the estimate <span class="math inline">\(\hat{\tau}^2 = \max(0, \|y\|^2/p - \sigma^2)\)</span>, this reduces to a form closely related to the James-Stein estimator: <span class="math display">\[
\hat{\theta}_i^{JS} = \left(1 - \frac{(p-2)\sigma^2}{\|y\|^2}\right) y_i
\]</span></p>
<p>Efron and Morris show that the empirical Bayes interpretation provides insight into why the James-Stein estimator dominates the MLE. The key insight is that the MLE implicitly assumes an improper flat prior <span class="math inline">\(\pi(\theta) \propto 1\)</span>, which leads to poor risk properties in high dimensions.</p>
<p>The Bayes risk of the James-Stein estimator can be explicitly calculated due to the conjugacy of the normal prior and likelihood: <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) = p\sigma^2 - (p-2)\sigma^2 \mathbb{E}\left[\frac{1}{\|\theta + \epsilon\|^2/\sigma^2}\right]
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim N(0, \sigma^2 I)\)</span>. Since the second term is always positive, we have: <span class="math display">\[
R(\theta, \hat{\theta}^{JS}) &lt; R(\theta, \hat{\theta}^{MLE}) \quad \forall \theta \in \mathbb{R}^p, \quad p \geq 3
\]</span></p>
<p>This uniform domination demonstrates the <strong>inadmissibility</strong> of the MLE under squared error loss for <span class="math inline">\(p \geq 3\)</span>.</p>
<p>In an applied problem, the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with <span class="math inline">\(p=100\)</span> and <span class="math inline">\(n=100\)</span>, the risk of the MLE is <span class="math inline">\(R(\theta,\hat{\theta}_{MLE}) = 100\)</span> while the risk of the JS estimator is <span class="math inline">\(R(\theta,\hat{\theta}_{JS}) = 1.5\)</span>. The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.</p>
<p>The James-Stein estimator illustrates how incorporating prior information (via shrinkage) can lead to estimators with lower overall risk compared to the MLE, especially in high-dimensional settings. However, it is not the only shrinkage estimator that dominates the MLE. Other shrinkage estimators, such as the ridge regression estimator, also have lower risk than the MLE. The key insight is that shrinkage estimators can leverage prior information to improve estimation accuracy, especially in high-dimensional settings.</p>
<p>Note, that we used the empirical Bayes version of the definition of risk. Full Bayes approach incorporates both the data and the prior distribution of the parameter as in <a href="#eq-bayes-risk" class="quarto-xref">Equation&nbsp;<span>17.2</span></a>.</p>
</section>
<section id="sec-unbiasedness-optimality" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="sec-unbiasedness-optimality"><span class="header-section-number">17.2</span> Unbiasedness and the Optimality of Bayes Rules</h2>
<p>The optimal Bayes rule <span class="math inline">\(\delta_\pi(x) = \E{\theta \mid y}\)</span> is the posterior mean under squared error loss. An interesting feature of the Bayes rule is that it is biased except in degenerate cases like improper priors, which can lose optimality properties. This can be seen using the following decomposition.</p>
<p>If the rule was unbiased, then the Bayes risk would be zero. This follows by contradiction: assume for the sake of argument that <span class="math inline">\(\E[x\mid\theta]{\delta_{\pi}(y)} = \theta\)</span>, then <span class="math display">\[
r(\pi, \delta_{\pi}(y)) = \E[\pi]{\E[y\mid\theta]{(\delta_{\pi}(y) - \theta)^2}} = \E[\pi]{\theta^2} + \E[y]{\delta_{\pi}(y)}^2 - 2\E[\pi]{\theta \E[y\mid\theta]{\delta_{\pi}(y)}} = 0
\]</span> which is a contradiction since the Bayes risk cannot be zero in non-degenerate cases.</p>
<p>The key feature of Bayes rules is the bias-variance tradeoff inherent in their nature. You achieve a large reduction in variance for a small amount of bias. This is the underpinning of James-Stein estimation.</p>
<p>Another interesting feature is that the Bayes rule <span class="math inline">\(\E{\theta \mid y}\)</span> is always Bayes sufficient in the sense that <span class="math display">\[
\E[\pi]{\theta \mid \E[\pi]{\theta \mid y}} = \E[\pi]{\theta \mid y}
\]</span> So conditioning on <span class="math inline">\(\E[\pi]{\theta \mid y}\)</span> is equivalent to conditioning on <span class="math inline">\(y\)</span> when estimating <span class="math inline">\(\theta\)</span>. This property is used in quantile neural network approaches to generative methods.</p>
<div id="exm-baseball" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.2 (Example: James-Stein for Baseball Batting Averages)</strong></span> We reproduce the baseball batting average example from <span class="citation" data-cites="efron1977steins">Efron and Morris (<a href="references.html#ref-efron1977steins" role="doc-biblioref">1977</a>)</span>. Data below has the number of hits for 18 baseball players after 45 at-bats in 1970 season.</p>
<p>Now, we can estimate overall mean and variance</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(baseball<span class="sc">$</span>BattingAverage)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>sigma2_hat <span class="ot">&lt;-</span> <span class="fu">var</span>(baseball<span class="sc">$</span>BattingAverage)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>As well as the posterior mean for each player (James-Stein estimator)</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">LastName</th>
<th style="text-align: right;">AtBats</th>
<th style="text-align: right;">BattingAverage</th>
<th style="text-align: right;">SeasonAverage</th>
<th style="text-align: right;">JS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Clemente</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.40</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr class="even">
<td style="text-align: left;">Robinson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.38</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Howard</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.31</td>
</tr>
<tr class="even">
<td style="text-align: left;">Johnstone</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.30</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Berry</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="even">
<td style="text-align: left;">Spencer</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kessinger</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.29</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.28</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvarado</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Santo</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">Swaboda</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.24</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.26</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Petrocelli</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rodriguez</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scott</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Unser</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Williams</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.22</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.25</td>
</tr>
<tr class="even">
<td style="text-align: left;">Campaneris</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.20</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Munson</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.30</td>
<td style="text-align: right;">0.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alvis</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">0.16</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.22</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Plot below shows the observed averages vs.&nbsp;James-Stein estimate</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
<p>Calculate mean squared error (MSE) for observed and James-Stein estimates</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mse_observed <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>BattingAverage <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>mse_js <span class="ot">&lt;-</span> <span class="fu">mean</span>((baseball<span class="sc">$</span>JS <span class="sc">-</span> mu_hat)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (Observed): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_observed))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="do">## MSE (Observed): 0.004584</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"MSE (James-Stein): %.6f</span><span class="sc">\n</span><span class="st">"</span>, mse_js))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="do">## MSE (James-Stein): 0.001031</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>Now if we look at the season dynamics for Clemente</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&amp;y=1970</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"../data/clemente.csv"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>H) <span class="sc">/</span> <span class="fu">cumsum</span>(cl<span class="sc">$</span>AB)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot x,y starting from index 2</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">-</span>ind], y[<span class="sc">-</span>ind], <span class="at">type =</span> <span class="st">"o"</span>, <span class="at">ylab =</span> <span class="st">"Batting Average"</span>, <span class="at">xlab =</span> <span class="st">"Number at Bats"</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">bg =</span> <span class="st">"lightblue"</span>, <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add horizontal line for season average 145/412 and add text above line `Season Average`</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, <span class="dv">145</span> <span class="sc">/</span> <span class="dv">412</span> <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"Season Average"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">145</span> <span class="sc">/</span> <span class="dv">412</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Ted williams record is .406 in 1941, so you know the first data points are noise</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>JS[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"JS"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>JS[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">200</span>, baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>] <span class="sc">+</span> <span class="fl">0.005</span>, <span class="st">"After 45 At Bats"</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> baseball<span class="sc">$</span>BattingAverage[<span class="dv">1</span>], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
</div>
<section id="full-bayes-shrinkage" class="level3">
<h3 class="anchored" data-anchor-id="full-bayes-shrinkage">Full Bayes Shrinkage</h3>
<p>The alternative approach to the regularization is to use full Bayes, which places a prior distribution on the parameters and computes the <em>full posterior distribution</em> using the Bayes rule: <span class="math display">\[
p( \theta \mid  \tau, y ) = \frac{ f( y | \theta ) p( \theta \mid \tau ) }{ m(y \mid \tau) },
\]</span> here <span class="math display">\[
m(y \mid \tau) = \int f( y\mid  \theta ) p( \theta \mid \tau ) d \theta,
\]</span> Here <span class="math inline">\(m(y \mid \tau)\)</span> is the marginal beliefs about the data.</p>
<p>The <em>empirical Bayes</em> approach is to estimate the prior distribution <span class="math inline">\(p( \theta \mid \tau )\)</span> from the data. This can be done by maximizing the marginal likelihood <span class="math inline">\(m(y \mid \tau )\)</span> with respect to <span class="math inline">\(\tau\)</span>. The resulting estimator is called the <em>type II maximum likelihood estimator</em> (MMLE). <span class="math display">\[
\hat{\tau} = \arg \max_{\tau} \log m( y \mid \tau ).
\]</span></p>
<p>For example, in the normal-normal model, when <span class="math inline">\(\theta \sim N(\mu,\tau^2)\)</span> with <span class="math inline">\(\mu=0\)</span>, we can integrate out the high dimensional <span class="math inline">\(\theta\)</span> and find <span class="math inline">\(m(y | \tau)\)</span> in closed form as <span class="math inline">\(y_i \sim N(0, \sigma^2 + \tau^2)\)</span> <span class="math display">\[
m( y | \tau ) = ( 2 \pi)^{-n/2} ( \sigma^2 + \tau^2 )^{- n/2}  \exp \left ( - \frac{ \sum y_i^2 }{ 2 ( \sigma^2 + \tau^2) } \right )
\]</span> The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want to shrink to overall mean <span class="math inline">\(\bar y\)</span> and in this approach <span class="math display">\[
\theta \sim N(\mu,\tau^2).
\]</span> The original JS is <span class="math inline">\(\mu=0\)</span>. To estimate the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> you can do full Bayes or empirical Bayes that shrinks to overall grand mean <span class="math inline">\(\bar y\)</span>, which serves as the estimate of the original prior mean <span class="math inline">\(\mu\)</span>. It seems paradoxical that you estimate prior parameters from the data. However, this is not the case. You simply use mixture prior <span class="citation" data-cites="diaconis1983quantifying">Diaconis and Ylvisaker (<a href="references.html#ref-diaconis1983quantifying" role="doc-biblioref">1983</a>)</span> with marginal MLE (MMLE). The MMLE is the product <span class="math display">\[
\int_{\theta_i}\prod_{i=1}^k p(\bar y_i \mid \theta_i)p(\theta_i \mid \mu, \tau^2).
\]</span></p>
<p>The motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments. They have an ability to balance signal detection and noise suppression in high-dimensional settings. Unlike flat uniform priors, shrinkage priors adaptively shrink small signals towards zero while preserving large signals. This behavior is crucial for sparse estimation problems, where most parameters are expected to be zero or near-zero. The James-Stein procedure is an example of <em>global shrinkage</em>, when the overall sparsity level across all parameters is controlled, ensuring that the majority of parameters are shrunk towards zero. Later in this section we will discuss <em>local shrinkage</em> priors, such as the horseshoe prior, which allow individual parameters to escape shrinkage if they represent significant signals.</p>
<p>In summary, flat uniform priors (MLE) fail to provide adequate regularization in high-dimensional settings, leading to poor risk properties and overfitting. By incorporating probabilistic arguments and hierarchical structures, shrinkage priors offer a principled approach to regularization that aligns with Bayesian decision theory and modern statistical practice.</p>
</section>
</section>
<section id="bias-variance-decomposition" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="bias-variance-decomposition"><span class="header-section-number">17.3</span> Bias-Variance Decomposition</h2>
<p>The discussion of shrinkage priors and James-Stein estimation naturally leads us to a fundamental concept in statistical learning: the bias-variance decomposition. This decomposition provides the theoretical foundation for understanding why shrinkage methods like James-Stein can outperform maximum likelihood estimation, even when they introduce bias.</p>
<p>The key insight is that estimation error can be decomposed into two components: <em>bias</em> (systematic error) and <em>variance</em> (random error). While unbiased estimators like maximum likelihood have zero bias, they often suffer from high variance, especially in high-dimensional settings. Shrinkage methods intentionally introduce a small amount of bias to achieve substantial reductions in variance, leading to better overall performance.</p>
<p>This trade-off between bias and variance is not just a theoretical curiosity—it’s the driving force behind many successful machine learning algorithms, from ridge regression to neural networks with dropout. Understanding this decomposition helps us make principled decisions about model complexity and regularization.</p>
<p>For parameter estimation, we can decompose the mean squared error as follows: <span class="math display">\[
\begin{aligned}
\E{(\hat{\theta} - \theta)^2} &amp;= \E{(\hat{\theta} - \E{\hat{\theta}} + \E{\hat{\theta}} - \theta)^2} \\
&amp;= \E{(\hat{\theta} - \E{\hat{\theta}})^2} + \E{(\E{\hat{\theta}} - \theta)^2} \\
&amp;= \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2
\end{aligned}
\]</span> The cross term has expectation zero.</p>
<p>For prediction problems, we have <span class="math inline">\(y = \theta + \epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is independent with <span class="math inline">\(\text{Var}(\epsilon) = \sigma^2\)</span>. Hence <span class="math display">\[
\E{(y - \hat{y})^2} = \sigma^2 + \E{(\hat{\theta} - \theta)^2}
\]</span></p>
<p>This decomposition shows that prediction error consists of irreducible noise, estimation variance, and estimation bias. Bayesian methods typically trade a small increase in bias for a substantial reduction in variance, leading to better overall performance.</p>
<p>The bias-variance decomposition provides a framework for understanding estimation error, but it doesn’t tell us how to construct optimal estimators. To find the best decision rule—whether for estimation, hypothesis testing, or model selection—we need a more general framework that can handle different types of loss functions and prior information. This leads us to Bayesian decision theory and the concept of risk decomposition.</p>
<section id="risk-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="risk-decomposition">Risk Decomposition</h3>
<p>How does one find an optimal decision rule? It could be a test region, an estimation procedure or the selection of a model. Bayesian decision theory addresses this issue.</p>
<p>The <em>a posteriori</em> Bayes risk approach is as follows. Let <span class="math inline">\(\delta(x)\)</span> denote the decision rule. Given the prior <span class="math inline">\(\pi(\theta)\)</span>, we can simply calculate <span class="math display">\[
R_n  ( \pi , \delta ) = \int_x m( x )  \left \{ \int_\Theta \mathcal{L}( \theta , \delta(x) )  p( \theta | x ) d \theta \right \} d x .
\]</span> Then the optimal Bayes rule is to <em>pointwise</em> minimize the inner integral (a.k.a. the posterior Bayes risk), namely <span class="math display">\[
\delta^\star ( x ) = \arg \max_\delta \int_\Theta \mathcal{L}( \theta , \delta(x) )  p( \theta | x ) d \theta .
\]</span> The caveat is that this gives us no intuition into the characteristics of the prior which are important. Moreover, we do not need the marginal beliefs <span class="math inline">\(m(x)\)</span>.</p>
<p>For example, under squared error estimation loss, the optimal estimator is simply the posterior mean, <span class="math inline">\(\delta^\star (x)  = E( \theta | x )\)</span>. The properties of this optimal rule—including its inherent bias and the bias-variance tradeoff it embodies—were established in <a href="#sec-unbiasedness-optimality" class="quarto-xref"><span>Section 17.2</span></a>.</p>
</section>
</section>
<section id="sparsity" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="sparsity"><span class="header-section-number">17.4</span> Sparsity</h2>
<p>Let the true parameter be sparse with the form <span class="math inline">\(\theta_p = \left( \sqrt{d/p}, \ldots, \sqrt{d/p}, 0, \ldots, 0 \right)\)</span>. The problem of recovering a vector with many zero entries is called sparse signal recovery. The “ultra-sparse” or “nearly black” vector case occurs when <span class="math inline">\(p_n\)</span>, denoting the number of non-zero parameter values, satisfies <span class="math inline">\(\theta \in l_0[p_n]\)</span>, which denotes the set <span class="math inline">\(\#(\theta_i \neq 0) \leq p_n\)</span> where <span class="math inline">\(p_n = o(n)\)</span> and <span class="math inline">\(p_n \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>High-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks and present a challenge for classical statistical methods. Historically, James-Stein estimation (or <span class="math inline">\(\ell_2\)</span>-regularization) functions as a global shrinkage rule. Because it lacks local parameters to adapt to sparsity, it struggles to recover sparse signals effectively.</p>
<p>James-Stein is equivalent to the model: <span class="math display">\[
y_i = \theta_i + \epsilon_i \quad \text{and} \quad \theta_i \sim \mathcal{N}(0, \tau^2)
\]</span> For the sparse <span class="math inline">\(r\)</span>-spike problem, <span class="math inline">\(\hat{\theta}_{JS}\)</span> performs poorly and we require a different rule. rather than using softmax For <span class="math inline">\(\theta_p\)</span> we have: <span class="math display">\[
\frac{p \|\theta\|^2}{p + \|\theta\|^2} \leq R(\hat{\theta}^{JS}, \theta_p) \leq 2 + \frac{p \|\theta\|^2}{d + \|\theta\|^2}.
\]</span> This implies that <span class="math inline">\(R(\hat{\theta}^{JS}, \theta_p) \geq (p/2)\)</span>.</p>
<p>In the sparse case, a simple thresholding rule can beat MLE and JS when the signal is sparse. Assuming <span class="math inline">\(\sigma^2 = 1\)</span>, the thresholding estimator is: <span class="math display">\[
\hat{\theta}_{thr} = \begin{cases}
\hat{\theta}_i &amp; \text{if } \hat{\theta}_i &gt; \sqrt{2 \ln p} \\
0 &amp; \text{otherwise}
\end{cases}
\]</span> This simple example shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.</p>
<p>One such estimator that achieves the optimal minimax rate is the horseshoe estimator <span class="citation" data-cites="carvalho2010horseshoe">Carvalho, Polson, and Scott (<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">2010</a>)</span>, which we discuss in detail in <a href="#sec-horseshoe" class="quarto-xref"><span>Section 17.10</span></a>. It is a local shrinkage estimator that dominates the sample mean in MSE and has good posterior concentration properties for sparse signal problems.</p>
</section>
<section id="maximum-a-posteriori-estimation-map-and-regularization" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="maximum-a-posteriori-estimation-map-and-regularization"><span class="header-section-number">17.5</span> Maximum A posteriori Estimation (MAP) and Regularization</h2>
<p>Having established that Bayesian shrinkage reduces estimation risk, we now examine a computationally tractable alternative: maximum a posteriori (MAP) estimation. While Bayesian shrinkage provably reduces estimation risk, full posterior inference can be computationally demanding. Maximum a posteriori (MAP) estimation offers a tractable alternative that captures many regularization benefits without the cost of full integration. MAP captures the regularization benefits of Bayesian methods without requiring full posterior inference.</p>
<p>Given input-output pairs <span class="math inline">\((x_i,y_i)\)</span>, MAP learns the function <span class="math inline">\(f\)</span> that maps inputs <span class="math inline">\(x_i\)</span> to outputs <span class="math inline">\(y_i\)</span> by minimizing <span class="math display">\[
\underset{f}{\mathrm{minimize}} \quad \sum_{i=1}^N  \mathcal{L}(y_i,f(x_i)) + \lambda \phi(f).
\]</span> The first term is the <em>loss function</em> that measures the difference between the predicted output <span class="math inline">\(f(x_i)\)</span> and the true output <span class="math inline">\(y_i\)</span>. The second term is a <em>regularization term</em> that penalizes complex functions <span class="math inline">\(f\)</span> to prevent overfitting. The parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between fitting the data well and keeping the function simple. In the case when <span class="math inline">\(f\)</span> is a parametric model, then we simply replace <span class="math inline">\(f\)</span> with the parameters <span class="math inline">\(\theta\)</span> of the model, and the regularization term becomes a penalty on the parameters.</p>
<p>The loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when <span class="math inline">\(y\)</span> is numeric and <span class="math inline">\(y_i \mid x_i \sim N(f(x_i),\sigma^2)\)</span>, we get the squared loss <span class="math inline">\(\mathcal{L}(y,f(x)) = (y-f(x))^2\)</span>. When <span class="math inline">\(y_i\in \{0,1\}\)</span> is binary, we use the logistic loss <span class="math inline">\(\mathcal{L}(y,f(x)) = \log(1+\exp(-yf(x)))\)</span>.</p>
<p>The penalty term <span class="math inline">\(\lambda \phi(f)\)</span> discourages complex functions <span class="math inline">\(f\)</span>. Then, we can think of regularization as a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.</p>
<div id="exm-traffic" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.3 (Traffic)</strong></span> Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. <a href="#fig-speed-profile" class="quarto-xref">Figure&nbsp;<span>17.1</span></a> shows measured speed data, averaged over five minute intervals on one of the weekdays.</p>
<div id="fig-speed-profile" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/day_295.svg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-speed-profile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: Speed profile over 24 hour period on I-55, on October 22, 2013
</figcaption>
</figure>
</div>
<p>The statistical model is <span class="math display">\[
y_t = f_t + \epsilon_t, ~ \epsilon_t \sim N(0,\sigma^2), ~ t=1,\ldots,n,
\]</span> where <span class="math inline">\(y_t\)</span> is the speed measurement at time <span class="math inline">\(t\)</span>, <span class="math inline">\(f_t\)</span> is the true underlying speed at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\epsilon_t\)</span> is the measurement noise. There are two sources of noise. The first is the measurement noise, caused by the inherent nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where the sensor is installed might not represent well traffic in other lanes. A naive MLE approach would be to estimate the speed profile <span class="math inline">\(f = (f_1, \ldots, f_n)\)</span> by minimizing the squared loss <span class="math display">\[
\hat f = \arg\min_{f} \sum_{t=1}^{n} (y_t - f_t)^2.
\]</span> However, the minima of this loss function is 0 and corresponds to the case when <span class="math inline">\(\hat f_t = y_t\)</span> for all <span class="math inline">\(t\)</span>. We have learned nothing about the speed profile, and the estimate is simply the noisy observation <span class="math inline">\(y_t\)</span>. In this case, we have no way to distinguish between the true speed profile and the noise.</p>
<p>However, we can use regularization and bring some prior knowledge about traffic speed profiles to improve the estimate of the speed profile and to remove the noise.</p>
<p>Specifically, we will use a <em>trend filtering</em> approach. Under this approach, we assume that the speed profile <span class="math inline">\(f\)</span> is a piece-wise linear function of time, and we want to find a function that captures the underlying trend while ignoring the noise. The regularization term <span class="math inline">\(\phi(f)\)</span> is then the second difference of the speed profile, <span class="math display">\[
\lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|
\]</span> which penalizes the “kinks” in the speed profile. The value of this penalty is zero, when <span class="math inline">\(f_{t-1}, f_t, f_{t+1}\)</span> lie on a straight line, and it increases when the speed profile has a kink. The parameter <span class="math inline">\(\lambda\)</span> is a regularization parameter that controls the strength of the penalty.</p>
<p>Trend filtering penalized function is then <span class="math display">\[
(1/2) \sum_{t=1}^{n}(y_t - f_t)^2 + \lambda \sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|,
\]</span> which is a variation of a well-known Hodrick-Prescott filter.</p>
<p>This approach requires us to choose the regularization parameter <span class="math inline">\(\lambda\)</span>. A small value of <span class="math inline">\(\lambda\)</span> will lead to a function that fits the data well, but may not capture the underlying trend. A large value of <span class="math inline">\(\lambda\)</span> will lead to a function that captures the underlying trend, but may not fit the data well. The optimal value of <span class="math inline">\(\lambda\)</span> can be chosen using cross-validation or other model selection techniques. The left panel of <a href="#fig-traffic" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> shows the trend filtering for different values of <span class="math inline">\(\lambda \in \{5,50,500\}\)</span>. The right panel shows the optimal value of <span class="math inline">\(\lambda\)</span> chosen by cross-validation (by visual inspection).</p>
<div id="fig-traffic" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-traffic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/traffic_l1.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filter for different penalty</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/day_295_tf.svg" class="img-fluid figure-img"></p>
<figcaption>Trend filtering with optimal penalty</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-traffic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: Trend Filtering for Traffic Speed Data
</figcaption>
</figure>
</div>
</div>
</section>
<section id="the-duality-between-regularization-and-priors" class="level2" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="the-duality-between-regularization-and-priors"><span class="header-section-number">17.6</span> The Duality Between Regularization and Priors</h2>
<p>A large number of statistical problems can be expressed in the canonical optimization form:</p>
<p><span id="eq-canonical-form"><span class="math display">\[
\begin{aligned}
&amp; \underset{x \in \mathbb{R}^d}{\text{minimize}}
&amp; &amp; l(x) + \phi(x)
\end{aligned}
\tag{17.3}\]</span></span></p>
<p>Perhaps the most common example arises in estimating the regression coefficients <span class="math inline">\(x\)</span> in a generalized linear model. Here <span class="math inline">\(l(x)\)</span> is a negative log likelihood or some other measure of fit, and <span class="math inline">\(\phi(x)\)</span> is a penalty function that effects a favorable bias-variance tradeoff.</p>
<p>From the Bayesian perspective, <span class="math inline">\(l(x)\)</span> and <span class="math inline">\(\phi(x)\)</span> correspond to the negative logarithms of the sampling model and prior distribution in the hierarchical model:</p>
<p><span id="eq-bayes-version"><span class="math display">\[
p(y \mid x) \propto \exp\{-l(x)\}, \quad p(x) \propto \exp\{-\phi(x)\}
\tag{17.4}\]</span></span></p>
<p>and the solution to <a href="#eq-canonical-form" class="quarto-xref">Equation&nbsp;<span>17.3</span></a> may be interpreted as a maximum <em>a posteriori</em> (MAP) estimate.</p>
<p>Another common case is where <span class="math inline">\(x\)</span> is a variable in a decision problem where options are to be compared on the basis of expected loss, and where <span class="math inline">\(l(x)\)</span> and <span class="math inline">\(\phi(x)\)</span> represent conceptually distinct contributions to the loss function. For example, <span class="math inline">\(l(x)\)</span> may be tied to the data, and <span class="math inline">\(\phi(x)\)</span> to the intrinsic cost associated with the decision.</p>
<section id="map-as-a-poor-mans-bayesian" class="level3">
<h3 class="anchored" data-anchor-id="map-as-a-poor-mans-bayesian">MAP as a Poor Man’s Bayesian</h3>
<p>There is a duality between using regularization term in optimization problem and assuming a prior distribution over the parameters of the model <span class="math inline">\(f\)</span>. Given the likelihood <span class="math inline">\(L(y_i,f(x_i))\)</span>, the posterior is given by Bayes’ rule: <span class="math display">\[
p(f \mid y, x) = \frac{\prod_{i=1}^n L(y_i,f(x_i)) p(f)}{p(y \mid x)}.
\]</span> If we take the negative log of this posterior, we get: <span class="math display">\[
-\log p(f \mid y, x) = - \sum_{i=1}^n \log L(y_i,f(x_i)) - \log p(f) + \log p(y \mid x).
\]</span> Since loss is the negative log-likelihood <span class="math inline">\(\mathcal{L}(y_i,f(x_i)) = -\log L(y_i,f(x_i))\)</span>, the posterior maximization is equivalent to minimizing the following regularized loss function: <span class="math display">\[
\sum_{i=1}^n \mathcal{L}(y_i,f(x_i)) - \log p(f).
\]</span> The last term <span class="math inline">\(\log p(y_i \mid x_i)\)</span> does not depend on <span class="math inline">\(f\)</span> and can be ignored in the optimization problem. Thus, the equivalence is given by: <span class="math display">\[
\lambda \phi(f) = -\log p(f),
\]</span> where <span class="math inline">\(\phi(f)\)</span> is the penalty term that corresponds to the prior distribution of <span class="math inline">\(f\)</span>. Below we will consider several choices for the prior distribution of <span class="math inline">\(f\)</span> and the corresponding penalty term <span class="math inline">\(\phi(f)\)</span> commonly used in practice.</p>
</section>
</section>
<section id="ridge-regression-ell_2-norm" class="level2" data-number="17.7">
<h2 data-number="17.7" class="anchored" data-anchor-id="ridge-regression-ell_2-norm"><span class="header-section-number">17.7</span> Ridge Regression (<span class="math inline">\(\ell_2\)</span> Norm)</h2>
<section id="tikhonov-regularization-framework" class="level3">
<h3 class="anchored" data-anchor-id="tikhonov-regularization-framework">Tikhonov Regularization Framework</h3>
<p>The Tikhonov regularization framework provides a general setting for regression that connects classical regularization theory with modern Bayesian approaches. Given observed data <span class="math inline">\((x_i, y_i)_{i=1}^n\)</span> and a parametric model <span class="math inline">\(f(x,w)\)</span> with parameter vector <span class="math inline">\(w \in \mathbb{R}^k\)</span>, we define a data misfit functional: <span class="math display">\[
E_D(w) = \sum_{i=1}^n (y_i - f(x_i,w))^2
\]</span></p>
<p>This yields a Gaussian likelihood: <span class="math display">\[
p(y \mid w) = \frac{1}{Z_D}\exp\left(-\frac{1}{2\sigma^2}E_D(w)\right)
\]</span> where <span class="math inline">\(\sigma^2\)</span> denotes the noise variance and <span class="math inline">\(Z_D\)</span> is a normalization constant.</p>
<p>A Gaussian prior on the weights can be specified as: <span class="math display">\[
p(w) = \frac{1}{Z_W}\exp\left(-\frac{1}{2\sigma_w^2}E_W(w)\right)
\]</span> where <span class="math inline">\(E_W(w)\)</span> denotes a quadratic penalty on <span class="math inline">\(w\)</span> (e.g., <span class="math inline">\(E_W(w) = w^T w\)</span>), <span class="math inline">\(\sigma_w^2\)</span> is the prior variance, and <span class="math inline">\(Z_W\)</span> is its normalization constant.</p>
<p>The hyperparameters <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\sigma_w^2\)</span> control the strength of the noise and the prior. Following MacKay’s notation, we define precisions <span class="math inline">\(\tau_D^2 = 1/\sigma^2\)</span> and <span class="math inline">\(\tau_w^2 = 1/\sigma_w^2\)</span>. Let <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> denote the Hessians of <span class="math inline">\(E_D(w)\)</span> and <span class="math inline">\(E_W(w)\)</span> at the maximum a posteriori (MAP) estimate <span class="math inline">\(w^{\text{MAP}}\)</span>. The Hessian of the negative log-posterior is then <span class="math inline">\(A = \tau_w^2 C + \tau_D^2 B\)</span>.</p>
<p>Evaluating the log-evidence under a Gaussian Laplace approximation yields: <span class="math display">\[
\log p(D \mid \tau_w^2,\tau_D^2) = -\tau_w^2 E_W^{\text{MAP}} - \tau_D^2 E_D^{\text{MAP}} - \frac{1}{2}\log\det A - \log Z_W(\tau_w^2) - \log Z_D(\tau_D^2) + \frac{k}{2}\log(2\pi)
\]</span></p>
<p>The associated Occam factor, which penalizes excessively small prior variances, is: <span class="math display">\[
-\tau_w^2 E_W^{\text{MAP}} - \frac{1}{2}\log\det A - \log Z_W(\tau_w^2)
\]</span></p>
<p>This represents the reduction in effective volume of parameter space from prior to posterior.</p>
<blockquote class="blockquote">
<p>[!NOTE] <strong>Notation:</strong> In the previous sections on Normal Means, we used <span class="math inline">\(\theta\)</span> to denote the parameters. In the context of regression, we will follow standard convention and use <span class="math inline">\(\beta\)</span> to denote the vector of coefficients.</p>
</blockquote>
<p>The ridge regression uses a Gaussian prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to a squared penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta \sim N(0, \sigma_\beta^2 I),
\]</span> where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(\sigma_\beta^2\)</span> is the prior variance (distinct from the noise variance <span class="math inline">\(\sigma^2\)</span>). The prior distribution of <span class="math inline">\(\beta\)</span> is a multivariate normal distribution with mean 0 and covariance <span class="math inline">\(\sigma_\beta^2 I\)</span>. The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{2\sigma_\beta^2} \|\beta\|_2^2 + \text{const},
\]</span> where <span class="math inline">\(\|\beta\|_2^2 = \sum_{j=1}^p \beta_j^2\)</span> is the squared 2-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{2\sigma_\beta^2} \|\beta\|_2^2.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 + \lambda \|\beta\|_2^2,
\]</span> where <span class="math inline">\(\lambda = 1/\sigma^2\)</span> is the regularization parameter that controls the strength of the prior. The solution to this optimization problem is given by: <span class="math display">\[
\hat{\beta}_{\text{ridge}} = ( X^T X + \lambda I )^{-1} X^T y.
\]</span> The regularization parameter <span class="math inline">\(\lambda\)</span> is related to the variance of the prior distribution. When <span class="math inline">\(\lambda=0\)</span>, the function <span class="math inline">\(f\)</span> is the maximum likelihood estimate of the parameters. When <span class="math inline">\(\lambda\)</span> is large, the function <span class="math inline">\(f\)</span> is the prior mean of the parameters. When <span class="math inline">\(\lambda\)</span> is infinite, the function <span class="math inline">\(f\)</span> is the prior mode of the parameters.</p>
<p>Notice, that the OLS estimate (invented by Gauss) is a special case of ridge regression when <span class="math inline">\(\lambda = 0\)</span>: <span class="math display">\[
\hat{\beta}_{\text{OLS}} = ( X^T X )^{-1} X^T y.
\]</span></p>
<p>The original motivation for ridge regularization was to address the problem of numerical instability in the OLS solution when the design matrix <span class="math inline">\(X\)</span> is ill-conditioned, i.e.&nbsp;when <span class="math inline">\(X^T X\)</span> is close to singular. In this case, the OLS solution can be very sensitive to small perturbations in the data, leading to large variations in the estimated coefficients <span class="math inline">\(\hat{\beta}\)</span>. This is particularly problematic when the number of features <span class="math inline">\(p\)</span> is large, as the condition number of <span class="math inline">\(X^T X\)</span> can grow rapidly with <span class="math inline">\(p\)</span>. The ridge regression solution stabilizes the OLS solution by adding a small positive constant <span class="math inline">\(\lambda\)</span> to the diagonal of the <span class="math inline">\(X^T X\)</span> matrix, which improves the condition number and makes the solution more robust to noise in the data. The additional term <span class="math inline">\(\lambda I\)</span> simply shifts the eigenvalues of <span class="math inline">\(X^T X\)</span> away from zero, thus improving the numerical stability of the inversion.</p>
<p>Another way to think and write the objective function of Ridge as the following constrained optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_2^2 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the size of the coefficients <span class="math inline">\(\beta\)</span>. This formulation emphasizes the idea that ridge regression is a form of regularization that constrains the size of the coefficients, preventing them from growing too large and leading to overfitting. The constraint <span class="math inline">\(\|\beta\|_2^2 \leq t\)</span> can be interpreted as a budget on the size of the coefficients, where larger values of <span class="math inline">\(t\)</span> allow for larger coefficients and more complex models.</p>
<p>Constraint on the model parameters (and the original Ridge estimator) was proposed by <span class="citation" data-cites="tikhonov1943stability">Tikhonov et al. (<a href="references.html#ref-tikhonov1943stability" role="doc-biblioref">1943</a>)</span> for solving inverse problems to “discover” physical laws from observations. The norm of the <span class="math inline">\(\beta\)</span> vector would usually represent amount of energy required. Many processes in nature are energy minimizing!</p>
<p>Again, the tuning parameter <span class="math inline">\(\lambda\)</span> controls trade-off between how well model fits the data and how small <span class="math inline">\(\beta\)</span>s are. Different values of <span class="math inline">\(\lambda\)</span> will lead to different models. We select <span class="math inline">\(\lambda\)</span> using cross validation.</p>
<div id="exm-simulated-bias" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.4 (Shrinkage)</strong></span> Consider a simulated data with <span class="math inline">\(n=50\)</span>, <span class="math inline">\(p=30\)</span>, and <span class="math inline">\(\sigma^2=1\)</span>. The true model is linear with <span class="math inline">\(10\)</span> large coefficients between <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>Our approximators <span class="math inline">\(\hat f_{\beta}\)</span> is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss <span class="math inline">\(1/n\|y -\hat y\|_2^2\)</span> and variance can be empirically calculated as <span class="math inline">\(1/n\sum  (\bar{\hat{y}} - \hat y_i)^2\)</span></p>
<p>Bias squared <span class="math inline">\(\mathrm{Bias}(\hat{y})^2=0.006\)</span> and variance <span class="math inline">\(\Var{\hat{y}} =0.627\)</span>. Thus, the prediction error = <span class="math inline">\(1 + 0.006 + 0.627 = 1.633\)</span></p>
<p>We’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate how big a gain we can achieve with Ridge regression.</p>
<p>The figure below shows the distribution of true coefficient values used to generate the data. The histogram reveals a bimodal structure: approximately 20 coefficients are exactly zero (the tall bar at 0.0), while 10 coefficients are non-zero with values concentrated between 0.5 and 1.0. This sparse structure, where most features are irrelevant and only a subset truly matter for prediction, is common in many real-world problems such as genomics, text analysis, and high-dimensional sensor data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/ridge_beta.svg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>True model coefficients</figcaption>
</figure>
</div>
<p>The key question is: what value of the regularization parameter <span class="math inline">\(\lambda\)</span> should we choose? Too small, and we don’t shrink enough to reduce variance; too large, and we introduce excessive bias by shrinking the true non-zero coefficients toward zero. The optimal choice balances these competing concerns.</p>
<p><a href="#fig-ridge-mse" class="quarto-xref">Figure&nbsp;<span>17.3 (a)</span></a> shows how prediction error varies with the amount of shrinkage (controlled by <span class="math inline">\(\lambda\)</span>). The horizontal dashed line represents the constant prediction error of 1.633 from ordinary linear regression (OLS). The red curve shows Ridge regression’s prediction error as we increase regularization. At low shrinkage (left side), Ridge behaves similarly to OLS with high variance. As we increase shrinkage, the prediction error decreases substantially, reaching a minimum around <span class="math inline">\(\lambda \approx 5\)</span>. Beyond this point, excessive shrinkage introduces too much bias, and prediction error begins to rise again. The U-shaped curve is characteristic of the bias-variance trade-off: we need enough regularization to stabilize predictions, but not so much that we distort the true signal.</p>
<p><a href="#fig-ridge-bias-variance" class="quarto-xref">Figure&nbsp;<span>17.3 (b)</span></a> decomposes Ridge regression’s prediction error into its constituent parts as a function of <span class="math inline">\(\lambda\)</span>. The black dashed horizontal line at approximately 1.0 represents the irreducible error from noise in the data. The blue curve (Ridge Var) shows variance decreasing monotonically as <span class="math inline">\(\lambda\)</span> increases: stronger regularization makes the model more stable across different training sets. The red curve (Ridge Bias<span class="math inline">\(^2\)</span>) shows squared bias increasing as <span class="math inline">\(\lambda\)</span> grows: we move further from the true model by shrinking coefficients. The black curve (Ridge MSE) is the sum of these components plus the irreducible error. The optimal <span class="math inline">\(\lambda\)</span> occurs where the rate of variance reduction exactly balances the rate of bias increase, minimizing total prediction error.</p>
<div id="fig-ridge-analysis" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ridge-analysis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ridge-analysis" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ridge-mse" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ridge-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/ridge_mse.svg" class="img-fluid figure-img" data-ref-parent="fig-ridge-analysis">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ridge-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Prediction error as a function of <span class="math inline">\(\lambda\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ridge-analysis" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ridge-bias-variance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ridge-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/ridge_bias_variance.svg" class="img-fluid figure-img" data-ref-parent="fig-ridge-analysis">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ridge-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Bias-variance decomposition
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ridge-analysis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: Ridge regression performance metrics
</figcaption>
</figure>
</div>
<p>At the optimal value of <span class="math inline">\(\lambda\)</span>, Ridge regression achieves squared bias of 0.077 and variance of 0.402, yielding a total prediction error of <span class="math inline">\(1 + 0.077 + 0.402 = 1.48\)</span>. Compare this to the OLS solution with squared bias of 0.006 and variance of 0.627, giving prediction error of <span class="math inline">\(1 + 0.006 + 0.627 = 1.633\)</span>. By accepting a modest increase in bias (from 0.006 to 0.077), we achieve a substantial reduction in variance (from 0.627 to 0.402), resulting in an overall improvement of approximately 9% in prediction error.</p>
<p>This example illustrates a fundamental principle in statistical learning: the best predictor is not necessarily the one that fits the training data most closely. By deliberately introducing bias through regularization, we can build models that generalize better to new data. The Bayesian perspective makes this trade-off explicit: the prior distribution encodes our belief that coefficients should be small, and the posterior balances this belief against the evidence in the data.</p>
</div>
</section>
<section id="kernel-view-of-ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="kernel-view-of-ridge-regression">Kernel View of Ridge Regression</h3>
<p>Another interesting view stems from what is called the push-through matrix identity: <span class="math display">\[
(aI + UV)^{-1}U = U(aI + VU)^{-1}
\]</span> for <span class="math inline">\(a\)</span>, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> such that the products are well-defined and the inverses exist. We can obtain this from <span class="math inline">\(U(aI + VU) = (aI + UV)U\)</span>, followed by multiplication by <span class="math inline">\((aI + UV)^{-1}\)</span> on the left and the right. Applying the identity above to the ridge regression solution with <span class="math inline">\(a = \lambda\)</span>, <span class="math inline">\(U = X^T\)</span>, and <span class="math inline">\(V = X\)</span>, we obtain an alternative form for the ridge solution: <span class="math display">\[
\hat{\beta} = X^T (XX^T + \lambda I)^{-1} Y.
\]</span> This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as <span class="math display">\[
X\hat{\beta} = XX^T (XX^T + \lambda I)^{-1} Y.
\]</span> What does this remind you of? This is precisely <span class="math inline">\(K(K + \lambda I)^{-1}Y\)</span> where <span class="math inline">\(K = XX^T\)</span>, which, recall, is the fit from RKHS regression with a linear kernel <span class="math inline">\(k(x, z) = x^T z\)</span>. Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)</p>
</section>
</section>
<section id="scale-mixtures-representations" class="level2" data-number="17.8">
<h2 data-number="17.8" class="anchored" data-anchor-id="scale-mixtures-representations"><span class="header-section-number">17.8</span> Scale Mixtures Representations</h2>
<p>Why should we care about expressing distributions as mixtures? The answer lies in computational tractability and theoretical insight. Many important priors used in sparse estimation—including the Laplace (Lasso), horseshoe, and logistic—can be represented as Gaussian distributions with random variance. This representation is far more than a mathematical curiosity: it enables efficient Gibbs sampling algorithms where each conditional distribution has a closed form, and it reveals deep connections between seemingly different regularization approaches.</p>
<p>Scale mixtures of normals provide a powerful framework for constructing flexible priors and computational algorithms in Bayesian statistics. The key insight is that many useful distributions can be represented as Gaussian distributions with random variance, leading to tractable MCMC algorithms and analytical insights.</p>
</section>
<section id="lasso-regression-ell_1-norm" class="level2" data-number="17.9">
<h2 data-number="17.9" class="anchored" data-anchor-id="lasso-regression-ell_1-norm"><span class="header-section-number">17.9</span> Lasso Regression (<span class="math inline">\(\ell_1\)</span> Norm)</h2>
<p>The Lasso (Least Absolute Shrinkage and Selection Operator) regression uses a Laplace prior on the parameters of the model <span class="math inline">\(f\)</span>, which leads to an <span class="math inline">\(\ell_1\)</span> penalty term. Specifically, we assume that the parameters <span class="math inline">\(\beta\)</span> of the model <span class="math inline">\(f(x) = x^T\beta\)</span> are distributed as: <span class="math display">\[
\beta_j \sim \text{Laplace}(0, b) \quad \text{independently for } j = 1, \ldots, p,
\]</span> where <span class="math inline">\(b &gt; 0\)</span> is the scale parameter. The Laplace distribution has the probability density function: <span class="math display">\[
p(\beta_j \mid b) = \frac{1}{2b}\exp\left(-\frac{|\beta_j|}{b}\right)
\]</span> and is shown in <a href="#fig-laplace-distribution" class="quarto-xref">Figure&nbsp;<span>17.4</span></a>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PLot Laplace distribution</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>laplace_pdf <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, b) {</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">2</span> <span class="sc">*</span> b)) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">abs</span>(beta) <span class="sc">/</span> b)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>laplace_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">beta =</span> beta, <span class="at">pdf =</span> <span class="fu">laplace_pdf</span>(beta, b))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(laplace_data, <span class="fu">aes</span>(<span class="at">x =</span> beta, <span class="at">y =</span> pdf)) <span class="sc">+</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">""</span>, <span class="at">x =</span> <span class="st">"Beta"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output-display">
<div id="fig-laplace-distribution" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-laplace-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-laplace-distribution-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="336">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-laplace-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: Laplace Distribution PDF
</figcaption>
</figure>
</div>
</div>
</div>
<p>The negative log of this prior distribution is given by: <span class="math display">\[
-\log p(\beta) = \frac{1}{b} \|\beta\|_1 + \text{const},
\]</span> where <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^p |\beta_j|\)</span> is the <span class="math inline">\(\ell_1\)</span>-norm of the vector <span class="math inline">\(\beta\)</span>. The regularization term <span class="math inline">\(\phi(f)\)</span> is then given by: <span class="math display">\[
\phi(f) = \frac{1}{b} \|\beta\|_1.  
\]</span> This leads to the following optimization problem: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 + \lambda \|\beta\|_1,
\]</span> where <span class="math inline">\(\lambda = 2\sigma^2/b\)</span> is the regularization parameter that controls the strength of the prior. Unlike ridge regression, the Lasso optimization problem does not have a closed-form solution due to the non-differentiable nature of the <span class="math inline">\(\ell_1\)</span> penalty. However, efficient algorithms such as coordinate descent and proximal gradient methods can be used to solve it.</p>
<p>The key distinguishing feature of Lasso is its ability to perform automatic variable selection. The <span class="math inline">\(\ell_1\)</span> penalty encourages sparsity in the coefficient vector <span class="math inline">\(\hat{\beta}\)</span>, meaning that many coefficients will be exactly zero. This property makes Lasso particularly useful for high-dimensional problems where feature selection is important.</p>
<p>When <span class="math inline">\(\lambda=0\)</span>, the Lasso reduces to the ordinary least squares (OLS) estimate. As <span class="math inline">\(\lambda\)</span> increases, more coefficients are driven to exactly zero, resulting in a sparser model. When <span class="math inline">\(\lambda\)</span> is very large, all coefficients become zero.</p>
<p>The geometric intuition behind Lasso’s sparsity-inducing property comes from the constraint formulation. We can write the Lasso problem as: <span class="math display">\[
\underset{\beta}{\mathrm{minimize}}\quad \|y- X\beta\|_2^2 \quad \text{subject to} \quad \|\beta\|_1 \leq t,
\]</span> where <span class="math inline">\(t\)</span> is a positive constant that controls the sparsity of the solution. The constraint region <span class="math inline">\(\|\beta\|_1 \leq t\)</span> forms a diamond (in 2D) or rhombus-shaped region with sharp corners at the coordinate axes. The optimal solution often occurs at these corners, where some coefficients are exactly zero.</p>
<p>From a Bayesian perspective, the Lasso estimator corresponds to the maximum a posteriori (MAP) estimate under independent Laplace priors on the coefficients. We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior: <span class="math display">\[
-\log p(\beta \mid y, b) = \|y-X\beta\|_2^2 + \frac{2\sigma^2}{b}\|\beta\|_1.
\]</span> For fixed <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(b&gt;0\)</span>, the posterior mode is equivalent to the Lasso estimate with <span class="math inline">\(\lambda = 2\sigma^2/b\)</span>. Large variance <span class="math inline">\(b\)</span> of the prior is equivalent to small penalty weight <span class="math inline">\(\lambda\)</span> in the Lasso objective function.</p>
<p>One of the most popular algorithms for solving the Lasso problem is coordinate descent. The algorithm iteratively updates each coefficient while holding all others fixed. For the <span class="math inline">\(j\)</span>-th coefficient, the update rule is: <span class="math display">\[
\hat{\beta}_j \leftarrow \text{soft}\left(\frac{1}{n}\sum_{i=1}^n x_{ij}(y_i - \sum_{k \neq j} x_{ik}\hat{\beta}_k), \frac{\lambda}{n}\right),
\]</span> where the soft-thresholding operator is defined as: <span class="math display">\[
\text{soft}(z, \gamma) = \text{sign}(z)(|z| - \gamma)_+ = \begin{cases}
z - \gamma &amp; \text{if } z &gt; \gamma \\
0 &amp; \text{if } |z| \leq \gamma \\
z + \gamma &amp; \text{if } z &lt; -\gamma
\end{cases}
\]</span></p>
<div id="exm-lasso-sparsity" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.5 (Sparsity and Variable Selection)</strong></span> We will demonstrate the Lasso’s ability to perform variable selection and shrinkage using simulated data. The data will consist of a design matrix with correlated predictors and a sparse signal, where only a 5 out of 20 predictors have non-zero coefficients.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># True coefficients - sparse signal</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sparse_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(beta_true <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate response</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> sigma <span class="sc">*</span> <span class="fu">rnorm</span>(n)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Then we use <code>glmnet</code> package to fit the Lasso model and visualize the coefficient paths. We will also perform cross-validation to select the optimal regularization parameter <span class="math inline">\(\lambda\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit LASSO path using glmnet</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot coefficient paths</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>LASSO Coefficient Paths</figcaption>
</figure>
</div>
</div>
</div>
<p>The coefficient paths plot shows how LASSO coefficients shrink toward zero as the regularization parameter lambda increases. The colored lines represent different predictors, demonstrating LASSO’s variable selection property. Note, that <code>glmnet</code> fitted the model for a sequence of <span class="math inline">\(\lambda\)</span> values. The algorithms starts with a large lambda value, where all coefficients are penalized to zero. Then, it gradually decreases lambda, using the coefficients from the previous, slightly more penalized model as a “warm start” for the current calculation. This pathwise approach is significantly more efficient than starting the optimization from scratch for every single <span class="math inline">\(\lambda\)</span>. By default, glmnet computes the coefficients for a sequence of 100 lambda values spaced evenly on the logarithmic scale, starting from a data-driven maximum value (where all coefficients are zero) down to a small fraction of that maximum. The user can specify their own sequence of lambda values if specific granularity or range is desired</p>
<p>Finally, we will perform cross-validation to select the optimal <span class="math inline">\(\lambda\)</span> value and compare the estimated coefficients with the true values.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Cross-validation for LASSO</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, we can extract the coefficients <code>lambda.min</code> and <code>lambda.1se</code> from the cross-validation results, which correspond to the minimum cross-validated error and the most regularized model within one standard error of the minimum, respectively.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients at optimal lambda</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>lambda_min <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda.min</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>lambda_1se <span class="ot">&lt;-</span> cv_lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>coef_min <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_min)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>coef_1se <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_fit, <span class="at">s =</span> lambda_1se)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print values of lambda</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal lambda (min):"</span>, lambda_min, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Optimal lambda (min): 0.016</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal lambda (1se):"</span>, lambda_1se, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Optimal lambda (1se): 0.1</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="17-theoryai_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Coefficient Estimates Comparison</figcaption>
</figure>
</div>
</div>
</div>
<p>It seems like LASSO has successfully identified the non-zero coefficients and shrunk the noise variables to zero. The coefficient estimates at <code>lambda.min</code> and <code>lambda.1se</code> show that LASSO retains the true signals while effectively ignoring the noise. Let’s calculate the prediction errors and evaluate the variable selection performance of LASSO at both optimal <span class="math inline">\(\lambda\)</span> values.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate prediction errors</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>pred_min <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_min)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>pred_1se <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_fit, <span class="at">newx =</span> X, <span class="at">s =</span> lambda_1se)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>mse_min <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_min)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>mse_1se <span class="ot">&lt;-</span> <span class="fu">mean</span>((y <span class="sc">-</span> pred_1se)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.min):"</span>, <span class="fu">round</span>(mse_min, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean Squared Error (lambda.min): 0.68</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean Squared Error (lambda.1se):"</span>, <span class="fu">round</span>(mse_1se, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Mean Squared Error (lambda.1se): 0.85</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In summary, this example demonstrates how LASSO regression can be used for both variable selection and regularization in high-dimensional settings. By tuning the regularization parameter <span class="math inline">\(\lambda\)</span>, LASSO is able to shrink irrelevant coefficients to zero, effectively identifying the true underlying predictors while controlling model complexity. The comparison of coefficient estimates and prediction errors at different <span class="math inline">\(\lambda\)</span> values highlights the trade-off between model sparsity and predictive accuracy. LASSO’s ability to produce interpretable, sparse models makes it a powerful tool in modern statistical learning, especially when dealing with datasets where the number of predictors may be large relative to the number of observations.</p>
</div>
<section id="lasso-as-a-scale-mixture" class="level3">
<h3 class="anchored" data-anchor-id="lasso-as-a-scale-mixture">Lasso as a Scale Mixture</h3>
<p>The Laplace distribution can be represented as a scale mixture of Normal distributions <span class="citation" data-cites="andrews1974scale">(<a href="references.html#ref-andrews1974scale" role="doc-biblioref">Andrews and Mallows 1974</a>)</span>: <span class="math display">\[
\begin{aligned}
\beta_j \mid \sigma^2,\tau_j &amp;\sim N(0,\tau_j^2\sigma^2)\\
\tau_j^2 \mid \alpha &amp;\sim \text{Exp}(\alpha^2/2)\\
\sigma^2 &amp;\sim \pi(\sigma^2).
\end{aligned}
\]</span> We can show equivalence by integrating out <span class="math inline">\(\tau_j\)</span>: <span class="math display">\[
\begin{aligned}
p(\beta_j\mid \sigma^2,\alpha) &amp;= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi \tau_j\sigma^2}}\exp\left(-\frac{\beta_j^2}{2\sigma^2\tau_j^2}\right)\frac{\alpha^2}{2}\exp\left(-\frac{\alpha^2\tau_j^2}{2}\right)d\tau_j \\
&amp;= \frac{\alpha}{2\sigma}\exp\left(-\frac{\alpha|\beta_j|}{\sigma}\right).
\end{aligned}
\]</span> Thus it is a Laplace distribution with location 0 and scale <span class="math inline">\(\alpha/\sigma\)</span>. Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from <span class="math inline">\(\theta \mid a,y\)</span> and <span class="math inline">\(b\mid \theta,y\)</span> to estimate joint distribution over <span class="math inline">\((\hat \theta, \hat b)\)</span>. Thus, we so not need to apply cross-validation to find optimal value of <span class="math inline">\(b\)</span>, the Bayesian algorithm does it “automatically”.</p>
</section>
<section id="more-mixture-representations" class="level3">
<h3 class="anchored" data-anchor-id="more-mixture-representations">More Mixture Representations</h3>
<p>The power of scale mixture representations extends beyond the Laplace distribution to many common loss functions used in machine learning and statistics. <span class="citation" data-cites="polson2011data">Polson and Scott (<a href="references.html#ref-polson2011data" role="doc-biblioref">2011</a>)</span> provide a unified framework showing that various loss functions can be represented as variance-mean Gaussian mixtures, enabling efficient posterior sampling via data augmentation.</p>
<p>Consider the regression or classification setting where <span class="math inline">\(z_i = y_i - x_i^T\beta\)</span> for regression, or <span class="math inline">\(z_i = y_i x_i^T\beta\)</span> for binary classification (with <span class="math inline">\(y_i \in \{-1, +1\}\)</span>). The key insight is that many loss functions <span class="math inline">\(f(z_i \mid \beta, \sigma)\)</span> can be expressed through the variance-mean mixture:</p>
<p><span class="math display">\[
p(z_i \mid \beta, \sigma) = \int_0^{\infty} \phi(z_i \mid \mu_z + \kappa_z \omega_i, \sigma^2 \omega_i) \, dP(\omega_i)
\]</span></p>
<p>where <span class="math inline">\(\phi(\cdot \mid m, v)\)</span> denotes the normal density with mean <span class="math inline">\(m\)</span> and variance <span class="math inline">\(v\)</span>, and <span class="math inline">\(P(\omega_i)\)</span> is an appropriate mixing distribution. The following table summarizes representations for several important loss functions:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 28%">
<col style="width: 12%">
<col style="width: 9%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Loss Function</th>
<th style="text-align: left;"><span class="math inline">\(f(z_i \mid \beta, \sigma)\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\kappa_z\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\mu_z\)</span></th>
<th style="text-align: left;">Mixing Distribution <span class="math inline">\(P(\omega_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Squared-error</td>
<td style="text-align: left;"><span class="math inline">\(z_i^2/\sigma^2\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"><span class="math inline">\(\omega_i = 1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Absolute-error</td>
<td style="text-align: left;"><span class="math inline">\(\|z_i/\sigma\|\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Exponential</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cheek loss</td>
<td style="text-align: left;"><span class="math inline">\(\|z_i\| + (2q-1)z_i\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1-2q\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Generalized inverse Gaussian</td>
</tr>
<tr class="even">
<td style="text-align: left;">Support vector machines</td>
<td style="text-align: left;"><span class="math inline">\(\max(1-z_i, 0)\)</span></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Generalized inverse Gaussian</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Logistic</td>
<td style="text-align: left;"><span class="math inline">\(\log(1+e^{z_i})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1/2\)</span></td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">Pólya-Gamma</td>
</tr>
</tbody>
</table>
<p>This unified representation has profound computational implications. By introducing latent variables <span class="math inline">\(\{\omega_i\}\)</span> and <span class="math inline">\(\{\lambda_j\}\)</span> through appropriate equations, the exponential form <span class="math inline">\(\exp\{-L(\beta)\}\)</span> reduces to a Gaussian linear model with heteroscedastic errors. This enables straightforward Gibbs sampling where each conditional distribution has closed form.</p>
<p>The working response <span class="math inline">\(z_i\)</span> equals <span class="math inline">\(y_i - x_i^T\beta\)</span> for Gaussian regression, or <span class="math inline">\(y_i x_i^T\beta\)</span> for binary classification using logistic regression or support-vector machines. Both <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span> (from the prior) are hyperparameters typically estimated jointly with <span class="math inline">\(\beta\)</span>, though they may also be specified by the user or chosen by cross-validation. Importantly, in logistic regression <span class="math inline">\(\sigma\)</span> does not appear in the model as a hyperparameter.</p>
<p>This framework characterizes the general case for many models. Previous studies <span class="citation" data-cites="polson2011data">(<a href="references.html#ref-polson2011data" role="doc-biblioref">Polson and Scott 2011</a>)</span> have presented similar results for specific models, including support-vector machines and the powered-logit likelihood. The variance-mean mixture approach provides both theoretical insight into the connections between different loss functions and practical algorithms for efficient Bayesian inference.</p>
</section>
</section>
<section id="sec-horseshoe" class="level2" data-number="17.10">
<h2 data-number="17.10" class="anchored" data-anchor-id="sec-horseshoe"><span class="header-section-number">17.10</span> Horseshoe</h2>
<p>The horseshoe prior represents a significant advancement in Bayesian sparse estimation, addressing fundamental limitations of the Lasso while maintaining computational tractability <span class="citation" data-cites="carvalho2010horseshoe">(<a href="references.html#ref-carvalho2010horseshoe" role="doc-biblioref">Carvalho, Polson, and Scott 2010</a>)</span>. The name derives from the shape of its shrinkage profile, which resembles an inverted horseshoe: it applies minimal shrinkage to large signals while aggressively shrinking noise coefficients toward zero. This behavior makes it particularly attractive for high-dimensional problems where strong sparsity is expected but we want to avoid over-shrinking the true signals.</p>
<p>Unlike the Lasso, which applies constant shrinkage across all coefficient magnitudes, the horseshoe exhibits <em>adaptive shrinkage</em>. Small coefficients receive heavy regularization approaching zero, while large coefficients are left nearly unregularized. This is precisely the behavior we desire in sparse estimation: confidently shrink noise to zero while preserving signal fidelity. The horseshoe achieves this through a hierarchical Bayesian structure that places a half-Cauchy prior on local scale parameters.</p>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<p>The horseshoe prior is defined through a hierarchical scale mixture of normals. For the regression model <span class="math inline">\(y = X\beta + \epsilon\)</span> with <span class="math inline">\(\epsilon \sim N(0, \sigma^2 I)\)</span>, the horseshoe prior on coefficients <span class="math inline">\(\beta_j\)</span> is specified as:</p>
<p><span id="eq-horseshoe-prior"><span class="math display">\[
\begin{aligned}
\beta_j \mid \lambda_j, \tau, \sigma^2 &amp;\sim N(0, \lambda_j^2 \tau^2 \sigma^2) \\
\lambda_j &amp;\sim \text{Cauchy}^+(0, 1) \\
\tau &amp;\sim \text{Cauchy}^+(0, 1)
\end{aligned}
\tag{17.5}\]</span></span></p>
<p>where <span class="math inline">\(\text{Cauchy}^+(0, 1)\)</span> denotes the half-Cauchy distribution (the positive half of a standard Cauchy). The parameter <span class="math inline">\(\lambda_j\)</span> is the <em>local</em> shrinkage parameter for coefficient <span class="math inline">\(\beta_j\)</span>, while <span class="math inline">\(\tau\)</span> is the <em>global</em> shrinkage parameter that controls overall sparsity. The product <span class="math inline">\(\kappa_j = \lambda_j \tau\)</span> determines the effective scale of <span class="math inline">\(\beta_j\)</span>.</p>
<p>The half-Cauchy distribution can be represented as a scale mixture itself:</p>
<p><span class="math display">\[
\lambda_j \sim \text{Cauchy}^+(0, 1) \iff \lambda_j^2 \mid \nu_j \sim \text{Inv-Gamma}(1/2, 1/\nu_j), \quad \nu_j \sim \text{Inv-Gamma}(1/2, 1)
\]</span></p>
<p>This hierarchical representation enables efficient Gibbs sampling for posterior inference, as each conditional distribution has a closed form.</p>
</section>
<section id="shrinkage-properties" class="level3">
<h3 class="anchored" data-anchor-id="shrinkage-properties">Shrinkage Properties</h3>
<p>The horseshoe prior induces a shrinkage factor <span class="math inline">\(\kappa_j(\beta_j) = E[\kappa_j \mid \beta_j, y]\)</span> that adapts to the data. For large coefficient values <span class="math inline">\(|\beta_j|\)</span>, the shrinkage factor approaches 1 (no shrinkage), while for small values it approaches 0 (complete shrinkage). This can be seen through the marginal prior:</p>
<p><span class="math display">\[
p(\beta_j \mid \tau, \sigma^2) = \int_0^{\infty} N(\beta_j \mid 0, \lambda_j^2 \tau^2 \sigma^2) \cdot \text{Cauchy}^+(\lambda_j \mid 0, 1) d\lambda_j
\]</span></p>
<p>The resulting marginal distribution has extremely heavy tails compared to the Laplace prior used in Lasso. Specifically, the horseshoe has infinite moments, while the Laplace has exponential tails. This heavy-tailed behavior means that large signals are barely regularized, avoiding the systematic bias inherent in <span class="math inline">\(\ell_1\)</span> penalties.</p>
<p>The posterior mean under the horseshoe prior can be approximated as:</p>
<p><span class="math display">\[
E[\beta_j \mid y] \approx (1 - \kappa_j) \cdot 0 + \kappa_j \cdot \hat{\beta}_j^{\text{OLS}}
\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_j^{\text{OLS}}\)</span> is the ordinary least squares estimate and <span class="math inline">\(\kappa_j \in (0,1)\)</span> is the data-dependent shrinkage factor. Unlike Lasso, where the shrinkage is constant across coefficients, <span class="math inline">\(\kappa_j\)</span> adapts to the observed magnitude of <span class="math inline">\(\hat{\beta}_j^{\text{OLS}}\)</span>.</p>
</section>
<section id="comparison-with-other-priors" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-other-priors">Comparison with Other Priors</h3>
<p>The horseshoe prior occupies a distinctive position in the landscape of sparse priors. Consider the shrinkage behavior as a function of the observed coefficient:</p>
<ul>
<li><em>Ridge</em> (<span class="math inline">\(\ell_2\)</span>): Shrinkage factor <span class="math inline">\(\kappa_j = 1/(1 + \lambda)\)</span> is constant, providing uniform shrinkage regardless of signal strength</li>
<li><em>Lasso</em> (<span class="math inline">\(\ell_1\)</span>): Shrinkage is constant for small coefficients and linear for large ones, <span class="math inline">\(\kappa_j \approx 1 - \lambda/|\beta_j|\)</span> for large <span class="math inline">\(|\beta_j|\)</span></li>
<li><em>Horseshoe</em>: Shrinkage factor varies from 0 for noise to nearly 1 for signals, <span class="math inline">\(\kappa_j \approx (\lambda_j^2 \tau^2)/(\lambda_j^2 \tau^2 + \sigma^2)\)</span></li>
</ul>
<p>The global-local structure separates two tasks: <span class="math inline">\(\tau\)</span> determines how many coefficients should be non-zero (controlling overall sparsity), while each <span class="math inline">\(\lambda_j\)</span> determines whether its specific coefficient belongs to the signal or noise group. This separation leads to superior performance in recovering sparse signals compared to the Lasso, particularly when true coefficients vary substantially in magnitude.</p>
</section>
<section id="computational-implementation" class="level3">
<h3 class="anchored" data-anchor-id="computational-implementation">Computational Implementation</h3>
<p>The horseshoe prior admits efficient posterior sampling through Gibbs sampling, leveraging its hierarchical structure. The full conditional distributions are:</p>
<p><span class="math display">\[
\begin{aligned}
(\beta \mid y, \lambda, \tau, \sigma^2) &amp;\sim N\left((X'X + D_\lambda^{-1})^{-1}X'y, \sigma^2(X'X + D_\lambda^{-1})^{-1}\right) \\
(\sigma^2 \mid y, \beta) &amp;\sim \text{Inv-Gamma}\left(\frac{n}{2}, \frac{1}{2}\|y - X\beta\|^2\right) \\
(\lambda_j^2 \mid \beta_j, \tau, \sigma^2, \nu_j) &amp;\sim \text{Inv-Gamma}\left(1, \frac{1}{\nu_j} + \frac{\beta_j^2}{2\tau^2\sigma^2}\right) \\
(\tau^2 \mid \beta, \lambda, \sigma^2, \xi) &amp;\sim \text{Inv-Gamma}\left(\frac{p+1}{2}, \frac{1}{\xi} + \frac{1}{2\sigma^2}\sum_{j=1}^p \frac{\beta_j^2}{\lambda_j^2}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(D_\lambda = \text{diag}(\lambda_1^2\tau^2, \ldots, \lambda_p^2\tau^2)\)</span> and <span class="math inline">\(\nu_j, \xi\)</span> are auxiliary variables from the scale mixture representation of the half-Cauchy.</p>
<div id="exm-horseshoe" class="theorem example">
<p><span class="theorem-title"><strong>Example 17.6 (Horseshoe Prior for Sparse Regression)</strong></span> We demonstrate the horseshoe prior’s ability to recover sparse signals in a high-dimensional regression setting. Consider a scenario with <span class="math inline">\(n = 100\)</span> observations and <span class="math inline">\(p = 50\)</span> predictors, where only 5 coefficients are truly non-zero with varying magnitudes.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-horseshoe-comparison" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-horseshoe-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-horseshoe-comparison-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-horseshoe-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: Comparison of true, Horseshoe, and Lasso estimates
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-horseshoe-comparison" class="quarto-xref">Figure&nbsp;<span>17.5</span></a> reveals the distinctive shrinkage behaviors of the horseshoe prior compared to the Lasso penalty. The figure displays coefficient estimates (vertical axis) across all 50 predictors (horizontal axis), with black points indicating the true sparse signal where only the first five coefficients are non-zero with magnitudes ranging from 50 to -30. The horseshoe estimates (blue) exhibit a remarkable ability to preserve the large true coefficients nearly intact while aggressively shrinking the truly zero coefficients toward zero, demonstrating minimal bias for strong signals. In contrast, the Lasso estimates (red) systematically undershrink the large coefficients while leaving small spurious non-zero estimates scattered across many truly null predictors. This comparison illustrates the horseshoe prior’s theoretical advantage: its aggressive shrinkage of noise parameters combined with minimal penalization of signal parameters, resulting from the heavy tails of the half-Cauchy prior that allow large coefficients to escape shrinkage while the sharp peak near zero effectively eliminates noise.</p>
<p>The quantitative comparison in <a href="#tbl-horseshoe-mse" class="quarto-xref">Table&nbsp;<span>17.1</span></a> confirms the theoretical advantages of the horseshoe prior observed in <a href="#fig-horseshoe-comparison" class="quarto-xref">Figure&nbsp;<span>17.5</span></a>. The horseshoe achieves substantially lower coefficient MSE, reflecting its superior recovery of the true sparse signal structure through minimal bias on large coefficients and aggressive shrinkage of noise, while both methods perform reasonably well for prediction MSE since the Lasso’s systematic bias in coefficient estimates can be partially compensated by its lower variance in zero coefficients. The horseshoe’s adaptive shrinkage through the heavy-tailed half-Cauchy prior provides better overall performance across both metrics, more accurately capturing the underlying signal while effectively suppressing noise. The global shrinkage parameter <span class="math inline">\(\tau\)</span> is automatically learned from the data, with posterior mean <span class="math inline">\(\tau^2 \approx\)</span> 0.005, reflecting the appropriate level of sparsity and effectively performing automatic model selection without cross-validation.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div id="tbl-horseshoe-mse" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-null_prefix="true" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-horseshoe-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.1: Comparison of Coefficient and Prediction Mean Squared Errors
</figcaption>
<div aria-describedby="tbl-horseshoe-mse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: right;">Coefficient.MSE</th>
<th style="text-align: right;">Prediction.MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Horseshoe</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.52</td>
</tr>
<tr class="even">
<td style="text-align: left;">Lasso</td>
<td style="text-align: right;">0.05</td>
<td style="text-align: right;">3.17</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The example above shows the horseshoe prior with manual Gibbs sampling implementation for pedagogical purposes. In practice, the <code>horseshoe</code> package provides efficient implementations of horseshoe regression that are easier to use. Below we demonstrate a practical application with <span class="math inline">\(n = 50\)</span> observations and <span class="math inline">\(p = 100\)</span> predictors, where only 10 coefficients have true effects.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(horseshoe)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 50 by 100 design matrix X</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">50</span> <span class="sc">*</span> <span class="dv">100</span>), <span class="dv">50</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># True betas: first 10 are 6 (signals), rest are 0 (noise)</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">6</span>, <span class="dv">10</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">90</span>))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate response</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> X2 <span class="sc">%*%</span> beta2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">50</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit Horseshoe model</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(<span class="fu">capture.output</span>(</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  hs.object <span class="ot">&lt;-</span> <span class="fu">horseshoe</span>(y2, X2, <span class="at">method.tau =</span> <span class="st">"truncatedCauchy"</span>, <span class="at">method.sigma =</span> <span class="st">"Jeffreys"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-hs-prior" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hs-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-hs-prior-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hs-prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.6: Horseshoe prior for sparse regression using horseshoe package
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-hs-prior" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> demonstrates the Horseshoe prior’s remarkable ability to distinguish signal from noise in high-dimensional settings. The first 10 coefficients (the true signals with <span class="math inline">\(\beta_j = 6\)</span>) are accurately estimated with tight credible intervals around the true values, shown in red. Meanwhile, the remaining 90 noise coefficients (true <span class="math inline">\(\beta_j = 0\)</span>) are strongly shrunk toward zero, with their credible intervals tightly concentrated near the origin. This behavior exemplifies the Horseshoe’s “selective shrinkage” property: it applies minimal shrinkage to large coefficients (allowing signals to remain unbiased) while aggressively shrinking small coefficients toward zero (effectively removing noise). This makes the Horseshoe particularly well-suited for sparse recovery problems where the number of predictors greatly exceeds the sample size.</p>
<p>We can also perform variable selection by checking if the credible intervals exclude zero.</p>
</div>
</section>
<section id="comparison-and-package-implementation" class="level3">
<h3 class="anchored" data-anchor-id="comparison-and-package-implementation">Comparison and Package Implementation</h3>
<p>To illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for <span class="math inline">\(\beta_i\)</span> as a function of <span class="math inline">\(y_i\)</span> for three different values of <span class="math inline">\(\tau\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-hs-posterior-mean" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hs-posterior-mean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-hs-posterior-mean-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hs-posterior-mean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.7: Horseshoe posterior mean for three values of tau
</figcaption>
</figure>
</div>
</div>
</div>
<p>Smaller values of <span class="math inline">\(\tau\)</span> lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to <span class="math inline">\(\sqrt{2\sigma^2\log(1/\tau)}\)</span> are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of <span class="math inline">\(\tau\)</span> is the proportion of true signals. This value is typically not known in practice but can be estimated.</p>
<section id="the-normal-means-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-normal-means-problem">The normal means problem</h4>
<p>The normal means model is: <span class="math display">\[
y_i = \beta_i + \varepsilon_i, \quad i = 1, \ldots, n,
\]</span> with <span class="math inline">\(\varepsilon_i\)</span> i.i.d. <span class="math inline">\(\mathcal{N}(0, \sigma^2/n_i)\)</span>. first, we will be computing the posterior mean only, with known variance <span class="math inline">\(\sigma^2\)</span>. The function <code>HS.post.mean</code> computes the posterior mean of <span class="math inline">\((\beta_1, \ldots, \beta_n)\)</span>.</p>
<p>As an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 <span class="math inline">\(\beta_i\)</span>’s are equal to five and the remaining <span class="math inline">\(\beta_i\)</span>’s are equal to zero. Then, we estimate <span class="math inline">\(\tau\)</span> using the MMLE and use it to find the posterior mean (red). Finally, we use the function <code>HS.normal.means</code> to compute posterior means and credible intervals using MCMC.</p>
<div id="fig-hs-normal-means" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hs-normal-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-hs-normal-means" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-hs-normal-means-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hs-normal-means-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-hs-normal-means-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-hs-normal-means" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-hs-normal-means-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Truth vs observations
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-hs-normal-means" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-hs-normal-means-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hs-normal-means-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-hs-normal-means-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-hs-normal-means" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-hs-normal-means-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Horseshoe posterior mean estimates
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-hs-normal-means" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-hs-normal-means-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hs-normal-means-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-hs-normal-means-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-hs-normal-means" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-hs-normal-means-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Posterior mean with 95% credible intervals
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hs-normal-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.8: Normal means problem. Black = truth, Blue = observations, Red = estimates
</figcaption>
</figure>
</div>
<p>The three panels in <a href="#fig-hs-normal-means" class="quarto-xref">Figure&nbsp;<span>17.8</span></a> illustrate the horseshoe prior’s effectiveness in sparse estimation. Panel (a) shows the challenge: noisy observations (blue) obscure the true sparse signal structure (black), where only the first 10 parameters are non-zero. Panel (b) demonstrates the horseshoe posterior mean estimates (red), which successfully identify the signal locations while shrinking noise toward zero. The horseshoe’s adaptive shrinkage becomes most apparent in panel (c), where the full Bayesian analysis provides posterior means with credible intervals. Notice how the horseshoe correctly identifies the signal region (shown in teal) with narrow credible intervals that exclude zero, while the noise region (shown in red) has intervals that include zero. This automatic variable selection through credible intervals exemplifies the horseshoe’s oracle-like behavior: it simultaneously performs strong shrinkage on noise components while leaving true signals largely unshrunk, achieving near-optimal estimation without knowing the true sparsity level in advance.</p>
</section>
</section>
</section>
<section id="bridge-ell_alpha" class="level2" data-number="17.11">
<h2 data-number="17.11" class="anchored" data-anchor-id="bridge-ell_alpha"><span class="header-section-number">17.11</span> Bridge (<span class="math inline">\(\ell_{\alpha}\)</span>)</h2>
<p>The bridge estimator represents a powerful generalization that unifies many popular regularization approaches, bridging the gap between subset selection (<span class="math inline">\(\ell_0\)</span>) and Lasso (<span class="math inline">\(\ell_1\)</span>) penalties. For the regression model <span class="math inline">\(y = X\beta + \epsilon\)</span> with unknown vector <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)'\)</span>, the bridge estimator minimizes:</p>
<p><span id="eq-bridge-objective"><span class="math display">\[
Q_y(\beta) = \frac{1}{2} \|y - X\beta\|^2 + \lambda \sum_{j=1}^p |\beta_j|^\alpha
\tag{17.6}\]</span></span></p>
<p>where <span class="math inline">\(\alpha \in (0,2]\)</span> is the bridge parameter and <span class="math inline">\(\lambda &gt; 0\)</span> controls the regularization strength. This penalty interpolates between different sparsity-inducing behaviors. As <span class="math inline">\(\alpha \to 0\)</span>, the penalty approaches best subset selection (<span class="math inline">\(\ell_0\)</span>); when <span class="math inline">\(\alpha = 1\)</span>, it reduces to the Lasso penalty (<span class="math inline">\(\ell_1\)</span>); and when <span class="math inline">\(\alpha = 2\)</span>, it becomes the Ridge penalty (<span class="math inline">\(\ell_2\)</span>). The bridge penalty is non-convex when <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>, making optimization challenging but providing superior theoretical properties. Specifically, when <span class="math inline">\(\alpha &lt; 1\)</span>, the penalty is concave over <span class="math inline">\((0,\infty)\)</span>, leading to the oracle property under certain regularity conditions—the ability to identify the true sparse structure and estimate non-zero coefficients as efficiently as if the true model were known.</p>
<section id="bayesian-framework-and-data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-framework-and-data-augmentation">Bayesian Framework and Data Augmentation</h3>
<p>From a Bayesian perspective, the bridge estimator corresponds to the MAP estimate under an exponential-power prior. The Bayesian bridge model treats <span class="math inline">\(p(\beta \mid y) \propto \exp\{-Q_y(\beta)\}\)</span> as a posterior distribution, arising from assuming a Gaussian likelihood for <span class="math inline">\(y\)</span> and independent exponential-power priors: <span id="eq-exponential-power-prior"><span class="math display">\[
p(\beta_j \mid \alpha, \tau) = \frac{\alpha}{2\tau \Gamma(1 + 1/\alpha)} \exp\left(-\left|\frac{\beta_j}{\tau}\right|^\alpha\right)
\tag{17.7}\]</span></span></p>
<p>where <span class="math inline">\(\tau = \lambda^{-1/\alpha}\)</span> is the scale parameter. The Bayesian framework offers compelling advantages over classical bridge estimation. Rather than providing only a point estimate, it yields the full posterior distribution, enabling uncertainty quantification and credible intervals. The regularization parameter <span class="math inline">\(\lambda\)</span> can be learned from the data through hyperpriors, avoiding cross-validation. Most importantly, the bridge posterior is often multimodal, especially with correlated predictors, and MCMC naturally explores all modes while optimization may get trapped in local optima.</p>
<p>Posterior inference for the Bayesian bridge is facilitated by two key data augmentation representations. The first represents the exponential-power prior as a scale mixture of normals using Bernstein’s theorem: <span class="math inline">\(\exp(-|t|^{\alpha}) = \int_0^{\infty} e^{-s t^2/2} g(s) \, ds\)</span>, where <span class="math inline">\(g(s)\)</span> is the density of a positive <span class="math inline">\(\alpha/2\)</span>-stable random variable. However, the conditional posterior for the mixing variables becomes an exponentially tilted stable distribution, which lacks a closed form and requires specialized sampling algorithms.</p>
<p>A novel alternative representation avoids stable distributions by expressing the exponential-power prior as a scale mixture of triangular (Bartlett-Fejer) kernels: <span id="eq-bartlett-fejer"><span class="math display">\[
\begin{aligned}
(y \mid \beta, \sigma^2) &amp;\sim N(X\beta, \sigma^2 I) \\
p(\beta_j \mid \tau, \omega_j, \alpha) &amp;= \frac{1}{\tau \omega_j^{1/\alpha}} \left\{ 1 - \left| \frac{\beta_j}{\tau \omega_j^{1/\alpha}} \right| \right\}_+ \\
(\omega_j \mid \alpha) &amp;\sim \frac{1+\alpha}{2} \cdot \text{Gamma}(2+1/\alpha,1) + \frac{1-\alpha}{2} \cdot \text{Gamma}(1+1/\alpha,1)
\end{aligned}
\tag{17.8}\]</span></span></p>
<p>where <span class="math inline">\(\{a\}_+ = \max(a,0)\)</span>. This mixture of gamma distributions is much simpler to sample from and naturally captures the bimodality of the bridge posterior through its two-component structure. The choice of representation depends on the design matrix structure: the Bartlett-Fejer representation is 2-3 times more efficient for orthogonal designs, while the scale mixture of normals performs better for collinear designs.</p>
<p>The bridge prior with <span class="math inline">\(\alpha &lt; 1\)</span> satisfies the oracle property under regularity conditions, correctly identifying the true sparsity pattern while avoiding over-shrinkage of large signals through its heavier-than-exponential tails and redescending score function. The Bartlett-Fejer representation enables an efficient Gibbs sampler that marginalizes over local scale parameters, achieving excellent mixing properties, though a distinctive feature is the posterior’s multimodality with correlated predictors, which reflects genuine model uncertainty that the Bayesian approach properly accounts for by averaging over all modes. Extensive simulations demonstrate dramatic improvements over classical methods: with <span class="math inline">\(p = 100\)</span>, <span class="math inline">\(n = 101\)</span>, and <span class="math inline">\(\alpha = 0.5\)</span>, mean squared error was 2254 for least squares, 1611 for classical bridge, and only 99 for Bayesian bridge, with similar gains on benchmark datasets often exceeding 50% reduction in prediction error. Practical guidelines recommend <span class="math inline">\(\alpha \in [0.5, 0.8]\)</span> or learning from data with a uniform prior, using Gamma(2,2) for the global scale <span class="math inline">\(\tau\)</span>, selecting the Bartlett-Fejer representation for nearly orthogonal designs and scale mixture of normals for collinear predictors, and examining posterior multimodality to understand model uncertainty. The framework extends naturally to other likelihoods and is implemented in the R package <code>BayesBridge</code>, occupying a unique position in the regularization landscape by providing less bias than Lasso for large coefficients while maintaining sparsity, achieving variable selection unlike ridge regression, and offering computational advantages over spike-and-slab priors, making it an excellent choice for modern high-dimensional inference problems.</p>
</section>
</section>
<section id="full-bayes-for-sparsity-shrinkage" class="level2" data-number="17.12">
<h2 data-number="17.12" class="anchored" data-anchor-id="full-bayes-for-sparsity-shrinkage"><span class="header-section-number">17.12</span> Full Bayes for Sparsity Shrinkage</h2>
<p>Thus far we have considered penalized optimization approaches that yield point estimates through maximum a posteriori (MAP) estimation. While these methods provide computationally efficient solutions for high-dimensional problems, they do not fully quantify uncertainty about which variables should be included in the model. In this section, we explore full Bayesian approaches to variable selection that compute the complete posterior distribution over both parameters and model structure, allowing us to assess uncertainty about sparsity patterns themselves.</p>
<section id="spike-and-slab-prior" class="level3">
<h3 class="anchored" data-anchor-id="spike-and-slab-prior">Spike-and-Slab Prior</h3>
<p>The gold standard for Bayesian variable selection are spike-and-slab priors, also known as Bernoulli-Gaussian mixtures. These priors provide full model uncertainty quantification by explicitly modeling which variables should be included. Consider a linear regression problem <span class="math display">\[
y = \beta_1x_1+\ldots+\beta_px_p + e \ , \ \  \text{where } e \sim N(0, \sigma^2),~-\infty \le \beta_i \le \infty \ .
\]</span> Our goal is to identify which input variables <span class="math inline">\(x_i\)</span> should be included in the model, seeking a sparse solution where <span class="math inline">\(\|\beta\|_0 = k \ll p\)</span>, with <span class="math inline">\(\|\beta\|_0 \defeq \#\{i : \beta_i\neq0\}\)</span> denoting the cardinality of the support of <span class="math inline">\(\beta\)</span>, also known as the <span class="math inline">\(\ell_0\)</span> (pseudo)norm. While continuous shrinkage priors like the Lasso and horseshoe adaptively shrink coefficients, spike-and-slab priors take a more explicit approach by placing positive probability mass at exactly zero.</p>
<p>Under spike-and-slab, each <span class="math inline">\(\beta_i\)</span> exchangeably follows a mixture prior consisting of <span class="math inline">\(\delta_0\)</span>, a point mass at <span class="math inline">\(0\)</span>, and a Gaussian distribution centered at zero. Hence we write,</p>
<p><span class="math display">\[
\label{eqn:ss}
\beta_i | \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N\left(0, \sigma_\beta^2\right) \ .
\]</span> Here <span class="math inline">\(\theta\in \left(0, 1\right)\)</span> controls the overall sparsity in <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_\beta^2\)</span> accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.</p>
<p>A useful re-parameterization, the parameters <span class="math inline">\(\beta\)</span> is given by two independent random variable vectors <span class="math inline">\(\gamma = \left(\gamma_1, \ldots, \gamma_p\right)'\)</span> and <span class="math inline">\(\alpha = \left(\alpha_1, \ldots, \alpha_p\right)'\)</span> such that <span class="math inline">\(\beta_i  =  \gamma_i\alpha_i\)</span>, with probabilistic structure <span class="math display">\[
\label{eq:bg}
\begin{array}{rcl}
\gamma_i\mid\theta &amp; \sim &amp; \text{Bernoulli}(\theta) \ ;
\\
\alpha_i \mid \sigma_\beta^2 &amp;\sim &amp; N\left(0, \sigma_\beta^2\right) \ .
\\
\end{array}
\]</span> Since <span class="math inline">\(\gamma_i\)</span> and <span class="math inline">\(\alpha_i\)</span> are independent, the joint prior density becomes <span class="math display">\[
p\left(\gamma_i, \alpha_i \mid \theta, \sigma_\beta^2\right) =
\theta^{\gamma_i}\left(1-\theta\right)^{1-\gamma_i}\frac{1}{\sqrt{2\pi}\sigma_\beta}\exp\left\{-\frac{\alpha_i^2}{2\sigma_\beta^2}\right\}
\ , \ \ \ \text{for } 1\leq i\leq p \ .
\]</span> The indicator <span class="math inline">\(\gamma_i\in \{0, 1\}\)</span> can be viewed as a dummy variable to indicate whether <span class="math inline">\(\beta_i\)</span> is included in the model.</p>
<p>Let <span class="math inline">\(S = \{i: \gamma_i = 1\} \subseteq \{1, \ldots, p\}\)</span> be the “active set” of <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\|\gamma\|_0 = \sum\limits_{i = 1}^p\gamma_i\)</span> be its cardinality. The joint prior on the vector <span class="math inline">\(\{\gamma, \alpha\}\)</span> then factorizes as <span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right) &amp; = &amp; \prod\limits_{i = 1}^p p\left(\alpha_i, \gamma_i \mid \theta, \sigma_\beta^2\right) \\
&amp; = &amp;
\theta^{\|\gamma\|_0}
\left(1-\theta\right)^{p - \|\gamma\|_0}
\left(2\pi\sigma_\beta^2\right)^{-\frac p2}\exp\left\{-\frac1{2\sigma_\beta^2}\sum\limits_{i = 1}^p\alpha_i^2\right\} \ .
\end{array}
\]</span></p>
<p>Let <span class="math inline">\(X_\gamma \defeq \left[X_i\right]_{i \in S}\)</span> be the set of “active explanatory variables" and <span class="math inline">\(\alpha_\gamma \defeq \left(\alpha_i\right)'_{i \in S}\)</span> be their corresponding coefficients. We can write <span class="math inline">\(X\beta = X_\gamma \alpha_\gamma\)</span>. The likelihood can be expressed in terms of <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\alpha\)</span> as <span class="math display">\[
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)
=
\left(2\pi\sigma_e^2\right)^{-\frac n2}
\exp\left\{
-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
\right\} \ .
\]</span></p>
<p>Under this re-parameterization by <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span>, the posterior is given by</p>
<p><span class="math display">\[
\begin{array}{rcl}
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y\right) &amp; \propto &amp;
p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2\right)
p\left(y \mid \gamma, \alpha, \theta, \sigma_e^2\right)\\
&amp; \propto &amp;
\exp\left\{-\frac1{2\sigma_e^2}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
-\frac1{2\sigma_\beta^2}\left\|\alpha\right\|_2^2
-\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0
\right\} \ .
\end{array}
\]</span> Our goal then is to find the regularized maximum a posterior (MAP) estimator <span class="math display">\[
\arg\max\limits_{\gamma, \alpha}p\left(\gamma, \alpha \mid \theta, \sigma_\beta^2, \sigma_e^2, y \right) \ .
\]</span> By construction, the <span class="math inline">\(\gamma\)</span> <span class="math inline">\(\in\left\{0, 1\right\}^p\)</span> will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over <span class="math inline">\(\left\{\gamma, \alpha\right\}\)</span> the regularized least squares objective function</p>
<p><span id="eq-obj:map"><span class="math display">\[
\min\limits_{\gamma, \alpha}\left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2
+ 2\sigma_e^2\log\left(\frac{1-\theta}{\theta}\right)
\left\|\gamma\right\|_0 \ .
\tag{17.9}\]</span></span> This objective possesses several interesting properties:</p>
<ol type="1">
<li><p>The first term is essentially the least squares loss function.</p></li>
<li><p>The second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) <span class="math inline">\(\sigma_\beta^2/\sigma_e^2\)</span>. Smaller SNR will be more likely to shrink the estimates towards <span class="math inline">\(0\)</span>. If <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want <span class="math inline">\(\sigma_\beta \to \infty\)</span> or to be “sufficiently large" in order to avoid imposing harsh shrinkage to non-zero signals.</p></li>
<li><p>If we further assume that <span class="math inline">\(\theta &lt; \frac12\)</span>, meaning that the coefficients are known to be sparse <em>a priori</em>, then <span class="math inline">\(\log\left(\left(1-\theta\right) / \theta\right) &gt; 0\)</span>, and the third term can be seen as an <span class="math inline">\(\ell_0\)</span> regularization.</p></li>
</ol>
<p>Therefore, our Bayesian objective inference is connected to <span class="math inline">\(\ell_0\)</span>-regularized least squares, which we summarize in the following proposition.</p>
<p>(Spike-and-slab MAP &amp; <span class="math inline">\(\ell_0\)</span> regularization)</p>
<p>For some <span class="math inline">\(\lambda &gt; 0\)</span>, assuming <span class="math inline">\(\theta &lt; \frac12\)</span>, <span class="math inline">\(\sigma_\beta^2 \gg \sigma_e^2\)</span>, the Bayesian MAP estimate defined by <a href="#eq-obj:map" class="quarto-xref">Equation&nbsp;<span>17.9</span></a> is equivalent to the <span class="math inline">\(\ell_0\)</span> regularized least squares objective, for some <span class="math inline">\(\lambda &gt; 0\)</span>, <span id="eq-obj:l0"><span class="math display">\[
\min_{\beta}
\frac12\left\|y - X\beta\right\|_2^2
+ \lambda
\left\|\beta\right\|_0 \ .
\tag{17.10}\]</span></span></p>
<p>First, assuming that <span class="math display">\[
\theta &lt; \frac12, \ \ \  \sigma_\beta^2 \gg \sigma_e^2, \ \ \  \frac{\sigma_e^2}{\sigma_\beta^2}\left\|\alpha\right\|_2^2 \to 0 \ ,
\]</span> gives us an objective function of the form <span id="eq-obj:vs"><span class="math display">\[
\min_{\gamma, \alpha}
\frac12 \left\|y - X_\gamma \alpha_\gamma\right\|_2^2
+ \lambda
\left\|\gamma\right\|_0,  \ \ \ \  \text{where } \lambda \defeq \sigma_e^2\log\left(\left(1-\theta\right) / \theta\right) &gt; 0 \ .
\tag{17.11}\]</span></span></p>
<p>This can be seen as a variable selection version of equation <a href="#eq-obj:l0" class="quarto-xref">Equation&nbsp;<span>17.10</span></a>. To show this, we need only to check that the optimal solution corresponds to a feasible solution and vice versa. This is explained as follows.</p>
<p>On the one hand, assuming <span class="math inline">\(\hat\beta\)</span> is an optimal solution, then we can correspondingly define <span class="math inline">\(\hat\gamma_i \defeq I\left\{\hat\beta_i \neq 0\right\}\)</span>, <span class="math inline">\(\hat\alpha_i \defeq \hat\beta_i\)</span>, such that <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is feasible and gives the same objective value as <span class="math inline">\(\hat\beta\)</span>..</p>
<p>On the other hand, assuming <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span> is optimal, implies that we must have all of the elements in <span class="math inline">\(\hat\alpha_\gamma\)</span> should be non-zero, otherwise a new <span class="math inline">\(\tilde\gamma_i \defeq I\left\{\hat\alpha_i \neq 0\right\}\)</span> will give a lower objective value. As a result, if we define <span class="math inline">\(\hat\beta_i \defeq \hat\gamma_i\hat\alpha_i\)</span>, <span class="math inline">\(\hat\beta\)</span> will be feasible and gives the same objective value as <span class="math inline">\(\left\{\hat\gamma, \hat\alpha\right\}\)</span>.</p>
</section>
</section>
<section id="subset-selection-ell_0-norm" class="level2" data-number="17.13">
<h2 data-number="17.13" class="anchored" data-anchor-id="subset-selection-ell_0-norm"><span class="header-section-number">17.13</span> Subset Selection (<span class="math inline">\(\ell_0\)</span> Norm)</h2>
<p>The <span class="math inline">\(\ell_0\)</span> norm directly counts the number of non-zero parameters, making it the most natural penalty for variable selection. However, <span class="math inline">\(\ell_0\)</span>-regularized optimization problems are NP-hard due to their combinatorial nature. The optimization problem is:</p>
<p><span class="math display">\[
\min_{\beta} \frac{1}{2}\|y - X\beta\|_2^2 + \lambda \|\beta\|_0
\]</span></p>
<p>where <span class="math inline">\(\|\beta\|_0 = \#\{j : \beta_j \neq 0\}\)</span> is the number of non-zero coefficients. This directly penalizes model complexity by limiting the number of active predictors.</p>
<section id="connection-to-spike-and-slab-priors" class="level3">
<h3 class="anchored" data-anchor-id="connection-to-spike-and-slab-priors">Connection to Spike-and-Slab Priors</h3>
<p>A remarkable connection exists between Bayesian spike-and-slab priors and <span class="math inline">\(\ell_0\)</span> regularization. Consider the spike-and-slab prior where each coefficient follows:</p>
<p><span class="math display">\[
\beta_j \mid \theta, \sigma_\beta^2 \sim (1-\theta)\delta_0 + \theta N(0, \sigma_\beta^2)
\]</span></p>
<p>Here <span class="math inline">\(\theta \in (0,1)\)</span> controls the sparsity level and <span class="math inline">\(\sigma_\beta^2\)</span> governs the size of non-zero coefficients. This can be reparametrized using indicator variables <span class="math inline">\(\gamma_j \in \{0,1\}\)</span> and continuous coefficients <span class="math inline">\(\alpha_j\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\beta_j &amp;= \gamma_j \alpha_j \\
\gamma_j \mid \theta &amp;\sim \text{Bernoulli}(\theta) \\
\alpha_j \mid \sigma_\beta^2 &amp;\sim N(0, \sigma_\beta^2)
\end{aligned}
\]</span></p>
<p>The maximum a posteriori (MAP) estimator under this prior yields the objective:</p>
<p><span class="math display">\[
\min_{\gamma, \alpha} \|y - X_\gamma \alpha_\gamma\|_2^2 + \frac{\sigma^2}{\sigma_\beta^2}\|\alpha\|_2^2 + 2\sigma^2\log\left(\frac{1-\theta}{\theta}\right)\|\gamma\|_0
\]</span></p>
<p>where <span class="math inline">\(X_\gamma\)</span> contains only the columns corresponding to <span class="math inline">\(\gamma_j = 1\)</span>. Under the assumptions <span class="math inline">\(\theta &lt; 1/2\)</span> (favoring sparsity) and <span class="math inline">\(\sigma_\beta^2 \gg \sigma^2\)</span> (weak shrinkage on non-zero coefficients), this reduces to the <span class="math inline">\(\ell_0\)</span>-regularized least squares with <span class="math inline">\(\lambda = 2\sigma^2\log\left(\frac{1-\theta}{\theta}\right)\)</span>.</p>
</section>
<section id="single-best-replacement-sbr-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="single-best-replacement-sbr-algorithm">Single Best Replacement (SBR) Algorithm</h3>
<p>Since exact <span class="math inline">\(\ell_0\)</span> optimization is intractable, practical algorithms focus on finding good local optima. The Single Best Replacement (SBR) algorithm addresses the fundamental challenge in sparse regression: finding the optimal subset of predictors when the search space is exponentially large. For <span class="math inline">\(p\)</span> predictors, there are <span class="math inline">\(2^p\)</span> possible subsets to consider, making exhaustive search computationally prohibitive for even moderate <span class="math inline">\(p\)</span>.</p>
<p>Rather than searching over all possible coefficient vectors <span class="math inline">\(\beta\)</span>, SBR reformulates the <span class="math inline">\(\ell_0\)</span>-regularized problem as a discrete optimization over active sets <span class="math inline">\(S \subseteq \{1,2,\ldots,p\}\)</span>:</p>
<p><span class="math display">\[
\min_{S} f(S) = \frac{1}{2}\|y - X_S \hat{\beta}_S\|_2^2 + \lambda |S|
\]</span></p>
<p>where <span class="math inline">\(\hat{\beta}_S = (X_S^T X_S)^{-1} X_S^T y\)</span> is the least squares solution on the active set <span class="math inline">\(S\)</span>. This reformulation creates a natural bias-variance tradeoff where larger models with bigger active sets reduce bias but increase the penalty, while smaller models reduce the penalty but may increase bias.</p>
<p>The SBR algorithm operates through a systematic iterative process. The initialization phase begins with an empty active set <span class="math inline">\(S_0 = \emptyset\)</span>, computes the initial objective <span class="math inline">\(f(S_0) = \frac{1}{2}\|y\|_2^2\)</span> (corresponding to no predictors), and sets the iteration counter <span class="math inline">\(k = 0\)</span>.</p>
<p>The main iteration loop proceeds as follows for each iteration <span class="math inline">\(k\)</span>. During candidate generation, the algorithm considers each variable <span class="math inline">\(j \in \{1,\ldots,p\}\)</span> and defines the single replacement operation: <span class="math display">\[S_k \cdot j = \begin{cases}
S_k \cup \{j\} &amp; \text{if } j \notin S_k \text{ (addition)} \\
S_k \setminus \{j\} &amp; \text{if } j \in S_k \text{ (removal)}
\end{cases}\]</span></p>
<p>For objective evaluation, each candidate <span class="math inline">\(S_k \cdot j\)</span> is assessed by computing: <span class="math display">\[f(S_k \cdot j) = \frac{1}{2}\|y - X_{S_k \cdot j} \hat{\beta}_{S_k \cdot j}\|_2^2 + \lambda |S_k \cdot j|\]</span></p>
<p>The best replacement selection identifies: <span class="math display">\[j^* = \arg\min_{j \in \{1,\ldots,p\}} f(S_k \cdot j)\]</span></p>
<p>Finally, the improvement check determines whether to accept the move: if <span class="math inline">\(f(S_k \cdot j^*) &lt; f(S_k)\)</span>, the algorithm accepts the move and sets <span class="math inline">\(S_{k+1} = S_k \cdot j^*\)</span>; otherwise, it stops and returns <span class="math inline">\(S_k\)</span> as the final solution.</p>
<p>Unlike pure forward selection which only adds variables or backward elimination which only removes variables, SBR can both add and remove variables at each step. This bidirectionality provides substantial advantages. The algorithm can escape local optima by correcting early mistakes through the removal of previously selected variables. When variables are correlated, the algorithm can swap between equivalent predictors to find better solutions. Additionally, the adaptive model size capability allows the algorithm to both grow and shrink the model as needed during the optimization process.</p>
<p>When compared with standard stepwise methods, the advantages become clear. Forward selection uses greedy addition only and can become trapped if early selections are poor. Backward elimination starts with the full model, making it computationally expensive for large <span class="math inline">\(p\)</span>. Traditional forward-backward approaches use separate forward and backward phases, while SBR provides a unified framework that considers both additions and removals at each step.</p>
<p>The key computational challenge lies in evaluating <span class="math inline">\(f(S \cdot j)\)</span> for all <span class="math inline">\(p\)</span> variables at each iteration, as naive implementation would require <span class="math inline">\(p\)</span> separate least squares computations per iteration. Efficient matrix updates provide the solution to this challenge.</p>
<p>For the addition case where <span class="math inline">\(j \notin S\)</span>, adding variable <span class="math inline">\(j\)</span> to active set <span class="math inline">\(S\)</span> employs rank-one updates to the Cholesky decomposition. If <span class="math inline">\(X_S^T X_S = L_S L_S^T\)</span>, then updating for <span class="math inline">\(X_{S \cup \{j\}}^T X_{S \cup \{j\}}\)</span> requires only <span class="math inline">\(O(|S|^2)\)</span> operations instead of <span class="math inline">\(O(|S|^3)\)</span>. Similarly, for the removal case where <span class="math inline">\(j \in S\)</span>, removing variable <span class="math inline">\(j\)</span> from active set <span class="math inline">\(S\)</span> uses rank-one downdates to the Cholesky decomposition with similar <span class="math inline">\(O(|S|^2)\)</span> complexity.</p>
<p>The overall computational complexity analysis reveals that each iteration requires <span class="math inline">\(O(p|S|^2)\)</span> operations where <span class="math inline">\(|S|\)</span> is the current active set size, the total number of iterations is typically <span class="math inline">\(O(|S_{final}|)\)</span> in practice, and the overall complexity becomes <span class="math inline">\(O(p|S|^3)\)</span>, which is much more efficient than exhaustive search requiring <span class="math inline">\(O(2^p)\)</span> operations.</p>
<section id="theoretical-properties" class="level4">
<h4 class="anchored" data-anchor-id="theoretical-properties">Theoretical Properties</h4>
<p>The convergence properties of SBR are well-established. The algorithm demonstrates finite convergence, provably converging in finite steps since there are only finitely many possible active sets. It exhibits monotonic improvement where the objective function decreases or stays the same at each iteration. Finally, the algorithm achieves local optimality, with the final solution satisfying local optimality conditions.</p>
<p>SBR can be viewed as a coordinate-wise proximal gradient method, establishing a connection to proximal gradient methods. The proximal operator for the <span class="math inline">\(\ell_0\)</span> norm is: <span class="math display">\[\text{prox}_{\lambda\|\cdot\|_0}(z) = z \odot \mathbf{1}_{|z|&gt;\sqrt{2\lambda}}\]</span></p>
<p>This hard thresholding operation is exactly what SBR implements in a coordinate-wise manner.</p>
<p>Under certain regularity conditions, SBR can achieve statistical consistency across multiple dimensions. It demonstrates variable selection consistency by correctly identifying the true active set with high probability. The algorithm provides estimation consistency through consistent estimates of the non-zero coefficients. Additionally, it achieves prediction consistency by attaining optimal prediction error rates.</p>
</section>
<section id="practical-implementation-considerations" class="level4">
<h4 class="anchored" data-anchor-id="practical-implementation-considerations">Practical Implementation Considerations</h4>
<p>Several practical considerations affect SBR implementation. For regularization parameter selection, cross-validation provides the standard approach but is computationally expensive, while information criteria such as BIC and AIC can provide faster alternatives. Stability selection offers another approach by running SBR on bootstrap samples and selecting stable variables.</p>
<p>Initialization strategies vary in their effectiveness. The empty start approach using <span class="math inline">\(S_0 = \emptyset\)</span> is most common, while forward start begins with forward selection for a few steps. Random start employs multiple random initializations for better global search capabilities.</p>
<p>Handling numerical issues requires attention to several factors. Multicollinearity concerns necessitate checking condition numbers of <span class="math inline">\(X_S^T X_S\)</span>. Rank deficiency situations require handling cases where <span class="math inline">\(X_S\)</span> is rank deficient. Numerical stability can be improved by using QR decomposition instead of normal equations when needed.</p>
</section>
<section id="statistical-properties-and-performance" class="level4">
<h4 class="anchored" data-anchor-id="statistical-properties-and-performance">Statistical Properties and Performance</h4>
<p>Empirical studies demonstrate that SBR achieves statistical performance comparable to the gold-standard spike-and-slab priors while being orders of magnitude faster. The algorithm shows superior variable selection performance compared to Lasso and elastic net in high-correlation settings, achieves lower mean squared error than convex relaxation methods for estimation accuracy, and provides better recovery of the true sparse structure compared to <span class="math inline">\(\ell_1\)</span> penalties for sparsity detection.</p>
<p>The connection between spike-and-slab priors and <span class="math inline">\(\ell_0\)</span> regularization provides theoretical justification for why SBR performs well: it approximates the MAP estimator of a principled Bayesian model while remaining computationally tractable.</p>
</section>
<section id="advantages-and-limitations" class="level4">
<h4 class="anchored" data-anchor-id="advantages-and-limitations">Advantages and Limitations</h4>
<p>SBR offers several computational and theoretical advantages. The algorithm provides computational efficiency, running much faster than full Bayesian methods. It maintains a theoretical foundation through its principled connection to spike-and-slab priors. The approach demonstrates flexibility in handling various problem sizes and correlation structures. Finally, it produces sparse, interpretable models that enhance model interpretability.</p>
<p>However, SBR also has certain limitations. The algorithm provides no guarantee of global optimality, potentially stopping at local optima. Its greedy nature may lead to suboptimal early decisions that affect the final solution. Performance sensitivity depends heavily on <span class="math inline">\(\lambda\)</span> selection, requiring careful parameter tuning. Additionally, the algorithm may struggle with highly correlated predictors in certain scenarios.</p>
</section>
<section id="extensions-and-variations" class="level4">
<h4 class="anchored" data-anchor-id="extensions-and-variations">Extensions and Variations</h4>
<p>Several extensions expand SBR’s applicability. Grouped SBR extends to group selection by replacing single variables with groups: <span class="math display">\[S \cdot G = \begin{cases}
S \cup G &amp; \text{if } G \cap S = \emptyset \\
S \setminus G &amp; \text{if } G \subseteq S
\end{cases}\]</span></p>
<p>Regularization path computation involves computing solutions for a sequence of <span class="math inline">\(\lambda\)</span> values while using warm starts from previous solutions. Multiple best replacements consider the <span class="math inline">\(k\)</span> best replacements at each step for more thorough search rather than restricting to single best replacement.</p>
</section>
<section id="proximal-perspective" class="level4">
<h4 class="anchored" data-anchor-id="proximal-perspective">Proximal Perspective</h4>
<p>The SBR algorithm can be deeply understood through the lens of proximal operators. The proximal operator for the <span class="math inline">\(\ell_0\)</span> norm corresponds to <em>hard thresholding</em>: <span class="math display">\[
[\text{prox}_{\gamma \lambda \|\cdot\|_0}(v)]_j = v_j \odot \mathbf{1}_{|v_j| &gt; \sqrt{2\gamma\lambda}}
\]</span></p>
<p>This reveals that SBR essentially performs coordinate-wise hard thresholding, where the decision to include or exclude a variable depends on whether its magnitude exceeds a threshold derived from the regularization parameter. This contrasts with the Lasso’s soft thresholding (which shrinks all coefficients) and Ridge’s uniform shrinkage. This perspective unifies SBR with continuous optimization approaches, showing it implements the “hard” decision boundary required for exact sparsity.</p>
</section>
<section id="spike-and-slab-examples-bernoulli-gaussian-and-bernoulli-laplace" class="level4">
<h4 class="anchored" data-anchor-id="spike-and-slab-examples-bernoulli-gaussian-and-bernoulli-laplace">Spike-and-Slab Examples: Bernoulli-Gaussian and Bernoulli-Laplace</h4>
<p>To illustrate the practical implications of different spike-and-slab priors in the proximal framework, we examine two important cases: Bernoulli-Gaussian and Bernoulli-Laplace priors. These examples demonstrate how different prior specifications lead to different shrinkage behaviors and associated penalty functions.</p>
<p>For the normal means problem where <span class="math inline">\(y \mid \beta \sim N(\beta, \sigma^2)\)</span> with Bernoulli-Gaussian prior <span class="math inline">\(\beta \sim (1-\theta)\delta_0 + \theta N(0, \sigma_\beta^2)\)</span>, the marginal distribution of <span class="math inline">\(y\)</span> is: <span class="math display">\[
y \mid \theta \sim (1-\theta) N(0, \sigma^2) + \theta N(0, \sigma^2 + \sigma_\beta^2)
\]</span></p>
<p>The posterior mean takes the form: <span class="math display">\[
\hat{\beta}^{BG} = w(y) y
\]</span> where the weight function is: <span class="math display">\[
w(y) = \frac{\sigma_\beta^2}{\sigma^2 + \sigma_\beta^2} \left(1 + \frac{(1-\theta)\phi(y/\sigma)}{\theta\phi(y/\sqrt{\sigma^2 + \sigma_\beta^2})\sqrt{1 + \sigma_\beta^2/\sigma^2}}\right)^{-1}
\]</span></p>
<p>and <span class="math inline">\(\phi(\cdot)\)</span> denotes the standard normal density.</p>
<p>For the Bernoulli-Laplace prior <span class="math inline">\(\beta \sim (1-\theta)\delta_0 + \theta \text{Laplace}(0, b)\)</span>, the expressions become more complex, involving the cumulative distribution function of the standard normal distribution.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-posterior-means" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-posterior-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-posterior-means-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-posterior-means-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.9: Posterior mean functions for Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) priors
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-penalty-functions" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-penalty-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="17-theoryai_files/figure-html/fig-penalty-functions-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-penalty-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.10: Penalty functions associated with Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) posterior means
</figcaption>
</figure>
</div>
</div>
</div>
<p>These figures illustrate several key properties of spike-and-slab priors in the proximal framework. Both priors shrink small observations towards zero, but their behavior differs significantly for larger observations. When the slab variance <span class="math inline">\(\sigma_\beta^2\)</span> (or scale parameter <span class="math inline">\(b\)</span>) is small, Bernoulli-Gaussian priors behave similarly to ridge regression, applying excessive shrinkage to large observations. In contrast, Bernoulli-Laplace priors exhibit behavior more similar to Lasso, with softer shrinkage characteristics.</p>
<p>As the slab parameters increase, both priors approach hard thresholding behavior, and their associated penalty functions <span class="math inline">\(\phi\)</span> become increasingly similar to non-convex penalties such as SCAD. The penalty functions reveal the “spiky” nature around zero that induces sparsity for small observations, while the different tail behaviors reflect the distinct shrinkage properties of Gaussian versus Laplace slab components.</p>
</section>
</section>
</section>
<section id="advanced-topics-in-regularization" class="level2" data-number="17.14">
<h2 data-number="17.14" class="anchored" data-anchor-id="advanced-topics-in-regularization"><span class="header-section-number">17.14</span> Advanced Topics in Regularization</h2>
<section id="the-vertical-likelihood-duality" class="level3">
<h3 class="anchored" data-anchor-id="the-vertical-likelihood-duality">The Vertical Likelihood Duality</h3>
<p>For computational efficiency in model evaluation, consider the problem of estimating <span class="math inline">\(\int_{\mathcal{X}} L(x) P(dx)\)</span> where <span class="math inline">\(L: \mathcal{X} \to \mathbb{R}\)</span>. By letting <span class="math inline">\(Y = L(x)\)</span>, we can transform this to a one-dimensional integral <span class="math inline">\(\int_0^1 F_Y^{-1}(s) ds\)</span>.</p>
<p>We therefore have a duality: <span class="math display">\[
\int_{\mathcal{X}} L(x) P(dx) = \int_0^1 F_Y^{-1}(s) ds
\]</span> where <span class="math inline">\(Y\)</span> is a random variable <span class="math inline">\(Y = L(X)\)</span> with <span class="math inline">\(X \sim P(dx)\)</span>.</p>
<p>This approach offers several advantages:</p>
<ul>
<li>We can use sophisticated grids (Riemann) to approximate one-dimensional integrals</li>
<li>A grid on inverse CDF space is equivalent to importance weighting in the original <span class="math inline">\(\mathcal{X}\)</span> space</li>
<li>If <span class="math inline">\(F_Y^{-1}(s)\)</span> is known and bounded, we could use deterministic grids on <span class="math inline">\([0,1]\)</span> with <span class="math inline">\(O(N^{-4})\)</span> convergence properties</li>
</ul>
<p>The main caveats are:</p>
<ul>
<li><span class="math inline">\(F_Y^{-1}(s)\)</span> is typically unknown</li>
<li>It becomes infinite if <span class="math inline">\(L\)</span> is unbounded</li>
<li>We often resort to stochastic Riemann sums</li>
</ul>
<p>The duality implies that finding a good importance function on <span class="math inline">\([0,1]\)</span> corresponds to finding good “weighting” in <span class="math inline">\(\mathcal{X}\)</span> space. As a diagnostic for any importance sampling scheme, you should plot the equivalent grid on <span class="math inline">\([0,1]\)</span> and estimated values of <span class="math inline">\(F_Y^{-1}\)</span> to assess performance.</p>
</section>
<section id="fundamental-integral-identities" class="level3">
<h3 class="anchored" data-anchor-id="fundamental-integral-identities">Fundamental Integral Identities</h3>
<p>The two key integral identities for hyperbolic/GIG (Generalized Inverse Gaussian) and Pólya mixtures are: <span class="math display">\[
\begin{aligned}
\frac{\alpha^2 - \kappa^2}{2\alpha} e^{-\alpha|\theta - \mu| + \kappa(\theta - \mu)} &amp;= \int_0^\infty \phi\left(\theta \mid \mu + \kappa\omega, \omega\right) p_{\text{gig}}\left(\omega \mid 1, 0, \sqrt{\alpha^2 - \kappa^2}\right) d\omega \\
\frac{1}{B(\alpha,\kappa)} \frac{e^{\alpha(\theta - \mu)}}{(1 + e^{\theta - \mu})^{2(\alpha - \kappa)}} &amp;= \int_0^\infty \phi\left(\theta \mid \mu + \kappa\omega, \omega\right) p_{\text{pol}}(\omega \mid \alpha, \alpha - 2\kappa) d\omega
\end{aligned}
\]</span> where <span class="math inline">\(p_{\text{pol}}\)</span> denotes the Pólya density and <span class="math inline">\(p_{\text{gig}}\)</span> the GIG density.</p>
</section>
<section id="quantile-regression-example" class="level3">
<h3 class="anchored" data-anchor-id="quantile-regression-example">Quantile Regression Example</h3>
<p>To illustrate these integral identities in practice, consider quantile regression. Start by choosing <span class="math inline">\(q \in (0,1)\)</span> and define the quantile loss function <span class="math inline">\(l(x) = |x| + (2q-1)x\)</span>. This is also known as the check loss or hinge loss function, and is used in quantile regression for the <span class="math inline">\(q\)</span>th quantile <span class="citation" data-cites="koenker2005quantile">Koenker (<a href="references.html#ref-koenker2005quantile" role="doc-biblioref">2005</a>)</span>. <span class="citation" data-cites="johannes2009quantile">Johannes, Polson, and Yae (<a href="references.html#ref-johannes2009quantile" role="doc-biblioref">2009</a>)</span> represent this as a pseudo-likelihood involving the asymmetric Laplace distribution.</p>
<p>We can derive the corresponding envelope representation as a variance-mean Gaussian mixture. Let <span class="math inline">\(\kappa = 2q-1\)</span>. Then <span class="math inline">\(g(x) = l(x) + \kappa x = |x|\)</span> is symmetric in <span class="math inline">\(x\)</span> and concave in <span class="math inline">\(x^2\)</span>. Using the general envelope representation framework, we obtain: <span class="math display">\[
l(x) = \inf_{\lambda \geq 0} \left\{ \frac{\lambda}{2}\left( x - \frac{2q-1}{\lambda} \right)^2 - \psi(\lambda) \right\} \, .
\]</span> where <span class="math inline">\(\psi(\lambda) = \frac{\kappa^2}{2\lambda} - \frac{1}{2 \lambda^2}\)</span>.</p>
<p>This representation shows how the asymmetric quantile loss can be expressed as an envelope of quadratic functions, connecting it to the Gaussian mixture framework and enabling efficient computational algorithms.</p>
</section>
<section id="improper-limit-cases" class="level3">
<h3 class="anchored" data-anchor-id="improper-limit-cases">Improper Limit Cases</h3>
<p>These expressions lead to three important identities concerning improper limits of GIG and Pólya mixing measures for variance-mean Gaussian mixtures: <span class="math display">\[
\begin{aligned}
a^{-1} \exp\left\{-2c^{-1} \max(a\theta, 0)\right\} &amp;= \int_0^\infty \phi(\theta \mid -av, cv) dv \\
c^{-1} \exp\left\{-2c^{-1} \rho_q(\theta)\right\} &amp;= \int_0^\infty \phi(\theta \mid -(2\tau - 1)v, cv) e^{-2\tau(1-\tau)v} dv \\
(1 + \exp\{\theta - \mu\})^{-1} &amp;= \int_0^\infty \phi(\theta \mid \mu - \frac{1}{2}v, v) p_{\text{pol}}(v \mid 0, 1) dv
\end{aligned}
\]</span> where <span class="math inline">\(\rho_q(\theta) = \frac{1}{2}|\theta| + (q - \frac{1}{2})\theta\)</span> is the check-loss function.</p>
<p>These representations connect to several important regression methods. The first identity relates to <em>Support Vector Machines</em> through the hinge loss in the max function. The second identity connects to <em>Quantile and Lasso Regression</em> via check-loss and <span class="math inline">\(\ell_1\)</span> penalties. The third identity provides the <em>Logistic Regression</em> representation.</p>
<p>With GIG and Pólya mixing distributions alone, one can generate the following objective functions: <span class="math display">\[
\theta^2, \; |\theta|, \; \max(\theta,0), \; \frac{1}{2}|\theta| + (\tau - \frac{1}{2})\theta, \; \frac{1}{1+e^{-\theta}}, \; \frac{1}{(1+e^{-\theta})^r}
\]</span></p>
<p>These correspond to ridge, lasso, support vector machine, check loss/quantile regression, logit, and multinomial models, respectively. More general families can generate other penalty functions—for example, the bridge penalty <span class="math inline">\(|u|^\alpha\)</span> from a stable mixing distribution.</p>
</section>
<section id="computational-advantages-of-scale-mixtures" class="level3">
<h3 class="anchored" data-anchor-id="computational-advantages-of-scale-mixtures">Computational Advantages of Scale Mixtures</h3>
<p>The scale mixture representation provides several computational benefits. <em>Gibbs Sampling</em> becomes efficient because the conditional distributions in the augmented parameter space are often conjugate, enabling tractable MCMC algorithms. <em>EM Algorithms</em> benefit from the E-step involving expectations with respect to the mixing distribution.</p>
<p>This framework has been instrumental in developing efficient algorithms for sparse regression, robust statistics, and non-conjugate Bayesian models. The ability to represent complex penalties as hierarchical Gaussian models bridges the gap between computational tractability and statistical flexibility.</p>
</section>
<section id="generalized-ridge-regression-in-the-canonical-basis" class="level3">
<h3 class="anchored" data-anchor-id="generalized-ridge-regression-in-the-canonical-basis">Generalized Ridge Regression in the Canonical Basis</h3>
<p>We revisit ridge regression from the perspective of the canonical coordinate system, using the singular value decomposition <span class="math inline">\(X = UDW^T\)</span> and rotated coefficients <span class="math inline">\(\alpha = W^T\beta\)</span>. In this basis: <span class="math display">\[
Y = X\beta + \epsilon = UD\alpha + \epsilon
\]</span></p>
<p>Projecting onto <span class="math inline">\(U\)</span>: <span class="math display">\[
\hat{\alpha}_i = \alpha_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2/d_i^2)
\]</span></p>
<p>With independent priors <span class="math inline">\(\alpha_i \sim N(0, v_i)\)</span>, the posterior mean yields a shrinkage estimator: <span class="math display">\[
\alpha_i^* = \kappa_i \hat{\alpha}_i, \quad \text{with} \quad \kappa_i = \frac{d_i^2}{d_i^2 + k_i}
\]</span> where <span class="math inline">\(k_i = \sigma^2/v_i\)</span> is the generalized ridge penalty.</p>
<p>This framework offers componentwise optimality and anticipates global-local shrinkage priors where individual parameters receive adaptive regularization.</p>
</section>
<section id="trend-filtering-and-composite-penalties" class="level3">
<h3 class="anchored" data-anchor-id="trend-filtering-and-composite-penalties">Trend Filtering and Composite Penalties</h3>
<p>Trend filtering provides an example of regularized least squares with composite penalty, useful when a 1D signal has jumps at few positions: <span class="math display">\[
\hat{\beta} = \arg\min_{\beta} \left\{\frac{1}{2}\|y - \beta\|_2^2 + \lambda\phi(D\beta)\right\}
\]</span> where <span class="math inline">\(D\)</span> is the difference matrix and <span class="math inline">\(\phi\)</span> is a penalty function that encourages sparsity in the differences, leading to piecewise-constant or piecewise-polynomial solutions.</p>
</section>
</section>
<section id="final-thoughts" class="level2" data-number="17.15">
<h2 data-number="17.15" class="anchored" data-anchor-id="final-thoughts"><span class="header-section-number">17.15</span> Final Thoughts</h2>
<p>Our exploration began with a sobering revelation: the maximum likelihood estimator, long considered the gold standard of classical statistics, is inadmissible in high-dimensional settings. Stein’s paradox demonstrates that for <span class="math inline">\(p \geq 3\)</span> dimensions, there always exist estimators with uniformly lower risk than the MLE. This is not merely a theoretical curiosity—in the normal means problem with <span class="math inline">\(p=100\)</span>, the James-Stein estimator can achieve 67 times lower risk than the MLE. This dramatic improvement illustrates why classical approaches often fail in modern high-dimensional problems and why shrinkage methods have become essential tools in contemporary data science.</p>
<p>The James-Stein estimator’s success stems from its ability to “borrow strength” across components through global shrinkage, demonstrating that multivariate estimation problems are fundamentally easier than their univariate counterparts when approached with appropriate regularization. However, global shrinkage alone is insufficient for sparse signals, motivating the development of more sophisticated approaches that can adapt to local signal structure.</p>
<p>A central theme throughout this chapter is the profound duality between Bayesian priors and regularization penalties. Every regularization term <span class="math inline">\(\lambda \phi(f)\)</span> corresponds to a prior distribution through the relationship <span class="math inline">\(\phi(f) = -\log p(f)\)</span>, making maximum a posteriori (MAP) estimation equivalent to penalized optimization. This duality provides both theoretical justification for regularization methods and practical guidance for prior specification:</p>
<table class="tab-small caption-top table">
<caption>Penalty types and their corresponding prior distributions.</caption>
<colgroup>
<col style="width: 15%">
<col style="width: 27%">
<col style="width: 13%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Penalty</th>
<th>Prior</th>
<th>Key Property/Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ridge</td>
<td><span class="math inline">\(\beta^2\)</span></td>
<td>Gaussian</td>
<td>Grows Quadratically. Shrinks slowly. Encourages smoothness and numerical stability</td>
</tr>
<tr class="even">
<td>Lasso</td>
<td><span class="math inline">\(|\beta|\)</span></td>
<td>Laplace</td>
<td>Grows Linearly. Shrinks strongly. Induces sparsity but shrinks strong signals</td>
</tr>
<tr class="odd">
<td>Bridge</td>
<td><span class="math inline">\(|\beta|^\alpha\)</span></td>
<td>Exponential</td>
<td>Interpolates between Ridge and Lasso.</td>
</tr>
<tr class="even">
<td>Cauchy</td>
<td><span class="math inline">\(\log(1+\beta^2)\)</span></td>
<td>Cauchy</td>
<td>Heavy-tailed. Grows logarithmically. Strong shrinkage near zero, minimal shrinkage for large signals</td>
</tr>
<tr class="odd">
<td>Horseshoe</td>
<td><span class="math inline">\(-\log\log(1 + 2\tau^2/\beta^2)\)</span></td>
<td>Horseshoe</td>
<td>Grows almost flat for large signals. Shrinks slowly. Optimal sparsity with strong signals preserved. <span class="math inline">\(\tau\)</span> is a scale parameter.</td>
</tr>
<tr class="even">
<td>Subset selection</td>
<td><span class="math inline">\(I(\beta \neq 0)\)</span></td>
<td>Spike-and-slab</td>
<td>Shrinks strongly. Exact sparsity through hard thresholding. Computationally hard.</td>
</tr>
</tbody>
</table>
<p>This mapping reveals why certain penalties work well for particular problem types and guides the selection of appropriate regularization strategies based on problem structure and prior beliefs about the solution. <a href="#fig-horseshoe" class="quarto-xref">Figure&nbsp;<span>17.11</span></a> shows the unit balls defined by the norms induced by different priors.</p>
<!-- ![Comparison of unit balls for norms induced by different priors](fig/horseshoe.png){#fig-horseshoe} -->
<div id="fig-horseshoe" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-horseshoe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-horseshoe-bridge" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-horseshoe-bridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/fig_bridge_detailed.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-horseshoe-bridge-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Bridge
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-horseshoe-unit-ball" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-horseshoe-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/horseshoe_unit_ball_annotated.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-horseshoe-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Horseshoe
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-horseshoe" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-cauchy-unit-ball" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-cauchy-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/cauchy_unit_ball_annotated.png" class="img-fluid figure-img" data-ref-parent="fig-horseshoe">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-cauchy-unit-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Cauchy
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-horseshoe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.11: Comparison of unit balls for norms induced by different priors
</figcaption>
</figure>
</div>
<!-- reproduce_fig_horseshoe.R
code/cauchy_unit_ball.R 
code/horseshoe_unit_ball.R -->
<p>The <a href="#fig-horseshoe-bridge" class="quarto-xref">Figure&nbsp;<span>17.11 (a)</span></a> illustrates the unit balls for different penalty norms, each revealing distinct geometric properties. The <span class="math inline">\(\ell_2\)</span> (Ridge) penalty creates a circular unit ball that encourages smoothness across all parameters. The <span class="math inline">\(\ell_1\)</span> (Lasso) penalty forms a diamond shape that induces sparsity through soft thresholding. Bridge penalties with <span class="math inline">\(\alpha = 0.5\)</span> create more pointed shapes that provide stronger sparsity than Lasso, while <span class="math inline">\(\alpha = 1.5\)</span> produces shapes between the diamond and circle. The <span class="math inline">\(\ell_0\)</span> penalty reduces to discrete points on the coordinate axes, representing exact subset selection.</p>
<p><a href="#fig-horseshoe-unit-ball" class="quarto-xref">Figure&nbsp;<span>17.11 (b)</span></a> shows the unit balls for penalty norms induced by the horseshoe prior. Visually, it looks similar to the unit ball induced by the bridge prior with <span class="math inline">\(\alpha = 0.5\)</span>. It demonstrates several remarkable properties that make it particularly effective for sparse estimation problems. Its penalty function <span class="math inline">\(\phi(\theta_i) = -\log(\log(1 + 2\tau^2/\theta_i^2))\)</span> exhibits a unique double-logarithmic structure that creates an ideal balance between sparsity induction and signal preservation.</p>
<p>The horseshoe’s most distinctive characteristic is its asymmetric shrinkage behavior. Near the origin, when parameters approach zero, the penalty function grows to infinity, creating extremely strong shrinkage that encourages exact zeros and thus induces sparsity. However, for large parameter values, the penalty grows slowly, allowing large signals to escape penalization with minimal distortion. This behavior is fundamentally different from methods like Lasso, which applies uniform shrinkage regardless of signal magnitude.</p>
<p>The global shrinkage parameter <span class="math inline">\(\tau\)</span> provides crucial control over the prior’s behavior. Smaller values of <span class="math inline">\(\tau\)</span> impose more aggressive shrinkage across all parameters, while larger values become more permissive, allowing signals to emerge more easily. This parameter effectively controls the trade-off between sparsity and signal detection.</p>
<p>From a theoretical perspective, the horseshoe prior achieves optimal minimax convergence rates of <span class="math inline">\(O(s \log(p/s))\)</span> for <span class="math inline">\(s\)</span>-sparse vectors, making it particularly well-suited for “needle-in-a-haystack” problems where researchers seek to identify a few large signals among many noise variables. Under appropriate regularity conditions, the horseshoe also possesses oracle properties, meaning it can correctly identify the true signal structure.</p>
<p>The geometric visualization reveals why this prior is so effective. The characteristic “horseshoe” shape displays concave contours near the coordinate axes, reflecting strong shrinkage toward zero, while showing convex contours away from the origin, indicating minimal shrinkage for large coefficients. Unlike the Lasso’s diamond shape or Ridge’s circular contours, the horseshoe’s heavy tails allow large coefficients to escape penalization entirely.</p>
<p>This comprehensive visualization demonstrates why the horseshoe prior has become increasingly popular for modern high-dimensional problems where the true signal structure is sparse but individual signals may be large in magnitude. The prior’s ability to provide aggressive shrinkage for noise while preserving signal integrity makes it an ideal choice for contemporary machine learning applications.</p>
<p><a href="#fig-cauchy-unit-ball" class="quarto-xref">Figure&nbsp;<span>17.11 (c)</span></a> shows the unit ball for the Cauchy prior. It demonstrates several remarkable properties that make it particularly effective for sparse estimation problems. Its penalty function <span class="math inline">\(\phi(\theta_i) = \log(1 + \theta_i^2)\)</span> exhibits a unique logarithmic structure that creates an ideal balance between sparsity induction and signal preservation.</p>
<p>The Cauchy prior’s most distinctive characteristic is its heavy-tailed behavior. Near the origin, when parameters approach zero, the penalty function grows logarithmically, creating extremely strong shrinkage that encourages exact zeros and thus induces sparsity. However, for large parameter values, the penalty grows slowly, allowing large signals to escape penalization with minimal distortion. This behavior is fundamentally different from methods like Lasso, which applies uniform shrinkage regardless of signal magnitude.</p>
<p>The global shrinkage parameter <span class="math inline">\(\tau\)</span> provides crucial control over the prior’s behavior. Smaller values of <span class="math inline">\(\tau\)</span> impose more aggressive shrinkage across all parameters, while larger values become more permissive, allowing signals to emerge more easily. This parameter effectively controls the trade-off between sparsity and signal detection.</p>
<p>Our analysis reveals a rich spectrum of approaches to sparsity, each with distinct theoretical properties and practical advantages.</p>
<p><em>Global shrinkage methods</em> such as James-Stein estimation and Ridge regression apply uniform regularization across all parameters. While computationally simple and numerically stable, they cannot adapt to local signal structure and may over-shrink large signals in sparse settings.</p>
<p><em>Adaptive shrinkage methods</em> like Lasso and horseshoe priors provide differential shrinkage based on signal strength. Lasso employs soft thresholding, while the horseshoe applies aggressive shrinkage to small signals while preserving large ones, achieving the optimal minimax rate <span class="math inline">\(p_n \log(n/p_n)\)</span>. The horseshoe is particularly effective for needle-in-a-haystack problems.</p>
<p><em>Variable selection methods</em> including spike-and-slab priors and <span class="math inline">\(\ell_0\)</span> regularization make discrete inclusion or exclusion decisions. These produce the most interpretable sparse solutions and can achieve oracle properties under appropriate conditions, though they are combinatorially challenging to implement.</p>
<p>Building on the theoretical foundations and geometric intuition developed above, we can distill several actionable guidelines for practitioners and researchers working with regularized models and sparse estimation:</p>
<div id="tbl-practical-guidelines" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-practical-guidelines-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.2: Practical guidelines for regularized models and sparse estimation.
</figcaption>
<div aria-describedby="tbl-practical-guidelines-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 50%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Scenario</th>
<th style="text-align: left;">Recommended Methods</th>
<th style="text-align: left;">Rationale</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Low-dimensional (<span class="math inline">\(p &lt; n\)</span>)</td>
<td style="text-align: left;">Ridge regression</td>
<td style="text-align: left;">Numerical stability; sufficient for moderate <span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">High-dimensional sparse</td>
<td style="text-align: left;">Lasso, Bridge (<span class="math inline">\(\alpha \approx 0.7\)</span>), Horseshoe</td>
<td style="text-align: left;">Lasso balances efficiency and sparsity; Bridge and Horseshoe offer better properties</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ultra-sparse</td>
<td style="text-align: left;">Spike-and-slab, <span class="math inline">\(\ell_0\)</span> methods</td>
<td style="text-align: left;">Optimal performance and exact sparsity</td>
</tr>
<tr class="even">
<td style="text-align: left;">Correlated predictors</td>
<td style="text-align: left;">Bridge, full Bayesian</td>
<td style="text-align: left;">Handle correlation better than Lasso</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Uncertainty quantification</td>
<td style="text-align: left;">Full Bayesian methods</td>
<td style="text-align: left;">Provide credible intervals and posterior probabilities</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While our focus has been on linear models, the principles developed here extend broadly to modern AI systems. Deep learning architectures routinely employ regularization techniques (dropout, weight decay, batch normalization) that can be understood through the Bayesian lens developed in this chapter. The success of techniques like variational autoencoders and Bayesian neural networks demonstrates the continued relevance of probabilistic thinking in contemporary machine learning.</p>
<p>Moreover, the sparse estimation techniques discussed here are fundamental to interpretable AI, compressed sensing, and efficient neural architecture design. The theoretical insights about when different regularization approaches succeed provide guidance for designing and analyzing complex learning systems.</p>
<p>The evolution from maximum likelihood estimation to sophisticated Bayesian regularization represents more than technical progress—it reflects a fundamental shift in how we approach learning from data. Rather than seeking single “best” estimates, modern methods embrace uncertainty, incorporate prior knowledge, and adaptively balance model complexity with empirical fit.</p>
<p>The remarkable fact that Bayesian approaches often dominate classical frequentist methods, even from a frequentist perspective (as demonstrated by Stein’s paradox), suggests that probabilistic thinking provides not just philosophical appeal but concrete practical advantages. In an era of ever-growing data complexity and dimensionality, these theoretical insights become increasingly valuable for developing robust, interpretable, and effective learning algorithms.</p>
<p>The unified framework presented in this chapter—connecting classical statistics, Bayesian inference, optimization theory, and modern machine learning—provides both historical perspective and forward-looking guidance for the continued development of artificial intelligence systems. As we confront increasingly complex learning challenges, from personalized medicine to autonomous systems, the principled approach to regularization and uncertainty quantification developed here will remain fundamental to building trustworthy and effective AI systems.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-andrews1974scale" class="csl-entry" role="listitem">
Andrews, D. F., and C. L. Mallows. 1974. <span>“Scale <span>Mixtures</span> of <span>Normal Distributions</span>.”</span> <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 36 (1): 99–102.
</div>
<div id="ref-carvalho2010horseshoe" class="csl-entry" role="listitem">
Carvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. <span>“The Horseshoe Estimator for Sparse Signals.”</span> <em>Biometrika</em>, asq017.
</div>
<div id="ref-diaconis1983quantifying" class="csl-entry" role="listitem">
Diaconis, P., and D. Ylvisaker. 1983. <span>“Quantifying <span>Prior Opinion</span>.”</span>
</div>
<div id="ref-efron1975data" class="csl-entry" role="listitem">
Efron, Bradley, and Carl Morris. 1975. <span>“Data <span>Analysis Using Stein</span>’s <span>Estimator</span> and Its <span>Generalizations</span>.”</span> <em>Journal of the American Statistical Association</em> 70 (350): 311–19.
</div>
<div id="ref-efron1977steins" class="csl-entry" role="listitem">
———. 1977. <span>“Stein’s Paradox in Statistics.”</span> <em>Scientific American</em> 236 (5): 119–27.
</div>
<div id="ref-johannes2009quantile" class="csl-entry" role="listitem">
Johannes, Michael S., Nick Polson, and Seung M. Yae. 2009. <span>“Quantile <span>Filtering</span> and <span>Learning</span>.”</span> <em>SSRN Electronic Journal</em>.
</div>
<div id="ref-koenker2005quantile" class="csl-entry" role="listitem">
Koenker, Roger. 2005. <em>Quantile <span>Regression</span></em>. Econometric <span>Society Monographs</span>. Cambridge: Cambridge University Press.
</div>
<div id="ref-polson2012good" class="csl-entry" role="listitem">
Polson, Nicholas G., and James G. Scott. 2012. <span>“Good, Great, or Lucky? <span>Screening</span> for Firms with Sustained Superior Performance Using Heavy-Tailed Priors.”</span> <em>The Annals of Applied Statistics</em> 6 (1): 161–85.
</div>
<div id="ref-polson2011data" class="csl-entry" role="listitem">
Polson, Nicholas G., and Steven L. Scott. 2011. <span>“Data Augmentation for Support Vector Machines.”</span> <em>Bayesian Analysis</em> 6 (1): 1–23.
</div>
<div id="ref-tikhonov1943stability" class="csl-entry" role="listitem">
Tikhonov, Andrey Nikolayevich et al. 1943. <span>“On the Stability of Inverse Problems.”</span> In <em>Dokl. <span>Akad</span>. <span>Nauk</span> Sssr</em>, 39:195–98.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./16-select.html" class="pagination-link" aria-label="Model Selection">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./18-nn.html" class="pagination-link" aria-label="Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>