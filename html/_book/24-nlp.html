<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>24&nbsp; Natural Language Processing – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./25-llm.html" rel="next">
<link href="./23-cnn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head><body class="nav-sidebar docked quarto-light"><div class="hidden">
<p><span class="math display">\[
\newcommand{\prob}[1]{\operatorname{P}\left(#1\right)}
\newcommand{\Var}[1]{\operatorname{Var}\left(#1\right)}
\newcommand{\sd}[1]{\operatorname{sd}\left(#1\right)}
\newcommand{\Cor}[1]{\operatorname{Corr}\left(#1\right)}
\newcommand{\Cov}[1]{\operatorname{Cov}\left(#1\right)}
\newcommand{\E}[2][]{\operatorname{E}_{#1}\left[#2\right]}
\newcommand{\defeq}{\overset{\text{\tiny def}}{=}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\mini}{minimize}
\DeclareMathOperator*{\minf}{minimize \quad}
\newcommand{\mininlineeq}[4]{\begin{equation}\label{#4}\mbox{minimize}_{#1}\quad#2\qquad\mbox{subject to }#3\end{equation}}
\]</span></p>
</div>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-nlp.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Robots and AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-representation-problem" id="toc-the-representation-problem" class="nav-link active" data-scroll-target="#the-representation-problem"><span class="header-section-number">24.1</span> The Representation Problem</a></li>
  <li><a href="#word2vec-and-distributional-semantics" id="toc-word2vec-and-distributional-semantics" class="nav-link" data-scroll-target="#word2vec-and-distributional-semantics"><span class="header-section-number">24.2</span> Word2Vec and Distributional Semantics</a>
  <ul class="collapse">
  <li><a href="#the-skip-gram-model" id="toc-the-skip-gram-model" class="nav-link" data-scroll-target="#the-skip-gram-model">The Skip-Gram Model</a></li>
  <li><a href="#the-continuous-bag-of-words-cbow-model" id="toc-the-continuous-bag-of-words-cbow-model" class="nav-link" data-scroll-target="#the-continuous-bag-of-words-cbow-model">The Continuous Bag of Words (CBOW) Model</a></li>
  <li><a href="#pretraining-word2vec" id="toc-pretraining-word2vec" class="nav-link" data-scroll-target="#pretraining-word2vec">Pretraining Word2Vec</a></li>
  </ul></li>
  <li><a href="#computational-efficiency-through-negative-sampling" id="toc-computational-efficiency-through-negative-sampling" class="nav-link" data-scroll-target="#computational-efficiency-through-negative-sampling"><span class="header-section-number">24.3</span> Computational Efficiency Through Negative Sampling</a></li>
  <li><a href="#global-vectors-and-matrix-factorization" id="toc-global-vectors-and-matrix-factorization" class="nav-link" data-scroll-target="#global-vectors-and-matrix-factorization"><span class="header-section-number">24.4</span> Global Vectors and Matrix Factorization</a></li>
  <li><a href="#beyond-words-subword-and-character-models" id="toc-beyond-words-subword-and-character-models" class="nav-link" data-scroll-target="#beyond-words-subword-and-character-models"><span class="header-section-number">24.5</span> Beyond Words: Subword and Character Models</a></li>
  <li><a href="#contextual-representations-and-the-transformer-architecture" id="toc-contextual-representations-and-the-transformer-architecture" class="nav-link" data-scroll-target="#contextual-representations-and-the-transformer-architecture"><span class="header-section-number">24.6</span> Contextual Representations and the Transformer Architecture</a></li>
  <li><a href="#pretraining-at-scale-bert-and-beyond" id="toc-pretraining-at-scale-bert-and-beyond" class="nav-link" data-scroll-target="#pretraining-at-scale-bert-and-beyond"><span class="header-section-number">24.7</span> Pretraining at Scale: BERT and Beyond</a>
  <ul class="collapse">
  <li><a href="#bert-architecture-and-training-details" id="toc-bert-architecture-and-training-details" class="nav-link" data-scroll-target="#bert-architecture-and-training-details">BERT Architecture and Training Details</a></li>
  <li><a href="#data-preparation-for-bert-pretraining" id="toc-data-preparation-for-bert-pretraining" class="nav-link" data-scroll-target="#data-preparation-for-bert-pretraining">Data Preparation for BERT Pretraining</a></li>
  <li><a href="#beyond-bert-improvements-and-variants" id="toc-beyond-bert-improvements-and-variants" class="nav-link" data-scroll-target="#beyond-bert-improvements-and-variants">Beyond BERT: Improvements and Variants</a></li>
  <li><a href="#practical-considerations-for-pretraining" id="toc-practical-considerations-for-pretraining" class="nav-link" data-scroll-target="#practical-considerations-for-pretraining">Practical Considerations for Pretraining</a></li>
  </ul></li>
  <li><a href="#transfer-learning-and-downstream-applications" id="toc-transfer-learning-and-downstream-applications" class="nav-link" data-scroll-target="#transfer-learning-and-downstream-applications"><span class="header-section-number">24.8</span> Transfer Learning and Downstream Applications</a></li>
  <li><a href="#model-compression-and-efficiency" id="toc-model-compression-and-efficiency" class="nav-link" data-scroll-target="#model-compression-and-efficiency"><span class="header-section-number">24.9</span> Model Compression and Efficiency</a></li>
  <li><a href="#theoretical-perspectives-and-future-directions" id="toc-theoretical-perspectives-and-future-directions" class="nav-link" data-scroll-target="#theoretical-perspectives-and-future-directions"><span class="header-section-number">24.10</span> Theoretical Perspectives and Future Directions</a></li>
  <li><a href="#natural-language-processing-applications" id="toc-natural-language-processing-applications" class="nav-link" data-scroll-target="#natural-language-processing-applications"><span class="header-section-number">24.11</span> Natural Language Processing: Applications</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-from-opinions-to-insights" id="toc-sentiment-analysis-from-opinions-to-insights" class="nav-link" data-scroll-target="#sentiment-analysis-from-opinions-to-insights">Sentiment Analysis: From Opinions to Insights</a></li>
  <li><a href="#natural-language-inference-reasoning-about-meaning" id="toc-natural-language-inference-reasoning-about-meaning" class="nav-link" data-scroll-target="#natural-language-inference-reasoning-about-meaning">Natural Language Inference: Reasoning About Meaning</a></li>
  <li><a href="#token-level-applications-precision-at-the-word-level" id="toc-token-level-applications-precision-at-the-word-level" class="nav-link" data-scroll-target="#token-level-applications-precision-at-the-word-level">Token-Level Applications: Precision at the Word Level</a></li>
  <li><a href="#fine-tuning-pretrained-models" id="toc-fine-tuning-pretrained-models" class="nav-link" data-scroll-target="#fine-tuning-pretrained-models">Fine-Tuning Pretrained Models</a></li>
  <li><a href="#challenges-and-future-directions" id="toc-challenges-and-future-directions" class="nav-link" data-scroll-target="#challenges-and-future-directions">Challenges and Future Directions</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">24.12</span> Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./19-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./24-nlp.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>The ability to understand and generate human language has long been considered a hallmark of intelligence. When Alan Turing proposed his famous test in 1950, he chose natural conversation as the ultimate benchmark for machine intelligence. Yet for decades, this goal remained frustratingly elusive. Early attempts at machine translation in the 1950s, which simply replaced words using bilingual dictionaries, produced nonsensical results that highlighted the profound complexity of human language. The phrase “The spirit is willing, but the flesh is weak” allegedly translated to Russian and back as “The vodka is good, but the meat is rotten”—a cautionary tale about the subtleties of meaning that transcend mere word substitution.</p>
<p>This chapter traces the remarkable journey from those early failures to today’s language models that can engage in nuanced dialogue, translate between languages with near-human accuracy, and even generate creative text. At the heart of this transformation lies a fundamental shift in how we represent language computationally: from discrete symbols manipulated by hand-crafted rules to continuous vector spaces learned from vast corpora of text.</p>
<section id="the-representation-problem" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="the-representation-problem"><span class="header-section-number">24.1</span> The Representation Problem</h2>
<p>Language presents unique challenges for mathematical modeling. Unlike images, which naturally exist as arrays of continuous pixel values, or audio signals, which are continuous waveforms, text consists of discrete symbols with no inherent geometric structure. The word “cat” is not inherently closer to “dog” than to “quantum”—at least not in any obvious mathematical sense. Yet humans effortlessly recognize that cats and dogs share semantic properties that neither shares with abstract physics concepts.</p>
<p>Consider the seemingly simple task of determining whether two sentences have similar meanings. The sentences “The cat sat on the mat” and “A feline rested on the rug” express nearly identical ideas despite sharing no words except “the” and “on.” Conversely, “The bank is closed” could refer to a financial institution or a river’s edge—the same words encoding entirely different meanings. These examples illustrate why early symbolic approaches to natural language processing, based on logical rules and hand-crafted features, struggled to capture the fluid, contextual nature of meaning.</p>
<p>The breakthrough came from reconceptualizing the representation problem. Instead of treating words as atomic symbols, what if we could embed them in a continuous vector space where geometric relationships encode semantic relationships? This idea, simple in retrospect, revolutionized the field. In such a space, we might find that <span class="math inline">\(\vec{v}_{\text{cat}} - \vec{v}_{\text{dog}}\)</span> has similar direction to <span class="math inline">\(\vec{v}_{\text{car}} - \vec{v}_{\text{bicycle}}\)</span>, capturing the analogical relationship “cat is to dog as car is to bicycle” through vector arithmetic.</p>
<p>To formalize this intuition, we seek a mapping <span class="math inline">\(\phi: \mathcal{V} \rightarrow \mathbb{R}^d\)</span> from a vocabulary <span class="math inline">\(\mathcal{V}\)</span> of discrete tokens to <span class="math inline">\(d\)</span>-dimensional vectors. The challenge lies in learning this mapping such that the resulting geometry reflects semantic relationships. The naive approach of one-hot encoding, where each word is represented by a vector with a single non-zero entry, fails catastrophically: in an <span class="math inline">\(N\)</span>-word vocabulary, this produces <span class="math inline">\(N\)</span>-dimensional vectors where every pair of distinct words has cosine similarity zero, erasing all notion of semantic relatedness.</p>
</section>
<section id="word2vec-and-distributional-semantics" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="word2vec-and-distributional-semantics"><span class="header-section-number">24.2</span> Word2Vec and Distributional Semantics</h2>
<p>The theoretical foundation for learning meaningful word representations comes from the distributional hypothesis, articulated by linguist J.R. Firth in 1957: “You shall know a word by the company it keeps.” This principle suggests that words appearing in similar contexts tend to have similar meanings. If “coffee” and “tea” both frequently appear near words like “drink,” “hot,” “cup,” and “morning,” we can infer their semantic similarity.</p>
<p>The word2vec framework, introduced by Mikolov et al.&nbsp;in 2013, operationalized this insight through a beautifully simple probabilistic model. The skip-gram variant posits that a word can be used to predict its surrounding context words. Given a corpus of text represented as a sequence of words <span class="math inline">\(w_1, w_2, \ldots, w_T\)</span>, the model maximizes the likelihood:</p>
<p><span class="math display">\[\mathcal{L} = \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} \mid w_t)\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the context window size. The conditional probability is parameterized using two sets of embeddings: <span class="math inline">\(\mathbf{v}_w\)</span> for words as centers and <span class="math inline">\(\mathbf{u}_w\)</span> for words as context:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^T \mathbf{v}_c)}{\sum_{w \in \mathcal{V}} \exp(\mathbf{u}_w^T \mathbf{v}_c)}\]</span></p>
<p>This formulation reveals deep connections to the theoretical frameworks discussed in previous chapters. The dot product <span class="math inline">\(\mathbf{u}_o^T \mathbf{v}_c\)</span> acts as a compatibility score between center and context words, while the softmax normalization ensures a valid probability distribution. From the perspective of ridge functions, we can view this as learning representations where the function <span class="math inline">\(f(w_c, w_o) = \mathbf{u}_o^T \mathbf{v}_c\)</span> captures the log-odds of co-occurrence.</p>
<section id="the-skip-gram-model" class="level3">
<h3 class="anchored" data-anchor-id="the-skip-gram-model">The Skip-Gram Model</h3>
<p>The skip-gram model operates on a simple yet powerful principle: given a center word, predict the surrounding context words within a fixed window. This approach assumes that words appearing in similar contexts tend to have similar meanings, directly implementing the distributional hypothesis.</p>
<p>Consider the sentence “The man loves his son” with “loves” as the center word and a context window of size 2. The skip-gram model aims to maximize the probability of generating the context words “the,” “man,” “his,” and “son” given the center word “loves.” This relationship can be visualized as follows:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[loves] --&gt; B[the]
    A --&gt; C[man]
    A --&gt; D[his]
    A --&gt; E[son]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    
    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class A centerWord
    class B,C,D,E contextWord
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The mathematical foundation assumes conditional independence among context words given the center word, allowing the joint probability to factorize:</p>
<p><span class="math display">\[P(\text{"the"}, \text{"man"}, \text{"his"}, \text{"son"} \mid \text{"loves"}) = P(\text{"the"} \mid \text{"loves"}) \cdot P(\text{"man"} \mid \text{"loves"}) \cdot P(\text{"his"} \mid \text{"loves"}) \cdot P(\text{"son"} \mid \text{"loves"})\]</span></p>
<p>This independence assumption, while not strictly true in natural language, proves crucial for computational tractability. Each conditional probability is modeled using the softmax function over the entire vocabulary:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^T \mathbf{v}_c)}{\sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^T \mathbf{v}_c)}\]</span></p>
<p>The skip-gram objective seeks to maximize the likelihood of observing all context words across the entire corpus. For a text sequence of length <span class="math inline">\(T\)</span> with words <span class="math inline">\(w^{(1)}, w^{(2)}, \ldots, w^{(T)}\)</span>, the objective becomes:</p>
<p><span class="math display">\[\mathcal{L}_{\text{skip-gram}} = \frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w^{(t+j)} \mid w^{(t)})\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the context window size. The normalization by <span class="math inline">\(T\)</span> ensures that the objective remains bounded as corpus size grows.</p>
<p>The gradient with respect to the center word embedding reveals the learning dynamics:</p>
<p><span class="math display">\[\frac{\partial \log P(w_o \mid w_c)}{\partial \mathbf{v}_c} = \mathbf{u}_o - \sum_{i \in \mathcal{V}} P(w_i \mid w_c) \mathbf{u}_i\]</span></p>
<p>This elegant form shows that the gradient pushes the center word embedding toward the observed context word (<span class="math inline">\(\mathbf{u}_o\)</span>) while pulling it away from all other words, weighted by their predicted probabilities. This creates a natural contrast between positive and negative examples, even in the original formulation without explicit negative sampling.</p>
<p>The skip-gram architecture assigns two vector representations to each word: <span class="math inline">\(\mathbf{v}_w\)</span> when the word serves as a center word and <span class="math inline">\(\mathbf{u}_w\)</span> when it appears in context. This asymmetry allows the model to capture different aspects of word usage. After training, the center word vectors <span class="math inline">\(\mathbf{v}_w\)</span> are typically used as the final word embeddings, though some implementations average or concatenate both representations.</p>
</section>
<section id="the-continuous-bag-of-words-cbow-model" class="level3">
<h3 class="anchored" data-anchor-id="the-continuous-bag-of-words-cbow-model">The Continuous Bag of Words (CBOW) Model</h3>
<p>While skip-gram predicts context words from a center word, the Continuous Bag of Words (CBOW) model reverses this relationship: it predicts a center word based on its surrounding context. For the same text sequence “the”, “man”, “loves”, “his”, “son” with “loves” as the center word, CBOW models the conditional probability:</p>
<p><span class="math display">\[P(\text{"loves"} \mid \text{"the"}, \text{"man"}, \text{"his"}, \text{"son"})\]</span></p>
<p>The CBOW architecture can be visualized as multiple context words converging to predict a single center word:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    B[the] --&gt; A[loves]
    C[man] --&gt; A
    D[his] --&gt; A
    E[son] --&gt; A
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#f3e5f5
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    
    classDef centerWord fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef contextWord fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class A centerWord
    class B,C,D,E contextWord
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The key difference from skip-gram lies in how CBOW handles multiple context words. Rather than treating each context word independently, CBOW averages their embeddings. For a center word <span class="math inline">\(w_c\)</span> with context words <span class="math inline">\(w_{o_1}, \ldots, w_{o_{2m}}\)</span>, the conditional probability is:</p>
<p><span class="math display">\[P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\exp\left(\mathbf{u}_c^T \bar{\mathbf{v}}_o\right)}{\sum_{i \in \mathcal{V}} \exp\left(\mathbf{u}_i^T \bar{\mathbf{v}}_o\right)}\]</span></p>
<p>where <span class="math inline">\(\bar{\mathbf{v}}_o = \frac{1}{2m}\sum_{j=1}^{2m} \mathbf{v}_{o_j}\)</span> is the average of context word vectors. Note that in CBOW, <span class="math inline">\(\mathbf{u}_i\)</span> represents word <span class="math inline">\(i\)</span> as a center word and <span class="math inline">\(\mathbf{v}_i\)</span> represents it as a context word—the opposite of skip-gram’s convention.</p>
<p>The CBOW objective maximizes the likelihood of generating all center words given their contexts:</p>
<p><span class="math display">\[\mathcal{L}_{\text{CBOW}} = \sum_{t=1}^T \log P(w^{(t)} \mid w^{(t-m)}, \ldots, w^{(t-1)}, w^{(t+1)}, \ldots, w^{(t+m)})\]</span></p>
<p>The gradient with respect to context word vectors reveals how CBOW learns:</p>
<p><span class="math display">\[\frac{\partial \log P(w_c \mid \mathcal{W}_o)}{\partial \mathbf{v}_{o_i}} = \frac{1}{2m}\left(\mathbf{u}_c - \sum_{j \in \mathcal{V}} P(w_j \mid \mathcal{W}_o) \mathbf{u}_j\right)\]</span></p>
<p>This gradient is scaled by <span class="math inline">\(\frac{1}{2m}\)</span>, effectively distributing the learning signal across all context words. CBOW tends to train faster than skip-gram because it predicts one word per context window rather than multiple words, but skip-gram often produces better representations for rare words since it generates more training examples per sentence.</p>
</section>
<section id="pretraining-word2vec" class="level3">
<h3 class="anchored" data-anchor-id="pretraining-word2vec">Pretraining Word2Vec</h3>
<p>Training word2vec models requires careful attention to implementation details that significantly impact the quality of learned representations. The training process begins with data preprocessing on large text corpora. Using the Penn Tree Bank dataset as an example—a carefully annotated corpus of Wall Street Journal articles containing about 1 million words—we implement several crucial preprocessing steps.</p>
<p>First, we build a vocabulary by counting word frequencies and retaining only words that appear at least a minimum number of times (typically 5-10). This thresholding serves two purposes: it reduces the vocabulary size from potentially millions to tens of thousands of words, and it prevents the model from wasting capacity on rare words that appear too infrequently to learn meaningful representations. Words below the threshold are replaced with a special <code>&lt;unk&gt;</code> token.</p>
<p>The training procedure uses stochastic gradient descent with a carefully designed learning rate schedule. The initial learning rate (typically 0.025 for skip-gram and 0.05 for CBOW) is linearly decreased to a minimum value (usually 0.0001) as training progresses:</p>
<p><span class="math display">\[\alpha_t = \alpha_0 \left(1 - \frac{\text{words\_processed}}{\text{total\_words} \times \text{epochs}}\right)\]</span></p>
<p>This schedule ensures larger updates early in training when representations are poor, transitioning to fine-tuning as the model converges.</p>
<p>The implementation uses several optimizations for efficiency. Word vectors are typically initialized randomly from a uniform distribution over <span class="math inline">\([-0.5/d, 0.5/d]\)</span> where <span class="math inline">\(d\)</span> is the embedding dimension (commonly 100-300). During training, we maintain two embedding matrices: one for center words and one for context words. After training, these can be combined (usually by averaging) or just the center word embeddings can be used.</p>
<p>For minibatch processing, training examples are grouped to enable efficient matrix operations. Given a batch of center-context pairs, the forward pass computes scores using matrix multiplication, applies the loss function (either full softmax, hierarchical softmax, or negative sampling), and backpropagates gradients. A typical training configuration might process batches of 512 word pairs, iterating 5-15 times over a corpus.</p>
<p>The quality of learned embeddings can be evaluated through word similarity and analogy tasks. For similarity, we compute cosine distances between word vectors and verify that semantically similar words have high cosine similarity. For analogies, we test whether vector arithmetic captures semantic relationships: the famous “king - man + woman ≈ queen” example demonstrates that vector differences encode meaningful semantic transformations.</p>
<p>Training word2vec on real data requires several practical considerations. Using the Penn Tree Bank dataset—a carefully annotated corpus of Wall Street Journal articles—we must first tokenize the text and build a vocabulary. Typically, we keep only words appearing at least 10 times, replacing rare words with a special <code>&lt;unk&gt;</code> token. This thresholding reduces vocabulary size and helps the model focus on learning good representations for common words.</p>
<p>A crucial but often overlooked aspect is subsampling of frequent words. Words like “the,” “a,” and “is” appear so frequently that they provide little information about the semantic content of their neighbors. The probability of discarding a word <span class="math inline">\(w\)</span> during training is:</p>
<p><span class="math display">\[P(\text{discard } w) = 1 - \sqrt{\frac{t}{f(w)}}\]</span></p>
<p>where <span class="math inline">\(f(w)\)</span> is the frequency of word <span class="math inline">\(w\)</span> and <span class="math inline">\(t\)</span> is a threshold (typically <span class="math inline">\(10^{-5}\)</span>). This formula ensures that very frequent words are aggressively subsampled while preserving most occurrences of informative words.</p>
<p>For training, we extract examples by sliding a window over the text. For each center word, we collect its context words within a window of size <span class="math inline">\(m\)</span>. Importantly, the actual window size is sampled uniformly from <span class="math inline">\([1, m]\)</span> for each center word, which helps the model learn representations that are robust to varying context sizes. Consider the sentence “the cat sat on the mat” with maximum window size 2. For the center word “sat,” we might sample a window size of 1, giving context words [“cat”, “on”], or a window size of 2, giving context words [“the”, “cat”, “on”, “the”].</p>
</section>
</section>
<section id="computational-efficiency-through-negative-sampling" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="computational-efficiency-through-negative-sampling"><span class="header-section-number">24.3</span> Computational Efficiency Through Negative Sampling</h2>
<p>The elegance of word2vec’s formulation belies a serious computational challenge. Computing the normalization term in the softmax requires summing over the entire vocabulary—potentially millions of terms—for every gradient update. With large corpora containing billions of words, this quickly becomes intractable.</p>
<p>Negative sampling transforms the problem from multi-class classification to binary classification. Instead of predicting which word from the entire vocabulary appears in the context, we ask a simpler question: given a word pair, is it a real center-context pair from the corpus or a randomly generated negative example? The objective becomes:</p>
<p><span class="math display">\[\mathcal{L}_{\text{NS}} = \log \sigma(\mathbf{u}_o^T \mathbf{v}_c) + \sum_{k=1}^K \mathbb{E}_{w_k \sim P_n} \left[\log \sigma(-\mathbf{u}_{w_k}^T \mathbf{v}_c)\right]\]</span></p>
<p>where <span class="math inline">\(\sigma(x) = 1/(1 + e^{-x})\)</span> is the sigmoid function, and <span class="math inline">\(P_n\)</span> is a noise distribution over words. The clever insight is that by carefully choosing the noise distribution—typically <span class="math inline">\(P_n(w) \propto f(w)^{3/4}\)</span> where <span class="math inline">\(f(w)\)</span> is word frequency—we can approximate the original objective while reducing computation from <span class="math inline">\(O(|\mathcal{V}|)\)</span> to <span class="math inline">\(O(K)\)</span>, where <span class="math inline">\(K \ll |\mathcal{V}|\)</span> is a small number of negative samples (typically 5-20).</p>
<p>An alternative solution is hierarchical softmax, which replaces the flat softmax over the vocabulary with a binary tree where each leaf represents a word. The probability of a word is then the product of binary decisions along the path from root to leaf:</p>
<p><span class="math display">\[P(w_o \mid w_c) = \prod_{j=1}^{L(w_o)-1} \sigma\left([\![n(w_o, j+1) = \text{leftChild}(n(w_o, j))]\!] \cdot \mathbf{u}_{n(w_o,j)}^T \mathbf{v}_c\right)\]</span></p>
<p>where <span class="math inline">\(L(w_o)\)</span> is the length of the path to word <span class="math inline">\(w_o\)</span>, <span class="math inline">\(n(w_o, j)\)</span> is the <span class="math inline">\(j\)</span>-th node on this path, and <span class="math inline">\([\![\cdot]\!]\)</span> is the indicator function that returns 1 or -1. This reduces computational complexity from <span class="math inline">\(O(|\mathcal{V}|)\)</span> to <span class="math inline">\(O(\log |\mathcal{V}|)\)</span>, though negative sampling typically performs better in practice.</p>
</section>
<section id="global-vectors-and-matrix-factorization" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="global-vectors-and-matrix-factorization"><span class="header-section-number">24.4</span> Global Vectors and Matrix Factorization</h2>
<p>While word2vec learns from local context windows, GloVe (Global Vectors) leverages global co-occurrence statistics. The key observation is that the ratio of co-occurrence probabilities can encode semantic relationships. Consider the words “ice” and “steam” in relation to “solid” and “gas”: <span class="math inline">\(P(\text{solid} \mid \text{ice}) / P(\text{solid} \mid \text{steam})\)</span> is large (around 8.9), while <span class="math inline">\(P(\text{gas} \mid \text{ice}) / P(\text{gas} \mid \text{steam})\)</span> is small (around 0.085), and <span class="math inline">\(P(\text{water} \mid \text{ice}) / P(\text{water} \mid \text{steam})\)</span> is close to 1 (around 1.36).</p>
<p>These ratios capture the semantic relationships: ice is solid, steam is gas, and both relate to water. GloVe learns embeddings that preserve these ratios through the objective:</p>
<p><span class="math display">\[\mathcal{L}_{\text{GloVe}} = \sum_{i,j} h(X_{ij}) \left(\mathbf{v}_i^T \mathbf{u}_j + b_i + c_j - \log X_{ij}\right)^2\]</span></p>
<p>where <span class="math inline">\(X_{ij}\)</span> counts co-occurrences, <span class="math inline">\(h(\cdot)\)</span> is a weighting function that prevents very common or very rare pairs from dominating, and <span class="math inline">\(b_i, c_j\)</span> are bias terms. A typical choice is <span class="math inline">\(h(x) = (x/x_{\max})^{\alpha}\)</span> if <span class="math inline">\(x &lt; x_{\max}\)</span>, else 1, with <span class="math inline">\(\alpha = 0.75\)</span> and <span class="math inline">\(x_{\max} = 100\)</span>.</p>
<p>This formulation reveals GloVe as a weighted matrix factorization problem. We seek low-rank factors <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> such that <span class="math inline">\(\mathbf{V}^T\mathbf{U} \approx \log \mathbf{X}\)</span>, where the approximation is weighted by the function <span class="math inline">\(h(\cdot)\)</span>. This connection to classical linear algebra provides theoretical insights: the optimal embeddings lie in the subspace spanned by the top singular vectors of an appropriately transformed co-occurrence matrix.</p>
</section>
<section id="beyond-words-subword-and-character-models" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="beyond-words-subword-and-character-models"><span class="header-section-number">24.5</span> Beyond Words: Subword and Character Models</h2>
<p>A fundamental limitation of word-level embeddings is their inability to handle out-of-vocabulary words or capture morphological relationships. The word “unhappiness” shares obvious morphological connections with “happy,” “unhappy,” and “happiness,” but word2vec treats these as completely independent tokens.</p>
<p>FastText addresses this limitation by representing words as bags of character n-grams. For the word “where” with n-grams of length 3 to 6, we extract: “&lt;wh”, “whe”, “her”, “ere”, “re&gt;”, and longer n-grams up to the full word. The word embedding is then the sum of its n-gram embeddings:</p>
<p><span class="math display">\[\mathbf{v}_{\text{where}} = \sum_{g \in \mathcal{G}_{\text{where}}} \mathbf{z}_g\]</span></p>
<p>This approach naturally handles out-of-vocabulary words by breaking them into known n-grams and provides better representations for rare words by sharing parameters across morphologically related words.</p>
<p>An even more flexible approach is Byte Pair Encoding (BPE), which learns a vocabulary of subword units directly from the data. Starting with individual characters, BPE iteratively merges the most frequent pair of adjacent units until reaching a desired vocabulary size. For example, given a corpus with word frequencies {“fast”: 4, “faster”: 3, “tall”: 5, “taller”: 4}, BPE might learn merges like “t” + “a” → “ta”, then “ta” + “l” → “tal”, and so on. This data-driven approach balances vocabulary size with representation power, enabling models to handle arbitrary text while maintaining reasonable computational requirements.</p>
</section>
<section id="contextual-representations-and-the-transformer-architecture" class="level2" data-number="24.6">
<h2 data-number="24.6" class="anchored" data-anchor-id="contextual-representations-and-the-transformer-architecture"><span class="header-section-number">24.6</span> Contextual Representations and the Transformer Architecture</h2>
<p>Static word embeddings suffer from a fundamental limitation: they assign a single vector to each word, ignoring context. The word “bank” receives the same representation whether it appears in “river bank” or “investment bank.” This conflation of multiple senses into a single vector creates an information bottleneck that limits performance on downstream tasks.</p>
<p>Early approaches to contextual embeddings used recurrent neural networks. Models like ELMo (Embeddings from Language Models) employ bidirectional LSTMs to encode context, computing forward and backward hidden states that are concatenated to form context-sensitive representations. ELMo further combines representations from multiple layers, allowing downstream tasks to mix different levels of abstraction through learned weights.</p>
<p>The transformer architecture revolutionized contextual representations by replacing recurrence with self-attention. For a sequence with embeddings <span class="math inline">\(\mathbf{H} = [\mathbf{h}_1, \ldots, \mathbf{h}_n]^T\)</span>, self-attention computes:</p>
<p><span class="math display">\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}\]</span></p>
<p>where <span class="math inline">\(\mathbf{Q} = \mathbf{H}\mathbf{W}_Q\)</span>, <span class="math inline">\(\mathbf{K} = \mathbf{H}\mathbf{W}_K\)</span>, and <span class="math inline">\(\mathbf{V} = \mathbf{H}\mathbf{W}_V\)</span> are learned linear projections. This mechanism allows each position to attend to all other positions, with attention weights determined by the scaled dot product between queries and keys.</p>
<p>From a kernel method perspective, self-attention implements a form of kernel smoothing where each position’s representation is a weighted combination of all positions. As discussed in Chapter 19, this connects to Nadaraya-Watson kernel regression, with the key difference that the kernel is both learned and input-dependent.</p>
<p>Multi-head attention extends this by computing multiple attention functions in parallel, allowing the model to capture different types of relationships simultaneously. The complete transformer block combines multi-head attention with position-wise feedforward networks and residual connections:</p>
<p><span class="math display">\[\mathbf{H}' = \text{LayerNorm}(\mathbf{H} + \text{MultiHead}(\mathbf{H}))\]</span> <span class="math display">\[\mathbf{H}'' = \text{LayerNorm}(\mathbf{H}' + \text{FFN}(\mathbf{H}'))\]</span></p>
<p>where the feedforward network typically has a hidden dimension 4 times the model dimension.</p>
</section>
<section id="pretraining-at-scale-bert-and-beyond" class="level2" data-number="24.7">
<h2 data-number="24.7" class="anchored" data-anchor-id="pretraining-at-scale-bert-and-beyond"><span class="header-section-number">24.7</span> Pretraining at Scale: BERT and Beyond</h2>
<p>The availability of powerful architectures raised a crucial question: how can we best leverage unlabeled text to learn general-purpose representations? BERT (Bidirectional Encoder Representations from Transformers) introduced a pretraining framework that has become the foundation for modern NLP.</p>
<p>BERT’s key innovation was masked language modeling (MLM), where 15% of tokens are selected for prediction. For each selected token, 80% are replaced with [MASK], 10% with random tokens, and 10% left unchanged. This prevents the model from simply learning to copy tokens when they’re not masked. The loss function only considers predictions for masked positions:</p>
<p><span class="math display">\[\mathcal{L}_{\text{MLM}} = -\sum_{m \in \mathcal{M}} \log P(x_m \mid \mathbf{x}_{\backslash \mathcal{M}})\]</span></p>
<p>BERT combines MLM with next sentence prediction (NSP), which trains the model to understand relationships between sentence pairs. Training examples contain 50% consecutive sentences and 50% randomly paired sentences. The input representation concatenates both sentences with special tokens and segment embeddings to distinguish between them.</p>
<p>The scale of BERT pretraining represents a quantum leap from earlier approaches. The original BERT models were trained on a combination of BookCorpus (800 million words from over 11,000 books) and English Wikipedia (2,500 million words). This massive dataset enables the model to see diverse writing styles, topics, and linguistic phenomena. The preprocessing pipeline removes duplicate paragraphs, filters very short or very long sentences, and maintains document boundaries to ensure coherent sentence pairs for NSP.</p>
<section id="bert-architecture-and-training-details" class="level3">
<h3 class="anchored" data-anchor-id="bert-architecture-and-training-details">BERT Architecture and Training Details</h3>
<p>To understand BERT’s architectural significance, it’s helpful to compare it with its predecessors ELMo and GPT, which represent different approaches to contextual representation learning:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph col1 [" "]
        subgraph ELMo ["ELMo"]
            direction TB
            E1["BiLSTM&lt;br/&gt;Layer 1"]
            E2["BiLSTM&lt;br/&gt;Layer 2"]
            E3["Task-specific&lt;br/&gt;Architecture"]
            E4["Output"]
            
            E1 --&gt; E2
            E2 --&gt; E3
            E3 --&gt; E4
        end
        
        style E1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style E2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style E3 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style E4 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    subgraph col2 [" "]
        subgraph GPT ["GPT"]
            direction TB
            G1["Transformer&lt;br/&gt;Layer 1"]
            G2["Transformer&lt;br/&gt;Layer 2"]
            G3["Transformer&lt;br/&gt;Layer 3"]
            G4["Linear&lt;br/&gt;Output"]
            G5["Output"]
            
            G1 --&gt; G2
            G2 --&gt; G3
            G3 --&gt; G4
            G4 --&gt; G5
        end
        
        style G1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style G4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style G5 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    subgraph col3 [" "]
        subgraph BERT ["BERT"]
            direction TB
            B1["Transformer&lt;br/&gt;Encoder 1"]
            B2["Transformer&lt;br/&gt;Encoder 2"]
            B3["Transformer&lt;br/&gt;Encoder 3"]
            B4["Linear&lt;br/&gt;Output"]
            B5["Output"]
            
            B1 --&gt; B2
            B2 --&gt; B3
            B3 --&gt; B4
            B4 --&gt; B5
        end
        
        style B1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B2 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B3 fill:#ffcccc,stroke:#cc0000,stroke-width:2px
        style B4 fill:#ccffcc,stroke:#00cc00,stroke-width:2px
        style B5 fill:#ccccff,stroke:#0000cc,stroke-width:2px
    end
    
    style col1 fill:none,stroke:none
    style col2 fill:none,stroke:none
    style col3 fill:none,stroke:none
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><strong>ELMo</strong> uses bidirectional LSTMs with task-specific architectures, requiring custom model design for each application. <strong>GPT</strong> employs a unidirectional Transformer decoder that processes text left-to-right, making it task-agnostic but unable to see future context. <strong>BERT</strong> combines the best of both: bidirectional context understanding through Transformer encoders with minimal task-specific modifications (just an output layer).</p>
<p>BERT comes in two main configurations that balance model capacity with computational requirements:</p>
<p><strong>BERT-Base</strong>: 12 transformer layers, 768 hidden dimensions, 12 attention heads, 110M parameters <strong>BERT-Large</strong>: 24 transformer layers, 1024 hidden dimensions, 16 attention heads, 340M parameters</p>
<p>Both models use a vocabulary of 30,000 WordPiece tokens, learned using a data-driven tokenization algorithm similar to BPE. The maximum sequence length is 512 tokens, though most pretraining uses sequences of 128 tokens to improve efficiency, with only the final 10% of training using full-length sequences.</p>
<p>The pretraining procedure involves several sophisticated techniques:</p>
<p><strong>Warm-up Learning Rate</strong>: The learning rate increases linearly for the first 10,000 steps to <span class="math inline">\(10^{-4}\)</span>, then decreases linearly. This warm-up prevents large gradients early in training when the model is randomly initialized.</p>
<p><strong>Gradient Accumulation</strong>: To simulate larger batch sizes on limited hardware, gradients are accumulated over multiple forward passes before updating weights. BERT uses an effective batch size of 256 sequences.</p>
<p><strong>Mixed Precision Training</strong>: Using 16-bit floating point for most computations while maintaining 32-bit master weights speeds up training significantly on modern GPUs.</p>
<p>The computational requirements are substantial. BERT-Large required: - 16 TPU v3 chips (or 64 V100 GPUs) - 4 days of continuous training - Approximately 1 million update steps - An estimated cost of $7,000-$50,000 depending on hardware</p>
</section>
<section id="data-preparation-for-bert-pretraining" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-for-bert-pretraining">Data Preparation for BERT Pretraining</h3>
<p>The data preparation pipeline for BERT is surprisingly complex. Starting with raw text, the process involves:</p>
<ol type="1">
<li><p><strong>Document Segmentation</strong>: Text is split into documents, maintaining natural boundaries. For books, this means chapter boundaries; for Wikipedia, article boundaries.</p></li>
<li><p><strong>Sentence Segmentation</strong>: Documents are split into sentences using heuristic rules (periods followed by whitespace and capital letters, with exceptions for abbreviations).</p></li>
<li><p><strong>WordPiece Tokenization</strong>: Text is tokenized using the learned WordPiece vocabulary. Unknown words are broken into known subwords, with a special handling for the beginning of words.</p></li>
<li><p><strong>Creating Training Examples</strong>: For each sequence, we:</p>
<ul>
<li>Sample a target sequence length from a geometric distribution (to vary lengths)</li>
<li>Pack sentences until reaching the target length</li>
<li>For NSP, 50% of the time select the actual next segment, 50% a random segment</li>
<li>Apply MLM masking to 15% of tokens with the 80/10/10 strategy</li>
<li>Add special tokens: [CLS] at the beginning, [SEP] between segments</li>
<li>Create segment embeddings (0 for first segment, 1 for second)</li>
<li>Pad to a fixed length with [PAD] tokens</li>
</ul></li>
<li><p><strong>Creating TFRecord Files</strong>: Examples are serialized into TFRecord format for efficient I/O during training, with examples grouped by sequence length to minimize padding.</p></li>
</ol>
</section>
<section id="beyond-bert-improvements-and-variants" class="level3">
<h3 class="anchored" data-anchor-id="beyond-bert-improvements-and-variants">Beyond BERT: Improvements and Variants</h3>
<p>The success of BERT sparked numerous improvements and variants:</p>
<p><strong>RoBERTa</strong> (Robustly Optimized BERT Approach) showed that BERT was significantly undertrained. By training longer (1000K steps vs 1M), with bigger batches (8K vs 256), on more data (160GB vs 16GB), and removing NSP, RoBERTa achieved substantial improvements. The key insight was that with sufficient data, the next sentence prediction task was unnecessary and potentially harmful.</p>
<p><strong>ALBERT</strong> (A Lite BERT) introduced parameter sharing across layers and factorized embedding parameterization to create more parameter-efficient models. By sharing all parameters between transformer layers, ALBERT reduces memory consumption and speeds up training while maintaining performance.</p>
<p><strong>ELECTRA</strong> replaced MLM with a more sample-efficient replaced token detection task. Instead of masking tokens, ELECTRA corrupts input text by replacing some tokens with plausible alternatives from a small generator network, then trains the model to distinguish real from replaced tokens. This allows the model to learn from all input tokens, not just the 15% that are masked.</p>
<p>The pretraining paradigm established by BERT has become the foundation for even larger models. GPT-3 with 175 billion parameters, T5 exploring different pretraining objectives, and multimodal models like CLIP extending these ideas beyond text all build on BERT’s insights about self-supervised pretraining at scale.</p>
</section>
<section id="practical-considerations-for-pretraining" class="level3">
<h3 class="anchored" data-anchor-id="practical-considerations-for-pretraining">Practical Considerations for Pretraining</h3>
<p>For researchers and practitioners looking to pretrain their own models, several practical considerations are crucial:</p>
<p><strong>Data Quality</strong>: The quality of pretrained representations depends heavily on the training data. Domain-specific corpora (e.g., biomedical text, legal documents) can produce better representations for specialized applications than general-purpose models.</p>
<p><strong>Computational Resources</strong>: Full-scale pretraining remains expensive, but several strategies can help: - Start from an existing pretrained model and continue pretraining on domain-specific data - Use smaller model configurations (e.g., 6 layers instead of 12) - Leverage techniques like knowledge distillation to create efficient models</p>
<p><strong>Pretraining Objectives</strong>: While MLM and NSP work well, other objectives may be better for specific applications: - Span-based masking (masking contiguous sequences) for extractive tasks - Sentence order prediction for discourse-heavy applications - Denoising objectives for robustness to noisy input</p>
<p>The democratization of pretraining through libraries like Hugging Face Transformers has made it possible for smaller organizations to leverage these powerful techniques, either by fine-tuning existing models or pretraining specialized models for their domains.</p>
</section>
</section>
<section id="transfer-learning-and-downstream-applications" class="level2" data-number="24.8">
<h2 data-number="24.8" class="anchored" data-anchor-id="transfer-learning-and-downstream-applications"><span class="header-section-number">24.8</span> Transfer Learning and Downstream Applications</h2>
<p>The power of pretrained models lies in their transferability. For sentiment analysis, we add a linear layer on top of the [CLS] token representation and fine-tune on labeled data. Popular datasets include IMDb movie reviews (50K examples) and Stanford Sentiment Treebank (11,855 sentences). Fine-tuning typically requires only 2-4 epochs, demonstrating the effectiveness of transfer learning.</p>
<p>Natural language inference (NLI) determines logical relationships between premise and hypothesis sentences. The Stanford Natural Language Inference corpus contains 570,000 sentence pairs labeled as entailment, contradiction, or neutral. For BERT-based NLI, we concatenate premise and hypothesis with [SEP] tokens and classify using the [CLS] representation.</p>
<p>Token-level tasks like named entity recognition classify each token independently. Common datasets include CoNLL-2003 (English and German entities) and OntoNotes 5.0 (18 entity types). The BIO tagging scheme marks entity boundaries: B-PER for beginning of person names, I-PER for inside, and O for outside any entity.</p>
<p>Question answering presents unique challenges. The SQuAD dataset contains 100,000+ questions where answers are text spans from Wikipedia articles. BERT approaches this by predicting start and end positions independently, with the final answer span selected to maximize the product of start and end probabilities subject to length constraints.</p>
</section>
<section id="model-compression-and-efficiency" class="level2" data-number="24.9">
<h2 data-number="24.9" class="anchored" data-anchor-id="model-compression-and-efficiency"><span class="header-section-number">24.9</span> Model Compression and Efficiency</h2>
<p>While large pretrained models achieve impressive performance, their computational requirements limit deployment. Knowledge distillation trains a small “student” model to mimic a large “teacher” model through a combined loss:</p>
<p><span class="math display">\[\mathcal{L}_{\text{distill}} = \alpha \mathcal{L}_{\text{task}} + (1-\alpha) \text{KL}(p_{\text{teacher}} \| p_{\text{student}})\]</span></p>
<p>DistilBERT achieves 97% of BERT’s performance with 40% fewer parameters and 60% faster inference. Quantization reduces numerical precision from 32-bit to 8-bit or even lower, while pruning removes connections below a magnitude threshold. These techniques can reduce model size by an order of magnitude with minimal performance degradation.</p>
</section>
<section id="theoretical-perspectives-and-future-directions" class="level2" data-number="24.10">
<h2 data-number="24.10" class="anchored" data-anchor-id="theoretical-perspectives-and-future-directions"><span class="header-section-number">24.10</span> Theoretical Perspectives and Future Directions</h2>
<p>The success of language models connects to several theoretical frameworks. Transformers are universal approximators for sequence-to-sequence functions—given sufficient capacity, they can approximate any continuous function to arbitrary precision. The self-attention mechanism provides an inductive bias well-suited to capturing long-range dependencies.</p>
<p>Despite having hundreds of millions of parameters, these models generalize remarkably well. This connects to implicit regularization in overparameterized models, where gradient descent dynamics bias toward solutions with good generalization properties. Language models automatically learn hierarchical features: early layers capture syntax and morphology, middle layers semantic relationships, and later layers task-specific abstractions.</p>
<p>Yet significant challenges remain. Models struggle with compositional generalization—understanding “red car” and “blue house” doesn’t guarantee understanding “red house” if that combination is rare in training. Sample efficiency remains poor compared to human learning. A child masters basic grammar from thousands of examples; BERT sees billions. This gap suggests fundamental differences in learning mechanisms.</p>
<p>Interpretability poses ongoing challenges. While attention visualizations provide some insights, we lack principled methods for understanding distributed representations across hundreds of layers and attention heads. Future directions include multimodal understanding (integrating text with vision and speech), more efficient architectures that maintain performance while reducing computational requirements, and developing theoretical frameworks to predict and understand model behavior.</p>
</section>
<section id="natural-language-processing-applications" class="level2" data-number="24.11">
<h2 data-number="24.11" class="anchored" data-anchor-id="natural-language-processing-applications"><span class="header-section-number">24.11</span> Natural Language Processing: Applications</h2>
<p>Having established the theoretical foundations and pretraining methodologies for natural language understanding, we now turn to the practical application of these techniques to solve real-world problems. The power of pretrained representations lies not merely in their mathematical elegance, but in their ability to transfer learned linguistic knowledge to diverse downstream tasks with minimal architectural modifications.</p>
<p>The landscape of NLP applications can be broadly categorized into two fundamental types based on their input structure and prediction granularity. <strong>Sequence-level tasks</strong> operate on entire text sequences, producing a single output per input sequence or sequence pair. These include sentiment classification, where we determine the emotional polarity of a review, and natural language inference, where we assess logical relationships between premise-hypothesis pairs. <strong>Token-level tasks</strong> make predictions for individual tokens within sequences, such as named entity recognition that identifies person names, locations, and organizations, or question answering that pinpoints answer spans within passages.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph "Pretrained Representations" ["Pretrained Text Representations"]
        P1[Word2Vec/GloVe] 
        P2[BERT/RoBERTa]
        P3[GPT/T5]
    end
    
    subgraph "Deep Learning Architectures" ["Model Architectures"]
        A1[MLPs]
        A2[CNNs] 
        A3[RNNs]
        A4[Transformers]
        A5[Attention Mechanisms]
    end
    
    subgraph "Sequence-Level Tasks" ["Sequence-Level Applications"]
        S1[Sentiment Analysis]
        S2[Text Classification]
        S3[Natural Language Inference]
        S4[Semantic Similarity]
    end
    
    subgraph "Token-Level Tasks" ["Token-Level Applications"]
        T1[Named Entity Recognition]
        T2[Part-of-Speech Tagging]
        T3[Question Answering]
        T4[Text Summarization]
    end
    
    P1 --&gt; A1
    P1 --&gt; A2
    P1 --&gt; A3
    P2 --&gt; A4
    P2 --&gt; A5
    P3 --&gt; A4
    
    A1 --&gt; S1
    A1 --&gt; S3
    A2 --&gt; S1
    A2 --&gt; S2
    A3 --&gt; S1
    A3 --&gt; S3
    A4 --&gt; S2
    A4 --&gt; S3
    A4 --&gt; T1
    A4 --&gt; T3
    A5 --&gt; S3
    A5 --&gt; T3
    
    style P1 fill:#e8f4f8
    style P2 fill:#e8f4f8
    style P3 fill:#e8f4f8
    style A1 fill:#f0f8e8
    style A2 fill:#f0f8e8
    style A3 fill:#f0f8e8
    style A4 fill:#f0f8e8
    style A5 fill:#f0f8e8
    style S1 fill:#fdf2e8
    style S2 fill:#fdf2e8
    style S3 fill:#fdf2e8
    style S4 fill:#fdf2e8
    style T1 fill:#f8e8f4
    style T2 fill:#f8e8f4
    style T3 fill:#f8e8f4
    style T4 fill:#f8e8f4
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>This architectural flexibility represents one of the most significant advantages of the representation learning paradigm. Unlike earlier rule-based or feature-engineering approaches that required domain-specific expertise for each task, modern NLP systems can leverage the same pretrained representations across vastly different applications. A BERT model pretrained on general text can be fine-tuned for medical document classification, legal contract analysis, or social media sentiment detection with only task-specific output layers.</p>
<section id="sentiment-analysis-from-opinions-to-insights" class="level3">
<h3 class="anchored" data-anchor-id="sentiment-analysis-from-opinions-to-insights">Sentiment Analysis: From Opinions to Insights</h3>
<p>Sentiment analysis exemplifies the practical value of learned text representations. Consider the challenge of processing customer reviews for a major e-commerce platform. Traditional approaches might rely on hand-crafted lexicons of positive and negative words, but such methods fail to capture context-dependent sentiment. The phrase “not bad” expresses mild approval despite containing the negative word “bad,” while “insanely good” uses typically negative intensity (“insanely”) to convey strong positive sentiment.</p>
<p>Modern sentiment analysis systems leverage pretrained embeddings that capture these nuanced semantic relationships. A convolutional neural network architecture can effectively identify local sentiment-bearing phrases through learned filters, while recurrent networks model the sequential dependencies that determine how sentiment evolves throughout a text. The Stanford Sentiment Treebank, with its fine-grained annotations ranging from very negative to very positive, provides a testing ground where contemporary models achieve accuracy levels approaching human inter-annotator agreement.</p>
</section>
<section id="natural-language-inference-reasoning-about-meaning" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-inference-reasoning-about-meaning">Natural Language Inference: Reasoning About Meaning</h3>
<p>Natural language inference represents a more sophisticated challenge that requires understanding logical relationships between text pairs. Given a premise “The company’s profits increased by 15% this quarter” and a hypothesis “The business is performing well,” a model must determine whether the hypothesis follows from the premise (entailment), contradicts it, or remains neutral.</p>
<p>The Stanford Natural Language Inference corpus provides over 570,000 such premise-hypothesis pairs, enabling large-scale training of inference models. Successful approaches often employ attention mechanisms to align relevant portions of premises with hypotheses, identifying which words and phrases support or contradict the proposed logical relationship. The decomposable attention model, for instance, computes element-wise attention between premise and hypothesis tokens, allowing fine-grained comparison of semantic content.</p>
</section>
<section id="token-level-applications-precision-at-the-word-level" class="level3">
<h3 class="anchored" data-anchor-id="token-level-applications-precision-at-the-word-level">Token-Level Applications: Precision at the Word Level</h3>
<p>Token-level tasks require models to make predictions for individual words or subwords within sequences. Named entity recognition illustrates this paradigm: given the sentence “Apple Inc.&nbsp;was founded by Steve Jobs in Cupertino,” a model must identify “Apple Inc.” as an organization, “Steve Jobs” as a person, and “Cupertino” as a location.</p>
<p>The BIO (Begin-Inside-Outside) tagging scheme provides a standard framework for such tasks. The word “Apple” receives the tag “B-ORG” (beginning of organization), “Inc.” gets “I-ORG” (inside organization), “Steve” becomes “B-PER” (beginning of person), and so forth. This encoding allows models to handle multi-word entities while maintaining the token-level prediction structure.</p>
<p>Question answering presents another compelling token-level application. In extractive question answering, models must identify spans within passages that answer given questions. The SQuAD dataset poses questions about Wikipedia articles, requiring models to pinpoint exact text spans as answers. For example, given the passage about Apple Inc.&nbsp;and the question “Who founded Apple?”, the model should identify “Steve Jobs” as the answer span.</p>
</section>
<section id="fine-tuning-pretrained-models" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-pretrained-models">Fine-Tuning Pretrained Models</h3>
<p>The emergence of large-scale pretrained models like BERT has revolutionized the application landscape. Rather than training task-specific models from scratch, practitioners can fine-tune pretrained representations on downstream tasks with remarkable efficiency. This approach typically requires only 2-4 epochs of training on task-specific data, compared to the hundreds of epochs needed for training from random initialization.</p>
<p>The fine-tuning process involves freezing most pretrained parameters while allowing task-specific layers to adapt. For sequence classification, this might involve adding a single linear layer on top of BERT’s [CLS] token representation. For token-level tasks like named entity recognition, each token’s representation feeds through the same classification layer to produce per-token predictions.</p>
<p>This transfer learning paradigm has democratized access to state-of-the-art NLP capabilities. Organizations without massive computational resources can leverage pretrained models fine-tuned on their specific domains, achieving performance that would have required substantial research and development investments just a few years ago.</p>
</section>
<section id="challenges-and-future-directions" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-future-directions">Challenges and Future Directions</h3>
<p>Despite remarkable progress, significant challenges remain in NLP applications. Models often exhibit brittleness when faced with adversarial examples or distribution shifts between training and deployment data. A sentiment classifier trained on movie reviews might fail on product reviews due to domain-specific language patterns. Similarly, named entity recognition systems trained on news articles may struggle with social media text due to informal language and unconventional capitalization.</p>
<p>Computational efficiency presents another ongoing concern. While large pretrained models achieve impressive performance, their size and inference requirements limit deployment in resource-constrained environments. Knowledge distillation and model pruning techniques offer promising approaches for creating smaller, faster models that retain much of the original performance.</p>
<p>The interpretability challenge looms large as models become more complex. Understanding why a model makes specific predictions becomes crucial for high-stakes applications like medical diagnosis or legal document analysis. Attention visualizations provide some insight, but we lack comprehensive frameworks for understanding decision-making processes in large neural networks.</p>
<p>Future directions point toward multimodal understanding that integrates text with visual and auditory information, few-shot learning that adapts quickly to new domains with minimal examples, and more robust architectures that maintain performance across diverse linguistic contexts. The field continues evolving rapidly, driven by the interplay between theoretical advances in representation learning and practical demands from real-world applications.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="24.12">
<h2 data-number="24.12" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">24.12</span> Conclusion</h2>
<p>The journey from symbolic manipulation to neural language understanding represents one of the great success stories of modern artificial intelligence. By reconceptualizing language as geometry in high-dimensional spaces, leveraging self-supervision at scale, and developing powerful architectural innovations like transformers, the field has achieved capabilities that seemed like science fiction just a decade ago.</p>
<p>The mathematical frameworks developed—from distributional semantics to attention mechanisms—provide not just engineering tools but lenses through which to examine fundamental questions about meaning and understanding. As these systems become more capable and widely deployed, understanding their theoretical foundations, practical limitations, and societal implications becomes ever more critical.</p>
<p>The techniques discussed in this chapter—word embeddings, contextual representations, pretraining, and fine-tuning—form the foundation of modern NLP systems. Yet despite impressive engineering achievements, we’ve only begun to scratch the surface of true language understanding. The rapid progress offers both tremendous opportunities and sobering responsibilities. The most exciting chapters in this story are yet to be written.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./23-cnn.html" class="pagination-link" aria-label="Convolutional Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./25-llm.html" class="pagination-link" aria-label="Large Language Models: A Revolution in AI">
        <span class="nav-page-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>