<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Tree Models – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./15-forecasting.html" rel="next">
<link href="./13-logistic.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/chess.jpg">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/chess.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#building-a-tree-via-recursive-binary-splitting" id="toc-building-a-tree-via-recursive-binary-splitting" class="nav-link active" data-scroll-target="#building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</a></li>
  <li><a href="#pruning-taming-an-overfit-tree" id="toc-pruning-taming-an-overfit-tree" class="nav-link" data-scroll-target="#pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</a></li>
  <li><a href="#classification-trees" id="toc-classification-trees" class="nav-link" data-scroll-target="#classification-trees"><span class="header-section-number">14.3</span> Classification Trees</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="header-section-number">14.4</span> Random Forest</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="header-section-number">14.5</span> Boosting</a></li>
  <li><a href="#finding-good-bayes-predictors" id="toc-finding-good-bayes-predictors" class="nav-link" data-scroll-target="#finding-good-bayes-predictors"><span class="header-section-number">14.6</span> Finding Good Bayes Predictors</a></li>
  <li><a href="#ensemble-averaging-and-1n-rule" id="toc-ensemble-averaging-and-1n-rule" class="nav-link" data-scroll-target="#ensemble-averaging-and-1n-rule"><span class="header-section-number">14.7</span> Ensemble Averaging and <span class="math inline">\(1/N\)</span> Rule</a>
  <ul class="collapse">
  <li><a href="#classification-variance-decomposition" id="toc-classification-variance-decomposition" class="nav-link" data-scroll-target="#classification-variance-decomposition">Classification variance decomposition</a></li>
  <li><a href="#conditional-and-unconditional-dependence" id="toc-conditional-and-unconditional-dependence" class="nav-link" data-scroll-target="#conditional-and-unconditional-dependence">Conditional and unconditional dependence</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of <span class="math inline">\(y\)</span> to use for a given <span class="math inline">\(x\)</span>. Similar to a decision tree, a predictive tree model is a nested sequence of if-else statements that map any input data point <span class="math inline">\(x\)</span> to a predicted output <span class="math inline">\(y\)</span>. Each if-else statement checks a feature of <span class="math inline">\(x\)</span> and sends the data left or right along the tree branch. At the end of the branch, a single value of <span class="math inline">\(y\)</span> is predicted.</p>
<p><a href="#fig-chesstree" class="quarto-xref">Figure&nbsp;<span>14.1</span></a> shows a decision tree for predicting a chess piece given a four-dimensional input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.</p>
<div id="fig-chesstree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chess.jpg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Elementary tree scheme; visualization of the splitting process
</figcaption>
</figure>
</div>
<p>The prediction algorithm is simple. Start at the root node and move down the tree until you reach a leaf node. The process of building a tree, given a set of training data, is more complicated and has three main components:</p>
<ol type="1">
<li><strong>Splitting</strong>. The process of dividing the training data into subsets based on the value of a single feature. The goal is to create subsets that are as homogeneous as possible. The subsets are then used to create the nodes of the tree.</li>
<li><strong>Stopping</strong>. The process of deciding when to stop splitting. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
<li><strong>Pruning</strong>. The process of removing nodes from the tree that do not improve the accuracy of the tree. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
</ol>
<p>The splitting process is the most important part of the tree-building process. At each step the splitting process needs to decide on the feature index <span class="math inline">\(j\)</span> to be used for splitting and the location of the split. For a binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.</p>
<p>Imagine you’re a jewelry appraiser tasked with determining a diamond’s value. You might follow a series of questions: Is the carat weight above 1.0? If yes, is the clarity VS1 or better? Each question leads to another, creating a decision path that eventually arrives at a price estimate. This is precisely how decision trees work—they mirror our natural decision-making process by creating a flowchart of if-then rules.</p>
<p>Below we’ll explore tree-based models using the classic diamonds dataset, which contains prices and attributes for 53,940 diamonds. We’ll start with simple decision trees, progress to ensemble methods like random forests and gradient boosting, and develop deep insights into how these algorithms work, when to use them, and how to avoid common pitfalls.</p>
<p>Let’s start with a quick demo and look at the data, which has 10 variables</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 40%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>carat</code></td>
<td>Weight of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>cut</code></td>
<td>Quality of the cut</td>
<td>Fair, Good, Very Good, Premium, Ideal</td>
</tr>
<tr class="odd">
<td><code>color</code></td>
<td>Color of the diamond</td>
<td>J, I, H, G, F, E, D</td>
</tr>
<tr class="even">
<td><code>clarity</code></td>
<td>Clarity of the diamond</td>
<td>I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF</td>
</tr>
<tr class="odd">
<td><code>depth</code></td>
<td>Depth of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>table</code></td>
<td>Width of the diamond’s table</td>
<td>Numeric</td>
</tr>
</tbody>
</table>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamonds)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(diamonds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">carat</th>
<th style="text-align: left;">cut</th>
<th style="text-align: left;">color</th>
<th style="text-align: left;">clarity</th>
<th style="text-align: right;">depth</th>
<th style="text-align: right;">table</th>
<th style="text-align: right;">price</th>
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Ideal</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.21</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI1</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">VS1</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">327</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.29</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">I</td>
<td style="text-align: left;">VS2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">334</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">2.6</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.31</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">335</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">2.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.24</td>
<td style="text-align: left;">Very Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">VVS2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Let’s plot price vs carat.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Notice the strong non-linear relationship between carat and price. This suggests that log-transformations might help make the relationship linear.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log-transformed price for better linear relationships</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>diamonds <span class="ot">&lt;-</span> diamonds <span class="sc">%&gt;%</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">mutate</span>(<span class="at">log_price =</span> <span class="fu">log</span>(price),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a> <span class="at">log_carat =</span> <span class="fu">log</span>(carat))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine the linearized relationship</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> log_carat, <span class="at">y =</span> log_price)) <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">color =</span> <span class="st">"darkblue"</span>) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Log-transformed Price vs Carat Shows Linear Relationship"</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> <span class="st">"Log(Carat)"</span>, <span class="at">y =</span> <span class="st">"Log(Price)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>However, as we will see later tree models are not very sensitive to the linearity of the relationship between the predictors and the response. In general, we do not need to transform the variables.</p>
<p>Although carat is the most important factor in determining the price of a diamond, it is not the only factor. We can see that there is a lot of variability in the price of diamonds with the same carat.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use density plot to compare price for different clarity levels</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_density</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Let’s start with a simple decision tree using just two predictors to visualize how trees partition the feature space:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(price <span class="sc">~</span> carat <span class="sc">+</span> clarity, <span class="at">data =</span> diamonds)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The decision tree plot shows how the algorithm partitions the feature space based on carat and clarity to predict diamond prices. The tree structure reveals several interesting patterns:</p>
<ol type="1">
<li><p>Primary split on carat: The tree first splits on carat at 1.05, indicating this is the most important predictor for price. This makes intuitive sense since carat weight is typically the strongest determinant of diamond value.</p></li>
<li><p>Secondary splits on clarity: After the carat split, the tree further partitions based on clarity levels. This shows that while carat is primary, clarity still provides important predictive value for price.</p></li>
<li><p>Interpretability: Each terminal node (leaf) shows the predicted price for that region. For example, diamonds with carat &lt; 1.05 and clarity in the lower categories (I1, SI2, SI1) have an average predicted price of $2,847.</p></li>
<li><p>Feature interactions: The tree reveals how carat and clarity interact - the effect of clarity on price depends on the carat weight, which is captured through the hierarchical splitting structure.</p></li>
</ol>
<p>This simple two-predictor tree demonstrates the key advantages of decision trees: they can handle non-linear relationships, provide interpretable rules, and naturally capture feature interactions without requiring explicit specification of interaction terms.</p>
<p>Let’s plot the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x=</span>carat, <span class="at">y=</span>clarity, <span class="at">colour=</span>price)) <span class="sc">+</span> </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low=</span><span class="st">"blue"</span>, <span class="at">high=</span><span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that for small and large diamonds, the price is consistently low and does not depend much on the clarity. However, at around 1 carat, we see some overlap in the price for different clarity levels. Clarity becomes important at this level</p>
<p>Now let’s plot the data with the tree regions.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PLot the rectangle areas that represent the regions of the tree</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a> <span class="at">carat =</span> <span class="fu">seq</span>(<span class="fu">min</span>(diamonds<span class="sc">$</span>carat), <span class="fu">max</span>(diamonds<span class="sc">$</span>carat), <span class="at">length.out =</span> <span class="dv">50</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a> <span class="at">clarity =</span> diamonds<span class="sc">$</span>clarity</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>pred_data<span class="sc">$</span>pred_price <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, pred_data)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot regions</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(pred_data, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> clarity, <span class="at">fill =</span> pred_price)) <span class="sc">+</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">"blue"</span>, <span class="at">high =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Tree Regions: Carat vs Clarity"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The plot above shows the decision tree’s prediction regions as colored tiles, where each tile represents a specific combination of carat and clarity values. The color gradient from blue to red indicates the predicted price, with darker red representing higher predicted prices.</p>
<p>Looking at this visualization, we can see several key patterns. The strongest predictor is clearly carat, as evidenced by the vertical bands of similar colors. As carat increases (moving right on the x-axis), the predicted prices generally increase (colors shift from blue to red). The tree captures non-linear patterns that a simple linear model would miss. For example, the rate of price increase with carat is not uniform across all clarity levels. Unlike smooth regression surfaces, the tree creates distinct rectangular regions with sharp boundaries, reflecting the binary splitting nature of decision trees.</p>
<p>The prediction is rather straightforward. The tree divides the predictor space-that is, the set of possible values for <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> - into <span class="math inline">\(J\)</span> distinct and non-overlapping boxes, <span class="math inline">\(R_1,R_2,...,R_J\)</span>. For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>.</p>
<p><span class="math display">\[
f(x) = \bar y_j, \text{ for } x \in R_j, \text{ where } \bar y_j = \text{Average}(y_i \mid x_i \in R_j)
\]</span></p>
<section id="building-a-tree-via-recursive-binary-splitting" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</h2>
<p>The overall goal of building a tree is to find regions that lead to minima of the total Residual Sum of Squares (RSS) <span class="math display">\[
\mathrm{RSS} = \sum_{j=1}^J\sum_{i \in R_j}(y_i - \bar{y}_j)^2 \rightarrow \mathrm{minimize}
\]</span></p>
<p>Unfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes. We can find a good approximate solution, using top-down approach (the CART algorithm).</p>
<p>It begins with the entire dataset at the “root” node and repeatedly splits the data into two “child” nodes. This process continues recursively on each new node, with the goal of making the resulting groups (nodes) as homogeneous as possible with respect to the target variable, price. At each iteration we decide on which variable <span class="math inline">\(j\)</span> to split and the split point <span class="math inline">\(s\)</span>. <span class="math display">\[
R_1(j, s) = \{x\mid x_j &lt; s\} \mbox{ and } R_2(j, s) = \{x\mid x_j \ge s\},
\]</span> thus, we seek to minimize (in case of regression tree) <span class="math display">\[
\min_{j,s}\left[ \sum_{i:x_i\in R_1}(y_i - \bar{y}_1)^2 + \sum_{i:x_i \in R_2}(y_i - \bar{y}_2)^2\right]
\]</span> As a result, every observed input point belongs to a single region.</p>
</section>
<section id="pruning-taming-an-overfit-tree" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</h2>
<p>Now let’s discuss how many regions we should have. At one extreme end, we can have <span class="math inline">\(n\)</span> regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed <span class="math inline">\(y\)</span>’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1,...,R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias.</p>
<p>How do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree <span class="math inline">\(T_0\)</span>, and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!</p>
<p>Instead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span>. For each value of <span class="math inline">\(\alpha\)</span> there corresponds a subtree <span class="math inline">\(T \subset T_0\)</span> such that minimizes <span class="math display">\[
\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \bar{y}_m)^2 + \alpha |T|
\]</span> The parameter <span class="math inline">\(\alpha\)</span> balances the complexity of the subtree and its adherence to the training data. When we increment <span class="math inline">\(\alpha\)</span> starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of <span class="math inline">\(\alpha\)</span>. We determine the optimal value <span class="math inline">\(\hat \alpha\)</span> through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to <span class="math inline">\(\hat \alpha\)</span>.</p>
</section>
<section id="classification-trees" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="classification-trees"><span class="header-section-number">14.3</span> Classification Trees</h2>
<p>A classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use the classification error rate, which is the proportion of observations in that region that do not belong to the most prevalent class.</p>
<p>We start by introducing some notations <span class="math display">\[
p_{mk} = \dfrac{1}{N_m}\sum_{x_i \in R_m} I(y_i=k),
\]</span> which is proportion of observations of class <span class="math inline">\(k\)</span> in region <span class="math inline">\(m\)</span>.</p>
<p>The classification then done as follows <span class="math display">\[
p_m = \max_k p_{mk},~~~ E_m = 1-p_m
\]</span> i.e the most frequent observation in region <span class="math inline">\(m\)</span></p>
<p>Then classification is done as follows <span class="math display">\[
P(y=k) = \sum_{j=1}^J p_j I(x \in R_j)
\]</span></p>
<p>An alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.</p>
<p>Now, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region.</p>
<p>Consider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).</p>
<p>In both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it is perfectly classifying all observations in that region. This is an ideal situation in classification problems. On the other hand, the first tree, despite having the same overall misclassification rate, does not have any region where all observations are correctly classified.</p>
<p>This illustrates that while the misclassification rate is a useful metric, it does not always capture the full picture. Other metrics like the Gini Index or Cross-Entropy can provide a more nuanced view of the quality of a split, taking into account not just the overall error rate, but also the distribution of errors across different regions.</p>
<p>Another way to measure the quality of the split is to use the Gini Index and Cross-Entropy Say, I have 400 observations in each class (400,400). I create a tree with two regions: (300,100) and (100,300). Say I have another tree: (200,400) and (200,0). In both cases misclassification rate is 0.25. The latter tree is preferable. We prefer to have more “pure nodes” and Gini index does a better job.</p>
<p>The Gini index: <span class="math display">\[
G_m = \sum_{k=1}^K p_{mk}(1-p_{mk})
\]</span> It measures a variance across the <span class="math inline">\(K\)</span> classes. It takes on a small value if all of the <span class="math inline">\(p_{mk}\)</span>’s are close to zero or one.</p>
<p>An alternative to the Gini index is cross-entropy (a.k.a deviance), given by <span class="math display">\[
D_m = -\sum_{k=1}^Kp_{mk}\log p_{mk}
\]</span> It is near zero if the <span class="math inline">\(p_mk\)</span>’s are all near zero or near one. Gini index and the cross-entropy led to similar results.</p>
<p>Now we apply the tree model to the Boston housing dataset.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS); <span class="fu">data</span>(Boston); <span class="fu">attach</span>(Boston)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Boston)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 7%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 11%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">crim</th>
<th style="text-align: right;">zn</th>
<th style="text-align: right;">indus</th>
<th style="text-align: right;">chas</th>
<th style="text-align: right;">nox</th>
<th style="text-align: right;">rm</th>
<th style="text-align: right;">age</th>
<th style="text-align: right;">dis</th>
<th style="text-align: right;">rad</th>
<th style="text-align: right;">tax</th>
<th style="text-align: right;">ptratio</th>
<th style="text-align: right;">black</th>
<th style="text-align: right;">lstat</th>
<th style="text-align: right;">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.54</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">296</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">24</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">9.1</td>
<td style="text-align: right;">22</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">393</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">395</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">33</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.3</td>
<td style="text-align: right;">36</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">394</td>
<td style="text-align: right;">5.2</td>
<td style="text-align: right;">29</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>First we build a big tree</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>lstat,<span class="at">data=</span>Boston,<span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co"># first big tree size</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 73</code></pre>
</div>
</div>
<p>Then prune it down to one with 7 leaves</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(boston.tree<span class="sc">$</span>where)) <span class="co"># pruned tree size</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 7</code></pre>
</div>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>boston.fit <span class="ot">=</span> <span class="fu">predict</span>(boston.tree) <span class="co">#get training fitted values</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">cex=</span>.<span class="dv">5</span>,<span class="at">pch=</span><span class="dv">16</span>) <span class="co">#plot data</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],boston.fit[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>cvals<span class="ot">=</span><span class="fu">c</span>(<span class="fl">9.725</span>,<span class="fl">4.65</span>,<span class="fl">3.325</span>,<span class="fl">5.495</span>,<span class="fl">16.085</span>,<span class="fl">19.9</span>) <span class="co">#cutpoints from tree</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(cvals)) <span class="fu">abline</span>(<span class="at">v=</span>cvals[i],<span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lty=</span><span class="dv">2</span>) <span class="co">#cutpoints</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Pick off <code>dis,lstat,medv</code></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df2<span class="ot">=</span>Boston[,<span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">13</span>,<span class="dv">14</span>)]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">names</span>(df2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "dis"   "lstat" "medv" </code></pre>
</div>
</div>
<p>Build the big tree</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>.,df2,<span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co">#</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 74</code></pre>
</div>
</div>
<p>Then prune it down to one with 7 leaves</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boston.tree,<span class="at">type=</span><span class="st">"u"</span>)<span class="co"># plot tree and partition in x.</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">partition.tree</span>(boston.tree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Get predictions on 2d grid</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pv<span class="ot">=</span><span class="fu">seq</span>(<span class="at">from=</span>.<span class="dv">01</span>,<span class="at">to=</span>.<span class="dv">99</span>,<span class="at">by=</span>.<span class="dv">05</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>x1q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>lstat,<span class="at">probs=</span>pv)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>x2q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>dis,<span class="at">probs=</span>pv)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">=</span> <span class="fu">expand.grid</span>(x1q,x2q) <span class="co">#matrix with two columns using all combinations of x1q and x2q</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>dfpred <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">dis=</span>xx[,<span class="dv">2</span>],<span class="at">lstat=</span>xx[,<span class="dv">1</span>])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>lmedpred <span class="ot">=</span> <span class="fu">predict</span>(boston.tree,dfpred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Make perspective plot</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">persp</span>(x1q,x2q,<span class="fu">matrix</span>(lmedpred,<span class="at">ncol=</span><span class="fu">length</span>(x2q),<span class="at">byrow=</span>T),</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a> <span class="at">theta=</span><span class="dv">150</span>,<span class="at">xlab=</span><span class="st">'dis'</span>,<span class="at">ylab=</span><span class="st">'lstat'</span>,<span class="at">zlab=</span><span class="st">'medv'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a> <span class="at">zlim=</span><span class="fu">c</span>(<span class="fu">min</span>(df2<span class="sc">$</span>medv),<span class="fl">1.1</span><span class="sc">*</span><span class="fu">max</span>(df2<span class="sc">$</span>medv)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p><em>Advantages of Decision Trees</em>: Decision trees are incredibly intuitive and simple to explain, often even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods we’ve discussed in previous chapters. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics, particularly when the trees are not overly complex. Additionally, decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.</p>
<p><em>Disadvantages of Decision Trees</em>: Large trees can exhibit high variance, meaning that a minor change in the data can lead to a significant change in the final estimated tree, making the model unstable. Conversely, small trees, while more stable, may not be powerful predictors as they might oversimplify the problem. It can be challenging to find a balance between bias and variance when using decision trees. A model with too much bias oversimplifies the problem and performs poorly, while a model with too much variance overfits the data and may not generalize well to unseen data.</p>
<p>There are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier, and hence improve predictive accuracy by reducing overfitting. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.</p>
<p>In the <strong>bagging</strong> approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.</p>
<p>To Bootsrap Aggregate (Bag) we:</p>
<ul>
<li>Take <span class="math inline">\(B\)</span> bootstrap samples from the training data, each of the same size as the training data.</li>
<li>Fit a large tree to each bootstrap sample (we know how to do this fast!). This will give us <span class="math inline">\(B\)</span> trees.</li>
<li>Combine the results from each of the B trees to get an overall prediction.</li>
</ul>
<p>When the target variable <span class="math inline">\(y\)</span> is numeric, the bagging process is straightforward. The final prediction is simply the average of the predictions from each of the <span class="math inline">\(B\)</span> trees. However, when <span class="math inline">\(y\)</span> is categorical, the process of combining results from different trees is less straightforward. One common approach is to use a voting system. In this system, each tree in the ensemble makes a prediction for a given input <span class="math inline">\(x\)</span>. The predicted category that receives the most votes (out of <span class="math inline">\(B\)</span> total votes) is chosen as the final prediction. Another approach is to average the predicted probabilities <span class="math inline">\(\hat p\)</span> from each tree. This method can provide a more nuanced prediction, especially in cases where the voting results are close.</p>
<p>Despite the potential benefits of averaging predicted probabilities, most software implementations of bagging for decision trees use the voting method. This is likely due to its simplicity and intuitive appeal. However, the best method to use can depend on the specific characteristics of the problem at hand.</p>
<p>The simple idea behind every ensemble method is that the variance of the average is lower than the variance of individual models. Say we have <span class="math inline">\(B\)</span> models <span class="math inline">\(f_1(x),\ldots,f_B(x)\)</span> then we combine those <span class="math display">\[
f_{avg}(x) = \dfrac{1}{B}\sum_{b=1}^Bf_b(x)
\]</span> Combining models helps fight overfitting. On the negative side, it is harder to interpret these ensembles</p>
<p>Let’s experiment with the number of trees in the model</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Boston)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>ntreev <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">500</span>,<span class="dv">5000</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>fmat <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,n,<span class="dv">3</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a> rffit <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>lstat,<span class="at">data=</span>Boston,<span class="at">ntree=</span>ntreev[i],<span class="at">maxnodes=</span><span class="dv">15</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a> fmat[,i] <span class="ot">=</span> <span class="fu">predict</span>(rffit)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="fu">mean</span>((fmat[,i] <span class="sc">-</span> Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>, <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 31
 29
 29</code></pre>
</div>
</div>
<p>Let’s plot the results</p>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>oo <span class="ot">=</span> <span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">xlab=</span><span class="st">'lstat'</span>,<span class="at">ylab=</span><span class="st">'medv'</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],fmat[oo,i],<span class="at">col=</span>i<span class="sc">+</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">title</span>(<span class="at">main=</span><span class="fu">paste</span>(<span class="st">'bagging ntrees = '</span>,ntreev[i]))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="3" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li>With 10 trees our fit is too jumbly.</li>
<li>With 1,000 and 5,000 trees the fit is not bad and very similar.</li>
<li>Note that although our method is based multiple trees (average over) so we no longer have a simple step function!!</li>
</ul>
</section>
<section id="random-forest" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">14.4</span> Random Forest</h2>
<p>In the bagging technique, models can become correlated, which prevents the achievement of a <span class="math inline">\(1/n\)</span> reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.</p>
<p>Random Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees, making the ensemble more robust and improving prediction accuracy. This randomness comes into play when considering a split in a tree. Instead of considering all <span class="math inline">\(p\)</span> predictors for a split, a random sample of <span class="math inline">\(m\)</span> predictors is chosen as split candidates. This subset of predictors is different for each split, which means that different trees are likely to use different predictors in the top split, leading to a more diverse set of trees.</p>
<p>The number of predictors considered at each split, <span class="math inline">\(m\)</span>, is typically chosen to be the square root of the total number of predictors, <span class="math inline">\(p\)</span>. This choice is a rule of thumb that often works well in practice, but it can be tuned based on the specific characteristics of the dataset.</p>
<p>By decorrelating the trees, Random Forests can often achieve better performance than bagging, especially when there’s a small number of very strong predictors in the dataset. In such cases, bagging can end up with an ensemble of very similar trees that all rely heavily on these strong predictors, while Random Forests can leverage the other, weaker predictors more effectively.</p>
<p>One of the “interpretation” tools that comes with ensemble models is importance ranking: the total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all trees</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">4</span>,<span class="at">importance=</span><span class="cn">TRUE</span>,<span class="at">ntree=</span><span class="dv">50</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston,<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"lightblue"</span>,<span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">6</span>,<span class="at">ntree=</span><span class="dv">50</span>, <span class="at">maxnodes=</span><span class="dv">50</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>yhat.rf <span class="ot">=</span> <span class="fu">predict</span>(rf.boston,<span class="at">newdata=</span>Boston)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat[oo],Boston<span class="sc">$</span>medv[oo],<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"grey"</span>, <span class="at">xlab=</span><span class="st">"lstat"</span>, <span class="at">ylab=</span><span class="st">"medv"</span>) <span class="co">#plot data</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],yhat.rf[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="boosting" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="boosting"><span class="header-section-number">14.5</span> Boosting</h2>
<p>Boosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.</p>
<p>Here’s how Boosting works:</p>
<ol type="1">
<li>Initially, a single decision tree is fitted to the data.</li>
<li>This initial tree is intentionally made weak, meaning it doesn’t perfectly fit the data.</li>
<li>We then examine the residuals, which represent the portion of the target variable <span class="math inline">\(y\)</span> not explained by the weak tree.</li>
<li>A new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.</li>
<li>This new tree is also “weakened” or “shrunk”. The prediction from this tree is then added to the prediction of the first tree.</li>
<li>This process is repeated iteratively. In each iteration, a new tree is fitted to the residuals of the current ensemble of trees, shrunk, and then added to the ensemble.</li>
<li>The final model is the sum of all these “shrunk” trees. The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals). Each new tree is trying to correct the mistakes of the ensemble of previous trees. By adding together many weak models (the shrunk trees), Boosting can often achieve a strong overall model.</li>
</ol>
<p>Pick a loss function <span class="math inline">\(\mathcal{L}\)</span> that reflects setting; e.g., for continuous <span class="math inline">\(y\)</span>, could take <span class="math inline">\(\mathcal{L}(y_i , \theta_i ) = (y_i - \theta_i )^2\)</span> Want to solve <span class="math display">\[\mathrm{minimize}_{\beta \in R^M} \sum_{i=1}^n \mathcal{L} \left(y_i, \sum_{j=1}^M \beta_j \cdot T_j(x_i)\right)\]</span></p>
<ul>
<li>Indexes all trees of a fixed size (e.g., depth = 5), so <span class="math inline">\(M\)</span> is huge</li>
<li>Space is simply too big to optimize</li>
<li>Gradient boosting: basically a version of gradient descent that is forced to work with trees</li>
<li>First think of optimization as <span class="math inline">\(\min_\theta f (\theta)\)</span>, over predicted values <span class="math inline">\(\theta\)</span> (subject to <span class="math inline">\(\theta\)</span> coming from trees)</li>
</ul>
<!-- ## Boosting -->
<!-- Start with initial model, e.g., fit a single tree $\theta^{(0)} = T_0$. Repeat: -->
<!-- - Evaluate gradient $g$ at latest prediction $\theta^{(k-1)}$, -->
<!--        $$g_i = \left.\left[\frac{\partial \mathcal{L}(y_i, \theta_i)}{\partial \theta_i}\right]\right|_{\theta_i = \theta_i^{(k-1)}},\ i=1,\ldots,n$$ -->
<!-- - Find a tree $T_k$ that is close to $-g$, i.e., $T_k$ solves -->
<!--        $$\mathrm{minimize}_{\text{trees }T} \sum_{i=1}^n (-g_i - T(x_i))^2$$ -->
<!--        Not hard to (approximately) solve for a single tree -->
<!-- - Update our prediction: -->
<!--        $$\theta^{(k)} = \theta^{(k-1)} + \alpha_k \cdot T_k$$ -->
<!--        Note: predictions are weighted sums of trees, as desired -->
<p>Set <span class="math inline">\(f_1(x)=0\)</span> (constant predictor) and <span class="math inline">\(r_i=y_i\)</span></p>
<p>For <span class="math inline">\(b=1,2,\ldots,B\)</span></p>
<ol type="a">
<li>Fit a tree <span class="math inline">\(f_b\)</span> with <span class="math inline">\(d\)</span> splits to the training set <span class="math inline">\((X,r)\)</span></li>
<li>Update the model <span class="math display">\[f(x) = f(x) +\lambda f_b(x)\]</span></li>
<li>Update the residuals <span class="math display">\[r_i=r_i - \lambda f_b(x)\]</span></li>
</ol>
<p>Here are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = <span class="math inline">\(\lambda\)</span> at .2.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>boost.boston<span class="ot">=</span><span class="fu">gbm</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">distribution=</span><span class="st">"gaussian"</span>,<span class="at">n.trees=</span><span class="dv">5000</span>,<span class="at">interaction.depth=</span><span class="dv">4</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>yhat.boost<span class="ot">=</span><span class="fu">predict</span>(boost.boston,<span class="at">newdata=</span>Boston,<span class="at">n.trees=</span><span class="dv">5000</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost<span class="sc">-</span>Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 4e-04</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boost.boston, <span class="at">plotit=</span><span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">var</th>
<th style="text-align: right;">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">lstat</td>
<td style="text-align: left;">lstat</td>
<td style="text-align: right;">36.32</td>
</tr>
<tr class="even">
<td style="text-align: left;">rm</td>
<td style="text-align: left;">rm</td>
<td style="text-align: right;">30.98</td>
</tr>
<tr class="odd">
<td style="text-align: left;">dis</td>
<td style="text-align: left;">dis</td>
<td style="text-align: right;">7.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">crim</td>
<td style="text-align: left;">crim</td>
<td style="text-align: right;">5.09</td>
</tr>
<tr class="odd">
<td style="text-align: left;">nox</td>
<td style="text-align: left;">nox</td>
<td style="text-align: right;">4.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">age</td>
<td style="text-align: left;">age</td>
<td style="text-align: right;">4.50</td>
</tr>
<tr class="odd">
<td style="text-align: left;">black</td>
<td style="text-align: left;">black</td>
<td style="text-align: right;">3.45</td>
</tr>
<tr class="even">
<td style="text-align: left;">ptratio</td>
<td style="text-align: left;">ptratio</td>
<td style="text-align: right;">3.11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tax</td>
<td style="text-align: left;">tax</td>
<td style="text-align: right;">1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">rad</td>
<td style="text-align: left;">rad</td>
<td style="text-align: right;">1.17</td>
</tr>
<tr class="odd">
<td style="text-align: left;">indus</td>
<td style="text-align: left;">indus</td>
<td style="text-align: right;">0.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">chas</td>
<td style="text-align: left;">chas</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr class="odd">
<td style="text-align: left;">zn</td>
<td style="text-align: left;">zn</td>
<td style="text-align: right;">0.13</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"rm"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"lstat"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-22-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Advantages of Boosting over Random Forests include performance and model interpretability. Boosting, in many cases, provides better predictive accuracy than Random Forests. By focusing on the residuals or mistakes, Boosting can incrementally improve model performance. While both methods are not as interpretable as a single decision tree, Boosting models can sometimes be more interpretable than Random Forests, especially when the number of weak learners (trees) is small.</p>
<p>Disadvantages of Boosting compared to Random Forests include computational complexity, overfitting potential, and sensitivity to outliers and noise. Boosting can be more computationally intensive than Random Forests because trees are built sequentially in Boosting, while in Random Forests they are built independently and can be parallelized. Boosting can overfit the training data if the number of trees is too large or if the trees are too complex. This is less of a problem with Random Forests, which are less prone to overfitting due to the randomness injected into the tree building process. Boosting can also be sensitive to outliers since it tries to correct the mistakes of the predecessors, while Random Forests are more robust to outliers. Finally, Boosting can overemphasize instances that are hard to classify and can overfit to noise, whereas Random Forests are more robust to noise.</p>
<p>Remember, the choice between Boosting and Random Forests (or any other model) should be guided by the specific requirements of your task, including the nature of your data, the computational resources available, and the trade-off between interpretability and predictive accuracy.</p>
</section>
<section id="finding-good-bayes-predictors" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="finding-good-bayes-predictors"><span class="header-section-number">14.6</span> Finding Good Bayes Predictors</h2>
<p>The ensemble methods we’ve discussed—bagging, random forests, and boosting—all share a common goal: improving predictive performance by combining multiple weak learners. But what is the theoretical foundation for why these methods work so well? And how do we determine the optimal way to combine these predictors? These questions lead us naturally to a Bayesian perspective on prediction, which provides both theoretical justification and practical guidance for ensemble methods.</p>
<p>Bayesian methods tackle the problem of good predictive performance in a number of ways. The goal is to find a good predictive MSE <span class="math inline">\(E_{Y,\hat{Y}}(\Vert\hat{Y} - Y \Vert^2)\)</span>. First, Stein shrinkage (a.k.a regularization with an <span class="math inline">\(\ell_2\)</span> norm) has long been known to provide good mean squared error properties in estimation, namely <span class="math inline">\(E(||\hat{\theta} - \theta||^2)\)</span> as well. These gains translate into predictive performance (in an iid setting) for <span class="math inline">\(E(||\hat{Y}-Y||^2)\)</span>. One of the main issues is how to tune the amount of regularisation (a.k.a prior hyper-parameters). Stein’s unbiased estimator of risk provides a simple empirical rule to address this problem, as does cross-validation. From a Bayes perspective, the marginal likelihood (and full marginal posterior) provides a natural method for hyper-parameter tuning. The issue is computational tractability and scalability. The posterior for <span class="math inline">\((W,b)\)</span> is extremely high dimensional and multimodal and posterior MAP provides good predictors <span class="math inline">\(\hat{Y}(X)\)</span>.</p>
<p><em>Bayes conditional averaging</em> can also perform well in high dimensional regression and classification problems. High dimensionality brings with it the curse of dimensionality and it is instructive to understand why certain kernel can perform badly.</p>
<p>Adaptive Kernel predictors (a.k.a. smart conditional averager) are of the form</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{r=1}^R K_r ( X_i , X ) \hat{Y}_r (X)
\]</span></p>
<p>Here <span class="math inline">\(\hat{Y}_r(X)\)</span> is a deep predictor with its own trained parameters. For tree models, the kernel <span class="math inline">\(K_r( X_i , X)\)</span> is a <em>cylindrical</em> region <span class="math inline">\(R_r\)</span> (open box set). <a href="#fig-cilinder" class="quarto-xref">Figure&nbsp;<span>14.2</span></a> illustrates the implied kernels for trees (cylindrical sets) and random forests. Not many points will be neighbors in high-dimensional input space.</p>
<div id="fig-cilinder" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/cylinder_kernel.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/rf_kernel.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Cylindrical kernels for trees (left) and random forests (right).
</figcaption>
</figure>
</div>
<p>Constructing the regions is fundamental to reducing the curse of dimensionality. It is useful to imagine a very large dataset, e.g., 100k images, and think about how a new image’s input coordinates, <span class="math inline">\(X\)</span>, are “neighbors” to data points in the training set. Our predictor will then be a smart conditional average of the observed outputs, <span class="math inline">\(Y\)</span>, for our neighbors. When <span class="math inline">\(p\)</span> is large, spheres (<span class="math inline">\(L^2\)</span> balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.</p>
<p>To illustrate the problem further, <a href="#fig-hd-ball" class="quarto-xref">Figure&nbsp;<span>14.3</span></a> below shows the 2D image of 1000 uniform samples from a 50-dimensional ball <span class="math inline">\(B_{50}\)</span>. The image is calculated as <span class="math inline">\(w^T Y\)</span>, where <span class="math inline">\(w = (1,1,0,\ldots,0)\)</span> and <span class="math inline">\(Y \sim U(B_{50})\)</span>. Samples are centered around the equators and none of the samples fall close to the boundary of the set.</p>
<div id="fig-hd-ball" class="quarto-float quarto-figure quarto-figure-center anchored" width="40%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hd_ball.svg" id="fig-hd-ball" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3
</figcaption>
</figure>
</div>
<p>As dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from <a href="#fig-hd-hist" class="quarto-xref">Figure&nbsp;<span>14.4</span></a>, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e.&nbsp;<span class="math inline">\(e_1^T Y\)</span>, where <span class="math inline">\(e_1 = (1,0,\ldots,0)\)</span>.</p>
<div id="fig-hd-hist" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_100.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_200.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_300.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_400.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4: Histogram of marginal distribution of <span class="math inline">\(Y\sim U(B_p)\)</span> for different dimensions <span class="math inline">\(p\)</span>.
</figcaption>
</figure>
</div>
<p>Similar central limit results were known to Maxwell who showed that random variable <span class="math inline">\(w^TY\)</span> is close to standard normal, when <span class="math inline">\(Y \sim U(B_p)\)</span>, <span class="math inline">\(p\)</span> is large, and <span class="math inline">\(w\)</span> is a unit vector (lies on the boundary of the ball). For the history of this fact, see <span class="citation" data-cites="diaconis1987dozen">Diaconis and Freedman (<a href="references.html#ref-diaconis1987dozen" role="doc-biblioref">1987</a>)</span>. More general results in this direction were obtained in <span class="citation" data-cites="klartag2007central">Klartag (<a href="references.html#ref-klartag2007central" role="doc-biblioref">2007</a>)</span>. Further, <span class="citation" data-cites="milman2009asymptotic">Milman and Schechtman (<a href="references.html#ref-milman2009asymptotic" role="doc-biblioref">2009</a>)</span> presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.</p>
<p>Deep learning improves on this by performing a sequence of GLM-like transformations; effectively, DL learns a distributed partition of the input space. Specifically, suppose that we have <span class="math inline">\(K\)</span> partitions. Then the DL predictor takes the form of a weighted average, or in the case of classification, a soft-max of the weighted average of observations in this partition. Given a new high-dimensional input <span class="math inline">\(X_{\mathrm{new}}\)</span>, many deep learners are an average of learners obtained by our hyperplane decomposition. Generically, we have</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{k \in K} w_k(X)\hat{Y}_k(X),
\]</span> where <span class="math inline">\(w_k\)</span> are the weights learned in region <span class="math inline">\(k\)</span>, and <span class="math inline">\(w_k(X)\)</span> is an indicator of the region with appropriate weighting given the training data. The weight <span class="math inline">\(w_k\)</span> also indicates which partition the new <span class="math inline">\(X_{\mathrm{new}}\)</span> lies in.</p>
<p>The use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form of clever conditional averaging) are prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, with the caveat that they have large variances due to dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the <span class="math inline">\(1/N\)</span>-rule and average to reduce risk. Specifically, suppose that we have <span class="math inline">\(K\)</span> exchangeable, <span class="math inline">\(\mathbb{E} ( \hat{Y}_i ) = \mathbb{E} ( \hat{Y}_{\pi(i)} )\)</span>, predictors</p>
<p><span class="math display">\[
\hat{Y} = ( \hat{Y}_1 , \ldots , \hat{Y}_K )
\]</span></p>
<p>Find <span class="math inline">\(w\)</span> to attain <span class="math inline">\(\operatorname{argmin}_W E l( Y , w^T \hat{Y} )\)</span> where <span class="math inline">\(l\)</span> convex in the second argument;</p>
<p><span class="math display">\[
E l( Y , w^T \hat{Y} ) = \frac{1}{K!} \sum_\pi E l( Y , w^T \hat{Y} ) \geq E l \left ( Y , \frac{1}{K!} \sum_\pi w_\pi^T \hat{Y} )\right ) = E l \left ( Y , (1/K) \iota^T \hat{Y} \right )
\]</span></p>
<p>where <span class="math inline">\(\iota = ( 1 , \ldots ,1 )\)</span>. Hence, the randomized multiple predictor with weights <span class="math inline">\(w = (1/K)\iota\)</span> provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.</p>
<p>An alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule. We formalize the gains in Classification Risk with the following discussion.</p>
</section>
<section id="ensemble-averaging-and-1n-rule" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="ensemble-averaging-and-1n-rule"><span class="header-section-number">14.7</span> Ensemble Averaging and <span class="math inline">\(1/N\)</span> Rule</h2>
<p>In high dimensions, when we have a large number of predictive models that generate uncorrelated predictions, the optimal approach to generate a prediction is to average predictions from those individual models/weak predictors. This is called the <span class="math inline">\(1/N\)</span> rule. The variance in the prediction is reduced by a factor of <span class="math inline">\(N\)</span> when we average out <span class="math inline">\(N\)</span> uncorrelated predictions. <span class="math display">\[
\mbox{Var} \left ( \frac{1}{N} \sum_{i=1}^N \hat y_i \right ) = \frac{1}{N^2} \mbox{Var} \left ( \hat y_i \right ) + \frac{2}{N^2} \sum_{i \neq j} \mbox{Cov} \left ( \hat y_i, \hat y_j \right )
\]</span> In high dimensions it is relatively easy to find uncorrelated predictors, and these techniques prove to lead to winning solutions in many machine learning competitions. The <span class="math inline">\(1/N\)</span> rule is optimal due to exchangeability of the weak predictors, see <span class="citation" data-cites="polson2017deep">Polson, Sokolov, et al. (<a href="references.html#ref-polson2017deep" role="doc-biblioref">2017</a>)</span></p>
<!-- ### Exchangability Result
The original rule for classification approach was to do k-nearest neighbors. It can be shown that Bayes is the optimal clasifier. Assume, you know $p(y\mid x)$, when
$$
p(+1| x)=0.8\\
p(-1| x)=0.2\\
$$
The Best prediction: $y^* = h_\mathrm{opt} = argmax_y p(y\mid x)$, you predict the most likely class. What is the error of the Bayes classifier?
$$
\epsilon_{BayesOpt}=1-\mathrm{P}(h_\mathrm{opt}(\mathbf{x})|y) = 1- \mathrm{P}(y^*|\mathbf{x})
$$ -->
<section id="classification-variance-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="classification-variance-decomposition">Classification variance decomposition</h3>
<p>The famous result is due to <span class="citation" data-cites="cover1967nearest">Cover and Hart (<a href="references.html#ref-cover1967nearest" role="doc-biblioref">1967</a>)</span> who proved that k-nearest neighbors are at most twice the bayes risk.</p>
<p><span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span> use the population conditional probability distribution of a point <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=c\)</span>, denoted by <span class="math inline">\(P_{c}\)</span>, and the associated conditional expectation and variance operators will be denoted <span class="math inline">\(E_{c}\)</span> and <span class="math inline">\(V a r_{c}\)</span>. Define the vectors of average aggregates conditional on class <span class="math inline">\(c\)</span> as <span class="math display">\[
M_{c}(d)=E_{c}\left[H_{\mathbf{Q}}(X, d)\right]=E\left[H_{\mathbf{Q}}(X, d) \mid Y=c\right]
\]</span></p>
<p>for <span class="math inline">\(d=1, \ldots, K\)</span>. The average conditional margin (ACM) for class <span class="math inline">\(c\)</span> is defined as</p>
<p><span class="math display">\[
\theta_{c}=\min _{d \neq c}\left(M_{c}(c)-M_{c}(d)\right)
\]</span></p>
<p>We assume that <span class="math inline">\(\theta_{c}&gt;0\)</span>. This assumption is very weak since it involves only the average over the population of class <span class="math inline">\(c\)</span>. It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.</p>
<p>Given that <span class="math inline">\(\theta_{c}&gt;0\)</span>, the error rate for class <span class="math inline">\(c\)</span> depends on the extent to which the aggregate classifier <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> is concentrated around <span class="math inline">\(M_{c}(d)\)</span> for each <span class="math inline">\(d=1, \ldots, K\)</span>. The simplest measure of concentration is the variance of <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> with respect to the distribution <span class="math inline">\(P_{c}\)</span>. Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to <span class="math inline">\(P_{c}\)</span> as follows.</p>
<p><span class="math display">\[\begin{align*}
P_{c}\left(C_{\mathbf{Q}}(X) \neq c\right) \leq &amp; P_{c}\left(H_{\mathbf{Q}}(X, c)&lt;M_{c}(c)-\theta_{c} / 2\right) \\
&amp; +\sum_{d \neq c} P_{c}\left(H_{\mathbf{Q}}(X, d)&gt;M_{c}(d)+\theta_{c} / 2\right) \\
\leq &amp; \sum_{d=1}^{K} P_{c}\left(\left|H_{\mathbf{Q}}(X, d)-M_{c}(d)\right|&gt;\theta_{c} / 2\right) \\
\leq &amp; \frac{4}{\theta_{c}^{2}} \sum_{d=1}^{K} \operatorname{Var}_{c}\left[H_{\mathbf{Q}}(X, d)\right] . \tag{10}
\end{align*}\]</span></p>
<p>Of course Chebyshev’s inequality is coarse and will not give very sharp results in itself, be we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities.</p>
<p>We rewrite each of the variance terms of the last equation as</p>
<p><span class="math display">\[\begin{align*}
\operatorname{Var}_{c}\left[E_{\mathbf{Q}} h(X, d)\right] &amp; =E_{c}\left[E_{\mathbf{Q}} h(X, d)\right]^{2}-\left[E_{c} E_{\mathbf{Q}} h(X, d)\right]^{2} \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} E_{c}\left[h_{1}(X, d) h_{2}(X, d)\right]-E_{\mathbf{Q} \otimes \mathbf{Q}}\left[E_{c}\left[h_{1}(X, d)\right] E_{c}\left[h_{2}(X, d)\right]\right] \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} \operatorname{Cov}_{c}\left[h_{1}(X, d), h_{2}(X, d)\right] \doteq \gamma_{c, d} \tag{11}
\end{align*}\]</span></p>
<p>where the notation <span class="math inline">\(E_{\mathbf{Q} \otimes \mathbf{Q}}\)</span> means that <span class="math inline">\(h_{1}, h_{2}\)</span> are two classifiers sampled independently from the distribution <span class="math inline">\(\mathbf{Q}\)</span>. We can therefore interpret this variance term as the conditional covariance of two classifiers independently sampled from <span class="math inline">\(\mathbf{Q}\)</span>. We call this quantity the average conditional covariance (ACC). Even if <span class="math inline">\(\mathbf{Q}\)</span> is a discrete distribution, such as that provided by a particular run of <span class="math inline">\(N\)</span> classifiers, when it is supported on a moderate number of classifiers, it is dominated by the conditional covariances of which there are order <span class="math inline">\(N^{2}\)</span>, and not the conditional variances of which there are order <span class="math inline">\(N\)</span>.</p>
</section>
<section id="conditional-and-unconditional-dependence" class="level3">
<h3 class="anchored" data-anchor-id="conditional-and-unconditional-dependence">Conditional and unconditional dependence</h3>
<p>It should be emphasized that two classifiers, provided that they achieve reasonable classification rate (that is, better than just picking a class at random) will not be unconditionally independent. If we do not know the class label of a point, and vector <span class="math inline">\(\left(h_{1}(X, i)\right)_{i}\)</span> is large at class <span class="math inline">\(c\)</span>, then we actually change our expectations regarding <span class="math inline">\(\left(h_{2}(X, i)\right)_{i}\)</span>. On the other hand if we were given in advance the class label <span class="math inline">\(Y\)</span>, then knowing <span class="math inline">\(h_{1}(X)\)</span> would hardly affect our guess about <span class="math inline">\(h_{2}(X)\)</span>. This is the motivation behind the notion of weak conditional dependence.</p>
<p>This is in contrast to the measure of dependence introduced in Dietterich (1998), which involves the unconditional covariance. The <span class="math inline">\(\kappa\)</span> statistic used there is</p>
<p><span class="math display">\[
\kappa\left(h_{1}, h_{2}\right)=\frac{\sum_{d} \operatorname{Cov}\left[h_{1}(X, d), h_{2}(X, d)\right]}{1-\sum_{d} E h_{1}(X, d) E h_{2}(X, d)},
\]</span></p>
<p>A simple decomposition of the numerator yields: <span class="math inline">\(\operatorname{Cov}\left[h_{1}(X, d), h_{2}(X, d)\right]=E \operatorname{Cov}\left[h_{1}(X, d), h_{2}(X, d) \mid Y\right]+\operatorname{Cov}\left[E\left[h_{1}(X, d) \mid Y\right], E\left[h_{2}(X, d) \mid Y\right]\right]\)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-amit2000multiple" class="csl-entry" role="listitem">
Amit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. <span>“Multiple Randomized Classifiers: <span>MRCL</span>.”</span>
</div>
<div id="ref-cover1967nearest" class="csl-entry" role="listitem">
Cover, T., and P. Hart. 1967. <span>“Nearest Neighbor Pattern Classification.”</span> <em>IEEE Transactions on Information Theory</em> 13 (1): 21–27.
</div>
<div id="ref-diaconis1987dozen" class="csl-entry" role="listitem">
Diaconis, Persi, and David Freedman. 1987. <span>“A Dozen de <span class="nocase">Finetti-style</span> Results in Search of a Theory.”</span> In <em>Annales de l’<span>IHP</span> Probabilit<span>é</span>s Et Statistiques</em>, 23:397–423.
</div>
<div id="ref-klartag2007central" class="csl-entry" role="listitem">
Klartag, Bo’az. 2007. <span>“A Central Limit Theorem for Convex Sets.”</span> <em>Inventiones Mathematicae</em> 168 (1): 91–131.
</div>
<div id="ref-milman2009asymptotic" class="csl-entry" role="listitem">
Milman, Vitali D, and Gideon Schechtman. 2009. <em>Asymptotic Theory of Finite Dimensional Normed Spaces: <span>Isoperimetric</span> Inequalities in Riemannian Manifolds</em>. Vol. 1200. Springer.
</div>
<div id="ref-polson2017deep" class="csl-entry" role="listitem">
Polson, Nicholas G, Vadim Sokolov, et al. 2017. <span>“Deep <span>Learning</span>: <span>A Bayesian Perspective</span>.”</span> <em>Bayesian Analysis</em> 12 (4): 1275–1304.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./13-logistic.html" class="pagination-link" aria-label="Logistic Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./15-forecasting.html" class="pagination-link" aria-label="Forecasting">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>