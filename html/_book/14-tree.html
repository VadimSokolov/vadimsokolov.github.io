<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Tree Models – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./15-forecasting.html" rel="next">
<link href="./13-logistic.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/chess.jpg">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="14&nbsp; Tree Models – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/chess.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#building-a-tree-via-recursive-binary-splitting" id="toc-building-a-tree-via-recursive-binary-splitting" class="nav-link active" data-scroll-target="#building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</a></li>
  <li><a href="#pruning-taming-an-overfit-tree" id="toc-pruning-taming-an-overfit-tree" class="nav-link" data-scroll-target="#pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</a></li>
  <li><a href="#classification-trees" id="toc-classification-trees" class="nav-link" data-scroll-target="#classification-trees"><span class="header-section-number">14.3</span> Classification Trees</a></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="header-section-number">14.4</span> Random Forest</a></li>
  <li><a href="#boosting" id="toc-boosting" class="nav-link" data-scroll-target="#boosting"><span class="header-section-number">14.5</span> Boosting</a></li>
  <li><a href="#why-ensembles-work-a-geometric-perspective" id="toc-why-ensembles-work-a-geometric-perspective" class="nav-link" data-scroll-target="#why-ensembles-work-a-geometric-perspective"><span class="header-section-number">14.6</span> Why Ensembles Work: A Geometric Perspective</a>
  <ul class="collapse">
  <li><a href="#smoothing-and-variance-reduction" id="toc-smoothing-and-variance-reduction" class="nav-link" data-scroll-target="#smoothing-and-variance-reduction">Smoothing and Variance Reduction</a></li>
  <li><a href="#the-problem-of-high-dimensions" id="toc-the-problem-of-high-dimensions" class="nav-link" data-scroll-target="#the-problem-of-high-dimensions">The Problem of High Dimensions</a></li>
  <li><a href="#trees-as-adaptive-nearest-neighbors" id="toc-trees-as-adaptive-nearest-neighbors" class="nav-link" data-scroll-target="#trees-as-adaptive-nearest-neighbors">Trees as “Adaptive” Nearest Neighbors</a></li>
  </ul></li>
  <li><a href="#theoretical-foundations-why-trees-work" id="toc-theoretical-foundations-why-trees-work" class="nav-link" data-scroll-target="#theoretical-foundations-why-trees-work"><span class="header-section-number">14.7</span> Theoretical Foundations: Why Trees Work</a>
  <ul class="collapse">
  <li><a href="#ensemble-averaging-and-the-1n-rule" id="toc-ensemble-averaging-and-the-1n-rule" class="nav-link" data-scroll-target="#ensemble-averaging-and-the-1n-rule">Ensemble Averaging and the <span class="math inline">\(1/N\)</span> Rule</a></li>
  <li><a href="#classification-variance-decomposition" id="toc-classification-variance-decomposition" class="nav-link" data-scroll-target="#classification-variance-decomposition">Classification variance decomposition</a></li>
  <li><a href="#conditional-and-unconditional-dependence" id="toc-conditional-and-unconditional-dependence" class="nav-link" data-scroll-target="#conditional-and-unconditional-dependence">Conditional and unconditional dependence</a></li>
  <li><a href="#the-nearest-neighbor-insight" id="toc-the-nearest-neighbor-insight" class="nav-link" data-scroll-target="#the-nearest-neighbor-insight">The Nearest Neighbor Insight</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14.8</span> Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./14-tree.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>We’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of <span class="math inline">\(y\)</span> to use for a given <span class="math inline">\(x\)</span>. Similar to a decision tree, a predictive tree model is a nested sequence of if-else statements that map any input data point <span class="math inline">\(x\)</span> to a predicted output <span class="math inline">\(y\)</span>. Each if-else statement checks a feature of <span class="math inline">\(x\)</span> and sends the data left or right along the tree branch. At the end of the branch, a single value of <span class="math inline">\(y\)</span> is predicted.</p>
<p><a href="#fig-chesstree" class="quarto-xref">Figure&nbsp;<span>14.1</span></a> shows a decision tree for predicting a chess piece given a four-dimensional input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.</p>
<div id="fig-chesstree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/chess.jpg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-chesstree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Elementary tree scheme; visualization of the splitting process
</figcaption>
</figure>
</div>
<p>The prediction algorithm is simple. Start at the root node and move down the tree until you reach a leaf node. The process of building a tree, given a set of training data, is more complicated and has three main components:</p>
<ol type="1">
<li><strong>Splitting</strong>. The process of dividing the training data into subsets based on the value of a single feature. The goal is to create subsets that are as homogeneous as possible. The subsets are then used to create the nodes of the tree.</li>
<li><strong>Stopping</strong>. The process of deciding when to stop splitting. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
<li><strong>Pruning</strong>. The process of removing nodes from the tree that do not improve the accuracy of the tree. The goal is to create a tree that is as accurate as possible without overfitting the training data.</li>
</ol>
<p>The splitting process is the most important part of the tree-building process. At each step the splitting process needs to decide on the feature index <span class="math inline">\(j\)</span> to be used for splitting and the location of the split. For a binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.</p>
<p>Imagine you’re a jewelry appraiser tasked with determining a diamond’s value. You might follow a series of questions: Is the carat weight above 1.0? If yes, is the clarity VS1 or better? Each question leads to another, creating a decision path that eventually arrives at a price estimate. This is precisely how decision trees work—they mirror our natural decision-making process by creating a flowchart of if-then rules.</p>
<p>Below we’ll explore tree-based models using the classic diamonds dataset, which contains prices and attributes for 53,940 diamonds. We’ll start with simple decision trees, progress to ensemble methods like random forests and gradient boosting, and develop deep insights into how these algorithms work, when to use them, and how to avoid common pitfalls.</p>
<p>Let’s start with a quick demo and look at the data, which has 10 variables</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 40%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>carat</code></td>
<td>Weight of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>cut</code></td>
<td>Quality of the cut</td>
<td>Fair, Good, Very Good, Premium, Ideal</td>
</tr>
<tr class="odd">
<td><code>color</code></td>
<td>Color of the diamond</td>
<td>J, I, H, G, F, E, D</td>
</tr>
<tr class="even">
<td><code>clarity</code></td>
<td>Clarity of the diamond</td>
<td>I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF</td>
</tr>
<tr class="odd">
<td><code>depth</code></td>
<td>Depth of the diamond</td>
<td>Numeric</td>
</tr>
<tr class="even">
<td><code>table</code></td>
<td>Width of the diamond’s table</td>
<td>Numeric</td>
</tr>
</tbody>
</table>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(diamonds)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(diamonds)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">carat</th>
<th style="text-align: left;">cut</th>
<th style="text-align: left;">color</th>
<th style="text-align: left;">clarity</th>
<th style="text-align: right;">depth</th>
<th style="text-align: right;">table</th>
<th style="text-align: right;">price</th>
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Ideal</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.4</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.21</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">SI1</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">326</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.23</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">E</td>
<td style="text-align: left;">VS1</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">327</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">2.3</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.29</td>
<td style="text-align: left;">Premium</td>
<td style="text-align: left;">I</td>
<td style="text-align: left;">VS2</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">334</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">2.6</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.31</td>
<td style="text-align: left;">Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">SI2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">335</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">4.3</td>
<td style="text-align: right;">2.8</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.24</td>
<td style="text-align: left;">Very Good</td>
<td style="text-align: left;">J</td>
<td style="text-align: left;">VVS2</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">2.5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>Let’s plot price vs carat.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Notice the strong non-linear relationship between carat and price. This suggests that log-transformations might help make the relationship linear.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create log-transformed price for better linear relationships</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>diamonds <span class="ot">&lt;-</span> diamonds <span class="sc">%&gt;%</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">mutate</span>(<span class="at">log_price =</span> <span class="fu">log</span>(price),</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a> <span class="at">log_carat =</span> <span class="fu">log</span>(carat))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine the linearized relationship</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> log_carat, <span class="at">y =</span> log_price)) <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.05</span>, <span class="at">color =</span> <span class="st">"darkblue"</span>) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Log-transformed Price vs Carat Shows Linear Relationship"</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> <span class="st">"Log(Carat)"</span>, <span class="at">y =</span> <span class="st">"Log(Price)"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Unlike linear regression, tree models are naturally indifferent to non-linear relationships between predictors and the response. In general, we do not need to transform the variables.</p>
<p>Although carat is the most important factor in determining the price of a diamond, it is not the only factor. We can see that there is a lot of variability in the price of diamonds with the same carat.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># use density plot to compare price for different clarity levels</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x =</span> price, <span class="at">color =</span> clarity)) <span class="sc">+</span> <span class="fu">geom_density</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Let’s start with a simple decision tree using just two predictors to visualize how trees partition the feature space:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>tree_model <span class="ot">&lt;-</span> <span class="fu">rpart</span>(price <span class="sc">~</span> carat <span class="sc">+</span> clarity, <span class="at">data =</span> diamonds)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The decision tree plot shows how the algorithm partitions the feature space based on carat and clarity to predict diamond prices. The tree structure reveals several interesting patterns:</p>
<ol type="1">
<li><p>Primary split on carat: The tree first splits on carat at 1.05, indicating this is the most important predictor for price. This makes intuitive sense since carat weight is typically the strongest determinant of diamond value.</p></li>
<li><p>Secondary splits on clarity: After the carat split, the tree further partitions based on clarity levels. This shows that while carat is primary, clarity still provides important predictive value for price.</p></li>
<li><p>Interpretability: Each terminal node (leaf) shows the predicted price for that region. For example, diamonds with carat &lt; 1.05 and clarity in the lower categories (I1, SI2, SI1) have an average predicted price of $2,847.</p></li>
<li><p>Feature interactions: The tree reveals how carat and clarity interact - the effect of clarity on price depends on the carat weight, which is captured through the hierarchical splitting structure.</p></li>
</ol>
<p>This simple two-predictor tree demonstrates the key advantages of decision trees: they can handle non-linear relationships, provide interpretable rules, and naturally capture feature interactions without requiring explicit specification of interaction terms.</p>
<p>Let’s plot the data.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(diamonds, <span class="fu">aes</span>(<span class="at">x=</span>carat, <span class="at">y=</span>clarity, <span class="at">colour=</span>price)) <span class="sc">+</span> </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low=</span><span class="st">"blue"</span>, <span class="at">high=</span><span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>We can see that for small and large diamonds, the price is consistently low and does not depend much on the clarity. However, at around 1 carat, we see some overlap in the price for different clarity levels. Clarity becomes important at this level.</p>
<p>Now let’s plot the data with the tree regions.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the rectangle areas that represent the regions of the tree</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>pred_data <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a> <span class="at">carat =</span> <span class="fu">seq</span>(<span class="fu">min</span>(diamonds<span class="sc">$</span>carat), <span class="fu">max</span>(diamonds<span class="sc">$</span>carat), <span class="at">length.out =</span> <span class="dv">50</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a> <span class="at">clarity =</span> diamonds<span class="sc">$</span>clarity</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>pred_data<span class="sc">$</span>pred_price <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, pred_data)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot regions</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(pred_data, <span class="fu">aes</span>(<span class="at">x =</span> carat, <span class="at">y =</span> clarity, <span class="at">fill =</span> pred_price)) <span class="sc">+</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a> <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">"blue"</span>, <span class="at">high =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Tree Regions: Carat vs Clarity"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The plot above shows the decision tree’s prediction regions as colored tiles, where each tile represents a specific combination of carat and clarity values. The color gradient from blue to red indicates the predicted price, with darker red representing higher predicted prices.</p>
<p>Looking at this visualization, we can see several key patterns. The strongest predictor is clearly carat, as evidenced by the vertical bands of similar colors. As carat increases (moving right on the x-axis), the predicted prices generally increase (colors shift from blue to red). The tree captures non-linear patterns that a simple linear model would miss. For example, the rate of price increase with carat is not uniform across all clarity levels. Unlike smooth regression surfaces, the tree creates distinct rectangular regions with sharp boundaries, reflecting the binary splitting nature of decision trees.</p>
<p>The prediction is rather straightforward. The tree divides the predictor space-that is, the set of possible values for <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> - into <span class="math inline">\(J\)</span> distinct and non-overlapping boxes, <span class="math inline">\(R_1,R_2,...,R_J\)</span>. For every observation that falls into the region <span class="math inline">\(R_j\)</span>, we make the same prediction, which is simply the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span>.</p>
<p><span class="math display">\[
f(x) = \bar y_j, \text{ for } x \in R_j, \text{ where } \bar y_j = \text{Average}(y_i \mid x_i \in R_j)
\]</span></p>
<section id="building-a-tree-via-recursive-binary-splitting" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="building-a-tree-via-recursive-binary-splitting"><span class="header-section-number">14.1</span> Building a Tree via Recursive Binary Splitting</h2>
<p>The overall goal of building a tree is to find regions that lead to minima of the total Residual Sum of Squares (RSS) <span class="math display">\[
\mathrm{RSS} = \sum_{j=1}^J\sum_{i \in R_j}(y_i - \bar{y}_j)^2 \rightarrow \mathrm{minimize}
\]</span></p>
<p>Unfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into <span class="math inline">\(J\)</span> boxes. We can find a good approximate solution, using top-down approach (the CART algorithm).</p>
<p>It begins with the entire dataset at the “root” node and repeatedly splits the data into two “child” nodes. This process continues recursively on each new node, with the goal of making the resulting groups (nodes) as homogeneous as possible with respect to the target variable, price. At each iteration we decide on which variable <span class="math inline">\(j\)</span> to split and the split point <span class="math inline">\(s\)</span>. <span class="math display">\[
R_1(j, s) = \{x\mid x_j &lt; s\} \mbox{ and } R_2(j, s) = \{x\mid x_j \ge s\},
\]</span> thus, we seek to minimize (in case of regression tree) <span class="math display">\[
\min_{j,s}\left[ \sum_{i:x_i\in R_1}(y_i - \bar{y}_1)^2 + \sum_{i:x_i \in R_2}(y_i - \bar{y}_2)^2\right]
\]</span> As a result, every observed input point belongs to a single region.</p>
</section>
<section id="pruning-taming-an-overfit-tree" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="pruning-taming-an-overfit-tree"><span class="header-section-number">14.2</span> Pruning: Taming an Overfit Tree</h2>
<p>Now let’s discuss how many regions we should have. At one extreme end, we can have <span class="math inline">\(n\)</span> regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed <span class="math inline">\(y\)</span>’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions <span class="math inline">\(R_1,...,R_J\)</span>) might lead to lower variance and better interpretation at the cost of a little bias.</p>
<p>How do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree <span class="math inline">\(T_0\)</span>, and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!</p>
<p>Instead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span>. For each value of <span class="math inline">\(\alpha\)</span> there corresponds a subtree <span class="math inline">\(T \subset T_0\)</span> such that minimizes <span class="math display">\[
\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \bar{y}_m)^2 + \alpha |T|
\]</span> The parameter <span class="math inline">\(\alpha\)</span> balances the complexity of the subtree and its adherence to the training data. When we increment <span class="math inline">\(\alpha\)</span> starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of <span class="math inline">\(\alpha\)</span>. We determine the optimal value <span class="math inline">\(\hat \alpha\)</span> through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to <span class="math inline">\(\hat \alpha\)</span>.</p>
</section>
<section id="classification-trees" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="classification-trees"><span class="header-section-number">14.3</span> Classification Trees</h2>
<p>A classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use the classification error rate, which is the proportion of observations in that region that do not belong to the most prevalent class.</p>
<p>We start by introducing some notations <span class="math display">\[
p_{mk} = \dfrac{1}{N_m}\sum_{x_i \in R_m} I(y_i=k),
\]</span> which is proportion of observations of class <span class="math inline">\(k\)</span> in region <span class="math inline">\(m\)</span>.</p>
<p>The classification then done as follows <span class="math display">\[
p_m = \max_k p_{mk},~~~ E_m = 1-p_m
\]</span> i.e the most frequent observation in region <span class="math inline">\(m\)</span></p>
<p>Then classification is done as follows <span class="math display">\[
P(y=k) = \sum_{j=1}^J p_j I(x \in R_j)
\]</span></p>
<p>An alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.</p>
<p>Now, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region.</p>
<p>Consider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).</p>
<p>In both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it is perfectly classifying all observations in that region. This is an ideal situation in classification problems. On the other hand, the first tree, despite having the same overall misclassification rate, does not have any region where all observations are correctly classified.</p>
<p>This illustrates that while the misclassification rate is a useful metric, it does not always capture the full picture. Other metrics like the Gini Index or Cross-Entropy can provide a more nuanced view of the quality of a split, taking into account not just the overall error rate, but also the distribution of errors across different regions.</p>
<p>Another way to measure the quality of the split is to use the Gini Index and Cross-Entropy Say, I have 400 observations in each class (400,400). I create a tree with two regions: (300,100) and (100,300). Say I have another tree: (200,400) and (200,0). In both cases misclassification rate is 0.25. The latter tree is preferable. We prefer to have more “pure nodes” and Gini index does a better job.</p>
<p>The Gini index: <span class="math display">\[
G_m = \sum_{k=1}^K p_{mk}(1-p_{mk})
\]</span> It measures a variance across the <span class="math inline">\(K\)</span> classes. It takes on a small value if all of the <span class="math inline">\(p_{mk}\)</span>’s are close to zero or one.</p>
<p>An alternative to the Gini index is cross-entropy (a.k.a deviance), given by <span class="math display">\[
D_m = -\sum_{k=1}^Kp_{mk}\log p_{mk}
\]</span> It is near zero if the <span class="math inline">\(p_mk\)</span>’s are all near zero or near one. Gini index and the cross-entropy led to similar results.</p>
<p>Now we apply the tree model to the Boston housing dataset.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS); <span class="fu">data</span>(Boston); <span class="fu">attach</span>(Boston)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Boston)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 7%">
<col style="width: 4%">
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 11%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">crim</th>
<th style="text-align: right;">zn</th>
<th style="text-align: right;">indus</th>
<th style="text-align: right;">chas</th>
<th style="text-align: right;">nox</th>
<th style="text-align: right;">rm</th>
<th style="text-align: right;">age</th>
<th style="text-align: right;">dis</th>
<th style="text-align: right;">rad</th>
<th style="text-align: right;">tax</th>
<th style="text-align: right;">ptratio</th>
<th style="text-align: right;">black</th>
<th style="text-align: right;">lstat</th>
<th style="text-align: right;">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.54</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">296</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">24</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">9.1</td>
<td style="text-align: right;">22</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">7.1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">242</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">393</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">395</td>
<td style="text-align: right;">2.9</td>
<td style="text-align: right;">33</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">7.2</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">397</td>
<td style="text-align: right;">5.3</td>
<td style="text-align: right;">36</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.03</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0.46</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">222</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">394</td>
<td style="text-align: right;">5.2</td>
<td style="text-align: right;">29</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>First we build a big tree</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>lstat,<span class="at">data=</span>Boston,<span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co"># first big tree size</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 73</code></pre>
</div>
</div>
<p>Then prune it down to one with 7 leaves</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(boston.tree<span class="sc">$</span>where)) <span class="co"># pruned tree size</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 7</code></pre>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>boston.fit <span class="ot">=</span> <span class="fu">predict</span>(boston.tree) <span class="co">#get training fitted values</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">cex=</span>.<span class="dv">5</span>,<span class="at">pch=</span><span class="dv">16</span>) <span class="co">#plot data</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],boston.fit[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>cvals<span class="ot">=</span><span class="fu">c</span>(<span class="fl">9.725</span>,<span class="fl">4.65</span>,<span class="fl">3.325</span>,<span class="fl">5.495</span>,<span class="fl">16.085</span>,<span class="fl">19.9</span>) <span class="co">#cutpoints from tree</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(cvals)) <span class="fu">abline</span>(<span class="at">v=</span>cvals[i],<span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lty=</span><span class="dv">2</span>) <span class="co">#cutpoints</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-10-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Pick off <code>dis,lstat,medv</code></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>df2<span class="ot">=</span>Boston[,<span class="fu">c</span>(<span class="dv">8</span>,<span class="dv">13</span>,<span class="dv">14</span>)]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">names</span>(df2))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> "dis"   "lstat" "medv" </code></pre>
</div>
</div>
<p>Build the big tree</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">=</span> <span class="fu">tree</span>(medv<span class="sc">~</span>.,df2,<span class="at">mindev=</span>.<span class="dv">0001</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(temp<span class="sc">$</span>where)) <span class="co">#</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 74</code></pre>
</div>
</div>
<p>Then prune it down to one with 7 leaves</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>boston.tree<span class="ot">=</span><span class="fu">prune.tree</span>(temp,<span class="at">best=</span><span class="dv">7</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boston.tree,<span class="at">type=</span><span class="st">"u"</span>)<span class="co"># plot tree and partition in x.</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(boston.tree,<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">label=</span><span class="fu">c</span>(<span class="st">"yval"</span>),<span class="at">cex=</span>.<span class="dv">8</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="fu">partition.tree</span>(boston.tree)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/boston-partition-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Get predictions on 2d grid</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>pv<span class="ot">=</span><span class="fu">seq</span>(<span class="at">from=</span>.<span class="dv">01</span>,<span class="at">to=</span>.<span class="dv">99</span>,<span class="at">by=</span>.<span class="dv">05</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>x1q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>lstat,<span class="at">probs=</span>pv)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>x2q <span class="ot">=</span> <span class="fu">quantile</span>(df2<span class="sc">$</span>dis,<span class="at">probs=</span>pv)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">=</span> <span class="fu">expand.grid</span>(x1q,x2q) <span class="co">#matrix with two columns using all combinations of x1q and x2q</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>dfpred <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">dis=</span>xx[,<span class="dv">2</span>],<span class="at">lstat=</span>xx[,<span class="dv">1</span>])</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>lmedpred <span class="ot">=</span> <span class="fu">predict</span>(boston.tree,dfpred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Make perspective plot</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a perspective plot (3D surface)</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># theta sets the viewing angle, zlim ensures the vertical axis covers the data range</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">persp</span>(x1q, x2q, <span class="fu">matrix</span>(lmedpred, <span class="at">ncol=</span><span class="fu">length</span>(x2q), <span class="at">byrow=</span>T),</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>      <span class="at">theta=</span><span class="dv">150</span>, <span class="at">xlab=</span><span class="st">'dis'</span>, <span class="at">ylab=</span><span class="st">'lstat'</span>, <span class="at">zlab=</span><span class="st">'medv'</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">zlim=</span><span class="fu">c</span>(<span class="fu">min</span>(df2<span class="sc">$</span>medv), <span class="fl">1.1</span><span class="sc">*</span><span class="fu">max</span>(df2<span class="sc">$</span>medv)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p><em>Advantages of Decision Trees</em>: Decision trees are incredibly intuitive and simple to explain, often even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods we’ve discussed in previous chapters. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics, particularly when the trees are not overly complex. Additionally, decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.</p>
<p><em>Disadvantages of Decision Trees</em>: Large trees can exhibit high variance, meaning that a minor change in the data can lead to a significant change in the final estimated tree, making the model unstable. Conversely, small trees, while more stable, may not be powerful predictors as they might oversimplify the problem. It can be challenging to find a balance between bias and variance when using decision trees. A model with too much bias oversimplifies the problem and performs poorly, while a model with too much variance overfits the data and may not generalize well to unseen data.</p>
<p>There are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier, and hence improve predictive accuracy by reducing overfitting. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.</p>
<p>In the <strong>bagging</strong> approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.</p>
<p>To bootsrap aggregate (Bag) we:</p>
<ul>
<li>Take <span class="math inline">\(B\)</span> bootstrap samples from the training data, each of the same size as the training data.</li>
<li>Fit a large tree to each bootstrap sample (we know how to do this fast!). This will give us <span class="math inline">\(B\)</span> trees.</li>
<li>Combine the results from each of the B trees to get an overall prediction.</li>
</ul>
<p>When the target variable <span class="math inline">\(y\)</span> is numeric, the bagging process is straightforward. The final prediction is simply the average of the predictions from each of the <span class="math inline">\(B\)</span> trees. However, when <span class="math inline">\(y\)</span> is categorical, the process of combining results from different trees is less straightforward. One common approach is to use a voting system. In this system, each tree in the ensemble makes a prediction for a given input <span class="math inline">\(x\)</span>. The predicted category that receives the most votes (out of <span class="math inline">\(B\)</span> total votes) is chosen as the final prediction. Another approach is to average the predicted probabilities <span class="math inline">\(\hat p\)</span> from each tree. This method can provide a more nuanced prediction, especially in cases where the voting results are close.</p>
<p>Despite the potential benefits of averaging predicted probabilities, most software implementations of bagging for decision trees use the voting method. This preference stems from its computational simplicity and intuitive alignment with the democratic process. However, the best method to use can depend on the specific characteristics of the problem at hand.</p>
<p>The simple idea behind every ensemble method is that the variance of the average is lower than the variance of individual models. Say we have <span class="math inline">\(B\)</span> models <span class="math inline">\(f_1(x),\ldots,f_B(x)\)</span> then we combine those <span class="math display">\[
f_{avg}(x) = \dfrac{1}{B}\sum_{b=1}^Bf_b(x)
\]</span> Combining models helps fight overfitting. On the negative side, it is harder to interpret these ensembles</p>
<p>Let’s experiment with the number of trees in the model</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Boston)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>ntreev <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">500</span>,<span class="dv">5000</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>fmat <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>,n,<span class="dv">3</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a> rffit <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>lstat,<span class="at">data=</span>Boston,<span class="at">ntree=</span>ntreev[i],<span class="at">maxnodes=</span><span class="dv">15</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a> fmat[,i] <span class="ot">=</span> <span class="fu">predict</span>(rffit)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a> <span class="fu">print</span>(<span class="fu">mean</span>((fmat[,i] <span class="sc">-</span> Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>, <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 30
 29
 29</code></pre>
</div>
</div>
<p>Let’s plot the results</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>oo <span class="ot">=</span> <span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>) {</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a> <span class="fu">plot</span>(Boston<span class="sc">$</span>lstat,Boston<span class="sc">$</span>medv,<span class="at">xlab=</span><span class="st">'lstat'</span>,<span class="at">ylab=</span><span class="st">'medv'</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a> <span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],fmat[oo,i],<span class="at">col=</span>i<span class="sc">+</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a> <span class="fu">title</span>(<span class="at">main=</span><span class="fu">paste</span>(<span class="st">'bagging ntrees = '</span>,ntreev[i]))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="3" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-17-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li>With 10 trees our fit is too jumbly.</li>
<li>With 1,000 and 5,000 trees the fit is not bad and very similar.</li>
<li>Note that although our method is based multiple trees (average over) so we no longer have a simple step function!!</li>
</ul>
</section>
<section id="random-forest" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">14.4</span> Random Forest</h2>
<p>In the bagging technique, models can become correlated, which prevents the achievement of a <span class="math inline">\(1/n\)</span> reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.</p>
<p>Random Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees, making the ensemble more robust and improving prediction accuracy. This randomness comes into play when considering a split in a tree. Instead of considering all <span class="math inline">\(p\)</span> predictors for a split, a random sample of <span class="math inline">\(m\)</span> predictors is chosen as split candidates. This subset of predictors is different for each split, which means that different trees are likely to use different predictors in the top split, leading to a more diverse set of trees.</p>
<p>The number of predictors considered at each split, <span class="math inline">\(m\)</span>, is typically chosen to be the square root of the total number of predictors, <span class="math inline">\(p\)</span>. This choice is a rule of thumb that often works well in practice, but it can be tuned based on the specific characteristics of the dataset.</p>
<p>By decorrelating the trees, Random Forests can often achieve better performance than bagging, especially when there’s a small number of very strong predictors in the dataset. In such cases, bagging can end up with an ensemble of very similar trees that all rely heavily on these strong predictors, while Random Forests can leverage the other, weaker predictors more effectively.</p>
<p>One of the “interpretation” tools that comes with ensemble models is importance ranking: the total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all trees</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">4</span>,<span class="at">importance=</span><span class="cn">TRUE</span>,<span class="at">ntree=</span><span class="dv">50</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston,<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"lightblue"</span>,<span class="at">main=</span><span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rf.boston <span class="ot">=</span> <span class="fu">randomForest</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">mtry=</span><span class="dv">6</span>,<span class="at">ntree=</span><span class="dv">50</span>, <span class="at">maxnodes=</span><span class="dv">50</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>yhat.rf <span class="ot">=</span> <span class="fu">predict</span>(rf.boston,<span class="at">newdata=</span>Boston)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>oo<span class="ot">=</span><span class="fu">order</span>(Boston<span class="sc">$</span>lstat)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>lstat[oo],Boston<span class="sc">$</span>medv[oo],<span class="at">pch=</span><span class="dv">21</span>,<span class="at">bg=</span><span class="st">"grey"</span>, <span class="at">xlab=</span><span class="st">"lstat"</span>, <span class="at">ylab=</span><span class="st">"medv"</span>) <span class="co">#plot data</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(Boston<span class="sc">$</span>lstat[oo],yhat.rf[oo],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">3</span>) <span class="co">#step function fit</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="boosting" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="boosting"><span class="header-section-number">14.5</span> Boosting</h2>
<p>Boosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.</p>
<p>Here’s how Boosting works:</p>
<ol type="1">
<li>Initially, a single decision tree is fitted to the data.</li>
<li>This initial tree is intentionally made weak, meaning it doesn’t perfectly fit the data.</li>
<li>We then examine the residuals, which represent the portion of the target variable <span class="math inline">\(y\)</span> not explained by the weak tree.</li>
<li>A new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.</li>
<li>This new tree is also “weakened” or “shrunk”. The prediction from this tree is then added to the prediction of the first tree.</li>
<li>This process is repeated iteratively. In each iteration, a new tree is fitted to the residuals of the current ensemble of trees, shrunk, and then added to the ensemble.</li>
<li>The final model is the sum of all these “shrunk” trees. The key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals). Each new tree is trying to correct the mistakes of the ensemble of previous trees. By adding together many weak models (the shrunk trees), Boosting can often achieve a strong overall model.</li>
</ol>
<p>Pick a loss function <span class="math inline">\(\mathcal{L}\)</span> that reflects setting; e.g., for continuous <span class="math inline">\(y\)</span>, could take <span class="math inline">\(\mathcal{L}(y_i , \theta_i ) = (y_i - \theta_i )^2\)</span> Want to solve <span class="math display">\[\mathrm{minimize}_{\beta \in R^M} \sum_{i=1}^n \mathcal{L} \left(y_i, \sum_{j=1}^M \beta_j \cdot T_j(x_i)\right)\]</span></p>
<ul>
<li>Indexes all trees of a fixed size (e.g., depth = 5), so <span class="math inline">\(M\)</span> is huge</li>
<li>Space is simply too big to optimize</li>
<li>Gradient boosting: basically a version of gradient descent that is forced to work with trees</li>
<li>First think of optimization as <span class="math inline">\(\min_\theta f (\theta)\)</span>, over predicted values <span class="math inline">\(\theta\)</span> (subject to <span class="math inline">\(\theta\)</span> coming from trees)</li>
</ul>
<!-- ## Boosting -->
<!-- Start with initial model, e.g., fit a single tree $\theta^{(0)} = T_0$. Repeat: -->
<!-- - Evaluate gradient $g$ at latest prediction $\theta^{(k-1)}$, -->
<!--        $$g_i = \left.\left[\frac{\partial \mathcal{L}(y_i, \theta_i)}{\partial \theta_i}\right]\right|_{\theta_i = \theta_i^{(k-1)}},\ i=1,\ldots,n$$ -->
<!-- - Find a tree $T_k$ that is close to $-g$, i.e., $T_k$ solves -->
<!--        $$\mathrm{minimize}_{\text{trees }T} \sum_{i=1}^n (-g_i - T(x_i))^2$$ -->
<!--        Not hard to (approximately) solve for a single tree -->
<!-- - Update our prediction: -->
<!--        $$\theta^{(k)} = \theta^{(k-1)} + \alpha_k \cdot T_k$$ -->
<!--        Note: predictions are weighted sums of trees, as desired -->
<p>Set <span class="math inline">\(f_1(x)=0\)</span> (constant predictor) and <span class="math inline">\(r_i=y_i\)</span></p>
<p>For <span class="math inline">\(b=1,2,\ldots,B\)</span></p>
<ol type="a">
<li>Fit a tree <span class="math inline">\(f_b\)</span> with <span class="math inline">\(d\)</span> splits to the training set <span class="math inline">\((X,r)\)</span></li>
<li>Update the model <span class="math display">\[f(x) = f(x) +\lambda f_b(x)\]</span></li>
<li>Update the residuals <span class="math display">\[r_i=r_i - \lambda f_b(x)\]</span></li>
</ol>
<p>The parameter <span class="math inline">\(\lambda\)</span> is the <strong>learning rate</strong> or <em>shrinkage</em> parameter. By multiplying the new tree’s contribution by a small number (e.g., 0.01 or 0.1), we prevent the model from adapting too quickly to outliers or noise. We take many small steps towards the solution rather than a few large ones, which generally leads to better generalization.</p>
<p>Here are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = <span class="math inline">\(\lambda\)</span> at .2.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>boost.boston<span class="ot">=</span><span class="fu">gbm</span>(medv<span class="sc">~</span>.,<span class="at">data=</span>Boston,<span class="at">distribution=</span><span class="st">"gaussian"</span>,<span class="at">n.trees=</span><span class="dv">5000</span>,<span class="at">interaction.depth=</span><span class="dv">4</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>yhat.boost<span class="ot">=</span><span class="fu">predict</span>(boost.boston,<span class="at">newdata=</span>Boston,<span class="at">n.trees=</span><span class="dv">5000</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost<span class="sc">-</span>Boston<span class="sc">$</span>medv)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 4e-04</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boost.boston, <span class="at">plotit=</span><span class="cn">FALSE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="kable-table">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">var</th>
<th style="text-align: right;">rel.inf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">lstat</td>
<td style="text-align: left;">lstat</td>
<td style="text-align: right;">36.32</td>
</tr>
<tr class="even">
<td style="text-align: left;">rm</td>
<td style="text-align: left;">rm</td>
<td style="text-align: right;">30.98</td>
</tr>
<tr class="odd">
<td style="text-align: left;">dis</td>
<td style="text-align: left;">dis</td>
<td style="text-align: right;">7.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">crim</td>
<td style="text-align: left;">crim</td>
<td style="text-align: right;">5.09</td>
</tr>
<tr class="odd">
<td style="text-align: left;">nox</td>
<td style="text-align: left;">nox</td>
<td style="text-align: right;">4.63</td>
</tr>
<tr class="even">
<td style="text-align: left;">age</td>
<td style="text-align: left;">age</td>
<td style="text-align: right;">4.50</td>
</tr>
<tr class="odd">
<td style="text-align: left;">black</td>
<td style="text-align: left;">black</td>
<td style="text-align: right;">3.45</td>
</tr>
<tr class="even">
<td style="text-align: left;">ptratio</td>
<td style="text-align: left;">ptratio</td>
<td style="text-align: right;">3.11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tax</td>
<td style="text-align: left;">tax</td>
<td style="text-align: right;">1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">rad</td>
<td style="text-align: left;">rad</td>
<td style="text-align: right;">1.17</td>
</tr>
<tr class="odd">
<td style="text-align: left;">indus</td>
<td style="text-align: left;">indus</td>
<td style="text-align: right;">0.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">chas</td>
<td style="text-align: left;">chas</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr class="odd">
<td style="text-align: left;">zn</td>
<td style="text-align: left;">zn</td>
<td style="text-align: right;">0.13</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"rm"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(boost.boston,<span class="at">i=</span><span class="st">"lstat"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="14-tree_files/figure-html/unnamed-chunk-22-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p>Advantages of Boosting over Random Forests include performance and model interpretability. Boosting, in many cases, provides better predictive accuracy than Random Forests. By focusing on the residuals or mistakes, Boosting can incrementally improve model performance. While both methods are not as interpretable as a single decision tree, Boosting models can sometimes be more interpretable than Random Forests, especially when the number of weak learners (trees) is small.</p>
<p>Disadvantages of Boosting compared to Random Forests include computational complexity, overfitting potential, and sensitivity to outliers and noise. Boosting can be more computationally intensive than Random Forests because trees are built sequentially in Boosting, while in Random Forests they are built independently and can be parallelized. Boosting can overfit the training data if the number of trees is too large or if the trees are too complex. This is less of a problem with Random Forests, which are less prone to overfitting due to the randomness injected into the tree building process. Boosting can also be sensitive to outliers since it tries to correct the mistakes of the predecessors, while Random Forests are more robust to outliers. Finally, Boosting can overemphasize instances that are hard to classify and can overfit to noise, whereas Random Forests are more robust to noise.</p>
<p>Remember, the choice between Boosting and Random Forests (or any other model) should be guided by the specific requirements of your task, including the nature of your data, the computational resources available, and the trade-off between interpretability and predictive accuracy.</p>
</section>
<section id="why-ensembles-work-a-geometric-perspective" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="why-ensembles-work-a-geometric-perspective"><span class="header-section-number">14.6</span> Why Ensembles Work: A Geometric Perspective</h2>
<p>The ensemble methods we’ve discussed—bagging, random forests, and boosting—all share a common goal: improving predictive performance by combining multiple weak learners. But why does this work so well? The answer lies in the <strong>bias-variance tradeoff</strong> and the geometry of high-dimensional spaces.</p>
<section id="smoothing-and-variance-reduction" class="level3">
<h3 class="anchored" data-anchor-id="smoothing-and-variance-reduction">Smoothing and Variance Reduction</h3>
<p>In essence, individual decision trees are low-bias but high-variance estimators. They can capture complex patterns (low bias) but are sensitive to noise (high variance). When we average many such trees, as in Bagging or Random Forests, we are effectively performing <strong>smoothing</strong>.</p>
<p>Quantitatively, if we have <span class="math inline">\(N\)</span> uncorrelated predictors <span class="math inline">\(f_1, \dots, f_N\)</span>, the variance of their average is: <span class="math display">\[
\text{Var}\left(\frac{1}{N}\sum_{i=1}^N f_i\right) = \frac{1}{N} \text{Var}(f_i)
\]</span> Averaging <span class="math inline">\(N\)</span> uncorrelated models reduces the variance by a factor of <span class="math inline">\(N\)</span>. In practice, the trees in a forest are correlated, so the reduction isn’t quite <span class="math inline">\(1/N\)</span>, but the principle holds: the more diverse (uncorrelated) the trees, the more variance reduction we get. This is why Random Forests Randomly select features at each split—to force the trees to be different.</p>
</section>
<section id="the-problem-of-high-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-high-dimensions">The Problem of High Dimensions</h3>
<p>One might ask: “Why not just use K-Nearest Neighbors (KNN)? It also averages local points.” The problem is the <strong>curse of dimensionality</strong>. In high-dimensional feature spaces, data points become incredibly sparse.</p>
<p>Consider a 50-dimensional sphere. As shown in <a href="#fig-hd-ball" class="quarto-xref">Figure&nbsp;<span>14.4</span></a>, if we sample points uniformly, almost all of them will reside near the “crust” or surface of the sphere, not in the center.</p>
<div id="fig-hd-ball" class="quarto-float quarto-figure quarto-figure-center anchored" width="40%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hd_ball.svg" id="fig-hd-ball" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2
</figcaption>
</figure>
</div>
<p>This phenomenon means that in high dimensions, “local neighbors” are not actually close to you—they are far away on the other side of the space. A standard KNN algorithm fails because it averages points that aren’t truly similar.</p>
</section>
<section id="trees-as-adaptive-nearest-neighbors" class="level3">
<h3 class="anchored" data-anchor-id="trees-as-adaptive-nearest-neighbors">Trees as “Adaptive” Nearest Neighbors</h3>
<p>Decision trees solve this by defining “neighbors” differently. Instead of using a fixed distance metric (like Euclidean distance), trees define a neighborhood as a rectangular box (or <strong>cylindrical region</strong>) learned from the data (<a href="#fig-cilinder" class="quarto-xref">Figure&nbsp;<span>14.3</span></a>).</p>
<div id="fig-cilinder" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/cylinder_kernel.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/rf_kernel.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cilinder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Cylindrical kernels for trees (left) and random forests (right).
</figcaption>
</figure>
</div>
<p>Constructing the regions is fundamental to reducing the curse of dimensionality. It is useful to imagine a very large dataset, e.g., 100k images, and think about how a new image’s input coordinates, <span class="math inline">\(X\)</span>, are “neighbors” to data points in the training set. Our predictor will then be a smart conditional average of the observed outputs, <span class="math inline">\(Y\)</span>, for our neighbors. When <span class="math inline">\(p\)</span> is large, spheres (<span class="math inline">\(L^2\)</span> balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.</p>
<p>To illustrate the problem further, <a href="#fig-hd-ball" class="quarto-xref">Figure&nbsp;<span>14.4</span></a> below shows the 2D image of 1000 uniform samples from a 50-dimensional ball <span class="math inline">\(B_{50}\)</span>. The image is calculated as <span class="math inline">\(w^T Y\)</span>, where <span class="math inline">\(w = (1,1,0,\ldots,0)\)</span> and <span class="math inline">\(Y \sim U(B_{50})\)</span>. Samples are centered around the equators and none of the samples fall close to the boundary of the set.</p>
<div id="fig-hd-ball" class="quarto-float quarto-figure quarto-figure-center anchored" width="40%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hd_ball.svg" id="fig-hd-ball" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-hd-ball-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4
</figcaption>
</figure>
</div>
<p>As dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from <a href="#fig-hd-hist" class="quarto-xref">Figure&nbsp;<span>14.5</span></a>, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e.&nbsp;<span class="math inline">\(e_1^T Y\)</span>, where <span class="math inline">\(e_1 = (1,0,\ldots,0)\)</span>.</p>
<div id="fig-hd-hist" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_100.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_200.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_300.svg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<p><img src="fig/hd_hist_400.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hd-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.5: Histogram of marginal distribution of <span class="math inline">\(Y\sim U(B_p)\)</span> for different dimensions <span class="math inline">\(p\)</span> (x-axis).
</figcaption>
</figure>
</div>
<p>Similar central limit results were known to Maxwell who showed that random variable <span class="math inline">\(w^TY\)</span> is close to standard normal, when <span class="math inline">\(Y \sim U(B_p)\)</span>, <span class="math inline">\(p\)</span> is large, and <span class="math inline">\(w\)</span> is a unit vector (lies on the boundary of the ball). For the history of this fact, see <span class="citation" data-cites="diaconis1987dozen">Diaconis and Freedman (<a href="references.html#ref-diaconis1987dozen" role="doc-biblioref">1987</a>)</span>. More general results in this direction were obtained in <span class="citation" data-cites="klartag2007central">Klartag (<a href="references.html#ref-klartag2007central" role="doc-biblioref">2007</a>)</span>. Further, <span class="citation" data-cites="milman2009asymptotic">Milman and Schechtman (<a href="references.html#ref-milman2009asymptotic" role="doc-biblioref">2009</a>)</span> presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.</p>
<p>Deep learning improves on this by performing a sequence of GLM-like transformations; effectively, DL learns a distributed partition of the input space. Specifically, suppose that we have <span class="math inline">\(K\)</span> partitions. Then the DL predictor takes the form of a weighted average, or in the case of classification, a soft-max of the weighted average of observations in this partition. Given a new high-dimensional input <span class="math inline">\(X_{\mathrm{new}}\)</span>, many deep learners are an average of learners obtained by our hyperplane decomposition. Generically, we have</p>
<p><span class="math display">\[
\hat{Y}(X) = \sum_{k \in K} w_k(X)\hat{Y}_k(X),
\]</span> where <span class="math inline">\(w_k\)</span> are the weights learned in region <span class="math inline">\(k\)</span>, and <span class="math inline">\(w_k(X)\)</span> is an indicator of the region with appropriate weighting given the training data. The weight <span class="math inline">\(w_k\)</span> also indicates which partition the new <span class="math inline">\(X_{\mathrm{new}}\)</span> lies in.</p>
<p>The use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form of clever conditional averaging) are prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, with the caveat that they have large variances due to dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the <span class="math inline">\(1/N\)</span>-rule and average to reduce risk. Specifically, suppose that we have <span class="math inline">\(K\)</span> exchangeable, <span class="math inline">\(\mathbb{E} ( \hat{Y}_i ) = \mathbb{E} ( \hat{Y}_{\pi(i)} )\)</span>, predictors</p>
<p><span class="math display">\[
\hat{Y} = ( \hat{Y}_1 , \ldots , \hat{Y}_K )
\]</span></p>
<p>Find <span class="math inline">\(w\)</span> to attain <span class="math inline">\(\operatorname{argmin}_W E l( Y , w^T \hat{Y} )\)</span> where <span class="math inline">\(l\)</span> convex in the second argument;</p>
<p><span class="math display">\[
E l( Y , w^T \hat{Y} ) = \frac{1}{K!} \sum_\pi E l( Y , w^T \hat{Y} ) \geq E l \left ( Y , \frac{1}{K!} \sum_\pi w_\pi^T \hat{Y} )\right ) = E l \left ( Y , (1/K) \iota^T \hat{Y} \right )
\]</span></p>
<p>where <span class="math inline">\(\iota = ( 1 , \ldots ,1 )\)</span>. Hence, the randomized multiple predictor with weights <span class="math inline">\(w = (1/K)\iota\)</span> provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.</p>
<p>An alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule. We formalize the gains in Classification Risk with the following discussion.</p>
</section>
</section>
<section id="theoretical-foundations-why-trees-work" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="theoretical-foundations-why-trees-work"><span class="header-section-number">14.7</span> Theoretical Foundations: Why Trees Work</h2>
<p>The success of tree-based ensembles rests on solid theoretical foundations. Two key principles explain their power: the variance reduction from averaging (the <span class="math inline">\(1/N\)</span> rule) and the surprising efficiency of nearest-neighbor methods.</p>
<section id="ensemble-averaging-and-the-1n-rule" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-averaging-and-the-1n-rule">Ensemble Averaging and the <span class="math inline">\(1/N\)</span> Rule</h3>
<p>In high dimensions, predicting with a single model can be unstable (high variance). When we have <span class="math inline">\(N\)</span> uncorrelated predictors <span class="math inline">\(f_1, \dots, f_N\)</span>, the variance of their average is reduced by a factor of <span class="math inline">\(N\)</span>: <span class="math display">\[
\text{Var}\left(\frac{1}{N}\sum_{i=1}^N f_i\right) = \frac{1}{N} \text{Var}(f_i) + \frac{N-1}{N}\rho\sigma^2
\]</span> This is the <strong><span class="math inline">\(1/N\)</span> rule</strong>: averaging independent weak learners dramatically reduces error. The variance of an ensemble depends on the correlation <span class="math inline">\(\rho\)</span> between trees. The optimality of this rule is rooted in the exchangeability of weak predictors (<span class="citation" data-cites="polson2017deep">Polson, Sokolov, et al. (<a href="references.html#ref-polson2017deep" role="doc-biblioref">2017</a>)</span>).</p>
</section>
<section id="classification-variance-decomposition" class="level3">
<h3 class="anchored" data-anchor-id="classification-variance-decomposition">Classification variance decomposition</h3>
<p><span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span> provide a rigorous connection between the strength of individual classifiers and their correlation. They use the population conditional probability distribution of a point <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=c\)</span>, denoted by <span class="math inline">\(P_{c}\)</span>, and the associated conditional expectation and variance operators will be denoted <span class="math inline">\(E_{c}\)</span> and <span class="math inline">\(V a r_{c}\)</span>. Define the vectors of average aggregates conditional on class <span class="math inline">\(c\)</span> as <span class="math display">\[
M_{c}(d)=E_{c}\left[H_{\mathbf{Q}}(X, d)\right]=E\left[H_{\mathbf{Q}}(X, d) \mid Y=c\right]
\]</span></p>
<p>for <span class="math inline">\(d=1, \ldots, K\)</span>. Intuitively, <span class="math inline">\(M_c(d)\)</span> represents the <strong>expected vote share</strong> for class <span class="math inline">\(d\)</span> when the true class is <span class="math inline">\(c\)</span>. Ideally, <span class="math inline">\(M_c(c)\)</span> (votes for the correct class) should be much larger than <span class="math inline">\(M_c(d)\)</span> (votes for incorrect classes).</p>
<p>The average conditional margin (ACM) for class <span class="math inline">\(c\)</span> is defined as <span class="math display">\[
\theta_{c}=\min _{d \neq c}\left(M_{c}(c)-M_{c}(d)\right)
\]</span> This <span class="math inline">\(\theta_c\)</span> measures the “safety gap” between the correct class and the closest competing class. A larger margin means the classifier is more robust.</p>
<p>We assume that <span class="math inline">\(\theta_{c}&gt;0\)</span>. This assumption is very weak since it involves only the average over the population of class <span class="math inline">\(c\)</span>. It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.</p>
<p>Given that <span class="math inline">\(\theta_{c}&gt;0\)</span>, the error rate for class <span class="math inline">\(c\)</span> depends on the extent to which the aggregate classifier <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> is concentrated around <span class="math inline">\(M_{c}(d)\)</span> for each <span class="math inline">\(d=1, \ldots, K\)</span>. The simplest measure of concentration is the variance of <span class="math inline">\(H_{\mathbf{Q}}(X, d)\)</span> with respect to the distribution <span class="math inline">\(P_{c}\)</span>. Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to <span class="math inline">\(P_{c}\)</span> as follows.</p>
<p><span class="math display">\[\begin{align*}
P_{c}\left(C_{\mathbf{Q}}(X) \neq c\right) \leq &amp; P_{c}\left(H_{\mathbf{Q}}(X, c)&lt;M_{c}(c)-\theta_{c} / 2\right) \\
&amp; +\sum_{d \neq c} P_{c}\left(H_{\mathbf{Q}}(X, d)&gt;M_{c}(d)+\theta_{c} / 2\right) \\
\leq &amp; \sum_{d=1}^{K} P_{c}\left(\left|H_{\mathbf{Q}}(X, d)-M_{c}(d)\right|&gt;\theta_{c} / 2\right) \\
\leq &amp; \frac{4}{\theta_{c}^{2}} \sum_{d=1}^{K} \operatorname{Var}_{c}\left[H_{\mathbf{Q}}(X, d)\right] . \tag{10}
\end{align*}\]</span></p>
<p>Of course Chebyshev’s inequality is coarse and will not give very sharp results in itself, but we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities. To reduce error, we must either <strong>increase the margin</strong> <span class="math inline">\(\theta_c\)</span> (build stronger classifiers) or <strong>decrease the variance</strong> (add more diverse trees).</p>
<p>We rewrite each of the variance terms of the last equation as</p>
<p><span class="math display">\[\begin{align*}
\operatorname{Var}_{c}\left[E_{\mathbf{Q}} h(X, d)\right] &amp; =E_{c}\left[E_{\mathbf{Q}} h(X, d)\right]^{2}-\left[E_{c} E_{\mathbf{Q}} h(X, d)\right]^{2} \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} E_{c}\left[h_{1}(X, d) h_{2}(X, d)\right]-E_{\mathbf{Q} \otimes \mathbf{Q}}\left[E_{c}\left[h_{1}(X, d)\right] E_{c}\left[h_{2}(X, d)\right]\right] \\
&amp; =E_{\mathbf{Q} \otimes \mathbf{Q}} \operatorname{Cov}_{c}\left[h_{1}(X, d), h_{2}(X, d)\right] \doteq \gamma_{c, d} \tag{11}
\end{align*}\]</span></p>
<p>This equation formalizes the intuition: to minimize error, we must minimize the covariance <span class="math inline">\(\gamma_{c, d}\)</span> between trees. This is precisely what <strong>Random Forests</strong> do by randomizing splits (<span class="citation" data-cites="amit2000multiple">Amit, Blanchard, and Wilder (<a href="references.html#ref-amit2000multiple" role="doc-biblioref">2000</a>)</span>).</p>
</section>
<section id="conditional-and-unconditional-dependence" class="level3">
<h3 class="anchored" data-anchor-id="conditional-and-unconditional-dependence">Conditional and unconditional dependence</h3>
<p>It should be emphasized that two good classifiers will not be unconditionally independent. Since they both aim to predict the true class <span class="math inline">\(Y\)</span>, they will be correlated with <span class="math inline">\(Y\)</span> and thus with each other. If we do not know the class label of a point, knowing one classifier’s output effectively changes our expectation of the other. However, what matters for variance reduction is <strong>conditional independence</strong>: given the class label, do the classifiers make <em>independent errors</em>? This is the relevant quantity for ensemble performance.</p>
<p>This is in contrast to the measure of dependence introduced in Dietterich (1998), which involves the unconditional covariance via the <span class="math inline">\(\kappa\)</span> statistic:</p>
<p><span class="math display">\[
\kappa\left(h_{1}, h_{2}\right)=\frac{\sum_{d} \operatorname{Cov}\left[h_{1}(X, d), h_{2}(X, d)\right]}{1-\sum_{d} E h_{1}(X, d) E h_{2}(X, d)},
\]</span></p>
</section>
<section id="the-nearest-neighbor-insight" class="level3">
<h3 class="anchored" data-anchor-id="the-nearest-neighbor-insight">The Nearest Neighbor Insight</h3>
<p>Why do trees work so well on complex data? <span class="citation" data-cites="cover1967nearest">Cover and Hart (<a href="references.html#ref-cover1967nearest" role="doc-biblioref">1967</a>)</span> provided a fundamental result for nearest-neighbor classifiers: as sample size grows, the error rate of a simple 1-Nearest Neighbor classifier is bounded by <strong>twice the Bayes Risk</strong> (the irreducible error). <span class="math display">\[
R^* \le R_{NN} \le 2R^*(1-R^*)
\]</span> This means that a simple “look at your neighbor” strategy captures at least half the available signal. Decision trees can be viewed as <strong>adaptive nearest-neighbor</strong> models. Instead of using a fixed distance metric (which fails in high dimensions due to sparsity), trees learn a custom metric—dividing space into “blocky” regions based only on relevant features.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">14.8</span> Conclusion</h2>
<p>Tree-based models are among the most versatile tools in a data scientist’s toolkit. Single decision trees offer unmatched interpretability but suffer from instability. Ensemble methods like Random Forests and Boosting overcome this by combining many trees to reduce variance and bias, respectively.</p>
<ul>
<li><strong>Decision Trees</strong>: Interpretability champions, but prone to overfitting.</li>
<li><strong>Random Forests</strong>: Robust, parallelizable, and easy to tune. Great “out-of-the-box” performance.</li>
<li><strong>Boosting</strong>: High accuracy, sequential learning. Requires more careful tuning but often wins competitions.</li>
</ul>
<p>By understanding the theoretical foundations—whether through the geometry of high-dimensional spaces or the decomposition of variance—we can better appreciate why simple averaging of “weak” trees leads to such powerful “strong” predictors.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-amit2000multiple" class="csl-entry" role="listitem">
Amit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. <span>“Multiple Randomized Classifiers: <span>MRCL</span>.”</span>
</div>
<div id="ref-cover1967nearest" class="csl-entry" role="listitem">
Cover, T., and P. Hart. 1967. <span>“Nearest Neighbor Pattern Classification.”</span> <em>IEEE Transactions on Information Theory</em> 13 (1): 21–27.
</div>
<div id="ref-diaconis1987dozen" class="csl-entry" role="listitem">
Diaconis, Persi, and David Freedman. 1987. <span>“A Dozen de <span class="nocase">Finetti-style</span> Results in Search of a Theory.”</span> In <em>Annales de l’<span>IHP</span> Probabilités Et Statistiques</em>, 23:397–423.
</div>
<div id="ref-klartag2007central" class="csl-entry" role="listitem">
Klartag, Bo’az. 2007. <span>“A Central Limit Theorem for Convex Sets.”</span> <em>Inventiones Mathematicae</em> 168 (1): 91–131.
</div>
<div id="ref-milman2009asymptotic" class="csl-entry" role="listitem">
Milman, Vitali D, and Gideon Schechtman. 2009. <em>Asymptotic Theory of Finite Dimensional Normed Spaces: <span>Isoperimetric</span> Inequalities in Riemannian Manifolds</em>. Vol. 1200. Springer.
</div>
<div id="ref-polson2017deep" class="csl-entry" role="listitem">
Polson, Nicholas G, Vadim Sokolov, et al. 2017. <span>“Deep <span>Learning</span>: <span>A Bayesian Perspective</span>.”</span> <em>Bayesian Analysis</em> 12 (4): 1275–1304.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./13-logistic.html" class="pagination-link" aria-label="Logistic Regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./15-forecasting.html" class="pagination-link" aria-label="Forecasting">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>