<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-bayes.html" rel="next">
<link href="./00-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="01-prob_files/figure-html/unnamed-chunk-1-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="01-prob_files/figure-html/unnamed-chunk-1-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#odds-as-probabilities" id="toc-odds-as-probabilities" class="nav-link active" data-scroll-target="#odds-as-probabilities"><span class="header-section-number">1.1</span> Odds as Probabilities</a></li>
  <li><a href="#random-variables-quantifying-uncertainty" id="toc-random-variables-quantifying-uncertainty" class="nav-link" data-scroll-target="#random-variables-quantifying-uncertainty"><span class="header-section-number">1.2</span> Random Variables: Quantifying Uncertainty</a>
  <ul class="collapse">
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables">Continuous Random Variables</a></li>
  <li><a href="#the-inverse-cdf-method" id="toc-the-inverse-cdf-method" class="nav-link" data-scroll-target="#the-inverse-cdf-method">The Inverse CDF Method</a></li>
  <li><a href="#functional-transformations" id="toc-functional-transformations" class="nav-link" data-scroll-target="#functional-transformations">Functional Transformations</a></li>
  </ul></li>
  <li><a href="#expectation-and-variance-reward-and-risk" id="toc-expectation-and-variance-reward-and-risk" class="nav-link" data-scroll-target="#expectation-and-variance-reward-and-risk"><span class="header-section-number">1.3</span> Expectation and Variance (Reward and Risk)</a>
  <ul class="collapse">
  <li><a href="#standard-deviation-and-covariance" id="toc-standard-deviation-and-covariance" class="nav-link" data-scroll-target="#standard-deviation-and-covariance">Standard Deviation and Covariance</a></li>
  <li><a href="#portfolios-linear-combinations" id="toc-portfolios-linear-combinations" class="nav-link" data-scroll-target="#portfolios-linear-combinations">Portfolios: linear combinations</a></li>
  </ul></li>
  <li><a href="#limiting-behavior-of-averages" id="toc-limiting-behavior-of-averages" class="nav-link" data-scroll-target="#limiting-behavior-of-averages"><span class="header-section-number">1.4</span> Limiting Behavior of Averages</a>
  <ul class="collapse">
  <li><a href="#the-weak-law-of-large-numbers" id="toc-the-weak-law-of-large-numbers" class="nav-link" data-scroll-target="#the-weak-law-of-large-numbers">The Weak Law of Large Numbers</a></li>
  <li><a href="#kolmogorovs-strong-law-of-large-numbers" id="toc-kolmogorovs-strong-law-of-large-numbers" class="nav-link" data-scroll-target="#kolmogorovs-strong-law-of-large-numbers">Kolmogorov’s Strong Law of Large Numbers</a></li>
  </ul></li>
  <li><a href="#binomial-poisson-and-normal-distributions" id="toc-binomial-poisson-and-normal-distributions" class="nav-link" data-scroll-target="#binomial-poisson-and-normal-distributions"><span class="header-section-number">1.5</span> Binomial, Poisson, and Normal Distributions</a>
  <ul class="collapse">
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution">Bernoulli Distribution</a></li>
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution">Poisson Distribution</a></li>
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution">Normal Distribution</a></li>
  </ul></li>
  <li><a href="#conditional-marginal-and-joint-distributions" id="toc-conditional-marginal-and-joint-distributions" class="nav-link" data-scroll-target="#conditional-marginal-and-joint-distributions"><span class="header-section-number">1.6</span> Conditional, Marginal and Joint Distributions</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence"><span class="header-section-number">1.7</span> Independence</a></li>
  <li><a href="#sec-dutch-book" id="toc-sec-dutch-book" class="nav-link" data-scroll-target="#sec-dutch-book"><span class="header-section-number">1.8</span> Further Notes: Dutch Book Arguments</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-prob" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><em>“It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge…”</em> Pierre-Simon Laplace, 1812</p>
</blockquote>
<p>Probability deals with randomness and provides a language to communicate uncertainty, which is usually associated with our lack of knowledge or information. In the classical coin toss, for instance, if we knew the exact force applied, we could predict the outcome with certainty. However, practically it is never the case and we treat coin toss outcome as random.</p>
<p>Assigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (observed data). Consider the coin-tossing example. Say event <span class="math inline">\(A\)</span> represents a Head. Then, to empirically estimate probability of event <span class="math inline">\(A\)</span>, <span class="math inline">\(P(A)\)</span>, we can repeat the tosses experiment <span class="math inline">\(N\)</span> times and count <span class="math inline">\(n\)</span>, the number of times <span class="math inline">\(A\)</span> occurred. The plot below shows the proportion of heads after <span class="math inline">\(N\)</span> trials.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Graph of the proportion of heads after <span class="math inline">\(N\)</span> trials</figcaption>
</figure>
</div>
</div>
</div>
<p>We can see that as <span class="math inline">\(N\)</span> grows, the curve converges to <span class="math inline">\(0.5\)</span>. This is the law of large numbers. When <span class="math inline">\(N\)</span> is large, <span class="math inline">\(n/N\)</span> will be close to <span class="math inline">\(P(A)\)</span>. The probability as a limit definition is natural and was proposed by von Mises. <span class="math display">\[
P(\text{Heads}) = \lim_{N \to \infty} \frac{n}{N}.
\]</span></p>
<p>However, this definition is not operational. It requires the notion of a collective, an infinite sequence of repeatable trials with random outcomes. This is an untestable assumption and was criticized by <span class="citation" data-cites="ville1939etude">Ville (<a href="references.html#ref-ville1939etude" role="doc-biblioref">1939</a>)</span>. On the other hand, Kolmogorov tried to operationalize probability by proposing tests for randomness in his work on algorithmic complexity theory.</p>
<p>We can use a relaxed definition due to Bernoulli, and define probability as simply the ratio of the number of heads to the total number of trials in a given experiment. This definition is operational and can be used to estimate the probability of an event. This definition requires the experiment to be repeated under identical conditions. If we are to repeat this experiment under different conditions, e.g.&nbsp;when an unbalanced coin is used, our estimate of <span class="math inline">\(P(A)\)</span> will change as well.</p>
<p>An alternative operational definition of probability was proposed by Frank Ramsey <span class="citation" data-cites="ramsey1926truth">(<a href="references.html#ref-ramsey1926truth" role="doc-biblioref">1926</a>)</span> and later refined by Bruno de Finetti <span class="citation" data-cites="definetti1937foresight">(<a href="references.html#ref-definetti1937foresight" role="doc-biblioref">1937</a>)</span>. Rather than relying on long-run frequencies, this approach defines probability through the lens of rational betting behavior. The key insight is that your probability assignment for an event should correspond to the odds at which you would be willing to bet on that event.</p>
<p>de Finetti and Ramsey school of thought takes probability as subjective, namely personal to the observer. De Finetti famously concluded that <em>“Probability does not exist.”</em> Measuring uncertainty is personal to the observer. It’s not like mass which is a property of an object. If two different observers have differing “news” then there is an opportunity for them to bet (exchange contracts). Thus leading to an assessment of probability.</p>
<p>For many events most people will agree on their probabilities, for example <span class="math inline">\(P(\text{Heads}) = 0.5\)</span>. In the subjective view of probability, we can measure or elicit a personal probability as a “willingness to play”. Namely, will you be willing to bet $1 so you can get $2 if the coin lands Tail and $0 if Head occurs? The subjective view of probability also leads to subjective expected utility theory. You cannot separate the two. For more details, see <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a>.</p>
<p>This book primarily adopts the Bayesian (subjective) interpretation of probability, formalized through coherence (the Dutch book principle here, and de Finetti’s representation in <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>). We still use frequentist language and long-run intuition as operational tools, but the organizing viewpoint is probabilistic modeling and belief updating.</p>
<p>Suppose you believe the probability of an event <span class="math inline">\(A\)</span> is <span class="math inline">\(p\)</span>. According to Ramsey’s definition, this means you should be indifferent between paying <span class="math inline">\(p \cdot S\)</span> dollars to receive <span class="math inline">\(S\)</span> dollars if event <span class="math inline">\(A\)</span> occurs (and nothing otherwise) and accepting <span class="math inline">\(p \cdot S\)</span> dollars to pay someone <span class="math inline">\(S\)</span> dollars if event <span class="math inline">\(A\)</span> occurs. However, not all probability assignments lead to rational behavior. Suppose someone assigns probabilities to events in an incoherent way. In that case, a clever adversary could construct a series of bets, called a <em>Dutch book</em>, where the individual is guaranteed to lose money regardless of which events occur. The requirement that probabilities must be assigned in such a way that no Dutch book can be constructed against you is known as <em>coherence</em>. For a detailed derivation of probability rules using Dutch Book arguments, see the <a href="#sec-dutch-book">Appendix</a>.</p>
<p>The principle of coherence for subjective probability is the fundamental rationality constraint, in finance that would be called no-arbitrage condition <span class="citation" data-cites="poincare1952science">Bachelier (<a href="references.html#ref-bachelier1900theorie" role="doc-biblioref">1900</a>)</span>.</p>
<p>Let us consider a simple example. Suppose you assign probability <span class="math inline">\(P(A) = 0.7\)</span> to event <span class="math inline">\(A\)</span> occurring and probability <span class="math inline">\(P(\bar A) = 0.2\)</span> to event <span class="math inline">\(A\)</span> not occurring, where <span class="math inline">\(\bar A\)</span> denotes the complement of <span class="math inline">\(A\)</span>. A Dutch book can be constructed as follows:</p>
<ul>
<li>Bet 1: Pay <span class="math inline">\(\$0.70\)</span> to receive <span class="math inline">\(\$1\)</span> if <span class="math inline">\(A\)</span> occurs</li>
<li>Bet 2: Pay <span class="math inline">\(\$0.20\)</span> to receive <span class="math inline">\(\$1\)</span> if <span class="math inline">\(\bar A\)</span> occurs</li>
</ul>
<p>Your total payment is <span class="math inline">\(\$0.90\)</span>, but you will receive exactly <span class="math inline">\(\$1\)</span> regardless of whether <span class="math inline">\(A\)</span> or <span class="math inline">\(\bar A\)</span> occurs, for a guaranteed loss of <span class="math inline">\(\$0.10\)</span>. The incoherence arises because <span class="math inline">\(P(A) + P(\bar A) = 0.9 \neq 1\)</span>.</p>
<p>More generally, coherence implies that probabilities must satisfy the following basic properties:</p>
<ol type="1">
<li><em>Non-negativity</em>: For any event <span class="math inline">\(A\)</span>, we must have <span class="math inline">\(P(A) \geq 0\)</span></li>
<li><em>Normalization</em>: For the certain event <span class="math inline">\(\Omega\)</span> (the sample space), we must have <span class="math inline">\(P(\Omega) = 1\)</span></li>
<li><em>Additivity</em>: For mutually exclusive events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (i.e., <span class="math inline">\(A \cap B = \emptyset\)</span>), we must have <span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span></li>
</ol>
<p>To see why non-negativity must hold, suppose <span class="math inline">\(P(A) = -0.1\)</span> for some event <span class="math inline">\(A\)</span>. According to the betting interpretation, you would receive <span class="math inline">\(\$0.10\)</span> and then pay <span class="math inline">\(\$1\)</span> if <span class="math inline">\(A\)</span> occurs. If <span class="math inline">\(A\)</span> does not occur, you keep the <span class="math inline">\(\$0.10\)</span> but receive nothing. However, this means you are offering to pay someone to take a bet against <span class="math inline">\(A\)</span>—clearly irrational behavior. While standard probability theory requires non-negativity, extensions involving negative probabilities have been explored in fields like physics and quantum computing, as well as in Bayesian modeling as mixing distributions for unobserved latent variables <span class="citation" data-cites="polson2025negative">(<a href="references.html#ref-polson2025negative" role="doc-biblioref">Polson and Sokolov 2025</a>)</span>.</p>
<p>The normalization requirement <span class="math inline">\(P(\Omega) = 1\)</span> ensures that you assign probability one to something that is certain to happen. If <span class="math inline">\(P(\Omega) &lt; 1\)</span>, you would be willing to pay less than <span class="math inline">\(\$1\)</span> to receive <span class="math inline">\(\$1\)</span> with certainty, allowing an arbitrageur to make a riskless profit. Conversely, if <span class="math inline">\(P(\Omega) &gt; 1\)</span>, you would pay more than <span class="math inline">\(\$1\)</span> for a certain payoff of <span class="math inline">\(\$1\)</span>, guaranteeing a loss.</p>
<p>The additivity axiom ensures consistency across mutually exclusive events. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> cannot both occur, then betting on “<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>” should cost the same as placing separate bets on <span class="math inline">\(A\)</span> and on <span class="math inline">\(B\)</span>. Violating this principle again opens the door to Dutch books.</p>
<p>Another axiom <span class="math inline">\(P(A) + P(\bar A) = 1\)</span> just follows from the normalization requirement and the additivity axiom.</p>
<p>These are precisely the axioms proposed by Andrey Kolmogorov <span class="citation" data-cites="kolmogorov1933grundbegriffe">(<a href="references.html#ref-kolmogorov1933grundbegriffe" role="doc-biblioref">1933</a>)</span> in his foundational work on probability theory. They provide mathematical structure to probability. Although, Kolmogorov’s axioms are agnostic to any definition of probability and are purely mathematical in nature, the fact that they can be derived from the Dutch book argument shows their applicability to rational decision-making under uncertainty. Any violation of these axioms opens you to guaranteed losses through carefully constructed bets. However, Kolmogorov’s axiomatic approach helps us to derive results in more complex settings. For examples, Kolmogorov’s framework is applicable in infinite sample spaces and continuous random variables.</p>
<p>The axioms provide a number of rules that probabilities must follow. There are several important corollaries that can help us assign probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:</p>
<!-- https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf -->
<ol type="1">
<li><em>Complement rule</em>: Let “not <span class="math inline">\(A\)</span>” denote the complement of event <span class="math inline">\(A\)</span>. <span class="math display">\[
  P(\text{not } A) = 1- P(A).
\]</span></li>
<li><em>Monotonicity</em>: If <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(P(A)\le P(B)\)</span>. In other words, the probability of a larger set is greater than or equal to the probability of a subset.</li>
<li><em>Subadditivity</em>: This is a generalization of the addition rule, where the equality holds when events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive. <span class="math display">\[
P(A \text{ or } B)\le P(A)+P(B).
\]</span></li>
<li><em>Inclusion–exclusion principle</em>: This principle extends subadditivity to the case where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not necessarily mutually exclusive. <span class="math display">\[
P(A\text{ or } B)=P(A)+P(B)-P(A\text{ and }B).
\]</span></li>
<li><em>Conditional probability</em>: The conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is <span class="math display">\[
P(A\mid B) = \dfrac{P(A \text{ and } B)}{P(B)}.
\]</span></li>
<li><em>Bayes rule</em> simply reverses the conditioning to compute <span class="math inline">\(P(A\mid B)\)</span> from <span class="math inline">\(P(B\mid A)\)</span>—a disciplined probability accounting. <span class="math display">\[
P(A\mid B) = \dfrac{P(B\mid A)P(A)}{P(B)}.   
\]</span></li>
<li><em>Law of total probability</em> is a direct consequence of the definition of conditional probability and the normalization axiom. It states that if <span class="math inline">\(B_1, B_2, \ldots, B_n\)</span> are mutually exclusive and exhaustive events, then <span class="math display">\[
P(A) = \sum_{i=1}^n P(A \text{ and } B_i) = \sum_{i=1}^n P(A \mid B_i)P(B_i).
\]</span></li>
</ol>
<p>All of these axioms follow simply from the principle of coherence and the avoidance of Dutch book arbitrage. This includes the Bayes rule itself <span class="citation" data-cites="definetti1937foresight shimony1955coherence">(<a href="references.html#ref-definetti1937foresight" role="doc-biblioref">de Finetti 1937</a>; <a href="references.html#ref-shimony1955coherence" role="doc-biblioref">Shimony 1955</a>)</span>. If there is arbitrage present in the market, it should be “traded”. It often happens when subjective probabilities do not match.</p>
<p>Bayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.</p>
<p>Bayes rule simply provides a disciplined probability accounting of how probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities they cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.</p>
<p>These corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.</p>
<p>The rise of artificial intelligence has definitively established Bayesian inference as a cornerstone of modern learning algorithms. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics or the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.</p>
<!-- ## Bernoulli's Problem
Bernoulli considered the following problem. Suppose that we observe $m$ successes and $n$ failures of an event $A$, out of total $N=m+n$ trials. How do we assign a probability to the event $A$? A classic definition of the probability (due to Jacob Bernoulli) is the ratio of number of favorable outcomes $m$ to the total number of outcomes $N$, which is the sum of $m$ and the number of unfavorable outcomes $n$ 
$$
P_N = \dfrac{m}{m+n} = \dfrac{m}{N}.
$$

Moreover, can we construct a law of succession? What is the probability that the next trial will be a success, given that there are uncertainties in the underlying probabilities? @keynes1921treatise considered the rule of succession a.k.a. induction. For example, Bernoulli proposed that 
$$
P_{N+1} = \dfrac{m+1}{N+2}.
$$ 
@keynes1921treatise (p. 371) provided a fully Bayesian model based on what we know today as the Beta-Binomial model. @sec-bl provides a full analysis. The determination of the predictive rule is equivalent to the problem of finding a sufficient statistic (a.k.a. summary statistic) and performing feature engineering in modern day artificial intelligence applications.

de Finetti puts this in the framework of exchangeable random variables, see @kreps1988notes for further discussion. Jeffreys provides an alternative approach based on the principle of indifference. 
$$
P_{N+1} = \dfrac{m+1/2}{N+1}.
$$
Ramsey (1926) and de Finetti (1937) and Savage (1956) use a purely axiomatic approach in an effort to operationalize probability. In a famous quote de Finetti says "the probability does not exist". In this framework, probability is subjective and operationalized as a willingness to bet. If a gamble $A$ pays \$1 if it happens and \$0 otherwise, then the willingness to bet 50 cents to enter the gamble implies the subjective probability of $A$ is 0.5. Contrary to the frequentist approach, the probability is not a property of the event, but a property of the person. This is the basis for the Bayesian approach to probability. 

Leonard Jimmie Savage, an American statistician, developed a decision theory framework known as the "Savage axioms" or the "Sure-Thing Principle." This framework is a set of axioms that describe how a rational decision-maker should behave in the face of uncertainty. These axioms provide a foundation for subjective expected utility theory.

The Savage axioms consist of three main principles:

1.  *Completeness axiom.*
This axiom assumes that a decision-maker can compare and rank all possible outcomes or acts in terms of preferences. In other words, for any two acts (or lotteries), the decision-maker can express a preference for one over the other, or consider them equally preferable.

2.  *Transitivity axiom.*
This axiom states that if a decision-maker prefers act A to act B and prefers act B to act C, then they must also prefer act A to act C. It ensures that the preferences are consistent and do not lead to cycles or contradictions.

3.  *Continuity axiom (Archimedean axiom).*
The continuity axiom introduces the concept of continuity in preferences. It implies that if a decision-maker prefers act A to act B, and B to C, then there exists some probability at which the decision-maker is indifferent between A and a lottery that combines B and C. This axiom helps to ensure that preferences are not too "discontinuous" or erratic.

Savage's axioms provide a basis for the development of subjective expected utility theory. In this theory, decision-makers are assumed to assign subjective probabilities to different outcomes and evaluate acts based on the expected utility, which is a combination of the utility of outcomes and the subjective probabilities assigned to those outcomes.

Savage's framework has been influential in shaping the understanding of decision-making under uncertainty. It allows for a more flexible approach to decision theory that accommodates subjective beliefs and preferences. However, it's worth noting that different decision theorists may have alternative frameworks, and there are ongoing debates about the appropriateness of various assumptions in modeling decision-making.

Frequency probability is based on the idea that the probability of an event can be found by repeating the experiment many times and probability arises from some random process on the sample space (such as random selection). For example, if we toss a coin many times, the probability of getting a head is the number of heads divided by the total number of tosses. This is the basis for the frequentist approach to probability. -->
<section id="odds-as-probabilities" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="odds-as-probabilities"><span class="header-section-number">1.1</span> Odds as Probabilities</h2>
<p>Another way, sometimes more convenient, to talk about uncertainty and to express probabilities via odds, such as 9 to 2 or 3 to 1. Odds express the ratio of favorable to unfavorable outcomes (Success:Failure), while probability is the chance of an event happening out of all possibilities (Success / Total).</p>
<p>We assign odds “on <span class="math inline">\(A\)</span>” (or “in favor of <span class="math inline">\(A\)</span>”) versus odds “against <span class="math inline">\(A\)</span>”. For example, if the probability of a Chicago Bears’ Super Bowl win is <span class="math inline">\(P(A) = 2/11\)</span>, the odds against them are <span class="math inline">\((1 - 2/11) / (2/11) = 9/2\)</span>, or “9 to 2”. This means for every 2 times they win, they lose 9 times. Conventionally, “odds on” often refers to the reverse, but to avoid ambiguity, we will speak of probability or odds against. <span class="math display">\[
O(A) = \dfrac{P(\bar A) }{P(A)} = \dfrac{1-P(A)}{P(A)}
\]</span> Equivalently, probabilities can be determined from odds <span class="math display">\[
P(A) = \dfrac{1}{1+O(A)}
\]</span> For example, if the odds are one <span class="math inline">\(O(A) = 1\)</span>, then for every $1 bet you will pay out $1. This event has probability <span class="math inline">\(0.5\)</span>.</p>
<p>If <span class="math inline">\(O(A) = 2\)</span>, then you are willing to offer <span class="math inline">\(2:1\)</span>. For a $1 bet you’ll payback $3. In terms of probability <span class="math inline">\(P(A) = 1/3\)</span>.</p>
<p>Odds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.</p>
<div id="exm-odds" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Odds)</strong></span> One of the main sources of prediction markets is bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. <a href="#fig-odds" class="quarto-xref">Figure&nbsp;<span>1.1</span></a> shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hillary Clinton would win over Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.</p>
<div id="fig-odds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hilary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Presidential Odds 2016
</figcaption>
</figure>
</div>
<p>Ahead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hillary Clinton. The best odds on Clinton are <span class="math inline">\(1/3\)</span>; this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the Market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides, otherwise they are risking to stay out of business.</p>
</div>
<div id="exm-derby" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Kentucky Derby)</strong></span> The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is <span class="math inline">\(1 \frac{1}{4}\)</span> miles (2 kilometers) and is the first time the three-year old horses have raced the distance.</p>
<p>There was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have won many times in a row. The market is getting better at predicting who’s going to win. Here’s the data</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Horse Name</th>
<th>Year</th>
<th>Odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spectacular Bid</td>
<td>1979</td>
<td>3/5</td>
</tr>
<tr class="even">
<td>Fusaichi Pegasus</td>
<td>2000</td>
<td>2.3/1</td>
</tr>
<tr class="odd">
<td>Street Sense</td>
<td>2007</td>
<td>9/2</td>
</tr>
<tr class="even">
<td>Big Brown</td>
<td>2008</td>
<td>5/2</td>
</tr>
</tbody>
</table>
<p>Recently, favorites have had a lot more success</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Horse Name</th>
<th>Year</th>
<th>Odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>California Chrome</td>
<td>2014</td>
<td>5/2</td>
</tr>
<tr class="even">
<td>American Pharoah</td>
<td>2015</td>
<td>2/1</td>
</tr>
<tr class="odd">
<td>Nyquist</td>
<td>2016</td>
<td>3.3/1</td>
</tr>
<tr class="even">
<td>Always Dreaming</td>
<td>2017</td>
<td>5.2/1</td>
</tr>
</tbody>
</table>
<p>The most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only other horse that in 2001 has broken two minutes at odds 11.5/1.</p>
</div>
<div id="exm-harville" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Exacta Betting and the Harville Formula)</strong></span> How can probability help you with betting on the race? There are many different types of bets, and probability can help you find <em>fair</em> odds. The Derby is a Grade 1 stakes race for three-year-old thoroughbred horses. Colts and geldings carry 126 pounds and fillies 121. The odds are set by pari-mutuel betting by the public. After all the wagers have been placed, the racetrack takes a fee (18%). After the winning horse passes the finishing line, the pool of money is redistributed to the winning tickets. Random betting therefore loses you 18%, so it’s important to learn some empirical facts to try and tilt the odds in your favor.</p>
<p>For example, you can place bets as follows:</p>
<ul>
<li><em>Win</em>: “$2 win horse 1”</li>
<li><em>Straight Exacta</em>: “$2 exacta 1 <em>with</em> 2”<br>
</li>
<li><em>Exacta Box</em>: “$2 exacta box 1 <em>and</em> 2” You win with <em>either</em> order: 2 bets = $4.</li>
</ul>
<p>Consider a hypothetical race where <em>Sovereignty</em> wins at 9/1 odds and <em>Journalism</em> comes second at 7/2 odds. For a $2 bet on <em>Sovereignty</em> to Win at 9/1, the payout would be <span class="math inline">\(2 \cdot 9 + 2 = \$20\)</span> (the 9/1 win plus your initial $2 bet returned).</p>
<p>Let’s figure out the <em>fair</em> value for an exacta bet given that you know the win odds. This is known as the <em>Harville formula</em>. The exacta is probably one of the most popular bets for many horseplayers, corresponding to predicting the first two horses in the correct order.</p>
<p>The Harville formula provides an answer. We use the rule of conditional probability. The probability for the straight exacta of horses <span class="math inline">\(A\)</span> beating horse <span class="math inline">\(B\)</span> is: <span class="math display">\[
P(A \text{ beats } B) = P(A \text{ Wins}) \cdot P(B \text{ Second} \mid A \text{ Wins})
\]</span></p>
<p>A reasonable assessment of <span class="math inline">\(P(B \text{ Second} \mid A \text{ Wins})\)</span> can be derived as follows. Renormalizing the probabilities by removing the winner <span class="math inline">\(A\)</span> and distributing the probability mass to the remaining horses gives: <span class="math display">\[
P(B \text{ Second} \mid A \text{ Wins}) = \frac{P(B \text{ Wins})}{1 - P(A \text{ Wins})}
\]</span></p>
<p>In total, the fair price for the exacta is: <span class="math display">\[
P(A \text{ beats } B) = P(A \text{ Wins}) \cdot \frac{P(B \text{ Wins})}{1 - P(A \text{ Wins})}
\]</span></p>
<p>Therefore, we have: <span class="math display">\[
p_{12} = p_1 \cdot \frac{p_2}{1-p_1} \text{ where } p_1 = \frac{1}{1+O_1}, p_2 = \frac{1}{1+O_2}
\]</span></p>
<p>Solving for odds, we get the <em>Harville formula</em>: <span class="math display">\[
O_{12} = O_1(1 + O_2) - 1
\]</span></p>
<p>Using our example with 9/1 and 7/2 odds: <span class="math inline">\(O_{12} = 9 \cdot (1 + 3.5) - 1 = 39.5/1\)</span>.</p>
<p>Notice that the actual payout is determined solely by the volume of money wagered on that combination. There’s no requirement it matches our probabilistic analysis. However, the Harville formula gives us an idea of fair value. Some bettors searching for value try to find significantly undervalued exacta bets relative to the Harville formula.</p>
<p>There are many other factors to consider: jockey performance, bloodlines, and post positions can all matter significantly in determining the actual race outcome.</p>
</div>
<div id="exm-odds2" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Boy-Girl Paradox)</strong></span> If a woman has two children and one is a girl, the chance that the other child is also female has to be <span class="math inline">\(50-50\)</span>, right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are <span class="math display">\[
BB \; \; BG \; \; GB \; \; GG
\]</span> Conditional on the information that one is a girl means that you know we can’t have the <span class="math inline">\(BB\)</span> scenario. Hence we are left with three possibilities <span class="math display">\[
BG \; \; GB \; \; GG
\]</span> In one of these is the other a girl. Hence <span class="math inline">\(1/3\)</span>.</p>
<p>It’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is <span class="math inline">\(1/2\)</span> as there are two possibilities <span class="math display">\[
GB \; \; GG
\]</span> This leads to the probability of <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-galton" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Galton Paradox)</strong></span> You flip three fair coins. What is the <span class="math inline">\(P(\text{all} \; \text{alike})\)</span>?</p>
<p>Assuming a fair coin (i.e.&nbsp;<span class="math inline">\(P(H) = P(T) = 1/2\)</span>), a formal approach might consist of computing the probability for all heads or all tails, which is</p>
<p><span class="math display">\[\begin{align*}
P(HHH) &amp;\equiv P(H \text{ and } H \text{ and } H) \\
&amp;= P(H)\times P(H)\times P(H) \\
&amp;= \left(\frac{1}{2}\right)^3
\end{align*}\]</span> and, since we’re ultimately interested in the probability of either (mutually exclusive) case, <span class="math display">\[\begin{align*}
P(\text{all alike}) &amp;= P(HHH \text{ or } TTT) \\
&amp;= P(HHH) + P(TTT) \\
&amp;= 2 \times \frac{1}{8}
\end{align*}\]</span></p>
<p>One could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, <span class="math display">\[
P(\text{all alike}) = 1/2
\]</span> There are 8 equally likely outcomes. Two are ‘all alike’ (HHH, TTT). So 2/8 = 1/4. The error in reasoning is assuming that ‘two must correspond’ fixes the first two coins, but ‘two alike’ could be coins 1&amp;2, 2&amp;3, or 1&amp;3.</p>
<p>For a real treatment of the subject, we highly recommend reading Galton’s essay at <a href="http://galton.org/essays/1890-1899/galton-1894-chances.pdf">galton.org</a>.</p>
</div>
<div id="exm-odds3" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Three Cards)</strong></span> Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that it’s red. What’s the probability the other side is red? <span class="math inline">\(1/2\)</span>? No, it’s <span class="math inline">\(2/3\)</span>! By a similar logic there are six initial possibilities <span class="math display">\[
B_1 B_2 \; \; B_2 B_1 \; \; B R \; \; R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> where <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> index the sides of the same colored cards.</p>
<p>If we now condition on the top side being red we see that there are still three possibilities left <span class="math display">\[
R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> Hence the probability is <span class="math inline">\(2/3\)</span> and not the intuitive <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-Patriots" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (NFL: New England Patriots Coin Toss)</strong></span> Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The NFL team New England Patriots won 19 out of 25 coin tosses in the 2014-15 season. What is the probability of this happening?</p>
<p>Let <span class="math inline">\(X\)</span> be a random variable equal to <span class="math inline">\(1\)</span> if the Patriots win and <span class="math inline">\(0\)</span> otherwise. It’s reasonable to assume <span class="math inline">\(P(X = 1) = \frac{1}{2}\)</span>. The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is <span class="math inline">\((1/2)^{25}\)</span>. We can code a typical sequence as, <span class="math display">\[
1,1,1,\ldots,1,0,0,\ldots,0.
\]</span> There are <span class="math inline">\(177,100\)</span> different sequences of 25 games where the Patriots win 19. There are <span class="math inline">\(25! = 1\cdot 2\cdot \ldots \cdot 25\)</span> ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are <span class="math inline">\(19!\)</span> ways to re-arrange the ones and <span class="math inline">\(6!\)</span> ways to rearrange the sequence of zeroes. Thus, the total number of different winning sequences is</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">factorial</span>(<span class="dv">25</span>) <span class="sc">/</span> (<span class="fu">factorial</span>(<span class="dv">19</span>) <span class="sc">*</span> <span class="fu">factorial</span>(<span class="dv">25</span> <span class="sc">-</span> <span class="dv">19</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 177100</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Each potential sequence has probability <span class="math inline">\(0.5^{25}\)</span>, thus <span class="math display">\[
P\left(\text{Patriots win 19 out of 25 tosses}\right) =  177,100 \times 0.5^{25} = 0.005
\]</span></p>
<p>Often, it is easier to communicate uncertainties in the form of odds. In terms of betting odds of <span class="math inline">\(1:1\)</span> gives <span class="math inline">\(P = \frac{1}{2}\)</span>, odds of <span class="math inline">\(2:1\)</span> (I give <span class="math inline">\(2\)</span> for each <span class="math inline">\(1\)</span> you bet) is <span class="math inline">\(P = \frac{1}{3}\)</span>.</p>
<p>Remember, odds, <span class="math inline">\(O(A)\)</span>, is the ratio of the probability of happening over not happening, <span class="math display">\[
O(A) = (1 - P(A))/P(A),
\]</span> equivalently, <span class="math display">\[
P(A) = \frac{1}{1 + O(A)}.
\]</span></p>
<p>The odds of the Patriots winning sequence are then 1 to 199</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.005</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.005</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.005</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</div>
<div id="exm-Pete" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 (Hitting Streak)</strong></span> Pete Rose of the Cincinnati Reds set a National League record of hitting safely in <span class="math inline">\(44\)</span> consecutive games. How likely is such a long sequence of safe hits to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once in every one of those 44 games. Here are a couple of facts we know about him:</p>
<ol type="1">
<li>Rose was a <span class="math inline">\(300\)</span> hitter, he hits safely 3 times out of 10 attempts</li>
<li>Each at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.</li>
</ol>
<p>Assuming he comes to bat <span class="math inline">\(4\)</span> times each game, <em>what probability might reasonably be associated with that hitting streak?</em> First we define notation. We use <span class="math inline">\(A_i\)</span> to denote an event of hitting safely at game <span class="math inline">\(i\)</span>, then <span class="math display">\[
\begin{aligned}
&amp; P( \mathrm{Rose \; Hits \; Safely \; in \;44 \; consecutive \; games} ) = \\
&amp; P ( A_1 \; \text{and} \;  A_2  \ldots \text{and} \;  A_{44} ) = P ( A_1 ) P ( A_2 ) \ldots P ( A_{44} )
\end{aligned}
\]</span> We now need to find <span class="math inline">\(P(A_i)\)</span>s where <span class="math inline">\(P(A_i) = 1 - P(\text{not} \; A_i)\)</span> <span class="math display">\[\begin{align*}
P ( A_1 ) &amp; = 1 - P ( \mathrm{ not} \; A_1 ) \\
&amp; = 1 - P ( \mathrm{ Rose \; makes \; 4 \; outs } ) \\
&amp; = 1 - ( 0.7)^4 = 0.76
\end{align*}\]</span> For the winning streak, then we have <span class="math inline">\((0.76)^{44} = 0.0000057\)</span>, a very low probability. In terms of odds, there are three basic inferences</p>
<ol type="1">
<li>This means that the odds for a particular player as good as Pete Rose starting a hitting streak today are 175,470 to 1.</li>
<li>This doesn’t mean that the run of <span class="math inline">\(44\)</span> won’t be beaten by some player at some time: the Law of Very Large Numbers</li>
<li>Joe DiMaggio’s record is 56. He is a 325 hitter, thus we have <span class="math inline">\((0.792)^{56} = 2.13 \times 10^{-6}\)</span> or 455,962 to 1. It’s going to be hard to beat.</li>
</ol>
<p>The independence assumption underlying this calculation does not account for the popular belief in the “hot hand”—the idea that a player who has been successful recently is more likely to succeed again.</p>
</div>
<div id="exm-Jeter" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 (Derek Jeter)</strong></span> Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and finer subset selection can change your average effects.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>1995</th>
<th></th>
<th>1996</th>
<th></th>
<th>Combined</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Derek Jeter</td>
<td>12/48</td>
<td>0.250</td>
<td>183/582</td>
<td>0.314</td>
<td>195/630</td>
<td>0.310</td>
</tr>
<tr class="even">
<td>David Justice</td>
<td>104/411</td>
<td>0.253</td>
<td>45/140</td>
<td>0.321</td>
<td>149/551</td>
<td>0.270</td>
</tr>
</tbody>
</table>
<p>This situation is known as <em>confounding</em>. It occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if <span class="math inline">\(A,B,C\)</span> are events it is possible to have the three inequalities <span class="math display">\[\begin{align*}
&amp;P( A \mid B \text{ and } C ) &gt; P( A \mid B \text{ and } \bar C )\\
&amp;P( A \mid \bar  B \text{ and } C ) &gt; P( A \mid \bar  B \text{ and } \bar  C )\\
&amp;P( A \mid C ) &lt; P( A \mid \bar C )
\end{align*}\]</span> The three inequalities can’t hold simultaneously when <span class="math inline">\(P(B\mid C) = P(B\mid \bar  C)\)</span>.</p>
</div>
<div id="exm-birthday" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.10 (Birthday Problem)</strong></span> The birthday problem <span class="citation" data-cites="diaconis1989methods">(<a href="references.html#ref-diaconis1989methods" role="doc-biblioref">Diaconis and and Mosteller 1989</a>)</span> is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.</p>
<p>In general, given <span class="math inline">\(N\)</span> items (people) randomly distributed into <span class="math inline">\(c\)</span> categories (birthdays), where the number of items is small compared to the number of categories <span class="math inline">\(N \ll c\)</span>, the probability of no match is given by <span class="math display">\[
P(\text{no match}) \approx \exp\left(-N^2/2c\right).
\]</span> Given <span class="math inline">\(A_i\)</span> is the event that person <span class="math inline">\(i\)</span> has a matching birthday with someone, we have <span class="math display">\[
P(\text{no match})  = \prod_{i=1}^{N-1}(1-P(A_i)) = \exp\left(\sum_{i=1}^{N-1}\log (1-P(A_i))\right).
\]</span> Here <span class="math inline">\(P(A_i) =\dfrac{i}{c}\)</span> Then use the approximation <span class="math inline">\(\log(1-x) \approx -x\)</span> for small <span class="math inline">\(x\)</span> to get <span class="math inline">\(P(\text{no match})\)</span>. <span class="math display">\[
\sum_{i=1}^{N-1}\log (1-P(A_i)) \approx -\sum_{i=1}^{N-1}\dfrac{i}{c} = -\dfrac{N(N-1)}{2c}.
\]</span></p>
<p>The probability of at least two people sharing a birthday is then the complement of the probability above: <span class="math display">\[
P(\text{At least one shared birthday}) = 1 - P(\text{no match}).
\]</span> Solving for <span class="math inline">\(P(\text{match})=1/2\)</span>, leads to a square root law <span class="math inline">\(N=1.2\sqrt{c}\)</span>, if <span class="math inline">\(c=365\)</span> then <span class="math inline">\(N=23\)</span>, and if <span class="math inline">\(c=121\)</span> (near birthday match), then <span class="math inline">\(N=13\)</span>.</p>
<p>The unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.</p>
</div>
</section>
<section id="random-variables-quantifying-uncertainty" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="random-variables-quantifying-uncertainty"><span class="header-section-number">1.2</span> Random Variables: Quantifying Uncertainty</h2>
<p>A random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.</p>
<p>For example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.</p>
<p>While events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.</p>
<p>Random variables are quantities that we are not certain about. A random variable that can take a finite or a countable number of values is called a <em>discrete random variable</em> (number of rainy days next week). Otherwise, it will be a <em>continuous random variable</em> (amount of rain tomorrow).</p>
<p>Discrete random variables are often constructed by assigning specific values to events such as <span class="math inline">\(\{X=x\}\)</span> which corresponds to the outcomes where <span class="math inline">\(X\)</span> equals a specific number <span class="math inline">\(x\)</span>. For example</p>
<ol type="1">
<li>Will a user click-through on a Google ad? (0 or 1)</li>
<li>Who will win the next presidential election? (Republican=1, Democrat=2, Independent=3)</li>
</ol>
<p>To fix notation, we will use <span class="math inline">\(P(X=x)\)</span> to denote the probability that random variable <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(x\)</span>. A map from all possible values <span class="math inline">\(x\)</span> of a discrete random variable <span class="math inline">\(X\)</span> to probabilities is called a <em>probability mass function</em> <span class="math inline">\(p(x)\)</span>. We will interchangeably use <span class="math inline">\(P(X=x)\)</span> and <span class="math inline">\(p(x)\)</span>. An important property of the probability mass function is that (normalization Kolmogorov axiom) <span class="math display">\[
\sum_{x\in S} p(x) = 1.
\]</span> Here <span class="math inline">\(S\)</span> denotes the set of all possible values of random variable <span class="math inline">\(X\)</span>.</p>
<p>Clearly, all probabilities have to be greater than or equal to zero, so that <span class="math inline">\(p(x)\ge 0\)</span>.</p>
<p>Often, we are interested in <span class="math display">\[
F(x) = P(X\le x) = \sum_{y\le x} p(y),
\]</span> this is the cumulative distribution function (CDF).</p>
<p>The CDF is a monotonically increasing function (never decreases as <span class="math inline">\(x\)</span> increases). In other words, if <span class="math inline">\(a \leq b\)</span>, then <span class="math inline">\(F_X(a) \leq F_X(b)\)</span>. The value of the CDF always lies between 0 and 1, inclusive.</p>
<div id="exm-dcdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.11 (Discrete CDF)</strong></span> Suppose <span class="math inline">\(X\)</span> is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
P(X = x) = \frac{1}{6}
\]</span> for <span class="math inline">\(x = 1, 2, 3, 4, 5, 6\)</span></p>
<p>The CDF of <span class="math inline">\(X\)</span>, <span class="math inline">\(F(x)\)</span>, is calculated as follows:</p>
<ul>
<li>For <span class="math inline">\(x &lt; 1\)</span>, <span class="math inline">\(F(x) = 0\)</span> (since it’s impossible to roll less than 1).</li>
<li>For <span class="math inline">\(1 \leq x &lt; 2\)</span>, <span class="math inline">\(F(x) = \frac{1}{6}\)</span> (the probability of rolling a 1).</li>
<li>For <span class="math inline">\(2 \leq x &lt; 3\)</span>, <span class="math inline">\(F(x) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}\)</span> (the probability of rolling a 1 or 2).</li>
<li>This pattern continues, adding <span class="math inline">\(\frac{1}{6}\)</span> for each integer interval up to 6.</li>
<li>For <span class="math inline">\(x \geq 6\)</span>, <span class="math inline">\(F(x) = 1\)</span> (since it’s certain to roll a number 6 or less).</li>
</ul>
<p>Graphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>), <span class="at">main =</span> <span class="st">""</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>CDF of a discrete random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="continuous-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="continuous-random-variables">Continuous Random Variables</h3>
<p>If we want to build a probabilistic model of a stock price or return, we need to use a continuous random variable that can take an interval of values. Instead of a frequency function we will use a <em>density function</em>, <span class="math inline">\(p(x)\)</span> to describe a continuous variable. Unlike the discrete case, <span class="math inline">\(p(x)\)</span> is not the probability that the random variable takes value <span class="math inline">\(x\)</span>. Rather, we need to talk about the value being inside an interval. For example, the probability of <span class="math inline">\(X\)</span> with density <span class="math inline">\(p(x)\)</span> being inside any interval <span class="math inline">\([a,b]\)</span>, with <span class="math inline">\(a&lt;b\)</span> is given by <span class="math display">\[
P(a &lt; X &lt; b) = \int_{a}^{b}p(x)dx.
\]</span> The total probability is one as <span class="math inline">\(\int_{-\infty}^\infty p(x) dx=1\)</span>. The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then <span class="math display">\[
  p(x) = \begin{cases} 1/10, ~~~20 \le x \le 30\\0, ~~~\mbox{otherwise}\end{cases}
\]</span></p>
<p>Under this model, the probability of temperature being between 25 and 27 degrees is <span class="math display">\[
P(25 \le x \le 27) = \int_{25}^{27} p(x)dx = (27-25)/10 = 0.2
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/uniform.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Uniform Distribution: Probability of temperature being between 25 and 27</figcaption>
</figure>
</div>
<p>The Cumulative Distribution Function for a continuous random variable, it is defined similarly to discrete RV CDF as <span class="math display">\[
F(x) = P(X \leq x) = \int_{-\infty}^x p(t)dt
\]</span> It is a non-decreasing function and takes values in [0,1].</p>
<div id="exm-ccdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.12 (Continuous CDF for Uniform Distribution)</strong></span> <span class="math display">\[
p(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The CDF, <span class="math inline">\(F(x)\)</span>, is obtained by integrating the PDF:</p>
<ul>
<li>For <span class="math inline">\(x &lt; 0\)</span>, <span class="math inline">\(F(x) = 0\)</span>.</li>
<li>For <span class="math inline">\(0 \leq x \leq 1\)</span>, <span class="math inline">\(F(x) = \int_0^x 1 \, dt = x\)</span>.</li>
<li>For <span class="math inline">\(x &gt; 1\)</span>, <span class="math inline">\(F(x) = 1\)</span>.</li>
</ul>
<p>So, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as <span class="math inline">\(x\)</span> goes from 0 to 1.</p>
<p>Graphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as <span class="math inline">\(x\)</span> increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature. Figure below shows the CDF of a uniform and normal random variable, respectively.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">runif</span>(<span class="dv">500</span>)), <span class="at">main =</span> <span class="st">""</span>, <span class="at">col =</span> <span class="st">"lightblue"</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">bg =</span> <span class="st">"grey"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">rnorm</span>(<span class="dv">500</span>)), <span class="at">main =</span> <span class="st">""</span>, <span class="at">col =</span> <span class="st">"lightblue"</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">bg =</span> <span class="st">"grey"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-null_prefix="true" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>CDF of a uniform random variable</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid figure-img" width="480"></p>
<figcaption>CDF of a normal random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="the-inverse-cdf-method" class="level3">
<h3 class="anchored" data-anchor-id="the-inverse-cdf-method">The Inverse CDF Method</h3>
<p>The inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, <span class="math inline">\(F\)</span>. Since <span class="math inline">\(F\left(  x\right)\)</span> is uniformly distributed on <span class="math inline">\(\left[ 0,1\right]\)</span>, draw a uniform random variable and invert the CDF to get a draw from <span class="math inline">\(F\)</span>. Thus, to sample from <span class="math inline">\(F\)</span>, <span class="math display">\[\begin{align*}
&amp;  \text{Step 1}\text{: Draw }U\sim U\left[  0,1\right]  \ \\
&amp;  \text{Step 2}\text{: }\text{Set }X=F^{-1}\left(  U\right)  ,
\end{align*}\]</span> where <span class="math inline">\(F^{-1}\left(  U\right)  =\inf\left\{  x:F\left(  x\right)  =U\right\}\)</span>.</p>
<p>This inversion method provides i.i.d. draws from <span class="math inline">\(F\)</span> provided that <span class="math inline">\(F^{-1}\left(  U\right)\)</span> can be exactly calculated. For example, the CDF of an exponential random variable with parameter <span class="math inline">\(\mu\)</span> is <span class="math inline">\(F\left(  x\right) =1-\exp\left(  -\mu x\right)\)</span>, which can easily be inverted. When <span class="math inline">\(F^{-1}\)</span> cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, <span class="math inline">\(F\left(  x\right)\)</span> can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from <span class="math inline">\(F\)</span>. With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.</p>
</section>
<section id="functional-transformations" class="level3">
<h3 class="anchored" data-anchor-id="functional-transformations">Functional Transformations</h3>
<p>The second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that <span class="math inline">\(X\sim F\)</span>, admitting a density <span class="math inline">\(f\)</span>, and that <span class="math inline">\(y=h\left(  x\right)\)</span> is an increasing continuous function. Thus, we can define <span class="math inline">\(x=h^{-1}\left(  y\right)\)</span> as the inverse of the function <span class="math inline">\(h\)</span>. The distribution of <span class="math inline">\(y\)</span> is given by <span class="math display">\[
F_Y\left(y\right)  =P\left(  Y\leq y\right)  =\int_{-\infty}^{h^{-1}\left(  y\right)  }f\left(  x\right)  dx=F_X\left(  X\leq h^{-1}\left(y\right)  \right).
\]</span> Differentiating with respect to <span class="math inline">\(y\)</span> gives the density via Leibnitz’s rule: <span class="math display">\[
f_{Y}\left(  y\right)  =f\left(  h^{-1}\left(  y\right)  \right)  \left\vert\frac{d}{dy}\left(  h^{-1}\left(  y\right)  \right)  \right\vert,
\]</span> where we make explicit that the density is over the random variable <span class="math inline">\(Y\)</span>. This result is used widely. For example, if <span class="math inline">\(X\sim\mathcal{N}\left(  0,1\right)\)</span>, then <span class="math inline">\(Y=\mu+\sigma X\)</span>. Since <span class="math inline">\(x=h^{-1}\left(  y\right)  =\frac{y-\mu}{\sigma}\)</span>, the distribution function is <span class="math inline">\(F\left(  \frac{x-\mu}{\sigma}\right)\)</span> and density <span class="math display">\[
f_{Y}\left(  y\right)  =\frac{1}{\sqrt{2\pi}\sigma}\exp\left(  -\frac{1}{2}\left(  \frac{y-\mu}{\sigma}\right)  ^{2}\right).
\]</span> Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if <span class="math inline">\(Y\sim\mathcal{X}^{2}\left(  \nu\right)\)</span> and <span class="math inline">\(\nu\)</span> is an integer, then <span class="math inline">\(Y=\sum_{i=1}^{\nu}X_{i}^{2}\)</span> where each <span class="math inline">\(X_{i}\)</span> is independent standard normal. Exponential random variables can be used to simulate <span class="math inline">\(\mathcal{X}^{2}\)</span>, Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be simulated via sums of squared vectors of standard normal random variables.</p>
</section>
</section>
<section id="expectation-and-variance-reward-and-risk" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="expectation-and-variance-reward-and-risk"><span class="header-section-number">1.3</span> Expectation and Variance (Reward and Risk)</h2>
<p>An expected value of a random variable, denoted by <span class="math inline">\(\E{X}\)</span> is a weighted average. Each possible value of a random variable is weighted by its probability. For example, Google Maps uses expected value when calculating travel times. We might compute two different routes by their expected travel time. Typically, a forecast or expected value is all that is required — these expected values can be updated in real time as we travel. Say I am interested in travel time from Washington National airport to Fairfax in Virginia. The histogram below shows the travel times observed for a work day evening and were obtained from <a href="https://movement.uber.com/">Uber</a>.</p>
<div id="exm-Uber" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.13 (Uber)</strong></span> Let’s look at the histogram of travel times from Fairfax, VA to Washington, DC</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Travel times in the evening</figcaption>
</figure>
</div>
</div>
</div>
<p>From this dataset, we can empirically estimate the probabilities of observing different values of travel times</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">Travel Time</th>
<th style="text-align: right;">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">18</td>
<td style="text-align: right;">0.05</td>
</tr>
<tr class="even">
<td style="text-align: right;">22</td>
<td style="text-align: right;">0.77</td>
</tr>
<tr class="odd">
<td style="text-align: right;">28</td>
<td style="text-align: right;">0.18</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>There is a small chance (5%) I can get to Washington, DC in 18 minutes, which probably happens on a holiday and a non-trivial chance (18%) to travel for 28 minutes, possibly due to a sports game or bad weather. Most of the time (77%) our travel time is 22 minutes. However, when Uber shows you the travel time, it uses the expected value as a forecast rather than the full distribution. Specifically, you will be given an expected travel time of 23 minutes.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.05</span> <span class="sc">*</span> <span class="dv">18</span> <span class="sc">+</span> <span class="fl">0.77</span> <span class="sc">*</span> <span class="dv">22</span> <span class="sc">+</span> <span class="fl">0.18</span> <span class="sc">*</span> <span class="dv">28</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 23</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>It is a simple summary that takes into account travel accidents and other events that can affect travel time as best as it can.</p>
</div>
<p>The expected value <span class="math inline">\(\E{X}\)</span> of discrete random variable <span class="math inline">\(X\)</span> which takes possible values <span class="math inline">\(\{x_1,\ldots x_n\}\)</span> is calculated using</p>
<p><span class="math display">\[
\E{X} =\sum_{i=1}^{n}x_i P(X = x_i)
\]</span></p>
<p>For example, in a binary scenario, if <span class="math inline">\(X\in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p\)</span>, then <span class="math inline">\(\E{X} = 0\times(1-p)+1\times p = p\)</span>. The expected value of a Bernoulli random variable is simply the probability of success. In many binary scenarios, a probabilistic forecast is sufficient.</p>
<p>If <span class="math inline">\(X\)</span> is continuous with probability distribution <span class="math inline">\(p(x)\)</span>, then we have to calculate the expectation as an integral <span class="math display">\[
\E{X} = \int xp(x)d x  \text{ and } \int p(x)dx = 1.
\]</span></p>
<section id="standard-deviation-and-covariance" class="level3">
<h3 class="anchored" data-anchor-id="standard-deviation-and-covariance">Standard Deviation and Covariance</h3>
<p>Variance measures the spread of a random variable around its expected value <span class="math inline">\(\mu = \E{X}\)</span>. For a discrete random variable <span class="math inline">\(X\)</span> with possible values <span class="math inline">\(\{x_1,\ldots x_N\}\)</span>, we have <span class="math display">\[
\Var{X} = \E{(X-\mu)^2} =  \sum_{i=1}^N (x_i-\mu)^2 P(X=x_i).
\]</span> In the continuous case, we have <span class="math display">\[
\Var{X} = \int_{-\infty}^\infty (x-\mu) ^2 p(x)dx,\text{ where } \mu = \mathbb{E}(X)=\int_{-\infty}^{\infty}p_X(x)dx.
\]</span></p>
<p>The standard deviation is more convenient and is the square root of variance <span class="math inline">\(\sd{X} = \sqrt{\Var{X}}\)</span>. Standard deviation has the desirable property that it is measured in the same units as the random variable <span class="math inline">\(X\)</span> itself and is a more useful measure.</p>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We need to measure whether they move together or in opposite directions. The <em>covariance</em> is defined by <span class="math display">\[
\Cov{X,Y} = \E{\left[ (X- \E{X})(Y- \E{Y})\right]}.
\]</span></p>
<p>When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete and we are given the joint probability distribution, we need to calculate <span class="math display">\[
\Cov{X,Y} = \sum_{x,y}  ( x - \E{X} )(y - \E{Y})p(x,y).
\]</span> Covariance is measured in units of <span class="math inline">\(X\times\)</span>units of <span class="math inline">\(Y\)</span>. This can be inconvenient and makes it hard to compare covariances of different pairs of variables. A more convenient metric is the <em>correlation</em>, which is defined by <span class="math display">\[
\Cor{X,Y}= \frac{ \Cov{X,Y} }{ \sd{X} \sd{Y} }.
\]</span> Correlation, <span class="math inline">\(\Cor{X,Y}\)</span>, is unitless and takes values between -1 and 1.</p>
<p>In the case of joint continuous distribution it is convenient to use the covariance matrix <span class="math inline">\(\Sigma\)</span> which is defined as <span class="math display">\[
\Sigma = \begin{bmatrix}
\Var{X} &amp; \Cov{X,Y} \\
\Cov{X,Y} &amp; \Var{Y}
\end{bmatrix}.
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\Cov{X,Y} = 0\)</span> and <span class="math inline">\(\Sigma\)</span> is diagonal. The correlation matrix is defined as <span class="math display">\[
\rho = \begin{bmatrix}
1 &amp; \Cor{X,Y} \\
\Cor{X,Y} &amp; 1
\end{bmatrix}.
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have an exact linear relationship, then <span class="math inline">\(\Cor{X,Y} = 1\)</span> and <span class="math inline">\(\Cov{X,Y}\)</span> is the product of standard deviations. In matrix notation, the relation between the covariance matrix and correlation matrix is given by <span class="math display">\[
\rho = \mathrm{diag}\left(\Sigma\right)^{-1/2} \Sigma\mathrm{diag}\left(\Sigma\right)^{-1/2},
\]</span> where <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with standard deviations on the diagonal.</p>
</section>
<section id="portfolios-linear-combinations" class="level3">
<h3 class="anchored" data-anchor-id="portfolios-linear-combinations">Portfolios: linear combinations</h3>
<p>Calculating means and standard deviations of combinations of random variables is a central tool in probability. It is known as the portfolio problem. Let <span class="math inline">\(P\)</span> be your portfolio, which comprises a mix of two assets <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, typically stocks and bonds, <span class="math display">\[
P = aX + bY,
\]</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the portfolio weights, typically <span class="math inline">\(a+b=1\)</span>, as we are allocating our total capital. Imagine that you have placed <span class="math inline">\(a\)</span> dollars on the random outcome <span class="math inline">\(X\)</span>, and <span class="math inline">\(b\)</span> dollars on <span class="math inline">\(Y\)</span>. The portfolio <span class="math inline">\(P\)</span> measures your total weighted outcome.</p>
<p>Key portfolio rules: The expected value and variance follow the relations <span class="math display">\[\begin{align*}
\E{aX + bY} = &amp;      a\E{X}+b\E{Y}\\
\Var{ aX + bY }  = &amp; a^2 \Var{X} + b^2 \Var{Y} + 2 ab \Cov{X,Y },
\end{align*}\]</span> with <em>covariance</em> defined by <span class="math display">\[
\Cov{X,Y} = \E{ ( X- \E{X} )(Y- \E{Y})}.
\]</span> Expectation and variance help us to understand the long-run behavior. When we make long-term decisions, we need to use the expectations to avoid biases.</p>
<p>The covariance is related to the correlation by <span class="math inline">\(\Cov{X,Y} = \text{Corr}(X, Y) \cdot \sqrt{\text{Var}(X) \cdot \text{Var}(Y)}\)</span>.</p>
<div id="exm-Tortoise" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.14 (Tortoise and Hare)</strong></span> Tortoise and Hare are selling cars. Say <span class="math inline">\(X\)</span> is the number of cars sold and probability distributions, means and variances are given by the following table</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th></th>
<th></th>
<th></th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Variance</th>
<th style="text-align: center;">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td style="text-align: center;"><span class="math inline">\(\E{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\Var{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\sqrt{\Var{X}}\)</span></td>
</tr>
<tr class="even">
<td>Tortoise</td>
<td style="text-align: center;">0</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr class="odd">
<td>Hare</td>
<td style="text-align: center;">0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">1.5</td>
</tr>
</tbody>
</table>
<p>Let’s calculate Tortoise’s expectations and variances <span class="math display">\[\begin{align*}
\E{T} &amp; = (1/2) (1) + (1/2)(2) = 1.5 \\
\Var{T} &amp; = \E{T^2} - \E{T}^2 \\
&amp; =  (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25
\end{align*}\]</span></p>
<p>Now the Hare’s <span class="math display">\[\begin{align*}
\E{H} &amp; = (1/2)(0) + (1/2)(3) = 1.5 \\
\Var{H} &amp; =  (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25
\end{align*}\]</span></p>
<p>What do these tell us about the long run behavior?</p>
<ol type="1">
<li><p>Tortoise and Hare have the same expected number of cars sold.</p></li>
<li><p>Tortoise is more predictable than Hare. He has a smaller variance.</p></li>
</ol>
<p>The standard deviations <span class="math inline">\(\sqrt{\Var{X}}\)</span> are <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1.5\)</span>, respectively. Given two equal means, you always want to pick the lower variance. If we are to invest in one of those, we prefer Tortoise.</p>
<p>What about a portfolio of Tortoise and Hare? Suppose I want to evenly split my investment between Tortoise and Hare. What is the expected number of cars sold and the variance of the number of cars sold? <span class="math display">\[
\E{\frac{1}{2}T + \frac{1}{2}H} = \frac{1}{2} \E{T} + \frac{1}{2} \E{H} = 1.5
\]</span> For variance, we need to know <span class="math inline">\(\Cov{T,H}\)</span>. Let’s take <span class="math inline">\(\Cov{T,H} = -1\)</span> and see what happens. <span class="math display">\[
\begin{aligned}
\Var{\frac{1}{2}T + \frac{1}{2}H} &amp;= \frac{1}{4} \Var{T} + \frac{1}{4} \Var{H} + \frac{1}{2} \Cov{T,H} \\
&amp;= 0.0625 + 0.5625 -0.5 = 0.125
\end{aligned}
\]</span></p>
<p>Notice that the portfolio variance (0.125) is lower than both individual variances (0.25 and 2.25). This demonstrates the power of diversification - by combining investments with negative covariance, we can reduce overall risk while maintaining the same expected return. The negative covariance indicates that when Tortoise performs well, Hare tends to perform poorly, and vice versa, creating a natural hedge.</p>
<p>This example illustrates a fundamental principle in finance and decision theory: diversification can reduce risk without sacrificing expected returns when assets are not perfectly positively correlated. The key insight is that variance depends not only on individual asset volatilities but also on their covariances, making portfolio construction a crucial consideration in risk management.</p>
</div>
</section>
</section>
<section id="limiting-behavior-of-averages" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="limiting-behavior-of-averages"><span class="header-section-number">1.4</span> Limiting Behavior of Averages</h2>
<p>The Tortoise and Hare example illustrates how expectations and variances behave for a single period or a fixed portfolio. However, in many real-world applications—from insurance to machine learning—we are interested in what happens when we repeat an experiment many times. Does the average outcome settle down to a predictable value?</p>
<p>When we observe a stochastic process repeatedly, we naturally ask: what happens to averages as we collect more data? This question lies at the heart of statistical inference and forms the theoretical foundation for learning from experience. The answer is provided by the <em>law of large numbers</em>, one of the most fundamental results in probability theory.</p>
<section id="the-weak-law-of-large-numbers" class="level3">
<h3 class="anchored" data-anchor-id="the-weak-law-of-large-numbers">The Weak Law of Large Numbers</h3>
<p>The weak law of large numbers, in its simplest form, states that sample averages converge in probability to the expected value. We assume that observation <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are draws (realizations) of independent identically distributed (i.i.d.) random variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> with finite mean <span class="math inline">\(\mu = \E{X_i}\)</span>. Then the sample average <span class="math inline">\(\bar{X}_n = \frac{1}{n}\sum_{i=1}^n x_i\)</span> satisfies: <span class="math display">\[
\lim_{n \to \infty} P\left(\left|\bar{X}_n - \mu\right| &gt; \epsilon\right) = 0,
\]</span> for any <span class="math inline">\(\epsilon &gt; 0\)</span>. This form of convergence, known as <em>convergence in probability</em>, means that for large enough <span class="math inline">\(n\)</span>, the probability that the sample average deviates from the true mean by more than any fixed amount becomes arbitrarily small.</p>
<p>The proof of this result, when the variance <span class="math inline">\(\sigma^2 = \Var{X_i}\)</span> exists and is finite, follows elegantly from Chebyshev’s inequality. Since <span class="math inline">\(\E{\bar{X}_n} = \mu\)</span> and <span class="math inline">\(\Var{\bar{X}_n} = \sigma^2/n\)</span>, we have:</p>
<p><span class="math display">\[
P\left(\left|\bar{X}_n - \mu\right| &gt; \epsilon\right) \leq \frac{\Var{\bar{X}_n}}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0
\]</span></p>
<p>as <span class="math inline">\(n \to \infty\)</span>. This simple argument reveals why averages become more reliable as sample sizes grow: the variance of the sample mean shrinks at rate <span class="math inline">\(1/n\)</span>.</p>
<p>This <span class="math inline">\(1/n\)</span> scaling reappears throughout the book, from confidence intervals and A/B testing (<a href="05-ab.html" class="quarto-xref"><span>Chapter 5</span></a>) to the role of sample size in estimation risk and generalization.</p>
</section>
<section id="kolmogorovs-strong-law-of-large-numbers" class="level3">
<h3 class="anchored" data-anchor-id="kolmogorovs-strong-law-of-large-numbers">Kolmogorov’s Strong Law of Large Numbers</h3>
<p>While the weak law establishes convergence in probability, a stronger form of convergence is possible. The <em>strong law of large numbers</em> states that sample averages converge <em>almost surely</em> (with probability one) to the expected value. This is a fundamentally stronger statement: it means that for almost every realization of the sequence, the sample average actually approaches the true mean, not merely that the probability of large deviations vanishes.</p>
<p>Andrey Kolmogorov formalized this result in his groundbreaking 1933 monograph <em>Foundations of the Theory of Probability</em> <span class="citation" data-cites="kolmogoroff1933grundbegriffe">(<a href="references.html#ref-kolmogoroff1933grundbegriffe" role="doc-biblioref">Kolmogoroff 1933</a>)</span>, establishing the conditions under which strong convergence holds. His result revolutionized probability theory by providing a rigorous measure-theoretic foundation for probabilistic reasoning.</p>
<p><strong>Kolmogorov’s Strong Law</strong> states that if <span class="math inline">\(X_1, X_2, \ldots\)</span> are independent random variables (not necessarily identically distributed) with <span class="math inline">\(\E{|X_i|} &lt; \infty\)</span>, then:</p>
<p><span class="math display">\[
\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \to \mu \quad \text{almost surely}
\]</span></p>
<p>where <span class="math inline">\(\mu = \lim_{n \to \infty} \frac{1}{n}\sum_{i=1}^n \E{X_i}\)</span>, provided this limit exists. For i.i.d. random variables with finite mean <span class="math inline">\(\E{X_i} = \mu\)</span>, this simplifies to <span class="math inline">\(\bar{X}_n \to \mu\)</span> almost surely.</p>
<p>The distinction between convergence in probability and almost sure convergence is subtle but crucial. Convergence in probability allows for infinitely many large deviations, as long as they become increasingly rare. Almost sure convergence is stronger: it requires that eventually, after some finite (but random) time, all deviations remain small forever.</p>
<p>Formally, almost sure convergence means:</p>
<p><span class="math display">\[
P\left(\lim_{n \to \infty} \bar{X}_n = \mu\right) = 1
\]</span></p>
<p>This is equivalent to saying that the set of sequences for which the limit fails to equal <span class="math inline">\(\mu\)</span> has probability zero. For practical purposes, this means we can be confident that the specific sequence we observe will exhibit convergence—not merely that convergence is likely.</p>
<p>The independence assumption is central to Kolmogorov’s results, but it can be relaxed in various ways for different applications. For stochastic processes with dependent observations, the key question becomes: how much dependence can we tolerate while still obtaining convergence of averages?</p>
<p>For <em>stationary processes</em>—where the joint distribution of <span class="math inline">\((X_t, X_{t+1}, \ldots, X_{t+k})\)</span> does not depend on <span class="math inline">\(t\)</span>—a law of large numbers holds under considerably weaker conditions than independence. If the process is <em>ergodic</em> (roughly, if it eventually “forgets” its initial conditions), then time averages converge to ensemble averages:</p>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^n X_i \to \mu \quad \text{almost surely}
\]</span></p>
<p>This <em>ergodic theorem</em> extends Kolmogorov’s law of large numbers to dependent sequences and has profound implications for statistical inference from time series data. It justifies the common practice of estimating population means from a single long realization of a stochastic process.</p>
<p>The law of large numbers is not merely a theoretical curiosity—it forms the bedrock of statistical practice. Every time we estimate a population mean from a sample, test a hypothesis, or train a machine learning model, we implicitly rely on the law of large numbers. The confidence we place in larger samples, the use of cross-validation to assess model performance, and the convergence of stochastic gradient descent in deep learning all trace back to this fundamental result.</p>
<p>In the context of stochastic processes, the law of large numbers justifies estimating process parameters from a single long trajectory. When modeling financial returns, climate data, or network traffic, we typically observe one realization over time rather than multiple independent realizations. The ergodic theorem ensures that time averages from this single path converge to the true population moments, enabling inference from the data we actually have.</p>
<p>The Kolmogorov’s formalization is the culmination of several decades of work on the foundations of probability theory. The weak law of large numbers was first proved by Jakob Bernoulli in 1713 for the special case of binomial random variables—an achievement that took him over twenty years. The result was later generalized by Poisson (1837) and Chebyshev (1867) to broader classes of random variables.</p>
<p>The strong law required deeper mathematical machinery. Émile Borel proved a version for Bernoulli trials in 1909, but the general result awaited the development of measure-theoretic probability. Francesco Cantelli made progress in the 1910s, but it was Kolmogorov who provided the definitive treatment in 1933, unifying diverse results under a single rigorous framework.</p>
<p>Kolmogorov’s work transformed probability theory from a collection of special cases and heuristics into a branch of mathematics with the same rigor as analysis or algebra. His measure-theoretic foundations enabled precise statements about almost sure convergence, clarified the distinction between different modes of convergence, and opened the door to modern probability theory and stochastic processes.</p>
<p>The law of large numbers has profound implications for Bayesian inference and computational statistics. For applications to posterior consistency, Monte Carlo simulation, and Markov chain Monte Carlo (MCMC) methods, see <a href="03-bl.html#sec-computational-bridge" class="quarto-xref"><span>Section 3.11</span></a> in Chapter 3.</p>
</section>
</section>
<section id="binomial-poisson-and-normal-distributions" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="binomial-poisson-and-normal-distributions"><span class="header-section-number">1.5</span> Binomial, Poisson, and Normal Distributions</h2>
<p>We now examine three fundamental probability distributions that appear throughout statistics and machine learning.</p>
<section id="bernoulli-distribution" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>The formal model of a coin toss was described by Bernoulli. He modeled the notion of <em>probability</em> for a coin toss, now known as the Bernoulli distribution, where <span class="math inline">\(X \in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p, P(X=0) = 1-p\)</span>. Laplace gave us the <em>principle of insufficient reason</em>: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete uniform distribution on the set of possible outcomes.</p>
<p>A Bernoulli trial relates to an experiment with the following conditions</p>
<ol type="1">
<li>The result of each trial is either a success or failure.</li>
<li>The probability <span class="math inline">\(p\)</span> of a success is the same for all trials.</li>
<li>The trials are assumed to be <em>independent</em>.</li>
</ol>
<p>The Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by <span class="math inline">\(\text{Bernoulli}(p)\)</span>, where <span class="math inline">\(p\)</span> is the probability of success.</p>
<p>The probability mass function (PMF) of a Bernoulli distribution is defined as follows: <span class="math display">\[
P(X = x) = \begin{cases}
p &amp; \text{if } x = 1 \\
1 - p &amp; \text{if } x = 0
\end{cases}
\]</span> The expected value (mean) of a Bernoulli distributed random variable <span class="math inline">\(X\)</span> is given by: <span class="math display">\[\E{X} = p
\]</span> Simply speaking, if you are to toss a coin many times, you expect <span class="math inline">\(p\)</span> heads.</p>
<p>The variance of <span class="math inline">\(X\)</span> is given by: <span class="math display">\[
\Var{X} = p(1-p)
\]</span></p>
<div id="exm-cointoss" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.15 (Coin Toss)</strong></span> The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is <span class="math inline">\(S = \{H,T\}\)</span>, and <span class="math inline">\(P(X = H) = P(X = T) = 1/2\)</span>. On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads <span class="math inline">\(X\)</span> out of two coin tosses is</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p(x)\)</span></td>
<td style="text-align: center;">1/4</td>
<td style="text-align: center;">1/2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Given the probability mass function we can, for example, calculate the probability of at least one head as <span class="math inline">\(P(X \geq 1) = P(X =1) + P(X =2) = p(1)+p(2) = 3/4\)</span>.</p>
</div>
<p>The Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to <span class="math inline">\(X\)</span>, which is the number of successes. Its probability distribution is calculated via: <span class="math display">\[
P(X=x) = {n \choose x} p^x(1-p)^{n-x}.
\]</span> Here <span class="math inline">\({n \choose x}\)</span> is the combinatorial function, <span class="math display">\[
{n \choose x} = \frac{n!}{x!(n-x)!},
\]</span> Here the combinatorial function <span class="math inline">\({n \choose x}\)</span> counts the number of ways of getting <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials and <span class="math inline">\(n!=n(n-1)(n-2)\ldots 2 \cdot 1\)</span> counts the number of permutations of <span class="math inline">\(n\)</span> observations without replacement.</p>
<p>Plot below shows the probability mass function of a Binomial distribution with <span class="math inline">\(n = 20\)</span> and <span class="math inline">\(p = 0.3\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">size =</span> <span class="dv">20</span>, <span class="at">prob =</span> <span class="fl">0.3</span>),</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">names.arg =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">col =</span> <span class="st">"lightblue"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">"Number of Heads"</span>, <span class="at">ylab =</span> <span class="st">"Probability"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Probability Mass Function of a Binomial Distribution (n = 20, p = 0.3)</figcaption>
</figure>
</div>
</div>
</div>
<p>The table below shows the expected value and variance of a Binomial random variable. Those quantities can be calculated by plugging in the possible outcomes and corresponding probabilities into the definitions of expected value and variance.</p>
<table class="caption-top table">
<caption>Mean and Variance of Binomial</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Binomial Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = n p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = n p ( 1 - p )\)</span></td>
</tr>
</tbody>
</table>
<p>For large sample sizes <span class="math inline">\(n\)</span>, this distribution is approximately normal with mean <span class="math inline">\(np\)</span> and variance of <span class="math inline">\(np(1-p)\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of heads in three flips. Each possible outcome (“realization”) of <span class="math inline">\(X\)</span> is an <em>event</em>. Now consider the event of getting only two heads <span class="math display">\[
\{ X= 2\} = \{ HHT, HTH, THH \} ,
\]</span> The probability distribution of <span class="math inline">\(X\)</span> is Binomial with parameters <span class="math inline">\(n = 3, p= 1/2\)</span>, where <span class="math inline">\(n\)</span> denotes the sample size (a.k.a. number of trials) and <span class="math inline">\(p\)</span> is the probability of heads; we have a fair coin. The notation is <span class="math inline">\(X \sim \mathrm{Bin} \left ( n = 3 , p = \frac{1}{2} \right )\)</span> where the sign <span class="math inline">\(\sim\)</span> is read as <em>distributed as</em>.</p>
<table class="caption-top table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th style="text-align: right;"><span class="math inline">\(P(X=x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">HHH</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;"><span class="math inline">\(p^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">HHT</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1- p)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1 - p)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\((1-p)p^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p( 1-p)^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p ( 1-p)^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">TTH</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^2 p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">TTT</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^3\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="poisson-distribution" class="level3">
<h3 class="anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h3>
<p>The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. You can think of Poisson distribution as a limiting case of the Binomial distribution when the number of trials <span class="math inline">\(n\)</span> is large and the probability of success <span class="math inline">\(p\)</span> is small, such that <span class="math inline">\(np = \lambda\)</span> remains constant. Think of a soccer game where the goal is the “successful” event of interest. The soccer team does not have a predefined number of attempts to score a goal. Rather they continiously try to score a goal until they do. The number of goals scored in a game is the number of events occurring in a fixed interval of time. The mean number of goals scored in a game is the mean rate of events occurring in a fixed interval of time.</p>
<p>There are many examples of Poisson distributed random variables. For example, the number of phone calls received by a call center per hour, the number of emails received per day, the number of customers arriving at a store per hour, the number of defects in a manufactured product, the number of accidents on a highway per month.</p>
<p>A random variable <span class="math inline">\(X\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\lambda &gt; 0\)</span> if its probability mass function is given by: <span class="math display">\[
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\]</span> for <span class="math inline">\(k = 0, 1, 2, 3, \ldots\)</span>, where <span class="math inline">\(\lambda\)</span> is both the mean and variance of the distribution.</p>
<p>Plot below shows the probability mass function of a Poisson distribution with <span class="math inline">\(\lambda = 1, 5, 10\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Probability Mass Function of a Poisson Distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>We write <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span> to denote that <span class="math inline">\(X\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</p>
<table class="caption-top table">
<caption>Mean and Variance of Poisson</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Poisson Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = \lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = \lambda\)</span></td>
</tr>
</tbody>
</table>
<div id="exm-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.16 (Customer Arrivals)</strong></span> Suppose customers arrive at a coffee shop at an average rate of 3 customers per hour. What is the probability that exactly 5 customers will arrive in the next hour?</p>
<p>Using the Poisson distribution with <span class="math inline">\(\lambda = 3\)</span>: <span class="math display">\[
P(X = 5) = \frac{3^5 e^{-3}}{5!} = \frac{243 \times e^{-3}}{120} \approx 0.101
\]</span></p>
<p>So there is approximately a 10.1% chance that exactly 5 customers will arrive in the next hour.</p>
</div>
<p>The Poisson distribution can be derived as a limiting case of the binomial distribution when <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is small, such that <span class="math inline">\(np = \lambda\)</span> remains constant. This connection makes the Poisson distribution particularly useful for modeling rare events in large populations.</p>
</section>
<section id="normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="normal-distribution">Normal Distribution</h3>
<p>The Normal distribution is a continuous probability distribution that is widely used in statistics and probability theory. It is also known as the Gaussian distribution or the bell curve. The Normal distribution is characterized by its symmetric bell-shaped curve and is defined by two parameters: the mean (<span class="math inline">\(\mu\)</span>) and the variance (<span class="math inline">\(\sigma^2\)</span>).</p>
<p>The probability density function (PDF) of the Normal distribution is given by: <span class="math display">\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>, where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>The Normal distribution is often used to model real-world phenomena such as measurement errors, heights, weights, and scores on standardized tests. It is also used in hypothesis testing and confidence interval construction.</p>
<div id="exm-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.17 (Heights of Adults)</strong></span> The heights of adult males in a certain population are normally distributed with a mean of 70 inches and a standard deviation of 3 inches.</p>
<ol type="a">
<li>What is the probability that a randomly selected male is between 67 and 73 inches tall?</li>
</ol>
<p>Using the Normal distribution with <span class="math inline">\(\mu = 70\)</span> and <span class="math inline">\(\sigma = 3\)</span>:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-normal-height" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-height-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-prob_files/figure-html/fig-normal-height-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-height-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Normal Distribution (mu = 70, sigma = 3) with Highlighted Area
</figcaption>
</figure>
</div>
</div>
</div>
<p>The bell curve in <a href="#fig-normal-height" class="quarto-xref">Figure&nbsp;<span>1.2</span></a> shows the probability density function of a random variable <span class="math inline">\(X \sim N(70,3^2)\)</span> that follows a Normal distribution with mean 70 and standard deviation 3. As we can see, the Normal distribution is symmetric around the mean, and the mean, median, and mode are all equal. The probability of a randomly selected male being between 67 and 73 inches tall is the area under the curve between 67 and 73 inches and is approximately 0.6827.</p>
<p>Now let’s calculate this probability using <code>R</code>:</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>prob_between <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="dv">73</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">67</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"P(67 &lt; X &lt; 73) ="</span>, <span class="fu">round</span>(prob_between, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">## P(67 &lt; X &lt; 73) = 0.68</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The calculation shows that <span class="math inline">\(P(67 &lt; X &lt; 73) = P(X \leq 73) - P(X \leq 67) \approx 0.6827\)</span>.</p>
<p>This result makes sense because 67 and 73 inches are exactly one standard deviation below and above the mean (70 <span class="math inline">\(\pm\)</span> 3), respectively. According to the empirical rule (68-95-99.7 rule), approximately 68% of values in a normal distribution fall within one standard deviation of the mean.</p>
<p>So approximately 68.27% of adult males are between 67 and 73 inches tall.</p>
<ol start="2" type="a">
<li>What height corresponds to the 95th percentile?</li>
</ol>
<p>We need to find <span class="math inline">\(x\)</span> such that <span class="math inline">\(P(X \leq x) = 0.95\)</span>. From standard normal tables, <span class="math inline">\(\Phi^{-1}(0.95) \approx 1.645\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Height corresponding to the 95th percentile:"</span>, <span class="fu">round</span>(x, <span class="dv">2</span>), <span class="st">"inches</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Height corresponding to the 95th percentile: 75 inches</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Therefore: <span class="math inline">\(x = \mu + \sigma \cdot 1.645 = 70 + 3 \cdot 1.645 = 74.935\)</span> inches.</p>
</div>
</section>
</section>
<section id="conditional-marginal-and-joint-distributions" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="conditional-marginal-and-joint-distributions"><span class="header-section-number">1.6</span> Conditional, Marginal and Joint Distributions</h2>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which can be related to each other. Knowing <span class="math inline">\(X\)</span> would change your belief about <span class="math inline">\(Y\)</span>. For example, as a first pass, psychologists who study the phenomenon of happiness can be interested in understanding its relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.</p>
<p>Formally, the <em>joint distribution</em> of two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function given by <span class="math display">\[
P(x,y) = P(X=x,Y=y).
\]</span> This maps all combinations of possible values of these two variables to a probability on the interval [0,1].</p>
<p>The <em>conditional probability</em> is a measure of the probability of a random variable <span class="math inline">\(X\)</span>, given that the value of another random variable was observed <span class="math inline">\(Y = y\)</span>. <span class="math display">\[
P(x\mid y) = P(X = x \mid Y = y).
\]</span></p>
<p>The <em>marginal probability</em> of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the marginal probability <span class="math inline">\(P(X)\)</span> is the probability distribution of <span class="math inline">\(X\)</span> when the values of <span class="math inline">\(Y\)</span> are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of <span class="math inline">\(Y\)</span>. The converse is also true: the marginal distribution can be obtained for <span class="math inline">\(Y\)</span> by summing over the separate values of <span class="math inline">\(X\)</span>.</p>
<p>Marginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.</p>
<div id="exm-salary" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.18 (Salary-Happiness)</strong></span> Let’s look at an example. Suppose that to model the relationship between two quantities, salary <span class="math inline">\(Y\)</span> and happiness <span class="math inline">\(X\)</span>. After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Results of the Gallup survey. Rows are Salary (<span class="math inline">\(Y\)</span>) and columns are happiness (<span class="math inline">\(X\)</span>)</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">X = 0 (low)</th>
<th style="text-align: center;">X = 1 (medium)</th>
<th style="text-align: center;">X = 2 (high)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Y = low (0)</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = medium (1)</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Y = high (2)</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = very high (3)</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Each cell of the table is the joint probability, e.g.&nbsp;14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.</p>
<p>Now, if we want to answer the question what is the percent of high earners in the population. For that we need to calculate what is called a <em>marginal probability</em> <span class="math inline">\(P(y = 2)\)</span>. We can calculate the proportion of high earners <span class="math inline">\(P(y = 2)\)</span> by summing up the entries in the third row of the table, which is 0.17 in our case.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.07</span> <span class="sc">+</span> <span class="fl">0.01</span> <span class="sc">+</span> <span class="fl">0.09</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.17</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Formally marginal probability over <span class="math inline">\(y\)</span> is calculated by summing the joint probability over the other variable, <span class="math inline">\(x\)</span>, <span class="math display">\[
p(y) = \sum_{x \in S}p(x,y)
\]</span> Where <span class="math inline">\(S\)</span> is the set of all possible values of the random variable <span class="math inline">\(X\)</span>.</p>
<!--     Salary ($S$)   0 (low)   1 (medium)   2 (high) -->
<!--   -------------- --------- ------------ ---------- -->
<!--            low 0      0.03         0.12       0.07 -->
<!--         medium 1      0.02         0.13       0.11 -->
<!--           high 2      0.01         0.13       0.14 -->
<!--      very high 3      0.01         0.09       0.14 -->
<p>Another question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the <em>conditional probability</em> of <span class="math inline">\(X\)</span> given that the value of variable <span class="math inline">\(Y\)</span> is known. This is denoted by <span class="math inline">\(P(X=x\mid Y=y)\)</span> or simply <span class="math inline">\(p(x\mid y)\)</span>, where <span class="math inline">\(\mid\)</span> reads as “given” or “conditional upon”.</p>
<p>The conditional probability <span class="math inline">\(p(x\mid y)\)</span> also has interpretation as updating your probability over <span class="math inline">\(X\)</span> after you have learned the new information about <span class="math inline">\(Y\)</span>. In this sense, probability is also the language of how you change opinions in light of new evidence. The proportion of happy people among high earners is given by the conditional probability <span class="math inline">\(P(X=2\mid Y=2)\)</span> and can be calculated by dividing the proportion of those who are high earners and highly happy by the proportion of high earners <span class="math display">\[
P(X=2\mid Y=2) = \dfrac{P(X=2,Y=2)}{P(Y=2)} = \dfrac{0.09}{0.17} = 0.5294118.
\]</span></p>
<p>Now, if we compare it with the proportion of highly happy people <span class="math inline">\(P(X = 2) = 0.38\)</span>, we see that on average you are more likely to be happy given your income is high.</p>
</div>
</section>
<section id="independence" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="independence"><span class="header-section-number">1.7</span> Independence</h2>
<p>Historically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.</p>
<p>Using the notion of conditional probability, we can define independence of two variables. Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <em>independent</em> if <span class="math display">\[
P(Y = y \mid X = x) = P(Y = y),
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. That is, learning information <span class="math inline">\(X=x\)</span> doesn’t affect our probabilistic assessment of <span class="math inline">\(Y\)</span> for any value <span class="math inline">\(y\)</span>. In the case of independence, <span class="math inline">\(p(x \mid y) = p(x)\)</span> and <span class="math inline">\(p(y \mid x) = p(y)\)</span>.</p>
<p>Conditional probabilities are counter-intuitive. For example, one of the most important properties is typically <span class="math inline">\(p( x \mid y ) \neq p( y\mid x )\)</span>. Confusing these two—specifically equating <span class="math inline">\(P(A|B)\)</span> with <span class="math inline">\(P(B|A)\)</span>—is a common error known as the <em>Prosecutor’s Fallacy</em>.</p>
<p>We just derived an important relation that allows us to calculate conditional probability <span class="math inline">\(p(x \mid y)\)</span> when we know joint probability <span class="math inline">\(p(x,y)\)</span> and marginal probability <span class="math inline">\(p(y)\)</span>. The total probability or evidence can be calculated as usual, via <span class="math inline">\(p(y) = \sum_{x}p(x,y)\)</span>.</p>
<p>We will see that independence will lead to a different conclusion than the Bayes conditional probability decomposition: specifically, independence yields <span class="math inline">\(p( x,y ) = p(x) p(y)\)</span> whereas Bayes implies <span class="math inline">\(p(x ,y) = p(x)p(y \mid x)\)</span>.</p>
</section>
<section id="sec-dutch-book" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="sec-dutch-book"><span class="header-section-number">1.8</span> Further Notes: Dutch Book Arguments</h2>
<p>If probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.</p>
<p>The starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “<em>It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event</em>” (p.&nbsp;101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.</p>
<p>Formally, for any event <span class="math inline">\(A\)</span>, the identity <span class="math display">\[
P(A)  =\frac{1}{1+\text{odds}(A)}\text{ or }\text{odds}(A)=\frac{1-P(A)}{P(A)},
\]</span> where <span class="math inline">\(\bar A\)</span> is the complement of <span class="math inline">\(A\)</span>, links odds and probabilities. Throughout, we use <span class="math inline">\(P\)</span> as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is <span class="math inline">\(1/3\)</span>. The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are <em>fair</em> if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.</p>
<p>In constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “<em>could have a book made against him by a cunning bettor and would then stand to lose in any event</em>” (Ramsey (1931), p.&nbsp;22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional probability. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.</p>
<p>Dutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, <span class="math inline">\(S\)</span>. The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.</p>
<p>To derive the rules, consider the first axiom of probability: for any event <span class="math inline">\(A\)</span>, <span class="math inline">\(0\leq P(A) \leq 1\)</span>. Suppose that the odds imply probabilities <span class="math inline">\(P(A)\)</span> for <span class="math inline">\(A\)</span> occurring and <span class="math inline">\(P(\bar A)\)</span> for other outcomes, with associated payoffs of <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>. Then, having bet <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>, the gains if <span class="math inline">\(A\)</span> or <span class="math inline">\(\bar A\)</span> occur, <span class="math inline">\(G_{A}\)</span> and <span class="math inline">\(G_{\bar A}\)</span>, respectively, are <span class="math display">\[\begin{align*}
G(A)   &amp;  =S_{A}-P(A) S_{A}-P(\bar A)  S_{\bar A}\\
G(\bar A)   &amp;  =S_{\bar A}-P(A) S_{A}-P(\bar A)  S_{\bar A}.
\end{align*}\]</span> To see this, note that the bettor receives <span class="math inline">\(S_{A}\)</span> and pays <span class="math inline">\(P(A) S_{A}\)</span> for a bet on event <span class="math inline">\(A\)</span>. The bookie can always choose to place a zero stake on <span class="math inline">\(\bar A\)</span> occurring, which implies that <span class="math inline">\(G(A) =S_{A}-P(A) S_{A}\)</span> and <span class="math inline">\(G\left(\bar A\right) =-P(A) S_{A}\)</span>. Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus <span class="math inline">\(G(A) G(\bar A) \leq 0\)</span>. Substituting, <span class="math inline">\(\left( 1-P(A) \right) P(A) \geq0\)</span> or <span class="math inline">\(0\leq P(A) \leq 1\)</span>, which is the first axiom of probability. The second axiom, that the set of all possible outcomes has probability <span class="math inline">\(1\)</span>, is similarly straightforward to show.</p>
<p>The third axiom is that probabilities add, that is, for two disjoint events <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span>, <span class="math inline">\(P(A) =P\left( A_{1} \text{ or } A_{2}\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>. Assuming stakes sizes of <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}},\)</span> and <span class="math inline">\(S_{A_{2}}\)</span> (and zero stakes on their complements) there are three possible outcomes. If neither <span class="math inline">\(A_{1}\)</span> nor <span class="math inline">\(A_{2}\)</span> occur, the gain is <span class="math display">\[
G(\bar A)  =-P(A)  S_{A} -P\left(  A_{1}\right)  S_{A_{1}}-P\left( A_{2}\right)  S_{A_{2}}.
\]</span></p>
<p>If <span class="math inline">\(A_{1}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and the gain is <span class="math display">\[
G\left(  A_{1}\right)  =\left(  1-P(A) \right)  S_{A}+\left(  1-P\left(  A_{1}\right)  \right) S_{A_{1}}-P\left(  A_{2}\right)  S_{A_{2}},
\]</span> and finally if <span class="math inline">\(A_{2}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and <span class="math display">\[
G\left(  A_{2}\right)  =\left(  1-P(A) \right)  S_{A}-P\left(  A_{1}\right)  S_{A_{1}}+\left( 1-P\left(  A_{2}\right)  \right)  S_{A_{2}}.
\]</span> Arranging these into a matrix equation, <span class="math inline">\(G=PS\)</span>: <span class="math display">\[
\left( \begin{array}
[c]{c}%
G(\bar A) \\
G\left(  A_{1}\right) \\
G\left(  A_{2}\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
-P\left(  A_{2}\right) \\
1-P(A)  &amp; 1-P\left(  A_{1}\right)  &amp;
-P\left(  A_{2}\right) \\
1-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
1-P\left(  A_{2}\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{A}\\
S_{A_{1}}\\
S_{A_{2}}%
\end{array}
\right).
\]</span></p>
<p>The absence of a Dutch book arbitrage implies that there is no set of stakes, <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}}\)</span>, and <span class="math inline">\(S_{A_{2}}\)</span>, such that the winnings in all three events are positive. If the matrix <span class="math inline">\(P\)</span> is invertible, it is possible to find stakes with positive gains. To rule this out, the determinant of <span class="math inline">\(P\)</span> must be zero, which implies that <span class="math inline">\(0=-P(A) +P\left(A_{1}\right) +P\left( A_{2}\right)\)</span>, or <span class="math inline">\(P\left(A\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>.</p>
<p>The fourth axiom is conditional probability. Consider an event <span class="math inline">\(B\)</span>, with <span class="math inline">\(P\left( B\right) &gt;0\)</span>, an event <span class="math inline">\(A\)</span> that occurs conditional on <span class="math inline">\(B\)</span>, and the event that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur. The probabilities or prices of these bets are <span class="math inline">\(P\left( B\right)\)</span>, <span class="math inline">\(P\left( A \mid B\right)\)</span>, and <span class="math inline">\(P\left( A \text{ and } B\right)\)</span>. Consider bets with stakes <span class="math inline">\(S_{B}\)</span>, <span class="math inline">\(S_{A \mid B}\)</span> and <span class="math inline">\(S_{A \text{ and } B}\)</span>, with the understanding that if <span class="math inline">\(B\)</span> does not occur, the conditional bet on <span class="math inline">\(A\)</span> is canceled. The payoffs to the events that <span class="math inline">\(B\)</span> does not occur, <span class="math inline">\(B\)</span> occurs but not <span class="math inline">\(A\)</span>, and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur, are <span class="math display">\[
\left( \begin{array}
[c]{c}%
G\left(  \bar B\right) \\
G\left(  \bar A \text{ and } B\right) \\
G\left(  A \text{ and } B\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp; 0\\
1-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp;
-P\left(  A \mid B\right) \\
1-P\left(  B\right)  &amp; 1-P\left(  A \text{ and } B\right)  &amp;
1-P\left(  A \mid B\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{B}\\
S_{A \text{ and } B}\\
S_{A \mid B}%
\end{array}
\right).
\]</span> Similar arguments imply the determinant must be zero, which implies that <span class="math display">\[
P\left(  A \mid B\right)  =\frac{P\left(  A \text{ and } B\right) }{P\left(  B\right)  },
\]</span> which is the law of conditional probability, given <span class="math inline">\(P(B)&gt;0\)</span>, of course, otherwise the conditional probability is not defined, and the <span class="math inline">\(P\)</span> matrix has determinant 0.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bachelier1900theorie" class="csl-entry" role="listitem">
Bachelier, Louis. 1900. <span>“Théorie de La Spéculation.”</span> PhD thesis, Paris: Université de Paris.
</div>
<div id="ref-definetti1937foresight" class="csl-entry" role="listitem">
de Finetti, Bruno. 1937. <span>“Foresight: <span>Its</span> Logical Laws, Its Subjective Sources.”</span> In <em>Studies in Subjective Probability</em>, edited by Henry E. Kyburg and Howard E. Smokler, 93–158. New York: Wiley.
</div>
<div id="ref-diaconis1989methods" class="csl-entry" role="listitem">
Diaconis, Persi, and Frederick and Mosteller. 1989. <span>“Methods for <span>Studying Coincidences</span>.”</span> <em>Journal of the American Statistical Association</em> 84 (408): 853–61.
</div>
<div id="ref-kolmogoroff1933grundbegriffe" class="csl-entry" role="listitem">
Kolmogoroff, Andrei. 1933. <em>Grundbegriffe Der <span>Wahrscheinlichkeitsrechnung</span></em>. Vol. 2. Ergebnisse Der <span>Mathematik</span> Und <span>Ihrer Grenzgebiete</span>. Berlin: Springer.
</div>
<div id="ref-kolmogorov1933grundbegriffe" class="csl-entry" role="listitem">
Kolmogorov, Andrey N. 1933. <em>Grundbegriffe Der <span>Wahrscheinlichkeitsrechnung</span></em>. Berlin: Springer.
</div>
<div id="ref-poincare1952science" class="csl-entry" role="listitem">
Poincaré, Henri. 1952. <em>Science and Hypothesis</em>. New York]: Dover Publications.
</div>
<div id="ref-polson2025negative" class="csl-entry" role="listitem">
Polson, Nick, and Vadim Sokolov. 2025. <span>“Negative <span>Probability</span>.”</span> <em>Applied Stochastic Models in Business and Industry</em> 41 (1): e2910.
</div>
<div id="ref-ramsey1926truth" class="csl-entry" role="listitem">
Ramsey, Frank P. 1926. <span>“Truth and <span>Probability</span>.”</span> Histoy of {{Economic Thought Chapters}}. McMaster University Archive for the History of Economic Thought.
</div>
<div id="ref-shimony1955coherence" class="csl-entry" role="listitem">
Shimony, Abner. 1955. <span>“Coherence and the <span>Axioms</span> of <span>Confirmation</span>.”</span> <em>The Journal of Symbolic Logic</em> 20 (1): 1–28.
</div>
<div id="ref-ville1939etude" class="csl-entry" role="listitem">
Ville, Jean. 1939. <span>“Étude Critique de La Notion de Collectif.”</span> Thèses de l’entre-<span>Deux-Guerres</span>. PhD thesis, Université de Paris.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-intro.html" class="pagination-link" aria-label="The Modern AI Playbook">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">The Modern AI Playbook</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-bayes.html" class="pagination-link" aria-label="Bayes Rule">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>