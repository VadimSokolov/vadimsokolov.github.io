<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./02-bayes.html" rel="next">
<link href="./00-intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/hilary.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="1&nbsp; Probability and Uncertainty – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/hilary.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Exercises</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#bernoullis-problem" id="toc-bernoullis-problem" class="nav-link active" data-scroll-target="#bernoullis-problem"><span class="header-section-number">1.1</span> Bernoulli’s Problem</a></li>
  <li><a href="#kolmogorov-axioms" id="toc-kolmogorov-axioms" class="nav-link" data-scroll-target="#kolmogorov-axioms"><span class="header-section-number">1.2</span> Kolmogorov Axioms</a></li>
  <li><a href="#dutch-book-and-the-rules-of-probability" id="toc-dutch-book-and-the-rules-of-probability" class="nav-link" data-scroll-target="#dutch-book-and-the-rules-of-probability"><span class="header-section-number">1.3</span> Dutch book and the rules of probability</a></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables"><span class="header-section-number">1.4</span> Random Variables</a>
  <ul class="collapse">
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables">Continuous Random Variables</a></li>
  <li><a href="#the-inverse-cdf-method" id="toc-the-inverse-cdf-method" class="nav-link" data-scroll-target="#the-inverse-cdf-method">The Inverse CDF Method</a></li>
  <li><a href="#functional-transformations" id="toc-functional-transformations" class="nav-link" data-scroll-target="#functional-transformations">Functional Transformations</a></li>
  </ul></li>
  <li><a href="#expectation-and-variance-reward-and-risk" id="toc-expectation-and-variance-reward-and-risk" class="nav-link" data-scroll-target="#expectation-and-variance-reward-and-risk"><span class="header-section-number">1.5</span> Expectation and Variance (Reward and Risk)</a>
  <ul class="collapse">
  <li><a href="#standard-deviation-and-covariance" id="toc-standard-deviation-and-covariance" class="nav-link" data-scroll-target="#standard-deviation-and-covariance">Standard Deviation and Covariance</a></li>
  <li><a href="#portfolios-linear-combinations" id="toc-portfolios-linear-combinations" class="nav-link" data-scroll-target="#portfolios-linear-combinations">Portfolios: linear combinations</a></li>
  </ul></li>
  <li><a href="#commonly-used-distributions" id="toc-commonly-used-distributions" class="nav-link" data-scroll-target="#commonly-used-distributions"><span class="header-section-number">1.6</span> Commonly Used Distributions</a>
  <ul class="collapse">
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution">Bernoulli Distribution</a></li>
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution">Poisson Distribution</a></li>
  <li><a href="#normal-distribution" id="toc-normal-distribution" class="nav-link" data-scroll-target="#normal-distribution">Normal Distribution</a></li>
  </ul></li>
  <li><a href="#conditional-marginal-and-joint-distributions" id="toc-conditional-marginal-and-joint-distributions" class="nav-link" data-scroll-target="#conditional-marginal-and-joint-distributions"><span class="header-section-number">1.7</span> Conditional, Marginal and Joint Distributions</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence"><span class="header-section-number">1.8</span> Independence</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./01-prob.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><em>“It is remarkable that a science which began with the consideration of games of chance should have become the most important object of human knowledge…”</em> Pierre-Simon Laplace, 1812</p>
</blockquote>
<p>Probability deals with randomness. The art of data science is being able to “separate” signal from noise. For example, we need to account for randomness in human behavior. A random phenomenon, by its very nature, means a precise prediction of an outcome has to be described by a distribution. Surprisingly, random events typically have statistical regularity in many ways. For example, if we flip a coin, it would be hard to predict the outcome (head or tail) on an individual flip, but if we flip a coin many times and count the proportion of heads, the average will converge to something close to <span class="math inline">\(1/2\)</span>. This is called the law of large numbers.</p>
<p>Probability is a language that lets you communicate information about uncertain outcomes and events. By assigning a numeric value between zero and one to an event, or a collection of outcomes, in its simplest form, probability measures how likely an event is to occur.</p>
<p>Our goal here is to introduce you to the concepts of probability, conditional probability and their governing rules. The crowning achievement being Bayes rule for updating conditional probabilities. Understanding these concepts serves as a basis for more complex data analysis and machine learning algorithms. Building probabilistic models has many challenges and real world applications. You are about to learn about practical examples from fields as diverse as medical diagnosis, chess games to racetrack odds.</p>
<p>We start by defining probabilities of a finite number of events. An axiomatic approach was proposed by Kolmogorov. This approach is very powerful and allows us to derive many important results and rules for calculating probabilities. Furthermore, in this chapter, we will discuss the notion of conditional probability and independence as well as tools for summarizing the distribution of a random variable, namely expectation and variance.</p>
<p>The study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts because when we repeat these games the likelihood of different outcomes remains (mostly) the same (statistical regularity). The first rigorous treatment of probability was presented by Jacob Bernoulli in his paper “Ars Conjectandi” (art of guesses) where he claims that to make a guess is the same thing as to measure a probability.</p>
<section id="bernoullis-problem" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="bernoullis-problem"><span class="header-section-number">1.1</span> Bernoulli’s Problem</h2>
<p>Bernoulli considered the following problem. Suppose that we observe <span class="math inline">\(m\)</span> successes and <span class="math inline">\(n\)</span> failures of an event <span class="math inline">\(A\)</span>, out of total <span class="math inline">\(N=m+n\)</span> trials. How do we assign a probability to the event <span class="math inline">\(A\)</span>? A classic definition of the probability (due to Jacob Bernoulli) is the ratio of number of favorable outcomes <span class="math inline">\(m\)</span> to the total number of outcomes <span class="math inline">\(N\)</span>, which is the sum of <span class="math inline">\(m\)</span> and the number of unfavorable outcomes <span class="math inline">\(n\)</span> <span class="math display">\[
P_N = \dfrac{m}{m+n} = \dfrac{m}{N}.
\]</span></p>
<p>Moreover, can we construct a law of succession? What is the probability that the next trial will be a success, given that there are uncertainties in the underlying probabilities? <span class="citation" data-cites="keynes1921treatise">Keynes (<a href="references.html#ref-keynes1921treatise" role="doc-biblioref">1921</a>)</span> considered the rule of succession a.k.a. induction. For example, Bernoulli proposed that <span class="math display">\[
P_{N+1} = \dfrac{m+1}{N+2}.
\]</span> <span class="citation" data-cites="keynes1921treatise">Keynes (<a href="references.html#ref-keynes1921treatise" role="doc-biblioref">1921</a>)</span> (p.&nbsp;371) provided a fully Bayesian model based on what we know today as the Beta-Binomial model. <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a> provides a full analysis. The determination of the predictive rule is equivalent to the problem of finding a sufficient statistic (a.k.a. summary statistic) and performing feature engineering in modern day artificial intelligence applications.</p>
<p>de Finetti puts this in the framework of exchangeable random variables, see <span class="citation" data-cites="kreps1988notes">Kreps (<a href="references.html#ref-kreps1988notes" role="doc-biblioref">1988</a>)</span> for further discussion. Jeffreys provides an alternative approach based on the principle of indifference. <span class="math display">\[
P_{N+1} = \dfrac{m+1/2}{N+1}.
\]</span> Ramsey (1926) and de Finetti (1937) and Savage (1956) use a purely axiomatic approach in an effort to operationalize probability. In a famous quote de Finetti says “the probability does not exist”. In this framework, probability is subjective and operationalized as a willingness to bet. If a gamble <span class="math inline">\(A\)</span> pays $1 if it happens and $0 otherwise, then the willingness to bet 50 cents to enter the gamble implies the subjective probability of <span class="math inline">\(A\)</span> is 0.5. Contrary to the frequentist approach, the probability is not a property of the event, but a property of the person. This is the basis for the Bayesian approach to probability.</p>
<p>Leonard Jimmie Savage, an American statistician, developed a decision theory framework known as the “Savage axioms” or the “Sure-Thing Principle.” This framework is a set of axioms that describe how a rational decision-maker should behave in the face of uncertainty. These axioms provide a foundation for subjective expected utility theory.</p>
<p>The Savage axioms consist of three main principles:</p>
<ol type="1">
<li><p><em>Completeness axiom.</em> This axiom assumes that a decision-maker can compare and rank all possible outcomes or acts in terms of preferences. In other words, for any two acts (or lotteries), the decision-maker can express a preference for one over the other, or consider them equally preferable.</p></li>
<li><p><em>Transitivity axiom.</em> This axiom states that if a decision-maker prefers act A to act B and prefers act B to act C, then they must also prefer act A to act C. It ensures that the preferences are consistent and do not lead to cycles or contradictions.</p></li>
<li><p><em>Continuity axiom (Archimedean axiom).</em> The continuity axiom introduces the concept of continuity in preferences. It implies that if a decision-maker prefers act A to act B, and B to C, then there exists some probability at which the decision-maker is indifferent between A and a lottery that combines B and C. This axiom helps to ensure that preferences are not too “discontinuous” or erratic.</p></li>
</ol>
<p>Savage’s axioms provide a basis for the development of subjective expected utility theory. In this theory, decision-makers are assumed to assign subjective probabilities to different outcomes and evaluate acts based on the expected utility, which is a combination of the utility of outcomes and the subjective probabilities assigned to those outcomes.</p>
<p>Savage’s framework has been influential in shaping the understanding of decision-making under uncertainty. It allows for a more flexible approach to decision theory that accommodates subjective beliefs and preferences. However, it’s worth noting that different decision theorists may have alternative frameworks, and there are ongoing debates about the appropriateness of various assumptions in modeling decision-making.</p>
<p>Frequency probability is based on the idea that the probability of an event can be found by repeating the experiment many times and probability arises from some random process on the sample space (such as random selection). For example, if we toss a coin many times, the probability of getting a head is the number of heads divided by the total number of tosses. This is the basis for the frequentist approach to probability.</p>
<p>Another way, sometimes more convenient, to talk about uncertainty and to express probabilities via odds, such as 9 to 2 or 3 to 1. We assign odds “on <span class="math inline">\(A\)</span>” or “against <span class="math inline">\(A\)</span>”. For example, when we say that the odds on a Chicago Bears’ Super Bowl win are 2 to 9, it means that if they are to play 11 times (9+2), they will win 2 times. If <span class="math inline">\(A\)</span> is the win event, then odds on <span class="math inline">\(A\)</span> <span class="math display">\[
O(A) = \dfrac{P(\mbox{not A}) }{P(A)} = \dfrac{1-P(A)}{P(A)}
\]</span> Equivalently, probabilities can be determined from odds <span class="math display">\[
P(A) = \dfrac{1}{1+O(A)}
\]</span> For example, if the odds are one <span class="math inline">\(O(A) = 1\)</span>, then for every $1 bet you will pay out $1. This event has probability <span class="math inline">\(0.5\)</span>.</p>
<p>If <span class="math inline">\(O(A) = 2\)</span>, then you are willing to offer <span class="math inline">\(2:1\)</span>. For a $1 bet you’ll payback $3. In terms of probability <span class="math inline">\(P(A) = 1/3\)</span>.</p>
<p>Odds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.</p>
<div id="exm-odds" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Odds)</strong></span> One of the main sources of prediction markets is bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. <a href="#fig-odds" class="quarto-xref">Figure&nbsp;<span>1.1</span></a> shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hillary Clinton would win over Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.</p>
<div id="fig-odds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/hilary.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-odds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Presidential Odds 2016
</figcaption>
</figure>
</div>
<p>Ahead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hillary Clinton. The best odds on Clinton are <span class="math inline">\(1/3\)</span>, this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the Market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides, otherwise they are risking to stay out of business.</p>
</div>
<div id="exm-derby" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Kentucky Derby)</strong></span> The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is <span class="math inline">\(1 \frac{1}{4}\)</span> miles (2 kilometers) and is the first time the three-year old horses have raced the distance.</p>
<p>There was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have won many times in a row. The market is getting better at predicting who’s going to win. Here’s the data</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Horse Name</th>
<th>Year</th>
<th>Odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Spectacular Bid</td>
<td>1979</td>
<td>0.6/1</td>
</tr>
<tr class="even">
<td>Fusaichi Pegasus</td>
<td>2000</td>
<td>2.3/1</td>
</tr>
<tr class="odd">
<td>Street Sense</td>
<td>2007</td>
<td>9/2</td>
</tr>
<tr class="even">
<td>Big Brown</td>
<td>2008</td>
<td>5/2</td>
</tr>
</tbody>
</table>
<p>Recently, favorites have had a lot more success</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Horse Name</th>
<th>Year</th>
<th>Odds</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>California Chrome</td>
<td>2014</td>
<td>5/2</td>
</tr>
<tr class="even">
<td>American Pharoah</td>
<td>2015</td>
<td>2/1</td>
</tr>
<tr class="odd">
<td>Nyquist</td>
<td>2016</td>
<td>3.3/1</td>
</tr>
<tr class="even">
<td>Always Dreaming</td>
<td>2017</td>
<td>5.2/1</td>
</tr>
</tbody>
</table>
<p>The most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only other horse that in 2001 has broken two minutes at odds 11.5/1.</p>
</div>
<div id="exm-harville" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Exacta Betting and the Harville Formula)</strong></span> How can probability help you with betting on the race? There are many different types of bets, and probability can help you find <em>fair</em> odds. The Derby is a Grade 1 stakes race for three-year-old thoroughbred horses. Colts and geldings carry 126 pounds and fillies 121. The odds are set by pari-mutuel betting by the public. After all the wagers have been placed, the racetrack takes a fee (18%). After the winning horse passes the finishing line, the pool of money is redistributed to the winning tickets. Random betting therefore loses you 18%, so it’s important to learn some empirical facts to try and tilt the odds in your favor.</p>
<p>For example, you can place bets as follows:</p>
<ul>
<li><em>Win</em>: “$2 win horse 1”</li>
<li><em>Straight Exacta</em>: “$2 exacta 1 <em>with</em> 2”<br>
</li>
<li><em>Exacta Box</em>: “$2 exacta box 1 <em>and</em> 2” You win with <em>either</em> order: 2 bets = $4.</li>
</ul>
<p>Consider a hypothetical race where <em>Sovereignty</em> wins at 9/1 odds and <em>Journalism</em> comes second at 7/2 odds. For a $2 bet on <em>Sovereignty</em> to Win at 9/1, the payout would be <span class="math inline">\(2 \cdot 9 + 2 = \$20\)</span> (the 9/1 win plus your initial $2 bet returned).</p>
<p>Let’s figure out the <em>fair</em> value for an exacta bet given that you know the win odds. This is known as the <em>Harville formula</em>. The exacta is probably one of the most popular bets for many horseplayers, corresponding to predicting the first two horses in the correct order.</p>
<p>The Harville formula provides an answer. We use the rule of conditional probability. The probability for the straight exacta of horses <span class="math inline">\(A\)</span> beating horse <span class="math inline">\(B\)</span> is: <span class="math display">\[
P(A \text{ beats } B) = P(A \text{ Wins}) \cdot P(B \text{ Second} \mid A \text{ Wins})
\]</span></p>
<p>A reasonable assessment of <span class="math inline">\(P(B \text{ Second} \mid A \text{ Wins})\)</span> can be derived as follows. For horse <span class="math inline">\(B\)</span> to come second, imagine taking horse <span class="math inline">\(A\)</span> out of the betting pool and re-distributing the probability to <span class="math inline">\(B\)</span>. This gives: <span class="math display">\[
P(B \text{ Second} \mid A \text{ Wins}) = \frac{P(B \text{ Wins})}{1 - P(A \text{ Wins})}
\]</span></p>
<p>In total, the fair price for the exacta is: <span class="math display">\[
P(A \text{ beats } B) = P(A \text{ Wins}) \cdot \frac{P(B \text{ Wins})}{1 - P(A \text{ Wins})}
\]</span></p>
<p>Therefore, we have: <span class="math display">\[
p_{12} = p_1 \cdot \frac{p_2}{1-p_1} \text{ where } p_1 = \frac{1}{1+O_1}, p_2 = \frac{1}{1+O_2}
\]</span></p>
<p>Solving for odds, we get the <em>Harville formula</em>: <span class="math display">\[
O_{12} = O_1(1 + O_2) - 1
\]</span></p>
<p>Using our example with 9/1 and 7/2 odds: <span class="math inline">\(O_{12} = 9 \cdot (1 + 3.5) - 1 = 39.5/1\)</span>.</p>
<p>Notice that the actual payout is determined solely by the volume of money wagered on that combination. There’s no requirement it matches our probabilistic analysis. However, the Harville formula gives us an idea of fair value. Some bettors searching for value try to find significantly undervalued exacta bets relative to the Harville formula.</p>
<p>There are many other factors to consider: jockey performance, bloodlines, and post positions can all matter significantly in determining the actual race outcome.</p>
</div>
<div id="exm-odds2" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Boy-Girl Paradox)</strong></span> If a woman has two children and one is a girl, the chance that the other child is also female has to be <span class="math inline">\(50-50\)</span>, right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, and I’m still not sure I believe this, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are <span class="math display">\[
BB \; \; BG \; \; GB \; \; GG
\]</span> Conditional on the information that one is a girl means that you know we can’t have the <span class="math inline">\(BB\)</span> scenario. Hence we are left with three possibilities <span class="math display">\[
BG \; \; GB \; \; GG
\]</span> In one of these is the other a girl. Hence <span class="math inline">\(1/3\)</span>.</p>
<p>It’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is <span class="math inline">\(1/2\)</span> as there are two possibilities <span class="math display">\[
GB \; \; GG
\]</span> This leads to the probability of <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-galton" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Galton Paradox)</strong></span> You flip three fair coins. What is the <span class="math inline">\(P(\text{all} \; \text{alike})\)</span>?</p>
<p>Assuming a fair coin (i.e.&nbsp;<span class="math inline">\(p(H) = p(T) = 1/2\)</span>), a formal approach might consist of computing the probability for all heads or all tails, which is</p>
<p><span class="math display">\[\begin{align*}
p(HHH) &amp;\equiv p(H \text{ and } H \text{ and } H) \\
&amp;= p(H)\times p(H)\times p(H) \\
&amp;= \left(\frac{1}{2}\right)^3
\end{align*}\]</span> and, since we’re ultimately interested in the probability of either (mutually exclusive) case, <span class="math display">\[\begin{align*}
P(\text{all alike}) &amp;= P(HHH \text{ or } TTT) \\
&amp;= P(HHH) + P(TTT) \\
&amp;= 2 \times \frac{1}{8}
\end{align*}\]</span></p>
<p>One could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, <span class="math display">\[
p(\text{all alike}) = 1/2
\]</span> The fault lies somewhere within the terms “remaining/last” and their connotation. A faulty symmetry assumption is being made in that statement pertaining to the distribution of the “remaining/last” coin. Loosely put, you’re certain to ultimately be in the case where at least two are alike, as stated in the above argument, but within each case the probability of landing the “remaining/last” matching <span class="math inline">\(H\)</span> or <span class="math inline">\(T\)</span> is not <span class="math inline">\(1/2\)</span>, due to the variety of ways you can arrive at two matching coins.</p>
<p>For a real treatment of the subject, we highly recommend reading Galton’s essay at <a href="http://galton.org/essays/1890-1899/galton-1894-chances.pdf">galton.org</a>.</p>
</div>
<div id="exm-odds3" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Three Cards)</strong></span> Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that it’s red. What’s the probability the other side is red? <span class="math inline">\(1/2\)</span>? No, it’s <span class="math inline">\(2/3\)</span>! By a similar logic there are six initial possibilities <span class="math display">\[
B_1 B_2 \; \; B_2 B_1 \; \; B R \; \; R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> where <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> index the sides of the same colored cards.</p>
<p>If we now condition on the top side being red we see that there are still three possibilities left <span class="math display">\[
R B \; \; R_1 R_2 \; \; R_2 R_1
\]</span> Hence the probability is <span class="math inline">\(2/3\)</span> and not the intuitive <span class="math inline">\(1/2\)</span>.</p>
</div>
<div id="exm-Patriots" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (NFL: New England Patriots Coin Toss)</strong></span> Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The NFL team New England Patriots won 19 out of 25 coin tosses in the 2014-15 season. What is the probability of this happening?</p>
<p>Let <span class="math inline">\(X\)</span> be a random variable equal to <span class="math inline">\(1\)</span> if the Patriots win and <span class="math inline">\(0\)</span> otherwise. It’s reasonable to assume <span class="math inline">\(P(X = 1) = \frac{1}{2}\)</span>. The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is <span class="math inline">\((1/2)^{25}\)</span>. We can code a typical sequence as, <span class="math display">\[
1,1,1,\ldots,1,0,0,\ldots,0.
\]</span> There are <span class="math inline">\(177,100\)</span> different sequences of 25 games where the Patriots win 19. There are <span class="math inline">\(25! = 1\cdot 2\cdot \ldots \cdot 25\)</span> ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are <span class="math inline">\(19!\)</span> ways to re-arrange the ones and <span class="math inline">\(6!\)</span> ways to rearrange the sequence of zeroes. Thus, the total number of different winning sequences is</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">factorial</span>(<span class="dv">25</span>)<span class="sc">/</span>(<span class="fu">factorial</span>(<span class="dv">19</span>)<span class="sc">*</span><span class="fu">factorial</span>(<span class="dv">25-19</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 177100</code></pre>
</div>
</div>
<p>Each potential sequence has probability <span class="math inline">\(0.5^{25}\)</span>, thus <span class="math display">\[
P\left(\text{Patriots win 19 out of 25 tosses}\right) =  177,100 \times 0.5^{25} = 0.005
\]</span></p>
<p>Often, it is easier to communicate uncertainties in the form of odds. In terms of betting odds of <span class="math inline">\(1:1\)</span> gives <span class="math inline">\(P = \frac{1}{2}\)</span>, odds of <span class="math inline">\(2:1\)</span> (I give <span class="math inline">\(2\)</span> for each <span class="math inline">\(1\)</span> you bet) is <span class="math inline">\(P = \frac{1}{3}\)</span>.</p>
<p>Remember, odds, <span class="math inline">\(O(A)\)</span>, is the ratio of the probability of happening over not happening, <span class="math display">\[
O(A) = (1 - P(A))/P(A),
\]</span> equivalently, <span class="math display">\[
P(A) = \frac{1}{1 + O(A)}.
\]</span></p>
<p>The odds of the Patriots winning sequence are then 1 to 199</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.005</span><span class="sc">/</span>(<span class="dv">1</span><span class="fl">-0.005</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.005</code></pre>
</div>
</div>
</div>
<div id="exm-Pete" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 (Hitting Streak)</strong></span> Pete Rose of the Cincinnati Reds set a National League record of hitting safely in <span class="math inline">\(44\)</span> consecutive games. How likely is such a long sequence of safe hits to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once in every one of those 44 games. Here are a couple of facts we know about him:</p>
<ol type="1">
<li>Rose was a <span class="math inline">\(300\)</span> hitter, he hits safely 3 times out of 10 attempts</li>
<li>Each at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.</li>
</ol>
<p>Assuming he comes to bat <span class="math inline">\(4\)</span> times each game, <em>what probability might reasonably be associated with that hitting streak?</em> First we define notation. We use <span class="math inline">\(A_i\)</span> to denote an event of hitting safely at game <span class="math inline">\(i\)</span>, then <span class="math display">\[
\begin{aligned}
&amp; P( \mathrm{Rose \; Hits \; Safely \; in \;44 \; consecutive \; games} ) = \\
&amp; P ( A_1 \; \text{and} \;  A_2  \ldots \text{and} \;  A_{44} ) = P ( A_1 ) P ( A_2 ) \ldots P ( A_{44} )
\end{aligned}
\]</span> We now need to find <span class="math inline">\(P(A_i)\)</span>s where <span class="math inline">\(P (A_i ) = 1 - P ( \text{not} \; A_i )\)</span> <span class="math display">\[\begin{align*}
P ( A_1 ) &amp; = 1 - P ( \mathrm{ not} \; A_1 ) \\
&amp; = 1 - P ( \mathrm{ Rose \; makes \; 4 \; outs } ) \\
&amp; = 1 - ( 0.7)^4 = 0.76
\end{align*}\]</span> For the winning streak, then we have <span class="math inline">\((0.76)^{44} = 0.0000057\)</span>, a very low probability. In terms of odds, there are three basic inferences</p>
<ol type="1">
<li>This means that the odds for a particular player as good as Pete Rose starting a hitting streak today are <span class="math inline">\(175,470\)</span> to <span class="math inline">\(1\)</span>.</li>
<li>This doesn’t mean that the run of <span class="math inline">\(44\)</span> won’t be beaten by some player at some time: the Law of Very Large Numbers</li>
<li>Joe DiMaggio’s record is 56. He is a 325 hitter, thus we have <span class="math inline">\((0.792)^{56} = 2.13 \times 10^{-6}\)</span> or 455,962 to 1. It’s going to be hard to beat.</li>
</ol>
</div>
<div id="exm-Jeter" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 (Derek Jeter)</strong></span> Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and finer subset selection can change your average effects.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>1995</th>
<th></th>
<th>1996</th>
<th></th>
<th>Combined</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Derek Jeter</td>
<td>12/48</td>
<td>0.250</td>
<td>183/582</td>
<td>0.314</td>
<td>195/630</td>
<td>0.310</td>
</tr>
<tr class="even">
<td>David Justice</td>
<td>104/411</td>
<td>0.253</td>
<td>45/140</td>
<td>0.321</td>
<td>149/551</td>
<td>0.270</td>
</tr>
</tbody>
</table>
<p>This situation is known as <em>confounding</em>. It occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if <span class="math inline">\(A,B,C\)</span> are events it is possible to have the three inequalities <span class="math display">\[\begin{align*}
&amp;P( A \mid B \text{ and } C ) &gt; P( A \mid B \text{ and } \bar C )\\
&amp;P( A \mid \bar  B \text{ and } C ) &gt; P( A \mid \bar  B \text{ and } \bar  C )\\
&amp;P( A \mid C ) &lt; P( A \mid \bar C )
\end{align*}\]</span> The three inequalities can’t hold simultaneously when <span class="math inline">\(P(B\mid C) = P(B\mid \bar  C)\)</span>.</p>
</div>
<div id="exm-birthday" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.10 (Birthday Problem)</strong></span> The birthday problem <span class="citation" data-cites="diaconis1989methods">(<a href="references.html#ref-diaconis1989methods" role="doc-biblioref">Diaconis and and Mosteller 1989</a>)</span> is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.</p>
<p>In general, given <span class="math inline">\(N\)</span> items (people) randomly distributed into <span class="math inline">\(c\)</span> categories (birthdays), where the number of items is small compared to the number of categories <span class="math inline">\(N \ll c\)</span>, the probability of no match is given by <span class="math display">\[
P(\text{no match}) \approx \exp\left(-N^2/2c\right).
\]</span> Given <span class="math inline">\(A_i\)</span> is the event that person <span class="math inline">\(i\)</span> has a matching birthday with someone, we have <span class="math display">\[
P(\text{no match})  = \prod_{i=1}^{N-1}(1-P(A_i)) = \exp\left(\sum_{i=1}^{N-1}\log (1-P(A_i))\right).
\]</span> Here <span class="math inline">\(P(A_i) =\dfrac{i}{c}\)</span> Then use the approximation <span class="math inline">\(\log(1-x) \approx -x\)</span> for small <span class="math inline">\(x\)</span> to get <span class="math inline">\(P(\text{no match})\)</span>. <span class="math display">\[
\sum_{i=1}^{N-1}\log (1-P(A_i)) \approx -\sum_{i=1}^{N-1}\dfrac{i}{c} = -\dfrac{N(N-1)}{2c}.
\]</span></p>
<p>The probability of at least two people sharing a birthday is then the complement of the probability above: <span class="math display">\[
P(\text{At least one shared birthday}) = 1 - P(\text{no match}).
\]</span> Solving for <span class="math inline">\(P(\text{match})=1/2\)</span>, leads to a square root law <span class="math inline">\(N=1.2\sqrt{c}\)</span>, if <span class="math inline">\(c=365\)</span> then <span class="math inline">\(N=23\)</span>, and if <span class="math inline">\(c=121\)</span> (near birthday match), then <span class="math inline">\(N=13\)</span>.</p>
<p>The unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.</p>
</div>
</section>
<section id="kolmogorov-axioms" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="kolmogorov-axioms"><span class="header-section-number">1.2</span> Kolmogorov Axioms</h2>
<p>Later, in the early 1930s, Kolmogorov made significant contributions to the development of probability. He characterized it as a system of sets that meet specific criteria. The representation of the elements within this set is irrelevant. This is similar to how basic geometric concepts are typically introduced. For example, a circle is defined as the set of all points that are equidistant from a given point. The representation of the circle is irrelevant, as long as the set of points meets the criteria. Similarly, a probability field is defined as a set of events that meet specific criteria. This is the basis of the axiomatic approach to probability theory.</p>
<p>Kolmogorov’s axioms provided a rigorous foundation for probability theory. He showed that probability is immensely useful and adheres to only a few basic rules. These axioms provided a set of logical and mathematical rules that describe the properties of probability measures.</p>
<p>Let <span class="math inline">\(S\)</span> be a collection of elementary events and consider two random events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> that are subsets of <span class="math inline">\(S\)</span>. The three axioms are:</p>
<ol type="1">
<li><em>Non-negativity</em>: For any random event <span class="math inline">\(A\)</span>, the probability of <span class="math inline">\(A\)</span> is greater than or equal to zero: <span class="math display">\[
P(A)\ge 0
\]</span></li>
<li><em>Normalization</em>: The probability of the entire sample space <span class="math inline">\(S\)</span> is equal to 1: <span class="math display">\[
P(S) = 1
\]</span></li>
<li><em>Additivity</em>: For mutually exclusive events, we have <span class="math display">\[
P(A \text{ or } B) = P(A) + P(B)
\]</span> The probability of the union of these events is equal to the sum of their individual probabilities.</li>
</ol>
<p>Mutually exclusive means that only one of the events in the sequence can occur. These axioms provided a solid and consistent foundation for probability theory, allowing mathematicians to reason rigorously about uncertainty and randomness. Kolmogorov’s work helped unify and clarify many concepts in probability, and his axioms are now widely accepted as the basis for modern probability theory. His contributions had a profound impact on various fields, including statistics, mathematical physics, and information theory.</p>
<p>Assigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (observed data). Consider the coin-tossing example. We toss a coin twice and the possible outcomes are <em>HH, HT, TH, TT</em>. Say event <span class="math inline">\(A\)</span> represents a repetition, then it will consist of the first and second outcome of the two coin-tosses. Then, to empirically estimate <span class="math inline">\(P(A)\)</span> we can repeat the two-toss experiment <span class="math inline">\(n\)</span> times and count <span class="math inline">\(m\)</span>, the number of times <span class="math inline">\(A\)</span> occurred. When <span class="math inline">\(N\)</span> is large, <span class="math inline">\(m/N\)</span> will be close to <span class="math inline">\(P(A)\)</span>. However, if we are to repeat this experiment under different conditions, e.g.&nbsp;when an unbalanced coin is used, our estimate of <span class="math inline">\(P(A)\)</span> will change as well.</p>
<p>The axioms provide a number of rules that probabilities must follow. There are several important corollaries that can help us assign probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:</p>
<!-- https://www.york.ac.uk/depts/maths/histstat/kolmogorov_foundations.pdf -->
<ol type="1">
<li><em>Complement rule</em>: Let “not <span class="math inline">\(A\)</span>” denote the complement of event <span class="math inline">\(A\)</span>. <span class="math display">\[
  P(\text{not } A) = 1- P(A).
\]</span></li>
<li><em>Monotonicity</em>: If <span class="math inline">\(A\subset B\)</span>, then <span class="math inline">\(P(A)\le P(B)\)</span>. In other words, the probability of a larger set is greater than or equal to the probability of a subset.</li>
<li><em>Subadditivity</em>: This is a generalization of the addition rule, where the equality holds when events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive. <span class="math display">\[
P(A \text{ or } B)\le P(A)+P(B).
\]</span></li>
<li><em>Inclusion–exclusion principle</em>: This principle extends subadditivity to the case where <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not necessarily mutually exclusive. <span class="math display">\[
P(A\text{ or } B)=P(A)+P(B)-P(A\text{ and }B).
\]</span></li>
<li><em>Conditional probability</em>: The conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is <span class="math display">\[
P(A\mid B) = \dfrac{P(A \text{ and } B)}{P(B)}.
\]</span></li>
<li><em>Bayes rule</em> simply reverses the conditioning to compute <span class="math inline">\(P(A\mid B)\)</span> from <span class="math inline">\(P(B\mid A)\)</span>—a disciplined probability accounting. <span class="math display">\[
P(A\mid B) = \dfrac{P(B\mid A)P(A)}{P(B)}.   
\]</span></li>
<li><em>Law of total probability</em> is a direct consequence of the definition of conditional probability and the normalization axiom. It states that if <span class="math inline">\(B_1, B_2, \ldots, B_n\)</span> are mutually exclusive and exhaustive events, then <span class="math display">\[
P(A) = \sum_{i=1}^n P(A \text{ and } B_i) = \sum_{i=1}^n P(A \mid B_i)P(B_i).
\]</span></li>
</ol>
<p>All of these axioms follow simply from the principle of coherence and the avoidance of Dutch book arbitrage. This includes the Bayes rule itself (de Finetti, Shimony).</p>
<p>Bayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.</p>
<p>Bayes rule simply provides a disciplined probability accounting of how probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities he cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.</p>
<p>These corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.</p>
<p>The age of artificial intelligence (AI) has certainly proved that Bayes is a powerful tool. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics or the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.</p>
</section>
<section id="dutch-book-and-the-rules-of-probability" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="dutch-book-and-the-rules-of-probability"><span class="header-section-number">1.3</span> Dutch book and the rules of probability</h2>
<p>If probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.</p>
<p>The starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “<em>It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event</em>” (p.&nbsp;101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.</p>
<p>Formally, for any event <span class="math inline">\(A\)</span>, the identity <span class="math display">\[
P(A)  =\frac{1}{1+\text{odds}(A)}\text{ or }\text{odds}(A)=\frac{1-P(A)}{P(A)},
\]</span> where <span class="math inline">\(\bar A\)</span> is the complement of <span class="math inline">\(A\)</span>, links odds and probabilities. Throughout, we use <span class="math inline">\(P\)</span> as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is <span class="math inline">\(1/3\)</span>. The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are <em>fair</em> if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.</p>
<p>In constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “<em>could have a book made against him by a cunning bettor and would then stand to lose in any event</em>” (Ramsey (1931), p.&nbsp;22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional probability. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.</p>
<p>Dutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, <span class="math inline">\(S\)</span>. The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.</p>
<p>To derive the rules, consider the first axiom of probability: for any event <span class="math inline">\(A\)</span>, <span class="math inline">\(0\leq P(A) \leq 1\)</span>. Suppose that the odds imply probabilities <span class="math inline">\(P(A)\)</span> for <span class="math inline">\(A\)</span> occurring and <span class="math inline">\(P(\bar A)\)</span> for other outcomes, with associated payoffs of <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>. Then, having bet <span class="math inline">\(S_{A}\)</span> and <span class="math inline">\(S_{\bar A}\)</span>, the gains if <span class="math inline">\(A\)</span> or <span class="math inline">\(\bar A\)</span> occur, <span class="math inline">\(G_{A}\)</span> and <span class="math inline">\(G_{\bar A}\)</span>, respectively, are <span class="math display">\[\begin{align*}
G(A)   &amp;  =S_{A}-P(A) S_{A}-P(\bar A)  S_{\bar A}\\
G(\bar A)   &amp;  =S_{\bar A}-P(A) S_{A}-P(\bar A)  S_{\bar A}.
\end{align*}\]</span> To see this, note that the bettor receives <span class="math inline">\(S_{A}\)</span> and pays <span class="math inline">\(P(A) S_{A}\)</span> for a bet on event <span class="math inline">\(A\)</span>. The bookie can always choose to place a zero stake on <span class="math inline">\(\bar A\)</span> occurring, which implies that <span class="math inline">\(G(A) =S_{A}-P(A) S_{A}\)</span> and <span class="math inline">\(G\left(\bar A\right) =-P(A) S_{A}\)</span>. Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus <span class="math inline">\(G(A) G(\bar A) \leq 0\)</span>. Substituting, <span class="math inline">\(\left( 1-P(A) \right) P(A) \geq0\)</span> or <span class="math inline">\(0\leq P(A) \leq 1\)</span>, which is the first axiom of probability. The second axiom, that the set of all possible outcomes has probability <span class="math inline">\(1\)</span>, is similarly straightforward to show.</p>
<p>The third axiom is that probabilities add, that is, for two disjoint events <span class="math inline">\(A_{1}\)</span> and <span class="math inline">\(A_{2}\)</span>, <span class="math inline">\(P(A) =P\left( A_{1} \text{ or } A_{2}\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>. Assuming stakes sizes of <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}},\)</span> and <span class="math inline">\(S_{A_{2}}\)</span> (and zero stakes on their complements) there are three possible outcomes. If neither <span class="math inline">\(A_{1}\)</span> nor <span class="math inline">\(A_{2}\)</span> occur, the gain is <span class="math display">\[
G(\bar A)  =-P(A)  S_{A} -P\left(  A_{1}\right)  S_{A_{1}}-P\left( A_{2}\right)  S_{A_{2}}.
\]</span></p>
<p>If <span class="math inline">\(A_{1}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and the gain is <span class="math display">\[
G\left(  A_{1}\right)  =\left(  1-P(A) \right)  S_{A}+\left(  1-P\left(  A_{1}\right)  \right) S_{A_{1}}-P\left(  A_{2}\right)  S_{A_{2}},
\]</span> and finally if <span class="math inline">\(A_{2}\)</span> occurs, <span class="math inline">\(A\)</span> also occurs, and <span class="math display">\[
G\left(  A_{2}\right)  =\left(  1-P(A) \right)  S_{A}-P\left(  A_{1}\right)  S_{A_{1}}+\left( 1-P\left(  A_{2}\right)  \right)  S_{A_{2}}.
\]</span> Arranging these into a matrix equation, <span class="math inline">\(G=PS\)</span>: <span class="math display">\[
\left( \begin{array}
[c]{c}%
G(\bar A) \\
G\left(  A_{1}\right) \\
G\left(  A_{2}\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
-P\left(  A_{2}\right) \\
1-P(A)  &amp; 1-P\left(  A_{1}\right)  &amp;
-P\left(  A_{2}\right) \\
1-P(A)  &amp; -P\left(  A_{1}\right)  &amp;
1-P\left(  A_{2}\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{A}\\
S_{A_{1}}\\
S_{A_{2}}%
\end{array}
\right).
\]</span></p>
<p>The absence of a Dutch book arbitrage implies that there is no set of stakes, <span class="math inline">\(S_{A}\)</span>, <span class="math inline">\(S_{A_{1}}\)</span>, and <span class="math inline">\(S_{A_{2}}\)</span>, such that the winnings in all three events are positive. If the matrix <span class="math inline">\(P\)</span> is invertible, it is possible to find stakes with positive gains. To rule this out, the determinant of <span class="math inline">\(P\)</span> must be zero, which implies that <span class="math inline">\(0=-P(A) +P\left(A_{1}\right) +P\left( A_{2}\right)\)</span>, or <span class="math inline">\(P\left(A\right) =P\left( A_{1}\right) +P\left( A_{2}\right)\)</span>.</p>
<p>The fourth axiom is conditional probability. Consider an event <span class="math inline">\(B\)</span>, with <span class="math inline">\(P\left( B\right) &gt;0\)</span>, an event <span class="math inline">\(A\)</span> that occurs conditional on <span class="math inline">\(B\)</span>, and the event that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur. The probabilities or prices of these bets are <span class="math inline">\(P\left( B\right)\)</span>, <span class="math inline">\(P\left( A \mid B\right)\)</span>, and <span class="math inline">\(P\left( A \text{ and } B\right)\)</span>. Consider bets with stakes <span class="math inline">\(S_{B}\)</span>, <span class="math inline">\(S_{A \mid B}\)</span> and <span class="math inline">\(S_{A \text{ and } B}\)</span>, with the understanding that if <span class="math inline">\(B\)</span> does not occur, the conditional bet on <span class="math inline">\(A\)</span> is canceled. The payoffs to the events that <span class="math inline">\(B\)</span> does not occur, <span class="math inline">\(B\)</span> occurs but not <span class="math inline">\(A\)</span>, and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur, are <span class="math display">\[
\left( \begin{array}
[c]{c}%
G\left(  \bar B\right) \\
G\left(  \bar A \text{ and } B\right) \\
G\left(  A \text{ and } B\right)
\end{array}
\right)  =\left( \begin{array}
[c]{ccc}%
-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp; 0\\
1-P\left(  B\right)  &amp; -P\left(  A \text{ and } B\right)  &amp;
-P\left(  A \mid B\right) \\
1-P\left(  B\right)  &amp; 1-P\left(  A \text{ and } B\right)  &amp;
1-P\left(  A \mid B\right)
\end{array}
\right)  \left( \begin{array}
[c]{c}%
S_{B}\\
S_{A \text{ and } B}\\
S_{A \mid B}%
\end{array}
\right).
\]</span> Similar arguments imply the determinant must be zero, which implies that <span class="math display">\[
P\left(  A \mid B\right)  =\frac{P\left(  A \text{ and } B\right) }{P\left(  B\right)  },
\]</span> which is the law of conditional probability, given <span class="math inline">\(P(B)&gt;0\)</span>, of course, otherwise the conditional probability is not defined, and the <span class="math inline">\(P\)</span> matrix has determinant 0.</p>
<p><span class="citation" data-cites="polson2025negative">Polson and Sokolov (<a href="references.html#ref-polson2025negative" role="doc-biblioref">2025</a>)</span> explores the concept of negative probabilities, which arise in fields like physics and quantum computing, and also appear in Bayesian modeling as mixing distributions for unobserved latent variables. They establish a conceptual link between these two different applications. The article uses Bartlett’s definition of negative probabilities involving extraordinary random variables and presents a version of Bayes’ rule that incorporates negative mixing weights.</p>
</section>
<section id="random-variables" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="random-variables"><span class="header-section-number">1.4</span> Random Variables</h2>
<p>A random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.</p>
<p>For example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.</p>
<p>While events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.</p>
<p>Random variables are quantities that we are not certain about. A random variable that can take a finite or a countable number of values is called a <em>discrete random variable</em> (number of rainy days next week). Otherwise, it will be a <em>continuous random variable</em> (amount of rain tomorrow).</p>
<p>Discrete random variables are often constructed by assigning specific values to events such as <span class="math inline">\(\{X=x\}\)</span> which corresponds to the outcomes where <span class="math inline">\(X\)</span> equals a specific number <span class="math inline">\(x\)</span>. For example</p>
<ol type="1">
<li>Will a user click-through on a Google ad? (0 or 1)</li>
<li>Who will win the 2024 elections? (Trump=1, Biden=2, Independent=3)</li>
</ol>
<p>To fix notation, we will use <span class="math inline">\(\prob{X=x}\)</span> to denote the probability that random variable <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(x\)</span>. A map from all possible values <span class="math inline">\(x\)</span> of a discrete random variable <span class="math inline">\(X\)</span> to probabilities is called a <em>probability mass function</em> <span class="math inline">\(p(x)\)</span>. We will interchangeably use <span class="math inline">\(\prob{X=x}\)</span> and <span class="math inline">\(p(x)\)</span>. An important property of the probability mass function is that (normalization Kolmogorov axiom) <span class="math display">\[
\sum_{x\in S} p(x) = 1.
\]</span> Here <span class="math inline">\(S\)</span> denotes the set of all possible values of random variable <span class="math inline">\(X\)</span>.</p>
<p>Clearly, all probabilities have to be greater than or equal to zero, so that <span class="math inline">\(p(x)\ge 0\)</span>.</p>
<p>For a continuous random variable, the probability distribution is represented by a probability density function (PDF) <span class="math inline">\(p(x)\)</span>, which indicates the likelihood of the variable taking a specific value. Then to calculate probability of a variable falling within a particular range <span class="math inline">\(a \leq X \leq b\)</span> we need to integrate the PDF over the range: <span class="math display">\[
\prob{a \leq X \leq b} = \int_a^b p(x) dx.
\]</span> Often, we are interested in <span class="math inline">\(F(x) = P(X\le x)\)</span>, this is the cumulative distribution function (CDF).</p>
<p>The Cumulative Distribution Function (CDF) for a discrete random variable is a function that provides the probability that the random variable is less than or equal to a particular value. The CDF is a monotonically increasing function (never decreases as <span class="math inline">\(x\)</span> increases). In other words, if <span class="math inline">\(a \leq b\)</span>, then <span class="math inline">\(F_X(a) \leq F_X(b)\)</span>. The value of the CDF always lies between 0 and 1, inclusive.</p>
<div id="exm-dcdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.11 (Discrete CDF)</strong></span> Suppose <span class="math inline">\(X\)</span> is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
P(X = x) = \frac{1}{6}
\]</span> for <span class="math inline">\(x = 1, 2, 3, 4, 5, 6\)</span></p>
<p>The CDF of <span class="math inline">\(X\)</span>, <span class="math inline">\(F(x)\)</span>, is calculated as follows:</p>
<ul>
<li>For <span class="math inline">\(x &lt; 1\)</span>, <span class="math inline">\(F(x) = 0\)</span> (since it’s impossible to roll less than 1).</li>
<li>For <span class="math inline">\(1 \leq x &lt; 2\)</span>, <span class="math inline">\(F(x) = \frac{1}{6}\)</span> (the probability of rolling a 1).</li>
<li>For <span class="math inline">\(2 \leq x &lt; 3\)</span>, <span class="math inline">\(F(x) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6}\)</span> (the probability of rolling a 1 or 2).</li>
<li>This pattern continues, adding <span class="math inline">\(\frac{1}{6}\)</span> for each integer interval up to 6.</li>
<li>For <span class="math inline">\(x \geq 6\)</span>, <span class="math inline">\(F(x) = 1\)</span> (since it’s certain to roll a number 6 or less).</li>
</ul>
<p>Graphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>), <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a discrete random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="continuous-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="continuous-random-variables">Continuous Random Variables</h3>
<p>If we want to build a probabilistic model of a stock price or return, we need to use a continuous random variable that can take an interval of values. Instead of a frequency function we will use a <em>density function</em>, <span class="math inline">\(p(x)\)</span> to describe a continuous variable. Unlike the discrete case, <span class="math inline">\(p(x)\)</span> is not the probability that the random variable takes value <span class="math inline">\(x\)</span>. Rather, we need to talk about the value being inside an interval. For example, the probability of <span class="math inline">\(X\)</span> with density <span class="math inline">\(p(x)\)</span> being inside any interval <span class="math inline">\([a,b]\)</span>, with <span class="math inline">\(a&lt;b\)</span> is given by <span class="math display">\[
P(a &lt; X &lt; b) = \int_{a}^{b}p(x)dx.
\]</span> The total probability is one as <span class="math inline">\(\int_{-\infty}^\infty p(x) dx=1\)</span>. The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then <span class="math display">\[
  p(x) = \begin{cases} 1/10, ~~~20 \le x \le 30\\0, ~~~\mbox{otherwise}\end{cases}
\]</span></p>
<p>Under this model, the probability of temperature being between 25 and 27 degrees is <span class="math display">\[
P(25 \le x \le 27) = \int_{25}^{27} p(x)dx = (27-25)/10 = 0.2
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/uniform.svg" class="img-fluid figure-img" style="width:95.0%"></p>
<figcaption>Uniform Distribution: Probability of temperature being between 25 and 27</figcaption>
</figure>
</div>
<p>The Cumulative Distribution Function for a random variable <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(F_X(x)\)</span>. For a continuous random variable, it is defined similarly to discrete RV CDF as <span class="math display">\[
F(x) = P(X \leq x)
\]</span> It is a non-decreasing function and takes values in [0,1].</p>
<div id="exm-ccdf" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.12 (Continuous CDF for Uniform Distribution)</strong></span> <span class="math display">\[
p(x) = \begin{cases}
1 &amp; \text{if } 0 \leq x \leq 1 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The CDF, <span class="math inline">\(F(x)\)</span>, is obtained by integrating the PDF:</p>
<ul>
<li>For <span class="math inline">\(x &lt; 0\)</span>, <span class="math inline">\(F(x) = 0\)</span>.</li>
<li>For <span class="math inline">\(0 \leq x \leq 1\)</span>, <span class="math inline">\(F(x) = \int_0^x 1 \, dt = x\)</span>.</li>
<li>For <span class="math inline">\(x &gt; 1\)</span>, <span class="math inline">\(F(x) = 1\)</span>.</li>
</ul>
<p>So, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as <span class="math inline">\(x\)</span> goes from 0 to 1.</p>
<p>Graphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as <span class="math inline">\(x\)</span> increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature. Figure below shows the CDF of a uniform and normal random variable, respectively.</p>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">runif</span>(<span class="dv">500</span>)), <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">pch=</span><span class="dv">21</span>, <span class="at">bg=</span><span class="st">"grey"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ecdf</span>(<span class="fu">rnorm</span>(<span class="dv">500</span>)), <span class="at">main=</span><span class="st">""</span>, <span class="at">col=</span><span class="st">"lightblue"</span>, <span class="at">pch=</span><span class="dv">21</span>, <span class="at">bg=</span><span class="st">"grey"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-ncol="2" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a uniform random variable</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid figure-img" width="576"></p>
<figcaption>CDF of a normal random variable</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="the-inverse-cdf-method" class="level3">
<h3 class="anchored" data-anchor-id="the-inverse-cdf-method">The Inverse CDF Method</h3>
<p>The inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, <span class="math inline">\(F\)</span>. Since <span class="math inline">\(F\left(  x\right)\)</span> is uniformly distributed on <span class="math inline">\(\left[ 0,1\right]\)</span>, draw a uniform random variable and invert the CDF to get a draw from <span class="math inline">\(F\)</span>. Thus, to sample from <span class="math inline">\(F\)</span>, <span class="math display">\[\begin{align*}
&amp;  \text{Step 1}\text{: Draw }U\sim U\left[  0,1\right]  \ \\
&amp;  \text{Step 2}\text{: }\text{Set }X=F^{-1}\left(  U\right)  ,
\end{align*}\]</span> where <span class="math inline">\(F^{-1}\left(  U\right)  =\inf\left\{  x:F\left(  x\right)  =U\right\}\)</span>.</p>
<p>This inversion method provides i.i.d. draws from <span class="math inline">\(F\)</span> provided that <span class="math inline">\(F^{-1}\left(  U\right)\)</span> can be exactly calculated. For example, the CDF of an exponential random variable with parameter <span class="math inline">\(\mu\)</span> is <span class="math inline">\(F\left(  x\right) =1-\exp\left(  -\mu x\right)\)</span>, which can easily be inverted. When <span class="math inline">\(F^{-1}\)</span> cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, <span class="math inline">\(F\left(  x\right)\)</span> can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from <span class="math inline">\(F\)</span>. With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.</p>
</section>
<section id="functional-transformations" class="level3">
<h3 class="anchored" data-anchor-id="functional-transformations">Functional Transformations</h3>
<p>The second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that <span class="math inline">\(X\sim F\)</span>, admitting a density <span class="math inline">\(f\)</span>, and that <span class="math inline">\(y=h\left(  x\right)\)</span> is an increasing continuous function. Thus, we can define <span class="math inline">\(x=h^{-1}\left(  y\right)\)</span> as the inverse of the function <span class="math inline">\(h\)</span>. The distribution of <span class="math inline">\(y\)</span> is given by <span class="math display">\[
F_Y\left(y\right)  =\text{P}\left(  Y\leq y\right)  =\int_{-\infty}^{h^{-1}\left(  y\right)  }f\left(  x\right)  dx=F_X\left(  X\leq h^{-1}\left(y\right)  \right).
\]</span> Differentiating with respect to <span class="math inline">\(y\)</span> gives the density via Leibnitz’s rule: <span class="math display">\[
f_{Y}\left(  y\right)  =f\left(  h^{-1}\left(  y\right)  \right)  \left\vert\frac{d}{dy}\left(  h^{-1}\left(  y\right)  \right)  \right\vert,
\]</span> where we make explicit that the density is over the random variable <span class="math inline">\(Y\)</span>. This result is used widely. For example, if <span class="math inline">\(X\sim\mathcal{N}\left(  0,1\right)\)</span>, then <span class="math inline">\(Y=\mu+\sigma X\)</span>. Since <span class="math inline">\(x=h^{-1}\left(  y\right)  =\frac{y-\mu}{\sigma}\)</span>, the distribution function is <span class="math inline">\(F\left(  \frac{x-\mu}{\sigma}\right)\)</span> and density <span class="math display">\[
f_{Y}\left(  y\right)  =\frac{1}{\sqrt{2\pi}\sigma}\exp\left(  -\frac{1}{2}\left(  \frac{y-\mu}{\sigma}\right)  ^{2}\right).
\]</span> Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if <span class="math inline">\(Y\sim\mathcal{X}^{2}\left(  \nu\right)\)</span> and <span class="math inline">\(\nu\)</span> is an integer, then <span class="math inline">\(Y=\sum_{i=1}^{\nu}X_{i}^{2}\)</span> where each <span class="math inline">\(X_{i}\)</span> is independent standard normal. Exponential random variables can be used to simulate <span class="math inline">\(\mathcal{X}^{2}\)</span>, Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be simulated via sums of squared vectors of standard normal random variables.</p>
</section>
</section>
<section id="expectation-and-variance-reward-and-risk" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="expectation-and-variance-reward-and-risk"><span class="header-section-number">1.5</span> Expectation and Variance (Reward and Risk)</h2>
<p>An expected value of a random variable, denoted by <span class="math inline">\(\E{X}\)</span> is a weighted average. Each possible value of a random variable is weighted by its probability. For example, Google Maps uses expected value when calculating travel times. We might compute two different routes by their expected travel time. Typically, a forecast or expected value is all that is required — these expected values can be updated in real time as we travel. Say I am interested in travel time from Washington National airport to Fairfax in Virginia. The histogram below shows the travel times observed for a work day evening and were obtained from <a href="https://movement.uber.com/">Uber</a>.</p>
<div id="exm-Uber" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.13 (Uber)</strong></span> Let’s look at the histogram of travel times from Fairfax, VA to Washington, DC</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/dc_travel_time.csv"</span>) </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use evening travel times (column 18) and convert from seconds to minutes </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>evening_tt <span class="ot">=</span> d[,<span class="dv">18</span>]<span class="sc">/</span><span class="dv">60</span>; day_tt <span class="ot">=</span> d[,<span class="dv">15</span>]<span class="sc">/</span><span class="dv">60</span>; </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>evening_tt <span class="ot">=</span> evening_tt[<span class="sc">!</span><span class="fu">is.na</span>(evening_tt)] <span class="co"># remove missing observations </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(evening_tt, <span class="at">freq =</span> F,<span class="at">main=</span><span class="st">""</span>, <span class="at">xlab=</span><span class="st">"Travel Time [min]"</span>, <span class="at">nclass=</span><span class="dv">20</span>, <span class="at">col=</span><span class="st">"lightblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Travel times in the evening</figcaption>
</figure>
</div>
</div>
</div>
<p>From this dataset, we can empirically estimate the probabilities of observing different values of travel times</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>bins <span class="ot">=</span> <span class="fu">hist</span>(evening_tt, <span class="at">breaks =</span> <span class="dv">3</span>, <span class="at">plot =</span> F) </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">data.frame</span>(<span class="st">"tt"</span> <span class="ot">=</span> bins<span class="sc">$</span>mids, <span class="st">"Probability"</span> <span class="ot">=</span> bins<span class="sc">$</span>counts<span class="sc">/</span><span class="fu">length</span>(evening_tt)),<span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Travel Time"</span>,<span class="st">"Probability"</span>),<span class="at">digits=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">Travel Time</th>
<th style="text-align: right;">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">18</td>
<td style="text-align: right;">0.05</td>
</tr>
<tr class="even">
<td style="text-align: right;">22</td>
<td style="text-align: right;">0.77</td>
</tr>
<tr class="odd">
<td style="text-align: right;">28</td>
<td style="text-align: right;">0.18</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>There is a small chance (5%) I can get to Washington, DC in 18 minutes, which probably happens on a holiday and a non-trivial chance (18%) to travel for 28 minutes, possibly due to a sports game or bad weather. Most of the time (77%) our travel time is 22 minutes. However, when Uber shows you the travel time, it uses the expected value as a forecast rather than the full distribution. Specifically, you will be given an expected travel time of 23 minutes.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.05</span><span class="sc">*</span><span class="dv">18</span> <span class="sc">+</span> <span class="fl">0.77</span><span class="sc">*</span><span class="dv">22</span> <span class="sc">+</span> <span class="fl">0.18</span><span class="sc">*</span><span class="dv">28</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 23</code></pre>
</div>
</div>
<p>It is a simple summary that takes into account travel accidents and other events that can affect travel time as best as it can.</p>
</div>
<p>The expected value <span class="math inline">\(\E{X}\)</span> of discrete random variable <span class="math inline">\(X\)</span> which takes possible values <span class="math inline">\(\{x_1,\ldots x_n\}\)</span> is calculated using</p>
<p><span class="math display">\[
\E{X} =\sum_{i=1}^{n}x_i\prob{X = x_i}
\]</span></p>
<p>For example, in a binary scenario, if <span class="math inline">\(X\in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p\)</span>, then <span class="math inline">\(\E{X} = 0\times(1-p)+1\times p = p\)</span>. The expected value of a Bernoulli random variable is simply the probability of success. In many binary scenarios, a probabilistic forecast is sufficient.</p>
<p>If <span class="math inline">\(X\)</span> is continuous with probability distribution <span class="math inline">\(p(x)\)</span>, then we have to calculate the expectation as an integral <span class="math display">\[
\E{X} = \int xp(x)d x  \text{ and } \int p(x)dx = 1.
\]</span> When you have a random variable <span class="math inline">\(x\)</span> that has a support that is non-negative (that is, the variable has nonzero density/probability for only positive values), you can use the following property: <span class="math display">\[
\E{X} = \int_0^\infty \left( 1 - F(x) \right) \,\mathrm{d}x,
\]</span> where <span class="math inline">\(F(x)\)</span> is the cumulative distribution function (CDF) of <span class="math inline">\(X\)</span>. The proof is as follows: <span class="math display">\[
\E{X} = \int_0^\infty \left( 1 - F(x) \right) \,\mathrm{d}x = \int_0^\infty \int_x^\infty f(y) \,\mathrm{d}y \,\mathrm{d}x = \int_0^\infty \int_0^y \,\mathrm{d}x f(y) \,\mathrm{d}y = \int_0^\infty y f(y) \,\mathrm{d}y,
\]</span> where <span class="math inline">\(f(x)\)</span> is the probability density function (PDF) of <span class="math inline">\(X\)</span>.</p>
<p>There are cases, when the expectation does not exist. For example for a Cauchy distribution, the integral diverges, the area under the curve is infinite. In such cases, we say the expectation is undefined.</p>
<p>We can also calculate the expectation in terms of the quantile function (inverse CDF). <span class="math display">\[
\E{X} = \int_{0}^1 F^{-1}(p) \,\mathrm{d}p,
\]</span> where <span class="math inline">\(F^{-1}(p) = \inf\{y \in \mathbb{R} \mid F(y)\ge p\}\)</span> is the inverse CDF of <span class="math inline">\(X\)</span>. The proof is as follows: <span class="math display">\[
\E{X} = \int_{0}^1 F^{-1}(p) \,\mathrm{d}p = \int_{0}^1 \int_{-\infty}^{F^{-1}(p)} f(x) \,\mathrm{d}x \,\mathrm{d}p = \int_{-\infty}^{\infty} \int_{0}^{F(x)} \,\mathrm{d}p f(x) \,\mathrm{d}x = \int_{-\infty}^{\infty} x f(x) \,\mathrm{d}x.
\]</span> We will use this result later when we discuss quantile neural networks.</p>
<section id="standard-deviation-and-covariance" class="level3">
<h3 class="anchored" data-anchor-id="standard-deviation-and-covariance">Standard Deviation and Covariance</h3>
<p>Variance measures the spread of a random variable around its expected value <span class="math display">\[
\Var{X} = \E{(X-\E{X})^2} =  \sum_{i=1}^n (x_i-\mu)^2 \prob{X=x_i}.
\]</span> In the continuous case, we have <span class="math display">\[
\Var{X} = \int_{-\infty}^\infty (x-\mu) ^2 p(x)dx,
\]</span> where <span class="math inline">\(\mu = \mathbb{E}(X)=\int_{-\infty}^{\infty}p_X(x)dx\)</span>. The standard deviation is more convenient and is the square root of variance <span class="math inline">\(\sd{X} = \sqrt{\Var{X}}\)</span>. Standard deviation has the desirable property that it is measured in the same units as the random variable <span class="math inline">\(X\)</span> itself and is a more useful measure.</p>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We need to measure whether they move together or in opposite directions. The <em>covariance</em> is defined by <span class="math display">\[
\Cov{X,Y} = \E{\left[ (X- \E{X})(Y- \E{Y})\right]}.
\]</span></p>
<p>When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete and we are given the joint probability distribution, we need to calculate <span class="math display">\[
\Cov{X,Y} = \sum_{x,y}  ( x - \E{X} )(y - \E{Y})p(x,y).
\]</span> Covariance is measured in units of <span class="math inline">\(X\times\)</span>units of <span class="math inline">\(Y\)</span>. This can be inconvenient and makes it hard to compare covariances of different pairs of variables. A more convenient metric is the <em>correlation</em>, which is defined by <span class="math display">\[
\Cor{X,Y}= \frac{ \Cov{X,Y} }{ \sd{X} \sd{Y} }.
\]</span> Correlation, <span class="math inline">\(\Cor{X,Y}\)</span>, is unitless and takes values between -1 and 1.</p>
<p>In the case of joint continuous distribution it is convenient to use the covariance matrix <span class="math inline">\(\Sigma\)</span> which is defined as <span class="math display">\[
\Sigma = \begin{bmatrix}
\Var{X} &amp; \Cov{X,Y} \\
\Cov{X,Y} &amp; \Var{Y}
\end{bmatrix}.
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\Cov{X,Y} = 0\)</span> and <span class="math inline">\(\Sigma\)</span> is diagonal. The correlation matrix is defined as <span class="math display">\[
\rho = \begin{bmatrix}
1 &amp; \Cor{X,Y} \\
\Cor{X,Y} &amp; 1
\end{bmatrix}.
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have an exact linear relationship, then <span class="math inline">\(\Cor{X,Y} = 1\)</span> and <span class="math inline">\(\Cov{X,Y}\)</span> is the product of standard deviations. In matrix notation, the relation between the covariance matrix and correlation matrix is given by <span class="math display">\[
\rho = \mathrm{diag}\left(\Sigma\right)^{-1/2} \Sigma\mathrm{diag}\left(\Sigma\right)^{-1/2},
\]</span> where <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with standard deviations on the diagonal.</p>
</section>
<section id="portfolios-linear-combinations" class="level3">
<h3 class="anchored" data-anchor-id="portfolios-linear-combinations">Portfolios: linear combinations</h3>
<p>Calculating means and standard deviations of combinations of random variables is a central tool in probability. It is known as the portfolio problem. Let <span class="math inline">\(P\)</span> be your portfolio, which comprises a mix of two assets <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, typically stocks and bonds, <span class="math display">\[
P = aX + bY,
\]</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the portfolio weights, typically <span class="math inline">\(a+b=1\)</span>, as we are allocating our total capital. Imagine that you have placed <span class="math inline">\(a\)</span> dollars on the random outcome <span class="math inline">\(X\)</span>, and <span class="math inline">\(b\)</span> dollars on <span class="math inline">\(Y\)</span>. The portfolio <span class="math inline">\(P\)</span> measures your total weighted outcome.</p>
<p>Key portfolio rules: The expected value and variance follow the relations <span class="math display">\[\begin{align*}
\E{aX + bY} = &amp;      a\E{X}+b\E{Y}\\
\Var{ aX + bY }  = &amp; a^2 \Var{X} + b^2 \Var{Y} + 2 ab \Cov{X,Y },
\end{align*}\]</span> with <em>covariance</em> defined by <span class="math display">\[
\Cov{X,Y} = \E{ ( X- \E{X} )(Y- \E{Y})}.
\]</span> Expectation and variance help us to understand the long-run behavior. When we make long-term decisions, we need to use the expectations to avoid biases.</p>
<p>The covariance is related to the correlation by <span class="math inline">\(\Cov{X,Y} = \text{Corr}(X, Y) \cdot \sqrt{\text{Var}(X) \cdot \text{Var}(Y)}\)</span>.</p>
<div id="exm-Tortoise" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.14 (Tortoise and Hare)</strong></span> Tortoise and Hare are selling cars. Say <span class="math inline">\(X\)</span> is the number of cars sold and probability distributions, means and variances are given by the following table</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th></th>
<th></th>
<th></th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Variance</th>
<th style="text-align: center;">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td style="text-align: center;"><span class="math inline">\(\E{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\Var{X}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\sqrt{\Var{X}}\)</span></td>
</tr>
<tr class="even">
<td>Tortoise</td>
<td style="text-align: center;">0</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr class="odd">
<td>Hare</td>
<td style="text-align: center;">0.5</td>
<td>0</td>
<td>0</td>
<td>0.5</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.25</td>
<td style="text-align: center;">1.5</td>
</tr>
</tbody>
</table>
<p>Let’s calculate Tortoise’s expectations and variances <span class="math display">\[\begin{align*}
\E{T} &amp; = (1/2) (1) + (1/2)(2) = 1.5 \\
\Var{T} &amp; = \E{T^2} - \E{T}^2 \\
&amp; =  (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25
\end{align*}\]</span></p>
<p>Now the Hare’s <span class="math display">\[\begin{align*}
\E{H} &amp; = (1/2)(0) + (1/2)(3) = 1.5 \\
\Var{H} &amp; =  (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25
\end{align*}\]</span></p>
<p>What do these tell us about the long run behavior?</p>
<ol type="1">
<li><p>Tortoise and Hare have the same expected number of cars sold.</p></li>
<li><p>Tortoise is more predictable than Hare. He has a smaller variance.</p></li>
</ol>
<p>The standard deviations <span class="math inline">\(\sqrt{\Var{X}}\)</span> are <span class="math inline">\(0.5\)</span> and <span class="math inline">\(1.5\)</span>, respectively. Given two equal means, you always want to pick the lower variance. If we are to invest in one of those, we prefer Tortoise.</p>
<p>What about a portfolio of Tortoise and Hare? Suppose I want to evenly split my investment between Tortoise and Hare. What is the expected number of cars sold and the variance of the number of cars sold? <span class="math display">\[
\E{\frac{1}{2}T + \frac{1}{2}H} = \frac{1}{2} \E{T} + \frac{1}{2} \E{H} = 1.5
\]</span> For variance, we need to know <span class="math inline">\(\Cov{T,H}\)</span>. Let’s take <span class="math inline">\(\Cov{T,H} = -1\)</span> and see what happens. <span class="math display">\[
\Var{\frac{1}{2}T + \frac{1}{2}H} = \frac{1}{4} \Var{T} + \frac{1}{4} \Var{H} + \frac{1}{2} \Cov{T,H} = 0.0625 + 0.5625 -0.5 = 0.125
\]</span></p>
<p>Notice that the portfolio variance (0.125) is lower than both individual variances (0.25 and 2.25). This demonstrates the power of diversification - by combining investments with negative covariance, we can reduce overall risk while maintaining the same expected return. The negative covariance indicates that when Tortoise performs well, Hare tends to perform poorly, and vice versa, creating a natural hedge.</p>
<p>This example illustrates a fundamental principle in finance and decision theory: diversification can reduce risk without sacrificing expected returns when assets are not perfectly positively correlated. The key insight is that variance depends not only on individual asset volatilities but also on their covariances, making portfolio construction a crucial consideration in risk management.</p>
</div>
</section>
</section>
<section id="commonly-used-distributions" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="commonly-used-distributions"><span class="header-section-number">1.6</span> Commonly Used Distributions</h2>
<p>Having explored specific examples of random variables and their properties through the Tortoise and Hare scenario, we now turn to examining some of the most fundamental and widely-used probability distributions. These distributions form the building blocks of probability theory and statistics, appearing repeatedly across diverse fields from finance and engineering to biology and social sciences. Understanding their properties—including their probability mass or density functions, expected values, and variances—is essential for modeling real-world phenomena and making informed decisions under uncertainty. In this section, we’ll introduce several key distributions, starting with the simple yet powerful Bernoulli distribution and building toward more complex models that capture different types of random behavior.</p>
<section id="bernoulli-distribution" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>The formal model of a coin toss was described by Bernoulli. He modeled the notion of <em>probability</em> for a coin toss, now known as the Bernoulli distribution, where <span class="math inline">\(X \in \{0,1\}\)</span> and <span class="math inline">\(P(X=1)=p, P(X=0) = 1-p\)</span>. Laplace gave us the <em>principle of insufficient reason</em>: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete uniform distribution on the set of possible outcomes.</p>
<p>A Bernoulli trial relates to an experiment with the following conditions</p>
<ol type="1">
<li>The result of each trial is either a success or failure.</li>
<li>The probability <span class="math inline">\(p\)</span> of a success is the same for all trials.</li>
<li>The trials are assumed to be <em>independent</em>.</li>
</ol>
<p>The Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by <span class="math inline">\(\text{Bernoulli}(p)\)</span>, where <span class="math inline">\(p\)</span> is the probability of success.</p>
<p>The probability mass function (PMF) of a Bernoulli distribution is defined as follows: <span class="math display">\[
P(X = x) = \begin{cases}
p &amp; \text{if } x = 1 \\
1 - p &amp; \text{if } x = 0
\end{cases}
\]</span> The expected value (mean) of a Bernoulli distributed random variable <span class="math inline">\(X\)</span> is given by: <span class="math display">\[\E{X} = p
\]</span> Simply speaking, if you are to toss a coin many times, you expect <span class="math inline">\(p\)</span> heads.</p>
<p>The variance of <span class="math inline">\(X\)</span> is given by: <span class="math display">\[
\Var{X} = p(1-p)
\]</span></p>
<div id="exm-Coin" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.15 (Coin Toss)</strong></span> The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is <span class="math inline">\(S = \{H,T\}\)</span>, and <span class="math inline">\(p(X = H) = p(X = T) = 1/2\)</span>. On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads <span class="math inline">\(X\)</span> out of two coin tosses is</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(p(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Given the probability mass function we can, for example, calculate the probability of at least one head as <span class="math inline">\(\prob{X \geq 1} = \prob{X =1} + \prob{X =2} = p(1)+p(2) = 3/4\)</span>.</p>
</div>
<p>The Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to <span class="math inline">\(X\)</span>, which is the number of successes. Its probability distribution is calculated via: <span class="math display">\[
\prob{X=x} = {n \choose x} p^x(1-p)^{n-x}.
\]</span> Here <span class="math inline">\({n \choose x}\)</span> is the combinatorial function, <span class="math display">\[
{n \choose x} = \frac{n!}{x!(n-x)!},
\]</span> Here compbinatorial function <span class="math inline">\({n \choose x}\)</span> counts the number of ways of getting <span class="math inline">\(x\)</span> successes in <span class="math inline">\(n\)</span> trials and <span class="math inline">\(n!=n(n-1)(n-2)\ldots 2 \cdot 1\)</span> counts the number of permutations of <span class="math inline">\(n\)</span> observations without replacement.</p>
<p>Plot below shows the probability mass function of a Binomial distribution with <span class="math inline">\(n = 20\)</span> and <span class="math inline">\(p = 0.3\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">size =</span> <span class="dv">20</span>, <span class="at">prob =</span> <span class="fl">0.3</span>), <span class="at">names.arg =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a> <span class="at">xlab =</span> <span class="st">"Number of Heads"</span>, <span class="at">ylab =</span> <span class="st">"Probability"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Probability Mass Function of a Binomial Distribution (n = 20, p = 0.3)</figcaption>
</figure>
</div>
</div>
</div>
<p>The table below shows the expected value and variance of a Binomial random variable. Those quantities can be calculated by plugging in the possible outcomes ans corresponding probabilities into the definitions of expected value and variance.</p>
<table class="caption-top table">
<caption>Mean and Variance of Binomial</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Binomial Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = n p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = n p ( 1 - p )\)</span></td>
</tr>
</tbody>
</table>
<p>For large sample sizes <span class="math inline">\(n\)</span>, this distribution is approximately normal with mean <span class="math inline">\(np\)</span> and variance of <span class="math inline">\(np(1-p)\)</span>.</p>
<p>Suppose we are about to toss two coins. Let <span class="math inline">\(X\)</span> denote the number of heads. Then the following table specifies the probability distribution <span class="math inline">\(p(x)\)</span> for all possible values <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>. This leads to the following table</p>
<table class="caption-top table">
<caption>Outcomes of two coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1/4</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1/2</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2</td>
<td style="text-align: center;">1/4</td>
</tr>
</tbody>
</table>
<p>Thus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely the Binomial distribution.</p>
<p>Let <span class="math inline">\(X\)</span> be the number of heads in three flips. Each possible outcome (“realization”) of <span class="math inline">\(X\)</span> is an <em>event</em>. Now consider the event of getting only two heads <span class="math display">\[
\{ X= 2\} = \{ HHT, HTH, THH \} ,
\]</span> The probability distribution of <span class="math inline">\(X\)</span> is Binomial with parameters <span class="math inline">\(n = 3, p= 1/2\)</span>, where <span class="math inline">\(n\)</span> denotes the sample size (a.k.a. number of trials) and <span class="math inline">\(p\)</span> is the probability of heads; we have a fair coin. The notation is <span class="math inline">\(X \sim \mathrm{Bin} \left ( n = 3 , p = \frac{1}{2} \right )\)</span> where the sign <span class="math inline">\(\sim\)</span> is read as <em>distributed as</em>.</p>
<table class="caption-top table">
<caption>Outcomes of three coin flips</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Result</th>
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\prob{X=x}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">HHH</td>
<td style="text-align: center;">3</td>
<td style="text-align: right;"><span class="math inline">\(p^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">HHT</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1- p)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\(p^2 ( 1 - p)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THH</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;"><span class="math inline">\((1-p)p^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">HTT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p( 1-p)^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">THT</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\(p ( 1-p)^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">TTH</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^2 p\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">TTT</td>
<td style="text-align: center;">0</td>
<td style="text-align: right;"><span class="math inline">\((1-p)^3\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="poisson-distribution" class="level3">
<h3 class="anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h3>
<p>The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. You can think of Poisson distribution as a limiting case of the Binomial distribution when the number of trials <span class="math inline">\(n\)</span> is large and the probability of success <span class="math inline">\(p\)</span> is small, such that <span class="math inline">\(np = \lambda\)</span> remains constant. Think of a soccer game where the goal is the “successful” event of interest. The soccer team does not have a predefined number of attempts to score a goal. Rather they continiously try to score a goal until they do. The number of goals scored in a game is the number of events occurring in a fixed interval of time. The mean number of goals scored in a game is the mean rate of events occurring in a fixed interval of time.</p>
<p>There are many examples of Poisson distributed random variables. For example, the number of phone calls received by a call center per hour, the number of emails received per day, the number of customers arriving at a store per hour, the number of defects in a manufactured product, the number of accidents on a highway per month.</p>
<p>A random variable <span class="math inline">\(X\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\lambda &gt; 0\)</span> if its probability mass function is given by: <span class="math display">\[
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
\]</span> for <span class="math inline">\(k = 0, 1, 2, 3, \ldots\)</span>, where <span class="math inline">\(\lambda\)</span> is both the mean and variance of the distribution.</p>
<p>Plot below shows the probability mass function of a Poisson distribution with <span class="math inline">\(\lambda = 1, 5, 10\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x values</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the first distribution</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dpois</span>(x, <span class="at">lambda =</span> <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Number of Events"</span>, <span class="at">ylab =</span> <span class="st">"Probability"</span>, </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Poisson Distribution for Different Lambda Values"</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>))</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the other distributions</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dpois</span>(x, <span class="at">lambda =</span> <span class="dv">5</span>), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">dpois</span>(x, <span class="at">lambda =</span> <span class="dv">10</span>), <span class="at">col =</span> <span class="st">"green"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">type =</span> <span class="st">"b"</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"lmb = 1"</span>, <span class="st">"lmb = 5"</span>, <span class="st">"lmb = 10"</span>), </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Probability Mass Function of a Poisson Distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>We write <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span> to denote that <span class="math inline">\(X\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</p>
<table class="caption-top table">
<caption>Mean and Variance of Poisson</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Poisson Distribution</th>
<th style="text-align: center;">Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Expected value</td>
<td style="text-align: center;"><span class="math inline">\(\mu = \E{X} = \lambda\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Variance</td>
<td style="text-align: center;"><span class="math inline">\(\sigma^2 = \Var{X} = \lambda\)</span></td>
</tr>
</tbody>
</table>
<div id="exm-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.16 (Customer Arrivals)</strong></span> Suppose customers arrive at a coffee shop at an average rate of 3 customers per hour. What is the probability that exactly 5 customers will arrive in the next hour?</p>
<p>Using the Poisson distribution with <span class="math inline">\(\lambda = 3\)</span>: <span class="math display">\[
P(X = 5) = \frac{3^5 e^{-3}}{5!} = \frac{243 \times e^{-3}}{120} \approx 0.101
\]</span></p>
<p>So there is approximately a 10.1% chance that exactly 5 customers will arrive in the next hour.</p>
</div>
<p>The Poisson distribution can be derived as a limiting case of the binomial distribution when <span class="math inline">\(n\)</span> is large and <span class="math inline">\(p\)</span> is small, such that <span class="math inline">\(np = \lambda\)</span> remains constant. This connection makes the Poisson distribution particularly useful for modeling rare events in large populations.</p>
</section>
<section id="normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="normal-distribution">Normal Distribution</h3>
<p>The Normal distribution is a continuous probability distribution that is widely used in statistics and probability theory. It is also known as the Gaussian distribution or the bell curve. The Normal distribution is characterized by its symmetric bell-shaped curve and is defined by two parameters: the mean (<span class="math inline">\(\mu\)</span>) and the variance (<span class="math inline">\(\sigma^2\)</span>).</p>
<p>The probability density function (PDF) of the Normal distribution is given by: <span class="math display">\[
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>, where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation.</p>
<p>The plot below shows the probability density function of a random variable <span class="math inline">\(X \sim N(0,1)\)</span> that follows a Normal distribution with mean 0 and standard deviation 1.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"x"</span>, <span class="at">ylab =</span> <span class="st">"f(x)"</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Normal Distribution for Mean 0 and Standard Deviation 1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Probability Density Function of a Normal Distribution</figcaption>
</figure>
</div>
</div>
</div>
<p>The Normal distribution is symmetric around the mean, and the mean, median, and mode are all equal.</p>
<p>The Normal distribution is often used to model real-world phenomena such as measurement errors, heights, weights, and scores on standardized tests. It is also used in hypothesis testing and confidence interval construction.</p>
<div id="exm-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.17 (Heights of Adults)</strong></span> The heights of adult males in a certain population are normally distributed with a mean of 70 inches and a standard deviation of 3 inches.</p>
<ol type="a">
<li>What is the probability that a randomly selected male is between 67 and 73 inches tall?</li>
</ol>
<p>Using the Normal distribution with <span class="math inline">\(\mu = 70\)</span> and <span class="math inline">\(\sigma = 3\)</span>:</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">70</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create x values for the curve</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(mu <span class="sc">-</span> <span class="dv">4</span><span class="sc">*</span>sigma, mu <span class="sc">+</span> <span class="dv">4</span><span class="sc">*</span>sigma, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the normal curve</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Height (inches)"</span>, <span class="at">ylab =</span> <span class="st">"Density"</span>,</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Normal Distribution (mu = 70, sigma = 3)"</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Add vertical lines at 67 and 73</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">67</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="dv">73</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight the area between 67 and 73</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>x_area <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">67</span>, <span class="dv">73</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>y_area <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x_area, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span>(<span class="fu">c</span>(<span class="dv">67</span>, x_area, <span class="dv">73</span>), <span class="fu">c</span>(<span class="dv">0</span>, y_area, <span class="dv">0</span>), </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.3</span>), <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">67</span>, <span class="sc">-</span><span class="fl">0.005</span>, <span class="st">"67"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">73</span>, <span class="sc">-</span><span class="fl">0.005</span>, <span class="st">"73"</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">70</span>, <span class="fl">0.06</span>, <span class="st">"P(67 &lt; X &lt; 73)"</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">col =</span> <span class="st">"darkblue"</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add grid for better readability</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>(<span class="at">col =</span> <span class="st">"gray"</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="01-prob_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption>Normal Distribution with Highlighted Area</figcaption>
</figure>
</div>
</div>
</div>
<p>The prrobability of a randomly selected male being between 67 and 73 inches tall is the area under the curve between 67 and 73 inches and is approximately 0.6827.</p>
<p>Now let’s calculate this probability using <code>R</code>:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>prob_between <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(<span class="dv">73</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">67</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"P(67 &lt; X &lt; 73) ="</span>, <span class="fu">round</span>(prob_between, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## P(67 &lt; X &lt; 73) = 0.68</code></pre>
</div>
</div>
<p>The calculation shows that <span class="math inline">\(P(67 &lt; X &lt; 73) = P(X \leq 73) - P(X \leq 67) \approx 0.6827\)</span>.</p>
<p>This result makes sense because 67 and 73 inches are exactly one standard deviation below and above the mean (70 ± 3), respectively. According to the empirical rule (68-95-99.7 rule), approximately 68% of values in a normal distribution fall within one standard deviation of the mean.</p>
<p>So approximately 68.27% of adult males are between 67 and 73 inches tall.</p>
<ol start="2" type="a">
<li>What height corresponds to the 95th percentile?</li>
</ol>
<p>We need to find <span class="math inline">\(x\)</span> such that <span class="math inline">\(P(X \leq x) = 0.95\)</span>. From standard normal tables, <span class="math inline">\(\Phi^{-1}(0.95) \approx 1.645\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="at">mean =</span> <span class="dv">70</span>, <span class="at">sd =</span> <span class="dv">3</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Height corresponding to the 95th percentile:"</span>, <span class="fu">round</span>(x, <span class="dv">2</span>), <span class="st">"inches</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## Height corresponding to the 95th percentile: 75 inches</code></pre>
</div>
</div>
<p>Therefore: <span class="math inline">\(x = \mu + \sigma \cdot 1.645 = 70 + 3 \cdot 1.645 = 74.935\)</span> inches.</p>
</div>
</section>
</section>
<section id="conditional-marginal-and-joint-distributions" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="conditional-marginal-and-joint-distributions"><span class="header-section-number">1.7</span> Conditional, Marginal and Joint Distributions</h2>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, which can be related to each other. Knowing <span class="math inline">\(X\)</span> would change your belief about <span class="math inline">\(Y\)</span>. For example, as a first pass, psychologists who study the phenomenon of happiness can be interested in understanding its relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.</p>
<p>Formally, the <em>joint distribution</em> of two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a function given by <span class="math display">\[
p(x,y) = \prob{X=x,Y=y}.
\]</span> This maps all combinations of possible values of these two variables to a probability on the interval [0,1].</p>
<p>The <em>conditional probability</em> is a measure of the probability of a random variable <span class="math inline">\(X\)</span>, given that the value of another random variable was observed <span class="math inline">\(Y = y\)</span>. <span class="math display">\[
p(x\mid y) = \prob{X = x \mid Y = y}.
\]</span></p>
<p>The <em>marginal probability</em> of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the marginal probability <span class="math inline">\(\prob{X}\)</span> is the probability distribution of <span class="math inline">\(X\)</span> when the values of <span class="math inline">\(Y\)</span> are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of <span class="math inline">\(Y\)</span>. The converse is also true: the marginal distribution can be obtained for <span class="math inline">\(Y\)</span> by summing over the separate values of <span class="math inline">\(X\)</span>.</p>
<p>Marginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.</p>
<div id="exm-salary" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.18 (Salary-Happiness)</strong></span> Let’s look at an example. Suppose that to model the relationship between two quantities, salary <span class="math inline">\(Y\)</span> and happiness <span class="math inline">\(X\)</span>. After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Results of the Gallup survey. Rows are Salary (<span class="math inline">\(Y\)</span>) and columns are happiness (<span class="math inline">\(X\)</span>)</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">X = 0 (low)</th>
<th style="text-align: center;">X = 1 (medium)</th>
<th style="text-align: center;">X = 2 (high)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Y = low (0)</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = medium (1)</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Y = high (2)</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr class="even">
<td style="text-align: left;">Y = very high (3)</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.14</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Each cell of the table is the joint probability, e.g.&nbsp;14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.</p>
<p>Now, if we want to answer the question what is the percent of high earners in the population. For that we need to calculate what is called a <em>marginal probability</em> <span class="math inline">\(\prob{y = 2}\)</span>. We can calculate the proportion of high earners <span class="math inline">\(\prob{y = 2}\)</span> by summing up the entries in the third row of the table, which is 0.17 in our case.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.07</span> <span class="sc">+</span> <span class="fl">0.01</span> <span class="sc">+</span> <span class="fl">0.09</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>## [1] 0.17</code></pre>
</div>
</div>
<p>Formally marginal probability over <span class="math inline">\(y\)</span> is calculated by summing the joint probability over the other variable, <span class="math inline">\(x\)</span>, <span class="math display">\[
p(y) = \sum_{x \in S}p(x,y)
\]</span> Where <span class="math inline">\(S\)</span> is the set of all possible values of the random variable <span class="math inline">\(X\)</span>.</p>
<!--     Salary ($S$)   0 (low)   1 (medium)   2 (high) -->
<!--   -------------- --------- ------------ ---------- -->
<!--            low 0      0.03         0.12       0.07 -->
<!--         medium 1      0.02         0.13       0.11 -->
<!--           high 2      0.01         0.13       0.14 -->
<!--      very high 3      0.01         0.09       0.14 -->
<p>Another question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the <em>conditional probability</em> of <span class="math inline">\(X\)</span> given that the value of variable <span class="math inline">\(Y\)</span> is known. This is denoted by <span class="math inline">\(\prob{X=x\mid Y=y}\)</span> or simply <span class="math inline">\(p(x\mid y)\)</span>, where <span class="math inline">\(\mid\)</span> reads as “given” or “conditional upon”.</p>
<p>The conditional probability <span class="math inline">\(p(x\mid y)\)</span> also has interpretation as updating your probability over <span class="math inline">\(X\)</span> after you have learned the new information about <span class="math inline">\(Y\)</span>. In this sense, probability is also the language of how you change opinions in light of new evidence. The proportion of happy people among high earners is given by the conditional probability <span class="math inline">\(\prob{X=2\mid Y=2}\)</span> and can be calculated by dividing the proportion of those who are high earners and highly happy by the proportion of high earners <span class="math display">\[
\prob{X=2\mid Y=2} = \dfrac{\prob{X=2,Y=2}}{\prob{Y=2}} = \dfrac{0.09}{0.17} = 0.5294118.
\]</span></p>
<p>Now, if we compare it with the proportion of highly happy people <span class="math inline">\(\prob{X = 2} = 0.38\)</span>, we see that on average you are more likely to be happy given your income is high.</p>
</div>
</section>
<section id="independence" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="independence"><span class="header-section-number">1.8</span> Independence</h2>
<p>Historically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.</p>
<p>Using the notion of conditional probability, we can define independence of two variables. Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be <em>independent</em> if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. That is, learning information <span class="math inline">\(X=x\)</span> doesn’t affect our probabilistic assessment of <span class="math inline">\(Y\)</span> for any value <span class="math inline">\(y\)</span>. This is known as the <em>Prosecutor’s Fallacy</em> as it arises when probability is used as evidence in a court of law. In the case of independence, <span class="math inline">\(p(x \mid y) = p(x)\)</span> and <span class="math inline">\(p(y \mid x) = p(y)\)</span>. Specifically, the probability of innocence given the evidence is not the same as the probability of evidence given innocence. It is very important to ask the question “what exactly are we conditioning on?” Usually, the observed evidence or data. Probability, of course, given evidence was one of the first applications of Bayes. Central to personalized probability. Clearly this is a strong condition and rarely holds in practice.</p>
<p>Conditional probabilities are counter-intuitive. For example, one of the most important properties is typically <span class="math inline">\(p( x \mid y ) \neq p( y\mid x )\)</span>.</p>
<p>We just derived an important relation that allows us to calculate conditional probability <span class="math inline">\(p(x \mid y)\)</span> when we know joint probability <span class="math inline">\(p(x,y)\)</span> and marginal probability <span class="math inline">\(p(y)\)</span>. The total probability or evidence can be calculated as usual, via <span class="math inline">\(p(y) = \sum_{x}p(x,y)\)</span>.</p>
<p>We will see that independence will lead to a different conclusion than the Bayes conditional probability decomposition: specifically, independence yields <span class="math inline">\(p( x,y ) = p(x) p(y)\)</span> and Bayes says <span class="math inline">\(p(x ,y) = p(x)p(y \mid x)\)</span>.</p>
<p>We need to specify a distribution on each of those variables. Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math display">\[
\prob{Y = y \mid X = x} = \prob{Y = y},
\]</span> for all possible <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. The joint distribution will be given by <span class="math display">\[
p(x,y) = p(x)p(y).
\]</span> If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then the probability of event <span class="math inline">\(X\)</span> and event <span class="math inline">\(Y\)</span> happening 00 is the product of individual probabilities. From the conditional distribution formula it follows that <span class="math display">\[
p(x \mid y) = \dfrac{p(x,y)}{p(y)} = \dfrac{p(x)p(y)}{p(y)} = p(x).
\]</span> Another way to think of independence is to say that knowing the value of <span class="math inline">\(Y\)</span> doesn’t tell us anything about possible values of <span class="math inline">\(X\)</span>. For example when tossing a coin twice, the probability of getting <span class="math inline">\(H\)</span> in the second toss does not depend on the outcome of the first toss.</p>
<p>The expression of independence expresses the fact that knowing <span class="math inline">\(X=x\)</span> tells you nothing about <span class="math inline">\(Y\)</span>. In the coin tossing example, if <span class="math inline">\(X\)</span> is the outcome of the first toss and <span class="math inline">\(Y\)</span> is the outcome of the second toss <span class="math display">\[
\prob{ X=H  \mid  Y=T } = \prob{X=H  \mid  Y=H } = \prob{X=H}.
\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-diaconis1989methods" class="csl-entry" role="listitem">
Diaconis, Persi, and Frederick and Mosteller. 1989. <span>“Methods for <span>Studying Coincidences</span>.”</span> <em>Journal of the American Statistical Association</em> 84 (408): 853–61.
</div>
<div id="ref-keynes1921treatise" class="csl-entry" role="listitem">
Keynes, John Maynard. 1921. <em>A Treatise on Probability</em>. Macmillan.
</div>
<div id="ref-kreps1988notes" class="csl-entry" role="listitem">
Kreps, David. 1988. <em>Notes <span>On The Theory Of Choice</span></em>. Boulder: Westview Press.
</div>
<div id="ref-polson2025negative" class="csl-entry" role="listitem">
Polson, Nick, and Vadim Sokolov. 2025. <span>“Negative <span>Probability</span>.”</span> <em>Applied Stochastic Models in Business and Industry</em> 41 (1): e2910.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./00-intro.html" class="pagination-link" aria-label="The Modern AI Playbook">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">The Modern AI Playbook</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./02-bayes.html" class="pagination-link" aria-label="Bayes Rule">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>