<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>16&nbsp; Model Selection – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./17-theoryai.html" rel="next">
<link href="./15-forecasting.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="16&nbsp; Model Selection – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="16-select_files/figure-html/fig-qqplot-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="16&nbsp; Model Selection – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="16-select_files/figure-html/fig-qqplot-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./16-select.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#fundamental-considerations-in-model-selection" id="toc-fundamental-considerations-in-model-selection" class="nav-link active" data-scroll-target="#fundamental-considerations-in-model-selection"><span class="header-section-number">16.1</span> Fundamental Considerations in Model Selection</a>
  <ul class="collapse">
  <li><a href="#model-complexity-and-generalization" id="toc-model-complexity-and-generalization" class="nav-link" data-scroll-target="#model-complexity-and-generalization">Model Complexity and Generalization</a></li>
  <li><a href="#overfitting-and-underfitting" id="toc-overfitting-and-underfitting" class="nav-link" data-scroll-target="#overfitting-and-underfitting">Overfitting and Underfitting</a></li>
  <li><a href="#data-quality-and-quantity" id="toc-data-quality-and-quantity" class="nav-link" data-scroll-target="#data-quality-and-quantity">Data Quality and Quantity</a></li>
  <li><a href="#model-explainability" id="toc-model-explainability" class="nav-link" data-scroll-target="#model-explainability">Model Explainability</a></li>
  <li><a href="#computational-cost" id="toc-computational-cost" class="nav-link" data-scroll-target="#computational-cost">Computational Cost</a></li>
  <li><a href="#ethical-considerations" id="toc-ethical-considerations" class="nav-link" data-scroll-target="#ethical-considerations">Ethical Considerations</a></li>
  </ul></li>
  <li><a href="#prediction-vs-interpretation" id="toc-prediction-vs-interpretation" class="nav-link" data-scroll-target="#prediction-vs-interpretation"><span class="header-section-number">16.2</span> Prediction vs Interpretation</a>
  <ul class="collapse">
  <li><a href="#breimans-two-cultures" id="toc-breimans-two-cultures" class="nav-link" data-scroll-target="#breimans-two-cultures">Breiman’s Two Cultures</a></li>
  </ul></li>
  <li><a href="#diagnostics-for-model-assumptions" id="toc-diagnostics-for-model-assumptions" class="nav-link" data-scroll-target="#diagnostics-for-model-assumptions"><span class="header-section-number">16.3</span> Diagnostics for Model Assumptions</a></li>
  <li><a href="#out-of-sample-performance" id="toc-out-of-sample-performance" class="nav-link" data-scroll-target="#out-of-sample-performance"><span class="header-section-number">16.4</span> Out-of-Sample Performance</a></li>
  <li><a href="#bias-variance-trade-off" id="toc-bias-variance-trade-off" class="nav-link" data-scroll-target="#bias-variance-trade-off"><span class="header-section-number">16.5</span> Bias-Variance Trade-off</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">16.6</span> Cross-Validation</a></li>
  <li><a href="#sec-model-eval" id="toc-sec-model-eval" class="nav-link" data-scroll-target="#sec-model-eval"><span class="header-section-number">16.7</span> Model evaluation: calibration, discrimination, and scoring</a>
  <ul class="collapse">
  <li><a href="#calibration-and-calibration-plots" id="toc-calibration-and-calibration-plots" class="nav-link" data-scroll-target="#calibration-and-calibration-plots">Calibration and calibration plots</a></li>
  <li><a href="#proper-scoring-rules-and-the-decision-theory-link" id="toc-proper-scoring-rules-and-the-decision-theory-link" class="nav-link" data-scroll-target="#proper-scoring-rules-and-the-decision-theory-link">Proper scoring rules (and the decision-theory link)</a></li>
  </ul></li>
  <li><a href="#bayesian-model-selection" id="toc-bayesian-model-selection" class="nav-link" data-scroll-target="#bayesian-model-selection"><span class="header-section-number">16.8</span> Bayesian Model Selection</a>
  <ul class="collapse">
  <li><a href="#bayesian-information-criterion-bic" id="toc-bayesian-information-criterion-bic" class="nav-link" data-scroll-target="#bayesian-information-criterion-bic">Bayesian Information Criterion (BIC)</a></li>
  <li><a href="#neural-information-criterion-nic" id="toc-neural-information-criterion-nic" class="nav-link" data-scroll-target="#neural-information-criterion-nic">Neural Information Criterion (NIC)</a></li>
  <li><a href="#automatic-relevance-determination-ard" id="toc-automatic-relevance-determination-ard" class="nav-link" data-scroll-target="#automatic-relevance-determination-ard">Automatic Relevance Determination (ARD)</a></li>
  </ul></li>
  <li><a href="#model-selection-and-bayesian-relativity" id="toc-model-selection-and-bayesian-relativity" class="nav-link" data-scroll-target="#model-selection-and-bayesian-relativity"><span class="header-section-number">16.9</span> Model Selection and Bayesian Relativity</a>
  <ul class="collapse">
  <li><a href="#exhaustive-vs-non-exhaustive-hypotheses" id="toc-exhaustive-vs-non-exhaustive-hypotheses" class="nav-link" data-scroll-target="#exhaustive-vs-non-exhaustive-hypotheses">Exhaustive vs Non-Exhaustive Hypotheses</a></li>
  </ul></li>
  <li><a href="#the-asymptotic-carrier" id="toc-the-asymptotic-carrier" class="nav-link" data-scroll-target="#the-asymptotic-carrier"><span class="header-section-number">16.10</span> The Asymptotic Carrier</a></li>
  <li><a href="#model-explainability-1" id="toc-model-explainability-1" class="nav-link" data-scroll-target="#model-explainability-1"><span class="header-section-number">16.11</span> Model Explainability</a>
  <ul class="collapse">
  <li><a href="#the-imperative-for-explainability" id="toc-the-imperative-for-explainability" class="nav-link" data-scroll-target="#the-imperative-for-explainability">The Imperative for Explainability</a></li>
  <li><a href="#the-paradox-of-understanding-and-utility" id="toc-the-paradox-of-understanding-and-utility" class="nav-link" data-scroll-target="#the-paradox-of-understanding-and-utility">The Paradox of Understanding and Utility</a></li>
  </ul></li>
  <li><a href="#model-elaboration-and-nested-model-testing" id="toc-model-elaboration-and-nested-model-testing" class="nav-link" data-scroll-target="#model-elaboration-and-nested-model-testing"><span class="header-section-number">16.12</span> Model Elaboration and Nested Model Testing</a>
  <ul class="collapse">
  <li><a href="#the-dickey-savage-approach-to-nested-models" id="toc-the-dickey-savage-approach-to-nested-models" class="nav-link" data-scroll-target="#the-dickey-savage-approach-to-nested-models">The Dickey-Savage Approach to Nested Models</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">16.13</span> Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./16-select.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-select" class="quarto-section-identifier"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“<em>When you have eliminated the impossible, whatever remains, however improbable, must be the truth.</em>” - Sherlock Holmes</p>
</blockquote>
<p>Model selection is the art of choosing the model that best captures the true signal while avoiding the trap of fitting noise. Holmes’ words apply directly: when confronted with data, we must navigate countless possible models, each representing a different hypothesis, and systematically eliminate those that fail to generalize.</p>
<p>The next chapter (<a href="17-theoryai.html" class="quarto-xref"><span>Chapter 17</span></a>) covers regularization techniques—Ridge regression, LASSO, and their Bayesian interpretations—which can also be viewed as model selection mechanisms. Here we focus on the fundamental considerations: the bias-variance tradeoff, overfitting and underfitting, computational constraints, and whether our goal is prediction or interpretation. We draw on Breiman’s influential distinction between the “two cultures” of statistical modeling, then develop practical methodologies including cross-validation and information criteria.</p>
<section id="fundamental-considerations-in-model-selection" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="fundamental-considerations-in-model-selection"><span class="header-section-number">16.1</span> Fundamental Considerations in Model Selection</h2>
<section id="model-complexity-and-generalization" class="level3">
<h3 class="anchored" data-anchor-id="model-complexity-and-generalization">Model Complexity and Generalization</h3>
<p>Choosing the right model for the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is excessively complex (e.g., high-degree polynomials or deep neural networks trained on insufficient data) risks overfitting by memorizing training examples rather than learning the underlying pattern. This results in excellent training performance but poor generalization to unseen examples. This challenge is exacerbated when dealing with non-linear relationships, high-dimensional data, or noisy signals, where the optimal complexity is not immediately obvious. Systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning is often required to strike the right balance between signal capture and noise rejection.</p>
</section>
<section id="overfitting-and-underfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-and-underfitting">Overfitting and Underfitting</h3>
<p>Overfitting arises when a statistical learning algorithm captures stochastic noise rather than the underlying signal. This phenomenon typically manifests when a model possesses excessive degrees of freedom relative to the training data size, allowing it to “memorize” specific examples. While such a model may achieve near-zero error on the training set, it fails to generalize to new data because it has learned idiosyncrasies specific to the training sample rather than the general population structure. Common indicators of overfitting include a diverging gap between training and validation error curves or performance degradation on held-out data during training.</p>
<p>Underfitting, conversely, occurs when a model lacks the sufficient complexity to capture the true underlying structure between inputs and outputs. This often results from specifying a model class that is too rigid (e.g., fitting a linear model to quadratic data) or from over-regularization. An underfit model exhibits high bias, performing poorly on both training and test datasets. Unlike overfitting, underfitting is characterized by consistently high error rates across all data partitions, indicating a fundamental inability to model the signal.</p>
</section>
<section id="data-quality-and-quantity" class="level3">
<h3 class="anchored" data-anchor-id="data-quality-and-quantity">Data Quality and Quantity</h3>
<p>The reliability of predictive models is intrinsically linked to the quality and richness of the available data. Noisy, incomplete, or biased data inevitably leads to suboptimal model performance. While sufficient data volume is crucial for learning complex relationships, data <em>quality</em> often plays a more decisive role. Issues such as missing values, inconsistent formatting, label noise, and sampling bias are pervasive in real-world applications.</p>
<p>To address these challenges, the industry has seen the rise of data-centric AI platforms. Services like Scale AI and Toloka offer human-in-the-loop solutions for high-quality data annotation and validation. These platforms leverage globally distributed workforces to perform tasks ranging from image segmentation to text classification, ensuring that the ground truth labels used for training are accurate. By implementing rigorous quality control mechanisms—such as consensus voting among multiple annotators and dynamic skill-based routing—these services mitigate the risks associated with poor data quality.</p>
</section>
<section id="model-explainability" class="level3">
<h3 class="anchored" data-anchor-id="model-explainability">Model Explainability</h3>
<p>In many high-stakes domains—such as healthcare, finance, and criminal justice—predictive accuracy alone is insufficient. Stakeholders require <em>interpretability</em>: a clear understanding of how a model arrives at its decisions. This need drives the trade-off between using complex “black box” models (like deep neural networks) and simpler, transparent models (like logistic regression or decision trees).</p>
<p>Regulatory frameworks, including the EU’s GDPR, increasingly mandate a “right to explanation,” compelling organizations to deploy systems that are not just accurate but also accountable. While we explore specific techniques for achieving this—such as LIME, SHAP, and attention mechanisms—in detail later in this chapter, it is vital to recognize at the outset that the choice of model often dictates the ceiling of explainability.</p>
<!-- The detailed discussion of LIME, SHAP, attention, etc. has been consolidated into the "Model Explainability" section later in the chapter. -->
</section>
<section id="computational-cost" class="level3">
<h3 class="anchored" data-anchor-id="computational-cost">Computational Cost</h3>
<p>Training and serving predictive models can be computationally expensive, particularly for deep learning architectures operating on massive datasets. In resource-constrained environments, this necessitates a trade-off between model performance and computational efficiency.</p>
<p>Development of specialized hardware has played a pivotal role in addressing this. Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) allow for massive parallelization of matrix operations, reducing training times from weeks to hours. However, <em>inference</em> cost remains a challenge for deployment.</p>
<p>Edge computing—processing data locally on devices rather than in the cloud—has emerged as a solution for low-latency applications like autonomous driving and IoT. To enable this, techniques such as <em>quantization</em> (reducing numerical precision from 32-bit floats to 8-bit integers) and <em>model pruning</em> (removing redundant connections) are frequently employed. These methods allow complex models to run efficiently on mobile and embedded hardware with minimal loss of accuracy.</p>
</section>
<section id="ethical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="ethical-considerations">Ethical Considerations</h3>
<p>Predictive models are not value-neutral; their deployment can have profound societal consequences. Ethical failures often manifest as <em>algorithmic bias</em>, where models perpetuate or amplify existing discrimination. For instance, facial recognition systems trained on imbalanced datasets have demonstrated significantly higher error rates for darker-skinned individuals. Similarly, hiring algorithms trained on historical data may learn to replicate past discriminatory hiring practices.</p>
<p>Fairness in machine learning is an active area of research, dealing with metrics like statistical parity and equalized odds. However, maximizing fairness often requires trade-offs with predictive accuracy, necessitating careful ethical judgment during model development.</p>
<p>Privacy is another key concern. Deep learning models can inadvertently memorize sensitive training data, making them vulnerable to inversion attacks. <em>Differential privacy</em> offers a rigorous mathematical framework to mitigate this risk by adding calibrated noise to computations, ensuring that the model’s output does not reveal whether any specific individual’s data was included in the training set.</p>
<p>Finally, accountability is essential. “Algorithmic impact assessments” and “audits” are becoming standard practice to evaluate potential harms before deployment, ensuring that systems serve the public good while minimizing risk.</p>
</section>
</section>
<section id="prediction-vs-interpretation" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="prediction-vs-interpretation"><span class="header-section-number">16.2</span> Prediction vs Interpretation</h2>
<p>Predictive models can serve two distinct purposes: prediction and interpretation. These goals often conflict. Interpretation requires understanding the relationship between input and output variables, which typically demands simpler, more transparent models.</p>
<p>A model that excels at prediction might not be suitable for interpretation. For example, a complex deep neural network might achieve high predictive accuracy but provide little insight into how the input variables influence the output. Conversely, a simple linear model might be highly interpretable but lack the flexibility to capture complex relationships in the data. A key advantage of linear models is their ability to serve both purposes effectively, unlike more complex models with many parameters that can be difficult to interpret.</p>
<p>Interpretation problems typically require simpler models. We prioritize models that are easy to interpret and explain, even if they have slightly lower predictive accuracy. The evaluation metrics also differ: for interpretation, we typically use the coefficient of determination (R-squared) or p-values, which provide insights into the model’s fit and the statistical significance of the estimated relationships.</p>
<p>The choice between using a model for prediction or interpretation depends on the specific task and desired outcome. If the primary goal is accurate predictions, a complex model with high predictive accuracy might be preferred, even if it is less interpretable. However, if understanding the underlying relationships and causal mechanisms is crucial, a simpler and more interpretable model might be chosen, even if it has slightly lower predictive accuracy. Interpretive models are commonly used in scientific research, social sciences, and other fields where understanding the underlying causes and relationships is crucial.</p>
<p>In practice, it’s often beneficial to consider both prediction and interpretation when building and evaluating models. However, it is not unusual to build two different models, one for prediction and one for interpretation. This allows for a more nuanced analysis of the data and can lead to better insights than using a single model for both purposes.</p>
<section id="breimans-two-cultures" class="level3">
<h3 class="anchored" data-anchor-id="breimans-two-cultures">Breiman’s Two Cultures</h3>
<p>Let <span class="math inline">\(x\)</span> be a high-dimensional input containing a large set of potentially relevant data, and let <span class="math inline">\(y\)</span> represent an output (or response) to a task that we aim to solve based on the information in <span class="math inline">\(x\)</span>. Breiman [2000] summarizes the difference between statistical and machine learning philosophy as follows:</p>
<blockquote class="blockquote">
<p>“There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.”</p>
</blockquote>
<blockquote class="blockquote">
<p>“The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.”</p>
</blockquote>
<blockquote class="blockquote">
<p>“Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”</p>
</blockquote>
<p>Deep learning predictors offer several advantages over traditional predictors:</p>
<ul>
<li>Input data can include all data of possible relevance to the prediction problem at hand</li>
<li>Nonlinearities and complex interactions among input data are accounted for seamlessly</li>
<li>Overfitting is more easily avoided than with traditional high-dimensional procedures</li>
<li>Fast, scalable computational frameworks (such as TensorFlow) are available</li>
</ul>
<p>Tree-based models and deep learning models exemplify the “algorithmic culture” that Breiman describes. These models can capture complex, non-linear relationships in data without requiring explicit specification of the functional form. However, this flexibility comes at the cost of interpretability. While decision trees offer some interpretability through their hierarchical structure, deep neural networks are often considered “black boxes” due to their complex, multi-layered architecture.</p>
<p>The trade-off between interpretability and accuracy is a central theme in modern machine learning. Simple models like linear regression are highly interpretable but may lack the flexibility to capture complex patterns. Complex models like deep neural networks can achieve high accuracy but are difficult to interpret. This has led to the development of various techniques for making complex models more interpretable, including feature importance measures, attention mechanisms, and surrogate models that approximate the behavior of complex models with simpler, more interpretable ones.</p>
</section>
</section>
<section id="diagnostics-for-model-assumptions" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="diagnostics-for-model-assumptions"><span class="header-section-number">16.3</span> Diagnostics for Model Assumptions</h2>
<p>What makes a good model? If the goal is prediction, then the model is as good as its predictions. The easiest way to visualize the quality of predictions is to plot <span class="math inline">\(y\)</span> versus <span class="math inline">\(\hat{y}\)</span>. Most of the time we use empirical assessment of model quality. However, sometimes theoretical bounds can be derived for a model that describe its accuracy. For example, in the case of the linear regression model, the prediction interval is defined by <span class="math display">\[
\hat{y} \pm s\sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^2}{\sum_{i=1}^n(x_i-\bar{x})^2}}
\]</span> where <span class="math inline">\(s\)</span> is the standard deviation of the residuals. The prediction interval is the confidence interval for the prediction. The prediction interval is wider than the confidence interval because it includes the uncertainty in the prediction.</p>
<p>Assume we have a predictive model <span class="math display">\[
y = f(x) + \epsilon
\]</span> and we have some modeling assumption regarding the distribution of <span class="math inline">\(\epsilon\)</span>. For example, when we use linear regression or BART, we assume that <span class="math inline">\(\epsilon\)</span> follows a normal distribution. One simple approach to test if observed samples <span class="math inline">\(\epsilon_1,\ldots,\epsilon_n\)</span> follow a specific distribution is to use <em>Exploratory Data Analysis (EDA)</em>.</p>
<p>The most common tools for exploratory data analysis are Q-Q plots, scatter plots, and bar plots/histograms.</p>
<p>A Q-Q plot compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). A quantile is the fraction (or percent) of points below the given value. That is, the <span class="math inline">\(i\)</span>-th quantile is the point <span class="math inline">\(x\)</span> for which <span class="math inline">\(i\)</span>% of the data lies below <span class="math inline">\(x\)</span>. On a Q-Q plot, if the two datasets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two datasets <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> come from the same distribution, then the points <span class="math inline">\((x_{(i)}, y_{(i)})\)</span> should lie roughly on the line <span class="math inline">\(y = x\)</span>. If <span class="math inline">\(y\)</span> comes from a distribution that’s linear in <span class="math inline">\(x\)</span>, then the points <span class="math inline">\((x_{(i)}, y_{(i)})\)</span> should lie roughly on a line, but not necessarily on the line <span class="math inline">\(y = x\)</span>.</p>
<div id="exm-qqplot" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.1 (Normal Q-Q plot)</strong></span> <a href="#fig-qqplot" class="quarto-xref">Figure&nbsp;<span>16.1</span></a> shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in <a href="https://rdrr.io/cran/UsingR/man/babyboom.html"><code>UsingR manual</code></a>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>babyboom <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"../data/babyboom.csv"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(babyboom<span class="sc">$</span>wt, <span class="at">bg =</span> <span class="st">"lightblue"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(babyboom<span class="sc">$</span>wt, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div id="fig-qqplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-select_files/figure-html/fig-qqplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qqplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.1: Normal Q-Q plot of baby weights
</figcaption>
</figure>
</div>
</div>
</div>
<p>Visual inspection of the Q-Q plot strongly suggests that birth weights are not normally distributed. We can see that on the left side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.</p>
<p>The Q-Q plots look different if we split the data based on the gender</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="fl">0.7</span>, <span class="fl">0.5</span>), <span class="at">bty =</span> <span class="st">"n"</span>, <span class="at">cex.lab =</span> <span class="dv">1</span>, <span class="at">cex.axis =</span> <span class="fl">0.5</span>, <span class="at">cex.main =</span> <span class="fl">0.5</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">cex =</span> <span class="fl">1.3</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> babyboom <span class="sc">%&gt;%</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(gender <span class="sc">==</span> <span class="st">"girl"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pull</span>(wt)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> babyboom <span class="sc">%&gt;%</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">filter</span>(gender <span class="sc">==</span> <span class="st">"boy"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pull</span>(wt)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(g, <span class="at">bg =</span> <span class="st">"lightblue"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(g, <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(b, <span class="at">bg =</span> <span class="st">"lightblue"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(b, <span class="at">col =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell quarto-layout-panel" data-layout-ncol="2" data-null_prefix="true" data-layout-align="center">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="16-select_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Girls</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="16-select_files/figure-html/unnamed-chunk-1-2.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Boys</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p>Histogram of baby weights by gender</p>
</div>
</div>
</div>
<p>How about the times in hours between births of babies?</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>hr <span class="ot">&lt;-</span> <span class="fu">ceiling</span>(babyboom<span class="sc">$</span>running.time <span class="sc">/</span> <span class="dv">60</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>BirthsByHour <span class="ot">&lt;-</span> <span class="fu">tabulate</span>(hr)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of hours with 0, 1, 2, 3, 4 births</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>ObservedCounts <span class="ot">&lt;-</span> <span class="fu">table</span>(BirthsByHour)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Average number of births per hour</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>BirthRate <span class="ot">&lt;-</span> <span class="fu">sum</span>(BirthsByHour) <span class="sc">/</span> <span class="dv">24</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected counts for Poisson distribution</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>ExpectedCounts <span class="ot">&lt;-</span> <span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, BirthRate) <span class="sc">*</span> <span class="dv">24</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># bind into matrix for plotting</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>ObsExp <span class="ot">&lt;-</span> <span class="fu">rbind</span>(ObservedCounts, ExpectedCounts)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(ObsExp, <span class="at">names =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">beside =</span> <span class="cn">TRUE</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">"Observed"</span>, <span class="st">"Expected"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="16-select_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="480"></p>
<figcaption>Histogram of births by hour</figcaption>
</figure>
</div>
</div>
</div>
<p>What about the Q-Q plot?</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># birth intervals</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>birthinterval <span class="ot">&lt;-</span> <span class="fu">diff</span>(babyboom<span class="sc">$</span>running.time)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># quantiles of standard exponential distribution (rate=1)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>exponential.quantiles <span class="ot">&lt;-</span> <span class="fu">qexp</span>(<span class="fu">ppoints</span>(<span class="dv">43</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqplot</span>(exponential.quantiles, birthinterval, <span class="at">bg =</span> <span class="st">"lightblue"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>lmb <span class="ot">&lt;-</span> <span class="fu">mean</span>(birthinterval)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(exponential.quantiles, exponential.quantiles <span class="sc">*</span> lmb, <span class="at">col =</span> <span class="dv">2</span>) <span class="co"># Overlay a line</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="16-select_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480"></p>
</figure>
</div>
</div>
</div>
<p>Here</p>
<ul>
<li><code>ppoints</code> function computes the sequence of probability points</li>
<li><code>qexp</code> function computes the quantiles of the exponential distribution</li>
<li><code>diff</code> function computes the difference between consecutive elements of a vector</li>
</ul>
</div>
</section>
<section id="out-of-sample-performance" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="out-of-sample-performance"><span class="header-section-number">16.4</span> Out-of-Sample Performance</h2>
<p>A parametric model is selected from a family of functions, with optimization finding the best member by minimizing empirical loss or maximizing likelihood. Finding the right family is the <em>model selection</em> problem: which predictors, interactions, or transformations achieve the best balance between complexity and accuracy? In practice, several models often perform nearly identically.</p>
<p>The optimal model is rarely the one that fits training data perfectly—this typically indicates overfitting, where the model captures noise rather than signal. A good model balances fit against simplicity to ensure generalization. Including too many parameters can yield perfect in-sample fit, but poor out-of-sample performance.</p>
<p>The goal of model selection is not only to achieve a good fit but also to reduce complexity by excluding unnecessary parameters. This process typically involves selecting a model from a relevant class of functions while keeping in mind the trade-offs between bias, variance, and model complexity. Techniques such as cross-validation, information criteria (e.g., AIC, BIC), and regularization methods are commonly used to guide the model selection process.</p>
<p>The model selection task is sometimes one of the most time-consuming parts of data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem is as yet another optimization problem, with the goal of finding the best family of functions that describe the data. With a small number of predictors, we can use brute force (check all possible models). For example, with <span class="math inline">\(p\)</span> predictors there are <span class="math inline">\(2^p\)</span> possible models with no interactions. Thus, the number of potential function families is huge even for modest values of <span class="math inline">\(p\)</span>. One cannot consider all transformations and interactions.</p>
<p>Our goal is to build a model that predicts well for out-of-sample data, i.e., data that was not used for training. Ultimately, we are interested in using our models for prediction, and thus out-of-sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when choosing a predictive model, as one of the winners of the Netflix prize put it: “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it.” Out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in nature. To uncover the pattern, we start with a training dataset, denoted by <span class="math display">\[
D = (y_i,x_i)_{i=1}^n
\]</span> and to test the validity of our model, we use an out-of-sample testing dataset <span class="math display">\[
D^* = (y_j^*, x_j^*)_{j=1}^m,
\]</span> where <span class="math inline">\(x_i\)</span> is a set of <span class="math inline">\(p\)</span> predictors and <span class="math inline">\(y_i\)</span> is the response variable.</p>
<p>A good predictor will “generalize” well and provide low MSE out-of-sample. There are a number of methods/objective functions that we will use to find <span class="math inline">\(\hat{f}\)</span>. In a parameter-based approach, we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map <span class="math inline">\(f\)</span> that approximates the process that generated the data. For example, data could represent some physical observations, and our goal is to recover the “laws of nature” that led to those observations. One of the pitfalls is to find a map <span class="math inline">\(f\)</span> that does not generalize. Generalization means that our model actually learned the “laws of nature” and not just identified patterns present in training. The lack of generalization of the model is called overfitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of <span class="math inline">\(n\)</span> points can be approximated by a polynomial of degree <span class="math inline">\(n\)</span>, e.g., we can always draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to observations that were not in our training data. In other words, the empirical risk measure for <span class="math inline">\(D^*\)</span> is likely to be very high. Let us illustrate that in-sample fit can be deceiving.</p>
<div id="exm-hard" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.2 (Hard Function)</strong></span> Say we want to approximate the following function <span class="math display">\[
f(x) = \dfrac{1}{1+25x^2}.
\]</span> This function is simply a ratio of two polynomial functions and we will try to build a linear model to reconstruct this function</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-rungekutta" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rungekutta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="16-select_files/figure-html/fig-rungekutta-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rungekutta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.2: Runge-Kutta function
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-rungekutta" class="quarto-xref">Figure&nbsp;<span>16.2</span></a> shows the function itself (black line) on the interval <span class="math inline">\([-3,3]\)</span>. We used observations of <span class="math inline">\(x\)</span> from the interval <span class="math inline">\([-2,2]\)</span> to train the data (solid line) and from <span class="math inline">\([-3,-2) \cup (2,3]\)</span> (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model <span class="math inline">\(\hat{y} = \beta_0 + \beta_1 x\)</span> is not a good model. However, as we increase the degree of the polynomial to 20, the resulting model <span class="math inline">\(\hat{y} = \beta_0 + \beta_1x + \beta_2 x^2 +\ldots+\beta_{20}x^{20}\)</span> does fit the training dataset quite well, but does a very poor job on the test dataset. Thus, while in-sample performance is good, the out-of-sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice, in-sample loss or classification rates provide us with a metric for comparing different predictors. It is worth mentioning here that there should be a penalty for overly complex rules that fit extremely well in-sample but perform poorly on out-of-sample data. As Einstein famously said, “A model should be simple, but not simpler.”</p>
</div>
<p>To a Bayesian, the solutions to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.</p>
<p>These results have major implications for empirical work and practical applications, as they provide a guide for forecasting.</p>
</section>
<section id="bias-variance-trade-off" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="bias-variance-trade-off"><span class="header-section-number">16.5</span> Bias-Variance Trade-off</h2>
<p>For any predictive model, we seek to achieve the best possible results, i.e., the smallest MSE or misclassification rate. For a historical perspective on how aggregating independent estimates reduces variance while preserving low bias, see the discussion of Galton’s ox-weighing experiment in <a href="10-data.html#sec-bias-variance-galton" class="quarto-xref"><span>Section 10.1.3.2</span></a>. However, model performance can vary depending on the specific training/validation split used. A model that performed well on one test set may not produce good results given additional data. Sometimes we observe situations where a small change in the data leads to a large change in the final estimated model, e.g., the parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias can reduce variance in the final results. Similarly, low bias can result in high variance, but can also produce an oversimplification of the final model. The bias/variance concept is depicted below.</p>
<div id="fig-bias-variance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bias-variance.drawio.svg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.3: Bias-variance trade-off
</figcaption>
</figure>
</div>
<div id="exm-bias-variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.3 (Bias-variance)</strong></span> We demonstrate bias-variance concept using Boston housing example. We fit a model <span class="math inline">\(\mathrm{medv} = f(\mathrm{lstat})\)</span>. We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree <span class="math inline">\(1,\ldots,12\)</span> ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial <span class="math inline">\(f\)</span> we averaged MSE from each of the ten models.</p>
<p><a href="#fig-boston-bias-variance" class="quarto-xref">Figure&nbsp;<span>16.4 (a)</span></a> shows bias and variance for our twelve different models. As expected, bias increases while variance decreases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!</p>
<div id="fig-boston" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boston-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-boston" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-boston-bias-variance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-boston-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/boston-bias-variance.svg" class="img-fluid figure-img" data-ref-parent="fig-boston">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-boston-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Bias-Variance
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-boston" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-boston-optimal-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-boston-optimal-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/boston-optimal-model.svg" class="img-fluid figure-img" data-ref-parent="fig-boston">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-boston-optimal-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Optimal complexity model
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boston-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.4: Metrics for twelve polynomial functions fitted into Boston housing data set. Left panel: As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off. Right panel: Optimal model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots).
</figcaption>
</figure>
</div>
<p>Let’s take another, more formal look at the bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error <span class="math inline">\(\E{(y-\hat{y})^2}\)</span> as a function of bias <span class="math inline">\(\E{y-\hat{y}}\)</span> and variance <span class="math inline">\(\Var{\hat{y}}\)</span>.</p>
<p>Here <span class="math inline">\(\hat{y} = \hat{f}_{\beta}(x)\)</span> is the prediction from the model, and <span class="math inline">\(y = f(x) + \epsilon\)</span> is the true value, which is measured with noise <span class="math inline">\(\Var{\epsilon} = \sigma^2\)</span>, where <span class="math inline">\(f(x)\)</span> is the true unknown function. The expectation above measures the squared error of our model on a random sample <span class="math inline">\(x\)</span>. <span class="math display">\[
\begin{aligned}
\E{(y - \hat{y})^2}
&amp; = \E{y^2 + \hat{y}^2 - 2 y\hat{y}} \\
&amp; = \E{y^2} + \E{\hat{y}^2} - \E{2y\hat{y}} \\
&amp; = \Var{y} + \E{y}^2 + \Var{\hat{y}} + \E{\hat{y}}^2 - 2f\E{\hat{y}} \\
&amp; = \Var{y} + \Var{\hat{y}} + (f^2 - 2f\E{\hat{y}} + \E{\hat{y}}^2) \\
&amp; = \Var{y} + \Var{\hat{y}} + (f - \E{\hat{y}})^2 \\
&amp; = \sigma^2 + \Var{\hat{y}} + \mathrm{Bias}(\hat{y})^2\end{aligned}
\]</span> Here we used the following identity: <span class="math inline">\(\Var{X} = \E{X^2} - \E{X}^2\)</span> and the fact that <span class="math inline">\(f\)</span> is deterministic and <span class="math inline">\(\E{\epsilon} = 0\)</span>, thus <span class="math inline">\(\E{y} = \E{f(x)+\epsilon} = f + \E{\epsilon} = f\)</span>.</p>
</div>
</section>
<section id="cross-validation" class="level2" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">16.6</span> Cross-Validation</h2>
<p>If the dataset at hand is small and we cannot dedicate a large enough sample size for testing, simply measuring error on a test dataset can lead to wrong conclusions. When the size of the testing set <span class="math inline">\(D^*\)</span> is small, the estimated out-of-sample performance has high variance, depending on precisely which observations are included in the test set. On the other hand, when the training set <span class="math inline">\(D^*\)</span> is a large fraction of the entire sample available, the estimated out-of-sample performance will be underestimated. Why?</p>
<p>A simple solution is to perform the training/testing split randomly several times and then use the average out-of-sample errors. This procedure has two parameters: the fraction of samples to be selected for testing <span class="math inline">\(p\)</span> and the number of estimates to be performed <span class="math inline">\(K\)</span>. The resulting algorithm is as follows:</p>
<pre><code>fsz = as.integer(p*n)
error = rep(0,K)
for (k in 1:K)
{
    test_ind = sample(1:n,size = fsz)
    training = d[-test_ind,]
    testing = d[test_ind,]
    m = lm(y~x, data=training)
    yhat = predict(m,newdata = testing)
    error[k] = mean((yhat-testing$y)^2)
}
res = mean(error)</code></pre>
<p><a href="#fig-bootstrap" class="quarto-xref">Figure&nbsp;<span>16.5</span></a> shows the process of splitting the dataset randomly five times.</p>
<p>Cross-validation modifies the random splitting approach to use a more “disciplined” way to split the dataset for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations, and thus each data point is used once for testing. Like the random approach, CV helps address the high variance issue of out-of-sample performance estimation when the available dataset is small. <a href="#fig-cv" class="quarto-xref">Figure&nbsp;<span>16.6</span></a> shows the process of splitting the dataset five times using the cross-validation approach.</p>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-bootstrap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bag5-excel.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.5: Bootstrap
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-cv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/cv5-excel.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.6: Cross-validation
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Training set (red) and testing set (green)</p>
</div>
</div>
</div>
<div id="exm-simulated" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.4 (Simulated)</strong></span> We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used <span class="math inline">\(x=-2,-1.99,-1.98,\ldots,2\)</span> and <span class="math inline">\(y = 2+3x + \epsilon, ~ \epsilon \sim N(0,\sqrt{3})\)</span>. We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. <a href="#fig-test-error20" class="quarto-xref">Figure&nbsp;<span>16.7</span></a> compares empirical distribution of errors estimated from 35 samples.</p>
<div id="fig-test-error20" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-test-error20-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/test-error20.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-test-error20-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.7: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.
</figcaption>
</figure>
</div>
<p>As we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.</p>
</div>
</section>
<section id="sec-model-eval" class="level2" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="sec-model-eval"><span class="header-section-number">16.7</span> Model evaluation: calibration, discrimination, and scoring</h2>
<p>Variable selection is often presented as choosing a subset of predictors. A complementary view is that we are searching for a low-dimensional summary of the inputs that retains predictive information: an informal analogue of the idea of a minimal sufficient statistic from classical inference (<a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>).</p>
<p>When we evaluate models, it is useful to separate three questions. First, does the model rank or separate cases well (discrimination)? Second, do the model’s predicted probabilities match observed frequencies (calibration)? Third, when we must make decisions, how should we score probabilistic forecasts so that "better" means "closer to the truth" in a principled way?</p>
<p>For binary outcomes, discrimination is often summarized by the receiver operating characteristic (ROC) curve. For a thresholded score, define the true positive rate (TPR) and false positive rate (FPR) as the threshold varies. The ROC curve plots TPR against FPR across all thresholds, and the area under the curve (AUC) summarizes performance as a single number between 0 and 1, with 0.5 corresponding to random ranking; see <span class="citation" data-cites="fawcett2006roc">Fawcett (<a href="references.html#ref-fawcett2006roc" role="doc-biblioref">2006</a>)</span>.</p>
<section id="calibration-and-calibration-plots" class="level3">
<h3 class="anchored" data-anchor-id="calibration-and-calibration-plots">Calibration and calibration plots</h3>
<p>Calibration concerns the probabilistic interpretation of predicted probabilities. A well-calibrated model that outputs <span class="math inline">\(\hat p=0.8\)</span> on many cases should be correct about 80% of the time on those cases. A common diagnostic is a calibration plot (reliability diagram), which bins predictions and compares average predicted probability to the empirical frequency in each bin.</p>
</section>
<section id="proper-scoring-rules-and-the-decision-theory-link" class="level3">
<h3 class="anchored" data-anchor-id="proper-scoring-rules-and-the-decision-theory-link">Proper scoring rules (and the decision-theory link)</h3>
<p>A proper scoring rule assigns a numerical score to a probabilistic forecast and is designed so that, in expectation, the best strategy is to report the true predictive distribution; see <span class="citation" data-cites="gneiting2007strictly">Gneiting and Raftery (<a href="references.html#ref-gneiting2007strictly" role="doc-biblioref">2007</a>)</span>. Two widely used examples are the log score (negative log predictive density) and the Brier score for binary outcomes <span class="citation" data-cites="brier1950verification">Brier (<a href="references.html#ref-brier1950verification" role="doc-biblioref">1950</a>)</span>. These connect directly to <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a>: choosing a scoring rule is equivalent to choosing a loss for probabilistic prediction, and the expected score becomes a decision-theoretic risk.</p>
<p>For Bayesian models, posterior predictive checks compare observed data to replicated data drawn from the posterior predictive distribution. The workflow is: sample parameters from the posterior, simulate replicated datasets, and compare summary statistics or discrepancies between observed and replicated data; see <span class="citation" data-cites="gelman2013bda">Gelman et al. (<a href="references.html#ref-gelman2013bda" role="doc-biblioref">2013</a>)</span>.</p>
</section>
</section>
<section id="bayesian-model-selection" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="bayesian-model-selection"><span class="header-section-number">16.8</span> Bayesian Model Selection</h2>
<p>The probabilistic models of interest are the joint probability distribution <span class="math inline">\(p(D,\theta)\)</span> (called a generative model) and <span class="math inline">\(P(Y,\theta \mid X)\)</span> (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative models require modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying the topic of an article can be solved using a discriminative distribution. The problem of generating a new article requires a generative model.</p>
<p>Let <span class="math inline">\(D\)</span> denote data. Let <span class="math inline">\(\theta_M \in \Theta_M\)</span> denote a set of parameters under model <span class="math inline">\(M \in \mathcal{M}\)</span>. Let <span class="math inline">\(\theta_M = (\theta_1, \ldots, \theta_M)\)</span> be the <span class="math inline">\(p\)</span>-vector of parameters. The Bayesian approach is straightforward: implement the Bayesian paradigm by executing Bayes’ rule. This requires the laws of probability and not optimization techniques. The notion of model complexity is no different. Let <span class="math inline">\(\mathcal{M}\)</span> denote the space of models and <span class="math inline">\(\theta\)</span> be the parameter vector. The Bayesian paradigm simply places probabilities over parameters and models given the data, namely <span class="math inline">\(p(\theta_M, M \mid y)\)</span>, where <span class="math inline">\(y = (y_1, \ldots, y_n)\)</span>.</p>
<p>This has a number of decompositions. Bayes’ theorem calculates the joint posterior over parameters and models given data <span class="math inline">\(D\)</span>, namely <span class="math display">\[
    p(\theta_M,M\mid D) = p(\theta_M \mid M,D)P(M\mid D).
\]</span> Notice how this factors the posterior into two terms: the conditional posterior over parameters given the model and the posterior over models given data.</p>
<p>The key quantity is the weight of <em>evidence</em> (a.k.a. marginal distribution of the data <span class="math inline">\(D\)</span> given the model <span class="math inline">\(M\)</span>), defined by <span class="math display">\[
p(D \mid M) = \int_{\Theta_M} p(D \mid \theta_M, M) p(\theta_M \mid M) d\theta_M.
\]</span> Here <span class="math inline">\(p(D \mid \theta_M, M)\)</span> is the traditional likelihood function. The key conditional distribution, however, is the specification of the prior over parameters <span class="math inline">\(p(\theta_M \mid M)\)</span>. As this is used in the marginalization, it can affect the Bayes risk dramatically. Occam’s razor comes from the fact that this marginalization provides a weight of evidence that favors simpler models over more complex ones.</p>
<p>This leads to a posterior over models, which is calculated as: <span class="math display">\[\begin{align*}
    P(M\mid D)  &amp; = \dfrac{p(D\mid M)P(M)}{p(D)}, \\
    p(D\mid M ) &amp; = \int_{ \Theta_M} p(D\mid \theta_M , M ) p( \theta_M | M ) d \theta_M.
\end{align*}\]</span> Notice that this requires a joint prior specification <span class="math inline">\(p(\theta_M, M) = p(\theta_M | M)p(M)\)</span> over parameters and models. The quantity <span class="math inline">\(p(M| D)\)</span> is the marginal posterior for model complexity given the data. There is an equivalent posterior <span class="math inline">\(p(\theta_M | D)\)</span> for the parameters. <span class="math inline">\(p(D \mid M)\)</span> is the evidence of the data <span class="math inline">\(D\)</span> given the complexity (a.k.a. conditional likelihood). The full evidence is <span class="math display">\[
p( D ) = \int p( D| M ) p(M) d M.
\]</span> This has been used to select the amount of hyperparameter regularization; see, for example, <span class="citation" data-cites="mackay1992bayesian">MacKay (<a href="references.html#ref-mackay1992bayesian" role="doc-biblioref">1992</a>)</span>.</p>
<p>We will see that the prior <span class="math inline">\(p(\theta_M | M)\)</span> will lead to an Occam’s razor effect, namely that the marginal distribution will favor simpler models. Importantly, this Occam’s razor effect is not in conflict with the Bayesian double descent phenomenon, which emerges from the marginal posterior of models given data and the conditional prior specification <span class="math inline">\(p(\theta_M | M)\)</span>.</p>
<div id="exm-dice-example" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.5 (Dice Example)</strong></span> Let’s consider a simple example of throwing a dice. Say, there are three dice, one that has numbers 1 through 6 (regular dice), one with three sides with ones and three sides with 2s (dice 1-2), one with three sides with ones and three sides with 2s and three sides with 3s (dice 1-2-3).</p>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/dice-1-2.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>Dice 1-2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/dice-1-2-3.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>Dice 1-2-3</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/dice-regular.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>Regular Dice</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>You observe outcome of one dice throw and it is 3. Which dice is it?</p>
<p>Two out of three explanations are plausible (dice 1-2-3 and regular dice). Intuitively, the 1-2-3 dice is more likely to produce 3 than the regular dice. Thus, if we need to choose, we would choose the 1-2-3 dice. For the sake of completeness, we can use the Bayes rule to calculate the evidence for each model.</p>
<p>Using Bayes’ rule: <span class="math display">\[P(M_i | D) = \frac{P(D | M_i) P(M_i)}{P(D)}\]</span></p>
<p>where <span class="math inline">\(M_i\)</span> represents each dice model and <span class="math inline">\(D\)</span> is the observed data (outcome = 3).</p>
<p>We set equal prior probabilities for each dice: <span class="math display">\[P(M_1) = P(M_2) = P(M_3) = \frac{1}{3}.\]</span> Now, we calculate the likelihood for each model.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dice Type</th>
<th>Probability of Rolling a 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regular dice (<span class="math inline">\(M_1\)</span>)</td>
<td><span class="math inline">\(P(3 | M_1) = 1/6\)</span></td>
</tr>
<tr class="even">
<td>Dice 1-2 (<span class="math inline">\(M_2\)</span>)</td>
<td><span class="math inline">\(P(3 | M_2) = 0\)</span></td>
</tr>
<tr class="odd">
<td>Dice 1-2-3 (<span class="math inline">\(M_3\)</span>)</td>
<td><span class="math inline">\(P(3 | M_3) = \frac{1}{3}\)</span></td>
</tr>
</tbody>
</table>
<p>Then, the marginal likelihood is: <span class="math display">\[
P(D) = \sum_{i=1}^{3} P(D | M_i) P(M_i) = \frac{1}{6} \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + \frac{1}{3} \cdot \frac{1}{3} = \frac{1}{6}.
\]</span></p>
<p>Finally, posterior probabilities are</p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 80%">
</colgroup>
<thead>
<tr class="header">
<th>Dice Type</th>
<th>Posterior Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regular dice</td>
<td><span class="math inline">\(P(M_1 | D) = \frac{\frac{1}{6} \cdot \frac{1}{3}}{\frac{1}{6}}=1/3\)</span></td>
</tr>
<tr class="even">
<td>Dice 1-2</td>
<td><span class="math inline">\(P(M_2 | D) = \frac{0 \cdot \frac{1}{3}}{\frac{1}{6}}=0\)</span></td>
</tr>
<tr class="odd">
<td>Dice 1-2-3</td>
<td><span class="math inline">\(P(M_3 | D) = \frac{\frac{1}{3} \cdot \frac{1}{3}}{\frac{1}{6}}=2/3\)</span></td>
</tr>
</tbody>
</table>
<p>Given the observation of outcome 3, the dice 1-2-3 is twice as likely as the regular dice. The dice 1-2 is completely ruled out since it cannot produce a 3. This demonstrates how Bayesian model selection naturally eliminates impossible explanations and provides relative evidence for competing hypotheses.</p>
</div>
<p>This example demonstrates how the Bayesian paradigm provides a coherent framework to simultaneously infer parameters and model complexity. The fact that Bayesian approach selects the most probable model that explains the observed data, is called the automatic Occam’s razor. The Occam’s razor is a principle that states that the simplest explanation is the best explanation.</p>
<p>While performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: left;">Given</th>
<th style="text-align: left;">Hidden</th>
<th style="text-align: left;">What to find</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Training</td>
<td style="text-align: left;"><span class="math inline">\(D = (X,Y) = \{x_i,y_i\}_{i=1}^n\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\theta\)</span></td>
<td style="text-align: left;"><span class="math inline">\(p(\theta \mid D)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Prediction</td>
<td style="text-align: left;"><span class="math inline">\(x_{\text{new}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(y_{\text{new}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(p(y_{\text{new}}  \mid  x_{\text{new}}, D)\)</span></td>
</tr>
</tbody>
</table>
<p>The training can be performed via the Bayes rule <span class="math display">\[
p(\theta \mid D) = \dfrac{p(Y \mid \theta,X)p(\theta)}{\int p(Y \mid \theta,X)p(\theta)d\theta}.
\]</span> Now to perform the second step (prediction), we calculate <span class="math display">\[
p(y_{\text{new}}  \mid  x_{\text{new}}, D) = \int p(y_{\text{new}}  \mid  x_{\text{new}},\theta)p(\theta \mid D)d\theta
\]</span> Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with <span class="math display">\[
p(\theta \mid D) \approx \delta(\theta_{\text{MAP}}),~~\theta_{\text{MAP}} \in \arg\max_{\theta}p(\theta \mid D)
\]</span> To calculate <span class="math inline">\(\theta_{\text{MAP}}\)</span>, we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by <span class="math display">\[
p(y_{\text{new}}  \mid  x_{\text{new}}, D) = \int p(y_{\text{new}}  \mid  x_{\text{new}},\theta)p(\theta \mid D)d\theta \approx p(y_{\text{new}}  \mid  x_{\text{new}},\theta_{\text{MAP}}).
\]</span></p>
<p>The <a href="#fig-bayes-model-selection" class="quarto-xref">Figure&nbsp;<span>16.8</span></a> below illustrates the Bayesian model selection process. The figure shows the joint distribution over parameters and data for three models. You can think of each ellipse as the region where most of the probability mass is concentrated.</p>
<div id="fig-bayes-model-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayes-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/bayes-model-selection.drawio.svg" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayes-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16.8: Bayesian model Selection
</figcaption>
</figure>
</div>
<p>If we project the ellipses onto the parameter space, we get the prior distributions for each model. We can see that the <span class="math inline">\(M_2\)</span> is the most concentrated. If we project the ellipses onto the data space, we get the prior distributions over data for each model.</p>
<p>After observing data <span class="math inline">\(D\)</span> (horizontal line), each prior gets updated. The intersection of the observed data line with each ellipse shows how well each model can explain the data. Models with good overlap between prior and observed data will have higher posterior probability. <span class="math inline">\(M_3\)</span> appears to have the best intersection with the observed data, it is the model with the highest marginal likelihood.</p>
<p>This illustrates how Bayesian model selection naturally favors models that achieve the best balance between explaining the observed data and maintaining appropriate complexity, automatically implementing Occam’s razor through the evidence calculation.</p>
<div id="exm-racial" class="theorem example">
<p><span class="theorem-title"><strong>Example 16.6 (Racial discrimination)</strong></span> Say we want to analyze racial discrimination by the US courts. We have three variables:</p>
<ul>
<li>Murderer: <span class="math inline">\(m \in {0,1}\)</span> (black/white)</li>
<li>Victim: <span class="math inline">\(v \in \{0,1\}\)</span> (black/white)</li>
<li>Verdict: <span class="math inline">\(d \in \{0,1\}\)</span> (prison/death penalty)</li>
</ul>
<p>Say we have the data</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">m</th>
<th style="text-align: center;">v</th>
<th style="text-align: center;">d</th>
<th style="text-align: center;">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">132</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">52</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">97</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
</tr>
</tbody>
</table>
<p>We would like to establish a causal relations between the race and verdict variables. For this, we consider several models</p>
<ol type="1">
<li><p><span class="math inline">\(p(d \mid m,v) = p(d) = \theta\)</span></p></li>
<li><p><span class="math inline">\(p(d \mid m,v) = p(d \mid v)\)</span>; <span class="math inline">\(p(d \mid v=0) = \alpha, ~p(d \mid v=1)=\beta\)</span></p></li>
<li><p><span class="math inline">\(p(d \mid v,m) = p(d \mid m)\)</span>; <span class="math inline">\(p(d \mid m=0) = \gamma,~p(d \mid m=1) = \delta\)</span></p></li>
<li><p><span class="math inline">\(p(d|v,m)\)</span> cannot be reduced, and<br>
</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(p(d=1 \mid m,v)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(m=0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(m=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(v=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\tau\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\chi\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(v=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\nu\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\zeta\)</span></td>
</tr>
</tbody>
</table></li>
</ol>
<p>We calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model <span class="math display">\[
p(Y ,\theta \mid X) = p(Y \mid X,\theta)p(\theta \mid X)
\]</span> Here <span class="math inline">\(X\)</span> is the number of cases, and <span class="math inline">\(Y\)</span> is the number of death penalties. We use uninformative prior <span class="math inline">\(\theta \sim U[0,1]\)</span>. To specify the likelihood, we use Binomial distribution <span class="math display">\[
Y \mid X,\theta \sim \text{Bin}(X,\theta),~~\text{Bin}(Y \mid X,\theta) = \binom{X}{Y}\theta^Y(1-\theta)^{X-Y}
\]</span> We assume <span class="math inline">\(p(\theta)\sim Uniform\)</span>. Now lets calculate the evidence <span class="math display">\[
p(Y, \theta \mid X) = \int p(Y  \mid  X,\theta)p(\theta)d\theta
\]</span> for each of the four models</p>
<ol type="1">
<li><span class="math inline">\(p(Y \mid X) = \int \text{Bin}(19 \mid 151,\theta)\text{Bin}(0 \mid 9,\theta)\text{Bin}(11 \mid 63,\theta)\text{Bin}(6 \mid 103,\theta)d\theta\)</span> <span class="math inline">\(\propto \int_0^{1} \theta^{36}(1-\theta)^{290}d\theta = \text{Beta}(37,291) = 2.8\times 10^{-51}\)</span></li>
<li><span class="math inline">\(p(Y \mid X) = \int\int \text{Bin}(19 \mid 151,\alpha)\text{Bin}(0 \mid 9,\beta)\text{Bin}(11 \mid 63,\alpha)\text{Bin}(6 \mid 103,\beta)d\alpha d\beta \propto 4.7\times 10^{-51}\)</span></li>
<li><span class="math inline">\(p(d \mid v,m) = p(d \mid m)=\int\int \text{Bin}(19 \mid 151,\gamma)\text{Bin}(0 \mid 9,\gamma)\text{Bin}(11 \mid 63,\delta)\text{Bin}(6 \mid 103,\delta)d\gamma d\delta \propto 0.27\times10^{-51}\)</span></li>
<li><span class="math inline">\(p(d \mid v,m) = \int\int\int\int \text{Bin}(19 \mid 151,\tau)\text{Bin}(0 \mid 9,\nu)\text{Bin}(11 \mid 63,\chi)\text{Bin}(6 \mid 103,\zeta)d\tau d\nu d\chi d\zeta \propto 0.18\times10^{-51}\)</span></li>
</ol>
<p>The last model (4) is overly complex. With enough parameters, it can perfectly fit any dataset, including the noise. Ideally, we want a model that fits the data well but is also parsimonious. The Bayesian evidence calculation naturally penalized the complex model for its large parameter space, resulting in the lowest evidence score. This illustrates how the Bayesian approach inherently protects against overfitting without requiring an explicit regularization term.</p>
<p>This dataset also illustrates <strong>Simpson’s paradox</strong>, a phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This highlights the importance of considering confounding variables and the structure of the data. A related conceptual problem is <strong>Bertrand’s box paradox</strong>, which demonstrates how conditional probability can be counterintuitive.</p>
</div>
<section id="bayesian-information-criterion-bic" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-information-criterion-bic">Bayesian Information Criterion (BIC)</h3>
<p>The Bayesian Information Criterion (BIC) is a model selection criterion that penalizes the complexity of the model. It is derived from a Bayesian approach. The BIC is defined as:</p>
<p><span class="math display">\[
\mathrm{BIC} = \log p(D\mid \hat{\theta}_k, M_k) - \frac{k}{2} \log n.
\]</span></p>
<p>Here <span class="math inline">\(\hat{\theta}_k\)</span> is the MAP estimate of the <span class="math inline">\(k\)</span> parameters in model <span class="math inline">\(M_k\)</span>, and <span class="math inline">\(n\)</span> is the sample size. As such, there is a penalty <span class="math inline">\(-\frac{k}{2} \log n\)</span> for increasing the dimensionality <span class="math inline">\(k\)</span> of the model under consideration.</p>
<p>The BIC uses the marginal likelihood of the data under model <span class="math inline">\(M_k\)</span> (denoted <span class="math inline">\(M\)</span> for simplicity here), which is approximated using Laplace’s method.</p>
<p>The idea of Laplace’s method is to approximate integrals of the form <span class="math inline">\(\int f(\theta) e^{-g(\theta)} d\theta\)</span> where <span class="math inline">\(g(\theta)\)</span> has a sharp minimum at some point <span class="math inline">\(\hat{\theta}\)</span>. The method works by approximating <span class="math inline">\(g(\theta)\)</span> with its second-order Taylor expansion around the minimum <span class="math inline">\(\hat{\theta}\)</span>. Since <span class="math inline">\(g'(\hat{\theta}) = 0\)</span> at the minimum, we have</p>
<p><span class="math display">\[
g(\theta) \approx g(\hat{\theta}) + \frac{1}{2}g''(\hat{\theta})(\theta-\hat{\theta})^2.
\]</span> So the integral transforms into a Gaussian form: <span class="math display">\[
\int f(\theta) e^{-g(\theta)} d\theta \approx f(\hat{\theta}) e^{-g(\hat{\theta})} \int e^{-\frac{1}{2}g''(\hat{\theta})(\theta-\hat{\theta})^2} d\theta.
\]</span></p>
<p>The remaining integral is a standard Gaussian integral that evaluates to <span class="math inline">\(\sqrt{\frac{2\pi}{g''(\hat{\theta})}}\)</span>, giving us:</p>
<p><span class="math display">\[
\int f(\theta) e^{-g(\theta)} d\theta \approx f(\hat{\theta}) e^{-g(\hat{\theta})} \sqrt{\frac{2\pi}{g''(\hat{\theta})}}.
\]</span></p>
<p>In the multivariate case, we have <span class="math inline">\(\theta \in \mathbb{R}^k\)</span> is a <span class="math inline">\(k\)</span>-dimensional parameter vector. The second-order Taylor expansion around the minimum <span class="math inline">\(\hat{\theta}\)</span> becomes: <span class="math display">\[
g(\theta) \approx g(\hat{\theta}) + \frac{1}{2}(\theta-\hat{\theta})^T \mathbf{H}(\hat{\theta}) (\theta-\hat{\theta}),
\]</span> where <span class="math inline">\(\mathbf{H}(\hat{\theta})\)</span> is the <span class="math inline">\(k \times k\)</span> Hessian matrix of second derivatives at <span class="math inline">\(\hat{\theta}\)</span>. The multivariate Gaussian integral then evaluates to:</p>
<p><span class="math display">\[
\int f(\theta) e^{-g(\theta)} d\theta \approx f(\hat{\theta}) e^{-g(\hat{\theta})} (2\pi)^{k/2} |\det(\mathbf{H}(\hat{\theta}))|^{-\frac{1}{2}}
\]</span></p>
<p>In the context of Bayesian model selection, we apply this to approximate the marginal likelihood (evidence). We have:</p>
<p><span class="math display">\[
P(D\mid M) = \int P(D\mid \theta,M)P(\theta\mid M)d\theta
\]</span></p>
<p>Taking the logarithm and identifying <span class="math inline">\(g(\theta) = -\log P(D\mid \theta,M)P(\theta\mid M)\)</span>, the maximum a posteriori (MAP) estimate <span class="math inline">\(\hat{\theta}\)</span> corresponds to the minimum of <span class="math inline">\(g(\theta)\)</span>. The second derivative (Hessian) <span class="math inline">\(\mathbf{H}(\hat{\theta})\)</span> at this point determines the curvature of the log-posterior.</p>
<p><span class="math display">\[
p(D\mid M) = \int p(D\mid \theta,M)p(\theta\mid M)d\theta \approx p(D\mid \hat{\theta},M)p(\hat{\theta}\mid M) (2 \pi)^{k/2} |\det(\mathbf{H}(\hat{\theta}))|^{-\frac{1}{2}}.
\]</span></p>
<p>Here <span class="math inline">\(\hat{\theta}\)</span> is the posterior mode (MAP estimate), and <span class="math inline">\(\mathbf{H}(\hat{\theta})\)</span> is the negative Hessian of the log-posterior at the mode. Taking the logarithm, and assuming <span class="math inline">\(P(\hat{\theta}|M)\)</span> and Hessian terms are <span class="math inline">\(O_p(1)\)</span> or scale appropriately with <span class="math inline">\(n\)</span> (this assumption is justified because as <span class="math inline">\(n\)</span> increases, the likelihood dominates the prior, making the prior term negligible relative to the <span class="math inline">\(O(\log n)\)</span> likelihood term, while the Hessian determinant typically grows polynomially in <span class="math inline">\(n\)</span>, contributing at most <span class="math inline">\(O(\log n)\)</span> terms that are absorbed into the approximation), we get:</p>
<p><span class="math display">\[
\log p(D\mid M) \approx \log p(D\mid \hat{\theta},M) - \dfrac{k}{2}\log n,
\]</span></p>
<p>which is proportional to the BIC. (Note: The exact definition and derivation of BIC can vary slightly, but this captures the essence). The BIC approximation shows how the Bayesian approach naturally penalizes model complexity through the dimensionality term <span class="math inline">\(-\frac{k}{2}\log n\)</span>.</p>
<p>The Bayesian approach averages over the posterior distribution of models given data. Suppose that we have a finite list of models <span class="math inline">\(M \in \{M_1, \ldots, M_J\}\)</span>. Then we can calculate the posterior over models as:</p>
<p><span class="math display">\[
p(M_j | y) = \frac{p(y | M_j) p(M_j)}{\sum_{i=1}^J p(y | M_i) p(M_i)}, \quad \text{where}\; p(y | M_j) = \int L_j(\theta_j|y) p(\theta_j | M_j) d\theta_j.
\]</span></p>
<p>Laplace’s approximation provides a simple <span class="citation" data-cites="lindley1961use">(<a href="references.html#ref-lindley1961use" role="doc-biblioref">Lindley 1961</a>)</span> illustration of how dimensionality is weighted in the Bayesian paradigm. Hence, BIC is related to a log-posterior approximation. Hence, if prior model probabilities <span class="math inline">\(P(M_j)\)</span> are uniform, then <span class="math inline">\(P(M_j\mid D) \propto P(D \mid M_j) \approx \exp(\mathrm{BIC}_j)\)</span>.</p>
<p>In a more general case, the evidence (a.k.a. marginal likelihood) for hypotheses (a.k.a. models) <span class="math inline">\(M_i\)</span> is calculated as follows:</p>
<p><span class="math display">\[
P(D\mid M_i) = \int P(D\mid \theta, M_i)P(\theta\mid M_i)d\theta.
\]</span></p>
<p>Laplace approximation, in the one-dimensional case (<span class="math inline">\(k=1\)</span>), yields:</p>
<p><span class="math display">\[
P(D\mid M_i) \approx P(D\mid \hat{\theta}, M_i)P(\hat{\theta}\mid M_i)\sqrt{2\pi}\sigma_{\text{post}}.
\]</span></p>
<p>Here <span class="math inline">\(\hat{\theta}\)</span> is the maximum (MAP) estimate of the parameter and <span class="math inline">\(\sigma_{\text{post}} = (-H(\hat{\theta}))^{-1/2}\)</span> where <span class="math inline">\(H(\hat{\theta})\)</span> is the second derivative of the log-posterior at <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Generally, in the <span class="math inline">\(k\)</span>-dimensional case, we have:</p>
<p><span class="math display">\[
P(D\mid M_i) \approx P(D\mid \hat{\theta}, M_i)P(\hat{\theta}\mid M_i) (2\pi)^{k/2} |\det(-\mathbf{H}(\hat{\theta}))|^{-\frac{1}{2}}.
\]</span></p>
<p>Here <span class="math inline">\(\mathbf{H}(\hat{\theta}) = \nabla^2\log (P(D\mid \hat{\theta}, M_i)P(\hat{\theta}\mid M_i))\)</span> is the Hessian of the log-posterior function evaluated at the mode <span class="math inline">\(\hat{\theta}\)</span>. As the amount of data collected increases, this Gaussian approximation is expected to become increasingly accurate.</p>
<p>Mackay <span class="citation" data-cites="mackay1992bayesian">(<a href="references.html#ref-mackay1992bayesian" role="doc-biblioref">MacKay 1992</a>)</span> proposes the NIC criterion for selection of neural networks.</p>
</section>
<section id="neural-information-criterion-nic" class="level3">
<h3 class="anchored" data-anchor-id="neural-information-criterion-nic">Neural Information Criterion (NIC)</h3>
<p>David MacKay’s Neural Information Criterion (NIC) applies Bayesian model selection to neural networks by computing approximations to the marginal likelihood for different architectures and hyperparameter configurations.</p>
<p>The evidence framework builds upon the Laplace approximation to estimate the marginal likelihood of neural network models. Given a neural network with parameters <span class="math inline">\(\theta\)</span> and hyperparameters <span class="math inline">\(\alpha\)</span> (such as weight decay parameters), the evidence for a particular model configuration is:</p>
<p><span class="math display">\[
P(D|M,\alpha) = \int P(D|\theta,M) P(\theta|M,\alpha) d\theta
\]</span></p>
<p>where <span class="math inline">\(D\)</span> represents the training data, <span class="math inline">\(M\)</span> denotes the model architecture, and <span class="math inline">\(P(\theta|M,\alpha)\)</span> is the prior distribution over network weights. The Laplace approximation evaluates this integral by expanding the log-posterior around its mode <span class="math inline">\(\hat \theta\)</span>, yielding:</p>
<p><span class="math display">\[
\log P(D|M,\alpha) \approx \log P(D|\hat \theta,M) + \log P(\hat \theta|M,\alpha) - \frac{1}{2}\log|H|
\]</span></p>
<p>where <span class="math inline">\(H\)</span> is the Hessian of the negative log-posterior at the mode <span class="math inline">\(\hat \theta\)</span>. This approximation transforms the intractable integral into a computation involving the maximum a posteriori (MAP) estimate and the curvature of the posterior at that point.</p>
<p>The evidence framework provides a principled approach to several critical decisions in neural network design. For hyperparameter selection, the framework automatically determines optimal regularization strengths by maximizing the evidence with respect to hyperparameters such as weight decay coefficients. Rather than relying on cross-validation, which can be computationally expensive and may not capture the full uncertainty in hyperparameter selection, the evidence provides a direct measure of how well different hyperparameter values support the observed data.</p>
<p>Architecture comparison becomes feasible through direct evidence computation for different network structures. The framework can compare networks with different numbers of hidden units, layers, or connectivity patterns by evaluating their respective marginal likelihoods. This comparison naturally incorporates Occam’s razor, as more complex architectures are penalized through the integration over their larger parameter spaces, unless the additional complexity is justified by substantially improved fit to the data.</p>
<p>The Hessian computation required for the Laplace approximation presents significant computational challenges for modern deep networks with millions or billions of parameters. The full Hessian matrix would be prohibitively large to compute and store explicitly. MacKay’s original framework addressed this through various approximation strategies, including the use of automatic relevance determination (ARD) priors that allow the network to effectively prune irrelevant connections by driving their associated precision parameters to infinity.</p>
</section>
<section id="automatic-relevance-determination-ard" class="level3">
<h3 class="anchored" data-anchor-id="automatic-relevance-determination-ard">Automatic Relevance Determination (ARD)</h3>
<p>The key insight of <em>Automatic Relevance Determination (ARD)</em> is to introduce separate precision hyperparameters for different groups of parameters, allowing the model to automatically determine which features or components are relevant for the task.</p>
<p>In the context of neural networks, consider a network with weights <span class="math inline">\(\mathbf{w} = \{w_{ij}\}\)</span> connecting input features to hidden units. Instead of using a single precision parameter <span class="math inline">\(\alpha\)</span> for all weights, ARD introduces feature-specific precision parameters <span class="math inline">\(\{\alpha_i\}_{i=1}^{p}\)</span> where <span class="math inline">\(p\)</span> is the number of input features. The prior distribution for weights becomes:</p>
<p><span class="math display">\[
p(\mathbf{w}|\boldsymbol{\alpha}) = \prod_{i=1}^{p} \prod_{j=1}^{H} \mathcal{N}(w_{ij}|0, \alpha_i^{-1})
\]</span></p>
<p>where <span class="math inline">\(H\)</span> is the number of hidden units and <span class="math inline">\(\boldsymbol{\alpha} = (\alpha_1, \ldots, \alpha_p)\)</span> are the precision hyperparameters.</p>
<p>The hierarchical Bayesian model is completed by placing hyperpriors on the precision parameters:</p>
<p><span class="math display">\[
p(\alpha_i) = \text{Gamma}(\alpha_i|a_i, b_i)
\]</span></p>
<p>where <span class="math inline">\(a_i\)</span> and <span class="math inline">\(b_i\)</span> are shape and rate parameters, often set to small values (e.g., <span class="math inline">\(a_i = b_i = 10^{-6}\)</span>) to create weakly informative priors.</p>
<p>The ARD mechanism works through the evidence framework by optimizing the marginal likelihood with respect to the hyperparameters. For a given precision <span class="math inline">\(\alpha_i\)</span>, the effective contribution of feature <span class="math inline">\(i\)</span> to the model evidence can be approximated as:</p>
<p><span class="math display">\[
\log p(D|\alpha_i) \approx -\frac{1}{2}\alpha_i \|\mathbf{w}_i\|^2 + \frac{H}{2}\log\alpha_i - \frac{1}{2}\log|\mathbf{A}_i|
\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}_i\)</span> represents all weights associated with feature <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mathbf{A}_i\)</span> is the corresponding block of the Hessian matrix.</p>
<p>When a feature is irrelevant, the optimal precision <span class="math inline">\(\alpha_i^*\)</span> tends to infinity, effectively removing the feature from the model. This occurs because the evidence balances the model fit (first term) against the model complexity (second and third terms). For irrelevant features, the improvement in fit is insufficient to justify the complexity cost, driving <span class="math inline">\(\alpha_i\)</span> to large values.</p>
<p>The ARD update equations, derived by maximizing the marginal likelihood, are:</p>
<p><span class="math display">\[
\alpha_i^{\text{new}} = \frac{\gamma_i}{\|\mathbf{w}_i\|^2}
\]</span></p>
<p>where <span class="math inline">\(\gamma_i\)</span> is the effective number of parameters associated with feature <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[
\gamma_i = H - \alpha_i \text{Tr}(\mathbf{A}_i^{-1})
\]</span></p>
<p>Here, <span class="math inline">\(\text{Tr}(\mathbf{A}_i^{-1})\)</span> represents the trace of the inverse of the Hessian block corresponding to feature <span class="math inline">\(i\)</span>.</p>
<p><strong>Example: Linear Regression with ARD</strong></p>
<p>Consider a linear regression model with ARD priors:</p>
<p><span class="math display">\[
y = \sum_{i=1}^{p} w_i x_i + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \beta^{-1})
\]</span></p>
<p>with priors: <span class="math display">\[
w_i \sim \mathcal{N}(0, \alpha_i^{-1}), \quad i = 1, \ldots, p
\]</span></p>
<p>The posterior distribution for the weights is:</p>
<p><span class="math display">\[
p(\mathbf{w}|D, \boldsymbol{\alpha}, \beta) = \mathcal{N}(\mathbf{w}|\boldsymbol{\mu}, \mathbf{\Sigma})
\]</span></p>
<p>where: <span class="math display">\[
\mathbf{\Sigma} = (\beta \mathbf{X}^T\mathbf{X} + \text{diag}(\boldsymbol{\alpha}))^{-1}
\]</span> <span class="math display">\[
\boldsymbol{\mu} = \beta \mathbf{\Sigma} \mathbf{X}^T \mathbf{y}
\]</span></p>
<p>The ARD updates become: <span class="math display">\[
\alpha_i^{\text{new}} = \frac{1 - \alpha_i \Sigma_{ii}}{\mu_i^2}
\]</span></p>
<p>When <span class="math inline">\(\alpha_i\)</span> becomes very large, the corresponding <span class="math inline">\(\mu_i \approx 0\)</span> and <span class="math inline">\(\Sigma_{ii} \approx 0\)</span>, effectively removing feature <span class="math inline">\(i\)</span> from the model.</p>
<p><strong>Example: Neural Network Feature Selection</strong></p>
<p>In a neural network with <span class="math inline">\(p\)</span> input features and <span class="math inline">\(H\)</span> hidden units, ARD can automatically determine which input features are relevant. Suppose we have a dataset with features representing different types of measurements, some of which may be irrelevant for the prediction task.</p>
<p>The network architecture is: <span class="math display">\[
h_j = \tanh\left(\sum_{i=1}^{p} w_{ij} x_i + b_j\right), \quad j = 1, \ldots, H
\]</span> <span class="math display">\[
y = \sum_{j=1}^{H} v_j h_j + c
\]</span></p>
<p>With ARD priors on input weights: <span class="math display">\[
w_{ij} \sim \mathcal{N}(0, \alpha_i^{-1}), \quad \text{for all } j
\]</span></p>
<p>After training with the evidence framework, features with large <span class="math inline">\(\alpha_i\)</span> values (typically <span class="math inline">\(\alpha_i &gt; 10^6\)</span>) are considered irrelevant and can be pruned. This automatic feature selection often reveals that only a subset of the original features are necessary for good predictive performance.</p>
<p>The ARD principle extends beyond feature selection to other forms of model selection, including:</p>
<ul>
<li><strong>Unit pruning</strong>: Using separate precisions for different hidden units to determine optimal network architecture</li>
<li><strong>Group selection</strong>: Applying ARD to groups of related parameters (e.g., all weights in a particular layer)</li>
<li><strong>Sparse coding</strong>: In dictionary learning, ARD can automatically determine the effective dictionary size</li>
</ul>
<p>The computational implementation of ARD involves iterating between updating the model parameters (weights) and the hyperparameters (precisions) until convergence. Modern implementations often use variational approximations or sampling methods to handle the computational challenges of the full Bayesian treatment, while maintaining the automatic model selection capabilities that make ARD so valuable in practice.</p>
<p>Modern adaptations of the evidence framework have developed sophisticated methods to handle the computational challenges of contemporary deep learning. For example, linearized Laplace approximations—such as those described by Ritter et al.&nbsp;(2018) <span class="citation" data-cites="ritter2018scalable">(<a href="references.html#ref-ritter2018scalable" role="doc-biblioref">Ritter, Botev, and Barber 2018</a>)</span> and Immer et al.&nbsp;(2021) <span class="citation" data-cites="immer2021scalable">(<a href="references.html#ref-immer2021scalable" role="doc-biblioref">Immer et al. 2021</a>)</span>—approximate the neural network through its first-order Taylor expansion around the MAP estimate, reducing the complexity of Hessian computations while maintaining reasonable approximation quality. These linearized approaches are particularly effective for networks that are sufficiently wide or when the posterior is approximately Gaussian.</p>
<p>Kronecker-structured approximations represent another significant advancement, exploiting the structure of neural network computations to factorize the Hessian matrix into more manageable components. By recognizing that gradients in neural networks can be expressed as Kronecker products of activations and error signals, these methods achieve substantial computational savings while preserving much of the information contained in the full Hessian matrix. <span class="citation" data-cites="singh2024framework">Singh, Farrell-Maupin, and Faghihi (<a href="references.html#ref-singh2024framework" role="doc-biblioref">2024</a>)</span> revisit and advance the Laplace approximation for Bayesian deep learning, addressing its scalability and effectiveness for modern neural networks. The paper introduces new algorithmic and theoretical developments that make Laplace-based Bayesian inference practical for large-scale deep learning tasks. The authors propose efficient algorithms for computing the Laplace approximation in deep neural networks, leveraging block-diagonal and Kronecker-factored structures to approximate the Hessian of the loss function, which enables uncertainty quantification in models with millions of parameters.</p>
<p>On the theoretical side, the paper provides a new analysis of the Laplace approximation in high-dimensional and overparameterized regimes typical of deep learning. Specifically, the authors derive non-asymptotic error bounds for the Laplace approximation, showing how its accuracy depends on the curvature of the loss landscape and the concentration of the posterior. They analyze the impact of model width and data size on the quality of the Gaussian approximation, and clarify under what conditions the Laplace approximation remains reliable as the number of parameters grows. This theoretical work helps explain when and why Laplace-based uncertainty estimates are trustworthy in modern neural networks, and guides the design of scalable algorithms for practical Bayesian deep learning.</p>
<p>The evidence framework also naturally handles the multiple scales of uncertainty present in neural networks. Parameter uncertainty captures the uncertainty in individual weight values given the training data, while hyperparameter uncertainty reflects uncertainty about the appropriate level of regularization or architectural choices. Model uncertainty encompasses uncertainty about the fundamental model class or architecture family. The hierarchical Bayesian treatment allows simultaneous reasoning about all these sources of uncertainty within a unified framework.</p>
<p>Despite its theoretical elegance, the evidence framework faces practical limitations in very large-scale applications. The computational requirements of Hessian approximation, even with modern efficient methods, can be substantial for networks with hundreds of millions of parameters. The Laplace approximation itself may be inadequate when the posterior is highly non-Gaussian, which can occur in networks with many local minima or complex loss landscapes.</p>
<p>The enduring value of MacKay’s evidence framework lies in its principled approach to the fundamental trade-offs in machine learning model design. By providing a theoretically grounded method for balancing model complexity against data fit, the framework offers insights that remain relevant even as the scale and sophistication of machine learning models continue to evolve. The automatic hyperparameter selection and architecture comparison capabilities of the evidence framework continue to influence contemporary approaches to neural architecture search and automated machine learning.</p>
<p>Yet another approach is the Widely applicable Bayesian Information Criterion (WBIC) proposed by <span class="citation" data-cites="watanabe2013widely">Watanabe (<a href="references.html#ref-watanabe2013widely" role="doc-biblioref">2013</a>)</span>. It is a generalization of the traditional BIC that addresses some of its fundamental limitations, particularly when dealing with singular statistical models and complex machine learning architectures. It addresses the problem when BIC’s assumptions that the true parameter lies in the interior of the parameter space and that the information matrix is positive definite are violated. These regularity conditions fail for many important models such as neural networks with hidden units, mixture models where the number of components is unknown, tree-based models with unknown structure, and models with parameter constraints or boundaries. Second, BIC requires knowing the effective number of parameters <span class="math inline">\(k\)</span>, which can be ambiguous for complex models. This becomes problematic when dealing with shared parameters across different parts of the model, regularization that effectively reduces the parameter dimension, or hierarchical structures where the effective dimensionality depends on the data. The challenge of defining the “true” number of parameters in modern machine learning models makes BIC’s penalty term difficult to specify correctly.</p>
</section>
</section>
<section id="model-selection-and-bayesian-relativity" class="level2" data-number="16.9">
<h2 data-number="16.9" class="anchored" data-anchor-id="model-selection-and-bayesian-relativity"><span class="header-section-number">16.9</span> Model Selection and Bayesian Relativity</h2>
<p>Bayes’ rule provides only relative evidence between models. Classical approaches try to find absolute truth, but Bayesian model selection is fundamentally comparative. Consider the basic relationship: <span class="math display">\[
\frac{p(H_0 \mid D)}{p(H_1 \mid D)} = \frac{p(D \mid H_0)}{p(D \mid H_1)} \frac{p(H_0)}{p(H_1)}
\]</span></p>
<p>When <span class="math inline">\(D = \{T(y) &gt; t_{obs}\}\)</span>, the numerator is the p-value. However, this needs to be assessed relative to the p-value under the alternative, which might be even more unlikely! As Sherlock Holmes noted: “When you have eliminated the impossible, whatever remains, however improbable, must be the truth.” Even though an event may be unlikely under <span class="math inline">\(H_0\)</span>, it could be the best available description given the alternatives.</p>
<p>Fisher recognized this issue: “In scientific inference, a hypothesis is never proved but merely shown to be more or less probable relative to the available alternatives.” The problem with p-values is that they attempt to be an objective measure of model “fit” without considering alternatives. Unlikely events do occur under a “true” model.</p>
<section id="exhaustive-vs-non-exhaustive-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="exhaustive-vs-non-exhaustive-hypotheses">Exhaustive vs Non-Exhaustive Hypotheses</h3>
<p>A key point is that you can always calculate the relative evidence between two hypotheses. In cases where hypotheses are exhaustive, <span class="math inline">\(p(H_0) + p(H_1) = 1\)</span>, we can directly calculate <span class="math inline">\(p(H_0 \mid D)\)</span> and we obtain the true probability given the data. In general, we have some priot probability left over for a model that is not in the set of models under consideration. But, you can still use the Bayes rule for relative evidence to obtain just a relative ordering: <span class="math display">\[
\frac{p(H_0 \mid D)}{p(H_1 \mid D)} = \frac{p(D \mid H_0)}{p(D \mid H_1)} \frac{p(H_0)}{p(H_1)}
\]</span></p>
<p>This holds for <span class="math inline">\(p(H_0) + p(H_1) &lt; 1\)</span>. An important benefit of this rational approach is that if a new hypothesis <span class="math inline">\(H_2\)</span> comes along, the relative calculation between <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> doesn’t change! This is a benefit of rational decision-making. However, posterior probabilities can change if we re-normalize to account for the new alternative.</p>
</section>
</section>
<section id="the-asymptotic-carrier" class="level2" data-number="16.10">
<h2 data-number="16.10" class="anchored" data-anchor-id="the-asymptotic-carrier"><span class="header-section-number">16.10</span> The Asymptotic Carrier</h2>
<p>What happens when the “true” model is not in the set of models under consideration? This is a critical question in modern machine learning, where model misspecification is the norm rather than the exception.</p>
<p>The asymptotic behavior of the posterior <span class="math inline">\(p(\theta \mid y)\)</span> is characterized by the <em>asymptotic carrier</em> of the posterior. Let <span class="math inline">\(F\)</span> denote the true data-generating process. The set <span class="math inline">\(\mathcal{C}\)</span> is defined by: <span class="math display">\[
\mathcal{C} = \arg\min_{\theta \in \Theta} \int f(y) \log f_\theta(y) dy = \arg\min_{\theta \in \Theta} KL(f, f_\theta)
\]</span></p>
<p>That is, the posterior over parameters <span class="math inline">\(\theta\)</span> in the model class <span class="math inline">\(\mathcal{M}\)</span> converges to the density that minimizes the Kullback-Leibler (KL) distance between the data-generating process <span class="math inline">\(f\)</span> and the model class.</p>
<p>The posterior has the limiting property that for any <span class="math inline">\(A \subset \mathcal{C}\)</span>: <span class="math display">\[
\lim_{n \to \infty} P_{\mathcal{M}}[A \mid y_1, \ldots, y_n] = 1 \text{ almost surely under } F
\]</span></p>
<p>Since Berk (1966), there has been extensive work on the limiting behavior of the posterior when the true model <span class="math inline">\(f\)</span> lies outside the class <span class="math inline">\(\{f_\theta\}\)</span> indexed by the models under consideration. This theory provides important insights:</p>
<ol type="1">
<li><p><strong>Consistency under misspecification</strong>: Even when the true model is not in our class, the posterior will concentrate on the best approximation within that class.</p></li>
<li><p><strong>KL optimality</strong>: The limiting posterior focuses on parameters that minimize the KL divergence, which is often a reasonable criterion for model approximation.</p></li>
<li><p><strong>Practical implications</strong>: This suggests that Bayesian methods can be robust to model misspecification, concentrating probability mass on the best available approximation.</p></li>
</ol>
</section>
<section id="model-explainability-1" class="level2" data-number="16.11">
<h2 data-number="16.11" class="anchored" data-anchor-id="model-explainability-1"><span class="header-section-number">16.11</span> Model Explainability</h2>
<p>In some applications, model explainability is an important criterion for model selection. For example, in finance or insurance, model explainability is important for the model to be accepted by the regulators. It is highly unlikely a black box model will ever be used for medical or criminal justice applications. While earlier in this chapter we introduced several explainability techniques in the context of fundamental considerations, here we provide a comprehensive treatment of model explainability methods, their applications, and a broader perspective on the role of explanation in scientific and practical domains.</p>
<p>Modern machine learning has developed a rich toolkit of methods to interpret and explain model predictions. These methods vary in their scope (global vs.&nbsp;local explanations), their model specificity (model-agnostic vs.&nbsp;model-specific), and their theoretical foundations.</p>
<p><em>Model-Agnostic Methods</em> provide explanations that work with any machine learning model by treating it as a black box. LIME (Local Interpretable Model-agnostic Explanations) <span class="citation" data-cites="ribeiro2016why">Ribeiro, Singh, and Guestrin (<a href="references.html#ref-ribeiro2016why" role="doc-biblioref">2016</a>)</span> creates explanations for individual predictions by training simple, interpretable models locally around the prediction of interest. The algorithm perturbs the input and observes how the model’s predictions change, then fits a linear model weighted by proximity to the original instance. SHAP (SHapley Additive exPlanations) <span class="citation" data-cites="lundberg2017unified">Lundberg and Lee (<a href="references.html#ref-lundberg2017unified" role="doc-biblioref">2017</a>)</span> provides a unified framework based on game theory, specifically Shapley values from cooperative game theory. SHAP assigns each feature an importance value for a particular prediction, satisfying desirable properties like local accuracy, missingness, and consistency. The method has become widely adopted due to its solid theoretical foundation and availability of efficient implementations for various model types.</p>
<p>Partial Dependence Plots (PDP) <span class="citation" data-cites="friedman2001greedy">Friedman (<a href="references.html#ref-friedman2001greedy" role="doc-biblioref">2001</a>)</span> and Individual Conditional Expectation (ICE) plots <span class="citation" data-cites="goldstein2015peeking">Goldstein et al. (<a href="references.html#ref-goldstein2015peeking" role="doc-biblioref">2015</a>)</span> offer visual methods to understand how features influence predictions across the entire dataset. PDPs show the marginal effect of a feature on the predicted outcome by averaging over all other features, while ICE plots show the prediction dependence for individual instances. Accumulated Local Effects (ALE) <span class="citation" data-cites="apley2020visualizing">Apley and Zhu (<a href="references.html#ref-apley2020visualizing" role="doc-biblioref">2020</a>)</span> improve upon PDPs by accounting for feature correlations, providing more reliable interpretations in high-dimensional spaces.</p>
<p><em>Gradient-Based Methods</em> leverage the internal structure of differentiable models, particularly neural networks. Saliency maps <span class="citation" data-cites="simonyan2013deep">Simonyan, Vedaldi, and Zisserman (<a href="references.html#ref-simonyan2013deep" role="doc-biblioref">2013</a>)</span> compute gradients of the output with respect to input features to identify which inputs most influence predictions. Integrated Gradients <span class="citation" data-cites="sundararajan2017axiomatic">Sundararajan, Taly, and Yan (<a href="references.html#ref-sundararajan2017axiomatic" role="doc-biblioref">2017</a>)</span> improve upon simple gradients by integrating gradients along a path from a baseline to the actual input, satisfying important axioms like sensitivity and implementation invariance. Grad-CAM (Gradient-weighted Class Activation Mapping) <span class="citation" data-cites="selvaraju2017grad">Selvaraju et al. (<a href="references.html#ref-selvaraju2017grad" role="doc-biblioref">2017</a>)</span> generates visual explanations for convolutional neural networks by computing gradients of the target class with respect to feature maps in the final convolutional layer, producing class-discriminative localization maps that highlight important regions in images.</p>
<p><em>Attention Mechanisms</em> <span class="citation" data-cites="bahdanau2014neural">Bahdanau, Cho, and Bengio (<a href="references.html#ref-bahdanau2014neural" role="doc-biblioref">2014</a>)</span> provide built-in interpretability, particularly in natural language processing and computer vision. By learning to focus on relevant parts of the input, attention weights offer direct insight into which components the model considers important for its predictions. Transformer architectures <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="references.html#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span> have made attention mechanisms ubiquitous in modern deep learning, though interpreting multi-head attention in large language models remains an active research area <span class="citation" data-cites="clark2019does">Clark et al. (<a href="references.html#ref-clark2019does" role="doc-biblioref">2019</a>)</span>.</p>
<p><em>Model-Specific Interpretability</em> includes methods designed for particular model architectures. Tree-based models like random forests and gradient boosting machines provide natural feature importance measures based on how much each feature decreases impurity or loss across splits <span class="citation" data-cites="breiman2001random">Breiman (<a href="references.html#ref-breiman2001random" role="doc-biblioref">2001</a>)</span> <span class="citation" data-cites="friedman2001greedy">Friedman (<a href="references.html#ref-friedman2001greedy" role="doc-biblioref">2001</a>)</span>. Linear models offer direct coefficient interpretation, though care must be taken with correlated features. Rule extraction methods <span class="citation" data-cites="craven1996extracting">Craven and Shavlik (<a href="references.html#ref-craven1996extracting" role="doc-biblioref">1996</a>)</span> attempt to distill complex models into human-readable rule sets.</p>
<p><em>Counterfactual Explanations</em> <span class="citation" data-cites="wachter2017counterfactual">Wachter, Mittelstadt, and Russell (<a href="references.html#ref-wachter2017counterfactual" role="doc-biblioref">2017</a>)</span> answer the question “what would need to change for the prediction to be different?” by finding the minimal modifications to input features that would alter the model’s decision. This approach is particularly valuable in applications like loan decisions, where explaining why an application was rejected is less actionable than explaining what changes would lead to approval.</p>
<p>We now provide detailed mathematical foundations for the most widely-used explainability methods, accompanied by practical demonstrations in R. These methods represent the core toolkit for model interpretation in modern machine learning practice.</p>
<section id="the-imperative-for-explainability" class="level3">
<h3 class="anchored" data-anchor-id="the-imperative-for-explainability">The Imperative for Explainability</h3>
<p>The demand for explainability stems from multiple sources, each with distinct requirements and motivations. In regulated industries, explainability is often a legal requirement. The European Union’s GDPR includes provisions for algorithmic transparency, requiring that individuals affected by automated decisions receive meaningful information about the logic involved. The Equal Credit Opportunity Act in the United States mandates that financial institutions provide specific reasons for adverse credit decisions. Healthcare applications face similar regulatory scrutiny, where the FDA increasingly requires evidence of interpretability for AI-assisted diagnostic tools.</p>
<p>Trust and adoption present another critical dimension. Medical practitioners are unlikely to rely on diagnostic systems they don’t understand, regardless of their accuracy. A radiologist examining a potential tumor needs to see which image features led to the model’s assessment, allowing them to combine algorithmic insights with their clinical expertise. Financial advisors similarly require explanations to justify investment recommendations to clients and comply with fiduciary duties. In these contexts, explainability is not merely desirable but essential for practical deployment.</p>
<p>Debugging and model improvement benefit significantly from interpretability tools. When a model fails on certain types of inputs, understanding its decision-making process helps identify the root cause. Is the model relying on spurious correlations? Has it learned dataset biases? Are certain features being misinterpreted? Explainability methods allow practitioners to diagnose these issues and refine their models accordingly. The famous case of the “wolf detector” that was actually detecting snow backgrounds <span class="citation" data-cites="ribeiro2016why">Ribeiro, Singh, and Guestrin (<a href="references.html#ref-ribeiro2016why" role="doc-biblioref">2016</a>)</span> illustrates how explanations can reveal that models learn unexpected patterns.</p>
<p>Fairness and bias detection represent another crucial application of explainability. When models exhibit disparate performance across demographic groups, understanding which features drive predictions helps identify sources of bias. If a hiring model heavily weights features correlated with protected attributes, explanations can surface these problematic dependencies, enabling corrective action through feature engineering, reweighting, or architectural changes.</p>
<p>Scientific discovery increasingly relies on machine learning models that identify patterns humans might miss. In drug discovery, models that predict molecular properties need to be interpretable to generate scientific insights about structure-activity relationships <span class="citation" data-cites="jimenez2020drugex">Jiménez-Luna et al. (<a href="references.html#ref-jimenez2020drugex" role="doc-biblioref">2020</a>)</span>. Climate models, protein folding predictions <span class="citation" data-cites="jumper2021highly">Jumper et al. (<a href="references.html#ref-jumper2021highly" role="doc-biblioref">2021</a>)</span>, and materials science applications all benefit from understanding not just what the model predicts, but why it makes those predictions, as these explanations can suggest new hypotheses and research directions.</p>
</section>
<section id="the-paradox-of-understanding-and-utility" class="level3">
<h3 class="anchored" data-anchor-id="the-paradox-of-understanding-and-utility">The Paradox of Understanding and Utility</h3>
<p>The insistence on full explanatory understanding before deploying useful tools, however, represents a historically unusual stance. Many of our most valuable scientific and medical technologies were adopted and saved lives long before we fully understood their mechanisms.</p>
<p>Consider mammography, one of the most important breast cancer screening tools developed in the 20th century. Mammograms began widespread use in the 1960s and 1970s, and large-scale trials in the 1970s and 1980s demonstrated clear mortality reduction in screened populations <span class="citation" data-cites="shapiro1988selection">Shapiro (<a href="references.html#ref-shapiro1988selection" role="doc-biblioref">1988</a>)</span> <span class="citation" data-cites="tabar1985reduction">Tabar et al. (<a href="references.html#ref-tabar1985reduction" role="doc-biblioref">1985</a>)</span>. Yet our understanding of breast cancer biology, tumor heterogeneity, and the precise mechanisms by which early detection improves outcomes continued to evolve for decades afterward. We still grapple with questions about optimal screening intervals, the balance of benefits and harms, and which tumors are truly aggressive versus indolent. The initial adoption was based on empirical evidence of benefit, not complete mechanistic understanding. The lack of complete explanation did not prevent mammography from saving countless lives.</p>
<p>Similarly, many pharmaceutical interventions were discovered through empirical observation long before their mechanisms were understood. Aspirin was used for decades before we understood its inhibition of cyclooxygenase enzymes. Lithium became a treatment for bipolar disorder in the 1940s, but its precise mechanism of action remains incompletely understood. Anesthesia was used successfully for over 150 years before we developed adequate theories of how it works. In each case, careful empirical validation preceded mechanistic understanding, and the lack of explanation did not preclude tremendous benefit.</p>
<p>Engineering provides equally striking examples. The Navier-Stokes equations, which describe fluid flow and form the foundation of aerodynamics, weather prediction, and countless other applications, were formulated in the 19th century. We use numerical solutions to these equations daily for critical applications: designing aircraft, predicting hurricanes, optimizing turbine efficiency, and modeling blood flow in cardiovascular research. Yet one of the Clay Mathematics Institute’s Millennium Prize Problems, offering a million-dollar prize, asks whether solutions to the Navier-Stokes equations even exist and are unique in three dimensions <span class="citation" data-cites="fefferman2006existence">Fefferman (<a href="references.html#ref-fefferman2006existence" role="doc-biblioref">2006</a>)</span>. We have built an entire technological civilization on equations whose mathematical foundations remain unproven. Engineers successfully use these equations not through complete mathematical understanding but through empirical validation, computational approximation, and careful attention to when the models work well and when they fail.</p>
<p>This historical pattern suggests a more nuanced view of explainability in machine learning. The demand that we fully understand and explain every prediction from a neural network before deploying it represents a higher standard than we have applied to many successful technologies. This is not to argue against explainability research—it is valuable and important. Rather, it suggests that we should focus on:</p>
<p><em>Empirical validation</em> through rigorous testing, cross-validation, and real-world performance monitoring. A model that consistently makes accurate predictions on held-out test sets and in production may be deployable even if we cannot fully explain every decision.</p>
<p><em>Understanding failure modes</em> rather than complete mechanistic understanding. Knowing when and where a model is likely to fail, what types of inputs it handles poorly, and how confident it is in its predictions may be more actionable than detailed explanations of every prediction.</p>
<p><em>Complementary human oversight</em> in high-stakes decisions. Rather than requiring complete explainability, we might deploy powerful but less interpretable models in systems where humans remain in the loop, using model predictions as one input among many.</p>
<p><em>Appropriate deployment contexts</em> that match interpretability requirements to stakes and alternatives. A model that routes customer service calls can reasonably be less interpretable than one making parole decisions. The comparison should be to existing alternatives: is the model more or less fair, accurate, and interpretable than current practice?</p>
<p>The goal is not to abandon explainability but to maintain perspective. History suggests that empirically validated tools often precede complete understanding, and this gap need not prevent their careful deployment. As we develop more sophisticated explainability methods, we should simultaneously develop more sophisticated frameworks for when and why explanation is necessary, recognizing that the relationship between understanding and utility is subtle and context-dependent.</p>
<p>This balanced view—pursuing explainability while acknowledging the historical precedent for useful tools that outpace understanding—may lead to more productive deployment of machine learning in domains where it can provide real benefit, accompanied by the monitoring, validation, and human oversight appropriate to the stakes involved.</p>
</section>
</section>
<section id="model-elaboration-and-nested-model-testing" class="level2" data-number="16.12">
<h2 data-number="16.12" class="anchored" data-anchor-id="model-elaboration-and-nested-model-testing"><span class="header-section-number">16.12</span> Model Elaboration and Nested Model Testing</h2>
<p>An <em>elaborated model</em> in Bayesian statistics refers to a model that extends or generalizes a simpler, baseline (or “underlying”) model by introducing additional parameters or structure. The purpose of elaboration is to capture more complex features of the data, account for possible deviations from the assumptions of the simpler model, or to allow for greater flexibility in modeling.</p>
<p>Formally, suppose we start with a baseline model <span class="math inline">\(f(y \mid \theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a parameter of interest. An elaborated model introduces an additional parameter (or set of parameters) <span class="math inline">\(\lambda\)</span>, resulting in a family of models <span class="math inline">\(f(y \mid \theta, \lambda)\)</span> indexed by <span class="math inline">\(\lambda \in \Lambda\)</span>. The original model is recovered as a special case for some fixed value <span class="math inline">\(\lambda_0\)</span> (i.e., <span class="math inline">\(f(y \mid \theta) = f(y \mid \theta, \lambda_0)\)</span>). The set <span class="math inline">\(\Lambda\)</span> describes the ways in which the model can be elaborated.</p>
<p>When we use elaborated models then we need to compare nested models (where the simpler model is a special case of the more complex one). In Bayesian analysis, inference in an elaborated model involves integrating over the additional parameters, reflecting uncertainty about both the original and the elaborating parameters.</p>
<p>Applying the usual Bayesian paradigm (disciplined probability accounting) to the elaborated framework, we see that inference about <span class="math inline">\(\theta\)</span> is determined by <span class="math display">\[
p(\theta \mid y) = \int_\Lambda p(\theta \mid \lambda, y) p(\lambda \mid y) d\lambda
\]</span> where <span class="math display">\[
\begin{aligned}
p(\theta \mid \lambda, y) &amp;\propto p(y \mid \theta, \lambda) p(\theta \mid \lambda) \\
p(\lambda \mid y) &amp;\propto p(y \mid \lambda) p(\lambda) \\
\text{where } p(y \mid \lambda) &amp;= \int p(y \mid \theta, \lambda) p(\theta \mid \lambda) d\theta
\end{aligned}
\]</span></p>
<p>For consistency with the elaborated and underlying model, we take <span class="math inline">\(p(\theta \mid \lambda_0) = p(\theta)\)</span>. Since <span class="math inline">\(\lambda\)</span> labels the form of departure from the initial model <span class="math inline">\(M_0: \lambda = \lambda_0\)</span>, the form of <span class="math inline">\(p(\lambda)\)</span> should be chosen to reflect this departure.</p>
<p>A classical example is the exponential power elaboration of the traditional normal family, allowing for robustness. Here <span class="math inline">\(\lambda \in (0,3)\)</span> indexes the power and <span class="math inline">\(\lambda_0 = 2\)</span> is the Normal case.</p>
<p>The posterior mean is simply a weighted average with respect to <span class="math inline">\(p(\lambda \mid y)\)</span>: <span class="math display">\[
\E{\theta \mid y} = \int \E{\theta \mid \lambda, y} p(\lambda \mid y) d\lambda = \E[\lambda \mid y]{\E{\theta \mid \lambda, y}}
\]</span></p>
<section id="the-dickey-savage-approach-to-nested-models" class="level3">
<h3 class="anchored" data-anchor-id="the-dickey-savage-approach-to-nested-models">The Dickey-Savage Approach to Nested Models</h3>
<p>The Dickey-Savage approach provides a principled Bayesian method for testing nested models—situations where a simpler model is a special case of a more complex one. This approach is particularly useful when we want to assess whether the data support the inclusion of additional parameters or structure in our model, or whether the simpler, baseline model suffices.</p>
<p>In the context of Bayesian hypothesis testing, the Dickey-Savage method allows us to compute the Bayes factor for comparing a nested (elaborated) model to its simpler counterpart using only the posterior and prior distributions of the parameter(s) that distinguish the two models. This not only streamlines the computation but also clarifies the relationship between the models and the evidence provided by the data.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The Bayes Folklore
</div>
</div>
<div class="callout-body-container callout-body">
<p>In essence, the Dickey-Savage result supports the Bayesian strategy of fitting the most comprehensive model possible—sometimes referred to as the ‘elephant’ model—and testing simpler sub-models as special cases. You fit a model <span class="math inline">\(M\)</span> as complex as allowed by your computational budget and statistical skills. The calculations for any nested model, e.g., <span class="math inline">\(M_0\)</span>, can then be performed entirely within the framework of <span class="math inline">\(M\)</span>.</p>
</div>
</div>
<p>Suppose you are conducting a hypothesis test comparing two models, <span class="math inline">\(M_0\)</span> and <span class="math inline">\(M_1\)</span>. The null hypothesis <span class="math inline">\(H_0\)</span> is that the simpler model <span class="math inline">\(M_0\)</span> is true, and the alternative hypothesis <span class="math inline">\(H_1\)</span> is that the more complex model <span class="math inline">\(M_1\)</span> is true.</p>
<p>Let’s explore how this approach works and why it is both elegant and practical for model comparison in Bayesian analysis.</p>
<p>Suppose that <span class="math inline">\(M_0 \subset M\)</span>. Let <span class="math inline">\((\theta, \psi)\)</span> have matching priors such that <span class="math display">\[
p(\psi \mid \theta = 0, M) = p(\psi \mid M_0)
\]</span> where <span class="math inline">\(\theta = 0\)</span> corresponds to <span class="math inline">\(M_0\)</span>. That is, <span class="math inline">\(p(y \mid \theta = 0, \psi, M) = p(y \mid \psi, M_0)\)</span>.</p>
<p>Then, we can calculate <em>solely under model</em> <span class="math inline">\(M\)</span> the Bayes factor as follows: <span class="math display">\[
BF = \frac{p(\theta = 0 \mid y, M)}{p(\theta = 0 \mid M)} = \frac{p(y \mid M_0)}{p(y \mid M)}
\]</span></p>
<p>This is a ratio of posterior ordinates, valid as long as models are nested. By definition of marginals: <span class="math display">\[
\begin{aligned}
p(y \mid \theta = 0, M) &amp;= \int p(y \mid \theta = 0, \psi, M) p(\psi \mid \theta = 0, M) d\psi \\
&amp;= \int p(y \mid \psi, M_0) p(\psi \mid M_0) d\psi \\
&amp;= p(y \mid M_0)
\end{aligned}
\]</span></p>
<p>This elegant result shows that Bayes factors for nested models can be computed entirely within the larger model framework.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="16.13">
<h2 data-number="16.13" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">16.13</span> Conclusion</h2>
<p>Model selection lies at the heart of the machine learning workflow. Throughout this chapter, we have explored the fundamental tensions that define this task: the bias-variance trade-off, the pursuit of out-of-sample generalization, and the balance between predictive power and interpretability.</p>
<p>Several key principles emerge. First, in-sample performance is a poor guide to model quality; rigorous out-of-sample evaluation via cross-validation or held-out test sets is essential. Second, the Bayesian framework provides a principled, probabilistic approach to model comparison through the <em>evidence</em> or marginal likelihood, which naturally implements Occam’s razor by penalizing unnecessary complexity. Information criteria like BIC offer practical approximations to this ideal. Third, there is no single “best” model for all purposes—a model optimized for prediction may be entirely unsuitable for causal interpretation, and vice versa.</p>
<p>The chapter also engaged with the critical role of explainability. In high-stakes domains, understanding <em>why</em> a model makes its predictions is as important as the predictions themselves. We surveyed the modern toolkit of interpretability methods—SHAP, LIME, PDPs, and others—while cautioning against an unrealistic demand for complete mechanistic understanding before deployment. History teaches us that useful tools often precede full explanation.</p>
<p>Ultimately, model selection is an exercise in informed judgment!</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-apley2020visualizing" class="csl-entry" role="listitem">
Apley, Daniel W., and Jingyu Zhu. 2020. <span>“Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 82 (4): 1059–86.
</div>
<div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural <span>Machine Translation</span> by <span>Jointly Learning</span> to <span>Align</span> and <span>Translate</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-brier1950verification" class="csl-entry" role="listitem">
Brier, Glenn W. 1950. <span>“Verification of Forecasts Expressed in Terms of Probability.”</span> <em>Monthly Weather Review</em> 78 (1): 1–3.
</div>
<div id="ref-clark2019does" class="csl-entry" role="listitem">
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. <span>“What Does <span>BERT</span> Look at? <span>An</span> Analysis of <span>BERT</span>’s Attention.”</span> In <em>Proceedings of the 2019 <span>ACL Workshop BlackboxNLP</span>: <span>Analyzing</span> and <span>Interpreting Neural Networks</span> for <span>NLP</span></em>, 276–86. Association for Computational Linguistics.
</div>
<div id="ref-craven1996extracting" class="csl-entry" role="listitem">
Craven, Mark, and Jude W. Shavlik. 1996. <span>“Extracting Tree-Structured Representations of Trained Networks.”</span> In <em>Advances in Neural Information Processing Systems</em>, 8:24–30. MIT Press.
</div>
<div id="ref-fawcett2006roc" class="csl-entry" role="listitem">
Fawcett, Tom. 2006. <span>“An Introduction to <span>ROC</span> Analysis.”</span> <em>Pattern Recognition Letters</em> 27 (8): 861–74.
</div>
<div id="ref-fefferman2006existence" class="csl-entry" role="listitem">
Fefferman, Charles L. 2006. <span>“Existence and Smoothness of the <span>Navier</span>–<span>Stokes</span> Equation.”</span> <em>The Millennium Prize Problems</em>, 57–67.
</div>
<div id="ref-friedman2001greedy" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
<div id="ref-gelman2013bda" class="csl-entry" role="listitem">
Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian <span>Data Analysis</span></em>. 3rd ed. Boca Raton: <span>Chapman and Hall/CRC</span>.
</div>
<div id="ref-gneiting2007strictly" class="csl-entry" role="listitem">
Gneiting, Tilmann, and Adrian E Raftery. 2007. <span>“Strictly <span>Proper Scoring Rules</span>, <span>Prediction</span>, and <span>Estimation</span>.”</span> <em>Journal of the American Statistical Association</em> 102 (477): 359–78.
</div>
<div id="ref-goldstein2015peeking" class="csl-entry" role="listitem">
Goldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. <span>“Peeking Inside the Black Box: <span>Visualizing</span> Statistical Learning with Plots of Individual Conditional Expectation.”</span> <em>Journal of Computational and Graphical Statistics</em> 24 (1): 44–65.
</div>
<div id="ref-immer2021scalable" class="csl-entry" role="listitem">
Immer, Alexander, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Khan Mohammad Emtiyaz. 2021. <span>“Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 4563–73. PMLR.
</div>
<div id="ref-jimenez2020drugex" class="csl-entry" role="listitem">
Jiménez-Luna, José, Francesca Grisoni, Nils Weskamp, and Gisbert Schneider. 2020. <span>“<span>DrugEx</span> V2: <span>De</span> Novo Design of Drug Molecule by <span class="nocase">Pareto-based</span> Multi-Objective Reinforcement Learning in Polypharmacology.”</span> <em>Journal of Cheminformatics</em> 12 (1): 1–12.
</div>
<div id="ref-jumper2021highly" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“Highly Accurate Protein Structure Prediction with <span>AlphaFold</span>.”</span> <em>Nature</em> 596 (7873): 583–89.
</div>
<div id="ref-lindley1961use" class="csl-entry" role="listitem">
Lindley, D. V. 1961. <span>“The <span>Use</span> of <span>Prior Probability Distributions</span> in <span>Statistical Inference</span> and <span>Decisions</span>.”</span> In <em>Proceedings of the <span>Fourth Berkeley Symposium</span> on <span>Mathematical Statistics</span> and <span>Probability</span>, <span>Volume</span> 1: <span>Contributions</span> to the <span>Theory</span> of <span>Statistics</span></em>, 4.1:453–69. University of California Press.
</div>
<div id="ref-lundberg2017unified" class="csl-entry" role="listitem">
Lundberg, Scott M, and Su-In Lee. 2017. <span>“A <span>Unified Approach</span> to <span>Interpreting Model Predictions</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span> 30</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4765–74. Curran Associates, Inc.
</div>
<div id="ref-mackay1992bayesian" class="csl-entry" role="listitem">
MacKay, David JC. 1992. <span>“Bayesian Interpolation.”</span> <em>Neural Computation</em> 4 (3): 415–47.
</div>
<div id="ref-ribeiro2016why" class="csl-entry" role="listitem">
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. <span>“"<span>Why</span> Should <span>I</span> Trust You?": <span>Explaining</span> the Predictions of Any Classifier.”</span> In <em>Proceedings of the 22nd <span>ACM SIGKDD</span> International Conference on Knowledge Discovery and Data Mining</em>, 1135–44. ACM.
</div>
<div id="ref-ritter2018scalable" class="csl-entry" role="listitem">
Ritter, Hippolyt, Aleksandar Botev, and David Barber. 2018. <span>“A <span>Scalable Laplace Approximation For Neural Networks</span>.”</span>
</div>
<div id="ref-selvaraju2017grad" class="csl-entry" role="listitem">
Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. <span>“Grad-<span>CAM</span>: <span>Visual</span> Explanations from Deep Networks via Gradient-Based Localization.”</span> In <em>Proceedings of the <span>IEEE</span> International Conference on Computer Vision</em>, 618–26. IEEE.
</div>
<div id="ref-shapiro1988selection" class="csl-entry" role="listitem">
Shapiro, Sam. 1988. <span>“Selection, Follow-up, and Analysis in the <span>Health Insurance Plan Study</span>: <span>A</span> Randomized Trial with Breast Cancer Screening.”</span> <em>Journal of the National Cancer Institute</em> 80 (14): 1125–32.
</div>
<div id="ref-simonyan2013deep" class="csl-entry" role="listitem">
Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. 2013. <span>“Deep Inside Convolutional Networks: <span>Visualising</span> Image Classification Models and Saliency Maps.”</span> <em>arXiv Preprint arXiv:1312.6034</em>. <a href="https://arxiv.org/abs/1312.6034">https://arxiv.org/abs/1312.6034</a>.
</div>
<div id="ref-singh2024framework" class="csl-entry" role="listitem">
Singh, Pratyush Kumar, Kathryn A. Farrell-Maupin, and Danial Faghihi. 2024. <span>“A <span>Framework</span> for <span>Strategic Discovery</span> of <span>Credible Neural Network Surrogate Models</span> Under <span>Uncertainty</span>.”</span> arXiv. <a href="https://arxiv.org/abs/2403.08901">https://arxiv.org/abs/2403.08901</a>.
</div>
<div id="ref-sundararajan2017axiomatic" class="csl-entry" role="listitem">
Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. <span>“Axiomatic Attribution for Deep Networks.”</span> In <em>Proceedings of the 34th International Conference on Machine Learning</em>, 3319–28. PMLR.
</div>
<div id="ref-tabar1985reduction" class="csl-entry" role="listitem">
Tabar, Laszlo, CJ Gad Fagerberg, Anders Gad, Lennart Baldetorp, Lars H Holmberg, Ove Gröntoft, Ulf Ljungquist, et al. 1985. <span>“Reduction in Mortality from Breast Cancer After Mass Screening with Mammography: <span>Randomised</span> Trial from the <span>Breast Cancer Screening Working Group</span> of the <span>Swedish National Board</span> of <span>Health</span> and <span>Welfare</span>.”</span> <em>The Lancet</em> 325 (8433): 829–32.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30: 5998–6008.
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>.”</span> <em>Harvard Journal of Law &amp; Technology</em> 31: 841–87.
</div>
<div id="ref-watanabe2013widely" class="csl-entry" role="listitem">
Watanabe, Sumio. 2013. <span>“A Widely Applicable <span>Bayesian</span> Information Criterion.”</span> <em>The Journal of Machine Learning Research</em> 14 (1): 867–97.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./15-forecasting.html" class="pagination-link" aria-label="Forecasting">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./17-theoryai.html" class="pagination-link" aria-label="Statistical Learning Theory and Regularization">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>