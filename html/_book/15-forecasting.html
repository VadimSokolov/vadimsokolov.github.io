<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Forecasting – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./16-rct.html" rel="next">
<link href="./14-tree.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-680e7c22d93ef26f016bec9199f8e6d8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="15&nbsp; Forecasting – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="15-forecasting_files/figure-html/fig-randomwalk-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="15&nbsp; Forecasting – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="15-forecasting_files/figure-html/fig-randomwalk-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./15-forecasting.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Principles of Data Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AB Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#structural-time-series-models" id="toc-structural-time-series-models" class="nav-link active" data-scroll-target="#structural-time-series-models"><span class="header-section-number">15.1</span> Structural time series models</a></li>
  <li><a href="#regression-with-spike-and-slab-priors" id="toc-regression-with-spike-and-slab-priors" class="nav-link" data-scroll-target="#regression-with-spike-and-slab-priors"><span class="header-section-number">15.2</span> Regression with spike and slab priors</a></li>
  <li><a href="#model-diagnostics-did-the-google-data-help" id="toc-model-diagnostics-did-the-google-data-help" class="nav-link" data-scroll-target="#model-diagnostics-did-the-google-data-help"><span class="header-section-number">15.3</span> Model diagnostics: did the Google data help?</a></li>
  <li><a href="#final-remarks-on-structural-models" id="toc-final-remarks-on-structural-models" class="nav-link" data-scroll-target="#final-remarks-on-structural-models"><span class="header-section-number">15.4</span> Final Remarks on Structural Models</a></li>
  <li><a href="#algorithms" id="toc-algorithms" class="nav-link" data-scroll-target="#algorithms"><span class="header-section-number">15.5</span> Algorithms</a>
  <ul class="collapse">
  <li><a href="#kalman-filtering" id="toc-kalman-filtering" class="nav-link" data-scroll-target="#kalman-filtering">Kalman Filtering</a></li>
  <li><a href="#hmm-hidden-markov-models" id="toc-hmm-hidden-markov-models" class="nav-link" data-scroll-target="#hmm-hidden-markov-models">HMM: Hidden Markov Models</a></li>
  <li><a href="#mixture-kalman-filter" id="toc-mixture-kalman-filter" class="nav-link" data-scroll-target="#mixture-kalman-filter">Mixture Kalman filter</a></li>
  <li><a href="#regime-switching-models" id="toc-regime-switching-models" class="nav-link" data-scroll-target="#regime-switching-models">Regime Switching Models</a></li>
  </ul></li>
  <li><a href="#particle-learning-for-general-mixture-models" id="toc-particle-learning-for-general-mixture-models" class="nav-link" data-scroll-target="#particle-learning-for-general-mixture-models"><span class="header-section-number">15.6</span> Particle Learning for General Mixture Models</a>
  <ul class="collapse">
  <li><a href="#the-particle-learning-algorithm" id="toc-the-particle-learning-algorithm" class="nav-link" data-scroll-target="#the-particle-learning-algorithm">The Particle Learning Algorithm</a></li>
  </ul></li>
  <li><a href="#modern-era-forecasting" id="toc-modern-era-forecasting" class="nav-link" data-scroll-target="#modern-era-forecasting"><span class="header-section-number">15.7</span> Modern Era Forecasting</a></li>
  <li><a href="#quantile-regression-forests." id="toc-quantile-regression-forests." class="nav-link" data-scroll-target="#quantile-regression-forests."><span class="header-section-number">15.8</span> Quantile Regression Forests.</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-data.html">AI</a></li><li class="breadcrumb-item"><a href="./15-forecasting.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data science. This chapter describes the <code>bsts</code> software package, which makes it easy to fit some fairly sophisticated time series models with just a few lines of R code.</p>
<p>The BSTS (Bayesian Structural Time Series) section of this chapter is adapted from the excellent blog post by Steven Scott: <a href="https://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html">“Fitting Bayesian structural time series with the bsts R package”</a>. We thank Steven for making this material available and for his contributions to the time series community.</p>
<p>Time series data are encountered across a wide range of fields, including business, the sciences, healthcare, and engineering. While forecasting future values (such as predicting next month’s sales) is a common application, time series analysis is also used to understand the factors that influence these values. Recently, time series forecasting has gained increased attention in the technology sector, with tools like Facebook’s “Prophet” (<span class="citation" data-cites="seanj.taylor2017prophet">Sean J. Taylor and Ben Letham (<a href="references.html#ref-seanj.taylor2017prophet" role="doc-biblioref">2017</a>)</span>) and Google’s forecasting system (<span class="citation" data-cites="erictassone2017our">Eric Tassone and Farzan Rohani (<a href="references.html#ref-erictassone2017our" role="doc-biblioref">2017</a>)</span>) being highlighted in various tech blogs.</p>
<p>In this chapter, we introduce the <code>bsts</code> package for R, which provides a framework for fitting Bayesian structural time series models. These models, also referred to as “state space models,” “structural time series,” “Kalman filter models,” or “dynamic linear models,” are highly flexible and can be approached from both Bayesian and non-Bayesian perspectives. The <code>bsts</code> package, however, is specifically designed to leverage Bayesian posterior sampling.</p>
<p>As an open-source package available on CRAN (installable via <code>install.packages("bsts")</code>), <code>bsts</code> shares some similarities with the forecasting systems developed by Facebook and Google, but it was created with different objectives. Whereas the Facebook and Google tools are optimized for large-scale, automated forecasting—particularly for daily data and across many time series—<code>bsts</code> is intended to be more customizable. The focus of those large-scale systems is on automating analysis to handle many series efficiently, often by using techniques like regularized regression (Facebook) or ensemble averaging (Google), and by carefully accounting for holidays.</p>
<p>While <code>bsts</code> can be set up for automated forecasting and is even used as part of Google’s ensemble approach, it is also well-suited for analysts who want to tailor models to specific needs. This includes decisions about forecasting horizons, handling of seasonality, and whether the primary goal is prediction or explanation.</p>
<p>The foundation of <code>bsts</code> is the structural time series model, which will be outlined in the next section. The chapter then proceeds with several detailed examples that showcase advanced features of the package. The first example, <strong>Nowcasting</strong>, covers local linear trend and seasonal models, as well as spike-and-slab priors for high-dimensional regression. The second example, <strong>Long-term forecasting</strong>, addresses cases where standard local level or trend models are insufficient and introduces the semilocal linear trend model. The third example, <strong>Recession modeling</strong>, demonstrates how to handle non-Gaussian response variables and focuses on controlling for serial dependence in explanatory modeling. The chapter concludes with a brief overview of additional features in <code>bsts</code> that are not covered in depth.</p>
<section id="structural-time-series-models" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="structural-time-series-models"><span class="header-section-number">15.1</span> Structural time series models</h2>
<p>A structural time series model is defined by two equations. The observation equation relates the observed data <span class="math inline">\(y_t\)</span> to a vector of latent variables <span class="math inline">\(\alpha_t\)</span> known as the “state.” <span class="math display">\[
y_t = Z_t^T\alpha_t + \epsilon_t.
\]</span></p>
<p>The transition equation describes how the latent state evolves through time. <span class="math display">\[
\alpha_{t+1} = T_t \alpha_t + R_t \eta_t.
\]</span></p>
<p>The error terms <span class="math inline">\(\epsilon_t\)</span> and <span class="math inline">\(\eta_t\)</span> are Gaussian and independent of everything else. The arrays <span class="math inline">\(Z_t\)</span>, <span class="math inline">\(T_t\)</span> and <span class="math inline">\(R_t\)</span> are structural parameters. They may contain parameters in the statistical sense, but often they simply contain strategically placed 0’s and 1’s indicating which bits of <span class="math inline">\(\alpha_t\)</span> are relevant for a particular computation. An example will hopefully make things clearer.</p>
<p>The simplest useful model is the “local level model,” in which the vector <span class="math inline">\(\alpha_t\)</span> is just a scalar <span class="math inline">\(\mu_t\)</span>. The local level model is a random walk observed in noise. <span class="math display">\[\begin{align*}
y_t = &amp;\mu_t + \epsilon_t\\
\mu_{t+1} = &amp;\mu_t + \eta_t.
\end{align*}\]</span> Here <span class="math inline">\(\alpha_t=\mu_t\)</span>, and <span class="math inline">\(Z_t\)</span>, <span class="math inline">\(T_t\)</span>, and <span class="math inline">\(R_t\)</span> all collapse to the scalar value 1. Similar to Bayesian hierarchical models for nested data, the local level model is a compromise between two extremes. The compromise is determined by variances of <span class="math inline">\(\epsilon_t \sim N(0,\sigma^2)\)</span> and <span class="math inline">\(\eta_t \sim N(0,\tau^2)\)</span>. If <span class="math inline">\(\tau^2=0\)</span> then <span class="math inline">\(\mu_t\)</span> is a constant, so the data are IID Gaussian noise. In that case the best estimator of <span class="math inline">\(y_{t+1}\)</span> is the mean of <span class="math inline">\(y_1,\ldots,y_t\)</span>. Conversely, if <span class="math inline">\(\sigma^2=0\)</span> then the data follow a random walk, in which case the best estimator of <span class="math inline">\(y_{t+1}\)</span> is <span class="math inline">\(y_t\)</span>. Notice that in one case the estimator depends on all past data (weighted equally) while in the other it depends only on the most recent data point, giving past data zero weight. If both variances are positive then the optimal estimator of <span class="math inline">\(y_{t+1}\)</span> winds up being “exponential smoothing,” where past data are forgotten at an exponential rate determined by the ratio of the two variances. Also notice that while the state in this model is Markov (i.e.&nbsp;it only depends on the previous state), the dependence among the observed data extends to the beginning of the series.</p>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="2" data-null_prefix="true">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-randomwalk-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-randomwalk-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-randomwalk-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-randomwalk-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Apple Adjusted Closing Price
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-randomwalk-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-randomwalk-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-randomwalk-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-randomwalk-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Apple Adjusted Closing Price
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>In the example above, one of the plots shows the price of Apple stock from 2021-01-01 to 2022-12-31. The other plot is a sequence generated from a random walk model fitted to the Apple price data. Can you spot which one is which?</p>
<p>Structural time series models are useful because they are flexible and modular. The analyst chooses the structure of <span class="math inline">\(\alpha_t\)</span> based on things like whether short- or long-term predictions are more important, whether the data contains seasonal effects, and whether and how regressors are to be included. Many of these models are standard, and can be fit using a variety of tools, such as the <code>StructTS</code> function distributed with base R or one of several R packages for fitting these models (with the <code>dlm</code> package (<span class="citation" data-cites="petris2010package">Petris (<a href="references.html#ref-petris2010package" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="campagnoli2009dynamic">Campagnoli, Petrone, and Petris (<a href="references.html#ref-campagnoli2009dynamic" role="doc-biblioref">2009</a>)</span>) deserving special mention). The <code>bsts</code> package handles all the standard cases, but it also includes several useful extensions, described in the next few sections through a series of examples. Each example includes a mathematical description of the model and example <code>bsts</code> code showing how to work with the model using the <code>bsts</code> software. To keep things short, details about prior assumptions are largely avoided.</p>
<div id="exm-Nowcasting" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.1 (Nowcasting)</strong></span> <span class="citation" data-cites="scott2014predicting">S. Scott and Varian (<a href="references.html#ref-scott2014predicting" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="scott2015bayesian">Steven L. Scott and Varian (<a href="references.html#ref-scott2015bayesian" role="doc-biblioref">2015</a>)</span> used structural time series models to show how Google search data can be used to improve short-term forecasts (“nowcasts”) of economic time series. The figure below shows the motivating data set from <span class="citation" data-cites="scott2014predicting">S. Scott and Varian (<a href="references.html#ref-scott2014predicting" role="doc-biblioref">2014</a>)</span>, which is also included with the <code>bsts</code> package. The data consist of the weekly initial claims for unemployment insurance in the US, as reported by the US Federal Reserve. Like many official statistics, they are released with delay and subject to revision. At the end of the week, the economic activity determining these numbers has taken place, but the official numbers are not published until several days later. For economic decisions based on these and similar numbers, it would help to have an early forecast of the current week’s number as of the close of the week. Thus the output of this analysis is truly a “nowcast” of data that has already happened rather than a “forecast” of data that will happen in the future.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bsts)     <span class="co"># load the bsts package</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iclaims)     <span class="co"># bring the initial.claims data into scope</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(initial.claims<span class="sc">$</span>iclaimsNSA, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">ylab=</span><span class="st">"Unemployment claims (thousand)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-bsts" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bsts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-bsts-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bsts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: Weekly initial claims for unemployment in the US.
</figcaption>
</figure>
</div>
</div>
</div>
<p>There are two sources of information about the current value <span class="math inline">\(y_t\)</span> in the initial claims series: past values <span class="math inline">\(y_{t-\tau}\)</span> describing the time series behavior of the series, and contemporaneous predictors <span class="math inline">\(x_t\)</span> from a data source which is correlated with <span class="math inline">\(y_t\)</span>, but which is available without the delay exhibited by <span class="math inline">\(y_t\)</span>. The time series structure shows an obvious trend (in which the financial and housing crises in 2008 - 2009 are apparent) as well as a strong annual seasonal pattern. The external data source explored by Scott and Varian was search data from Google trends with search queries such as “how to file for unemployment” having obvious relevance.</p>
<p>Scott and Varian modeled the data using a structural time series with three state components:</p>
<ul>
<li>trend <span class="math inline">\(\mu_t\)</span></li>
<li>seasonal pattern <span class="math inline">\(\tau_t\)</span><br>
</li>
<li>regression component <span class="math inline">\(\beta^Tx_t\)</span>.</li>
</ul>
<p>The model is <span class="math display">\[\begin{align*}
y_t = &amp; \mu_t + \tau_t + \beta^T x_t + \epsilon_t\\
\mu_{t+1} = &amp;\mu_t + \delta_t + \eta_{0t}\\
\delta_{t+1} = &amp;\delta_t + \eta_{1t}\\
\tau_{t+1} = &amp;-\sum_{s = 1}^{S-1}\tau_{t} + \eta_{2t}.
\end{align*}\]</span></p>
<p>The trend component looks similar to the local level model above, but it has an extra term <span class="math inline">\(\delta_t\)</span>. Notice that <span class="math inline">\(\delta_t\)</span> is the amount of extra <span class="math inline">\(\mu\)</span> you can expect as <span class="math inline">\(t\rightarrow t+1\)</span>, so it can be interpreted as the slope of the local linear trend. Slopes normally multiply some <span class="math inline">\(x\)</span> variable, but in this case <span class="math inline">\(x=\Delta t\)</span>, which is omitted from the equation because it is always 1. The slope evolves according to a random walk, which makes the trend an integrated random walk with an extra drift term. The local linear trend is a better model than the local level model if you think the time series is trending in a particular direction and you want future forecasts to reflect a continued increase (or decrease) seen in recent observations. Whereas the local level model bases forecasts around the average value of recent observations, the local linear trend model adds in recent upward or downward slopes as well. As with most statistical models, the extra flexibility comes at the price of extra volatility.</p>
<p>The best way to understand the seasonal component <span class="math inline">\(\tau_t\)</span> is in terms of a regression with seasonal dummy variables. Suppose you had quarterly data, so that <span class="math inline">\(S=4\)</span>. You might include the annual seasonal cycle using 3 dummy variables, with one left out as a baseline. Alternatively, you could include all four dummy variables but constrain their coefficients to sum to zero. The seasonal state model takes the latter approach, but the constraint is that the <span class="math inline">\(S\)</span> most recent seasonal effects must sum to zero in expectation. This allows the seasonal pattern to slowly evolve. Scott and Varian described the annual cycle in the weekly initial claims data using a seasonal state component with <span class="math inline">\(S=52\)</span>. Of course weeks don’t neatly divide years, but given the small number of years for which Google data are available the occasional one-period seasonal discontinuity was deemed unimportant.</p>
<p>Let’s ignore the regression component for now and fit a <code>bsts</code> model with just the trend and seasonal components.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ss <span class="ot">&lt;-</span> <span class="fu">AddLocalLinearTrend</span>(<span class="fu">list</span>(), initial.claims<span class="sc">$</span>iclaimsNSA)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>ss <span class="ot">&lt;-</span> <span class="fu">AddSeasonal</span>(ss, initial.claims<span class="sc">$</span>iclaimsNSA, <span class="at">nseasons =</span> <span class="dv">52</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">bsts</span>(initial.claims<span class="sc">$</span>iclaimsNSA,<span class="at">state.specification =</span> ss,<span class="at">niter =</span> <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first thing to do when fitting a <code>bsts</code> model is to specify the contents of the latent state vector <span class="math inline">\(\alpha_t\)</span>. The <code>bsts</code> package offers a library of state models, which are included by adding them to a state specification (which is just a list with a particular format). The call to <code>AddLocalLinearTrend</code> above adds a local linear trend state component to an empty state specification (the <code>list()</code> in its first argument). The call to <code>AddSeasonal</code> adds a seasonal state component with 52 seasons to the state specification created on the previous line. The state vector <span class="math inline">\(\alpha_t\)</span> is formed by concatenating the state from each state model. Similarly, the vector <span class="math inline">\(Z_t\)</span> is formed by concatenating the <span class="math inline">\(Z\)</span> vectors from the two state models, while the matrices <span class="math inline">\(T_t\)</span> and <span class="math inline">\(R_t\)</span> are combined in block-diagonal fashion.</p>
<p>The state specification is passed as an argument to <code>bsts</code>, along with the data and the desired number of MCMC iterations. The model is fit using an MCMC algorithm, which in this example takes about 20 seconds to produce 1000 MCMC iterations. The returned object is a list (with class attribute <code>bsts</code>). You can see its contents by typing</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(model1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> "sigma.obs"                  "sigma.trend.level"         
 "sigma.trend.slope"          "sigma.seasonal.52"         
 "final.state"                "state.contributions"       
 "one.step.prediction.errors" "log.likelihood"            
 "has.regression"             "state.specification"       
 "prior"                      "timestamp.info"            
 "model.options"              "family"                    
 "niter"                      "original.series"           </code></pre>
</div>
</div>
<p>The first few elements contain the MCMC draws of the model parameters. Most of the other elements are data structures needed by various S3 methods (<code>plot, print, predict</code>, etc.) that can be used with the returned object. MCMC output is stored in vectors (for scalar parameters) or arrays (for vector or matrix parameters) where the first index in the array corresponds to MCMC iteration number, and the remaining indices correspond to dimension of the deviate being drawn.</p>
<p>Most users won’t need to look inside the returned <code>bsts</code> object because standard tasks like plotting and prediction are available through familiar S3 methods. For example, there are several plot methods available.</p>
<div class="cell" data-layout-nrow="2" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model1)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model1, <span class="st">"components"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-claims" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-claims-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-claims" style="flex-basis: 100.0%;justify-content: center;">
<div id="fig-claims-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-claims-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-claims-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-claims" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-claims-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Prediction
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-claims" style="flex-basis: 100.0%;justify-content: center;">
<div id="fig-claims-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-claims-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-claims-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-claims" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-claims-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Components
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-claims-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: Structural time series model for unemployment claims
</figcaption>
</figure>
</div>
</div>
<p>The <a href="#fig-claims-1" class="quarto-xref">Figure&nbsp;<span>15.4 (a)</span></a> above shows the posterior distribution of model state. Blue circles are actual data points. The <a href="#fig-claims-2" class="quarto-xref">Figure&nbsp;<span>15.4 (b)</span></a> shows the individual state components. The plot looks fuzzy because it is showing the marginal posterior distribution at each time point.</p>
<p>The default plot method plots the posterior distribution of the conditional mean <span class="math inline">\(Z_t^T\alpha_t\)</span> given the full data <span class="math inline">\(y=y_1,\ldots,y_T\)</span>. Other plot methods can be accessed by passing a string to the plot function. For example, to see the contributions of the individual state components, pass the string “components” as a second argument, as shown above. The figure below shows the output of these two plotting functions. You can get a list of all available plots by passing the string <code>help</code> as the second argument.</p>
<p>To predict future values there is a <code>predict</code> method. For example, to predict the next 12 time points you would use the following commands.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">horizon =</span> <span class="dv">12</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pred1, <span class="at">plot.original =</span> <span class="dv">156</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/predict1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The output of predict is an object of class <code>bsts.prediction</code>, which has its own <code>plot</code> method. The <code>plot.original = 156</code> argument says to plot the prediction along with the last 156 time points (3 years) of the original series.</p>
</div>
</section>
<section id="regression-with-spike-and-slab-priors" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="regression-with-spike-and-slab-priors"><span class="header-section-number">15.2</span> Regression with spike and slab priors</h2>
<p>Now let’s add a regression component to the model described above, so that we can use Google search data to improve the forecast. The <code>bsts</code> package only includes 10 search terms with the initial claims data set, to keep the package size small, but <span class="citation" data-cites="scott2014predicting">S. Scott and Varian (<a href="references.html#ref-scott2014predicting" role="doc-biblioref">2014</a>)</span> considered examples with several hundred predictor variables. When faced with large numbers of potential predictors it is important to have a prior distribution that induces sparsity. A spike and slab prior is a natural way to express a prior belief that most of the regression coefficients are exactly zero.</p>
<p>A spike and slab prior is a prior on a set of regression coefficients that assigns each coefficient a positive probability of being zero. Upon observing data, Bayes’ theorem updates the inclusion probability of each coefficient. When sampling from the posterior distribution of a regression model under a spike and slab prior, many of the simulated regression coefficients will be exactly zero. This is unlike the “lasso” prior (the Laplace, or double-exponential distribution), which yields MAP estimates at zero but where posterior simulations will be all nonzero. You can read about the mathematical details of spike and slab priors in <span class="citation" data-cites="scott2014predicting">S. Scott and Varian (<a href="references.html#ref-scott2014predicting" role="doc-biblioref">2014</a>)</span>.</p>
<p>When fitting <code>bsts</code> models that contain a regression component, extra arguments captured by <code>...</code> are passed to the SpikeSlabPrior function from the BoomSpikeSlab package. This allows the analyst to adjust the default prior settings for the regression component from the <code>bsts</code> function call. To include a regression component in a <code>bsts</code> model, simply pass a model formula as the first argument.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a `bsts` model with expected model size 1, the default.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">bsts</span>(iclaimsNSA <span class="sc">~</span> .,<span class="at">state.specification =</span> ss,<span class="at">niter =</span> <span class="dv">1000</span>,<span class="at">data =</span> initial.claims)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a `bsts` model with expected model size 5, to include more coefficients.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">bsts</span>(iclaimsNSA <span class="sc">~</span> .,<span class="at">state.specification =</span> ss,<span class="at">niter =</span> <span class="dv">1000</span>,<span class="at">data =</span> initial.claims,<span class="at">expected.model.size =</span> <span class="dv">5</span>)  <span class="co"># Passed to SpikeSlabPrior.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To examine the output you can use the same plotting functions as before. For example, to see the contribution of each state component you can type</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">0</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model2, <span class="st">"comp"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/plotmodel2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>It produces the contribution of each state component to the initial claims data, assuming a regression component with default prior. Compare to the previous model. The regression component is explaining a substantial amount of variation in the initial claims series.</p>
<p>There are also plotting functions that you can use to visualize the regression coefficients. The following commands plot posterior inclusion probabilities for predictors in the “initial claims” nowcasting example assuming an expected model size of 1 and 5.</p>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model2, <span class="st">"coef"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model3, <span class="st">"coef"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-model23" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model23-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-model23" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-model23-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-model23-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-model23-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-model23" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-model23-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Full
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-model23" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-model23-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-model23-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-model23-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-model23" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-model23-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Sparse
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model23-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: Variable Importance
</figcaption>
</figure>
</div>
<p>The search term “unemployment office” shows up with high probability in both models. Increasing the expected model size from 1 (the default) to 5 allows other variables into the model, though “Idaho unemployment” is the only one that shows up with high probability.</p>
<p>Those probabilities are calculated from the histogram of the samples of each <span class="math inline">\(\beta\)</span> calculated by the estimation algorithm (MCMC)</p>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># unemployment.office</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(model3<span class="sc">$</span>coefficients[,<span class="dv">10</span>], <span class="at">breaks =</span> <span class="dv">40</span>, <span class="at">main=</span><span class="st">""</span>,<span class="at">xlab=</span><span class="st">"unemployment.office"</span>, <span class="at">col=</span><span class="st">"lightblue"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># pennsylvania.unemployment</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(model3<span class="sc">$</span>coefficients[,<span class="dv">3</span>], <span class="at">breaks =</span> <span class="dv">40</span>, <span class="at">main =</span> <span class="st">""</span>, <span class="at">xlab=</span><span class="st">"pennsylvania.unemployment"</span>, <span class="at">col=</span><span class="st">"lightblue"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(model2<span class="sc">$</span>coefficients[,<span class="dv">3</span>], <span class="at">breaks =</span> <span class="dv">40</span>, <span class="at">main =</span> <span class="st">""</span>, <span class="at">xlab=</span><span class="st">"pennsylvania.unemployment"</span>, <span class="at">col=</span><span class="st">"lightblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-bstshist" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bstshist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bstshist" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-bstshist-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bstshist-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-bstshist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-bstshist" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bstshist-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Sparse
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bstshist" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-bstshist-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bstshist-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-bstshist-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-bstshist" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bstshist-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Sparse
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bstshist" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-bstshist-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-bstshist-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-bstshist-3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-bstshist" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-bstshist-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Full
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bstshist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.6: Sample from the distribution over two beta parameters
</figcaption>
</figure>
</div>
</section>
<section id="model-diagnostics-did-the-google-data-help" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="model-diagnostics-did-the-google-data-help"><span class="header-section-number">15.3</span> Model diagnostics: did the Google data help?</h2>
<p>As part of the model fitting process, the algorithm generates the one-step-ahead prediction errors <span class="math inline">\(y_t - E(y_t | Y_{t-1}, \theta)\)</span>, where <span class="math inline">\(Y_{t-1}=y_1,\ldots,y_{t-1}\)</span>, and the vector of model parameters <span class="math inline">\(\theta\)</span> is fixed at its current value in the MCMC algorithm. The one-step-ahead prediction errors can be obtained from the <code>bsts</code> model by calling <code>bsts.prediction.errors(model1)</code>.</p>
<p>The one step prediction errors are a useful diagnostic for comparing several <code>bsts</code> models that have been fit to the same data. They are used to implement the function <code>CompareBstsModels</code>, which is called as shown below.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">CompareBstsModels</span>(<span class="fu">list</span>(<span class="st">"Model 1"</span> <span class="ot">=</span> model1,</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Model 2"</span> <span class="ot">=</span> model2,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>               <span class="st">"Model 3"</span> <span class="ot">=</span> model3),</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">colors =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/compareRUE-1.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Comparison of Errors for the three models.</figcaption>
</figure>
</div>
</div>
</div>
<p>The bottom panel shows the original series. The top panel shows the cumulative total of the mean absolute one step prediction errors for each model. The final time point in the top plot is proportional to the mean absolute prediction error for each model, but plotting the errors as a cumulative total lets you see particular spots where each model encountered trouble, rather than just giving a single number describing each model’s predictive accuracy. This figure shows that the Google data help explain the large spike near 2009, where model 1 accumulates errors at an accelerated rate, but models 2 and 3 continue accumulating errors at about the same rate they had been before. The fact that the lines for models 2 and 3 overlap in this figure means that the additional predictors allowed by the relaxed prior used to fit model 3 do not yield additional predictive accuracy.</p>
<div id="exm-LongTermForecasting" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.2 (Long-term forecasting)</strong></span> A common question about <code>bsts</code> is “which trend model should I use?” To answer that question it helps to know a bit about the different models that the <code>bsts</code> software package provides, and what each model implies. In the local level model, the state evolves according to a random walk: <span class="math display">\[
\mu_{t+1}=\mu_t+\eta_t.
\]</span> If you place your eye at time 0 and ask what happens at time <span class="math inline">\(t\)</span>, you find that <span class="math inline">\(\mu_t \sim N(\mu_0,t\sigma^2_\eta)\)</span>. The variance continues to grow with <span class="math inline">\(t\)</span>, all the way to <span class="math inline">\(t=\infty\)</span>. The local linear trend is even more volatile. When forecasting far into the future, the flexibility provided by these models becomes a double-edged sword, as local flexibility in the near term translates into extreme variance in the long term.</p>
<p>An alternative is to replace the random walk with a stationary AR process. For example <span class="math display">\[
\mu_{t+1}=\rho\mu_t+\eta_t,
\]</span></p>
<p>with <span class="math inline">\(\eta_t \sim N(0,\sigma^2_{\eta})\)</span> and <span class="math inline">\(|\rho|&lt;1\)</span>. This model has stationary distribution <span class="math display">\[
\mu_{\infty} \sim N\left(0,\frac{\sigma^2_{\eta}}{1-\rho^2}\right),
\]</span> which means that uncertainty grows to a finite asymptote, rather than infinity, in the distant future. The <code>bsts</code> package offers autoregressive state models through the functions <code>AddAr</code>, when you want to specify a certain number of lags, and <code>AddAutoAr</code> when you want the software to choose the important lags for you.</p>
<p>A hybrid model modifies the local linear trend model by replacing the random walk on the slope with a stationary AR(1) process, while keeping the random walk for the level of the process. The <code>bsts</code> package refers to this is the “semilocal linear trend” model. <span class="math display">\[\begin{align*}
\mu_{t+1}=&amp;    \mu_t+\delta_t+\eta_{0t}\\
\delta_{t+1}=&amp; D+\rho(\delta_t-D)+\eta_{1t}
\end{align*}\]</span> The <span class="math inline">\(D\)</span> parameter is the long-run slope of the trend component, to which <span class="math inline">\(\delta_t\)</span> will eventually revert. However, <span class="math inline">\(\delta_t\)</span> can have short-term autoregressive deviations from the long-term trend, with memory determined by <span class="math inline">\(\rho\)</span>. Values of <span class="math inline">\(\rho\)</span> close to 1 will lead to long deviations from <span class="math inline">\(D\)</span>. To see the impact this can have on long-term forecasts, consider the time series of daily closing values for the S&amp;P 500 stock market index over the last 5 years, shown below.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>GSPC <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/GSPC.csv"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>GSPC <span class="ot">=</span> <span class="fu">xts</span>(GSPC, <span class="at">order.by =</span> <span class="fu">as.Date</span>(<span class="fu">rownames</span>(GSPC), <span class="st">"%Y-%m-%d"</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(<span class="fu">head</span>(GSPC))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(GSPC<span class="sc">$</span>GSPC.Adjusted, <span class="at">main=</span><span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/sp500-1.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Daily closing values for the S&amp;P 500 stock market index</figcaption>
</figure>
</div>
</div>
</div>
<p>Consider two forecasts of the daily values of this series for the next 360 days. The first assumes the local linear trend model. The second assumes the semilocal linear trend.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sp500 <span class="ot">=</span> GSPC<span class="sc">$</span>GSPC.Adjusted</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ss1 <span class="ot">&lt;-</span> <span class="fu">AddLocalLinearTrend</span>(<span class="fu">list</span>(), sp500)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">bsts</span>(sp500, <span class="at">state.specification =</span> ss1, <span class="at">niter =</span> <span class="dv">1000</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>ss2 <span class="ot">&lt;-</span> <span class="fu">AddSemilocalLinearTrend</span>(<span class="fu">list</span>(), sp500)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">bsts</span>(sp500, <span class="at">state.specification =</span> ss2, <span class="at">niter =</span> <span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The figure below shows long-term forecasts of the S&amp;P 500 closing values under the (left) local linear trend and (right) semilocal linear trend state models.</p>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">"../data/timeseries/model12-sp500.RData"</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">horizon =</span> <span class="dv">360</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>pred2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, <span class="at">horizon =</span> <span class="dv">360</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pred2, <span class="at">plot.original =</span> <span class="dv">360</span>, <span class="at">ylim =</span> <span class="fu">range</span>(pred1))</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pred1, <span class="at">plot.original =</span> <span class="dv">360</span>, <span class="at">ylim =</span> <span class="fu">range</span>(pred1))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-sp500" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sp500-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sp500" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-sp500-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sp500-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-sp500-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-sp500" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sp500-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Semi-local trend
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-sp500" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-sp500-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-sp500-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-sp500-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" data-ref-parent="fig-sp500" width="384">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-sp500-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Local trend
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sp500-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.7: S&amp;P 500 Prediction
</figcaption>
</figure>
</div>
<p>Not only are the forecast expectations from the two models different, but the forecast errors from the local linear trend model are implausibly wide, including a small but nonzero probability that the S&amp;P 500 index could close near zero in the next 360 days. The error bars from the semilocal linear trend model are far more plausible, and more closely match the uncertainty observed over the life of the series thus far.</p>
</div>
<div id="exm-Recession" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.3 (Recession modeling using non-Gaussian data)</strong></span> Although we have largely skipped details about how the <code>bsts</code> software fits models, the Gaussian error assumptions in the observation and transition equations are important for the model fitting process. Part of that process involves running data through the Kalman filter, which assumes Gaussian errors in both the state and transition equations. In many settings where Gaussian errors are obviously inappropriate, such as for binary or small count data, one can introduce latent variables that give the model a conditionally Gaussian representation. Well known “data augmentation” methods exist for probit regression (<span class="citation" data-cites="albert1993statistical">Albert (<a href="references.html#ref-albert1993statistical" role="doc-biblioref">1993</a>)</span>) and models with student-T errors (<span class="citation" data-cites="rubin2015bayesian">Rubin (<a href="references.html#ref-rubin2015bayesian" role="doc-biblioref">2015</a>)</span>). Somewhat more complex methods exist for logistic regression (<span class="citation" data-cites="fruhwirth-schnatter2007auxiliary">Frühwirth-Schnatter and Frühwirth (<a href="references.html#ref-fruhwirth-schnatter2007auxiliary" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="held2006bayesian">Held and Holmes (<a href="references.html#ref-held2006bayesian" role="doc-biblioref">2006</a>)</span>, <span class="citation" data-cites="gramacy2012simulationbased">Gramacy and Polson (<a href="references.html#ref-gramacy2012simulationbased" role="doc-biblioref">2012</a>)</span>) and Poisson regression (<span class="citation" data-cites="fruhwirth-schnatter2008improved">Frühwirth-Schnatter et al. (<a href="references.html#ref-fruhwirth-schnatter2008improved" role="doc-biblioref">2008</a>)</span>). Additional methods exist for quantile regression (<span class="citation" data-cites="benoit2012binary">Benoit and Van den Poel (<a href="references.html#ref-benoit2012binary" role="doc-biblioref">2012</a>)</span>), support vector machines (<span class="citation" data-cites="polson2011dataa">Polson and Scott (<a href="references.html#ref-polson2011dataa" role="doc-biblioref">2011</a>)</span>), and multinomial logit regression (<span class="citation" data-cites="fruhwirth-schnatter2010data">Frühwirth-Schnatter and Frühwirth (<a href="references.html#ref-fruhwirth-schnatter2010data" role="doc-biblioref">2010</a>)</span>). These are not currently provided by the <code>bsts</code> package, but they might be added in the future.</p>
<p>To see how non-Gaussian errors can be useful, consider the analysis done by <span class="citation" data-cites="berge2016which">Berge, Sinha, and Smolyansky (<a href="references.html#ref-berge2016which" role="doc-biblioref">2016</a>)</span>, who used Bayesian model averaging (BMA) to investigate which of several economic indicators would best predict the presence or absence of a recession. We will focus on their nowcasting example, which models the probability of a recession at the same time point as the predictor variables. <span class="citation" data-cites="berge2016which">Berge, Sinha, and Smolyansky (<a href="references.html#ref-berge2016which" role="doc-biblioref">2016</a>)</span> also analyzed the data with the predictors at several lags.</p>
<p>The model used in <span class="citation" data-cites="berge2016which">Berge, Sinha, and Smolyansky (<a href="references.html#ref-berge2016which" role="doc-biblioref">2016</a>)</span> was a probit regression, with Bayesian model averaging used to determine which predictors should be included. The response variable was the presence or absence of a recession (as determined by NBER).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"../data/timeseries/rec_data_20160613.csv"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>rec <span class="ot">=</span> <span class="fu">ts</span>(dat<span class="sc">$</span>nber, <span class="at">start=</span><span class="fu">c</span>(<span class="dv">1973</span>, <span class="dv">1</span>), <span class="at">end=</span><span class="fu">c</span>(<span class="dv">2016</span>, <span class="dv">5</span>), <span class="at">frequency=</span><span class="dv">12</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rec, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'blue'</span>, <span class="at">ylab=</span><span class="st">"Recession"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/nber-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Recession periods identified by NBER</figcaption>
</figure>
</div>
</div>
</div>
<p>The BMA done by <span class="citation" data-cites="berge2016which">Berge, Sinha, and Smolyansky (<a href="references.html#ref-berge2016which" role="doc-biblioref">2016</a>)</span> is essentially the same as fitting a logistic regression under a spike-and-slab prior with the prior inclusion probability of each predictor set to 1/2. That analysis can be run using the BoomSpikeSlab R package (<span class="citation" data-cites="scott2022boomspikeslab">Steven L. Scott (<a href="references.html#ref-scott2022boomspikeslab" role="doc-biblioref">2022</a>)</span>), which is similar to <code>bsts</code>, but with only a regression component and no time series.</p>
<p>The logistic regression model is highly predictive, but it ignores serial dependence in the data. To capture serial dependence, consider the following dynamic logistic regression model with a local level trend model. <span class="math display">\[\begin{align*}
\mathrm{logit}(p_t)= &amp;  \mu_t+\beta^Tx_t\\
\mu_{t+1}= &amp;            \mu_t+\eta_t
\end{align*}\]</span> Here <span class="math inline">\(p_t\)</span> is the probability of a recession at time <span class="math inline">\(t\)</span> ,and <span class="math inline">\(x_t\)</span> is the set of economic indicators used by <span class="citation" data-cites="berge2016which">Berge, Sinha, and Smolyansky (<a href="references.html#ref-berge2016which" role="doc-biblioref">2016</a>)</span> in their analysis. The variables are listed in the table below</p>
<table class="caption-top table">
<colgroup>
<col style="width: 37%">
<col style="width: 42%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Definition/notes</th>
<th>Transformation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Financial Variables</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Slope of yield curve</td>
<td>10-year Treasury less 3-month yield</td>
<td></td>
</tr>
<tr class="odd">
<td>Curvature of yield curve</td>
<td>2 x 2-year minus 3-month and 10-year</td>
<td></td>
</tr>
<tr class="even">
<td>GZ index</td>
<td>Gilchrist and Zakrajsek (AER, 2012)</td>
<td></td>
</tr>
<tr class="odd">
<td>TED spread</td>
<td>3-month ED less 3-month Treasury yield</td>
<td></td>
</tr>
<tr class="even">
<td>BBB corporate spread</td>
<td>BBB less 10-year Treasury yield</td>
<td></td>
</tr>
<tr class="odd">
<td>S 500, 1-month return</td>
<td></td>
<td>1-month log diff.</td>
</tr>
<tr class="even">
<td>S 500, 3-month return</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="odd">
<td>Trade-weighted dollar</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="even">
<td>VIX</td>
<td>CBOE and extended following Bloom</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Macroeconomic Indicators</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Real personal consumption expend.</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="odd">
<td>Real disposable personal income</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="even">
<td>Industrial production</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="odd">
<td>Housing permits</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="even">
<td>Nonfarm payroll employment</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="odd">
<td>Initial claims</td>
<td>4-week moving average</td>
<td>3-month log diff.</td>
</tr>
<tr class="even">
<td>Weekly hours, manufacturing</td>
<td></td>
<td>3-month log diff.</td>
</tr>
<tr class="odd">
<td>Purchasing managers index</td>
<td></td>
<td>3-month log dif</td>
</tr>
</tbody>
</table>
<p>First, we prepare the data by shifting it by <span class="math inline">\(h\)</span>, which is the forecast horizon.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>h<span class="ot">=</span><span class="dv">0</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predict h months ahead</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>y.h <span class="ot">&lt;-</span> dat<span class="sc">$</span>nber[<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>h)]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>hh <span class="ot">&lt;-</span> <span class="fu">length</span>(dat<span class="sc">$</span>nber) <span class="sc">-</span> h</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>dat.h <span class="ot">&lt;-</span> dat[<span class="dv">1</span><span class="sc">:</span>hh,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># h=0 is a special case</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(h<span class="sc">==</span><span class="dv">0</span>) y.h   <span class="ot">&lt;-</span> dat<span class="sc">$</span>nber</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span>(h<span class="sc">==</span><span class="dv">0</span>) dat.h <span class="ot">&lt;-</span> dat[,<span class="sc">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To fit this model, we can issue the commands shown below.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Because 'y' is 0/1 and the state is on the logit scale the default prior</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># assumed by AddLocalLevel won't work here, so we need to explicitly set the</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># priors for the variance of the state innovation errors and the initial value</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># of the state at time 0.  The 'SdPrior' and 'NormalPrior' functions used to</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># define these priors are part of the Boom package.  See R help for</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># documentation.  Note the truncated support for the standard deviation of the</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># random walk increments in the local level model.</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># A more complex model</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>ss <span class="ot">&lt;-</span> <span class="fu">AddLocalLevel</span>(<span class="fu">list</span>(),y.h,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">sigma.prior =</span> <span class="fu">SdPrior</span>(<span class="at">sigma.guess =</span> .<span class="dv">1</span>,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">sample.size =</span> <span class="dv">1</span>,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>                                          <span class="at">upper.limit =</span> <span class="dv">1</span>),</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>                    <span class="at">initial.state.prior =</span> <span class="fu">NormalPrior</span>(<span class="dv">0</span>, <span class="dv">5</span>))</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Tell bsts that the observation equation should be a logistic regression by</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># passing the 'family = "logit"' argument.</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>ts.model <span class="ot">&lt;-</span> <span class="fu">bsts</span>(y.h <span class="sc">~</span> ., ss, <span class="at">data =</span> dat.h, <span class="at">niter =</span> <span class="dv">20000</span>,<span class="at">family =</span> <span class="st">"logit"</span>, <span class="at">expected.model.size =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s plot the results</p>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ts.model,<span class="st">"coef"</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ts.model)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(y.h, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">col=</span><span class="st">"blue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-layout-align="center" data-layout-ncol="2" data-null_prefix="true">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/nber-plot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/nber-plot-2.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ts.model,<span class="st">"predictors"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/ts.model-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<p>Notice that the distribution of <span class="math inline">\(p_t\)</span> is moving to very large values during a recession, and to very small values outside of a recession. This effect captures the strong serial dependence in the recession data. Recessions are rare, but once they occur they tend to persist. Assuming independent time points is therefore unrealistic, and it substantially overstates the amount of information available to identify logistic regression coefficients.</p>
</div>
</section>
<section id="final-remarks-on-structural-models" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="final-remarks-on-structural-models"><span class="header-section-number">15.4</span> Final Remarks on Structural Models</h2>
<p>The preceding examples have shown that the <code>bsts</code> software package can handle several nonstandard, but useful, time series applications. These include the ability to handle large numbers of contemporaneous predictors with spike and slab priors, the presence of trend models suitable for long term forecasting, and the ability to handle non-Gaussian data. We have run out of space, but <code>bsts</code> can do much more.</p>
<p>For starters there are other state models you can use. Bsts has elementary support for holidays. It knows about 18 US holidays, and has capacity to add more, including holidays that occur on the same date each year, holidays that occur on a fixed weekday of a fixed month (e.g.&nbsp;3rd Tuesday in February, or last Monday in November). The model for each holiday is a simple random walk, but look for future versions to have improved holiday support via Bayesian shrinkage.</p>
<p>Bsts offers support for multiple seasonalities. For example, if you have several weeks of hourly data then you will have an hour-of-day effect as well as a day-of-week effect. You can model these using a single seasonal effect with 168 seasons (which would allow for different hourly effects on weekends and weekdays), or you can assume additive seasonal patterns using the season.duration argument to <code>AddSeasonal</code>,</p>
<pre><code>ss &lt;- AddSeasonal(ss, y, nseasons = 24)
ss &lt;- AddSeasonal(ss, y, nseasons = 7, season.duration = 24)</code></pre>
<p>The latter specifies that each daily effect should remain constant for 24 hours. For modeling physical phenomena, <code>bsts</code> also offers trigonometric seasonal effects, which are sine and cosine waves with time varying coefficients. You obtain these by calling AddTrig. Time varying effects are available for arbitrary regressions with small numbers of predictor variables through a call to <code>AddDynamicRegression</code>.</p>
<p>In addition to the trend models discussed so far, the function <code>AddStudentLocalLinearTrend</code> gives a version of the local linear trend model that assumes student-t errors instead of Gaussian errors. This is a useful state model for short term predictions when the mean of the time series exhibits occasional dramatic jumps. Student-t errors can be introduced into the observation equation by passing the <code>family = "student"</code> argument to the <code>bsts</code> function call. Allowing for heavy tailed errors in the observation equation makes the model robust against individual outliers, while heavy tails in the state model provides robustness against sudden persistent shifts in level or slope. This can lead to tighter prediction limits than Gaussian models when modeling data that have been polluted by outliers. The observation equation can also be set to a Poisson model for small count data if desired.</p>
<p>Finally, the most recent update to <code>bsts</code> supports data with multiple observations at each time stamp. The Gaussian version of the model is <span class="math display">\[\begin{align*}
y_{it} = &amp;\beta^T x_{it} + Z_t^T\alpha_t + \epsilon_{it}\\
\alpha_{t+1} = &amp; T_t \alpha_t + R_t \eta_t,
\end{align*}\]</span> which is best understood as a regression model with a time varying intercept.</p>
</section>
<section id="algorithms" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="algorithms"><span class="header-section-number">15.5</span> Algorithms</h2>
<p>The classic filtering and prediction algorithms for linear and Gaussian systems are described in <span class="citation" data-cites="kalman1960new">Rudolph Emil Kalman (<a href="references.html#ref-kalman1960new" role="doc-biblioref">1960</a>)</span> and <span class="citation" data-cites="kalman1961new">R. E. Kalman and Bucy (<a href="references.html#ref-kalman1961new" role="doc-biblioref">1961</a>)</span>. Early work on discrete recursions for hidden Markov models are in <span class="citation" data-cites="baum1970maximization">Baum et al. (<a href="references.html#ref-baum1970maximization" role="doc-biblioref">1970</a>)</span> who use an EM-type algorithm, <span class="citation" data-cites="viterbi1967error">Viterbi (<a href="references.html#ref-viterbi1967error" role="doc-biblioref">1967</a>)</span> who provides a modal state filter estimate and recursions developed in <span class="citation" data-cites="lindgren1978markov">Lindgren (<a href="references.html#ref-lindgren1978markov" role="doc-biblioref">1978</a>)</span>. While these can be used to evaluate the marginal likelihood for the parameters they are computationally too intensive to solve the filtering and learning, <span class="citation" data-cites="lindgren1978markov">Lindgren (<a href="references.html#ref-lindgren1978markov" role="doc-biblioref">1978</a>)</span>. <span class="citation" data-cites="scott2002bayesian">Steven L. Scott (<a href="references.html#ref-scott2002bayesian" role="doc-biblioref">2002</a>)</span> provides a review of FFBS algorithms for discrete HMMs.</p>
<p>Markov chain Monte Carlo (MCMC) algorithms for parameter learning in nonlinear non-Gaussian state space models were developed by <span class="citation" data-cites="carlin1992monte">Carlin, Polson, and Stoffer (<a href="references.html#ref-carlin1992monte" role="doc-biblioref">1992</a>)</span>. For linear and Gaussian systems, <span class="citation" data-cites="carter1994gibbs">Carter and Kohn (<a href="references.html#ref-carter1994gibbs" role="doc-biblioref">1994</a>)</span> introduced the filter forward and backwards sample (FFBS) algorithm, which efficiently draws the entire block of hidden states. For handling multinomial logit models, <span class="citation" data-cites="scott2002bayesian">Steven L. Scott (<a href="references.html#ref-scott2002bayesian" role="doc-biblioref">2002</a>)</span> and <span class="citation" data-cites="fruhwirth-schnatter2008improved">Frühwirth-Schnatter et al. (<a href="references.html#ref-fruhwirth-schnatter2008improved" role="doc-biblioref">2008</a>)</span> developed mixture of normals approximation methods. Additionally, <span class="citation" data-cites="west1997bayesian">West and Harrison (<a href="references.html#ref-west1997bayesian" role="doc-biblioref">1997</a>)</span> proposed conditionally conjugate priors that allow parameters to be marginalized out of the updating equations, leading to more efficient inference procedures.</p>
<section id="kalman-filtering" class="level3">
<h3 class="anchored" data-anchor-id="kalman-filtering">Kalman Filtering</h3>
<p>The Normal/ Normal Bayesian learning model provides the basis for shrinkage estimation of multiple means and the basis of the Kalman filter for dynamically tracking a path of an object.</p>
<p>The Kalman filter is arguably the most common application of Bayesian inference. The Kalman filter assumes a linear and Gaussian state-space model: <span class="math display">\[
y_{t}=x_{t}+\sigma\varepsilon_{t}^{y}\text{ and }x_{t}=x_{t-1}+\sigma
_{x}\varepsilon_{t}^{x},
\]</span> where <span class="math inline">\(\varepsilon_{t}^{y}\)</span> and <span class="math inline">\(\varepsilon_{t}^{x}\)</span> are i.i.d. standard normal and <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\sigma_{x}\)</span> are known. The observation equation posits that the observed data, <span class="math inline">\(y_{t}\)</span>, consists of the random-walk latent state, <span class="math inline">\(x_{t}\)</span>, that is polluted by noise, <span class="math inline">\(\sigma\varepsilon_{t}^{y}\)</span>. Further, <span class="math inline">\(\sigma_{x}/\sigma\)</span> is the “signal-to-noise” ratio, measures the information content of the signal. As <span class="math inline">\(\sigma\)</span> increases relatively to <span class="math inline">\(\sigma_{x}\)</span>, the observations become noisier and less informative. The model is initialized via a prior distribution over <span class="math inline">\(x_{0}\)</span>, which is for analytical tractability must be normally distributed, <span class="math inline">\(x_{0}\sim\mathcal{N}\left( \mu_{0},\sigma_{0}^{2}\right)\)</span>.</p>
<p>The posterior distribution solves the filtering problem and is defined recursively via Bayes rule: <span class="math display">\[
p\left(  x_{t+1} \mid y^{t+1}\right)  =\frac{p\left(  y_{t+1} \mid x_{t+1}\right)
p\left(  x_{t+1} \mid y^{t}\right)  }{p\left(  y_{t+1} \mid y^{t}\right)  }\propto
p\left(  y_{t+1} \mid x_{t+1}\right)  p\left(  x_{t+1} \mid y^{t}\right)  \text{.}%
\]</span> and the likelihood function, <span class="math inline">\(p\left( y_{t+1} \mid x_{t+1}\right)\)</span>. The predictive distribution summarizes all of the information about <span class="math inline">\(x_{t+1}\)</span> based on lagged observations. The likelihood function summarizes the new information in <span class="math inline">\(y_{t+1}\)</span> about <span class="math inline">\(x_{t+1}\)</span>.</p>
<p>The Kalman filter relies on an inductive argument: assume that <span class="math inline">\(p\left( x_{t} \mid y^{t}\right) \sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}\right)\)</span> and then verify that <span class="math inline">\(p\left( x_{t+1} \mid y^{t+1}\right) \sim\mathcal{N}\left( \mu_{t+1},\sigma_{t+1}^{2}\right)\)</span> with analytical expressions for the hyperparameters. To verify, note that since <span class="math inline">\(p\left(x_{t} \mid y^{t}\right) \sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}\right)\)</span>, <span class="math inline">\(x_{t}=\mu_{t}+\sigma_{t}\eta_{t}\)</span> for some standard normal <span class="math inline">\(\eta_{t}\)</span>. Substituting into the state evolution, the predictive is <span class="math inline">\(x_{t+1} \mid y^{t}\sim\mathcal{N}\left( \mu_{t},\sigma_{t}^{2}+\sigma_{x}^{2}\right)\)</span>. Since <span class="math inline">\(p\left( y_{t+1} \mid x_{t+1}\right) \sim\mathcal{N}\left(x_{t+1},\sigma^{2}\right)\)</span>, the posterior is <span class="math display">\[\begin{align*}
p\left(  x_{t+1} \mid y^{t+1}\right)   &amp;  \propto p\left(  y_{t+1} \mid x_{t+1}\right)
p\left(  x_{t+1} \mid y^{t}\right)  \propto\exp\left[  -\frac{1}{2}\left( \frac{\left(  y_{t+1}-x_{t+1}\right)  ^{2}}{\sigma^{2}}+\frac{\left( x_{t+1}-\mu_{t}\right)  ^{2}}{\sigma_{t}^{2}+\sigma_{x}^{2}}\right)  \right]
\\
&amp;  \propto\exp\left(  -\frac{1}{2}\frac{\left(  x_{t+1}-\mu_{t+1}\right)
^{2}}{\sigma_{t+1}^{2}}\right)
\end{align*}\]</span> where <span class="math inline">\(\mu_{t+1}\)</span> and <span class="math inline">\(\sigma_{t+1}^{2}\)</span> are computed by completing the square: <span class="math display">\[\begin{equation}
\frac{\mu_{t+1}}{\sigma_{t+1}^{2}}=\frac{y_{t+1}}{\sigma^{2}}+\frac{\mu_{t}%
}{\sigma_{t}^{2}+\sigma_{x}^{2}}\text{ and }\frac{1}{\sigma_{t+1}^{2}}%
=\frac{1}{\sigma^{2}}+\frac{1}{\sigma_{t}^{2}+\sigma_{x}^{2}}\text{.}\nonumber
\end{equation}\]</span> Here, inference on <span class="math inline">\(x_{t}\)</span> is merely running the Kalman filter, that is, sequential computing <span class="math inline">\(\mu_{t}\)</span> and <span class="math inline">\(\sigma_{t}^{2}\)</span>, which are state sufficient statistics.</p>
<p>The Kalman filter provides an excellent example of the mechanics of Bayesian inference: given a prior and likelihood, compute the posterior distribution. In this setting, it is hard to imagine an more intuitive or alternative approach. The same approach applied to learning fixed static parameters. In this case, <span class="math inline">\(y_{t}=\mu+\sigma\varepsilon_{t},\)</span> where <span class="math inline">\(\mu\sim\mathcal{N}\left( \mu_{0},\sigma_{0}^{2}\right)\)</span> is the initial distribution. Using the same arguments as above, it is easy to show that <span class="math inline">\(p\left(\mu \mid y^{t+1}\right) \sim\mathcal{N}\left( \mu_{t+1},\sigma_{t+1}^{2}\right)\)</span>, where <span class="math display">\[\begin{align*}
\frac{\mu_{t+1}}{\sigma_{t+1}^{2}}  &amp;  =\left(  \frac{y_{t+1}}{\sigma^{2}}+\frac{\mu_{t}}{\sigma_{t}^{2}}\right)  =\frac{\left(  t+1\right) \overline{y}_{t+1}}{\sigma^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\text{,}\\
\frac{1}{\sigma_{t+1}^{2}}  &amp;  =\frac{1}{\sigma^{2}}+\frac{1}{\sigma_{t}^{2}}=\frac{\left(  t+1\right)  }{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\text{,}
\end{align*}\]</span> and <span class="math inline">\(\overline{y}_{t}=t^{-1}\sum_{t=1}^{t}y_{t}\)</span>.</p>
<p>Now, given this example, the same statements can be posed as in the state variable learning problem: it is hard to think of a more intuitive or alternative approach for sequential learning. In this case, researchers often have different feelings about assuming a prior distribution over the state variable and a parameter. In the state filtering problem, it is difficult to separate the prior distribution and the likelihood. In fact, one could view the initial distribution over <span class="math inline">\(x_{0}\)</span>, the linear evolution for the state variable, and the Gaussian errors as the “prior” distribution.</p>
<p>Now consider,linear multivariate Gaussian state space model: <span class="math display">\[\begin{align*}
y_{t} &amp;  =F_{t}x_{t}+\varepsilon_{t} \; \; \text{where} \; \; \varepsilon_{t}\sim\mathcal{N}\left( 0,\Sigma_{t}\right) \\
x_{t} &amp;  =G_{t}x_{t-1}+\varepsilon_{t}^{x} \; \; \text{where} \; \; \varepsilon_{t}^x\sim\mathcal{N}\left( 0,\Sigma_{t}^x\right)
\end{align*}\]</span> where we allow for heteroscedascity in the error variance-covariance matrices. We complete the model specification with a normal prior on the initial starting condition <span class="math inline">\(x_{0}\sim\mathcal{N}\left(  \mu_{0},\Sigma_{0}\right)\)</span>. It is important to recognize that <span class="math inline">\(\varepsilon_{t}\)</span> and <span class="math inline">\(\varepsilon_{t}^{x}\)</span> need only be conditionally normal. There are a number of distributions of interest <span class="math display">\[\begin{align*}
\text{Filtering}  &amp;  :p\left(  x_{t}|y^{t}\right)  \text{ }t=1,...,T\\
\text{Forecasting}  &amp;  :p\left(  x_{t+1}|y^{t}\right)  \text{ } t=1,...,T\\
\text{Smoothing}  &amp;  :p\left(  x_{t}|y^{t+1}\right)  \text{ }t=1,...,T \\
\text{Prediction}  &amp;  :p\left(  y_{t+1}|y^{t}\right)  \text{ }t=1,...,T
\end{align*}\]</span> For known parameters with linearity and Gaussianity we have the following Kalman filter recursions for calculation these distributions.</p>
<p>The fundamental filtering relationship is based on the fact that the filtering distribution is of the form <span class="math display">\[
p(x_{t}|y^{t})\sim\mathcal{N}\left(  \mu_{t|t},\Sigma_{t|t}\right)
\;\;\mathrm{and}\;\;p(x_{t+1}|y^{t+1})\sim\mathcal{N}\left(  \mu
_{t+1|t+1},\Sigma_{t+1|t+1}\right)
\]</span> where <span class="math inline">\((\mu_{t+1|t+1},\Sigma_{t+1|t+1})\)</span> are related to <span class="math inline">\((\mu_{t|t}%
,\Sigma_{t|t})\)</span> via the Kalman filter recursions. In the following, it is sometimes useful to write this as <span class="math display">\[
p(x_{t}|y^{t})\sim\mathcal{N}\left(  \mu_{t|t},\Sigma_{t|t}\right) \; \Rightarrow \; x_{t}=\mu_{t|t}+\Sigma_{t|t}^{\frac{1}{2}}\widehat{\varepsilon}_{t}%
\]</span> where <span class="math inline">\(\widehat{\varepsilon}_{t} \sim \mathcal{N}(0,1)\)</span>. Before we derive the filtering recursions and characterize the state filtering distribution we first find the forecasting distribution. The predictive or forecast distribution is defined as follows.</p>
<p><strong>Predictive Distribution, <span class="math inline">\(p( x_{t+1} | y^{t} )\)</span>.</strong></p>
<p>The key distributions in Bayes rule are the predictive and the conditional state posterior given by <span class="math display">\[
p( x_{t+1}|y^{t} )\sim\mathcal{N}\left(  \mu_{t+1|t},\Sigma_{t+1|t}\right)
\]</span> To compute the predictive or forecasting distribution note that:<br>
<span class="math display">\[
p\left(  x_{t+1}|y^{t}\right)  =p\left(  G_{t+1}x_{t}+\varepsilon_{t+1}%
^{x}|y^{t}\right)  \sim\mathcal{N}\left(  \mu_{t+1|t},\Sigma_{t+1|t} \right)
\]</span> where the predictive moments are</p>
<p><span class="math display">\[\begin{align*}
\mu_{t+1|t}  &amp;  =G_{t+1}\mu_{t}\\
\Sigma_{t+1|t}  &amp;  =G_{t+1}\Sigma_{t}G_{t+1}^T+\Sigma_{t+1}^{x}.
\end{align*}\]</span> We now state and derive the main Kalman filtering recursions for linear Gaussian models with known parameters.</p>
<p><strong>Filtering Distribution</strong> The classic Kalman filter characterisation of the state filtering distributionp(x_{t+1}|y^{t+1})$ and moment recursions are given by <span class="math display">\[
p(x_{t+1}|y^{t+1})\sim\mathcal{N}\left(  \mu_{t+1|t+1},\Sigma_{t+1|t+1}\right)
\]</span> The updated posterior means and variances are defined by <span class="math display">\[\begin{align*}
\mu_{t+1|t+1} &amp;  =\mu_{t+1|t}+K_{t+1}e_{t+1}\\
\Sigma_{t+1|t+1} &amp;  =(I-K_{t+1}F_{t+1})\Sigma_{t+1|t}%
\end{align*}\]</span> where the Kalman gain <span class="math inline">\(K_{t+1}\)</span> matrix and innovations vector <span class="math inline">\(e_{t+1}\)</span> are <span class="math display">\[\begin{align*}
K_{t+1}  &amp;  = \Sigma_{t+1|t} F_{t+1}^T\left(  F_{t+1} \Sigma_{t+1|t}
F_{t+1}^T+ \Sigma_{t+1} \right)  ^{-1}\\
e_{t+1}  &amp;  = y_{t+1} - F_{t+1} \mu_{t+1|t}%
\end{align*}\]</span></p>
<p>To prove this result we use the predictive distribution and an application of Bayes rule which implies that<br>
<span class="math display">\[\begin{align*}
p\left(  x_{t+1}|y^{t+1}\right)   &amp; = p\left(  x_{t+1}|y_{t+1}%
,y^{t}\right) \\
&amp;  = \frac{ p\left(  y_{t+1}|x_{t+1}\right)  p\left(  x_{t+1}|y^{t}\right)}{  p\left(  y_{t+1}|y^t\right) }
\text{.}%
\end{align*}\]</span> Under the normality assumption, the likelihood term is <span class="math display">\[
p( y_{t+1} | x_{t+1} ) = ( 2 \pi )^{-\frac{p}{2}} | \Sigma_{t+1} |^{-\frac{1}{2}}
  \exp \left ( - \frac{1}{2} ( y_{t+1} - F_{t+1} x_{t+1} )^T\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}
x_{t+1} ) \right )
\]</span> Combining with the exponent term from the state predicitive distribution, then gives an exponent for the filtering distribution of the form <span class="math display">\[
( y_{t+1} - F_{t+1} x_{t+1} )^T\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}
x_{t+1} ) +( x_{t+1} - \mu_{t+1|t} )^T\Sigma_{t+1|t}^{-1} ( x_{t+1} -
\mu_{t+1|t} )
\]</span> Now we define the de-meaned state and innovations vectors,<br>
<span class="math display">\[
\tilde{x}_{t+1} = x_{t+1} - \mu_{t+1|t} \; \text{and} \; e_{t+1} = y_{t+1} - F_{t+1} \mu_{t+1|t}
\]</span> Using the usual completing the square trick we can re-write the exponent as <span class="math display">\[
( e_{t+1} - F_{t+1} \tilde{x}_{t+1} )^T\Sigma_{t+1}^{-1} ( e_{t+1} -
F_{t+1} \tilde{x}_{t+1} ) + \tilde{x}_{t+1}^T\Sigma_{t+1|t}^{-1}
\tilde{x}_{t+1}
\]</span> The sums of squares can be decomposed further as <span class="math display">\[
\tilde{x}_{t+1}^T\left(  F_{t+1}^T\Sigma_{t+1}^T+
\Sigma_{t+1|t}^{-1} \right)  \tilde{x}_{t+1} + 2 \tilde{x}_{t+1}^T\left(  F_{t+1}^T\Sigma_{t+1} e_{t+1} \right)  + e_{t+1}^T\Sigma_{t+1}^{-1} e_{t+1}
\]</span> The exponent is then a quadratic form implying that the vector <span class="math inline">\(\tilde{x}_{t+1}^T\)</span> is normal distributed with the appropriate mean and variance-covariance matrix. The definitions are given by <span class="math display">\[
\Sigma_{t+1|t+1} F_{t+1}^T\Sigma_{t+1} e_{t+1} \; \; \mathrm{and} \; \;
\Sigma_{t+1|t+1} = \left(  F_{t+1}^T\Sigma_{t+1}^{-1} F_{t+1} +
\Sigma_{t+1|t}^{-1} \right)  ^{-1}
\]</span> respectively. Hence, we obtain the identity <span class="math display">\[
\Sigma_{t+1|t+1} = \left(  F_{t+1}^T\Sigma_{t+1}^TF_{t+1} + \Sigma_{t+1|t}^{-1} \right)  ^{-1} = ( I - K_{t+1} F_{t+1} ) \Sigma_{t+1|t}
\]</span> where <span class="math inline">\(K_{t+1} = \Sigma_{t+1|t} F_{t+1}^T\left(  F_{t+1} \Sigma_{t+1|t}^{-1} F_{t+1}^T+ \Sigma_{t+1} \right)^{-1}\)</span> is the Kalman gain matrix.</p>
<p>The mean of the <span class="math inline">\(\tilde{x}_{t+1}^T = x_{t+1} - \mu_{t+1|t}\)</span> distribution is then <span class="math inline">\(K_{t+1}e_{t+1}\)</span>. Un de-meaning the vector, we have <span class="math inline">\(x_{t+1} = \tilde{x}_{t+1} + \mu_{t+1|t} =K_{t+1}e_{t+1}\)</span> leads to the following distributional result <span class="math display">\[
p( x_{t+1}|y^{t+1}) \sim\mathcal{N}\left(  \mu_{t+1|t+1},\Sigma_{t+1|t+1}%
\right)  ,
\]</span> The moments for the next filtering distribution are given by the classic recursions <span class="math display">\[\begin{align*}
\mu_{t+1|t+1}  &amp;  =\mu_{t+1|t}+K_{t+1}\left(  y_{t+1}-F_{t+1}\mu_{t+1|t}
\right) \\
\Sigma_{t+1|t+1}  &amp;  =\left(  I-K_{t+1}F_{t+1}\right)  \Sigma_{t+1|t}\text{.}%
\end{align*}\]</span> There are two other distributions to compute: the data predictive <span class="math inline">\(p( y_{t+1} | y^t )\)</span> and the state smoothing distribution <span class="math inline">\(p( x_t | y^{t+1} )\)</span>. These are derived as follows.</p>
<p>The data predictive <span class="math inline">\(p(y_{t+1}|y^t)\)</span> is determined from the observation equation and the state predictive distribution as follows <span class="math display">\[\begin{align*}
y_{t+1}  &amp; =F_{t+1}x_{t+1}+\varepsilon_{t+1} \; \text{with} \;   \varepsilon_{t+1}\sim\mathcal{N}\left( 0,\Sigma_{t+1}\right) \\
p( x_{t+1} | y^t ) &amp; \sim \mathcal{N} \left ( \mu_{t+1|t} , \Sigma_{t+1|t} \right )  
\end{align*}\]</span> Then substituting we have a predictive distribution for the next observation of the form <span class="math display">\[
p( y_{t+1}|y^{t}) \sim\mathcal{N}\left(  F_{t+1}\mu_{t+1|t},F_{t+1}%
\Sigma_{t+1|t} F_{t+1}+\Sigma_{t+1} \right)  \text{.}%
\]</span> The state smoothing distribution <span class="math inline">\(p(x_t | y^{t+1} )\)</span> is determined from the joint distribution, <span class="math inline">\(p( x_{t} , x_{t+1} | y^{t} )\)</span> as follows. First, factorise this joint distribution as <span class="math display">\[
p( x_{t+1} , x_{t} | y^{t} ) = p( x_{t+1} | x_{t} ) p( x_{t} | y^{t} )
\]</span> Then calculate the conditional posterior distribution, <span class="math inline">\(p( x_{t+1} | x_{t} , y^{t+1} )\)</span> by Bayes rule as <span class="math display">\[
p(x_{t+1}|x_{t},y^{t+1}) = \frac{p\left(  y_{t+1}|x_{t+1}\right)
p(x_{t+1}|x_{t})p(x_{t}|y^{t})}{p(x_{t+1}|y^{t})}%
\]</span> Now, we can view the system as having two observations on <span class="math inline">\(x_{t+1}\)</span>, namely <span class="math display">\[\begin{align*}
x_{t+1} &amp;  =F_{t+1}x_{t}+\Sigma_{t+1}^{x}\epsilon_{t+1}^{x}\\
x_{t} &amp;  =\mu_{t|t}+\Sigma_{t|t}^{\frac{1}{2}}\hat{\epsilon}_{t}%
\end{align*}\]</span> where the errors <span class="math inline">\(\epsilon_{t+1}^{x},\hat{\epsilon}_{t}\)</span> are independent.</p>
<p>This leads to a joint posterior with an exponent that is proportional to <span class="math display">\[
(x_{t+1}-G_{t+1}x_{t})^T\left(  \Sigma
_{t+1}^{x}\right)  ^{-1}(x_{t+1}-G_{t+1}x_{t})- (x_{t}-\mu
_{t|t})^T\Sigma_{t|t}^{-1}(x_{t}-\mu_{t|t})
\]</span> The first term comes from the state evolution and the second from the current filtering posterior. Completing the square gives <span class="math display">\[
(x_{t+1}-G_{t+1}x_{t})^T\left(  \Sigma_{t+1}^{x}\right)  ^{-1}%
(x_{t+1}-G_{t+1}x_{t})+(x_{t}-\mu_{t|t})^T\Sigma_{t|t}^{-1}(x_{t}%
-\mu_{t|t})
\]</span> <span class="math display">\[
=(x_{t+1}-\mu_{t+1|t})^T\Sigma_{t|t}^{-1}(x_{t+1}-\mu_{t+1|t}%
)+(x_{t}-\mu_{t|t+1})^T\Sigma_{t|t+1}^{-1}(x_{t}-\mu_{t|t+1})
\]</span> which leads to the smoothed state moments <span class="math display">\[\begin{align*}
\mu_{t|t+1} &amp;  =\Sigma_{t|t+1}\left(  \Sigma_{t|t}\mu_{t|t}+F_{t+1}^T\left(  \Sigma_{t+1}^{x}\right)  ^{-1}x_{t+1}\right)  \\
\Sigma_{t|t+1} &amp;  =F_{t+1}^T\left(  \Sigma_{t+1}^{x}\right)^{-1}F_{t+1}+\Sigma_{t|t}^{-1}%
\end{align*}\]</span> The Kalman filter recursions then follow by induction.</p>
<div id="exm-kalman" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.4 (Kalman Filter for Robot Localization)</strong></span> The Kalman filter is a powerful tool for estimating the state of a system, given noisy observations. It is used in a wide range of applications, from tracking the position of a robot to estimating the state of a financial market. The Kalman filter is particularly useful when the state of the system is not directly observable, and must be inferred from noisy measurements.</p>
<p>Often KF is used for localization problem: given noisy measurements about the position of a robot and the motion model of the robot, the Kalman filter can estimate the true position of the robot. The Kalman filter is a recursive algorithm that estimates the state of a system at each time step, based on the state estimate from the previous time step and a new observation. We will use the language of state-space models in this example and will use the notation <span class="math inline">\(x_t\)</span> to denote the state of the system at time <span class="math inline">\(t\)</span> (parameter we are trying to estimate), and <span class="math inline">\(y_t\)</span> to denote the observation at time <span class="math inline">\(t\)</span> (observed data). The state-space model is given by <span class="math display">\[
\begin{aligned}
x_{t+1} &amp; = A x_t + w,\quad w \sim N(0,Q)\\
y_t &amp;=G x_t + \nu, \quad \nu \sim N(0,R)\\
x_0 &amp; \sim N(\hat{x}_0, \Sigma_0),
\end{aligned}
\]</span> where <span class="math inline">\(A\)</span> is the state transition matrix, <span class="math inline">\(G\)</span> is the observation matrix, <span class="math inline">\(w\)</span> is the process noise, and <span class="math inline">\(\nu\)</span> is the observation noise. The process noise and observation noise are assumed to be independent and normally distributed with zero mean and covariance matrices <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>, respectively. The initial state <span class="math inline">\(x_0\)</span> is assumed to be normally distributed with mean <span class="math inline">\(\hat{x}_0\)</span> and covariance matrix <span class="math inline">\(\Sigma_0\)</span>. The Kalman filter provides a recursive algorithm for estimating the state of the system at each time step, based on the state estimate from the previous time step and a new observation. The state estimate is normal with mean <span class="math inline">\(\hat{x}_t\)</span> and the covariance matrix <span class="math inline">\(\Sigma_t\)</span>. The Kalman filter equations are given by <span class="math display">\[
\begin{aligned}
\hat{x}_{t+1} &amp;= A \hat{x}_t + K_{t} (y_t - G \hat{x}_t) \\
K_{t} &amp;= A \Sigma_t G^T (G \Sigma_t G^T + R)^{-1}\\
\Sigma_{t+1} &amp;= A \Sigma_t A^T - K_{t} G \Sigma_t A^T + Q
\end{aligned}
\]</span> Kalman filter performs a multivariate normal-normal update using <span class="math inline">\(N(A \hat{x}_t,A \Sigma_t A^T)\)</span> as prior and <span class="math inline">\(N(y_t, G \Sigma_t G^T + R)\)</span> as likelihood. The posterior distribution is <span class="math inline">\(N(\hat{x}_{t+1}, \Sigma_{t+1})\)</span>. Matrix <span class="math inline">\(K_{t}\)</span> is called the Kalman gain and provides a weight on the residual between observed and prior <span class="math inline">\(y_t - G \hat{x}_t\)</span> in the update.</p>
<p>Assume our robot starts at <span class="math inline">\(\hat x_0 = (0.2,-0.2)\)</span> (x-y Cartesian coordinates) and initial covariance is <span class="math display">\[
\Sigma_0 = \begin{bmatrix} 0.4 &amp; 0.3 \\ 0.3 &amp; 0.45 \end{bmatrix}.
\]</span> The prior distribution of the robot’s position can be visualized in R with a contour plot.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mnormt)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>xhat <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.2</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.4</span>, <span class="fl">0.3</span>, </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">0.3</span>, <span class="fl">0.45</span>), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">4</span>,<span class="at">length=</span><span class="dv">151</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">2</span>,<span class="at">length=</span><span class="dv">151</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x1,x2, <span class="at">mean=</span>xhat, <span class="at">varcov=</span>Sigma) </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dmnorm</span>(<span class="fu">cbind</span>(x1,x2), mean,varcov)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">outer</span>(x1,x2, f)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>mycols <span class="ot">&lt;-</span> <span class="fu">topo.colors</span>(<span class="dv">100</span>,<span class="fl">0.5</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1,x2,z, <span class="at">col=</span>mycols, <span class="at">main=</span><span class="st">"Prior density"</span>,</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">1</span>]), <span class="at">ylab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">2</span>]))</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1,x2,z, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">pch=</span><span class="dv">19</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.1</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)), <span class="at">adj =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/kf-prior-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now I get readings from GPS <span class="math inline">\(y_0 = (2.3, -1.9)\)</span> and I know from the manufacturer that the GPS has a covariance matrix of <span class="math inline">\(R = 0.5\Sigma_0\)</span>. We assume the measurement matrix <span class="math inline">\(G\)</span> to be identity matrix, thus <span class="math display">\[
y_t = Gx_t + \nu_t = x_t + \nu, \quad \nu \sim N(0, R).
\]</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> Sigma</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>z2 <span class="ot">&lt;-</span> <span class="fu">outer</span>(x1,x2, f, <span class="at">mean=</span><span class="fu">c</span>(<span class="fl">2.3</span>, <span class="sc">-</span><span class="fl">1.9</span>), <span class="at">varcov=</span>R)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1, x2, z2, <span class="at">col=</span>mycols, <span class="at">main=</span><span class="st">"Sensor density"</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z2, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">2.3</span>, <span class="sc">-</span><span class="fl">1.9</span>, <span class="at">pch=</span><span class="dv">19</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.2</span>, <span class="sc">-</span><span class="fl">1.9</span>, <span class="at">labels =</span> <span class="st">"y"</span>, <span class="at">adj =</span> <span class="dv">1</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2,z, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">pch=</span><span class="dv">19</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.1</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)), <span class="at">adj =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/kf-measure-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now we combine our initial guess about the location <span class="math inline">\(x_0\)</span> with the measure noisy location data <span class="math inline">\(y_0\)</span> to obtain posterior distribution of the location of the robot <span class="math inline">\(p(x\mid \hat x_0, \Sigma,R) = N(x\mid \hat x_f, \Sigma_f)\)</span> <span class="math display">\[
\begin{aligned}
\hat{x}_f &amp; = (\Sigma^{-1} + R^{-1})^{-1} (\Sigma^{-1} \hat{x} + R^{-1} y) \\
\Sigma_f &amp; = (\Sigma^{-1} + R^{-1})^{-1}
\end{aligned}
\]</span> Using the matrix inversion identity <span class="math display">\[
\begin{aligned}
(A^{-1} + B^{-1})^{-1} &amp; = A - A (A + B)^{-1}A = A (A + B)^{-1} B
\end{aligned}
\]</span> I can write the above as: <span class="math display">\[
\begin{aligned}
\hat{x}_f &amp; = (\Sigma - \Sigma (\Sigma + R)^{-1}\Sigma)(\Sigma^{-1} \hat{x} + R^{-1}y)\\
&amp; =\hat{x} - \Sigma (\Sigma + R)^{-1} \hat{x} + \Sigma R^{-1} y -\Sigma (\Sigma + R)^{-1}\Sigma R^{-1}y\\
&amp; = \hat{x} + \Sigma (\Sigma + R)^{-1} (y - \hat{x})\\
&amp; = (1.667, -1.333)\\
\Sigma_f &amp;= \Sigma - \Sigma (\Sigma + R)^{-1} \Sigma\\
&amp;=\left[\begin{array}{lll}
0.133 &amp; 0.10\\
0.100 &amp; 0.15
\end{array}
\right]
\end{aligned}
\]</span> In the more general case when <span class="math inline">\(G\)</span> is not the identity matrix I have <span class="math display">\[
\begin{aligned}
\hat{x}_f &amp; = \hat{x} + \Sigma G^T (G \Sigma G^T + R)^{-1} (y - G \hat{x})\\
\Sigma_f &amp;= \Sigma - \Sigma G^T (G \Sigma G^T + R)^{-1} G \Sigma
\end{aligned}
\]</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>G <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.4</span>, <span class="sc">-</span><span class="fl">1.9</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>xhatf <span class="ot">&lt;-</span> xhat <span class="sc">+</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">%*%</span> <span class="fu">solve</span>(G <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">+</span> R) <span class="sc">%*%</span> (y <span class="sc">-</span> G <span class="sc">%*%</span> xhat)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>Sigmaf <span class="ot">&lt;-</span> Sigma <span class="sc">-</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">%*%</span> <span class="fu">solve</span>(G <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">+</span> R) <span class="sc">%*%</span> G <span class="sc">%*%</span> Sigma</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>z3 <span class="ot">&lt;-</span> <span class="fu">outer</span>(x1, x2, f, <span class="at">mean=</span><span class="fu">c</span>(xhatf), <span class="at">varcov=</span>Sigmaf)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1, x2, z3, <span class="at">col=</span>mycols,</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">1</span>]), <span class="at">ylab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">2</span>]),</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">main=</span><span class="st">"Filtered density"</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z3, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(xhatf[<span class="dv">1</span>], xhatf[<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">19</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(xhatf[<span class="dv">1</span>]<span class="sc">-</span><span class="fl">0.1</span>, xhatf[<span class="dv">2</span>],</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)[f]), <span class="at">adj =</span> <span class="dv">1</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>lb <span class="ot">&lt;-</span> <span class="fu">adjustcolor</span>(<span class="st">"black"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span>lb)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">col=</span>lb)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.1</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)), <span class="at">adj =</span> <span class="dv">1</span>, <span class="at">col=</span>lb)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z2, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span>lb)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">2.3</span>, <span class="sc">-</span><span class="fl">1.9</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">col=</span>lb)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.2</span>, <span class="sc">-</span><span class="fl">1.9</span>,<span class="at">labels =</span> <span class="st">"y"</span>, <span class="at">adj =</span> <span class="dv">1</span>, <span class="at">col=</span>lb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/kf-post-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Now I assume my robot moves according to the following model <span class="math display">\[
x_t = A x_{t-1} + w_t, \quad w_t \sim N(0, Q)
\]</span> with <span class="math display">\[
\begin{split}
A = \left( \begin{array}{cc}
1.2 &amp; 0.0 \\
0.0 &amp; -0.2
\end{array} \right),
\qquad
Q = 0.3 \Sigma
\end{split}
\]</span> Then the next location is normally distributed with the parameters <span class="math display">\[
\begin{split}
A= \left(\begin{array}{cc}
1.2 &amp; 0.0 \\
0.0 &amp; -0.2
\end{array}\right),
\qquad
Q = 0.3 \Sigma
\end{split}
\]</span> Here <span class="math inline">\(K = A \Sigma G^T (G \Sigma G^T + R)^{-1}\)</span> is so-called Kalman gain matrix.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">1.2</span>, <span class="dv">0</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>              <span class="dv">0</span>, <span class="sc">-</span><span class="fl">0.2</span>), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="fl">0.3</span> <span class="sc">*</span> Sigma</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>K <span class="ot">&lt;-</span> A <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">%*%</span> <span class="fu">solve</span>(G<span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(G) <span class="sc">+</span> R)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>xhatnew <span class="ot">&lt;-</span> A <span class="sc">%*%</span> xhat <span class="sc">+</span> K <span class="sc">%*%</span> (y <span class="sc">-</span> G <span class="sc">%*%</span> xhat)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>Sigmanew <span class="ot">&lt;-</span> A <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(A) <span class="sc">-</span> K <span class="sc">%*%</span> G <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> <span class="fu">t</span>(A) <span class="sc">+</span> Q</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>z4 <span class="ot">&lt;-</span> <span class="fu">outer</span>(x1,x2, f, <span class="at">mean=</span><span class="fu">c</span>(xhatnew), <span class="at">varcov=</span>Sigmanew)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(x1, x2, z4, <span class="at">col=</span>mycols,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">1</span>]), <span class="at">ylab=</span><span class="fu">expression</span>(<span class="st">'x'</span>[<span class="dv">2</span>]),</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">main=</span><span class="st">"Predictive density"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z4, <span class="at">add=</span><span class="cn">TRUE</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(xhatnew[<span class="dv">1</span>], xhatnew[<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">19</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(xhatnew[<span class="dv">1</span>]<span class="sc">-</span><span class="fl">0.1</span>, xhatnew[<span class="dv">2</span>],</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)[new]), <span class="at">adj =</span> <span class="dv">1</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z3, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(xhatf[<span class="dv">1</span>], xhatf[<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">19</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(xhatf[<span class="dv">1</span>]<span class="sc">-</span><span class="fl">0.1</span>, xhatf[<span class="dv">2</span>], <span class="at">col=</span>lb, </span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)[f]), <span class="at">adj =</span> <span class="dv">1</span>)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">0.2</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">0.1</span>, <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">labels =</span> <span class="fu">expression</span>(<span class="fu">hat</span>(x)), <span class="at">adj =</span> <span class="dv">1</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(x1, x2, z2, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="fl">2.3</span>, <span class="sc">-</span><span class="fl">1.9</span>, <span class="at">pch=</span><span class="dv">19</span>, <span class="at">col=</span>lb)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="fl">2.2</span>, <span class="sc">-</span><span class="fl">1.9</span>,<span class="at">labels =</span> <span class="st">"y"</span>, <span class="at">adj =</span> <span class="dv">1</span>, <span class="at">col=</span>lb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/kf-predict-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
</div>
<p><strong>Forward filtering and Backwards Sampling</strong></p>
<p>The Kalman filtering recursions lead to a fully recursive algorithm for characterizing <span class="math inline">\(p( x| y)\)</span> known as FFBS (Forward filtering and Backwards Sampling). This provides the counterpart to the Baum-Welch algorithm developed earlier for HMMs. The details are as follows. The first step is to factorize the joint posterior distribution of the states via <span class="math display">\[\begin{align*}
p\left(  x|y^{T}\right)   &amp;  =p\left(  x_{T}|y^{T}\right)  \prod_{t=1}%
^{T-1}p\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\right)  \\
&amp;  =p\left(  x_{T}|y^{T}\right)  \prod_{t=1}^{T-1}p\left(  x_{t}|x_{t+1}%
,y^{t}\right) \label{ffbs-1}
\end{align*}\]</span> where we have used the fact that <span class="math inline">\(p\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\right)  =p\left(  x_{t}|x_{t+1},y^{T}\right)\)</span> by the Markov property (conditional on <span class="math inline">\(x_{t+1}\)</span>, <span class="math inline">\(x_{t}\)</span> is independent of all <span class="math inline">\(x_{t+2}\)</span>, etc.).</p>
<p>This forms the FFBS algorithm:&nbsp;forward-filtering, backward sampling algorithm for generating a block sample from <span class="math inline">\(p\left(  x|y^{T}\right)\)</span>. Filter forward using the Kalman recursions and obtain a sample from <span class="math inline">\(p( x_T|y^T)\)</span> and then backwards sample using <span class="math inline">\(p\left(x_{t}|x_{t+1},y^{t}\right)\)</span> to generate a block draw of <span class="math inline">\(x\)</span>. In what follows, we will often write <span class="math display">\[
p\left(  x|y^{T}\right)  \sim FFBS
\]</span> to denote that the FFBS algorithm can be used to generate a block draw.</p>
<p><strong>Backwards Sampling.</strong></p>
<p>The distribution of the final state given the data history <span class="math inline">\(p( x_{T} | y^{T} )\)</span> is given by the Kalman filter <span class="math display">\[
p( x_{T}|y^{T}) \sim\mathcal{N}\left(  \mu_{T},\Sigma_{T}\right)
\]</span> where <span class="math inline">\(( \mu_{T} , \Sigma_{T} )\)</span> are computed via the Kalman filter recursions. The second distribution comes from the factorization of <span class="math inline">\(p( x_{t}, x_{t+1} | y^{t} )\)</span> in the derivation of the Kalman filtering recursions. Hence, the conditional state filtering distribution given <span class="math inline">\(( x_{t+1} , y^t )\)</span> is <span class="math display">\[
p( x_{t} | x_{t+1} , y^{t} ) \sim\mathcal{N}\left(  \mu_{t|t+1} ,
\Sigma_{t|t+1} \right)
\]</span> where the one-step back smoothed moments are <span class="math display">\[\begin{align*}
\mu_{t|t+1}  &amp;  = \Sigma_{t|t+1} \left(  \Sigma_{t|t} \mu_{t|t} +
F_{t+1}^T\left(  \Sigma^{x}_{t+1} \right)  ^{-1} x_{t+1} \right) \\
\Sigma_{t|t+1}  &amp;  = F_{t+1}^T\left(  \Sigma^{x}_{t+1} \right)  ^{-1}
F_{t+1} + \Sigma_{t|t}^{-1}%
\end{align*}\]</span> as computed above. Then we can sequentially sample from this distribution.</p>
</section>
<section id="hmm-hidden-markov-models" class="level3">
<h3 class="anchored" data-anchor-id="hmm-hidden-markov-models">HMM: Hidden Markov Models</h3>
<p>The algorithms described in this section were originally developed by Baum and Welch <span class="citation" data-cites="baum1970maximization">Baum et al. (<a href="references.html#ref-baum1970maximization" role="doc-biblioref">1970</a>)</span> and <span class="citation" data-cites="viterbi1967error">Viterbi (<a href="references.html#ref-viterbi1967error" role="doc-biblioref">1967</a>)</span>. Baum developed original trading algorithms for Renaissance Technology which later became a multi-billion dollar hedge fund. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. They are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states. These algorithms are now widely used in many applications, including speech recognition, bioinformatics, and finance.</p>
<p>Viterbi, on the other hand, was one of the founders of what is now known as Qualcomm, a multi-billion dollar semiconductor and telecommunications equipment company. Viterbi’s algorithm is used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations.</p>
<p>Baum-Welch (1970) and Viterbi (1967) are the two famous discrete HMM algorithms.</p>
<p>Consider a model with a Hidden Chain or regime-switching variable</p>
<p><span class="math display">\[
y_{t} = \mu\left( x_{t}\right) + \sigma\left( x_{t}\right) \varepsilon _{t}\text{.}
\]</span></p>
<p>Suppose that <span class="math inline">\(x_{t}\)</span> is a finite state Markov chain with a time-homogeneous transition matrix P with entries <span class="math inline">\(\left\{ p_{ij}\right\}\)</span> which are given by</p>
<p><span class="math display">\[
p_{ij} = P\left( x_{t} = i | x_{t-1} = j, \theta\right) \text{.}
\]</span></p>
<p>We define the marginal filtering and smoothing distributions</p>
<p><span class="math display">\[
p_{i}^{t,t} = P\left( x_{t} = i | \theta, y^{t}\right) \text{ and } p_{i}^{t,T} = P\left( x_{t} = i | \theta, y^{T}\right)
\]</span></p>
<p>and the corresponding joint filtering and smoothing matrices:</p>
<p><span class="math display">\[
p_{ij}^{t,t} = P\left( x_{t-1} = i, x_{t} = j | \theta, y^{t}\right) \text{ and } p_{ij}^{t,T} = P\left( x_{t-1} = i, x_{t} = j | \theta, y^{T}\right) \text{.}
\]</span></p>
<p>The key to the algorithm is that we are just going to track the joint matrices, and then peel-off marginals from the rows and columns.</p>
<section id="forward-filtering" class="level4">
<h4 class="anchored" data-anchor-id="forward-filtering">Forward-filtering</h4>
<p>To derive the forward equations</p>
<p><span class="math display">\[
\begin{aligned}
p_{ij}^{t,t} &amp; = P\left( x_{t-1} = i, x_{t} = j | \theta, y^{t}\right) \propto p\left( y_{t}, x_{t-1} = i, x_{t} = j | \theta, y^{t-1}\right) \\
&amp; \propto p\left( y_{t} | x_{t} = j, \theta\right) p\left( x_{t} = j | x_{t-1} = i, \theta\right) p\left( x_{t-1} = i | \theta, y^{t-1}\right) \\
&amp; \propto p\left( y_{t} | x_{t} = j, \theta\right) p_{ij}\left( p_{i}^{t-1,t-1}\right) \text{.}
\end{aligned}
\]</span></p>
<p>This shows how to compute today’s filtering distribution given the likelihood. The advantage of this is that it only requires matrix multiplication.</p>
</section>
<section id="backward-sampling" class="level4">
<h4 class="anchored" data-anchor-id="backward-sampling">Backward-sampling</h4>
<p>The result of the forward-filtering is the final observation <span class="math inline">\(p_{ij}^{T,T}\)</span>. Like in the previous section, we can then filter in reverse to compute <span class="math inline">\(p_{ij}^{t,T}\)</span>, which is required for the MCMC algorithm. We have that</p>
<p><span class="math display">\[
\begin{aligned}
p_{ij}^{t,T} &amp; = p\left( x_{t-1} = i, x_{t} = j | \theta, y^{T}\right) \\
&amp; \propto p\left( x_{t-1} = i | x_{t} = j, \theta, y^{T}\right) p\left( x_{t} = j | \theta, y^{T}\right) \\
&amp; \propto p\left( x_{t-1} = i | x_{t} = j, \theta, y^{t}\right) p\left( x_{t} = j | \theta, y^{T}\right) \\
&amp; \propto \frac{p\left( x_{t-1} = i, x_{t} = j | \theta, y^{t}\right)}{p\left( x_{t} = j | \theta, y^{t}\right)} p\left( x_{t} = j | \theta, y^{T}\right) \\
&amp; \propto p_{ij}^{t,t} \frac{p_{j}^{t,T}}{p_{j}^{t,t}} \text{.}
\end{aligned}
\]</span></p>
<p>In deriving this, we have used the fact that</p>
<p><span class="math display">\[
p\left( x_{t-1} = i | x_{t} = j, \theta, y^{T}\right) \propto p\left( x_{t-1} = i | x_{t} = j, \theta , y^{t}\right)
\]</span></p>
<p>because conditional time <span class="math inline">\(t\)</span> information, the past transition is independent of the future. This is the discrete-state version of the FFBS algorithm.</p>
</section>
<section id="smoothing-forwards-and-backwards" class="level4">
<h4 class="anchored" data-anchor-id="smoothing-forwards-and-backwards">Smoothing: Forwards and Backwards</h4>
<p>Let <span class="math inline">\(y^T = \{y_1, \dots, y_T\}\)</span> be a sequence of random variables where the conditional distribution</p>
<p><span class="math display">\[
p(y^T | x^T) = p (y_1 | x_1 ) \prod_{t=2}^T p(y_t | x_t, y_{t-1})
\]</span></p>
<p>where we suppress the dependence of the mixture components on a parameter <span class="math inline">\(\theta\)</span>. The full smoothing distribution can be written</p>
<p><span class="math display">\[
p( x | y ) = p( x_T | y^T ) \prod_{t=1}^{T-1} p( x_t | x_{t+1} , \theta , y_{t+1} )
\]</span></p>
<p>Suppose <span class="math inline">\(\{x_t\}\)</span> follows a finite state Markov chain with initial distribution <span class="math inline">\(\pi_0\)</span> and transition probabilities</p>
<p><span class="math display">\[
Q_t(r,s) = Pr(x_t = s | x_{t-1} = r) \; \; \mathrm{and} \; \; P_t ( t, r ,s) = Pr(x_{t-1} = r, x_t = s | y_1^t)
\]</span></p>
<p>which we will compute sequentially. Let the current filtering distribution of the state be given by, for <span class="math inline">\(t &gt; 0\)</span>,</p>
<p><span class="math display">\[
p_t(s) = Pr(x_t = s | y^t) \; \; \mathrm{and} \; \; A_t(x_{t-1}, x_t, y_t) = p(x_{t-1}, x_t, y_t | y^{t-1})
\]</span></p>
<p>By definition,</p>
<p><span id="eq-fwd"><span class="math display">\[
A_t(r,s,y_t) = \frac{p_{t-1}(r) Q_t(r,s)}{p(y_t | y_{t-1})}.
\tag{15.1}\]</span></span></p>
<p>where the marginal likelihood is given by</p>
<p><span class="math display">\[
p(y_t | y_{1}^{t-1}) = \sum_{r,s} A_t(r,s,y_t)
\]</span></p>
<p>The filtered transition distribution</p>
<p><span class="math display">\[
p_{trs} = A_t(r,s,y_t) / p(y_t | y^{t-1})
\]</span></p>
<p>The forward-backward recursions are used to efficiently compute the observed data likelihood</p>
<p><span class="math display">\[
p(y) = \sum_{ x } p(y | x)p( x )
\]</span></p>
<p>where <span class="math inline">\(x = ( x_1 , \ldots , x_T )\)</span>. We also need the posterior distribution <span class="math inline">\(p( x | y )\)</span> of the latent Markov chain given observed data. The recursions consist of a forward step that computes the distribution of the <span class="math inline">\(t\)</span>’th transition given all the data up to time <span class="math inline">\(t\)</span>, and a backward recursion that updates each distribution to condition on all observed data.</p>
<p>The forward recursion operates on the set of transition distributions, represented by a sequence of matrices <span class="math inline">\(P_t = ( p_{trs} )\)</span>. Computing</p>
<p><span class="math display">\[
p_t(s) = \sum_{r}p_{trs}
\]</span></p>
<p>sets up the next step in the recursion. The recursion is initialized by replacing <span class="math inline">\(p\)</span> with <span class="math inline">\(p^0\)</span> in equation <a href="#eq-fwd" class="quarto-xref">Equation&nbsp;<span>15.1</span></a>.</p>
<p>The observed data log-likelihood can be computed as</p>
<p><span class="math display">\[
\log p(y) = \log p(y_1) + \sum_{t=2}^T \log p(y_t | y^{t-1})
\]</span></p>
<p>With appropriate use of logarithms, <span class="math inline">\(p\)</span> and <span class="math inline">\(p(y_t | y^{t-1})\)</span> need only ever be evaluated on the log scale.</p>
<p>The stochastic version of the backward recursion simulates from</p>
<p><span class="math display">\[
p(x | y).
\]</span></p>
<p>Begin with the factorization</p>
<p><span class="math display">\[
\label{eq:bkwd} p(x | y) = p(x_T | y^T) \prod_{t=1}^{T-1} p(x_{t} | x_{t+1}^T, y).
\]</span></p>
<p>Then notice that, given <span class="math inline">\(x_{t+1}\)</span>, <span class="math inline">\(x_t\)</span> is conditionally independent of <span class="math inline">\(y_{t+1}^T\)</span> and all later <span class="math inline">\(x\)</span>’s. Thus</p>
<p><span class="math display">\[
p(x_t | x_{t+1}, y) = P(x_t = r | x_{t+1} = s, y^{t+1}) \propto p_{t+1rs}
\]</span></p>
<p>Therefore, if one samples <span class="math inline">\((x_{t-1}, x_t)\)</span> from the discrete bivariate distribution given by <span class="math inline">\(P_t\)</span> and then repeatedly samples <span class="math inline">\(x_t\)</span> from a multinomial distribution proportional to column <span class="math inline">\(x_{t+1}\)</span> of <span class="math inline">\(P_{t+1}\)</span> then <span class="math inline">\(x=(x_1, \ldots , x_T)\)</span> is a draw from <span class="math inline">\(p(x|y)\)</span>.</p>
</section>
</section>
<section id="mixture-kalman-filter" class="level3">
<h3 class="anchored" data-anchor-id="mixture-kalman-filter">Mixture Kalman filter</h3>
<p>We can also introduce a <span class="math inline">\(\lambda_t\)</span> state variable and consider a system</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp; = F_{\lambda_t} x_t + D_{\lambda_t} \epsilon_t  \\
x_t &amp; = G_{\lambda_t} x_{t-1} + B_{\lambda_t} v_t
\end{aligned}
\]</span></p>
<p>The Kalman filter gives moments of the state filtering distribution</p>
<p><span class="math display">\[
x_t | \lambda^t , y^t \sim \mathcal{N} \left ( \mu_{t|t} , \Sigma_{t|t} \right )
\]</span></p>
<p>Here we assume that the iid auxiliary state variable shocks <span class="math inline">\(\lambda_t \sim p( \lambda_t )\)</span>.</p>
<p>First we marginalize over the state variable <span class="math inline">\(x_t\)</span>. Then we can track the sufficient statistics for the hidden <span class="math inline">\(z_t\)</span> variable dynamically in time, namely <span class="math inline">\(\left ( z_{t|t} , S_t \right )\)</span>, just the Kalman filter moments, in the conditionally Gaussian and discrete cases <span class="citation" data-cites="lindgren1978markov">Lindgren (<a href="references.html#ref-lindgren1978markov" role="doc-biblioref">1978</a>)</span>. Then we re-sample <span class="math inline">\(( z_{t|t} , S_t  , \theta )^{(i)}\)</span> particles.</p>
</section>
<section id="regime-switching-models" class="level3">
<h3 class="anchored" data-anchor-id="regime-switching-models">Regime Switching Models</h3>
<p>The general form of a continuous-time regime switching model is</p>
<p><span class="math display">\[
dy_{t}=\mu\left(  \theta,x_{t},y_{t}\right)  dt+\sigma\left(  \theta
,x_{t},y_{t}\right)  d B_{t}%
\]</span></p>
<p>where <span class="math inline">\(x_{t}\)</span> takes values in a discrete space with transition matrix <span class="math inline">\(P_{ij}\left(  t\right)\)</span> with parameters <span class="math inline">\(\theta=\left(  \theta_{1},\ldots,\theta_{J}\right)\)</span>. Common specifications assume the drift and diffusion coefficients are parametric functions and the parameters switch over time. In this case, it is common to write the model as</p>
<p><span class="math display">\[
dy_{t}=\mu\left(  \theta_{x_{t}},y_{t}\right)  dt+\sigma\left(  \theta_{x_{t}%
},y_{t}\right)  d B_{t}\text{.}%
\]</span></p>
<p>Scott (2002) provides a fast MCMC algorithm for state filtering by adapting the FFBS algorithm. Time discretized the model is:</p>
<p><span class="math display">\[
y_{t}=\mu\left(  \theta_{x_{t}},y_{t-1}\right)  +\sigma\left(  \theta_{x_{t}%
},y_{t-1}\right)  \varepsilon_{t}\text{.}%
\]</span></p>
<p>Note that we use the standard notation from discrete-time models where the time index on the Markov state is equal to the current observation. The discrete-time transition probabilities are</p>
<p><span class="math display">\[
p_{ij}=P\left(  x_{t}=i|x_{t-1}=j\right)
\]</span></p>
<p>and we assume, apriori, that the transition functions are time and state invariant. The joint likelihood is given by</p>
<p><span class="math display">\[
p\left(  y| x,\theta\right)  = \prod_{t=1}^{T} p\left(  y_{t}|y_{t-1}%
,x_{t-1},\theta\right)\]</span></p>
<p>where <span class="math inline">\(p\left(  y_{t}|y_{t-1},x_{t-1},\theta\right)  =N\left(  \mu\left(
\theta_{x_{t-1}},y_{t-1}\right)  ,\sigma^{2}\left(  \theta_{x_{t-1}}%
,y_{t-1}\right)  \right)\)</span>.</p>
<p>Clifford-Hammersley implies that the complete conditionals are given by <span class="math inline">\(p\left(  \theta| x ,s , y \right)\)</span>, <span class="math inline">\(p\left(  s | x , \theta, y \right)\)</span>, and <span class="math inline">\(p\left(  x | s ,\theta, y \right)\)</span>. Conditional on the states and the transition probabilities, updating the parameters is straightforward. Conditional on the states, the transition matrix has a Dirchlet distribution, and updating this is also straightforward. To update the states use FFBS.</p>
<p>An important component of regime switching models is the prior distribution. Regime switching models (and most mixture models) are not formally identified. For example, in all regime switching models, there is a labeling problem: there is no unique way to identify the states. A common approach to overcome this identification issue is to order the parameters.</p>
<section id="changepoint-problems" class="level4">
<h4 class="anchored" data-anchor-id="changepoint-problems">Changepoint Problems</h4>
<p><span class="citation" data-cites="smith1975bayesian">Smith (<a href="references.html#ref-smith1975bayesian" role="doc-biblioref">1975</a>)</span> introduced the single changepoint problem from a Bayesian perspective. Consider a sequence of random variables <span class="math inline">\(y_{1} , \ldots, y_{T}\)</span> which has a change-point at time <span class="math inline">\(\tau\)</span> in the sense that</p>
<p><span class="math display">\[
y_t | \theta_{k} \sim    \left\{
\begin{array}[c]{l}
p( y | \theta_{1} ) \; \; \; \; \; \; \mathrm{for} \; \; 1 \leq i \leq\tau \\
p( y | \theta_{2} ) \; \; \; \; \; \; \mathrm{for} \; \; \tau+1 \leq
i \leq T
\end{array}  \right\}\]</span></p>
<p>This can be rewritten as a state space model</p>
<p><span class="math display">\[
y_t = \theta_{ x_t } + \sigma_{ x_t } \epsilon_t
\]</span></p>
<p>where <span class="math inline">\(x_t\)</span> has a Markov transition evolution.</p>
<p>An idea that appears to be under-exploited is that of <em>“model reparametrisation”</em>. The multiple change-point problem, which is computationally expensive if approached directly, has a natural model reparametrisation that makes the implementation of MCMC methods straightforward (see <span class="citation" data-cites="chib1998estimation">Chib (<a href="references.html#ref-chib1998estimation" role="doc-biblioref">1998</a>)</span>). Specifically, suppose that the data generating process <span class="math inline">\(y^{T} = \{ y_{1} , \ldots, y_{T} \}\)</span> is given by a sequence of conditionals <span class="math inline">\(f ( y_{t} | y^{t-1} , \theta_{k} )\)</span> for parameters <span class="math inline">\(\theta_{k}\)</span> that change at unknown change-points <span class="math inline">\(\{ \tau_{1} , \ldots,
\tau_{k} \}\)</span>.</p>
<p>The model parameterization is based on using a hidden Markov state space model with a vector of latent variables <span class="math inline">\(s_{t}\)</span> where <span class="math inline">\(s_{t}
= k\)</span> indicates that <span class="math inline">\(y_{t}\)</span> is drawn from <span class="math inline">\(p ( y_{t} | y^{t-1} ,
\theta_{k} )\)</span>. Let the prior distribution on the <span class="math inline">\(s_{t}\)</span>’s have transition matrix where <span class="math inline">\(p_{ij} = P \left(  s_{t} = j | s_{t-1} = i
\right)\)</span> is the probability of jumping regimes. With this model parameterization the <span class="math inline">\(k\)</span>th change occurs at <span class="math inline">\(\tau_{k}\)</span> if <span class="math inline">\(s_{
\tau_{k} } = k\)</span> and <span class="math inline">\(s_{ \tau_{k} + 1 } = k + 1\)</span>. The reparameterisation automatically enforces the order constraints on the change-points and is it very easy to perform MCMC analysis on the posterior distribution. This provides a more efficient strategy for posterior computation. MCMC analysis of the <span class="math inline">\(s_{t}\)</span>’s is straightforward and the posterior for the <span class="math inline">\(\tau_{k}\)</span>’s can be obtained by inverting the definition above. Hence</p>
<p><span class="math display">\[
p( \tau = t | y ) = p( x_t =1 | y )
\]</span></p>
<p>The alternative is single state updating conditional on <span class="math inline">\(\tau\)</span> which is slow for finding the multiple-changeponts.</p>
</section>
</section>
</section>
<section id="particle-learning-for-general-mixture-models" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="particle-learning-for-general-mixture-models"><span class="header-section-number">15.6</span> Particle Learning for General Mixture Models</h2>
<p>Particle learning (PL) offers a powerful and flexible approach for sequential inference in general mixture models. Unlike traditional MCMC methods, which require repeated passes over the entire dataset and can be computationally demanding, particle learning operates in an online fashion. This means it can efficiently update inference as new data arrives, making it particularly well-suited for large or high-dimensional datasets and real-time applications.</p>
<p>The particle learning framework is designed to efficiently and sequentially learn from a broad class of mixture models. At its core, the approach models data as arising from a mixture distribution:</p>
<p><span class="math display">\[
f(z) = \int k(z;\theta) d G(\theta)
\]</span></p>
<p>where <span class="math inline">\(G\)</span> is a discrete mixing measure and <span class="math inline">\(k(z;\theta)\)</span> is a kernel parameterized by <span class="math inline">\(\theta\)</span>. The generality of this formulation allows PL to be applied to a wide variety of models, including finite mixture models, Dirichlet process mixtures, Indian buffet processes, and probit stick-breaking models. This flexibility is a significant advantage, as it enables practitioners to use PL across many settings without needing to redesign the inference algorithm for each new model structure.</p>
<p>In addition to its generality, particle learning provides an alternative to MCMC for tasks such as online model fitting, marginal likelihood estimation, and posterior cluster allocation. Its sequential nature makes it particularly attractive for streaming data and scenarios where computational resources are limited.</p>
<p>A general mixture model can be described by three components: a likelihood, a transition equation for latent allocations, and a prior over parameters. Specifically,</p>
<ul>
<li>The likelihood is given by <span class="math inline">\(p(y_{t+1} | k_{t+1}, \theta)\)</span>, representing the probability of the next observation given the current allocation and parameters.</li>
<li>The transition equation <span class="math inline">\(p(k_{t+1} | k^t, \theta)\)</span> governs how the latent allocation variables evolve over time, where <span class="math inline">\(k^t = \{k_1,\ldots, k_t\}\)</span> denotes the history of allocations.</li>
<li>The parameter prior <span class="math inline">\(p(\theta)\)</span> encodes prior beliefs about the mixture component parameters.</li>
</ul>
<p>This structure can be expressed in a state-space form:</p>
<p><span class="math display">\[\begin{align}
y_{t+1} &amp;= f(k_{t+1}, \theta) \\
k_{t+1} &amp;= g(k^t, \theta)
\end{align}\]</span></p>
<p>where the first equation is the observation model and the second describes the evolution of the latent allocation states.</p>
<p>The mixture modeling framework described above is closely related to hidden Markov models (HMMs). In this context, the observed data <span class="math inline">\(y_t\)</span> are assumed to be generated from a mixture, with allocation variables <span class="math inline">\(k_t\)</span> determining which mixture component is responsible for each observation. The parameters <span class="math inline">\(\theta_{k_t}\)</span> for each component are drawn from the mixing measure <span class="math inline">\(G\)</span>. This structure allows for both standard mixture models, where each observation is assigned to a single component, and more general latent feature models, where multivariate allocation variables <span class="math inline">\(k_t\)</span> allow an observation to be associated with multiple components simultaneously.</p>
<p>A central concept in particle learning is the essential state vector <span class="math inline">\(\mathcal{Z}_t\)</span>, which is tracked over time. This vector is constructed to be sufficient for sequential inference, meaning that it contains all the information needed to compute the posterior predictive distribution for new data, update the state as new observations arrive, and learn about the underlying parameters:</p>
<ul>
<li>Posterior predictive: <span class="math inline">\(p(y_{t+1} | \mathcal{Z}_t)\)</span></li>
<li>Posterior updating: <span class="math inline">\(p(\mathcal{Z}_{t+1} | \mathcal{Z}_t, y_{t+1})\)</span></li>
<li>Parameter learning: <span class="math inline">\(p(\theta | \mathcal{Z}_{t+1})\)</span></li>
</ul>
<section id="the-particle-learning-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-particle-learning-algorithm">The Particle Learning Algorithm</h3>
<p>Particle learning approximates the posterior distribution <span class="math inline">\(p(\mathcal{Z}_t | y^t)\)</span> with a set of equally weighted particles <span class="math inline">\(\{\mathcal{Z}_t^{(i)}\}_{i=1}^N\)</span>. When a new observation <span class="math inline">\(y_{t+1}\)</span> becomes available, the algorithm proceeds in two main steps:</p>
<ol type="1">
<li><strong>Resample:</strong> The current set of particles is resampled with weights proportional to the predictive likelihood <span class="math inline">\(p(y_{t+1} | \mathcal{Z}_t^{(i)})\)</span>. This step focuses computational effort on the most plausible states given the new data.</li>
<li><strong>Propagate:</strong> Each resampled particle is then propagated forward by sampling from the transition distribution <span class="math inline">\(p(\mathcal{Z}_{t+1} | \mathcal{Z}_t^{(i)}, y_{t+1})\)</span>, thus updating the state to incorporate the new observation.</li>
</ol>
<p>This two-step process is grounded in Bayes’ theorem, where the resampling step corresponds to updating the posterior with the new data, and the propagation step advances the state according to the model dynamics. After these steps, the set of particles provides an updated approximation to the posterior <span class="math inline">\(p(\mathcal{Z}_{t+1} | y^{t+1})\)</span>.</p>
<p>One important distinction between particle learning and standard particle filtering methods is that the essential state vector <span class="math inline">\(\mathcal{Z}_t\)</span> does not necessarily need to include the full history of allocation variables <span class="math inline">\(k^t\)</span> to be sufficient for inference. This makes PL both more efficient and more flexible than many existing particle filtering approaches for mixture models. Furthermore, the order of resampling and propagation steps is reversed compared to standard filters, which helps mitigate particle degeneracy and improves performance in mixture modeling contexts.</p>
<p>Particle learning also provides an efficient mechanism for sampling from the full posterior distribution of the allocation vector <span class="math inline">\(p(k^t | y^t)\)</span>. This is achieved using a backwards uncertainty update, which allows for the recovery of smoothed samples of the allocation history. For each particle, and for each time step in reverse order, the allocation variable <span class="math inline">\(k_r\)</span> is sampled with probability proportional to the product of the likelihood and the prior for that allocation, given the state vector. This results in an algorithm with computational complexity linear in the number of particles, making it practical even for large datasets.</p>
<p>The particle learning framework is applicable to a wide range of density estimation problems involving mixtures of the form</p>
<p><span class="math display">\[
f(y;G) = \int k(y ; \theta) dG(\theta)
\]</span></p>
<p>There are many possible choices for the prior on the mixing measure <span class="math inline">\(G\)</span>. Common examples include finite mixture models, which use a finite number of components; Dirichlet process mixtures, which allow for an infinite number of components via a stick-breaking construction; beta two-parameter processes; and kernel stick-breaking processes. Each of these priors offers different modeling flexibility and computational properties, and the choice depends on the specific application and desired level of model complexity.</p>
<p>In some cases, it is useful to consider a collapsed state-space model, where the predictive distribution for a new observation is expressed as an expectation over the mixing measure <span class="math inline">\(G\)</span> given the current state vector:</p>
<p><span class="math display">\[
\mathbb{E}[f(y_{t+1};G) | \mathcal{Z}_t] = \int k(y_{t+1};\theta) d \mathbb{E}[G(\theta) | \mathcal{Z}_t]
\]</span></p>
<p>If <span class="math inline">\(t\)</span> observations have been allocated to <span class="math inline">\(m_t\)</span> mixture components, the posterior expectation of <span class="math inline">\(G\)</span> can be written as a weighted sum of the base measure and point masses at the component parameters. The predictive density then combines contributions from both new and existing components, weighted according to their posterior probabilities.</p>
<p>Particle learning offers a versatile and efficient framework for sequential inference in general mixture models. By representing the posterior with a set of particles and updating these particles as new data arrives, PL enables real-time model fitting, efficient posterior allocation, and flexible density estimation across a wide range of mixture modeling scenarios. Its ability to handle both finite and infinite mixture models, as well as latent feature models, makes it a valuable tool for modern statistical analysis.</p>
<div id="exm-poisson-mixture" class="theorem example">
<p><span class="theorem-title"><strong>Example 15.5 (Particle Learning for Poisson Mixture Models)</strong></span> We will implement Particle Learning (PL) for a finite mixture of Poisson distributions based on the example from <span class="citation" data-cites="carvalho2010particlea">Carvalho et al. (<a href="references.html#ref-carvalho2010particlea" role="doc-biblioref">2010</a>)</span>. This example follows Algorithm 1 for finite mixture models from Section 2.1 of the paper.</p>
<p>We generate data from a mixture of two Poisson distributions (<span class="math inline">\(\lambda_1=2\)</span> with weight 0.7, <span class="math inline">\(\lambda_2\)</span>=10 with weight 0.3).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">8</span>) <span class="co"># Ovi</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate data (500 observations)</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>n_obs <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>true_z <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, n_obs, <span class="at">replace=</span><span class="cn">TRUE</span>, <span class="at">prob=</span><span class="fu">c</span>(<span class="fl">0.7</span>, <span class="fl">0.3</span>))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(true_z <span class="sc">==</span> <span class="dv">1</span>, <span class="fu">rpois</span>(n_obs, <span class="dv">2</span>), <span class="fu">rpois</span>(n_obs, <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot two empirical density plots for each mixture component. Put them in one plot </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(y[true_z <span class="sc">==</span> <span class="dv">1</span>]), <span class="at">xlab =</span> <span class="st">"y"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3</span>), <span class="at">main=</span><span class="st">""</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(y[true_z <span class="sc">==</span> <span class="dv">2</span>]), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(y),<span class="at">xlab =</span> <span class="st">"y"</span>, <span class="at">col =</span> <span class="st">"purple"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3</span>))</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"Component 1 (Lambda=2)"</span>, <span class="st">"Component 2 (Lambda=10)"</span>, <span class="st">"Mixture Density"</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"purple"</span>), <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The code below implements the Particle Learning algorithm using the following steps: 1 Particle Initialization: - Each particle tracks sufficient statistics: - <code>s</code>: Sum of observations per component - <code>n</code>: Count of observations per component 2. PL Algorithm: - Resample: Particles are weighted by the posterior predictive probability of the next observation - Propagate: For each particle: - Compute component allocation probabilities - Sample component assignment - Update sufficient statistics - Learn: Store posterior estimates of <span class="math inline">\(\lambda\)</span> parameters and mixture weights</p>
<p>The key features of this implementation are the use of posterior predictive with Poisson-Gamma conjugacy and allocation of probabilities by combining prior weights and likelihood.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model parameters</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">2</span>  <span class="co"># Number of components</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Gamma prior shape parameters</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)   <span class="co"># Gamma prior rate parameters</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Dirichlet prior parameters</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>n_particles <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># Number of particles</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize particles</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>particles <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>n_particles, <span class="cf">function</span>(i) {</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),    <span class="co"># Sufficient statistics (sum of y in each component)</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>))    <span class="co"># Counts per component</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Store posterior samples</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>posterior_lambda <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_obs, <span class="at">ncol =</span> m)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>posterior_weights <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_obs, <span class="at">ncol =</span> m)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Particle Learning algorithm</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_obs) {</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> y[t]</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>  log_weights <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_particles)</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1. Compute weights using posterior predictive</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_particles) {</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    total_obs <span class="ot">&lt;-</span> <span class="fu">sum</span>(particles[[i]]<span class="sc">$</span>n)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>    weight_components <span class="ot">&lt;-</span> (particles[[i]]<span class="sc">$</span>n <span class="sc">+</span> gamma) <span class="sc">/</span> (total_obs <span class="sc">+</span> <span class="fu">sum</span>(gamma))</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictive for each component (Poisson-Gamma)</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    pred_prob <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>m, <span class="cf">function</span>(j) {</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>      shape_post <span class="ot">&lt;-</span> alpha[j] <span class="sc">+</span> particles[[i]]<span class="sc">$</span>s[j]</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>      rate_post <span class="ot">&lt;-</span> beta[j] <span class="sc">+</span> particles[[i]]<span class="sc">$</span>n[j]</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>      <span class="fu">exp</span>(<span class="fu">dpois</span>(y_t, shape_post<span class="sc">/</span>rate_post, <span class="at">log =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>            <span class="fu">dgamma</span>(shape_post<span class="sc">/</span>rate_post, shape_post, rate_post, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>    log_weights[i] <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="fu">sum</span>(weight_components <span class="sc">*</span> pred_prob))</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Normalize weights</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>  max_logw <span class="ot">&lt;-</span> <span class="fu">max</span>(log_weights)</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="fu">exp</span>(log_weights <span class="sc">-</span> max_logw)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> weights <span class="sc">/</span> <span class="fu">sum</span>(weights)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2. Resample particles</span></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_particles, <span class="at">size =</span> n_particles, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> weights)</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>  particles <span class="ot">&lt;-</span> particles[idx]</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 3. Propagate state</span></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_particles) {</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute allocation probabilities</span></span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>    alloc_probs <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>m, <span class="cf">function</span>(j) {</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>      shape_post <span class="ot">&lt;-</span> alpha[j] <span class="sc">+</span> particles[[i]]<span class="sc">$</span>s[j]</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>      rate_post <span class="ot">&lt;-</span> beta[j] <span class="sc">+</span> particles[[i]]<span class="sc">$</span>n[j]</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>      log_prior <span class="ot">&lt;-</span> <span class="fu">log</span>(particles[[i]]<span class="sc">$</span>n[j] <span class="sc">+</span> gamma[j]) <span class="sc">-</span> <span class="fu">log</span>(<span class="fu">sum</span>(particles[[i]]<span class="sc">$</span>n) <span class="sc">+</span> <span class="fu">sum</span>(gamma))</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>      log_lik <span class="ot">&lt;-</span> <span class="fu">dpois</span>(y_t, shape_post<span class="sc">/</span>rate_post, <span class="at">log =</span> <span class="cn">TRUE</span>)</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>      <span class="fu">exp</span>(log_prior <span class="sc">+</span> log_lik)</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>    alloc_probs <span class="ot">&lt;-</span> alloc_probs <span class="sc">/</span> <span class="fu">sum</span>(alloc_probs)</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample component allocation</span></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>    k <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>m, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> alloc_probs)</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sufficient statistics</span></span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>    particles[[i]]<span class="sc">$</span>s[k] <span class="ot">&lt;-</span> particles[[i]]<span class="sc">$</span>s[k] <span class="sc">+</span> y_t</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>    particles[[i]]<span class="sc">$</span>n[k] <span class="ot">&lt;-</span> particles[[i]]<span class="sc">$</span>n[k] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 4. Learn parameters (store posterior means)</span></span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>  lambda_samples <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">sapply</span>(particles, <span class="cf">function</span>(p) {</span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>    (alpha <span class="sc">+</span> p<span class="sc">$</span>s) <span class="sc">/</span> (beta <span class="sc">+</span> p<span class="sc">$</span>n)</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>  }))</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>  weight_samples <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">sapply</span>(particles, <span class="cf">function</span>(p) {</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>    (gamma <span class="sc">+</span> p<span class="sc">$</span>n) <span class="sc">/</span> <span class="fu">sum</span>(gamma <span class="sc">+</span> p<span class="sc">$</span>n)</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>  }))</span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>  posterior_lambda[t,] <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(lambda_samples)</span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>  posterior_weights[t,] <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(weight_samples)</span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we are ready to plot the results</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert posterior estimates to data frames for ggplot</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>posterior_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Observation =</span> <span class="dv">1</span><span class="sc">:</span>n_obs,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lambda1 =</span> posterior_lambda[, <span class="dv">1</span>],</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lambda2 =</span> posterior_lambda[, <span class="dv">2</span>],</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">Weight1 =</span> posterior_weights[, <span class="dv">1</span>],</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">Weight2 =</span> posterior_weights[, <span class="dv">2</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Lambda parameters</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(posterior_df, <span class="fu">aes</span>(<span class="at">x =</span> Observation)) <span class="sc">+</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> Lambda1, <span class="at">color =</span> <span class="st">"Lambda1"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> Lambda2, <span class="at">color =</span> <span class="st">"Lambda2"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">2</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">10</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Posterior Estimates: Lambda Parameters"</span>,</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Observation"</span>, <span class="at">y =</span> <span class="st">"Lambda"</span>) <span class="sc">+</span></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot Weights</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(posterior_df, <span class="fu">aes</span>(<span class="at">x =</span> Observation)) <span class="sc">+</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> Weight1, <span class="at">color =</span> <span class="st">"Weight1"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> Weight2, <span class="at">color =</span> <span class="st">"Weight2"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.7</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.3</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Posterior Estimates: Mixture Weights"</span>,</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Observation"</span>, <span class="at">y =</span> <span class="st">"Weight"</span>) <span class="sc">+</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/pl-lambda.svg" class="img-fluid figure-img"></p>
<figcaption>Posterior Estimates: Lambda Parameters</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/pl-weight.svg" class="img-fluid figure-img"></p>
<figcaption>Posterior Estimates: Mixture Weights</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The first plot shows the posterior estimates of <span class="math inline">\(\lambda\)</span> parameters converging to true values (2 and 10). The second plot shows mixture weights converging to true weights (0.7 and 0.3). Convergence typically occurs after 100-200 observations. Particle degeneracy is mitigated through systematic resampling</p>
<p><strong>Advantages of PL for Mixtures:</strong></p>
<ol type="1">
<li>Sequential Updating: Processes observations one-at-a-time</li>
<li>Efficiency: Only tracks sufficient statistics, not full history</li>
<li>Flexibility: Easily extends to other mixture types (DP, IBP, etc.)</li>
<li>Real-time Inference: Posterior updates after each observation</li>
</ol>
<p>This implementation demonstrates PL’s ability to handle finite mixtures, but the same framework extends to infinite mixtures (DP mixtures) and other general mixture models described in the paper by modifying the propagation and resampling steps.</p>
</div>
</section>
</section>
<section id="modern-era-forecasting" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="modern-era-forecasting"><span class="header-section-number">15.7</span> Modern Era Forecasting</h2>
<p>A recent post by the Amazon Science group <span class="citation" data-cites="amazon2021history">Amazon (<a href="references.html#ref-amazon2021history" role="doc-biblioref">2021</a>)</span> describes the evolution of the time series algorithms used for forecasting from 2007 to 2021. Figure below shows the entire evolution of the algorithms.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/amazon.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
<p>They went from standard textbook time series forecasting methods to make predictions to the quantile-based transformer models. The main problem of the traditional TS models is that they assume stationary. A stationary time series is one whose properties do not depend on the time at which the series is observed. For example, a white noise series is stationary - it does not matter when you observe it, it should look much the same at any point in time.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>yt <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(yt,<span class="at">type=</span><span class="st">'l'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="15-forecasting_files/figure-html/whitenoise-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
<p>In other words, all the coefficients of a time series model do not change over time. We know how to deal with trends and seasonality quite well. Thus, those types of non-stationary are not an issue. Below some of the example of time series data. Although most of those are not stationary, we can model them using traditional techniques (<span class="citation" data-cites="hyndman2021forecasting">Hyndman and Athanasopoulos (<a href="references.html#ref-hyndman2021forecasting" role="doc-biblioref">2021</a>)</span>).</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-stationary" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="15-forecasting_files/figure-html/fig-stationary-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.8: Which of these series are stationary? (a) Dow Jones index on 292 consecutive days; (b) Daily change in the Dow Jones index on 292 consecutive days; (c) Annual number of strikes in the US; (d) Monthly sales of new one-family houses sold in the US; (e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs slaughtered in Victoria, Australia; (g) Annual total of lynx trapped in the McKenzie River district of north-west Canada; (h) Monthly Australian beer production; (i) Monthly Australian electricity production.
</figcaption>
</figure>
</div>
</div>
</div>
<p>However, when you try to forecast for a time series with no prior history or non-recurrent “jumps”, like recessions, traditional models are unlikely to work well.</p>
<p>Amazon used a sequence of “patches” to hack the model and to make it produce useful results. All of those required manual feature engineering and led to less transparent and fragile models. One solution is to use random forests.</p>
</section>
<section id="quantile-regression-forests." class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="quantile-regression-forests."><span class="header-section-number">15.8</span> Quantile Regression Forests.</h2>
<p>Most estimators during prediction return <span class="math inline">\(E(Y|X)\)</span>, which can be interpreted as the answer to the question, what is the expected value of your output given the input?</p>
<p>Quantile methods, return <span class="math inline">\(y\)</span> at <span class="math inline">\(q\)</span> for which <span class="math inline">\(F(Y=y|X)=q\)</span> where <span class="math inline">\(q\)</span> is the percentile and <span class="math inline">\(y\)</span> is the quantile. One quick use-case where this is useful is when there are a number of outliers which can influence the conditional mean. It is sometimes important to obtain estimates at different percentiles, (when grading on a curve is done for instance.)</p>
<p>Note, Bayesian models return the entire distribution of <span class="math inline">\(P(Y|X)\)</span>.</p>
<p>It is fairly straightforward to extend a standard decision tree to provide predictions at percentiles. When a decision tree is fit, the trick is to store not only the sufficient statistics of the target at the leaf node such as the mean and variance but also all the target values in the leaf node. At prediction, these are used to compute empirical quantile estimates.</p>
<p>The same approach can be extended to Random Forests. To estimate <span class="math inline">\(F(Y=y|x)=q\)</span> each target value in training <span class="math inline">\(y\)</span>s is given a weight. Formally, the weight given to <span class="math inline">\(y_j\)</span> while estimating the quantile is <span class="math display">\[
\frac{1}{T} \sum_{t=1}^{T} \frac{\mathbb{1}(y_j \in L(x))}{\sum_{i=1}^N \mathbb{1}(y_i \in L(x))},
\]</span> where <span class="math inline">\(L(x)\)</span> denotes the leaf that <span class="math inline">\(x\)</span> falls into.</p>
<p>Informally, what it means that for a new unknown sample, we first find the leaf that it falls into at each tree. Then for each <span class="math inline">\((X, y)\)</span> in the training data, a weight is given to <span class="math inline">\(y\)</span> at each tree in the following manner.</p>
<ol type="1">
<li>If it is in the same leaf as the new sample, then the weight is the fraction of samples in the same leaf.</li>
<li>If not, then the weight is zero.</li>
</ol>
<p>These weights for each y are summed up across all trees and averaged. Now since we have an array of target values and an array of weights corresponding to these target values, we can use this to measure empirical quantile estimates.</p>
<p>Motivated by the success of gradient boosting models for predicting Walmart sales (<span class="citation" data-cites="kaggle2020m5">kaggle (<a href="references.html#ref-kaggle2020m5" role="doc-biblioref">2020</a>)</span>), <span class="citation" data-cites="januschowski2022forecasting">Januschowski et al. (<a href="references.html#ref-januschowski2022forecasting" role="doc-biblioref">2022</a>)</span> tries to explain why tree-based methods were so widely used for forecasting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/tree.jpg" class="img-fluid figure-img"></p>
<figcaption><span class="citation" data-cites="januschowski2022forecasting">Januschowski et al. (<a href="references.html#ref-januschowski2022forecasting" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-albert1993statistical" class="csl-entry" role="listitem">
Albert, Jim. 1993. <span>“A <span>Statistical Analysis</span> of <span>Hitting Streaks</span> in <span>Baseball</span>: <span>Comment</span>.”</span> <em>Journal of the American Statistical Association</em> 88 (424): 1184–88. <a href="https://www.jstor.org/stable/2291255">https://www.jstor.org/stable/2291255</a>.
</div>
<div id="ref-amazon2021history" class="csl-entry" role="listitem">
Amazon. 2021. <span>“The History of <span>Amazon</span>’s Forecasting Algorithm.”</span> <em>Amazon Science</em>. https://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm.
</div>
<div id="ref-baum1970maximization" class="csl-entry" role="listitem">
Baum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970. <span>“A <span>Maximization Technique Occurring</span> in the <span>Statistical Analysis</span> of <span>Probabilistic Functions</span> of <span>Markov Chains</span>.”</span> <em>The Annals of Mathematical Statistics</em> 41 (1): 164–71. <a href="https://www.jstor.org/stable/2239727">https://www.jstor.org/stable/2239727</a>.
</div>
<div id="ref-benoit2012binary" class="csl-entry" role="listitem">
Benoit, Dries F., and Dirk Van den Poel. 2012. <span>“Binary Quantile Regression: A <span>Bayesian</span> Approach Based on the Asymmetric <span>Laplace</span> Distribution.”</span> <em>Journal of Applied Econometrics</em> 27 (7): 1174–88.
</div>
<div id="ref-berge2016which" class="csl-entry" role="listitem">
Berge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. <span>“Which <span>Market Indicators Best Forecast Recessions</span>?”</span> <em>FEDS Notes</em>, August.
</div>
<div id="ref-campagnoli2009dynamic" class="csl-entry" role="listitem">
Campagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009. <em>Dynamic <span>Linear Models</span> with <span>R</span></em>. New York, NY: Springer.
</div>
<div id="ref-carlin1992monte" class="csl-entry" role="listitem">
Carlin, Bradley P, Nicholas G Polson, and David S Stoffer. 1992. <span>“A <span>Monte Carlo</span> Approach to Nonnormal and Nonlinear State-Space Modeling.”</span> <em>Journal of the American Statistical Association</em> 87 (418): 493–500.
</div>
<div id="ref-carter1994gibbs" class="csl-entry" role="listitem">
Carter, Chris K, and Robert Kohn. 1994. <span>“On <span>Gibbs</span> Sampling for State Space Models.”</span> <em>Biometrika</em> 81 (3): 541–53.
</div>
<div id="ref-carvalho2010particlea" class="csl-entry" role="listitem">
Carvalho, Carlos M, Hedibert F Lopes, Nicholas G Polson, and Matt A Taddy. 2010. <span>“Particle Learning for General Mixtures.”</span> <em>Bayesian Analysis</em> 5 (4): 709–40.
</div>
<div id="ref-chib1998estimation" class="csl-entry" role="listitem">
Chib, Siddhartha. 1998. <span>“Estimation and Comparison of Multiple Change-Point Models.”</span> <em>Journal of Econometrics</em> 86 (2): 221–41.
</div>
<div id="ref-erictassone2017our" class="csl-entry" role="listitem">
Eric Tassone, and Farzan Rohani. 2017. <span>“Our Quest for Robust Time Series Forecasting at Scale.”</span>
</div>
<div id="ref-fruhwirth-schnatter2007auxiliary" class="csl-entry" role="listitem">
Frühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007. <span>“Auxiliary Mixture Sampling with Applications to Logistic Models.”</span> <em>Computational Statistics &amp; Data Analysis</em> 51 (April): 3509–28.
</div>
<div id="ref-fruhwirth-schnatter2010data" class="csl-entry" role="listitem">
———. 2010. <span>“Data <span>Augmentation</span> and <span>MCMC</span> for <span>Binary</span> and <span>Multinomial Logit Models</span>.”</span> In <em>Statistical <span>Modelling</span> and <span>Regression Structures</span>: <span>Festschrift</span> in <span>Honour</span> of <span>Ludwig Fahrmeir</span></em>, 111–32.
</div>
<div id="ref-fruhwirth-schnatter2008improved" class="csl-entry" role="listitem">
Frühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard Rue. 2008. <span>“Improved Auxiliary Mixture Sampling for Hierarchical Models of&nbsp;Non-<span>Gaussian</span> Data.”</span> <em>Statistics and Computing</em> 19 (4): 479.
</div>
<div id="ref-gramacy2012simulationbased" class="csl-entry" role="listitem">
Gramacy, Robert B., and Nicholas G. Polson. 2012. <span>“Simulation-Based <span>Regularized Logistic Regression</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1005.3430">https://arxiv.org/abs/1005.3430</a>.
</div>
<div id="ref-held2006bayesian" class="csl-entry" role="listitem">
Held, Leonhard, and Chris C. Holmes. 2006. <span>“Bayesian Auxiliary Variable Models for Binary and Multinomial Regression.”</span> <em>Bayesian Analysis</em> 1 (1): 145–68.
</div>
<div id="ref-hyndman2021forecasting" class="csl-entry" role="listitem">
Hyndman, Rob J., and George Athanasopoulos. 2021. <em>Forecasting: <span>Principles</span> and <span>Practice</span></em>. 3rd ed. edition. Melbourne, Australia: Otexts.
</div>
<div id="ref-januschowski2022forecasting" class="csl-entry" role="listitem">
Januschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf Hasson, and Jan Gasthaus. 2022. <span>“Forecasting with Trees.”</span> <em>International Journal of Forecasting</em>, Special <span>Issue</span>: <span>M5</span> competition, 38 (4): 1473–81.
</div>
<div id="ref-kaggle2020m5" class="csl-entry" role="listitem">
kaggle. 2020. <span>“M5 <span>Forecasting</span> - <span>Accuracy</span>.”</span> https://kaggle.com/competitions/m5-forecasting-accuracy.
</div>
<div id="ref-kalman1961new" class="csl-entry" role="listitem">
Kalman, R. E., and R. S. Bucy. 1961. <span>“New <span>Results</span> in <span>Linear Filtering</span> and <span>Prediction Theory</span>.”</span> <em>Journal of Basic Engineering</em> 83 (1): 95–108.
</div>
<div id="ref-kalman1960new" class="csl-entry" role="listitem">
Kalman, Rudolph Emil. 1960. <span>“A New Approach to Linear Filtering and Prediction Problems.”</span> <em>Transactions of the ASME–Journal of Basic Engineering</em> 82 (Series D): 35–45.
</div>
<div id="ref-lindgren1978markov" class="csl-entry" role="listitem">
Lindgren, Georg. 1978. <span>“Markov <span>Regime Models</span> for <span>Mixed Distributions</span> and <span>Switching Regressions</span>.”</span> <em>Scandinavian Journal of Statistics</em> 5 (2): 81–91. <a href="https://www.jstor.org/stable/4615692">https://www.jstor.org/stable/4615692</a>.
</div>
<div id="ref-petris2010package" class="csl-entry" role="listitem">
Petris, Giovanni. 2010. <span>“An <span>R Package</span> for <span>Dynamic Linear Models</span>.”</span> <em>Journal of Statistical Software</em> 36 (October): 1–16.
</div>
<div id="ref-polson2011dataa" class="csl-entry" role="listitem">
Polson, Nicholas, and Steven Scott. 2011. <span>“Data <span>Augmentation</span> for <span>Support Vector Machines</span>.”</span> <em>Bayesian Analysis</em> 6 (March).
</div>
<div id="ref-rubin2015bayesian" class="csl-entry" role="listitem">
Rubin, Hal S. Stern, John B. Carlin. 2015. <em>Bayesian <span>Data Analysis</span></em>. 3rd ed. New York: <span>Chapman and Hall/CRC</span>.
</div>
<div id="ref-scott2002bayesian" class="csl-entry" role="listitem">
Scott, Steven L. 2002. <span>“Bayesian <span>Methods</span> for <span>Hidden Markov Models</span>.”</span> <em>Journal of the American Statistical Association</em> 97 (457): 337–51.
</div>
<div id="ref-scott2022boomspikeslab" class="csl-entry" role="listitem">
Scott, Steven L. 2022. <span>“<span>BoomSpikeSlab</span>: <span>MCMC</span> for <span>Spike</span> and <span>Slab Regression</span>.”</span>
</div>
<div id="ref-scott2015bayesian" class="csl-entry" role="listitem">
Scott, Steven L., and Hal R. Varian. 2015. <span>“Bayesian <span>Variable Selection</span> for <span>Nowcasting Economic Time Series</span>.”</span> In <em>Economic <span>Analysis</span> of the <span>Digital Economy</span></em>, 119–35. University of Chicago Press.
</div>
<div id="ref-scott2014predicting" class="csl-entry" role="listitem">
Scott, Steven, and Hal Varian. 2014. <span>“Predicting the <span>Present</span> with <span>Bayesian Structural Time Series</span>.”</span> <em>Int. J. Of Mathematical Modelling and Numerical Optimisation</em> 5 (January): 4–23.
</div>
<div id="ref-seanj.taylor2017prophet" class="csl-entry" role="listitem">
Sean J. Taylor, and Ben Letham. 2017. <span>“Prophet: Forecasting at Scale - <span>Meta Research</span>.”</span> <em>Meta Research</em>. https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/.
</div>
<div id="ref-smith1975bayesian" class="csl-entry" role="listitem">
Smith, A. F. M. 1975. <span>“A <span>Bayesian Approach</span> to <span>Inference</span> about a <span>Change-Point</span> in a <span>Sequence</span> of <span>Random Variables</span>.”</span> <em>Biometrika</em> 62 (2): 407–16. <a href="https://www.jstor.org/stable/2335381">https://www.jstor.org/stable/2335381</a>.
</div>
<div id="ref-viterbi1967error" class="csl-entry" role="listitem">
Viterbi, A. 1967. <span>“Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.”</span> <em>IEEE Transactions on Information Theory</em> 13 (2): 260–69.
</div>
<div id="ref-west1997bayesian" class="csl-entry" role="listitem">
West, Mike, and Jeff Harrison. 1997. <em>Bayesian Forecasting and Dynamic Models</em>. Springer.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./14-tree.html" class="pagination-link" aria-label="Tree Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./16-rct.html" class="pagination-link" aria-label="Randomized Controlled Trials">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>