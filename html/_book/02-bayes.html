<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Bayes Rule – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-bl.html" rel="next">
<link href="./01-prob.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3fa4ff979380b88aedafe7599fa714ae.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }
  
  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="2&nbsp; Bayes Rule – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="fig/anand-prior.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="2&nbsp; Bayes Rule – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="fig/anand-prior.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./02-bayes.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-rct.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Randomized Controlled Trials</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Theory of AI: From MLE to Bayesian Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-qnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Large Language Models: A Revolution in AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#law-of-total-probability" id="toc-law-of-total-probability" class="nav-link active" data-scroll-target="#law-of-total-probability"><span class="header-section-number">2.1</span> Law of Total Probability</a></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes"><span class="header-section-number">2.2</span> Naive Bayes</a></li>
  <li><a href="#real-world-bayes" id="toc-real-world-bayes" class="nav-link" data-scroll-target="#real-world-bayes"><span class="header-section-number">2.3</span> Real World Bayes</a>
  <ul class="collapse">
  <li><a href="#search-and-rescue" id="toc-search-and-rescue" class="nav-link" data-scroll-target="#search-and-rescue">Search and Rescue</a></li>
  <li><a href="#games-and-puzzles" id="toc-games-and-puzzles" class="nav-link" data-scroll-target="#games-and-puzzles">Games and Puzzles</a></li>
  </ul></li>
  <li><a href="#sec-sensitivity" id="toc-sec-sensitivity" class="nav-link" data-scroll-target="#sec-sensitivity"><span class="header-section-number">2.4</span> Sensitivity and Specificity</a></li>
  <li><a href="#graphical-representation-of-probability-and-conditional-independence." id="toc-graphical-representation-of-probability-and-conditional-independence." class="nav-link" data-scroll-target="#graphical-representation-of-probability-and-conditional-independence."><span class="header-section-number">2.5</span> Graphical Representation of Probability and Conditional Independence.</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-prob.html">Bayes</a></li><li class="breadcrumb-item"><a href="./02-bayes.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p><em>When the facts change, I change my mind. What do you do, sir?</em> John Maynard Keynes</p>
</blockquote>
<p>One of the key questions in the theory of learning is: <em>How do you update your beliefs in the presence of new information?</em> Bayes rule provides the answer. Conditional probability can be interpreted as updating your probability of event <span class="math inline">\(A\)</span> after you have learned the new information that <span class="math inline">\(B\)</span> has occurred. In this sense probability is also the language of how you’ll change opinions in the light of new evidence. For example, we need to find the probability that a thrown die shows on its upper surface an odd number and we found out that the number shown is less than 4.</p>
<div id="exm-intuition" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Intuition)</strong></span> Our intuition is not well trained to make use of Bayes rule. If I tell you that Steve was selected at random from a representative sample. He is 6 feet 2 inches tall and an excellent basketball player. He goes to the gym every day and practices hard playing basketball. Do you think Steve is a custodian at a factory or an NBA player? Most people assume Steve is an NBA player which is wrong. The ratio of NBA players to custodians is very small, probabilistically Steve is more likely to be a custodian. Let’s look at it graphically. The key is to provide the right conditioning and to consider the prior probability! Even though the ratio of people who practice basketball hard is much higher among NBA players (it is 1) when compared to custodians, the larger number of the population means we still have more custodians in the US than NBA players.</p>
<p><span class="math display">\[\begin{align*}
\prob{\text{Practice hard}  \mid  \text{Play in NBA}} \approx  1\\
\prob{ \text{Play in NBA}  \mid  \text{Practice hard}} \approx  0.
\end{align*}\]</span></p>
<p>Even though you practice hard, the odds of playing in the NBA are low (<span class="math inline">\(1000\)</span> players out of <span class="math inline">\(7\)</span> billion). But given you’re in the NBA, you no doubt practice very hard. To understand this further, let’s look at the conditional probability implication and apply Bayes rule <span class="math display">\[
p \left ( \text{Play in NBA}  \mid  \text{Practice hard}  \right ) = \dfrac{p  \left ( \text{Practice hard}  \mid  \text{Play in NBA} \right )}{p(\text{Practice hard})}p( \text{Play in NBA}).
\]</span> This is written in the form <span class="math display">\[
\text{Posterior} = \frac{\text{Likelihood}}{\text{Marginal}}\times \text{Prior} = \text{Bayes Factor} \times \text{Prior}.
\]</span> The Likelihood/Marginal ratio is called the Bayes Factor. As we will see in the text, one of the key advantages over classical is the ability to sequentially update our beliefs as new evidence appears. It allows for disciplined probability accounting in “real-time”. With the advent of prediction markets and data science, it has become increasingly important to be able to update our beliefs as new evidence appears.</p>
<p>The initial (a.k.a. prior) probability <span class="math inline">\(p(\text{Play in NBA} ) = 450/(8 \cdot 10^9) = 5.625 \times 10^{-8}\)</span>, makes the conditional (or, so called, posterior) probability also very small. <span class="math display">\[
p   \left ( \text{Play in NBA}  \mid  \text{Practice hard}  \right ) \approx  0,
\]</span> <span class="math inline">\(P(\text{practice hard})\)</span> is not that small and <span class="math inline">\(P(\text{practice hard} \mid \text{play in NBA})=1\)</span>. Hence, when one ‘reverses the conditioning’ one gets a very small probability. This makes sense!</p>
<p>The Steve example illustrates how our intuition fails us, but let’s consider an even more striking case that demonstrates the power of Bayes rule with extreme probabilities. Consider the question: what is the probability that a randomly selected 7-foot-tall American male plays in the NBA?</p>
<p>Most people’s intuition suggests this probability should be quite high - after all, being exceptionally tall seems like the primary qualification for professional basketball. However, Bayes rule reveals a more nuanced picture that depends critically on the base rates involved.</p>
<p>To calculate <span class="math inline">\(P(\text{NBA player} \mid \text{7 feet tall})\)</span> using Bayes rule, we need to carefully estimate each component:</p>
<p><span class="math display">\[P(\text{NBA} \mid \text{7ft}) = \frac{P(\text{7ft} \mid \text{NBA}) \times P(\text{NBA})}{P(\text{7ft})}\]</span></p>
<p>The prior probability <span class="math inline">\(P(\text{NBA})\)</span> represents the baseline chance of being an NBA player. With approximately 450 active players drawn from roughly 40 million American males of playing age, this gives us <span class="math inline">\(P(\text{NBA}) \approx 1.1 \times 10^{-5}\)</span> - an extraordinarily small number.</p>
<p>The likelihood <span class="math inline">\(P(\text{7ft} \mid \text{NBA})\)</span> asks what fraction of NBA players are 7 feet or taller. Historically, this has been around 17% of the league, so <span class="math inline">\(P(\text{7ft} \mid \text{NBA}) \approx 0.17\)</span>.</p>
<p>The marginal probability <span class="math inline">\(P(\text{7ft})\)</span> requires us to estimate how rare 7-foot-tall men are in the general population. Male height follows approximately a normal distribution with mean 69 inches and standard deviation 3 inches. At 84 inches (7 feet), we’re looking at a z-score of 5.0, which corresponds to roughly 1 in 3.5 million men, giving us <span class="math inline">\(P(\text{7ft}) \approx 2.87 \times 10^{-7}\)</span>.</p>
<p>Applying Bayes rule: <span class="math display">\[P(\text{NBA} \mid \text{7ft}) = \frac{0.17 \times 1.1 \times 10^{-5}}{2.87 \times 10^{-7}} \approx 0.065\]</span></p>
<p>This yields approximately 6.5% - a dramatic increase from the baseline probability of 0.001%, yet still surprisingly low given our intuitions.</p>
<p>Let’s also consider a direct count-based calculation. As of September 2025, there are 39 players in the NBA who are 7 feet tall or taller, out of a total of 450 NBA players. This means the probability that a randomly selected NBA player is at least 7 feet tall is:</p>
<p><span class="math display">\[
P(\text{7ft} \mid \text{NBA}) = \frac{39}{450} = 0.0867
\]</span></p>
<p>This empirical estimate is slightly different from the earlier historical average, but it still highlights the rarity of extreme height even among elite basketball players.</p>
<p>Regardless of the exact calculation, this example powerfully demonstrates how Bayes rule forces us to account for base rates. Even when height provides enormous predictive value for NBA success (the likelihood ratio is massive), the extreme rarity of both 7-foot-tall individuals and NBA players means that most 7-footers will not be professional basketball players. This counterintuitive result exemplifies why disciplined probabilistic reasoning through Bayes rule is essential for making accurate inferences in the presence of rare events.</p>
</div>
<p>Probability rules allow us to change our mind if the facts change. For example, suppose that we have evidence <span class="math inline">\(E = \{ E_1 , E_2 \}\)</span> consists of two pieces of information and that we are interested in identifying a cause <span class="math inline">\(C\)</span>, <span class="math inline">\(P(C\mid E_1,E_2)\)</span>. Bayes rule simply lets you calculate this conditional probability in a sequential fashion. First, conditioning on the information contained in <span class="math inline">\(E_1\)</span>, lets us calculate <span class="math display">\[
P( C| E_1 ) = \frac{ p(  E_1 \mid C ) P( C) }{ P( E_1 ) }
\]</span> Then, using the posterior probability <span class="math inline">\(P( C| E_1 )\)</span> as the “new” prior for the next piece of information <span class="math inline">\(E_2\)</span> lets us find <span class="math display">\[
P( C| E_1 , E_2 ) = \frac{ p(  E_2 \mid E_1 , C ) P( C \mid E_1 ) }{ P( E_2 \mid E_1 ) }
\]</span> Hence, we see that we need assessments of the two conditional probabilities <span class="math inline">\(P( E_1 \mid C )\)</span> and <span class="math inline">\(P( E_2 \mid E_1 , C )\)</span>. In many situations, the latter will be simply <span class="math inline">\(P( E_2 \mid C )\)</span> and not involve <span class="math inline">\(E_1\)</span>. The events <span class="math inline">\(( E_1, E_2 )\)</span> will be said to be conditionally independent given <span class="math inline">\(C\)</span>.</p>
<p>This concept generalizes to a sequence of events where <span class="math inline">\(E = \{ E_1,\ldots E_n \}\)</span>. When learning from data we will use this property all the time. An illustrative example will be the Black Swan problem which we discuss later.</p>
<p>Bayes’ rule is a fundamental concept in probability theory and statistics. It describes how to update our <em>beliefs</em> about an event based on <em>new evidence</em>. We start with an initial belief about the probability of an event (called the <em>prior probability</em>). We then observe some conditional information (e.g.&nbsp;evidence). We use Bayes’ rule to update our initial belief based on the evidence, resulting in a new belief called the <em>posterior probability</em>. Remember, the formula is <span class="math display">\[
P(A\mid B) = \dfrac{P(B\mid A) P(A)}{P(B)}
\]</span> where:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Posterior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(A\mid B)\)</span> is the posterior probability of event <span class="math inline">\(A\)</span> occurring given that <span class="math inline">\(B\)</span> is known to happen for sure. This is the probability we’re trying to find.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Likelihood
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(B\mid A)\)</span> is the likelihood of observing event <span class="math inline">\(B\)</span> if event <span class="math inline">\(A\)</span> has occurred.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Prior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(A)\)</span> is the prior probability of event <span class="math inline">\(A\)</span> occurring. This is our initial belief about the probability of <span class="math inline">\(A\)</span> before we see any evidence.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Marginal probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(P(B)\)</span> is the marginal probability of observing event <span class="math inline">\(B\)</span>. This is the probability of observing B regardless of whether <span class="math inline">\(A\)</span> occurs.</p>
</div>
</div>
<p>The ability to use Bayes rule sequentially is key in many applications, when we need to update our beliefs in the presence of new information. For example, Bayesian learning was used by mathematician Alan Turing in England at Bletchley Park to break the German Enigma code - a development that helped the Allies win the Second World War <span class="citation" data-cites="simpson2010edward">(<a href="references.html#ref-simpson2010edward" role="doc-biblioref">Simpson 2010</a>)</span>. Turing called his algorithm Banburismus, it is a process he invented which used sequential conditional probability to infer information about the likely settings of the Enigma machine.</p>
<p>Dennis Lindley argued that we should all be trained in Bayes rule and conditional probability can be simply viewed as disciplined probability accounting. Akin to how market odds change as evidence changes. However, human intuition is rarely naturally calibrated for Bayesian reasoning; it is a skill that must be learned, much like literacy.</p>
<section id="law-of-total-probability" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="law-of-total-probability"><span class="header-section-number">2.1</span> Law of Total Probability</h2>
<p>The Law of Total Probability is a fundamental rule relating marginal probabilities to conditional probabilities. It’s particularly useful when you’re dealing with a set of mutually exclusive and collectively exhaustive events.</p>
<p>Suppose you have a set of events <span class="math inline">\(B_1, B_2, ..., B_n\)</span> that are mutually exclusive (i.e., no two events can occur at the same time) and collectively exhaustive (i.e., at least one of the events must occur). The Law of Total Probability states that for any other event <span class="math inline">\(A\)</span>, the probability of <span class="math inline">\(A\)</span> occurring can be calculated as the sum of the probabilities of <span class="math inline">\(A\)</span> occurring given each <span class="math inline">\(B_i\)</span>, multiplied by the probability of each <span class="math inline">\(B_i\)</span> occurring.</p>
<p>Mathematically, it is expressed as:</p>
<p><span class="math display">\[
P(A) = \sum_{i=1}^{n} P(A\mid  B_i) P(B_i)
\]</span></p>
<div id="exm-total" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Total Probability)</strong></span> Let’s consider a simple example to illustrate this. Suppose you have two bags of balls. Bag 1 contains 3 red and 7 blue balls, while Bag 2 contains 6 red and 4 blue balls. You randomly choose one of the bags and then randomly draw a ball from that bag. What is the probability of drawing a red ball?</p>
<p>Here, the events <span class="math inline">\(B_1\)</span> and <span class="math inline">\(B_2\)</span> can be choosing Bag 1 and Bag 2, respectively. You want to find drawing a red ball (event <span class="math inline">\(A\)</span>).</p>
<p>Applying the law:</p>
<ul>
<li><span class="math inline">\(P(A\midB_1)\)</span> is the probability of drawing a red ball from Bag 1, which is <span class="math inline">\(\frac{3}{10}\)</span>.</li>
<li><span class="math inline">\(P(A\midB_2)\)</span> is the probability of drawing a red ball from Bag 2, which is <span class="math inline">\(\frac{6}{10}\)</span>.</li>
<li>Assume the probability of choosing either bag is equal, so <span class="math inline">\(P(B_1) = P(B_2) = \frac{1}{2}\)</span>.</li>
</ul>
<p>Using the Law of Total Probability: <span class="math display">\[
P(A) = P(A\midB_1) \times P(B_1) + P(A\midB_2) \times P(B_2)= \frac{3}{10} \times \frac{1}{2} + \frac{6}{10} \times \frac{1}{2} = \frac{9}{20}
\]</span></p>
<p>So, the probability of drawing a red ball in this scenario is <span class="math inline">\(\frac{9}{20}\)</span>.</p>
</div>
<p>This law is particularly useful in complex probability problems where direct calculation of probability is difficult. By breaking down the problem into conditional probabilities based on relevant events, it simplifies the calculation and helps to derive a solution.</p>
<div id="exm-Craps" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Craps)</strong></span> Craps is a fast-moving dice game with a complex betting layout. It’s highly volatile, but eventually your bankroll will drift towards zero. Let’s look at the pass line bet. The expectation <span class="math inline">\(E(X)\)</span> governs the long run. When 7 or 11 comes up, you win. When 2, 3 or 12 comes up, this is known as “craps”, you lose. When 4, 5, 6, 8, 9 or 10 comes up, this number is called the “point”, the bettor continues to roll until a 7 (you lose) or the point comes up (you win).</p>
<p>We need to know the probability of winning. The pay-out, probability and expectation for a $1 bet</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Win</th>
<th>Prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.4929</td>
</tr>
<tr class="even">
<td>-1</td>
<td>0.5071</td>
</tr>
</tbody>
</table>
<p>This leads to an edge in favor of the house as <span class="math display">\[
E(X) = 1 \cdot 0.4929 + (- 1) \cdot  0.5071 = -0.014
\]</span> The house has a 1.4% edge.</p>
<p>To calculate the probability of winning: <span class="math inline">\(P( \text{Win} )\)</span> let’s use the law of total probability <span class="math display">\[
P( \text{Win} ) = \sum_{ \mathrm{Point} } P ( \text{Win} \mid \mathrm{Point} ) P ( \mathrm{Point} )
\]</span> The set of <span class="math inline">\(P( \mathrm{Point} )\)</span> are given by</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Value</th>
<th>Probability</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>1/36</td>
<td>2.78%</td>
</tr>
<tr class="even">
<td>3</td>
<td>2/36</td>
<td>5.56%</td>
</tr>
<tr class="odd">
<td>4</td>
<td>3/36</td>
<td>8.33%</td>
</tr>
<tr class="even">
<td>5</td>
<td>4/36</td>
<td>11.1%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>5/36</td>
<td>13.9%</td>
</tr>
<tr class="even">
<td>7</td>
<td>6/36</td>
<td>16.7%</td>
</tr>
<tr class="odd">
<td>8</td>
<td>5/36</td>
<td>13.9%</td>
</tr>
<tr class="even">
<td>9</td>
<td>4/36</td>
<td>11.1%</td>
</tr>
<tr class="odd">
<td>10</td>
<td>3/36</td>
<td>8.33%</td>
</tr>
<tr class="even">
<td>11</td>
<td>2/36</td>
<td>5.56%</td>
</tr>
<tr class="odd">
<td>12</td>
<td>1/36</td>
<td>2.78%</td>
</tr>
</tbody>
</table>
<p>The conditional probabilities <span class="math inline">\(P( \text{Win} \mid \mathrm{Point} )\)</span> are harder to calculate <span class="math display">\[
P( \text{Win} \mid 7 \; \mathrm{or} \; 11 ) = 1 \; \; \mathrm{and} \; \; P( \text{Win} \mid 2 ,
3 \; \mathrm{or} \; 12 ) = 0
\]</span> We still have to work out all the probabilities of winning given the point. Suppose the point is <span class="math inline">\(4\)</span> <span class="math display">\[
P( \text{Win} \mid 4 ) = P ( 4 \; \mathrm{before} \; 7 ) = \dfrac{P(4)}{P(7)+P(4)} = \frac{3}{9} =
\frac{1}{3}
\]</span> There are 6 ways of getting a 7, 3 ways of getting a 4 for a total of 9 possibilities. Now do all of them and sum them up. You get <span class="math display">\[
P( \text{Win}) = 0.4929
\]</span></p>
</div>
</section>
<section id="naive-bayes" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="naive-bayes"><span class="header-section-number">2.2</span> Naive Bayes</h2>
<p>Use of the Bayes rule allows us to build our first predictive model, called Naive Bayes classifier. Naive Bayes is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDMxNH0.uR3YM4nh6u7PkPxdhujkZSsjTt2i73Emgkl_h_IOh4k -->
<!-- ![](fig/color-shape-size.svg){width=60% fig-align="center"} -->
<div class="cell" data-fig-width="2" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    C((Color)) --&gt; F((Fruit))
    S((Size)) --&gt; F
    Sh((Shape)) --&gt; F
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Although it’s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification. In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features). In summary, the advantages are:</p>
<ul>
<li>It’s relatively simple to understand and build</li>
<li>It’s easily trained, even with a small dataset</li>
<li>It’s fast!</li>
<li>It’s not sensitive to irrelevant features</li>
</ul>
<p>The main disadvantage is that it assumes every feature is independent, which isn’t always the case.</p>
<p>Let’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Fruit</th>
<th>Long</th>
<th>Sweet</th>
<th>Yellow</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Banana</td>
<td>400</td>
<td>350</td>
<td>450</td>
<td>500</td>
</tr>
<tr class="even">
<td>Orange</td>
<td>0</td>
<td>150</td>
<td>300</td>
<td>300</td>
</tr>
<tr class="odd">
<td>Other</td>
<td>100</td>
<td>150</td>
<td>50</td>
<td>200</td>
</tr>
<tr class="even">
<td>Total</td>
<td>500</td>
<td>650</td>
<td>800</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>From this data we can calculate marginal probabilities</p>
<ul>
<li>50% of the fruits are bananas</li>
<li>30% are oranges</li>
<li>20% are other fruits</li>
</ul>
<p>Based on our training set we can also say the following:</p>
<ul>
<li>From 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow</li>
<li>Out of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow</li>
<li>From the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.</li>
</ul>
<p>Given the evidence <span class="math inline">\(E\)</span> (<span class="math inline">\(L\)</span> = Long, <span class="math inline">\(S\)</span> = Sweet and <span class="math inline">\(Y\)</span> = Yellow) we can calculate the probability of each class <span class="math inline">\(C\)</span> (<span class="math inline">\(B\)</span> = Banana, <span class="math inline">\(O\)</span> = Orange or <span class="math inline">\(F\)</span> = Other Fruit) using Bayes’ Theorem: <span class="math display">\[\begin{align*}
P(B \mid E) = &amp; \frac{P(L \mid B)P(S \mid B)P(Y \mid B)P(B)}{P(L)P(S)P(Y)}\\
=&amp;\frac{0.8\times 0.7\times 0.9\times 0.5}{P(E)}=\frac{0.252}{P(E)}
\end{align*}\]</span></p>
<p>Orange: <span class="math display">\[
P(O\mid E)=0.
\]</span></p>
<p>Other Fruit: <span class="math display">\[\begin{align*}
P(F \mid E) &amp; = \frac{P(L \mid F)P(S \mid F)P(Y \mid F)P(F)}{P(L)P(S)P(Y)}\\
=&amp;\frac{0.5\times 0.75\times 0.25\times 0.2}{P(E)}=\frac{0.01875}{P(E)}
\end{align*}\]</span></p>
<p>In this case, based on the higher score, we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.</p>
<p>Notice, we did not have to calculate <span class="math inline">\(P(E)\)</span> because it is a normalizing constant and it cancels out when we calculate the ratio</p>
<p><span class="math display">\[
\dfrac{P(B \mid E)}{P(F \mid E)} = 0.252/0.01875 = 13.44 &gt; 1.
\]</span></p>
<p>Now that we’ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g.&nbsp;“spam” and “ham” in case of spam detection) as classes we can start to classify documents and text automatically.</p>
<div id="exm-naivebayes" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Spam Filtering)</strong></span> The original spam filtering algorithm was based on Naive Bayes. The “naive” aspect of Naive Bayes comes from the assumption that inputs (words in the case of text classification) are conditionally independent, given the class label. Naive Bayes treats each word independently, and the model doesn’t capture the sequential or structural information inherent in the language. It does not consider grammatical relationships or syntactic structures. The algorithm doesn’t understand the grammatical rules that dictate how words should be combined to form meaningful sentences. Further, it doesn’t understand the context in which words appear. For example, it may treat the word “bank” the same whether it refers to a financial institution or the side of a river bank. Despite its simplicity and the naive assumption, Naive Bayes often performs well in practice, especially in text classification tasks.</p>
<p>We start by collecting a dataset of emails labeled as “spam” or “not spam” (ham) and calculate the prior probabilities of spam (<span class="math inline">\(P(\text{spam})\)</span>) and not spam (<span class="math inline">\(P(\text{ham})\)</span>) based on the training dataset, by simply counting the proportions of each in the data.</p>
<p>Then each email gets converted into a bag-of-words representation (ignoring word order and considering only word frequencies). Then, we create a vocabulary of unique words from the entire dataset <span class="math inline">\(w_1,w_2,\ldots,w_N\)</span> and calculate conditional probabilities <span class="math display">\[
P(\mathrm{word}_i  \mid  \text{spam}) = \frac{\text{Number of spam emails containing }\mathrm{word}_i}{\text{Total number of spam emails}}, ~ i=1,\ldots,n
\]</span> <span class="math display">\[
P(\mathrm{word}_i  \mid  \text{ham}) = \frac{\text{Number of ham emails containing }\mathrm{word}_i}{\text{Total number of ham emails}}, ~ i=1,\ldots,n
\]</span></p>
<p>Now, we are ready to use our model to classify new emails. We do it by calculating the posterior probability using Bayes’ theorem. Say an email has a set of <span class="math inline">\(k\)</span> words <span class="math inline">\(\text{email} = \{w_{e1},w_{e2},\ldots, w_{ek}\}\)</span>, then <span class="math display">\[
P(\text{spam}  \mid  \text{email}) = \frac{P(\text{email}  \mid  \text{spam}) \times P(\text{spam})}{P(\text{email})}
\]</span> Here <span class="math display">\[
P(\text{email}  \mid  \text{spam}) = P( w_{e1}  \mid  \text{spam})P( w_{e2}  \mid  \text{spam})\ldots P( w_{ek}  \mid  \text{spam})
\]</span> We calculate <span class="math inline">\(P(\text{ham} \mid \text{email})\)</span> in a similar way.</p>
<p>Finally, we classify the email as spam or ham based on the class with the highest posterior probability.</p>
<p>Suppose you have a spam email with the word “discount” appearing. Using Naive Bayes, you’d calculate the probability that an email containing “discount” is spam <span class="math inline">\(P(\text{spam} \mid \text{discount})\)</span> and ham <span class="math inline">\(P(\text{ham} \mid \text{discount})\)</span>, and then compare these probabilities to make a classification decision.</p>
<p>While the naive assumption simplifies the model and makes it computationally efficient, it comes at the cost of a more nuanced understanding of language. More sophisticated models, such as transformers, have been developed to address these limitations by considering the sequential nature of language and capturing contextual relationships between words.</p>
<p>In summary, naive Bayes, due to its simplicity and the naive assumption of independence, is not capable of understanding the rules of grammar, the order of words, or the intricate context in which words are used. It is a basic algorithm suitable for certain tasks but may lack the complexity needed for tasks that require a deeper understanding of language structure and semantics.</p>
</div>
</section>
<section id="real-world-bayes" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="real-world-bayes"><span class="header-section-number">2.3</span> Real World Bayes</h2>
<section id="search-and-rescue" class="level3">
<h3 class="anchored" data-anchor-id="search-and-rescue">Search and Rescue</h3>
<div id="exm-Scorpion" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 (USS Scorpion sank 5 June, 1968 in the middle of the Atlantic.)</strong></span> Experts placed bets on each casualty and how each would affect the sinking. Undersea soundings gave a prior on location. Bayes rule: <span class="math inline">\(L\)</span> is location and <span class="math inline">\(S\)</span> is scenario <span class="math display">\[
p (L \mid S) = \frac{ p(S \mid L) p(L)}{p(S)}
\]</span> The Navy spent <span class="math inline">\(5\)</span> months looking and found nothing. Built a probability map: within <span class="math inline">\(5\)</span> days, the submarine was found within <span class="math inline">\(220\)</span> yards of the most likely probability!</p>
<p>A similar story happened during the search of an Air France plane that flew from Rio to Paris.</p>
</div>
<div id="exm-Wald" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 (Wald and Airplane Safety)</strong></span> Many lives were saved by analysis of conditional probabilities performed by Abraham Wald during the Second World War. He was analyzing damages on the US planes that came back from bombing missions in Germany. Somebody suggested to analyze the distribution of the hits over different parts of the plane. The idea was to find a pattern in the damages and design a reinforcement strategy.</p>
<p>After examining hundreds of damaged airplanes, researchers came up with the following table</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Location</th>
<th style="text-align: right;">Number of Planes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">53</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">65</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">96</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">434</td>
</tr>
</tbody>
</table>
<p>We can convert those counts to probabilities</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Location</th>
<th style="text-align: right;">Number of Planes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">0.08</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">0.15</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">0.67</td>
</tr>
</tbody>
</table>
<p>We can conclude that the most likely area to be damaged on the returned planes was the wings and fuselage. <span class="math display">\[
\prob{\mbox{hit on wings or fuselage } \mid \mbox{returns safely}} = 0.67
\]</span> Wald realized that analyzing damages only on survived planes is not the right approach. Instead, he suggested that it is essential to calculate the inverse probability <span class="math display">\[
\prob{\mbox{returns safely} \mid \mbox{hit on wings or fuselage }} = ?
\]</span> To calculate that, he interviewed many engineers and pilots, he performed a lot of field experiments. He analyzed likely attack angles. He studied the properties of a shrapnel cloud from a flak gun. He suggested to the army that they fire thousands of dummy bullets at a plane sitting on the tarmac. Wald constructed a ‘probability model’ carefully to reconstruct an estimate for the joint probabilities. The table below shows the results.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: right;">Hit</th>
<th style="text-align: right;">Returned</th>
<th style="text-align: right;">Shot Down</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Engine</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">57</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cockpit</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">46</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Fuel system</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: right;">Wings, fuselage, etc.</td>
<td style="text-align: right;">434</td>
<td style="text-align: right;">33</td>
</tr>
</tbody>
</table>
<p>Which allows us to estimate joint probabilities, for example <span class="math display">\[
\prob{\mbox{outcome = returns safely} , \mbox{hit  =  engine }} = 53/800 = 0.066
\]</span> We also can calculate the conditional probabilities now <span class="math display">\[
\prob{\mbox{outcome = returns safely} \mid  \mbox{hit  =  wings or fuselage  }} = \dfrac{434}{434+33} = 0.9293362.
\]</span> Should we reinforce wings or fuselage? Which part of the airplane needs to be reinforced? <span class="math display">\[
\prob{\mbox{outcome = returns safely} \mid  \mbox{hit  =  engine  }} = \dfrac{53}{53+57} = 0.48
\]</span> Here is another illustration taken from the Economics literature. This insight led to George Akerlof winning the Nobel Prize for the concept of asymmetric information.</p>
</div>
</section>
<section id="games-and-puzzles" class="level3">
<h3 class="anchored" data-anchor-id="games-and-puzzles">Games and Puzzles</h3>
<div id="exm-jar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Coin Jar)</strong></span> Large jar containing 1024 fair coins and one two-headed coin. You pick one at random and flip it <span class="math inline">\(10\)</span> times and get all heads. What’s the probability that the coin is the two-headed coin? The probability of initially picking the two headed coin is 1/1025. There is a 1/1024 chance of getting <span class="math inline">\(10\)</span> heads in a row from a fair coin. Therefore, it’s a <span class="math inline">\(50/50\)</span> bet.</p>
<p>Let’s do the formal Bayes rule math. Let <span class="math inline">\(E\)</span> be the event that you get <span class="math inline">\(10\)</span> Heads in a row, then</p>
<p><span class="math display">\[
P \left ( \mathrm{two \; headed}  \mid  E \right ) = \frac{ P \left ( E  \mid  \mathrm{ two \; headed}  \right )P \left (  \mathrm{ two \; headed} \right )}
{P \left ( E  \mid  \mathrm{ fair}  \right )P \left ( \mathrm{ fair} \right ) + P \left ( E  \mid  \mathrm{ two \; headed}  \right )P \left ( \mathrm{ two \; headed} \right )}
\]</span> Therefore, the posterior probability <span class="math display">\[
P \left (  \mathrm{two \; headed}  \mid  E \right ) = \frac{ 1 \times \frac{1}{1025} }{ \frac{1}{1024} \times \frac{1024}{1025} + 1 \times \frac{1}{1025} } = 0.50
\]</span> What’s the probability that the next toss is a head? Using the law of total probability gives</p>
<p><span class="math display">\[\begin{align*}
  P( H ) &amp;= P( H  \mid  \mathrm{ two \; headed} )P( \mathrm{ two \; headed}  \mid E ) +  P( H  \mid  \mathrm{ fair} )P( \mathrm{ fair}  \mid E) \\
  &amp; = 1 \times \frac{1}{2} + \frac{1}{2} \times \frac{1}{2} = \frac{3}{4}
\end{align*}\]</span></p>
</div>
<div id="exm-monty" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Monty Hall Problem)</strong></span> Another example of a situation when calculating probabilities is counterintuitive. The Monty Hall problem was named after the host of the long-running TV show Let’s Make a Deal. The original solution was proposed by Marilyn vos Savant, who had a column with the correct answer that many Mathematicians thought was wrong!</p>
<p>The game set-up is as follows. A contestant is given the choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats. The game is as follows:</p>
<ol type="1">
<li>You pick a door.</li>
<li>Monty then opens one of the other two doors, revealing a goat. He can’t open your door or show you a car</li>
<li>You have the choice of switching doors.</li>
</ol>
<p>The question is, is it advantageous to switch? The answer is yes. The probability of winning if you switch is 2/3 and if you don’t switch is 1/3.</p>
<p>Conditional probabilities allow us to answer this question. Assume you pick door 2 (event <span class="math inline">\(A\)</span>) at random, given that the host opened Door 3 and showed a goat (event B), we need to calculate <span class="math inline">\(P(A\mid B)\)</span>. The prior probability that the car is behind Door 2 is <span class="math inline">\(P(A) =  1/3\)</span> and <span class="math inline">\(P(B\mid A) = 1\)</span>, if the car is behind Door 2, the host has no choice but to open Door 3. The Bayes rule then gives us <span class="math display">\[
P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)} = \frac{1/3}{1/2} = \frac{2}{3}.
\]</span> The overall probability of the host opening Door 3 <span class="math display">\[
P(B) = (1/3 \times 1/2) + (1/3 \times 1) = 1/6 + 1/3 = 1/2.
\]</span></p>
<p>The posterior probability that the car is behind Door 2 after the host opens Door 3 is 2/3. It is to your advantage to switch doors.</p>
</div>
<div id="exm-clicker" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Google random clicker)</strong></span> Google provides a service where they ask visitors to your website to answer a single survey question before they get access to the content on the page. Among all of the users, there are two categories</p>
<ol type="1">
<li>Random Clicker (RC)</li>
<li>Truthful Clicker (TC)</li>
</ol>
<p>There are two possible answers to the survey: yes and no. Random clickers would click either one with equal probability. You are also given the information that the expected fraction of random clickers is 0.3. After a trial period, you get the following survey results: 65% said Yes and 35% said No.</p>
<p>The question is: How many people who are truthful clickers answered yes <span class="math inline">\(P(Y\mid TC)\)</span>?</p>
<p>We are given <span class="math inline">\(P(Y\mid RC) = P(N\mid RC) = 0.5\)</span>, <span class="math inline">\(P(RC)= 0.3\)</span> and <span class="math inline">\(P(Y)\)</span> = 0.65</p>
<p>The total probability is <span class="math display">\[
P(Y) = P(Y\mid RC)P(RC) + P(Y\mid TC)P(TC) = 0.65,
\]</span> Thus <span class="math display">\[
P(Y\mid TC) = (P(Y) - P(Y\mid RC)P(RC))/P(TC) = (0.65-0.5\cdot 0.3)/0.7 = 0.71
\]</span></p>
</div>
<div id="exm-prosecutors-fallacy" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (The Prosecutor’s Fallacy)</strong></span> The Prosecutor’s Fallacy is a logical error that occurs when a prosecutor presents evidence or statistical data in a way that suggests a defendant’s guilt, even though the evidence is not as conclusive as it may seem. This fallacy arises from confounding conditional probabilities, specifically equating the probability of evidence given guilt <span class="math inline">\(P(E|G)\)</span> with the probability of guilt given evidence <span class="math inline">\(P(G|E)\)</span>. <span class="math display">\[
P(E\mid G) \ne P(G\mid E)
\]</span> A classic example involves DNA evidence. Suppose you’re serving on a jury in a city with a population of 10 million. A defendant is accused of a crime based on a DNA match found at the scene. A forensic scientist testifies that the probability of an innocent person’s DNA matching the sample is one in a million (<span class="math inline">\(P(E|\bar G) = 10^{-6}\)</span>).</p>
<p>The prosecutor argues that this means there is only a one in a million chance the defendant is innocent. This is the fallacy. You are charged with assessing <span class="math inline">\(P(G \mid E)\)</span> - the probability of guilt given the match.</p>
<p>Let’s apply Bayes’ rule. - <span class="math inline">\(P(G) \approx 1/10^7\)</span> (Prior probability, assuming random citizen). - <span class="math inline">\(P(E|\bar G) = 10^{-6}\)</span> (False positive rate). - <span class="math inline">\(P(E|G) \approx 1\)</span> (Sensitivity).</p>
<p><span class="math display">\[
P(G\mid E) = \frac{P(E\mid G)P(G)}{P(E\mid G)P(G) + P(E\mid \bar G)P(\bar G)}
\]</span> <span class="math display">\[
P(G\mid E) \approx \frac{1 \cdot 10^{-7}}{1 \cdot 10^{-7} + 10^{-6} \cdot 1} = \frac{10^{-7}}{1.1 \times 10^{-6}} \approx \frac{1}{11} \approx 0.09
\]</span></p>
<p>Despite the “one in a million” match rarity, the probability of guilt is only about 9%! There are 10 million people, so we expect about 10 innocent matches (<span class="math inline">\(10^7 \times 10^{-6}\)</span>) and 1 guilty match. Thus, out of 11 matches, only 1 is guilty. The prosecutor’s argument ignores the base rate.</p>
</div>
<div id="exm-island" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (Island Problem)</strong></span> There are <span class="math inline">\(N+1\)</span> people on the island and one is a criminal. We have probability of a trait of a criminal equal to <span class="math inline">\(p\)</span>, which is <span class="math inline">\(p = P(E\mid I)\)</span>, the probability of evidence, given innocence. Then we have a suspect who is matching the trait and we need to find probability of being guilty, given the evidence <span class="math inline">\(P(G \mid E)\)</span>. It is easier to do the Bayes rule in the odds form. There are three components to the calculations: the prior odds of innocence, <span class="math display">\[
O ( I ) = P (G) / P ( I ),
\]</span> the Bayes factor, <span class="math display">\[
\frac{P(E\mid G)}{P(E\mid I)}.
\]</span> and the posterior odds of innocence. <span class="math display">\[
    O(I\mid E) = \dfrac{P(G\mid E)}{P(I\mid E)} = \dfrac{1}{Np}.
\]</span></p>
<p>Cromwell’s rule states that the use of prior probability of 1 or 0 should be avoided except when it is known for certain that the probability is 1 or 0. It is named after Oliver Cromwell who wrote to the General Assembly of the Church of Scotland in 1650 “<em>I beseech you, in the bowels of Christ, think it possible that you may be mistaken</em>”. In other words, using the Bayes rule <span class="math display">\[
P(G\mid E) = \dfrac{P(E\mid G)}{P(E)}P(G),
\]</span> if <span class="math inline">\(P(G)\)</span> is zero, it does not matter what the evidence is. Symmetrically, probability of innocence is zero if the evidence is certain. In other words, if <span class="math inline">\(P(E\mid I) = 0\)</span>, then <span class="math inline">\(P(I\mid E) = 0\)</span>. This is a very strong statement. It is not always true, but it is a good rule of thumb, it is a good way to avoid the prosecutor’s fallacy.</p>
</div>
<div id="exm-nakamura" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 (Nakamura’s Alleged Cheating)</strong></span> In our paper <span class="citation" data-cites="maharaj2023kramnik">Maharaj, Polson, and Sokolov (<a href="references.html#ref-maharaj2023kramnik" role="doc-biblioref">2023</a>)</span>, we provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex-world champion) and Hikaru Nakamura. Kramnik called into question Nakamura’s 45.5 out of 46 win streak in a 3+0 online blitz contest at chess.com. In this example we reproduce this paper and assess the weight of evidence using an a priori probabilistic assessment of Viswanathan Anand and the streak evidence of Kramnik. Our analysis shows that Nakamura has a 99.6 percent chance of not cheating given Anand’s prior assumptions.</p>
<p>We start by addressing the argument of Kramnik which is based on the fact that the probability of such a streak is very small. This falls into precisely the <strong>Prosecutor’s Fallacy</strong>, as discussed in <a href="#exm-prosecutors-fallacy" class="quarto-xref">Example&nbsp;<span>2.10</span></a>. We denote by <span class="math inline">\(G\)</span> the event of being guilty and <span class="math inline">\(I\)</span> the event of innocence. We use <span class="math inline">\(E\)</span> to denote evidence (streak of wins). Kramnik’s argument is that because the probability of observing the streak is very low (<span class="math inline">\(P(E|I)\)</span> is small), it implies a high probability of cheating (<span class="math inline">\(P(G|E)\)</span> is high). As we have seen, this reasoning is flawed because it neglects the prior probability of cheating. Kramnik’s calculations neglect other relevant factors, such as the prior probability of cheating. The prosecutor’s fallacy can lead to an overestimation of the strength of the evidence and may result in an unjust conviction. In the cheating problem, at the top level of chess the prior probability of <span class="math inline">\(P(G)\)</span> is small! According to a recent statement by Viswanathan Anand, the probability of cheating is <span class="math inline">\(1/10000\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/anand-prior.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Anand’s Prior</figcaption>
</figure>
</div>
<p>Given the prior ratio of cheaters to not cheaters is <span class="math inline">\(1/N\)</span>, meaning out of <span class="math inline">\(N+1\)</span> players, there is one cheater, the Bayes calculations require two main terms. The first one is the prior odds of guilt: <span class="math display">\[
O ( G ) = P (I) / P ( G ).
\]</span> Here <span class="math inline">\(P(I)\)</span> and <span class="math inline">\(P(G)\)</span> are the prior probabilities of innocence and guilt respectively.</p>
<p>The second term is the Bayes factor, which is the ratio of the probability of the evidence under the guilt hypothesis to the probability of the evidence under the innocence hypothesis. The Bayes factor is given by <span class="math display">\[
    L(E\mid G) = \frac{P(E\mid I)}{P(E\mid G)}.
\]</span></p>
<p>The product of the Bayes factor and the prior odds is the posterior odds of guilt, given the evidence. The posterior odds of guilt is given by <span class="math display">\[
    O(G\mid E) = O(G) \times L(E\mid G).
\]</span></p>
<p>The odds of guilt is <span class="math display">\[
    O ( G )  = \dfrac{N/(N+1)}{1/(N+1)} = N.
\]</span></p>
<p>The Bayes factor is given by <span class="math display">\[
\frac{P(E\mid I)}{P(E\mid G)} = \dfrac{p}{1} = p.
\]</span> Thus, the posterior odds of guilt are <span class="math display">\[
    O(G\mid E) = Np.
\]</span> There are two numbers we need to estimate to calculate the odds of cheating given the evidence, namely the prior probability of cheating given via <span class="math inline">\(N\)</span> and the probability of a streak <span class="math inline">\(p = P(E\mid I)\)</span>.</p>
<p>There are multiple ways to calculate the probability of a streak. We can use the binomial distribution, the negative binomial distribution, or the Poisson distribution. The binomial distribution is the most natural choice. The probability of a streak of <span class="math inline">\(k\)</span> wins in a row is given by <span class="math display">\[
    P(E\mid I) = \binom{N}{k} q^k (1-q)^{N-k}.
\]</span> Here <span class="math inline">\(q\)</span> is the probability of winning a single game. Thus, for a streak of 45 wins in a row, we have <span class="math inline">\(k = 45\)</span> and <span class="math inline">\(N = 46\)</span>. We encode the outcome of a game as <span class="math inline">\(1\)</span> for a win and <span class="math inline">\(0\)</span> for a loss or a draw. The probability of a win is <span class="math inline">\(q = 0.8916\)</span> (Nakamura’s Estimate, he reported on his YouTube channel). The probability of a streak is then 0.029. The individual game win probability is calculated from the ELO rating difference between the players.</p>
<p>The ELO rating of Hikaru is 3300 and the average ELO rating of his opponents is 2950, according to Kramnik. The difference of 350 corresponds to the odds of winning of <span class="math inline">\(wo = 10^{350/400} = 10^{0.875} = 7.2\)</span>. The probability of winning a single game is <span class="math inline">\(q = wo/(1+wo) = 0.8916\)</span>.</p>
<p>Then we use Anand’s prior of <span class="math inline">\(N = 10000\)</span> to get the posterior odds of cheating given the evidence of a streak of 45 wins in a row. The posterior odds of being innocent are 285. The probability of cheating is then <span class="math display">\[
P(G\mid E) = 1/(1+O(G\mid E)) = 0.003491.
\]</span> Therefore the probability of innocence <span class="math display">\[
    P(I\mid E) = \frac{Np}{Np+1} = 0.9965.
\]</span></p>
<p>For completeness, we perform sensitivity analysis and also get the odds of not cheating for <span class="math inline">\(N = 500\)</span>, which should be a high prior probability given the status of the player and the importance of the event. We get <span class="math display">\[
    P(I\mid E) = \frac{Np}{Np+1} = 0.9445.
\]</span></p>
<p>There are several assumptions we made in this analysis.</p>
<ul>
<li>Instead of calculating game-by-game probability of winning, we used the average probability of winning of 0.8916, provided by Nakamura himself. This is a reasonable assumption given the fact that Nakamura is a much stronger player than his opponents. This assumption slightly shifts posterior odds in favor of not cheating. Due to Jensen’s inequality, we have <span class="math inline">\(E(q^{50}) &gt; E(q)^{50}\)</span>. Expected value of the probability of winning a single game is <span class="math inline">\(E(q) = 0.8916\)</span> and the expected value of the probability of a streak of 50 wins is <span class="math inline">\(E(q^{50})\)</span>. We consider the difference between the two to be small. Further, there is some correlation between the games, which also shifts the posterior odds in favor of not cheating. For example, some players are on tilt. Given they lost the first game, they are more likely to lose the second game.</li>
<li>There are many ways to win 3+0 unlike in classical chess. For example, one can win on time. We argue that the probability of winning calculated from the ELO rating difference is underestimated.</li>
</ul>
<p>Next, we can use the Bayes analysis to solve an inverse problem and to find what prior you need to assume and how long of a sequence you need to observe to get 0.99 posterior? Small sample size, we have <span class="math inline">\(p\)</span> close to 1. <a href="#fig-n-p" class="quarto-xref">Figure&nbsp;<span>2.1</span></a> shows the combination of prior (<span class="math inline">\(N\)</span>) and the probability of a streak (<span class="math inline">\(p\)</span>) that gives posterior odds of 0.99.</p>
<p>Indeed, the results of the Bayesian analysis contradict the results of a traditional p-value based approach. A p-value is a measure used in frequentist statistical hypothesis testing. It represents the probability of obtaining the observed results, or results more extreme, assuming that the null hypothesis is true. The null hypothesis is a default position that Nakamura is not cheating and we compare the ELO-based expected win probability of <span class="math inline">\(q=0.8916\)</span> to the observed one of <span class="math inline">\(s=45/46=0.978\)</span>. Under the null hypothesis, Nakamura should perform at the level predicted by <span class="math inline">\(q\)</span>.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>q <span class="ot">=</span> <span class="fl">0.8916</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">dbinom</span>(<span class="dv">45</span>,<span class="dv">46</span>,q)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>odds <span class="ot">=</span> p<span class="sc">*</span>N</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="dv">1-1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>odds))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>odds))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.0035</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(N<span class="sc">*</span>p<span class="sc">/</span>(N<span class="sc">*</span>p<span class="sc">+</span><span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from=</span><span class="fl">0.006</span>, <span class="at">to=</span><span class="fl">0.07</span>, <span class="at">length.out=</span><span class="dv">500</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">500</span>,<span class="dv">10000</span>, <span class="at">by=</span><span class="dv">250</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">99</span><span class="sc">/</span>N,N,<span class="at">xlab=</span><span class="st">"p"</span>, <span class="at">ylab=</span><span class="st">"N"</span>, <span class="at">type=</span><span class="st">'l'</span>, <span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span><span class="st">"blue"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div id="fig-n-p" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-n-p-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-bayes_files/figure-html/fig-n-p-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-n-p-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The combination of prior (<span class="math inline">\(N\)</span>) and the probability of a streak (<span class="math inline">\(p\)</span>) that gives posterior odds of 0.99.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>David Hume discussed the problem similar to the Island problem in his “On Miracles” essay. Hume is making the following argument on miracles:</p>
<blockquote class="blockquote">
<p>“<em>…no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavors to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior.</em>”</p>
</blockquote>
<p>One can view this as an application of the Island problem. Assuming the probability of a miracle <span class="math inline">\(A\)</span> is <span class="math inline">\(p( A) = p\)</span> and <span class="math inline">\(p( not \; A ) = 1 -p\)</span>. Then Bayes rule gives <span class="math display">\[
p( A| a ) = \frac{ p( a| A) p }{  p( a| A) p  +  p( a |  not \; A) (1-p) }  
\]</span> Prosecutor’s fallacy, <span class="math inline">\(p( a| not \; A)  \neq 1 -  p( a| A)\)</span>, in general.</p>
<p>In Hume’s assessment of miracles (has to be something not in the laws of nature) we have <span class="math inline">\(p(A) = 10^{-6}\)</span>. This assessment takes into account background information, <span class="math inline">\(I\)</span>. Rare to have a contradiction to the laws of nature. More informative to write <span class="math inline">\(p( A | I )\)</span>. Furthermore, we take <span class="math inline">\(p( a| A) =0.99\)</span>. The hardest bit is to assess <span class="math inline">\(p(a | not \; A )\)</span>. The “frequency” of faked miracles and mankind’s propensity to be marvelous. We assess <span class="math inline">\(p(a | not \; A )  = 10^{-3}\)</span>. This yields the chance of a miracle to be unlikely as <span class="math display">\[
p( A| a ) = \frac{ 0.99 \times 10^{-6}  }{  0.99 \times 10^{-6}    +  10^{-3} (1- 10^{-6}) }   \approx 10^{-3}.
\]</span> Feynman considers the inverse problem: can we learn the laws of nature purely from empirical observation? Uses chess as an example. Is it a miracle that we have two bishops of the same color? No! according to Hume. We just didn’t know the laws of nature (a.k.a. model).</p>
<div id="exm-Sally" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (Sally Clark Case: Independence or Bayes Rule?)</strong></span> To show that independence can lead to dramatically different results from Bayes conditional probabilities, consider the Sally Clark case. Sally Clark was accused and convicted of killing her two children who could have both died of SIDS. One explanation is that this was a random occurrence, the other one is that they both died of sudden infant death syndrome (SIDS). How can we use conditional probability to figure out a reasonable assessment of the probability that she murdered her children. First, some known probability assessments</p>
<ol type="1">
<li>The chance of a family of non-smokers having a SIDS death is <span class="math inline">\(1\)</span> in <span class="math inline">\(8,500\)</span>.</li>
<li>The chance of a second SIDS death is <span class="math inline">\(1\)</span> in <span class="math inline">\(100\)</span>.</li>
<li>The chance of a mother killing her two children is around <span class="math inline">\(1\)</span> in <span class="math inline">\(1,000,000\)</span>.</li>
</ol>
<p>Under Bayes <span class="math display">\[\begin{align*}
\prob{\mathrm{both} \; \; \mathrm{SIDS}}   &amp;  = \prob{\mathrm{first} \; \mathrm{SIDS}} \prob{\mathrm{Second} \; \;\mathrm{SIDS} \mid \mathrm{first} \; \mathrm{SIDS}}\\
&amp;  = \frac{1}{8500} \cdot \frac{1}{100} = \frac{1}{850,000}.
\end{align*}\]</span></p>
<p>The <span class="math inline">\(1/100\)</span> comes from taking into account the genetic properties of SIDS. Independence, as implemented by the court, gets you to a probabilistic assessment of <span class="math display">\[
P \left(  \mathrm{both} \; \; \mathrm{SIDS} \right)  = (1/8500) (1/8500) = (1/73,000,000).
\]</span> This is a low probability. It is still not the answer to our question of context. We need a conditional probability, this will come to the Bayes rule.</p>
<p>First, some general comment on the likelihood ratio calculation used to assess the weight of evidence in favor of guilty v.s. innocent evidence. Under Bayes we’ll find that there’s reasonable evidence that she’d be acquitted. We need the relative odds ratio. Let <span class="math inline">\(I\)</span> denote the event that Sally Clark is innocent and <span class="math inline">\(G\)</span> denotes guilty. Let <span class="math inline">\(E\)</span> denote the evidence. In most cases, <span class="math inline">\(E\)</span> contains a sequence <span class="math inline">\(E_1, E_2, \ldots\)</span> of ‘facts’ and we have to use the likelihood ratios in turn. Bayes rule then tells you to combine via multiplicative fashion. If likelihood ratio <span class="math inline">\(&gt;1\)</span>, odds of guilty. If likelihood ratio <span class="math inline">\(&lt;1\)</span>, more likelihood to be <span class="math inline">\(I\)</span>. By Bayes rule <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{p( E\text{ and } I)}{p( E\text{ and } G)}.
\]</span> If we further decompose <span class="math inline">\(p(E \text{ and } I) = p(E\mid I )p(I)\)</span> then we have to discuss the prior probability of innocence, namely <span class="math inline">\(p(I)\)</span>. Hence this is one subtle advantage of the above decomposition.</p>
<p>The underlying intuition that Bayes gives us in this example, is that of the two possible explanations of the data, both of which are unlikely, it is the relative likelihood comparison that should matter. Here is a case where the <span class="math inline">\(p\)</span>-value would be non-sensible (<span class="math inline">\(p(E\mid I) \neq p(I\mid E)\)</span>). Effectively comparing two rare event probabilities from the two possible models or explanations.</p>
<p>Hence putting these two together gives the odds of guilt as <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{1/850,000}{1/1,000,000} = 1.15.
\]</span> Solving for the posterior probability yields <span class="math inline">\(46.5\%\)</span> for probability of guilty given evidence. <span class="math display">\[
p( G\mid E) = \frac{1}{1 + O(G\mid E)} = 0.465.
\]</span> Basically a <span class="math inline">\(50/50\)</span> bet. Not enough to definitively convict! But remember that our initial prior probability on guilt <span class="math inline">\(p(G)\)</span> was <span class="math inline">\(10^{-6}\)</span>. So now there has been a dramatic increase to a posterior probability of <span class="math inline">\(0.465\)</span>. So it’s not as if Bayes rule thinks this is evidence in the suspect’s favor – but the magnitude is still not in the <span class="math inline">\(0.999\)</span> range though, where most jurors would have to be to feel comfortable with a guilt verdict.</p>
<p>If you use the “wrong” model of independence (as the court did) you get <span class="math display">\[
P \left(  \mathrm{both} \; \; \mathrm{SIDS} \right)  = \frac{1}{8500}
  \cdot\frac{1}{8500} = \frac{1}{73,000,000}.
\]</span> With the independence assumption, you make the assessment <span class="math display">\[
\frac{p(I\mid E)}{p(G\mid E)} = \frac{1}{73} \; \mathrm{ and} \; p( G\mid E) \approx 0.99.
\]</span> Given these probability assumptions, the suspect looks guilty with probability 99%.</p>
<p>Experts also mis-interpret the evidence by saying: <em>1 in 73 million chance that it is someone else</em>. This is clearly false and misleading to the jury and has led to appeals.</p>
</div>
<div id="exm-Simpson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14 (O. J. Simpson Case: Dershowitz Fallacy)</strong></span> &nbsp;</p>
<!-- https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00839.x -->
<p>This example is based on I. J. Good’s, “When batterer turns murderer.” Nature, 15 June 1995, p.&nbsp;541. Alan Dershowitz, on the O. J. Simpson defense team, stated on T.V. and in newspapers that only 1 in 2,500 of men who abuse their wives go on to murder them. He clearly wanted his audience to interpret this to mean that the evidence of abuse by Simpson would only suggest a 0.04% probability of his being guilty of murdering her. He used probability to argue that because so few husbands who batter their wives actually go on to murder their wives. Thus, O.J. is highly likely to be not guilty. This leaves out the most relevant conditioning information that we also know that Nicole Brown Simpson was actually murdered. Both authors believe the jury would be more interested in the probability that the husband is guilty of the murder of his wife given that he abused his wife and his wife was murdered. They both solve this problem by using Bayes’ theorem.</p>
<p>In this example, the notation <span class="math inline">\(B\)</span> represents “woman battered by her husband, boyfriend, or lover”, <span class="math inline">\(M\)</span> represents the event “woman murdered”, and <span class="math inline">\(G\)</span> denotes “woman murdered by her batterer”. Our goal is to show that <span class="math display">\[
% P(M,B \mid M) \neq P(M,B \mid B).
P(G \mid M,B) \neq P(G\mid B).
\]</span></p>
<p>It is not hard to come to a wrong conclusion if you don’t take into account all the relevant conditional information. He intended this information to exonerate O.J. In 1992 the women population of the US was 125 million and 4936 women were murdered, thus <span class="math display">\[
P(M) = 4936/125,000,000 = 0.00004 = 1/25,000.
\]</span> At the same year about 3.5 million women were battered <span class="math display">\[
P(B) = 3.5/125 = 0.028.
\]</span> That same year 1432 women were murdered by their previous batterers, so the marginal probability of that event is <span class="math inline">\(P(G) = 1432/125,000,000 = 0.00001 = 1/87,290\)</span>, and the conditional probability, <span class="math inline">\(P(G | B)\)</span> is 1432 divided by 3.5 million, or <span class="math inline">\(1/2444\)</span>. These are the numbers Dershowitz used to obtain his estimate that about 1 in 2500 battered women go on to be murdered by their batterers.</p>
<p>We need to calculate <span class="math display">\[
P(G \mid M,B) = P(M | G,B) P(G) / P(M).
\]</span> We know <span class="math inline">\(P(M | G,B) = 1\)</span> and <span class="math inline">\(P(G) / P(M) = 0.00001/0.00004 = 0.29\)</span>, or about 1 in 3.5.</p>
<p>Alan Dershowitz provided the jury with an accurate but irrelevant probability. The fact the woman was murdered increases the probability that she was murdered by her batterer by a factor of 709 (0.29/(1/2444)). <span class="math display">\[
P(G\mid M,B)\approx 709\times P(G\mid B).
\]</span></p>
<p>The argument used by Dershowitz relating to the Simpson case has been discussed by John Paulos in an op-ed article in the Philadelphia Inquirer (15 Oct.&nbsp;1995, C7) and his book “Once Upon a Number”, by I.J. Good in an article in Nature (June 15,1995, p 541) and by Jon Merz and Jonathan Caulkins in an article in Chance Magazine, (Spring 1995, p 14).</p>
</div>
<div id="exm-election" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15</strong></span> ## Obama Elections This example demonstrates a Bayesian approach to election forecasting using polling data from the 2012 US presidential election. The goal is to predict the probability of Barack Obama winning the election by combining polling data across different states.</p>
<p>The data used includes polling data from various pollsters across all 50 states plus DC. Each state has polling percentages for Republican (GOP) and Democratic (Dem) candidates along with their electoral vote counts. The data is aggregated by state, taking the most recent polls available.</p>
<p>The techniques applied involve Bayesian simulation using a Dirichlet distribution to model uncertainty in polling percentages. Monte Carlo simulation runs 10,000 simulations of the election to estimate win probabilities. The analysis is conducted state-by-state, calculating Obama’s probability of winning each individual state. Electoral college modeling combines state probabilities with electoral vote counts to determine the overall election outcome. The simulation runs the entire election multiple times to account for uncertainty and determines the likelihood of Obama reaching the required 270 electoral votes to win. This approach demonstrates how pattern matching through statistical modeling can be used for prediction, showing how polling data can be transformed into probabilistic forecasts of election outcomes.</p>
<p>We start by loading the data and aggregating it by state.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plyr)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: "http://www.electoral-vote.com/evp2012/Pres/pres_polls.csv"</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>election<span class="fl">.2012</span> <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/pres_polls.csv"</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove a pollster: elect2012 &lt;- election.2012[!grepl('Rasmussen', election.2012$Pollster),]</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> election<span class="fl">.2012</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Aggregrate the data</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), subset, Day <span class="sc">==</span> <span class="fu">max</span>(Day))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>elect2012 <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), summarise, <span class="at">R.pct =</span> <span class="fu">mean</span>(GOP), <span class="at">O.pct =</span> <span class="fu">mean</span>(Dem), <span class="at">EV =</span> <span class="fu">mean</span>(EV))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(elect2012[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>,], <span class="at">caption =</span> <span class="st">"Election 2012 Data"</span>,<span class="at">longtable=</span><span class="cn">TRUE</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(elect2012[<span class="dv">26</span><span class="sc">:</span><span class="dv">51</span>,], <span class="at">caption =</span> <span class="st">"Election 2012 Data"</span>,<span class="at">longtable=</span><span class="cn">TRUE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="cell quarto-layout-cell" data-null_prefix="true" style="flex-basis: 50.0%;justify-content: center;">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Election 2012 Data</caption>
<thead>
<tr class="header">
<th style="text-align: left;">state</th>
<th style="text-align: right;">R.pct</th>
<th style="text-align: right;">O.pct</th>
<th style="text-align: right;">EV</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Alabama</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alaska</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Arizona</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">Arkansas</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">California</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">55</td>
</tr>
<tr class="even">
<td style="text-align: left;">Colorado</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Connecticut</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">D.C.</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Delaware</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">Florida</td>
<td style="text-align: right;">49</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Georgia</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: left;">Hawaii</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">71</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Idaho</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Illinois</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Indiana</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">Iowa</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kansas</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Kentucky</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Louisiana</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">Maine</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maryland</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">Massachusetts</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Michigan</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">16</td>
</tr>
<tr class="even">
<td style="text-align: left;">Minnesota</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mississippi</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Election 2012 Data</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">state</th>
<th style="text-align: right;">R.pct</th>
<th style="text-align: right;">O.pct</th>
<th style="text-align: right;">EV</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">26</td>
<td style="text-align: left;">Missouri</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">27</td>
<td style="text-align: left;">Montana</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">28</td>
<td style="text-align: left;">Nebraska</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">38</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">29</td>
<td style="text-align: left;">Nevada</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">30</td>
<td style="text-align: left;">New Hampshire</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">31</td>
<td style="text-align: left;">New Jersey</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">32</td>
<td style="text-align: left;">New Mexico</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="even">
<td style="text-align: left;">33</td>
<td style="text-align: left;">New York</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">29</td>
</tr>
<tr class="odd">
<td style="text-align: left;">34</td>
<td style="text-align: left;">North Carolina</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">15</td>
</tr>
<tr class="even">
<td style="text-align: left;">35</td>
<td style="text-align: left;">North Dakota</td>
<td style="text-align: right;">59</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">36</td>
<td style="text-align: left;">Ohio</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">18</td>
</tr>
<tr class="even">
<td style="text-align: left;">37</td>
<td style="text-align: left;">Oklahoma</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: left;">38</td>
<td style="text-align: left;">Oregon</td>
<td style="text-align: right;">43</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">39</td>
<td style="text-align: left;">Pennsylvania</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">52</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">40</td>
<td style="text-align: left;">Rhode Island</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">63</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">41</td>
<td style="text-align: left;">South Carolina</td>
<td style="text-align: right;">55</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">42</td>
<td style="text-align: left;">South Dakota</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">43</td>
<td style="text-align: left;">Tennessee</td>
<td style="text-align: right;">60</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="odd">
<td style="text-align: left;">44</td>
<td style="text-align: left;">Texas</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">38</td>
</tr>
<tr class="even">
<td style="text-align: left;">45</td>
<td style="text-align: left;">Utah</td>
<td style="text-align: right;">73</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">46</td>
<td style="text-align: left;">Vermont</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">67</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="even">
<td style="text-align: left;">47</td>
<td style="text-align: left;">Virginia</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">51</td>
<td style="text-align: right;">13</td>
</tr>
<tr class="odd">
<td style="text-align: left;">48</td>
<td style="text-align: left;">Washington</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">12</td>
</tr>
<tr class="even">
<td style="text-align: left;">49</td>
<td style="text-align: left;">West Virginia</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">50</td>
<td style="text-align: left;">Wisconsin</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">53</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">51</td>
<td style="text-align: left;">Wyoming</td>
<td style="text-align: right;">69</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>We then run the simulation and plot probabilities by state.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MCMCpack)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>prob.Obama <span class="ot">&lt;-</span> <span class="cf">function</span>(mydata) {</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">rdirichlet</span>(<span class="dv">1000</span>, <span class="dv">500</span> <span class="sc">*</span> <span class="fu">c</span>(mydata<span class="sc">$</span>R.pct, mydata<span class="sc">$</span>O.pct, <span class="dv">100</span> <span class="sc">-</span> mydata<span class="sc">$</span>R.pct <span class="sc">-</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    mydata<span class="sc">$</span>O.pct)<span class="sc">/</span><span class="dv">100</span> <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(p[, <span class="dv">2</span>] <span class="sc">&gt;</span> p[, <span class="dv">1</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>win.probs <span class="ot">&lt;-</span> <span class="fu">ddply</span>(elect2012, .(state), prob.Obama)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>win.probs<span class="sc">$</span>Romney <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> win.probs<span class="sc">$</span>V1</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(win.probs)[<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="st">"Obama"</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>win.probs<span class="sc">$</span>EV <span class="ot">&lt;-</span> elect2012<span class="sc">$</span>EV</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>win.probs <span class="ot">&lt;-</span> win.probs[<span class="fu">order</span>(win.probs<span class="sc">$</span>EV), ]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(win.probs) <span class="ot">&lt;-</span> win.probs<span class="sc">$</span>state</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We then plot the probabilities of Obama winning by state.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(usmap)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_usmap</span>(<span class="at">data =</span> win.probs, <span class="at">values =</span> <span class="st">"Obama"</span>) <span class="sc">+</span> </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_continuous</span>(<span class="at">low =</span> <span class="st">"red"</span>, <span class="at">high =</span> <span class="st">"blue"</span>, <span class="at">name =</span> <span class="st">"Obama Win Probability"</span>, <span class="at">label =</span> scales<span class="sc">::</span>comma) <span class="sc">+</span> <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"right"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="02-bayes_files/figure-html/obama-map-1.png" class="img-fluid figure-img" width="576"></p>
<figcaption>Probabilities of Obama winning by state</figcaption>
</figure>
</div>
</div>
</div>
<p>We use those probabilities to simulate the probability of Obama winning the election. First, we calculate the probability of Obama having 270 EV or more</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>sim.election <span class="ot">&lt;-</span> <span class="cf">function</span>(win.probs) {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    winner <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">51</span>, <span class="dv">1</span>, win.probs<span class="sc">$</span>Obama)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(win.probs<span class="sc">$</span>EV <span class="sc">*</span> winner)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>sim.EV <span class="ot">&lt;-</span> <span class="fu">replicate</span>(<span class="dv">10000</span>, <span class="fu">sim.election</span>(win.probs))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>oprob <span class="ot">&lt;-</span> <span class="fu">sum</span>(sim.EV <span class="sc">&gt;=</span> <span class="dv">270</span>)<span class="sc">/</span><span class="fu">length</span>(sim.EV)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>oprob</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code> 0.97</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Lattice Graph</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">densityplot</span>(sim.EV, <span class="at">plot.points =</span> <span class="st">"rug"</span>, <span class="at">xlab =</span> <span class="st">"Electoral Votes for Obama"</span>, </span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">panel =</span> <span class="cf">function</span>(x, ...) {</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">panel.densityplot</span>(x, ...)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">panel.abline</span>(<span class="at">v =</span> <span class="dv">270</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>      <span class="fu">panel.text</span>(<span class="at">x =</span> <span class="dv">285</span>, <span class="at">y =</span> <span class="fl">0.01</span>, <span class="st">"270 EV to Win"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>      <span class="fu">panel.abline</span>(<span class="at">v =</span> <span class="dv">332</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">panel.text</span>(<span class="at">x =</span> <span class="dv">347</span>, <span class="at">y =</span> <span class="fl">0.01</span>, <span class="st">"Actual Obama"</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>}, <span class="at">main =</span> <span class="st">"Electoral College Results Probability"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="02-bayes_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>Results of recent state polls in the 2008 United States Presidential Election between Barack Obama and John McCain.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Source: LearnBayes library</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>election<span class="fl">.2008</span> <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">"../data/election2008.csv"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(election<span class="fl">.2008</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(election<span class="fl">.2008</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  Dirichlet simulation</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>prob.Obama <span class="ot">=</span> <span class="cf">function</span>(j)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>p<span class="ot">=</span><span class="fu">rdirichlet</span>(<span class="dv">5000</span>,<span class="dv">500</span><span class="sc">*</span><span class="fu">c</span>(M.pct[j],O.pct[j],<span class="dv">100</span><span class="sc">-</span>M.pct[j]<span class="sc">-</span>O.pct[j])<span class="sc">/</span><span class="dv">100</span><span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(p[,<span class="dv">2</span>]<span class="sc">&gt;</span>p[,<span class="dv">1</span>])</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="do">## sapply function to compute Obama win prob for all states</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>Obama.win.probs<span class="ot">=</span><span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">51</span>,prob.Obama)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  sim.EV function</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>sim.election <span class="ot">=</span> <span class="cf">function</span>()</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>winner <span class="ot">=</span> <span class="fu">rbinom</span>(<span class="dv">51</span>,<span class="dv">1</span>,Obama.win.probs)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(EV<span class="sc">*</span>winner)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>sim.EV <span class="ot">=</span> <span class="fu">replicate</span>(<span class="dv">1000</span>,<span class="fu">sim.election</span>())</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="do">## histogram of simulated election</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(sim.EV,<span class="fu">min</span>(sim.EV)<span class="sc">:</span><span class="fu">max</span>(sim.EV),<span class="at">col=</span><span class="st">"blue"</span>,<span class="at">prob=</span>T)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">365</span>,<span class="at">lwd=</span><span class="dv">3</span>)   <span class="co"># Obama received 365 votes</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="dv">375</span>,<span class="dv">30</span>,<span class="st">"Actual </span><span class="sc">\n</span><span class="st"> Obama </span><span class="sc">\n</span><span class="st"> total"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="02-bayes_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The analysis of the 2008 U.S. Presidential Election data reveals several key insights about the predictive power of state-level polling and the uncertainty inherent in electoral forecasting. The actual result of 365 electoral votes falls within the simulated range, demonstrating the model’s validity. The 270-vote threshold needed to win the presidency is clearly marked and serves as a critical reference point.</p>
<p>We used a relatively simple model to simulate the election outcome. The model uses Dirichlet distributions to capture uncertainty in state-level polling percentages. Obama’s win probabilities vary significantly across states, reflecting the competitive nature of the election. The simulation approach accounts for both sampling uncertainty and the discrete nature of electoral vote allocation. The histogram of simulated results shows the distribution of possible outcomes. The actual Obama total of 365 electoral votes is marked and falls within the reasonable range of simulated outcomes. This validates the probabilistic approach to election forecasting.</p>
<p>This analysis demonstrates how Bayesian methods can be effectively applied to complex prediction problems with multiple sources of uncertainty, providing both point estimates and measures of uncertainty around those estimates.</p>
</div>
</section>
</section>
<section id="sec-sensitivity" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-sensitivity"><span class="header-section-number">2.4</span> Sensitivity and Specificity</h2>
<p>Conditional probabilities are used to define two fundamental metrics used for many probabilistic and statistical learning models, namely <em>sensitivity</em> and <em>specificity</em>.</p>
<p>Sensitivity and specificity are two key metrics used to evaluate the performance of diagnostic tests, classification models, or screening tools. These metrics help assess how well a test can correctly identify individuals with a condition (true positives) and those without the condition (true negatives). Let’s break down each term:</p>
<ol type="1">
<li><em>Sensitivity (true‐positive rate or recall)</em> is the ability of a test <span class="math inline">\(T\)</span> to correctly identify individuals who have a particular condition or disease (<span class="math inline">\(D\)</span>), <span class="math inline">\(P ( T=1 \mid D=1 )\)</span>, the probability of a positive test given that the individual has the disease. It is calculated as the ratio of true positives to the sum of true positives and false negatives. <span class="math display">\[
p(T=1\mid D=1) = \dfrac{p(T=1,D=1)}{p(D=1)}.
\]</span> A high sensitivity indicates that the test is good at identifying individuals with the condition, minimizing false negatives.</li>
<li><em>Specificity (true‐negative rate)</em> is the ability of a test to correctly identify individuals who do not have a particular condition or disease, <span class="math inline">\(P (T=0 \mid D=0 )\)</span>. It is calculated as the ratio of true negatives to the sum of true negatives and false positives. <span class="math display">\[
p(T=0\mid D=0) = \dfrac{p(T=0,D=0)}{p(D=0)}
\]</span> A high specificity indicates that the test is good at correctly excluding individuals without the condition, minimizing false positives.</li>
</ol>
<p>Sensitivity and specificity are often trade-offs. Increasing sensitivity might decrease specificity, and vice versa. Thus, depending on the application, you might prefer sensitivity over specificity or vice versa, depending on the consequences of false positives and false negatives in a particular application.</p>
<p>Consider a medical test designed to detect a certain disease. If the test has high sensitivity, it means that it is good at correctly identifying individuals with the disease. On the other hand, if the test has high specificity, it is good at correctly identifying individuals without the disease. The goal is often to strike a balance between sensitivity and specificity based on the specific needs and implications of the test results.</p>
<!-- -   **Sensitivity** measures the true positive rate, or recall, for
  example, the percentage of sick people who are correctly identified
  $P ( T=1 \mid  D=1 )$.
-   **Specificity** measures true negative rate, e.g % of negatives
  corrected identified as such. $P (T=0  \mid  D=0 )$. -->
<p>Sensitivity is often called the power of a procedure (a.k.a. test). There are two kinds of errors (type I and type II) as well as sensitivity and specificity are dual concepts.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Type I error (false positive rate)
</div>
</div>
<div class="callout-body-container callout-body">
<p>is the percentage of healthy people who tested positive, <span class="math inline">\(P(T=1\mid D=0)\)</span>, it is the mistake of thinking something is true when it is not.</p>
</div>
</div>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Type II error (or false negative rate)
</div>
</div>
<div class="callout-body-container callout-body">
<p>is the percentage of sick people who are tested negative, <span class="math inline">\(P(T=0\mid D=1)\)</span>, it is the mistake of thinking something is not true when in fact it is true.</p>
</div>
</div>
<p>We would like to control both conditional probabilities with our test. Also if someone tests positive, how likely is it that they actually have the disease. There are two ‘errors’ one can make. Falsely diagnosing someone, or not correctly finding the disease.</p>
<p>In the stock market, one can think of type I error as not selling a losing stock quickly enough, and a type II error as failing to buy a growing stock, e.g.&nbsp;Amazon or Google.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 18%">
<col style="width: 26%">
<col style="width: 31%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p(T=1\mid D=1)\)</span></td>
<td style="text-align: center;">Sensitivity</td>
<td style="text-align: center;">True Positive Rate</td>
<td style="text-align: center;"><span class="math inline">\(1-\beta\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(p(T=0\mid D=0 )\)</span></td>
<td style="text-align: center;">Specificity</td>
<td style="text-align: center;">True Negative Rate</td>
<td style="text-align: center;"><span class="math inline">\(1-\alpha\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(p(T=1\mid D=0)\)</span></td>
<td style="text-align: center;">1-Specificity</td>
<td style="text-align: center;">False Positive Rate</td>
<td style="text-align: center;"><span class="math inline">\(\alpha\)</span> (type I error)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(p(T=0\mid D =1)\)</span></td>
<td style="text-align: center;">1-Sensitivity</td>
<td style="text-align: center;">False Negative Rate</td>
<td style="text-align: center;"><span class="math inline">\(\beta\)</span> (type II error)</td>
</tr>
</tbody>
</table>
<p>Often it is convenient to write those four values in the form of a two-by-two matrix, called the confusion matrix:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Actual/Predicted</th>
<th>Positive</th>
<th>Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Positive</td>
<td>TP</td>
<td>FN</td>
</tr>
<tr class="even">
<td>Negative</td>
<td>FP</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>where: TP: True Positive. FN: False Negative, FP: False Positive, TN: True Negative</p>
<p>We will extensively use the concepts of errors, specificity and sensitivity later in the book, when describing AB testing and predictive models. These examples illustrate why people can commonly miscalculate and mis-interpret probabilities. Those quantities can be calculated using the Bayes rule.</p>
<div id="exm-Apple" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.16 (Apple Watch Series 4 ECG and Bayes’ Theorem)</strong></span> The Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives) <span class="citation" data-cites="Kim2024ZenicorAF Bumgarner2018AppleWatch">(<a href="references.html#ref-Kim2024ZenicorAF" role="doc-biblioref">Kim et al. 2024</a>; <a href="references.html#ref-Bumgarner2018AppleWatch" role="doc-biblioref">Bumgarner et al. 2018</a>)</span>.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 27%">
<col style="width: 31%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Predicted</th>
<th style="text-align: center;">atrial fibrillation</th>
<th style="text-align: center;">no atrial fibrillation</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">atrial fibrillation</td>
<td style="text-align: center;">1960</td>
<td style="text-align: center;">980</td>
<td style="text-align: center;">2940</td>
</tr>
<tr class="even">
<td style="text-align: left;">no atrial fibrillation</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">97020</td>
<td style="text-align: center;">97060</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">98000</td>
<td style="text-align: center;">100000</td>
</tr>
</tbody>
</table>
<p>However, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4? We use Bayes theorem to answer this question. <span class="math display">\[
p(\text{atrial fibrillation}\mid \text{atrial fibrillation is identified }) = \frac{0.01960}{ 0.02940} = 0.6667
\]</span></p>
<p>The conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%.</p>
<p>Apple Watch’s positive predictive value is just 19.6 percent. That means in this group – which constitutes more than 90 percent of users of wearable devices like the Apple Watch – the app incorrectly diagnoses atrial fibrillation 79.4 percent of the time. (You can try the calculation yourself using this Bayesian calculator: enter 0.02 for prevalence, 0.98 for sensitivity, and 0.99 for specificity).</p>
<p>The electrocardiogram app becomes more reliable in older individuals: The positive predictive value is 76 percent among users between the ages of 60 and 64, 91 percent among those aged 70 to 74, and 96 percent for those older than 85.</p>
<p>In the case of medical diagnostics, the sensitivity is the ratio of people who have disease and tested positive to the total number of positive cases in the population <span class="math display">\[
p(T=1\mid D=1) = \dfrac{p(T=1,D=1)}{p(D=1)} = 0.0196/0.02 = 0.98
\]</span> The specificity is given by <span class="math display">\[
p(T=0\mid D=0) = \dfrac{p(T=0,D=0)}{p(D=0)} = 0.9702/0.98 = 0.99.
\]</span> As we see the test is highly sensitive and specific. However, only 66% of those who are tested positive will have a disease. This is due to the fact that the number of sick people is much less than the number of healthy and presence of type I error.</p>
</div>
</section>
<section id="graphical-representation-of-probability-and-conditional-independence." class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="graphical-representation-of-probability-and-conditional-independence."><span class="header-section-number">2.5</span> Graphical Representation of Probability and Conditional Independence.</h2>
<!-- https://cedar.buffalo.edu/~srihari/CSE674/Chap3/3.6-ConditionalIndependence.pdf -->
<p>We can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by <span class="math display">\[
p(x_1, x_2, \ldots, x_n) = p(x_1)p(x_2 \mid x_1)p(x_3 \mid x_1, x_2) \ldots p(x_n \mid x_1, x_2, \ldots, x_{n-1}).
\]</span> The expression on the right hand side can be simplified if some of the variables are conditionally independent. For example, if <span class="math inline">\(x_3\)</span> is conditionally independent of <span class="math inline">\(x_2\)</span>, given <span class="math inline">\(x_1\)</span>, then we can write <span class="math display">\[
p(x_3 \mid x_1, x_2) =p(x_3 \mid x_1).
\]</span></p>
<p>In a high-dimensional case, when we have a joint distribution over a large number of random variables, we can often simplify the expression by using independence or conditional independence assumptions. Sometimes it is convenient to represent these assumptions in a graphical form. This is the idea behind the concept of a Bayesian network. Essentially, the graph is a compact representation of a set of independencies that hold in the distribution.</p>
<p>Let’s consider an example of joint distribution with three random variables, we have the following joint distribution: <span class="math display">\[
p(a,b,c) = p(a\mid b,c)p(b\mid c)p(c)
\]</span></p>
<p>Graphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network. Each node represents a random variable and the arrows represent the conditional dependencies between the variables. When two nodes are connected they are not independent. Consider the following three cases:</p>
<div class="quarto-layout-panel" data-layout-align="center" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/bayes-net-Page-3.drawio.svg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><span class="math display">\[
p(b\mid c,a) = p(b\mid c),~ p(a,b,c) = p(a)p(c\mid a)p(b\mid c)
\]</span></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p>Line Structure</p>
</div>
</div>
</div>
<div class="quarto-layout-panel" data-layout-align="center" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/bayes-net-Page-1.drawio.svg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><span class="math display">\[
p(a\mid b,c) = p(a\mid c), ~ p(a,b,c) = p(a\mid c)p(b\mid c)p(c)
\]</span></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p>Lambda Structure</p>
</div>
</div>
</div>
<div class="quarto-layout-panel" data-layout-align="center" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="fig/bayes-net-Page-5.drawio.svg" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><span class="math display">\[
p(a\mid b) = p(a),~ p(a,b,c) = p(c\mid a,b)p(a)p(b)
\]</span></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p>V-structure</p>
</div>
</div>
</div>
<p>Although the graph shows us the conditional independence assumptions, we can also derive other independencies from the graph. An interesting question is whether they are connected through a third node. In the first case (a), we have <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> connected through <span class="math inline">\(c\)</span>. Thus, <span class="math inline">\(a\)</span> can influence <span class="math inline">\(b\)</span>. However, once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent. In case (b) the logic here is similar, <span class="math inline">\(a\)</span> can influence <span class="math inline">\(b\)</span> through <span class="math inline">\(c\)</span>, but once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent. In the third case (c), <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent, but once <span class="math inline">\(c\)</span> is known, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are not independent. You can formally derive these independencies from the graph by comparing <span class="math inline">\(p(a,b\mid c)\)</span> and <span class="math inline">\(p(a\mid c)p(b\mid c)\)</span>.</p>
<div id="exm-alarm" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.17 (Bayes Home Diagnostics)</strong></span> Suppose that a house alarm system sends me a text notification when some motion inside my house is detected. It detects motion when I have a person inside (burglar) or during an earthquake. Say, from prior data we know that during an earthquake alarm is triggered in 10% of the cases. Once I receive a text message, I start driving back home. While driving I hear on the radio about a small earthquake in our area. Now we want to know <span class="math inline">\(p(b \mid a)\)</span> and <span class="math inline">\(p(b \mid a,r)\)</span>. Here <span class="math inline">\(b\)</span> = burglary, <span class="math inline">\(e\)</span> = earthquake, <span class="math inline">\(a\)</span> = alarm, and <span class="math inline">\(r\)</span> = radio message about small earthquake.</p>
<p>The joint distribution is then given by <span class="math display">\[
  p(b,e,a,r) = p(r \mid a,b,e)p(a \mid b,e)p(b\mid e)p(e).
\]</span> Since we know the causal relations, we can simplify this expression <span class="math display">\[
p(b,e,a,r) = p(r \mid e)p(a \mid b,e)p(b)p(e).
\]</span> The <span class="math inline">\(p(a \mid b,e)\)</span> distribution is defined by</p>
<div id="tbl-alarm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Conditional probability of alarm given burglary and earthquake
</figcaption>
<div aria-describedby="tbl-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(p(a=1 \mid b,e)\)</span></th>
<th style="text-align: center;">b</th>
<th style="text-align: center;">e</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Graphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network.</p>
<!-- https://www.mermaidchart.com/app/projects/ab5d5333-d1a5-42f8-ac1d-8e287a49d7b8/diagrams/30323a8e-087d-4321-b95e-7d89ad5d2f25/share/invite/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkb2N1bWVudElEIjoiMzAzMjNhOGUtMDg3ZC00MzIxLWI5NWUtN2Q4OWFkNWQyZjI1IiwiYWNjZXNzIjoiRWRpdCIsImlhdCI6MTc1MDIyMDIyOX0.jgsw8lFITwTKK38jbomGp6OEafDwLPCkz2INBBA4xI4 -->
<!-- ![Bayesian network for alarm.](fig/bayes-net.svg){#fig-bayes-net width=60%} -->
<div class="cell" data-fig-width="4" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bayes-net" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bayes-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-bayes-net">graph TB
    b((b)) --&gt; a((a))
    e((e)) --&gt; a
    e --&gt; r((r))
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bayes-net-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Bayesian network for alarm
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we can easily calculate <span class="math inline">\(p(a=0 \mid b,e)\)</span>, from the property of a probability distribution <span class="math inline">\(p(a=1 \mid b,e) + p(a=0 \mid b,e) = 1\)</span>. In addition, we are given <span class="math inline">\(p(r=1 \mid e=1) = 0.5\)</span> and <span class="math inline">\(p(r=1 \mid e=0) = 0\)</span>. Further, based on historic data we have <span class="math inline">\(p(b) = 2\cdot10^{-4}\)</span> and <span class="math inline">\(p(e) = 10^{-2}\)</span>. Note that causal relations allowed us to have a more compact representation of the joint probability distribution. The original naive representation requires specifying <span class="math inline">\(2^4\)</span> parameters.</p>
<p>To answer our original question, calculate <span class="math display">\[
p(b \mid a) = \dfrac{p(a \mid b)p(b)}{p(a)},~~p(a) = p(a=1 \mid b=1)p(b=1) + p(a=1 \mid b=0)p(b=0).
\]</span> We have everything but <span class="math inline">\(p(a \mid b)\)</span>. This is obtained by marginalizing <span class="math inline">\(p(a=1 \mid b,e)\)</span>, to yield <span class="math display">\[
p(a \mid b) = p(a \mid b,e=1)p(e=1) + p(a \mid b,e=0)p(e=0).
\]</span> We can calculate <span class="math display">\[
p(a=1 \mid b=1) = 1, ~p(a=1 \mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.
\]</span> This leads to <span class="math inline">\(p(b \mid a) = 2\cdot10^{-4}/(2\cdot10^{-4} + 10^{-3}(1-2\cdot10^{-4})) = 1/6\)</span>.</p>
<p>This result is somewhat counterintuitive. We get such a low probability of burglary because its prior is very low compared to the prior probability of an earthquake. What will happen to the posterior if we live in an area with higher crime rates, say <span class="math inline">\(p(b) = 10^{-3}\)</span>. <a href="#fig-alarm" class="quarto-xref">Figure&nbsp;<span>2.3</span></a> shows the relationship between the prior and posterior. <span class="math display">\[
p(b \mid a) = \dfrac{p(b)}{p(b) + 10^{-3}(1-p(b))}
\]</span></p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, .<span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">&lt;-</span> prior <span class="sc">/</span> (prior <span class="sc">+</span> <span class="fl">0.001</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> prior))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(prior, post, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div id="fig-alarm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-bayes_files/figure-html/fig-alarm-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alarm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Relationship between the prior and posterior
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, <span class="math display">\[
p(b=1 \mid a=1,r=1) =  \dfrac{p(a,r  \mid  b)p(b)}{p(a,r)}
\]</span> and <span class="math display">\[
p(a,r  \mid  b)p(b) = \dfrac{\sum_e p(b=1,e,a=1,r=1)}{\sum_b\sum_ep(b,e,a=1,r=1)}
\]</span> <span class="math display">\[
=\dfrac{\sum_ep(r=1 \mid e)p(a=1 \mid b=1,e)p(b=1)p(e)}{\sum_b\sum_ep(r=1 \mid e)p(a=1 \mid b,e)p(b)p(e)}
\]</span> which is <span class="math inline">\(\approx 2\%\)</span> in our case. This effect is called <em>explaining away</em>, namely when new information explains some previously known fact.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Bumgarner2018AppleWatch" class="csl-entry" role="listitem">
Bumgarner, John M., Chad T. Lambert, Ayman A. Hussein, Daniel J. Cantillon, Bryan Baranowski, Kathy Wolski, Bruce D. Lindsay, Oussama M. Wazni, and Khaldoun G. Tarakji. 2018. <span>“Smartwatch Algorithm for Automated Detection of Atrial Fibrillation.”</span> <em>Journal of the American College of Cardiology</em> 71 (21): 2381–88.
</div>
<div id="ref-Kim2024ZenicorAF" class="csl-entry" role="listitem">
Kim, Young-Hoon, Jaehyung Shim, Hyoung-Seob Park, et al. 2024. <span>“<a href="https://www.ncbi.nlm.nih.gov/pubmed/38412780">Diagnostic Accuracy of Single-Lead Handheld <span>ECG</span> Devices for Atrial Fibrillation Detection</a>.”</span> <em>Journal of Cardiovascular Electrophysiology</em> 35: 614–21.
</div>
<div id="ref-maharaj2023kramnik" class="csl-entry" role="listitem">
Maharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. <span>“Kramnik Vs <span>Nakamura</span> or <span>Bayes</span> Vs p-Value.”</span> {{SSRN Scholarly Paper}}. Rochester, NY.
</div>
<div id="ref-simpson2010edward" class="csl-entry" role="listitem">
Simpson, Edward. 2010. <span>“Edward <span>Simpson</span>: <span>Bayes</span> at <span>Bletchley Park</span>.”</span> <em>Significance</em> 7 (2): 76–80.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-prob.html" class="pagination-link" aria-label="Probability and Uncertainty">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-bl.html" class="pagination-link" aria-label="Bayesian Learning">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>