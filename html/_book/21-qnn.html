<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>21&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./22-cnn.html" rel="next">
<link href="./20-sgd.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b4985e4eddee1e63d72746df2b00da28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
  // Load MathJax with custom macros
  window.MathJax = {
    tex: {
      macros: {
        Cov: ["\\mathrm{Cov}\\left(#1\\right)", 1],
        Cor: ["\\mathrm{Cor}\\left(#1\\right)", 1],
        Var: ["\\mathrm{Var}\\left(#1\\right)", 1],
        sd: ["\\mathrm{sd}\\left(#1\\right)", 1],
        E: ["\\mathrm{E}_{#1}\\left(#2\\right)", 2, ""],
        prob: ["\\mathrm{P}\\left(#1\\right)", 1],
        defeq: "\\stackrel{\\mathrm{def}}{=}",
        mini: "\\operatorname*{minimize}"
      }
    }
  };
</script>

<style>
  /* Custom styling for math content */
  .MathJax {
    font-size: 1em !important;
  }

  /* Ensure consistent math rendering */
  mjx-container[jax="CHTML"] {
    line-height: 1.2;
  }
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="21&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning">
<meta property="og:description" content="">
<meta property="og:image" content="21-qnn_files/figure-html/fig-plot-mtcars-1.png">
<meta property="og:site_name" content="Bayes, AI and Deep Learning">
<meta name="twitter:title" content="21&nbsp; Quantile Neural Networks – Bayes, AI and Deep Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="21-qnn_files/figure-html/fig-plot-mtcars-1.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./21-qnn.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayes, AI and Deep Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Modern AI Playbook</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Bayes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability and Uncertainty</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bayes Rule</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-bl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Utility, Risk and Decisions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-ab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">A/B Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-hyp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-sp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-gp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Gaussian Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Reinforcement Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">AI</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unreasonable Effectiveness of Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-pattern.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Pattern Matching</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Logistic Regression and Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tree Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-forecasting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Forecasting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-select.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-theoryai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Statistical Learning Theory and Regularization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-nn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-theorydl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Theory of Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-sgd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-qnn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-nlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-robots.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">AI Agents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-linalg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Linear algebra and multivariate normal toolkit</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-qnn-quantile-regression" id="toc-sec-qnn-quantile-regression" class="nav-link active" data-scroll-target="#sec-qnn-quantile-regression"><span class="header-section-number">21.1</span> Quantile Regression</a></li>
  <li><a href="#sec-qnn-generative" id="toc-sec-qnn-generative" class="nav-link" data-scroll-target="#sec-qnn-generative"><span class="header-section-number">21.2</span> From Densities to Quantiles: A Generative Approach</a></li>
  <li><a href="#sec-qnn-trend-filtering" id="toc-sec-qnn-trend-filtering" class="nav-link" data-scroll-target="#sec-qnn-trend-filtering"><span class="header-section-number">21.3</span> Nonlinear Quantile Regression via Trend Filtering</a></li>
  <li><a href="#sec-qnn-bayes-quantiles" id="toc-sec-qnn-bayes-quantiles" class="nav-link" data-scroll-target="#sec-qnn-bayes-quantiles"><span class="header-section-number">21.4</span> Bayes Rule for Quantiles</a></li>
  <li><a href="#sec-qnn-meu" id="toc-sec-qnn-meu" class="nav-link" data-scroll-target="#sec-qnn-meu"><span class="header-section-number">21.5</span> Maximum Expected Utility via Quantile Neural Networks</a>
  <ul class="collapse">
  <li><a href="#implementation-strategy" id="toc-implementation-strategy" class="nav-link" data-scroll-target="#implementation-strategy">Implementation Strategy</a></li>
  </ul></li>
  <li><a href="#sec-qnn-implementation" id="toc-sec-qnn-implementation" class="nav-link" data-scroll-target="#sec-qnn-implementation"><span class="header-section-number">21.6</span> Neural Network Implementation</a>
  <ul class="collapse">
  <li><a href="#learning-multiple-quantiles-simultaneously" id="toc-learning-multiple-quantiles-simultaneously" class="nav-link" data-scroll-target="#learning-multiple-quantiles-simultaneously">Learning Multiple Quantiles Simultaneously</a></li>
  <li><a href="#non-crossing-constraints" id="toc-non-crossing-constraints" class="nav-link" data-scroll-target="#non-crossing-constraints">Non-Crossing Constraints</a></li>
  <li><a href="#cosine-embedding-for-tau" id="toc-cosine-embedding-for-tau" class="nav-link" data-scroll-target="#cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></a></li>
  <li><a href="#synthetic-data-example" id="toc-synthetic-data-example" class="nav-link" data-scroll-target="#synthetic-data-example">Synthetic Data Example</a></li>
  </ul></li>
  <li><a href="#sec-qnn-portfolio" id="toc-sec-qnn-portfolio" class="nav-link" data-scroll-target="#sec-qnn-portfolio"><span class="header-section-number">21.7</span> Portfolio Optimization with Quantile Neural Networks</a>
  <ul class="collapse">
  <li><a href="#empirical-example" id="toc-empirical-example" class="nav-link" data-scroll-target="#empirical-example">Empirical Example</a></li>
  </ul></li>
  <li><a href="#sec-qnn-forecasting" id="toc-sec-qnn-forecasting" class="nav-link" data-scroll-target="#sec-qnn-forecasting"><span class="header-section-number">21.8</span> Supply Chain Forecasting at Scale</a>
  <ul class="collapse">
  <li><a href="#demand-forecasting-setup" id="toc-demand-forecasting-setup" class="nav-link" data-scroll-target="#demand-forecasting-setup">Demand Forecasting Setup</a></li>
  <li><a href="#implementation-strategy-1" id="toc-implementation-strategy-1" class="nav-link" data-scroll-target="#implementation-strategy-1">Implementation Strategy</a></li>
  </ul></li>
  <li><a href="#sec-qnn-rl" id="toc-sec-qnn-rl" class="nav-link" data-scroll-target="#sec-qnn-rl"><span class="header-section-number">21.9</span> Distributional Reinforcement Learning</a></li>
  <li><a href="#sec-qnn-discussion" id="toc-sec-qnn-discussion" class="nav-link" data-scroll-target="#sec-qnn-discussion"><span class="header-section-number">21.10</span> Discussion and Summary</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./18-nn.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="./21-qnn.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Quantile Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“<em>It is better to be roughly right than precisely wrong.</em>” – John Maynard Keynes</p>
</blockquote>
<p>In <a href="03-bl.html" class="quarto-xref"><span>Chapter 3</span></a>, we explored how to learn posterior distributions <span class="math inline">\(p(\theta\mid y)\)</span> using Bayesian learning and to use it for predictions, hypothesis testing, and other tasks. In <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a>, we explored how rational agents make decisions under uncertainty by maximizing expected utility. Traditional Bayesian approaches to such problems require computing posterior distributions <span class="math inline">\(p(\theta\mid y)\)</span>, which in turn requires specifying likelihood functions <span class="math inline">\(p(y\mid \theta)\)</span> and often involves challenging density estimation. But what if we could bypass density estimation entirely and directly learn the quantities we need for decision-making and other tasks?</p>
<p>This chapter introduces <em>quantile neural networks</em>, a powerful approach that learns posterior distributions through their quantile functions rather than their densities. This shift from densities to quantiles has profound implications: it enables likelihood-free inference, provides natural connections to decision theory through the quantile-expectation identity, and scales to high-dimensional problems where density estimation becomes intractable.</p>
<p>The key insight is straightforward. Any expectation—including the expected utility central to decision theory—can be represented as an integral over quantiles: <span class="math display">\[
E[f(\theta)] = \int_0^1 F^{-1}_{f(\theta) | y}(\tau) d\tau
\]</span> Rather than learning densities and then computing expectations via sampling, we can directly learn the quantile function <span class="math inline">\(F^{-1}_{f(\theta) | y}(\tau)\)</span> using neural networks. This approach is not only more efficient but also naturally handles simulation-based models where likelihoods are unavailable or computationally expensive.</p>
<p>Throughout this chapter, we develop quantile neural networks from theoretical foundations through practical applications. We begin by establishing the fundamentals of quantile regression (<a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 21.1</span></a>), deriving the check loss function and demonstrating its properties. We then show how to extend this framework to the generative approach (<a href="#sec-qnn-generative" class="quarto-xref"><span>Section 21.2</span></a>), using the noise outsourcing theorem to represent posterior distributions through quantile functions rather than densities. Before moving to neural networks, we examine trend filtering (<a href="#sec-qnn-trend-filtering" class="quarto-xref"><span>Section 21.3</span></a>) as an elegant middle ground that provides nonlinear function approximation while maintaining computational tractability. With these foundations in place, we establish Bayes rule for quantiles (<a href="#sec-qnn-bayes-quantiles" class="quarto-xref"><span>Section 21.4</span></a>), showing how Bayesian updating can be performed entirely in terms of quantile functions. We then turn to three major applications demonstrating the versatility of this framework: maximum expected utility problems (<a href="#sec-qnn-meu" class="quarto-xref"><span>Section 21.5</span></a>), where utility functions are incorporated directly into training; portfolio optimization under parameter uncertainty (<a href="#sec-qnn-portfolio" class="quarto-xref"><span>Section 21.7</span></a>), extending beyond cases with closed-form solutions; and supply chain forecasting (<a href="#sec-qnn-forecasting" class="quarto-xref"><span>Section 21.8</span></a>), where companies like Amazon use quantile methods to predict entire demand distributions for inventory optimization. After detailing the neural network implementation (<a href="#sec-qnn-implementation" class="quarto-xref"><span>Section 21.6</span></a>), including cosine embeddings and Wasserstein distance connections, we connect to modern artificial intelligence through distributional reinforcement learning (<a href="#sec-qnn-rl" class="quarto-xref"><span>Section 21.9</span></a>), showing how agents learn entire distributions of returns for more robust decision-making.</p>
<section id="sec-qnn-quantile-regression" class="level2" data-number="21.1">
<h2 data-number="21.1" class="anchored" data-anchor-id="sec-qnn-quantile-regression"><span class="header-section-number">21.1</span> Quantile Regression</h2>
<p>This section derives the quantile loss function from first principles.</p>
<p>Standard calculus shows that for observed values <span class="math inline">\(y_1, \ldots, y_n\)</span>, the median is the value that minimizes the expected absolute deviation: <span class="math display">\[
m = \arg\min_q \frac{1}{n} \sum_{i=1}^n |y_i - q|   = \arg\min_q E[|Y - q|]
\]</span> Intuitively, the sum of absolute deviations is minimized when the median is the value that is closest to half of the observations.</p>
<p>The median is the special case <span class="math inline">\(\tau = 0.5\)</span>, but the same principle generalizes to any quantile <span class="math inline">\(\tau \in (0,1)\)</span>. We use a generalization of the absolute value function—the <em>check loss</em> or <em>pinball loss</em>—to find the minimizer: <span class="math display">\[
q_\tau = \arg\min_q \frac{1}{n} \sum_{i=1}^n \rho_\tau(y_i - q).
\]</span> Here <span class="math inline">\(\rho_\tau(u)\)</span> is the <em>check loss</em> or <em>pinball loss</em> and is defined as: <span class="math display">\[
\rho_\tau(u) = u(\tau - I(u &lt; 0)) = \begin{cases}
\tau u &amp; \text{if } u \geq 0 \\
(\tau - 1) u &amp; \text{if } u &lt; 0
\end{cases}
\]</span> This can also be written in the more computationally convenient form: <span class="math display">\[
\rho_\tau(u) = \max(u\tau, u(\tau-1))
\]</span></p>
<div id="exm-quantile-regression-r" class="theorem example">
<p><span class="theorem-title"><strong>Example 21.1 (Linear Quantile Regression)</strong></span> To illustrate quantile regression in practice, we’ll analyze the relationship between engine displacement and fuel efficiency using the classic <code>mtcars</code> dataset. Rather than only estimating the mean relationship (as ordinary least squares would), we’ll also estimate conditional quantiles to understand how this relationship varies across the distribution of fuel efficiency. This allows us to capture the heteroskedasticity in the data. <a href="#fig-plot-mtcars" class="quarto-xref">Figure&nbsp;<span>21.1</span></a> shows the quantile regression results.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-plot-mtcars" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot-mtcars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="21-qnn_files/figure-html/fig-plot-mtcars-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="480">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot-mtcars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.1: Quantile regression on mtcars dataset. The relationship between engine displacement and fuel efficiency varies across quantiles, revealing heteroskedasticity in the data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Table <a href="#tbl-quantile-regression-results" class="quarto-xref">Table&nbsp;<span>21.1</span></a> shows the estimated coefficients.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div id="tbl-quantile-regression-results" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-null_prefix="true" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-quantile-regression-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;21.1: Quantile regression results
</figcaption>
<div aria-describedby="tbl-quantile-regression-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<caption>Quantile Regression Results: Fuel Efficiency vs.&nbsp;Engine Displacement</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Intercept</th>
<th style="text-align: right;">Slope</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: left;">Quantile tau = 0.1</td>
<td style="text-align: right;">5.6</td>
<td style="text-align: right;">-0.53</td>
</tr>
<tr class="even">
<td style="text-align: left;">(Intercept)1</td>
<td style="text-align: left;">Quantile tau = 0.5</td>
<td style="text-align: right;">5.5</td>
<td style="text-align: right;">-0.48</td>
</tr>
<tr class="odd">
<td style="text-align: left;">(Intercept)2</td>
<td style="text-align: left;">Quantile tau = 0.9</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: right;">-0.37</td>
</tr>
<tr class="even">
<td style="text-align: left;">(Intercept)3</td>
<td style="text-align: left;">OLS (Mean)</td>
<td style="text-align: right;">5.4</td>
<td style="text-align: right;">-0.46</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The quantile regression results reveal several important patterns in the relationship between engine displacement and fuel efficiency. First, the slopes differ substantially across quantiles, indicating heteroskedasticity in the conditional distribution. The 10th percentile slope is -0.5340, while the 90th percentile slope is -0.3660. This difference suggests that the negative relationship between displacement and fuel efficiency is stronger for more fuel-efficient cars.</p>
<p>Second, the widening gap between the 10th and 90th percentiles as displacement increases reveals increasing uncertainty in fuel efficiency for larger engines. This pattern likely reflects varying driving conditions, maintenance practices, and vehicle technology across different cars with similar engine sizes.</p>
<p>Third, the median regression (<span class="math inline">\(\tau\)</span> = 0.5) demonstrates robustness to outliers. Unlike OLS, which minimizes squared errors and thus heavily weights extreme values, quantile regression uses the asymmetric absolute loss that treats positive and negative residuals differently based on the quantile level. This asymmetry makes the estimator less sensitive to unusual observations.</p>
<p>Finally, the check loss <span class="math inline">\(\rho_\tau(u)\)</span> is piecewise linear, making it non-differentiable at zero. This property explains why we use <code>BFGS</code> rather than gradient-based methods that assume smoothness. The <code>BFGS</code> algorithm builds a quasi-Newton approximation that handles the kink effectively, converging reliably despite the non-smooth objective function.</p>
</div>
<p>Traditional quantile regression assumes <span class="math inline">\(f_\tau(x, \theta)\)</span> is linear in parameters. This limitation becomes severe when relationships are inherently nonlinear, when working with high-dimensional inputs requiring feature learning, when estimating multiple quantiles simultaneously (where shared representations improve efficiency), or when incorporating utility functions directly into learning (<a href="#sec-qnn-meu" class="quarto-xref"><span>Section 21.5</span></a>). Neural quantile regression addresses these limitations by combining the robustness of quantile methods with the flexibility of deep learning.</p>
</section>
<section id="sec-qnn-generative" class="level2" data-number="21.2">
<h2 data-number="21.2" class="anchored" data-anchor-id="sec-qnn-generative"><span class="header-section-number">21.2</span> From Densities to Quantiles: A Generative Approach</h2>
<p>Let <span class="math inline">\((X,Y) \sim P_{X,Y}\)</span> be input-output pairs and <span class="math inline">\(P_{X,Y}\)</span> a joint measure from which we can simulate a training dataset <span class="math inline">\((x_i, y_i)_{i=1}^N \sim P_{X,Y}\)</span>. Standard prediction techniques focus on the conditional posterior mean <span class="math inline">\(\hat{X}(Y) = E(X|Y) = f(Y)\)</span> of the input given the output. The standard approach formulates this as nonparametric regression <span class="math inline">\(X = f(Y) + \epsilon\)</span> and estimates the conditional mean using methods such as kernel smoothing or K-nearest neighbors. Recently, deep learning approaches have been proposed, with theoretical properties established by <span class="citation" data-cites="polson2023generative">N. G. Polson and Sokolov (<a href="references.html#ref-polson2023generative" role="doc-biblioref">2023</a>)</span>.</p>
<p>Generative methods take this approach one step further. Let <span class="math inline">\(Z \sim P_Z\)</span> be a base measure for a latent variable, <span class="math inline">\(Z\)</span>, typically a standard multivariate normal or vector of uniforms. The goal of generative methods is to characterize the posterior measure <span class="math inline">\(P_{X|Y}\)</span> from the training data <span class="math inline">\((x_i, y_i)_{i=1}^N \sim P_{X,Y}\)</span> where <span class="math inline">\(N\)</span> is chosen to be suitably large. A deep learner is used to estimate <span class="math inline">\(\hat{f}\)</span> via the non-parametric regression <span class="math inline">\(X = f(Y, Z)\)</span>. In the case where <span class="math inline">\(Z\)</span> is uniform, this amounts to inverse CDF sampling, namely <span class="math inline">\(X = F_{X|Y}^{-1}(Z)\)</span>—the <em>quantile function</em> that we develop formally in <a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 21.1</span></a>.</p>
<p>In general, we characterize the posterior map for <em>any</em> output <span class="math inline">\(Y\)</span>. We characterize the posterior by evaluating the network at any <span class="math inline">\(Y\)</span> using the transport map <span class="math display">\[
X = H(S(Y), \psi(Z))
\]</span> Here <span class="math inline">\(\psi\)</span> denotes the embedding function. The deep learners <span class="math inline">\(H\)</span> and <span class="math inline">\(S\)</span> are estimated from the triples <span class="math inline">\((X_i, Y_i, \psi(Z_i))_{i=1}^N \sim P_{X,Y} \times P_Z\)</span>. The ensuing estimator <span class="math inline">\(\hat{H}\)</span> can be thought of as a transport map from the base distribution to the posterior as required.</p>
<p>The following diagram illustrates the transport map architecture:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/transport-map.png" class="img-fluid figure-img"></p>
<figcaption>Transport map architecture for generative inference. Data <span class="math inline">\(Y\)</span> is summarized by statistics <span class="math inline">\(S(Y)\)</span>, while latent variable <span class="math inline">\(Z\)</span> is embedded via <span class="math inline">\(\psi\)</span>. The transport function <span class="math inline">\(H\)</span> maps these inputs to posterior samples.</figcaption>
</figure>
</div>
<p>Specifically, the idea of generative methods is straightforward. Let <span class="math inline">\(y\)</span> denote data and <span class="math inline">\(\theta\)</span> a vector of parameters including any hidden states (a.k.a. latent variables) <span class="math inline">\(z\)</span>. First, we generate a “look-up” table of “fake” data <span class="math inline">\(\{y^{(i)}, \theta^{(i)}\}_{i=1}^N\)</span>. By simulating a training dataset of outputs and parameters, we can use deep learning to solve for the inverse map via a supervised learning problem. Generative methods have the advantage of being likelihood-free. For example, our model might be specified by a forward map <span class="math inline">\(y^{(i)} = f(\theta^{(i)})\)</span> rather than a traditional random draw from a likelihood function <span class="math inline">\(y^{(i)} \sim p(y^{(i)}|\theta^{(i)})\)</span>. The base distribution <span class="math inline">\(P_Z\)</span> is typically uniform (for univariate problems) or a very high-dimensional Gaussian vector (for multivariate problems).</p>
<p>The theoretical foundation for this approach is the <em>noise outsourcing theorem</em>, which guarantees that we can represent any posterior distribution through a deterministic function of the data and a base random variable.</p>
<p><em>Noise Outsourcing Theorem</em> <span class="citation" data-cites="kallenberg1997foundations">(<a href="references.html#ref-kallenberg1997foundations" role="doc-biblioref">Kallenberg 1997</a>)</span>: If <span class="math inline">\((Y, \Theta)\)</span> are random variables in a standard probability space <span class="math inline">\((\mathcal{Y}, \Theta)\)</span>, then there exists a random variable <span class="math inline">\(\tau \sim U(0,1)\)</span> which is independent of <span class="math inline">\(Y\)</span> and a function <span class="math inline">\(H: [0,1] \times \mathcal{Y} \rightarrow \Theta\)</span> such that <span class="math display">\[
(Y, \Theta) \stackrel{a.s.}{=} (Y, H(Y, \tau))
\]</span> Moreover, if there is a sufficient statistic <span class="math inline">\(S(Y)\)</span> with <span class="math inline">\(Y\)</span> independent of <span class="math inline">\(\Theta | S(Y)\)</span>, then <span class="math display">\[
\Theta\mid Y \stackrel{a.s.}{=} H(S(Y), \tau).
\]</span></p>
<p>This result tells us that posterior uncertainty can be characterized via an inverse non-parametric regression problem where we predict <span class="math inline">\(\theta^{(i)}\)</span> from <span class="math inline">\(y^{(i)}\)</span> and <span class="math inline">\(\tau^{(i)}\)</span>, where <span class="math inline">\(\tau^{(i)}\)</span> is drawn from a base distribution <span class="math inline">\(p(\tau)\)</span>. We train a deep neural network <span class="math inline">\(H\)</span> on <span class="math display">\[
\theta^{(i)} = H(S(y^{(i)}), \tau^{(i)}).
\]</span> Here <span class="math inline">\(S(y)\)</span> is a statistic used to perform dimension reduction with respect to the signal distribution—analogous to sufficient statistics in traditional Bayesian inference. A result due to <span class="citation" data-cites="brillinger2012generalized">Brillinger (<a href="references.html#ref-brillinger2012generalized" role="doc-biblioref">2012</a>)</span> shows that for single-index models, we can estimate the subspace spanned by <span class="math inline">\(S\)</span> via ordinary least squares, effectively learning the sufficient summary statistics from data.</p>
<p>Specifying the architecture of <span class="math inline">\(H\)</span> is key to the efficiency of the approach. <span class="citation" data-cites="polson2024generative">N. Polson, Ruggeri, and Sokolov (<a href="references.html#ref-polson2024generative" role="doc-biblioref">2024</a>)</span> propose using quantile neural networks implemented with ReLU activation functions, which we detail in <a href="#sec-qnn-implementation" class="quarto-xref"><span>Section 21.6</span></a>.</p>
</section>
<section id="sec-qnn-trend-filtering" class="level2" data-number="21.3">
<h2 data-number="21.3" class="anchored" data-anchor-id="sec-qnn-trend-filtering"><span class="header-section-number">21.3</span> Nonlinear Quantile Regression via Trend Filtering</h2>
<p>Trend filtering combined with quantile loss, developed by <span class="citation" data-cites="polson2016mixtures">N. G. Polson and Scott (<a href="references.html#ref-polson2016mixtures" role="doc-biblioref">2016</a>)</span>, provides nonlinear function approximation while maintaining computational tractability through a hierarchical representation. Trend filtering estimates smooth, nonlinear functions by penalizing differences in derivatives rather than the function values themselves.</p>
<p>Consider the nonparametric regression problem where we observe pairs <span class="math inline">\((x_i, y_i)\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span> with <span class="math inline">\(x_1 &lt; x_2 &lt; \ldots &lt; x_n\)</span>. Traditional smoothing methods like cubic splines require choosing knot locations, while kernel smoothing requires bandwidth selection. Trend filtering offers an alternative: estimate a function <span class="math inline">\(f(x)\)</span> by solving</p>
<p><span class="math display">\[
\min_{f} \sum_{i=1}^n \rho_\tau(y_i - f(x_i)) + \lambda \sum_{i=1}^{n-k} |\Delta^k f_i|
\]</span></p>
<p>where <span class="math inline">\(\Delta^k\)</span> denotes the <span class="math inline">\(k\)</span>-th order discrete derivative operator and <span class="math inline">\(\rho_\tau\)</span> is the check loss for quantile <span class="math inline">\(\tau\)</span>. The penalty term controls smoothness: <span class="math inline">\(k=1\)</span> penalizes changes in slope (linear trend filtering), <span class="math inline">\(k=2\)</span> penalizes changes in curvature (quadratic trend filtering), and so on.</p>
<p>For <span class="math inline">\(k=2\)</span>, the penalty becomes <span class="math inline">\(\sum_{i=2}^{n-1} |f_{i+1} - 2f_i + f_{i-1}|\)</span>, which approximates the integrated squared second derivative <span class="math inline">\(\int (f''(x))^2 dx\)</span> used in smoothing splines. However, the <span class="math inline">\(\ell_1\)</span> penalty produces locally adaptive estimates—sharp changes are preserved while smooth regions remain smooth.</p>
<p><span class="citation" data-cites="polson2016mixtures">N. G. Polson and Scott (<a href="references.html#ref-polson2016mixtures" role="doc-biblioref">2016</a>)</span> show that trend filtering admits an elegant hierarchical representation through <em>envelope duality</em>. The key insight is that the <span class="math inline">\(\ell_1\)</span> penalty can be represented as an exponential prior in a hierarchical model. Specifically, for second-order trend filtering with quantile loss, we have the hierarchical model:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;\sim \text{AsymmetricLaplace}(f_i, \tau, \sigma) \\
\Delta^2 f_i &amp;\sim \text{Laplace}(0, 1/\lambda) \quad \text{for } i = 2, \ldots, n-1
\end{aligned}
\]</span></p>
<p>The asymmetric Laplace distribution naturally arises from the check loss—it is the distribution whose maximum likelihood estimator at quantile <span class="math inline">\(\tau\)</span> minimizes <span class="math inline">\(\rho_\tau\)</span>. This connection between optimization (minimizing penalized quantile loss) and probability (maximum a posteriori estimation in a hierarchical model) provides both computational and conceptual advantages.</p>
<p>The hierarchical formulation enables efficient computation through data augmentation schemes. Rather than directly optimizing the non-smooth objective, we introduce auxiliary variables that yield closed-form conditional distributions, leading to straightforward EM or Gibbs sampling algorithms.</p>
<p>We now demonstrate trend filtering for nonlinear quantile regression using synthetic data with both smooth regions and sharp transitions. Figure <a href="#fig-trend-filtering-quantile" class="quarto-xref">Figure&nbsp;<span>21.2</span></a> shows the results.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div id="fig-trend-filtering-quantile" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trend-filtering-quantile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="21-qnn_files/figure-html/fig-trend-filtering-quantile-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1152">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trend-filtering-quantile-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.2: Nonlinear quantile regression via trend filtering. The method adapts to both smooth regions and sharp transitions while estimating conditional quantiles.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-trend-filtering-quantile" class="quarto-xref">Figure&nbsp;<span>21.2</span></a> illustrates trend filtering quantile regression on synthetic data with heteroskedastic noise. The left panel shows how different quantile levels adapt to the nonlinear underlying structure. The 10th percentile (<span class="math inline">\(\tau=0.1\)</span>) tracks the lower boundary of the data cloud, the median (<span class="math inline">\(\tau=0.5\)</span>) estimates the central tendency, and the 90th percentile (<span class="math inline">\(\tau=0.9\)</span>) follows the upper boundary. Each estimated quantile function captures both the smooth regions and the more abrupt transitions in the data without requiring parametric assumptions about the functional form.</p>
<p>The right panel demonstrates how these estimated quantiles provide prediction intervals that adapt to the changing variability across the input space. The shaded region between the 10th and 90th percentiles represents an 80% prediction interval. Notice how this interval widens in regions where the data exhibits greater dispersion and narrows where the data are more tightly clustered—precisely the behavior we desire for uncertainty quantification. The empirical coverage for this interval is 68.0%, which is somewhat below the nominal 80% level, reflecting the typical challenge that estimated quantiles tend to undercover when sample sizes are modest and the underlying function is complex. This underscores an important practical consideration: prediction intervals from quantile regression should be interpreted as approximate, with their reliability improving as sample size increases and as the ratio of smoothing penalty to noise level is appropriately calibrated.</p>
<p>Trend filtering for quantile regression offers computational efficiency through sparse linear algebra and flexibility in capturing both smooth regions and abrupt transitions—properties that make it particularly attractive for applications with ordered, low-dimensional data.</p>
<p>Trend filtering provides an important conceptual bridge to neural quantile networks. Both approaches learn nonlinear functions through composition: trend filtering composes piecewise polynomials, while neural networks compose nonlinear activation functions. The key difference lies in how they handle high-dimensional inputs. Trend filtering is most effective for univariate or low-dimensional problems with ordered inputs, while neural networks excel when inputs are high-dimensional or lack natural ordering.</p>
<p>For problems with structured, low-dimensional inputs—time series, spatial data along a transect, dose-response curves—trend filtering often provides better interpretability and requires less data than neural networks. For high-dimensional problems—images, text, complex multivariate relationships—neural networks become essential. Understanding both approaches allows practitioners to choose the right tool for their specific problem structure.</p>
</section>
<section id="sec-qnn-bayes-quantiles" class="level2" data-number="21.4">
<h2 data-number="21.4" class="anchored" data-anchor-id="sec-qnn-bayes-quantiles"><span class="header-section-number">21.4</span> Bayes Rule for Quantiles</h2>
<p><span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> showed that quantile methods provide direct alternatives to density-based Bayesian computations. This section establishes the theoretical foundation for using quantiles to perform Bayesian updating.</p>
<p>Given a cumulative distribution function <span class="math inline">\(F_{\theta|y}(u)\)</span> (non-decreasing, right-continuous), we define the quantile function as: <span class="math display">\[Q_{\theta| y} (u) \defeq  F^{-1}_{\theta|y}  ( u ) = \inf \left \{ \theta : F_{\theta|y} (\theta) \geq u \right \}\]</span></p>
<p>The quantile function is non-decreasing and left-continuous. <span class="citation" data-cites="parzen2004quantile">Parzen (<a href="references.html#ref-parzen2004quantile" role="doc-biblioref">2004</a>)</span> established the fundamental probabilistic property: <span class="math display">\[
\theta \stackrel{P}{=} Q_\theta ( F_\theta (\theta ) )
\]</span></p>
<p>This identity enables efficient implementation: we can increase computational efficiency by ordering the samples of <span class="math inline">\(\theta\)</span> and the baseline uniform draws <span class="math inline">\(\tau\)</span>, exploiting the monotonicity of the inverse CDF map.</p>
<p>A crucial property for understanding why quantiles naturally compose (and thus suit deep learning) is the following. Let <span class="math inline">\(g(y)\)</span> be non-decreasing and left-continuous with <span class="math inline">\(g^{-1}(z) = \sup \{ y : g(y) \leq z \}\)</span>. Then the transformed quantile has a compositional nature: <span class="math display">\[Q_{g(Y)}(u) = g(Q(u))\]</span></p>
<p>This composition property shows that quantiles act as superpositions—exactly the structure that deep neural networks learn through their layered architecture.</p>
<p>The connection to Bayesian learning is made explicit through the <em>conditional quantile representation</em>. For the Bayesian learning problem, we have the following result for updating prior to posterior quantiles: <span class="math display">\[Q_{\theta | Y=y}(u) = Q_\theta(s) \quad \text{where} \quad s = Q_{F(\theta) | Y=y}(u)\]</span></p>
<p>To compute <span class="math inline">\(s\)</span>, note that by definition: <span class="math display">\[
u = F_{F(\theta) | Y=y}(s) = P(F(\theta) \leq s | Y=y) = P(\theta \leq Q_\theta(s) | Y=y) = F_{\theta | Y=y}(Q_\theta(s))
\]</span></p>
<p>This result shows that Bayesian updating can be performed entirely in terms of quantile functions, without ever computing or manipulating density functions. The posterior quantile function is obtained by composing the prior quantile function with a learned transformation.</p>
</section>
<section id="sec-qnn-meu" class="level2" data-number="21.5">
<h2 data-number="21.5" class="anchored" data-anchor-id="sec-qnn-meu"><span class="header-section-number">21.5</span> Maximum Expected Utility via Quantile Neural Networks</h2>
<p>Recall from <a href="04-dec.html" class="quarto-xref"><span>Chapter 4</span></a> that optimal Bayesian decisions maximize expected utility: <span class="math display">\[
d^\star(y) = \arg \max_d E_{\theta|y}[U(d, \theta)] = \arg \max_d \int U(d, \theta) p(\theta | y) d\theta
\]</span></p>
<p>The naive approach would be to first learn the posterior <span class="math inline">\(p(\theta|y)\)</span>, then use Monte Carlo to approximate the expected utility for each decision <span class="math inline">\(d\)</span>, and finally optimize over <span class="math inline">\(d\)</span>. However, this approach is inefficient for several reasons:</p>
<ol type="1">
<li><p><em>Computational waste</em>: Monte Carlo requires many samples in regions of high posterior probability, but utility functions often place high weight on tail events (risk scenarios) that have low posterior probability.</p></li>
<li><p><em>Density estimation</em>: We must first estimate the potentially high-dimensional posterior density before we can compute expectations.</p></li>
<li><p><em>Optimization difficulty</em>: The expectation must be recomputed for each candidate decision during optimization.</p></li>
</ol>
<p>Quantile neural networks provide a more direct path. The key insight is that we can incorporate the utility function directly into the training process rather than as a post-processing step.</p>
<p>The foundation of our approach is a classical result relating expectations to quantiles. Given any random variable <span class="math inline">\(U\)</span>, its expectation can be computed as an integral over its quantile function: <span class="math display">\[
E[U] = \int_0^1 F^{-1}_{U}(\tau) d\tau
\]</span> This is sometimes called the <em>quantile representation of expectations</em> or the <em>Lorenz curve</em> identity. For decision problems, this means: <span class="math display">\[
E_{\theta|y}[U(d, \theta)] = \int_0^1 F^{-1}_{U|d,y}(\tau) d\tau
\]</span></p>
<p>Rather than learning <span class="math inline">\(p(\theta|y)\)</span> and then computing the expectation, we directly learn the quantile function <span class="math inline">\(F^{-1}_{U|d,y}(\tau)\)</span> of the utility distribution.</p>
<section id="implementation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="implementation-strategy">Implementation Strategy</h3>
<p>To extend our generative method to MEU problems, we assume that the utility function <span class="math inline">\(U(d, \theta)\)</span> is given (a standard assumption in decision theory). The training procedure is as follows:</p>
<ol type="1">
<li><p><em>Generate synthetic dataset</em>: Simulate triples <span class="math inline">\(\{y^{(i)}, \theta^{(i)}, \tau^{(i)}\}_{i=1}^N\)</span> where <span class="math inline">\(y^{(i)} \sim p(y|\theta^{(i)})\)</span>, <span class="math inline">\(\theta^{(i)} \sim p(\theta)\)</span>, and <span class="math inline">\(\tau^{(i)} \sim U(0,1)\)</span>.</p></li>
<li><p><em>Compute utilities</em>: For each decision <span class="math inline">\(d\)</span> of interest, compute <span class="math inline">\(U^{(i)}_d \defeq U(d,\theta^{(i)})\)</span>.</p></li>
<li><p><em>Augment training data</em>: Create the augmented dataset <span class="math display">\[
\{U_d^{(i)}, S(y^{(i)}), \tau^{(i)}, d\}_{i=1}^N.
\]</span></p></li>
<li><p><em>Train quantile network</em>: Learn a neural network <span class="math inline">\(H\)</span> that predicts utilities by minimizing the check loss: <span class="math display">\[
U_d^{(i)} = H(S(y^{(i)}), \tau^{(i)}, d)
\]</span></p></li>
</ol>
<p>Once trained, the network <span class="math inline">\(H\)</span> represents the quantile function <span class="math inline">\(F^{-1}_{U|d,y}(\tau)\)</span>. For any observed data <span class="math inline">\(y\)</span> and candidate decision <span class="math inline">\(d\)</span>, we can:</p>
<ul>
<li><em>Compute expected utility</em>: Numerically integrate <span class="math inline">\(\int_0^1 H(S(y), \tau, d) d\tau\)</span></li>
<li><em>Find optimal decision</em>: <span class="math display">\[
d^\star(y) = \arg \max_d \int_0^1 H(S(y), \tau, d) d\tau
\]</span></li>
</ul>
<p>This approach has several advantages over naive Monte Carlo:</p>
<ol type="1">
<li>The network learns to focus on regions of the <span class="math inline">\((\theta, \tau)\)</span> space that matter for utility computation.</li>
<li>We avoid explicit density estimation of <span class="math inline">\(p(\theta|y)\)</span>.</li>
<li>The same network handles all decisions <span class="math inline">\(d\)</span> simultaneously if <span class="math inline">\(d\)</span> is included as an input.</li>
<li>The approach naturally handles likelihood-free models where <span class="math inline">\(p(y|\theta)\)</span> is unavailable but we can simulate from the forward model.</li>
</ol>
<p>For completeness, we provide the formal measure-theoretic framework. Let <span class="math inline">\(\mathcal{Y}\)</span> denote a locally compact metric space of signals <span class="math inline">\(y\)</span> and <span class="math inline">\(\Theta\)</span> a space of parameters <span class="math inline">\(\theta\)</span> (including any latent variables). Let <span class="math inline">\(P(dy|\theta)\)</span> denote the conditional distribution of signals given parameters. Let <span class="math inline">\(\Pi(d\theta|y)\)</span> denote the posterior distribution. In many cases, <span class="math inline">\(\Pi\)</span> is absolutely continuous with density <span class="math inline">\(\pi\)</span>: <span class="math display">\[
\Pi(d\theta|y) = \pi(\theta|y) \mu(d\theta).
\]</span></p>
<p>The framework handles both traditional likelihood-based models where <span class="math inline">\(P(dy|\theta) = p(y|\theta) \lambda(dy)\)</span> and likelihood-free models specified by forward simulators <span class="math inline">\(y = f(\theta)\)</span>. This generality is crucial for modern applications in economics, epidemiology, and climate science where complex simulation models replace closed-form likelihoods.</p>
<p>For multivariate parameters <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>, we can use autoregressive structures to model the sequence of conditional quantiles: <span class="math display">\[
(F^{-1}_{\theta_1}(\tau_1), F^{-1}_{\theta_2|\theta_1}(\tau_2), \ldots, F^{-1}_{\theta_p|\theta_{1:p-1}}(\tau_p))
\]</span> This factorization is analogous to autoregressive density models but operates directly on quantiles, avoiding normalization constraints.</p>
<p>An important architectural choice distinguishes our approach from standard posterior learning followed by Monte Carlo integration: we incorporate the utility function <span class="math inline">\(U(d, \theta)\)</span> directly into the first layer of the network. This allows the network to learn representations optimized for utility computation rather than pure posterior approximation. As utility functions often place high weight on tail events (representing rare but consequential outcomes), this direct incorporation significantly improves efficiency compared to the naive two-step approach.</p>
<div id="exm-qnn-wang" class="theorem example">
<p><span class="theorem-title"><strong>Example 21.2 (Normal-Normal Model and Wang Distortion)</strong></span> For illustration, we consider the normal-normal conjugate learning model—a case where the quantile updating rule can be derived analytically. This example connects quantile methods to Wang’s risk distortion measure from finance, showing that the distortion function is precisely the transformation that needs to be learned.</p>
<p>Consider the model: <span class="math display">\[
y_1, \ldots, y_n \mid \theta \sim N(\theta, \sigma^2)
\]</span> <span class="math display">\[
\theta \sim N(\mu,\alpha^2)
\]</span></p>
<p>The sufficient statistic for <span class="math inline">\(\mu\)</span> (assuming known <span class="math inline">\(\sigma^2\)</span>) is <span class="math inline">\(S(y) = \bar y = \frac{1}{n} \sum_{i=1}^n y_i\)</span>. Define <span class="math inline">\(t = \sigma^2 + n\alpha^2\)</span>. The posterior is <span class="math inline">\(\theta \mid y \sim N(\mu_*, \sigma_*^2)\)</span> with <span class="math display">\[
\mu_* = \frac{\sigma^2 \mu + n\alpha^2\bar{y}}{t}, \quad \sigma^2_* = \frac{\alpha^2 \sigma^2}{t}
\]</span></p>
<p>The remarkable result is that the posterior and prior CDFs are related via a Wang distortion function: <span class="math display">\[
1-\Phi(\theta; \mu_*,\sigma_*) = g(1 - \Phi(\theta; \mu, \alpha^2))
\]</span> where <span class="math inline">\(\Phi(\cdot; \mu, \sigma^2)\)</span> denotes the normal CDF. The Wang distortion is: <span class="math display">\[
g(p) = \Phi\left(\lambda_1 \Phi^{-1}(p) + \lambda\right)
\]</span> with distortion parameters: <span class="math display">\[
\lambda_1 = \frac{\alpha}{\sigma_*}, \quad \lambda = \frac{\alpha\lambda_1(n\bar{y}-n\mu)}{t}
\]</span></p>
<p>The detailed derivation, provided by <span class="citation" data-cites="wang1996premium">Wang (<a href="references.html#ref-wang1996premium" role="doc-biblioref">1996</a>)</span>, shows that the distortion parameters depend on the sample statistics and prior hyperparameters through the posterior updating formulas.</p>
<p>This analytical result has several implications:</p>
<ol type="1">
<li><p><em>Wang distortions</em> are natural: The distortion functions used in risk management <span class="citation" data-cites="wang1996premium">(<a href="references.html#ref-wang1996premium" role="doc-biblioref">Wang 1996</a>)</span> arise naturally from Bayesian updating in the normal case.</p></li>
<li><p><em>Learning the distortion</em>: In more complex models, a neural network can learn this distortion function <span class="math inline">\(g\)</span> directly from data, without requiring conjugacy.</p></li>
<li><p><em>Computational efficiency</em>: When the distortion is smooth and well-behaved, neural networks with relatively few parameters can accurately represent it.</p></li>
</ol>
<p><em>Numerical Example</em>: Consider prior <span class="math inline">\(\theta \sim N(0,5)\)</span> and data from <span class="math inline">\(y_i \sim N(3,10)\)</span> with <span class="math inline">\(n=100\)</span> observations. The posterior is <span class="math inline">\(\theta \mid y \sim N(3.28, 0.98)\)</span>.</p>
<div id="fig-wang" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-a" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-a-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Model for simulated data
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-b" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang2.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-b-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Distortion Function <span class="math inline">\(g\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-wang" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-wang-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/wang1.png" class="img-fluid figure-img" data-ref-parent="fig-wang">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-wang-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Survival Functions 1 - <span class="math inline">\(\Phi\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-wang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.3: The Normal-Normal learning model. Left: Prior, likelihood, and posterior densities. Center: The Wang distortion function <span class="math inline">\(g\)</span> that transforms the prior CDF to the posterior CDF. Right: Survival functions showing how <span class="math inline">\(g\)</span> maps the prior tail probabilities to posterior tail probabilities.
</figcaption>
</figure>
</div>
</div>
</section>
</section>
<section id="sec-qnn-implementation" class="level2" data-number="21.6">
<h2 data-number="21.6" class="anchored" data-anchor-id="sec-qnn-implementation"><span class="header-section-number">21.6</span> Neural Network Implementation</h2>
<p>The key implementation components are: (1) an appropriate loss function, (2) a neural architecture that handles the quantile input <span class="math inline">\(\tau\)</span>, and (3) training strategies for learning multiple quantiles simultaneously.</p>
<p>The 1-Wasserstein distance (also known as <em>earth mover’s distance</em>) provides theoretical justification for quantile methods. For two distributions with quantile functions <span class="math inline">\(F^{-1}_U\)</span> and <span class="math inline">\(F^{-1}_V\)</span>, the 1-Wasserstein distance is: <span class="math display">\[
W_1(F^{-1}_U, F^{-1}_V) = \int_0^1 |F^{-1}_U(\tau) - F^{-1}_V(\tau)| d\tau
\]</span></p>
<p>This distance can be computed efficiently using order statistics <span class="citation" data-cites="levina2001earth">(<a href="references.html#ref-levina2001earth" role="doc-biblioref">Levina and Bickel 2001</a>)</span>. The success of Wasserstein GANs <span class="citation" data-cites="arjovsky2017wasserstein">(<a href="references.html#ref-arjovsky2017wasserstein" role="doc-biblioref">Arjovsky, Chintala, and Bottou 2017</a>)</span> over vanilla GANs stems partly from this improved metric—Wasserstein distance provides meaningful gradients even when distributions have non-overlapping supports.</p>
<p>The key insight is that minimizing the quantile loss is equivalent to minimizing the 1-Wasserstein distance. For a target quantile <span class="math inline">\(q_\tau = F^{-1}_U(\tau)\)</span>, the check loss <span class="math inline">\(\rho_\tau\)</span> we derived in <a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 21.1</span></a> minimizes the expected prediction error: <span class="math display">\[
q_\tau = \arg\min_q E_U[\rho_{\tau}(U-q)]
\]</span></p>
<p>For training neural networks, we use a combination of quantile loss and mean-squared error (MSE). Given training data <span class="math inline">\(\{x_i, y_i\}_{i=1}^N\)</span> and a quantile <span class="math inline">\(\tau\)</span>, the loss is: <span class="math display">\[
L_{\tau}(\phi) = \sum_{i=1}^N \rho_{\tau}(y_i - f(\tau, x_i, \phi))
\]</span></p>
<p>Here <span class="math inline">\(\phi\)</span> denotes the neural network weights, distinct from the model parameters <span class="math inline">\(\theta\)</span> discussed elsewhere. Empirically, adding an MSE term improves stability and predictive accuracy: <span class="math display">\[
L(\phi) = \alpha L_{\tau}(\phi) + (1-\alpha) \cdot \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i, \phi))^2
\]</span></p>
<p>The weighting parameter <span class="math inline">\(\alpha \in [0,1]\)</span> balances quantile accuracy against overall fit. Typical values are <span class="math inline">\(\alpha \in [0.7, 0.9]\)</span>. The MSE term encourages the median prediction (<span class="math inline">\(\tau = 0.5\)</span>) to align with the conditional mean, which often improves generalization.</p>
<section id="learning-multiple-quantiles-simultaneously" class="level3">
<h3 class="anchored" data-anchor-id="learning-multiple-quantiles-simultaneously">Learning Multiple Quantiles Simultaneously</h3>
<p>Rather than training separate networks for each quantile <span class="math inline">\(\tau_k\)</span>, it is more efficient to learn all quantiles with a single network that takes <span class="math inline">\(\tau\)</span> as an input. Given quantiles <span class="math inline">\(0 &lt; \tau_1 &lt; \tau_2 &lt; \ldots &lt; \tau_K &lt; 1\)</span>, we minimize: <span class="math display">\[
L(\phi) = \frac{1}{NK} \sum_{i=1}^N \sum_{k=1}^K \rho_{\tau_k}(y_i - f_{\tau_k}(x_i, \phi))
\]</span></p>
<p>This approach has several advantages:</p>
<ol type="1">
<li><em>Shared representations</em>: The network learns features useful across all quantiles, improving sample efficiency.</li>
<li><em>Enforcing monotonicity</em>: A single network makes it easier to ensure quantiles don’t cross.</li>
<li><em>Smooth quantile function</em>: Interpolation between trained quantiles is more reliable.</li>
</ol>
</section>
<section id="non-crossing-constraints" class="level3">
<h3 class="anchored" data-anchor-id="non-crossing-constraints">Non-Crossing Constraints</h3>
<p>A valid distribution function must satisfy <span class="math inline">\(F^{-1}(\tau_i) \leq F^{-1}(\tau_j)\)</span> for <span class="math inline">\(\tau_i &lt; \tau_j\)</span>. Without explicit constraints, neural networks may learn quantile functions that cross: <span class="math display">\[
f_{\tau_i}(x, \theta) &gt; f_{\tau_j}(x, \theta) \quad \text{for some } x, \text{ despite } \tau_i &lt; \tau_j
\]</span></p>
<p>Several approaches address this <span class="citation" data-cites="chernozhukov2010quantile cannon2018noncrossing">(<a href="references.html#ref-chernozhukov2010quantile" role="doc-biblioref">Chernozhukov, Fernández-Val, and Galichon 2010</a>; <a href="references.html#ref-cannon2018noncrossing" role="doc-biblioref">Cannon 2018</a>)</span>:</p>
<ol type="1">
<li><em>Soft penalties</em>: Add a term to the loss penalizing violations.</li>
<li><em>Monotonic networks</em>: Design architectures that guarantee monotonicity in <span class="math inline">\(\tau\)</span> (e.g., using monotonic activation functions or cumulative link structures).</li>
<li><em>Post-processing</em>: After training, rearrange predictions to enforce monotonicity.</li>
</ol>
<p>For the implementations in this chapter, we use soft penalties during training combined with post-processing for final predictions.</p>
</section>
<section id="cosine-embedding-for-tau" class="level3">
<h3 class="anchored" data-anchor-id="cosine-embedding-for-tau">Cosine Embedding for <span class="math inline">\(\tau\)</span></h3>
<p>A key architectural choice is how to incorporate the quantile level <span class="math inline">\(\tau \in (0,1)\)</span> as an input to the network. Simply concatenating <span class="math inline">\(\tau\)</span> as an additional feature works but is inefficient—the network must learn the entire relationship between <span class="math inline">\(\tau\)</span> and the output from scratch.</p>
<p>A more effective approach uses a <em>cosine embedding</em> to represent <span class="math inline">\(\tau\)</span> in a higher-dimensional feature space. This leverages Fourier analysis: smooth functions can be well-approximated by cosine bases. The quantile function <span class="math inline">\(F^{-1}(\tau, x)\)</span> is typically smooth in <span class="math inline">\(\tau\)</span>, making Fourier representations natural.</p>
<p>We represent the quantile network as: <span class="math display">\[
F^{-1}(\tau, x) = f_\theta(\tau, x) = g(\psi(x) \circ \phi(\tau))
\]</span> where <span class="math inline">\(\circ\)</span> denotes element-wise multiplication (Hadamard product), <span class="math inline">\(g\)</span> and <span class="math inline">\(\psi\)</span> are feed-forward networks, and <span class="math inline">\(\phi\)</span> is the cosine embedding: <span class="math display">\[
\phi_j(\tau) = \mathrm{ReLU}\left(\sum_{i=0}^{n-1} \cos(\pi i \tau) w_{ij} + b_j\right)
\]</span></p>
<p>The cosine embedding <span class="math inline">\(\phi(\tau)\)</span> transforms the scalar <span class="math inline">\(\tau\)</span> into a vector of dimension <span class="math inline">\(m\)</span>, where <span class="math inline">\(n\)</span> controls the frequency resolution. This embedding has several advantages:</p>
<ol type="1">
<li><em>Smooth interpolation</em>: The cosine basis ensures smooth quantile functions.</li>
<li><em>Universal approximation</em>: <span class="citation" data-cites="barron1993universal">Barron (<a href="references.html#ref-barron1993universal" role="doc-biblioref">1993</a>)</span> showed that cosine-embedded networks achieve approximation rates of <span class="math inline">\(O(N^{-1/2})\)</span> for sufficiently smooth functions.</li>
<li><em>Parameter efficiency</em>: The embedding significantly reduces the number of parameters needed compared to learning the <span class="math inline">\(\tau\)</span> dependence from scratch.</li>
</ol>
<p>This architecture was successfully applied to distributional reinforcement learning by <span class="citation" data-cites="dabney2018implicit">Dabney et al. (<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">2018</a>)</span>, where it enabled agents to learn entire distributions of returns rather than just expectations. We use the same principle here for Bayesian posterior quantiles.</p>
</section>
<section id="synthetic-data-example" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-data-example">Synthetic Data Example</h3>
<p>To validate our approach before applying it to real data, we first test on synthetic data where the true quantile function is known. This allows us to assess both accuracy and the quality of uncertainty quantification.</p>
<p>Consider synthetic data generated from the model: <span class="math display">\[
x \sim U(-1, 1), \quad y | x \sim N\left(\frac{\sin(\pi x)}{\pi x}, \frac{\exp(1-x)}{10}\right)
\]</span></p>
<p>This model has heteroskedastic noise—the variance increases as <span class="math inline">\(x\)</span> decreases. The conditional mean is the sinc function <span class="math inline">\(\text{sinc}(x) = \sin(\pi x)/(\pi x)\)</span>, which has interesting non-linear behavior near zero.</p>
<p>The true conditional <span class="math inline">\(\tau\)</span>-quantile function is: <span class="math display">\[
f_{\tau}(x) = \frac{\sin(\pi x)}{\pi x} + \Phi^{-1}(\tau) \sqrt{\frac{\exp(1-x)}{10}}
\]</span></p>
<p>We train two types of quantile networks:</p>
<ol type="1">
<li><em>Implicit network</em>: Uses cosine embedding for <span class="math inline">\(\tau\)</span>, trained on random <span class="math inline">\((\tau, x, y)\)</span> triples</li>
<li><em>Explicit network</em>: Trains separate outputs for fixed quantiles <span class="math inline">\(\tau \in \{0.05, 0.5, 0.95\}\)</span></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/synthetic.svg" class="img-fluid figure-img"></p>
<figcaption>Quantile neural network predictions on synthetic data. Both implicit and explicit architectures recover the true quantile functions accurately. The shaded region shows the 90% prediction interval (<span class="math inline">\(\tau = 0.05\)</span> to <span class="math inline">\(\tau = 0.95\)</span>).</figcaption>
</figure>
</div>
<p>The figure shows both networks recover the true quantiles accurately. The implicit network provides smooth interpolation across all <span class="math inline">\(\tau\)</span> values, while the explicit network gives predictions only at the three trained quantiles. For applications requiring full distributional predictions, the implicit approach is preferable despite slightly higher computational cost during training.</p>
</section>
</section>
<section id="sec-qnn-portfolio" class="level2" data-number="21.7">
<h2 data-number="21.7" class="anchored" data-anchor-id="sec-qnn-portfolio"><span class="header-section-number">21.7</span> Portfolio Optimization with Quantile Neural Networks</h2>
<p>This example, developed by <span class="citation" data-cites="polson2024generative">N. Polson, Ruggeri, and Sokolov (<a href="references.html#ref-polson2024generative" role="doc-biblioref">2024</a>)</span>, demonstrates portfolio optimization under parameter uncertainty when the optimal decision depends on unknown parameters.</p>
<p>Consider power utility and log-normal returns without leverage. We assume that a portfolio value <span class="math inline">\(X = e^W\)</span> follows a log-normal distribution <span class="math display">\[
W(\omega) = (1-\omega)r_f + \omega R, \quad R \sim \mathcal{N}(\mu,\sigma^2)
\]</span> Here <span class="math inline">\(\omega \in (0,1)\)</span> is the portfolio weight, <span class="math inline">\(r_f\)</span> is the risk-free rate, <span class="math inline">\(\mu\)</span> is the mean return and <span class="math inline">\(\sigma^2\)</span> is the variance of the return. The utility function is then given by <span class="math display">\[
U(W) = -e^{-\gamma W}.
\]</span> Here, <span class="math inline">\(U^{-1}\)</span> exists, and the expected utility is <span class="math display">\[
U(\omega) = E(-e^{\gamma W}) = \exp\left\{\gamma \mathrm{E}(W) + \frac{1}{2}\omega^2\mathrm{Var}(W)\right\}.
\]</span> In this case, we have a closed-form solution for the expected utility, as a function of the decision variable <span class="math inline">\(\omega\)</span> (portfolio weight). It is the moment-generating function of the log-normal. We can plug-in the mean and variance of <span class="math inline">\(W\)</span> to get the expected utility <span class="math display">\[
U(\omega) = \exp\left\{\gamma \left\{(1-\omega)r_f + \omega\mu\right\}\right\} \exp \left \{ \dfrac{1}{2}\gamma^2\omega^2\sigma^2 \right \}.
\]</span> The optimal Kelly-Brieman-Thorpe-Merton value of <span class="math inline">\(\omega\)</span> is given by <span class="math display">\[
\omega^* = (\mu - r_f)/(\sigma^2\gamma).
\]</span></p>
<p>Within the GBC framework, it is easy to add learning or uncertainty on top of <span class="math inline">\(\sigma^2\)</span> and have a joint posterior distribution <span class="math inline">\(p(\mu, \sigma^2 \mid R)\)</span>.</p>
<p>Now we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of <span class="math inline">\(U\)</span> <span class="math display">\[
E(U(W)) = \int_{0}^{1}F_{U(W)}^{-1}(\tau)d\tau
\]</span> Hence, if we can approximate the inverse of the CDF of <span class="math inline">\(U(W)\)</span> with a quantile NN, we can approximate the expected utility and optimize over <span class="math inline">\(\omega\)</span>.</p>
<p>The stochastic utility is modeled with a deep neural network, and we write <span class="math display">\[
Z = U(W) \approx F, \quad W  = U^{-1}(F)
\]</span> We can do optimization by doing the grid search for <span class="math inline">\(\omega\)</span>.</p>
<p>The decision variable <span class="math inline">\(\omega\)</span> affects the distribution of the returns. The utility only depends on the returns <span class="math inline">\(W\)</span>. Our quantile neural network solution is given by the following algorithm:</p>
<ol type="1">
<li>Simulate log-returns <span class="math inline">\(W^{(i)}\mid \omega^{(i)} \sim N((1-\omega^{(i)})r_f + \omega^{(i)}\mu, \sigma^2 (\omega^{(i)})^2)\)</span></li>
<li>Calculate corresponding utilities <span class="math inline">\(Z^{(i)} = U(W^{(i)})\)</span></li>
<li>Learn <span class="math inline">\(F_{Z_{\omega}}^{-1}\)</span> with a quantile NN</li>
<li>Find the optimal portfolio weight <span class="math inline">\(\omega^\star\)</span> via <span class="math display">\[
E(Z_{\omega}) = \sum_{i=1}^{N}F^{-1}_{Z_{\omega}}(u_i) \rightarrow \underset{\omega}{\mathrm{maximize}}
\]</span></li>
</ol>
<section id="empirical-example" class="level3">
<h3 class="anchored" data-anchor-id="empirical-example">Empirical Example</h3>
<p>Consider <span class="math inline">\(\omega \in (0,1)\)</span>, <span class="math inline">\(r_f = 0.05\)</span>, <span class="math inline">\(\mu=0.1\)</span>, <span class="math inline">\(\sigma=0.25\)</span>, <span class="math inline">\(\gamma = 2\)</span>. We have the closed-form fractional Kelly criterion solution <span class="math display">\[
\omega^* = \frac{1}{\gamma}   \frac{ \mu - r_f}{ \sigma^2} = \frac{1}{2} \frac{ 0.1 - 0.05 }{ 0.25^2 } = 0.40
\]</span> We can simulate the expected utility and compare with the closed-form solution.</p>
<div id="fig-portfolio-qnn" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-portfolio-qnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-portfolio-qnn" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-portfolio-tau" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-portfolio-tau-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/portfolio-tau-z.png" class="img-fluid figure-img" data-ref-parent="fig-portfolio-qnn">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-portfolio-tau-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Quantile function of utility <span class="math inline">\(Z = -\exp(-0.1W)\)</span> versus <span class="math inline">\(\tau\)</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-portfolio-qnn" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-portfolio-eu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-portfolio-eu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="fig/portfolio.png" class="img-fluid figure-img" data-ref-parent="fig-portfolio-qnn">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-portfolio-eu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Expected utility integral as function of portfolio weight <span class="math inline">\(\omega\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-portfolio-qnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.4: Portfolio optimization via quantile neural networks. Left panel shows plot of sorted values of <span class="math inline">\(\tau\)</span> versus sorted values of random draws from <span class="math inline">\(-\exp(-\omega W)\)</span> for <span class="math inline">\(\omega=0.1\)</span>. Right panel shows values of integral of <span class="math inline">\(Z\)</span> with respect to <span class="math inline">\(\tau\)</span> versus corresponding values of <span class="math inline">\(\omega\)</span>. The integral was calculated using trapezoid rule. The red vertical line corresponds to <span class="math inline">\(\omega = 0.4\)</span>, which is the analytical optimum.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-qnn-forecasting" class="level2" data-number="21.8">
<h2 data-number="21.8" class="anchored" data-anchor-id="sec-qnn-forecasting"><span class="header-section-number">21.8</span> Supply Chain Forecasting at Scale</h2>
<p>This section examines probabilistic forecasting for inventory management, illustrating how quantile neural networks scale to millions of products while providing uncertainty quantification for optimal decision-making.</p>
<p>Amazon, Walmart, and other large retailers face a fundamental challenge: for millions of products across thousands of warehouses, how much inventory should be kept in stock? Too little inventory leads to stockouts and lost sales; too much ties up capital and risks obsolescence. The optimal inventory level depends critically on future demand, which is inherently uncertain.</p>
<p>Traditional forecasting provides point estimates—a single expected demand value. But optimal inventory decisions require understanding the entire demand distribution:</p>
<ul>
<li><em>Safety stock</em> (<span class="math inline">\(P_{10}\)</span>): Conservative estimate—ensures 90% of demand scenarios are covered, preventing stockouts</li>
<li><em>Base stock</em> (<span class="math inline">\(P_{50}\)</span>): Median demand—balances inventory holding costs against stockout risk</li>
<li><em>Capacity planning</em> (<span class="math inline">\(P_{90}\)</span>): Optimistic scenario—ensures warehouse and logistics capacity for high-demand periods</li>
</ul>
<p>The asymmetry matters enormously. For a highly profitable product with long lead times, the cost of stockouts (lost sales, customer dissatisfaction) far exceeds the cost of overstock. The optimal policy may target <span class="math inline">\(P_{70}\)</span> or <span class="math inline">\(P_{80}\)</span>. For perishable goods or fast-moving consumer products with low margins, the reverse holds—target <span class="math inline">\(P_{30}\)</span> or <span class="math inline">\(P_{40}\)</span> to minimize waste.</p>
<p>Amazon developed DeepAR <span class="citation" data-cites="salinas2019deepar">(<a href="references.html#ref-salinas2019deepar" role="doc-biblioref">Salinas, Flunkert, and Gasthaus 2019</a>)</span>, a deep learning approach for probabilistic time series forecasting. DeepAR uses recurrent neural networks (specifically LSTMs) to model temporal dependencies, but its key innovation is forecasting entire probability distributions rather than point estimates.</p>
<p>The model predicts parameters of a parametric distribution (e.g., mean and variance of a Gaussian or negative binomial). To obtain quantiles, one samples from this learned distribution. However, parametric assumptions can be restrictive. Amazon later extended the approach to directly forecast quantiles using the check loss functions we developed in <a href="#sec-qnn-quantile-regression" class="quarto-xref"><span>Section 21.1</span></a>.</p>
<p>The quantile approach has several advantages for demand forecasting:</p>
<ol type="1">
<li><em>Robustness</em>: Demand data often contains outliers (promotional events, viral products). Quantile regression is robust to these.</li>
<li><em>Intermittent demand</em>: Many products have sparse, intermittent demand (many zeros). Parametric distributions struggle; quantile methods handle this naturally.</li>
<li><em>Asymmetric costs</em>: Different quantiles inform different decisions. The <span class="math inline">\(P_{10}\)</span> quantile is more relevant for safety stock than the mean.</li>
<li><em>Model flexibility</em>: Neural quantile regression makes no distributional assumptions beyond smoothness.</li>
</ol>
<p>While production systems at Amazon and similar retailers deploy deep architectures with LSTMs, attention mechanisms, and learned embeddings, the core principles of quantile forecasting remain the same regardless of model complexity. In the following example, we illustrate these principles using interpretable linear quantile regression before discussing how to scale to neural network implementations.</p>
<section id="demand-forecasting-setup" class="level3">
<h3 class="anchored" data-anchor-id="demand-forecasting-setup">Demand Forecasting Setup</h3>
<p>Consider forecasting demand <span class="math inline">\(y_t\)</span> for a product given the features in <a href="#tbl-demand-features" class="quarto-xref">Table&nbsp;<span>21.2</span></a>:</p>
<div id="tbl-demand-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-demand-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;21.2: Features for demand forecasting
</figcaption>
<div aria-describedby="tbl-demand-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 47%">
<col style="width: 27%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Feature Category</th>
<th>Examples</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Temporal features</td>
<td>Day of week, month, holiday indicators, days since launch</td>
<td>Capture seasonal patterns and product lifecycle</td>
</tr>
<tr class="even">
<td>Lagged demand</td>
<td><span class="math inline">\(y_{t-1}, y_{t-7}, y_{t-28}\)</span> (yesterday, last week, last month)</td>
<td>Incorporate recent demand history and autocorrelation</td>
</tr>
<tr class="odd">
<td>Product features</td>
<td>Category, price, promotional flags</td>
<td>Account for product-specific characteristics</td>
</tr>
<tr class="even">
<td>External covariates</td>
<td>Weather, events, competitor prices</td>
<td>Include external factors affecting demand</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The model predicts conditional quantiles: <span class="math display">\[
q_\tau(t) = F^{-1}_{Y_t | X_t}(\tau)
\]</span></p>
<p>where <span class="math inline">\(X_t\)</span> represents all available features at time <span class="math inline">\(t\)</span>. Training data consists of historical demand <span class="math inline">\((y_t, x_t)\)</span> for <span class="math inline">\(t = 1, \ldots, T\)</span> across many products.</p>
<p>Traditional quantile regression assumes linear relationships between features and quantiles. For demand forecasting, this assumption is inadequate. Holiday effects interact with day-of-week patterns in complex, non-linear ways. Products exhibit substitution and complementarity effects that vary across categories and time periods. Promotional dynamics create non-linear price elasticity that depends on product category, timing, and competitive environment. Moreover, products with similar characteristics often exhibit similar demand patterns, suggesting opportunities for transfer learning across the product catalog.</p>
<p>Quantile neural networks address these limitations through their flexible architecture. They can learn shared embeddings for product categories, allowing the model to discover latent similarities between products. Recurrent or attention layers capture complex temporal patterns that extend beyond simple autoregressive relationships. The network automatically models interactions between features without requiring manual specification of interaction terms. Perhaps most importantly, the shared representations enable transfer learning from data-rich products to new products with limited historical data, a critical capability for retailers constantly introducing new items.</p>
</section>
<section id="implementation-strategy-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-strategy-1">Implementation Strategy</h3>
<p>For a retailer with millions of SKUs (stock-keeping units), computational efficiency is critical. The architecture typically involves:</p>
<ol type="1">
<li><em>Embedding layers</em>: Map categorical variables (product ID, category, location) to dense vectors</li>
<li><em>Temporal encoder</em>: LSTM or Transformer to process time series history</li>
<li><em>Feature fusion</em>: Combine embeddings with numerical features</li>
<li><em>Quantile head</em>: Final layer produces <span class="math inline">\(\hat{q}_\tau(t)\)</span> for input <span class="math inline">\(\tau\)</span>, using cosine embedding</li>
</ol>
<p>Training uses mini-batches sampling randomly across products and time periods, with the combined quantile + MSE loss we discussed. The model is trained to predict multiple quantiles simultaneously (<span class="math inline">\(\tau \in \{0.1, 0.2, \ldots, 0.9\}\)</span>).</p>
<p>The following diagram illustrates the neural network architecture for demand forecasting:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="fig/amazon-forecast.png" class="img-fluid figure-img"></p>
<figcaption>Neural network architecture for quantile demand forecasting. Categorical features are embedded, temporal history is encoded, and all features are fused before the quantile head produces conditional quantile predictions.</figcaption>
</figure>
</div>
<p>At inference time, for a given product and time period, the model outputs all quantiles instantly, enabling inventory optimization algorithms to compute optimal stock levels.</p>
<p>We now demonstrate demand forecasting using quantile regression to predict the entire demand distribution. The simulation generates realistic demand data for three products over two years, incorporating multiple real-world patterns. Each product exhibits a positive trend reflecting business growth, weekly seasonality capturing day-of-week effects, monthly seasonality for longer-term patterns, and random promotional events occurring on approximately 10% of days that boost demand by 30-50 units. The demand also features heteroskedastic noise where variance increases proportionally with demand level, mimicking the greater uncertainty in forecasting high-demand periods.</p>
<p>The model uses quantile regression with features including product identifier, time trend, promotional indicator, day of week, day of month, and three lagged demand values (1-day, 7-day, and 30-day lags) to capture short-term momentum, weekly patterns, and monthly cycles. Five quantile models are trained simultaneously at <span class="math inline">\(\tau \in \{0.1, 0.3, 0.5, 0.7, 0.9\}\)</span>, each estimating a different point in the conditional demand distribution. The training set spans 640 days, with the final 90 days held out for evaluation.</p>
<div class="cell" data-layout-align="center" data-null_prefix="true">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="21-qnn_files/figure-html/amazon-demand-forecast-1.png" class="img-fluid figure-img" width="960"></p>
<figcaption>Demand forecasting with quantile neural networks. Top: Historical demand with trend and seasonality. Bottom: Out-of-sample forecasts showing P10, P50, P90 quantiles (fan chart).</figcaption>
</figure>
</div>
</div>
</div>
<p>The quantile regression model demonstrates strong calibration on the held-out test period. The 80% prediction interval (P10 to P90) achieves 79.6% empirical coverage, meaning actual demand falls within this range about 80% of the time, closely matching the theoretical target. The 60% interval (P30 to P70) achieves 33.7% coverage, somewhat below the 60% target, suggesting the model may be overestimating uncertainty in the middle of the distribution. The median forecast (P50) produces a mean absolute error of 21.94 units, providing reasonably accurate central tendency estimates while the full quantile spectrum captures demand uncertainty.</p>
<p>The figure visualizes both historical patterns and probabilistic forecasts. The top panel displays historical demand for Product 1 from the training period, revealing the complex interplay of trend, seasonality, and promotional effects (marked in red). Demand oscillates around an upward trend with clear weekly and monthly cycles, punctuated by sharp spikes during promotional periods.</p>
<p>The bottom panel presents the out-of-sample forecasts as a fan chart, a standard visualization for probabilistic predictions. The black line shows actual realized demand, while the dark blue line represents the median forecast (P50). The shaded regions illustrate prediction intervals: the lighter blue band spans P10 to P90 (80% interval), while the darker blue band covers P30 to P70 (60% interval). This visualization immediately conveys forecast uncertainty, with wider intervals during high-demand periods reflecting heteroskedastic noise. The median forecast tracks actual demand reasonably well, while the intervals appropriately capture most realizations, demonstrating the model’s ability to quantify forecast risk rather than providing only point predictions.</p>
</section>
</section>
<section id="sec-qnn-rl" class="level2" data-number="21.9">
<h2 data-number="21.9" class="anchored" data-anchor-id="sec-qnn-rl"><span class="header-section-number">21.9</span> Distributional Reinforcement Learning</h2>
<p>Quantile neural networks have also been applied to reinforcement learning (RL). Recall from <a href="09-rl.html" class="quarto-xref"><span>Chapter 9</span></a> that RL agents learn policies <span class="math inline">\(\pi\)</span> that maximize expected cumulative reward. Traditional RL algorithms like Q-learning estimate the expected value function <span class="math inline">\(Q^\pi(s,a) = E[R | s, a, \pi]\)</span>—the expected return from taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span> thereafter.</p>
<p><em>Distributional reinforcement learning</em> <span class="citation" data-cites="bellemare2017distributional">(<a href="references.html#ref-bellemare2017distributional" role="doc-biblioref">Bellemare, Dabney, and Munos 2017</a>)</span> extends this by learning the entire distribution of returns rather than just the expectation. Using quantile neural networks, agents learn <span class="math inline">\(F^{-1}_{R|s,a}(\tau)\)</span>, the quantile function of returns.</p>
<p>Learning return distributions provides several advantages:</p>
<ol type="1">
<li><em>Risk-sensitive policies</em>: Different quantiles inform different behaviors. Conservative agents might maximize <span class="math inline">\(P_{10}\)</span> (avoid worst-case scenarios), while risk-seeking agents target <span class="math inline">\(P_{90}\)</span> (optimize for best-case outcomes).</li>
<li><em>Better learning signals</em>: The Bellman operator naturally contracts in Wasserstein distance when formulated for quantiles, leading to more stable learning <span class="citation" data-cites="dabney2018implicit">(<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">Dabney et al. 2018</a>)</span>.</li>
<li><em>Uncertainty quantification</em>: Understanding return variance helps agents explore intelligently—high uncertainty indicates potential for learning.</li>
</ol>
<p><span class="citation" data-cites="dabney2018implicit">Dabney et al. (<a href="references.html#ref-dabney2018implicit" role="doc-biblioref">2018</a>)</span> use quantile neural networks for distributional Q-learning. The key insight is that expectations can be computed as integrals over quantiles (the Lorenz curve identity): <span class="math display">\[
E[R] = \int_{-\infty}^{\infty} r dF(r) = \int_0^1 F^{-1}(u) du
\]</span></p>
<p>The distributional RL algorithm finds the optimal policy: <span class="math display">\[
\pi^\star(s) = \arg\max_a \int_0^1 F^{-1}_{R|s,a}(\tau) d\tau = \arg\max_a E_{Z \sim z(s, a)}[Z]
\]</span></p>
<p>The network is trained using the quantile loss <span class="math inline">\(\rho_\tau\)</span> we developed, and Q-learning updates can be applied since the quantile projection operator preserves the contraction property of the Bellman operator. This approach has achieved state-of-the-art performance on Atari games and robotic control tasks.</p>
<p>Connections to dual utility theory <span class="citation" data-cites="yaari1987dual">(<a href="references.html#ref-yaari1987dual" role="doc-biblioref">Yaari 1987</a>)</span> suggest that distributional RL naturally incorporates risk preferences—agents can be trained to maximize any utility functional, not just expectations.</p>
</section>
<section id="sec-qnn-discussion" class="level2" data-number="21.10">
<h2 data-number="21.10" class="anchored" data-anchor-id="sec-qnn-discussion"><span class="header-section-number">21.10</span> Discussion and Summary</h2>
<p>As Keynes observed, it is better to be roughly right than precisely wrong. Quantile methods embrace this philosophy: they provide the distributional information needed for sound decisions without claiming to know the complete probability model. In an era of increasingly complex data and high-stakes applications, this combination of flexibility, robustness, and decision-focus makes quantile neural networks an essential tool for the modern data scientist.</p>
<p>Quantile neural networks represent a convergence of classical statistical theory (quantile regression, robust statistics) with modern machine learning (deep learning, representation learning). By focusing on the quantities we actually need—quantiles for decision-making—rather than intermediate densities, these methods offer a pragmatic and powerful approach to uncertainty quantification.</p>
<p>Several foundational insights underpin this chapter. The quantile-expectation identity <span class="math inline">\(E[U] = \int_0^1 F^{-1}_U(\tau) d\tau\)</span> enables direct learning of expected utilities without density estimation. The check loss <span class="math inline">\(\rho_\tau(u) = u(\tau - I(u &lt; 0))\)</span> emerges from minimizing Wasserstein distance and handles asymmetric costs elegantly. Cosine embeddings for <span class="math inline">\(\tau\)</span> leverage Fourier approximation theory to provide universal approximators with <span class="math inline">\(O(N^{-1/2})\)</span> convergence rates. Applications span portfolio optimization (<a href="#sec-qnn-portfolio" class="quarto-xref"><span>Section 21.7</span></a>), demand forecasting (<a href="#sec-qnn-forecasting" class="quarto-xref"><span>Section 21.8</span></a>), posterior quantile learning via Wang distortions (<a href="#sec-qnn-bayes-quantiles" class="quarto-xref"><span>Section 21.4</span></a>), and distributional Q-learning (<a href="#sec-qnn-rl" class="quarto-xref"><span>Section 21.9</span></a>).</p>
<p>Quantile neural networks are well-suited for: decision problems requiring expectations rather than full densities; likelihood-free settings where <span class="math inline">\(p(y|\theta)\)</span> is unavailable; data with outliers or heavy tails; asymmetric costs where different quantiles drive different decisions; high-dimensional settings where density estimation is intractable; and real-time applications requiring fast inference.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-arjovsky2017wasserstein" class="csl-entry" role="listitem">
Arjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. <span>“Wasserstein Generative Adversarial Networks.”</span> <em>Proceedings of the 34th International Conference on Machine Learning</em>, 214–23.
</div>
<div id="ref-barron1993universal" class="csl-entry" role="listitem">
Barron, Andrew R. 1993. <span>“Universal Approximation Bounds for Superpositions of a Sigmoidal Function.”</span> <em>IEEE Transactions on Information Theory</em> 39 (3): 930–45.
</div>
<div id="ref-bellemare2017distributional" class="csl-entry" role="listitem">
Bellemare, Marc G., Will Dabney, and Rémi Munos. 2017. <span>“A Distributional Perspective on Reinforcement Learning.”</span> <em>Proceedings of the 34th International Conference on Machine Learning</em>, 449–58.
</div>
<div id="ref-brillinger2012generalized" class="csl-entry" role="listitem">
Brillinger, David R. 2012. <span>“A <span>Generalized Linear Model With</span> <span>‘<span>Gaussian</span>’</span> <span>Regressor Variables</span>.”</span> In <em>Selected <span>Works</span> of <span>David Brillinger</span></em>, edited by Peter Guttorp and David Brillinger, 589–606. Selected <span>Works</span> in <span>Probability</span> and <span>Statistics</span>. New York, NY: Springer.
</div>
<div id="ref-cannon2018noncrossing" class="csl-entry" role="listitem">
Cannon, Alex J. 2018. <span>“Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes.”</span> <em>Stochastic Environmental Research and Risk Assessment</em> 32 (11): 3207–25.
</div>
<div id="ref-chernozhukov2010quantile" class="csl-entry" role="listitem">
Chernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010. <span>“Quantile and <span>Probability Curves Without Crossing</span>.”</span> <em>Econometrica : Journal of the Econometric Society</em> 78 (3): 1093–1125.
</div>
<div id="ref-dabney2018implicit" class="csl-entry" role="listitem">
Dabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018. <span>“Implicit <span>Quantile Networks</span> for <span>Distributional Reinforcement Learning</span>.”</span> arXiv. <a href="https://arxiv.org/abs/1806.06923">https://arxiv.org/abs/1806.06923</a>.
</div>
<div id="ref-kallenberg1997foundations" class="csl-entry" role="listitem">
Kallenberg, Olav. 1997. <em>Foundations of <span>Modern Probability</span></em>. 2nd ed. edition. Springer.
</div>
<div id="ref-levina2001earth" class="csl-entry" role="listitem">
Levina, Elizaveta, and Peter Bickel. 2001. <span>“The Earth Mover’s Distance Is the Mallows Distance: <span>Some</span> Insights from Statistics.”</span> In <em>Proceedings Eighth <span>IEEE</span> International Conference on Computer Vision. <span>ICCV</span> 2001</em>, 2:251–56. IEEE.
</div>
<div id="ref-parzen2004quantile" class="csl-entry" role="listitem">
Parzen, Emanuel. 2004. <span>“Quantile <span>Probability</span> and <span>Statistical Data Modeling</span>.”</span> <em>Statistical Science</em> 19 (4): 652–62.
</div>
<div id="ref-polson2016mixtures" class="csl-entry" role="listitem">
Polson, Nicholas G., and James G. Scott. 2016. <span>“Mixtures, <span>Envelopes</span> and <span>Hierarchical Duality</span>.”</span> <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> 78 (4): 701–27.
</div>
<div id="ref-polson2023generative" class="csl-entry" role="listitem">
Polson, Nicholas G., and Vadim Sokolov. 2023. <span>“Generative <span>AI</span> for <span>Bayesian Computation</span>.”</span> <a href="https://arxiv.org/abs/2305.14972">https://arxiv.org/abs/2305.14972</a>.
</div>
<div id="ref-polson2024generative" class="csl-entry" role="listitem">
Polson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024. <span>“Generative <span>Bayesian Computation</span> for <span>Maximum Expected Utility</span>.”</span> <em>Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies</em> 26 (12): 1076.
</div>
<div id="ref-salinas2019deepar" class="csl-entry" role="listitem">
Salinas, David, Valentin Flunkert, and Jan Gasthaus. 2019. <span>“<span>DeepAR</span>: <span>Probabilistic Forecasting</span> with <span>Autoregressive Recurrent Networks</span>.”</span> <em>arXiv:1704.04110 [Cs, Stat]</em>, February. <a href="https://arxiv.org/abs/1704.04110">https://arxiv.org/abs/1704.04110</a>.
</div>
<div id="ref-wang1996premium" class="csl-entry" role="listitem">
Wang, Shaun. 1996. <span>“Premium Calculation by Transforming the Layer Premium Density.”</span> <em>ASTIN Bulletin</em> 26 (1): 71–92.
</div>
<div id="ref-yaari1987dual" class="csl-entry" role="listitem">
Yaari, Menahem E. 1987. <span>“The <span>Dual Theory</span> of <span>Choice</span> Under <span>Risk</span>.”</span> <em>Econometrica : Journal of the Econometric Society</em> 55 (1): 95–115.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./20-sgd.html" class="pagination-link" aria-label="Gradient Descent">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./22-cnn.html" class="pagination-link" aria-label="Convolutional Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>