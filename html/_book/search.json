[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nModern AI rests on three foundations: Bayesian reasoning, statistical learning, and deep neural networks. This book develops all three—starting from probability and decision theory, progressing through classical machine learning, and ending with transformers and autonomous agents. Whether you’re a business analyst seeking operational intuition or an engineer building systems, this book provides mathematical depth and practical fluency. Throughout, uncertainty is a first-class citizen: the foundation for better algorithms and clearer thinking about data, models, and decisions.\nThe material draws from courses we teach to MBAs at the University of Chicago Booth School of Business and engineers at George Mason University. These two audiences demand different emphases—business students want operational intuition and decision frameworks; engineers want mathematical rigor and implementation details—but each gains from the other—the same Bayesian methods that power marketing analytics and financial risk modeling also drive robotics and autonomous systems.\nWhen John McCarthy coined “artificial intelligence” in 1956, the field meant expert systems, hand-coded rules and algorithms. The foundations trace further back: Claude Shannon’s information theory, John von Neumann’s game theory and decision science, and Richard Bellman’s dynamic programming. The shift to data-driven learning—models that generalize from examples rather than following prescribed logic—defines modern AI. The contrast:\n\n\n\n\n\n\n\n\n\nNoteOld AI\n\n\n\nIF raining THEN take_umbrella\nA fixed rule. It works only one way. Seeing an umbrella tells us nothing about rain.\n\n\n\n\n \n\n\n\n\n\n\n\n\nNoteNew AI\n\n\n\nProbability of umbrella, given rain\nLearned from data. It works backwards as well. Seeing an umbrella tells us it’s likely raining.\n\n\n\n\n\nFor example, the Kalman filter navigated Apollo 11 to the moon in 1969. Today’s autonomous systems—from rocket landings to self-driving cars—rely on optimization and learning algorithms that would have been computationally infeasible then.\nHow we interact with AI as consumers has evolved through four stages:\n\nSearch. Early search engines answered a single question with a ranked list of webpages. The PageRank algorithm, developed by Google founders, used power iterations to rank these pages by relevance. Statistical tools like Kendall’s tau and Spearman’s rank correlation measured the similarity between the ranking and actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and that of others, easing the burden of choice.\nSummaries. Systems like ChatGPT go beyond retrieval: they synthesize and generalize across domains.\nAgents. Autonomous systems that perceive their environment, reason about goals, and take actions—orchestrating tools to execute multi-step tasks without step-by-step human guidance.\n\nBuilding systems at each stage requires the same foundations: probabilistic reasoning to handle uncertainty, statistical learning to extract patterns, and scalable computation to train models. This book develops all three.\nThe book is organized into three parts:\n\nPart 1: Bayesian Learning: Probability, decision theory, and Bayesian inference.\nPart 2: Statistical Learning: Pattern-matching algorithms including regression, decision trees, and generalized linear models.\nPart 3: Deep Learning: Neural network architectures, optimization via gradient descent, convolutional networks for vision, natural language processing, large language models, and autonomous agents.\n\nChapters include derivations, Python/R code, and problems and case studies.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "The Modern AI Playbook",
    "section": "",
    "text": "Levels of Automation: AIQ, AGI, ASI\nWhen you open an Amazon page, there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and others who have bought similar products, Amazon uses AI and machine learning to predict what might interest you the next time you shop. When you apply for a loan online, you typically get an immediate answer after filling out an application. The information you provide, combined with your credit history pulled from a credit bureau, is used by a predictive model to determine with high confidence whether you are likely to default on the loan.\nWhat do Amazon, the finance industry have in common? They all use AI-driven methods to improve operations. Automating routine workflows enables organizations to scale operations and tackle challenges that remain too complex for human labor alone. AI-driven methods are becoming increasingly important in all industries.\nBecause these technologies now drive core business value, organizations are competing aggressively for the limited pool of experts capable of building them. The demand for AI talent translates into substantial compensation premiums. According to the 2025 AI Talent Salary Report, AI professionals command a median salary of $160,000 annually (28% premium over traditional tech roles). Specialization matters: LLM engineers earn 25-40% more than general ML engineers, while AI Safety specialists have seen 45% salary increases since 2023.\nWhat are the key ingredients of AI-driven methods? At the heart of modern AI lie three fundamental pillars:\nThese software foundations are powered by High-Performance Computing Infrastructure. GPUs, originally designed for gaming, now make it feasible to process massive datasets and train billion-parameter models.\nThe interplay creates a virtuous cycle: computing power enables larger models (Deep Learning), while mathematical frameworks (Bayesian and Statistical Learning) ensure these models reason correctly and generalize well. Let’s clarify the terminology used to describe this landscape.\nData Science is the interdisciplinary field that uses these tools—along with statistics and software engineering—to extract insights from data.\nOver the last three decades, a major shift has occurred from “hardcoded” expertise to learned patterns. IBM’s Deep Blue (1997) played chess using rules and heuristics crafted by grandmasters. In contrast, DeepMind’s AlphaGo Zero (2017) learned to play Go, Chess, and Shogi solely by playing against itself, deriving strategies superior to human knowledge without any human instruction. This transition—replacing engineered rules with learned rules—defines the modern AI era.\nThe key concept behind many modern AI systems is pattern-recognition. A “pattern” is a prediction rule that maps an input to an expected output, and “learning a pattern” means fitting a good prediction rule to a data set. In AI, prediction rules are often referred to as “models.” The process of using data to find a good prediction rule is often called “training the model.” Mathematically, we can express this as learning a function \\(f\\) that maps inputs \\(x\\) to outputs \\(y\\), so that \\(y = f(x)\\).\nFor instance, in large language model, \\(x\\) represents a question and \\(y\\) represents the answer. In a chess game, \\(x\\) represents the board position and \\(y\\) represents the best move. The learning process involves finding the function \\(f\\) that best captures the relationship between inputs and outputs by examining many examples of input-output pairs in a training dataset. Deep learning excels at discovering complex, nonlinear functions \\(f\\) when the relationship between \\(x\\) and \\(y\\) is too intricate to specify manually—such as the mapping from raw pixel values to semantic image content, or from question and answer to text.\nWe can categorize AI systems by their level of autonomy. AIQ (Artificial Intelligence Quotient) represents the current paradigm where humans and machines work synergistically. As described by N. G. Polson and Scott (2018), this intelligence augmentation allows humans to offload routine tasks while retaining strategic control. Current systems, including LLMs, fall into this category—they are powerful tools that amplify human capability but lack independent volition.\nIn contrast, AGI (Artificial General Intelligence) refers to hypothetical systems matching human performance across all cognitive tasks, while ASI (Artificial Superintelligence) describes systems surpassing human intelligence in every dimension. While AGI and ASI remain topics of intense research and debate, this book focuses on the practical, mathematical foundations of the AIQ systems building the modern economy today.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#levels-of-automation-aiq-agi-asi",
    "href": "00-intro.html#levels-of-automation-aiq-agi-asi",
    "title": "The Modern AI Playbook",
    "section": "",
    "text": "Software Development: The First Frontier for Agents\nAndrej Karpathy’s talk, “Software Is Changing (Again),” illustrates a clear “product-market fit” for AI agents. Software development has become the first domain where agents have demonstrated significant, quantifiable impact, fundamentally transforming how code is written. Karpathy describes this new era as “Software 3.0,” where natural language becomes the primary interface.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face\n\n\nSoftware 3.0\nNatural-language prompts\nGuide & Verify\nPrompt libraries\n\n\n\nThis success in software engineering serves as a blueprint. The patterns established here—iterative planning, tool use, and verification—are already spreading to high-stakes fields like finance and healthcare, where agents will likely follow a similar trajectory of adoption.\n\n\nAI Agents: From Prediction to Action\nThe concept of software acting on our behalf has deep roots. Hal Varian’s 2010 framework of “Computer Mediated Transactions” (Varian 2010) foresaw a world where computers would automate not just calculation, but routine decision-making processes—monitoring markets, managing inventory, and coordinating complex transactions. This vision is now being realized and surpassed by modern AI Agents.\nWhile a Language Model (LLM) is like a brain in a jar—capable of thought (prediction) but isolated—an Agent is an LLM equipped with tools and agency. Agents can browse the web, execute code, query databases, and interact with other software systems to complete multi-step workflows. Unlike traditional rigid software, agents dynamically adapt their behavior: if a first attempt fails, they can “reason” about the error and try a different strategy.\nBuilding reliable agents requires robust orchestration frameworks that manage memory, planning, and tool execution. An example of a company that focuses on AI workloads is Nebius. Unlike traditional cloud architecture designed for general web services, their platform rethinks how data is stored, processed, and computed upon to support the massive parallel throughput required by modern AI models. This shift—from static models to dynamic agents—marks the transition from software that answers questions to software that iteratively plans and executes tasks in the digital and physical economy.\n\n\nPhysical AI: Embodied Intelligence\nWhile Large Language Models have mastered the digital realm of text and code, a new frontier is emerging: Physical AI. This field aims to give artificial intelligence a physical body, enabling it to perceive, understand, and interact with the tangible world. Unlike digital AI that processes symbols, Physical AI must contend with the chaotic laws of physics—gravity, friction, and unstructured environments. This represents a significant leap from “narrow” robotics, which followed rigid, pre-programmed instructions, to “embodied” intelligence that can learn, adapt, and operate alongside humans.\nThe convergence of vision-language models and advanced robotics is accelerating this shift. Companies like Tesla with Optimus, Boston Dynamics, and Figure are developing humanoid robots capable of performing complex tasks, from folding laundry to assembling cars. A key enabler is “sim-to-real” reinforcement learning, where robots train for millions of hours in hyper-realistic physics simulations (like NVIDIA’s Isaac Lab) before downloading those learned skills into a physical body. This allows them to master movements that would take decades to learn in the real world.\n\n\nThe Human Dimension: Dignity and Economics\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\nAs we delegate more authority to algorithms, the warnings of the cybernetics era become urgently relevant. Norbert Wiener, the mathematician who founded cybernetics and whose work laid foundations for modern control systems and artificial intelligence, was among the first to recognize both the power and peril of automation. Writing in The Human Use of Human Beings (1950), Wiener articulated a prescient warning:\n\nIf we combine our machine potentials of a factory with the valuation of human beings on which our present factory system is based, we are in for an Industrial Revolution of unmitigated cruelty. We must be willing to deal in facts rather than fashionable ideologies if we wish to get through this period unharmed.\n\nWiener’s concern was fundamentally about human identity and dignity. If workers derive their sense of worth from their role as factory laborers, and automation eliminates those roles, what becomes of their identity? This was not merely an economic question about displaced workers finding new employment—it was a deeper psychological and existential challenge. Wiener recognized that the transition to an automated economy would require not just retraining programs, but a fundamental reimagining of how humans find meaning and value in a world where machines perform an ever-expanding range of tasks.\nCrucially, Wiener rejected the notion that automation would deliver humanity into a comfortable retirement:\n\nThe world of the future will be an even more demanding struggle against the limitations of our intelligence, not a comfortable hammock in which we can lie down to be waited upon by our robot slaves.\n\nThis vision stands in stark contrast to utopian fantasies of automated abundance. Wiener foresaw that as machines took over routine cognitive and physical tasks, the remaining challenges would become more abstract, more complex, and more demanding of human creativity and judgment. The age of AI would not eliminate work—it would transform it into work that pushes against the very boundaries of human capability.\nJohn Maynard Keynes, writing during the Great Depression in his 1930 essay Economic Possibilities for our Grandchildren (1930), offered a remarkably optimistic counterpoint. Keynes predicted that within a century—roughly by 2030—technological progress and capital accumulation would “solve the economic problem” for humanity. By this he meant that productivity gains would become so substantial that meeting humanity’s basic material needs would require only minimal labor. Keynes envisioned a future where people might work perhaps fifteen hours per week, devoting the remainder of their time to leisure, culture, and the pursuit of fulfilling activities.\nKeynes distinguished between absolute needs—those we feel regardless of others’ circumstances, such as food, shelter, and safety—and relative needs—our desire to feel superior to our fellows. He argued that while relative needs are insatiable, absolute needs could be satisfied through technological abundance. Once this occurred, humanity would face a new challenge: learning to live wisely with leisure. Keynes worried that without the structure and purpose provided by work, many people would struggle to find meaning. He wrote that humanity would need to cultivate the “art of life itself” and learn to value activities pursued for their own sake rather than for economic gain.\nKeynes’s prediction that technology would dramatically increase productivity proved remarkably accurate. However, his assumption that increased productivity would translate into reduced working hours has not materialized as he expected. Rather than collectively choosing leisure, advanced economies have channeled productivity gains into increased consumption, higher living standards, and the expansion of service industries. The phenomenon of “Veblenian” conspicuous consumption helps explain why.\nThorstein Veblen, writing even earlier in The Theory of the Leisure Class (1899), offered a more cynical analysis of how elites use both leisure and consumption to signal status. Veblen introduced the concept of conspicuous consumption—the purchase of goods and services primarily to display wealth and social status rather than to satisfy genuine needs. The leisure class, in Veblen’s analysis, derives its social standing not from productive labor but from the ostentatious display of time and resources devoted to non-productive activities.\nVeblen’s insight reveals why Keynes’s vision of universal leisure has not materialized. In modern economies, work serves not only to produce income for consumption but also to confer identity, status, and social belonging. High-status professionals often work longer hours than necessary for material sustenance precisely because their work signals competence, dedication, and membership in elite circles. The “leisure” time that technology has created has often been filled not with Keynesian cultivation of the art of life, but with Veblenian status competitions—from luxury travel photographed for social media to the accumulation of credentials through continuous education.\nMoreover, as AI automates routine tasks, the remaining human work increasingly involves activities that are themselves forms of status display: strategic decision-making, creative innovation, and high-stakes problem-solving. These activities signal membership in cognitive elites in ways that parallel Veblen’s leisure class. The AI era has not eliminated status competition through work—it has transformed the nature of the work that confers status.\nIn his later work, The Engineers and the Price System (1921), Veblen examined the role of technical experts in modern industrial society. He distinguished between two fundamentally different modes of economic organization: the industrial system, driven by engineers and technical specialists focused on efficient production, and the price system, controlled by financiers and business owners focused on profit extraction. Veblen’s analysis is remarkably prescient for understanding tensions in the contemporary AI economy. Today’s “engineers”—the data scientists, machine learning researchers, and software developers building AI systems—possess technical knowledge that enables unprecedented productive capabilities. Yet the deployment of these capabilities is mediated through corporate structures optimized for profit maximization rather than social welfare.\nThe synthesis of these perspectives suggests that successfully navigating the AI transition requires more than technical solutions or economic policies. It requires cultivating new sources of meaning, identity, and social connection that are not solely dependent on traditional employment. It requires resisting purely Veblenian status competitions in favor of Keynesian cultivation of intrinsically valuable activities. And it requires heeding Wiener’s warning that the future will demand more, not less, of our intelligence, creativity, and ethical judgment—even as machines handle an expanding range of routine tasks.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#large-language-models-llms",
    "href": "00-intro.html#large-language-models-llms",
    "title": "The Modern AI Playbook",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\nThe most visible manifestation of the new AI age is Large Language Models. ChatGPT reached 100 million users in 2 months after its November 2022 launch—a milestone that took the internet 7 years and television 13 years. Unlike previous innovations requiring infrastructure changes, AI chatbots provide immediate value through simple web interfaces.\n\n\n\nSource: https://johnnosta.medium.com/the-most-important-chart-in-100-years-1095915e1605\n\n\nThe algorithmic foundations of deep learning have existed for decades: Kolmogorov’s superposition theorem (1956), Robbins-Monro’s stochastic approximation (1951), Tikhonov regularization (1940s), Polyak momentum optimization (1964), and Galushkin’s backpropagation (1972) is the “old math” that enables modern AI along with new GPU chips. The 2012 breakthrough with AlexNet would have been impossible without GPUs performing thousands of matrix multiplications simultaneously and convolutional neural networks developed by Fukushima in the early 80s. Current models rely on training clusters of thousands of interconnected GPUs working for weeks.\nGPU computational power has grown over 400,000x in two decades—from the 0.23 TeraFLOPS of 2006’s GeForce 7900 GTX to projected 100 PetaFLOPS for 2027’s Nvidia Rubin Ultra. This exponential growth, driven successively by gaming, cryptocurrency mining, and AI video generation, has made GPUs the engines of the deep learning revolution. Modern architectures feature specialized tensor cores and mixed-precision arithmetic (FP8/FP4) optimized for AI workloads rather than traditional floating-point operations.\n\n\n\n\n\n\nFigure 2: Compute requirements scale exponentially across AI tasks. Source: Michael Dell\n\n\n\nThe computational demands of AI tasks scale exponentially (Figure 2): while a single-shot chatbot represents the baseline (1x), image generation requires ~10x more compute, reasoning tasks need ~100x, video generation demands ~3,000x, and deep research capabilities require over 1,000,000x. Commercial LLMs measure computation in tokens—units of text processed as inputs or outputs—rather than traditional FLOPS. Early LLMs required fine-tuning for specific tasks; current models work effectively through in-context learning, where task-specific instructions and data are provided as context.\nThis exponential scaling illustrates a modern instance of Jevons paradox. In 1865, economist William Stanley Jevons observed that improvements in steam engine efficiency did not reduce coal consumption—instead, efficiency gains made coal-powered machinery economically viable for more applications, increasing total coal use. The same dynamic applies to AI compute: as models become more efficient and inference costs drop, usage expands faster than efficiency improves. Cheaper tokens enable longer conversations, more complex reasoning chains, and new applications like real-time video generation that were previously cost-prohibitive. The result is that total compute consumption continues to grow despite—and because of—efficiency improvements.\nWhile GPUs have driven the current AI revolution, the quest for quantum supremacy represents the next frontier. As Feynman’s seminal paper first discussed, the principles of quantum physics suggest a new foundation for computation. The work by N. Polson, Sokolov, and Xu (2023) on quantum Bayesian computation explores quantum algorithms that promise exponential speed-ups for Bayesian and neural network methods. While stable quantum computers remain in development, the algorithms are already being designed.\n\nBusiness Models and Market Impact\nFour distinct business models have emerged among leading LLM providers:\n\nOpenAI (ChatGPT) pioneered the subscription-plus-API model, combining consumer subscriptions ($20/month for Plus) with enterprise API pricing. Microsoft’s strategic partnership provides exclusive cloud infrastructure.\nAnthropic (Claude) focuses on enterprise safety, using Constitutional AI training methods that emphasize nuanced reasoning and acknowledgment of uncertainty—attracting regulated industries and research institutions.\nGoogle (Gemini) leverages deep integration with its ecosystem—Search, Workspace, Android, and Cloud—to distribute AI capabilities at scale. With access to proprietary data (YouTube, Maps, Scholar) and custom TPU hardware, Google competes on both consumer reach and enterprise infrastructure.\nPerplexity reimagines search as a citation-based conversational engine, synthesizing information from multiple sources with attribution rather than returning links.\n\nLLMs have found immediate applications across industries: customer service teams report 30-50% cost reductions with chatbots handling routine inquiries; coding assistants help developers complete tasks 55% faster; legal firms compress weeks of contract review into hours. The pattern is consistent: LLMs excel at processing, summarizing, and generating text-heavy work that previously required skilled human labor. Most deployments involve human oversight, but they fundamentally change the economics of knowledge work.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "The Modern AI Playbook",
    "section": "Generative AI",
    "text": "Generative AI\nBy 2025, Generative AI has moved beyond hype into practical application across personal and professional domains. A Harvard Business Review article by Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” reveals a notable trend: the top use cases have shifted from purely technical applications toward emotive and personal uses. “Therapy/companionship” now leads the list, followed by “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).”\nUsers are leveraging Gen AI for mental health support (especially in regions with limited access to therapists), daily habit planning, study guides, meal planning, travel itineraries, and drafting appeal letters. Gen AI users are also developing a deeper understanding of the technology’s limitations, including concerns around data privacy and over-reliance.\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.\n\n\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. The program worked by identifying keywords in user statements and transforming them into questions or reflective responses. For example, if a user typed “I am sad,” ELIZA might respond with “Why do you think you are sad?” or “Tell me more about being sad.” While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.\nWhat surprised Weizenbaum was not just that ELIZA worked, but how readily people attributed human-like understanding to the program. Users began forming emotional attachments to ELIZA, sharing deeply personal information and believing the computer genuinely cared about their problems. Some even requested private sessions without Weizenbaum present. This phenomenon, now known as the ELIZA effect, describes the human tendency to unconsciously assume computer behaviors are analogous to human behaviors, even when we know better intellectually.\nThe ELIZA effect reveals something profound about human psychology: we are predisposed to anthropomorphize systems that exhibit even rudimentary conversational abilities. This has significant implications for modern AI systems. Today’s large language models like ChatGPT and Claude are vastly more sophisticated than ELIZA, yet they still operate through pattern matching and statistical prediction rather than genuine understanding. However, their responses are so fluent and contextually appropriate that the ELIZA effect is amplified dramatically. Users often attribute consciousness, emotions, and intentionality to these systems, leading to both beneficial therapeutic interactions and concerning over-reliance on AI for emotional support.\nUnderstanding the ELIZA effect is crucial as we navigate the current AI landscape. While AI can provide valuable assistance for mental health support, learning, and personal organization, we must remain aware that these systems are sophisticated pattern matchers rather than conscious entities. The therapeutic value may be real—many users do find comfort and insight through AI interactions—but it stems from the human capacity for self-reflection prompted by the conversation, not from genuine empathy or understanding on the machine’s part.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers observes that LLMs can narrow down search spaces but falter when stuck, relying on intelligent guesswork rather than systematic problem-solving. Unlike humans, who respond to failed attempts with targeted adjustments based on what went wrong, LLMs often make another guess uninformed by previous failures. Humans build solutions incrementally; LLMs tend to skip intermediate steps and jump directly to answers, missing the structured, iterative reasoning that characterizes human problem-solving.\nDespite these limitations in systematic reasoning, deep learning excels through pattern recognition. Consider the classical three-body problem in physics, which has resisted analytical solution for centuries. Deep neural networks have approximated solutions by learning the underlying dynamics from training data, generating accurate orbital trajectory predictions where closed-form solutions remain elusive.\nWhile Deep Learning excels at recognizing patterns, we need a robust framework for reasoning about the uncertainty inherent in these predictions. This brings us to the Bayesian perspective.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#bayes-evidence-as-minus-log-probability",
    "href": "00-intro.html#bayes-evidence-as-minus-log-probability",
    "title": "The Modern AI Playbook",
    "section": "Bayes: Evidence as Minus Log-Probability",
    "text": "Bayes: Evidence as Minus Log-Probability\nImagine you’re searching for something lost—a missing ship, a hidden treasure, or a city abandoned centuries ago. You have multiple clues: historical documents, geological surveys, satellite imagery, and expert opinions. How do you combine all these disparate pieces of evidence into a coherent search strategy? This is exactly the type of problem where Bayesian reasoning shines, and it’s a powerful framework that underlies many modern AI applications.\nThe Bayesian approach provides a principled mathematical framework for updating our beliefs as new evidence arrives. At its core is Bayes’ rule, which tells us how to revise the probability of a hypothesis given new data:\n\\[\nP(\\text{hypothesis} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{hypothesis}) \\times P(\\text{hypothesis})}{P(\\text{data})}\n\\]\nWhile this formula is elegant, what makes Bayesian reasoning especially powerful is a simple mathematical trick: when we work with logarithms of probabilities, combining evidence becomes as simple as addition. Taking the logarithm of both sides of Bayes’ rule gives us:\n\\[\n\\log P(\\text{hypothesis} \\mid \\text{data}) = \\log P(\\text{data} \\mid \\text{hypothesis}) + \\log P(\\text{hypothesis}) - \\log P(\\text{data})\n\\]\nThis transformation reveals that the log-posterior (our updated belief) is simply the sum of the log-likelihood (evidence from data) and the log-prior (our initial belief), minus a normalization constant. In other words, on the log scale, we’re just adding up different sources of evidence. Each piece of information contributes its “weight” to the total, and we combine them linearly.\nThis additive property has profound practical implications. When you have multiple independent sources of evidence—say, historical documents, geological surveys, and geophysical measurements—each contributes a term to the sum. Strong evidence adds a large positive contribution, weak evidence adds little, and contradictory evidence subtracts from the total. The beauty is that the mathematical framework handles all the bookkeeping automatically.\nA remarkable application of this principle comes from the world of mineral exploration. In 2022, Aurania Resources announced that they had found the location of Logroño de los Caballeros, a “lost city” of Spanish gold miners that had been abandoned in the jungles of Ecuador for over 400 years. The discovery was made possible by Bayesian search theory, developed by Larry Stone who has a remarkable track record of finding lost objects—including the USS Scorpion nuclear submarine and Air France Flight 447.\nLarry Stone’s approach to finding Logroño exemplifies how Bayesian reasoning combines multiple sources of evidence. The team assembled a mountain of heterogeneous information:\n\nHistorical documents: Spanish colonial records from the 1580s-1590s describing Logroño’s location relative to rivers and other settlements\nArchaeological evidence: A 1574 map by Mendez showing approximate locations\n\nGeological data: Stream sediment samples analyzed for gold content\nGeophysical surveys: Magnetic and radiometric measurements\nModern geography: LiDAR topographic data and current river systems\nGeochemical patterns: Distribution of minerals indicating potential gold sources\n\nEach of these information sources provided a “clue” that was more or less reliable, more or less precise, and potentially contradictory with others. How do you reconcile a 450-year-old account that “Logroño was half a league from the Rio Zamora” with geological evidence suggesting gold-bearing formations in a different area?\nBayesian search theory provides the answer. Bayesian reasoning assigns each piece of evidence a reliability weight and used Bayes’ rule to generate probability maps. Historical documents considered highly reliable (such as official Spanish reports) contributed strongly to the probability distribution, while more ambiguous sources contributed less. Larry Stone, explained: “Our success in integrating historical documents with scientific data using Bayesian methods opens a range of potential applications in the mineral and energy exploration sectors.”\nThe power of this approach became clear when they combined evidence that initially seemed contradictory. A critical breakthrough came from multiple corroborating accounts: Juan Lopez de Avendaño reported in 1588 that Logroño was half a league from the Rio Zamora; that same year, two soldiers drowned crossing “the river” to fight an uprising; in the mid-1590s, seven soldiers drowned trying to reach a downstream garrison; and a 1684 Jesuit account described an elderly woman who remembered hearing Logroño’s church bells from her village at the mouth of the Rio Zamora. Each piece of evidence individually was ambiguous—which river? how far is “half a league”?—but together they pointed to a specific location along the Rio Santiago valley.\nOn the log-probability scale, each piece of evidence either added to or subtracted from the likelihood of different locations. Strong, consistent evidence (multiple drowning accounts suggesting a major river crossing) added significant weight. Weak or contradictory evidence contributed less. The final probability map was literally the sum of these contributions, with the peak probability occurring where the most evidence converged. Figure 3 shows the likelihood ratio surfaces generated for copper, silver, and gold deposits—visual representations of how different evidence sources combine to create probability distributions across the search area.\n\n\n\n\n\n\nFigure 3: Likelihood ratio surfaces generated by Metron showing potential locations for copper, silver, and gold deposits in Aurania’s concession area. These heat maps visualize how Bayesian analysis combines multiple sources of geological and geophysical evidence into a single probability distribution. Warmer colors indicate higher likelihood ratios where multiple pieces of evidence converge. Source: Metron Inc., via MIT Sloan\n\n\n\nThe result was dramatic: Bayesian reasoning generated probability maps that identified the Rio Santiago valley as the most likely location of Logroño, and subsequent fieldwork confirmed extensive alluvial gold deposits and active artisanal mining exactly where the Bayesian analysis predicted. As Dr. Keith Barron, Aurania’s CEO, noted: “This key discovery can ultimately lead us to Logroño’s gold source.” The location that seemed to reconcile all the disparate evidence—Spanish colonial records, drowning accounts, geological surveys, and modern geography—turned out to be correct.\nThis example illustrates why the Bayesian framework is so powerful in modern AI applications. Machine learning models constantly face the challenge of combining multiple sources of information: pixels in different regions of an image, words in different parts of a sentence, measurements from different sensors. The additive property of log-probabilities provides an efficient computational framework for this fusion. When you train a deep learning model, the loss function essentially measures how well the model combines evidence from the training data with prior knowledge (encoded in the model architecture and regularization). Optimization algorithms adjust model parameters to maximize this combined evidence, updating beliefs exactly as Bayes’ rule prescribes.\nThe mathematical elegance of working with log-probabilities extends beyond search problems. In natural language processing, transformer models compute attention weights that determine how much “evidence” each word provides about the meaning of other words. In computer vision, convolutional networks combine evidence from different receptive fields. In recommendation systems, collaborative filtering combines evidence from multiple users’ preferences. All of these applications benefit from the additive structure that log-probabilities provide.\n\nFrom Beliefs to Decisions: Expected Utility\nBayesian reasoning is not merely about updating beliefs—it is fundamentally about making decisions under uncertainty. The Logroño example illustrates this: the ultimate question was not “what is the probability that gold lies at location X?” but rather “where should we drill?” This is a decision, not an estimation problem.\nThe expected utility framework bridges beliefs and actions. The idea is simple: for each possible action, we weight its value in each scenario by how likely that scenario is, then choose the action with the highest weighted average. In the Logroño case, drilling at a location where multiple evidence sources converge has higher expected value than drilling where only one source points—even if that single source seems compelling.\nThis framework unifies diverse applications: A/B testing asks “which variant should we deploy?” given uncertain conversion rates. Portfolio optimization asks “how should we allocate capital?” given uncertain returns. Medical treatment selection asks “which therapy should we prescribe?” given uncertain patient response. In each case, the decision-maker must specify preferences—the relative costs of false positives versus false negatives, the risk tolerance for financial losses, or the tradeoff between treatment efficacy and side effects.\nDecision theory is integral to the Bayesian approach precisely because probability alone is insufficient for action. Two analysts may agree on the posterior probability of a drug’s effectiveness yet disagree on whether to approve it, depending on how they weigh the costs of denying a beneficial treatment against approving a harmful one. The Bayesian framework makes these tradeoffs explicit and principled.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#statistical-learning-patterns-in-data",
    "href": "00-intro.html#statistical-learning-patterns-in-data",
    "title": "The Modern AI Playbook",
    "section": "Statistical Learning: Patterns in Data",
    "text": "Statistical Learning: Patterns in Data\nWhile Bayesian methods provide the framework for reasoning under uncertainty, statistical learning supplies the algorithmic toolkit for extracting patterns from data. The core problem is simple to state: given examples of inputs \\(x\\) and outputs \\(y\\), find a function \\(f\\) such that \\(y \\approx f(x)\\) for new, unseen inputs.\nRegression is the workhorse of prediction. Linear regression models the relationship as \\(y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p + \\epsilon\\), where the coefficients \\(\\beta\\) quantify how each input variable influences the output. Despite its simplicity, linear regression remains the starting point for most predictive analyses—from housing price prediction to demand forecasting—because its interpretability allows domain experts to understand and trust the model.\nTree-based methods offer an alternative that naturally captures non-linear relationships and interactions. A decision tree recursively partitions the input space, asking questions like “is income &gt; $50K?” and “is age &lt; 30?” to create regions with homogeneous outcomes. Random forests average many such trees trained on bootstrap samples, reducing variance while maintaining flexibility. Gradient boosting sequentially builds trees that correct the errors of previous ones. These ensemble methods dominate Kaggle competitions and production ML systems because they require minimal tuning, handle mixed data types gracefully, and provide variable importance measures that aid interpretation.\nThe tension between model complexity and generalization is formalized by the bias-variance tradeoff. Simple models (high bias) may miss important patterns; complex models (low variance in training but high variance in prediction) may overfit to noise. Cross-validation—holding out data for testing—provides a principled approach to model selection: we choose the model complexity that minimizes prediction error on data the model has not seen during training.\nThese classical statistical learning methods form the foundation for understanding deep learning. Neural networks can be viewed as highly flexible function approximators that automatically learn useful representations of \\(x\\). The same principles—minimizing prediction error, regularizing to prevent overfitting, validating on held-out data—apply across the spectrum from linear regression to billion-parameter language models.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#examples-ai-in-action",
    "href": "00-intro.html#examples-ai-in-action",
    "title": "The Modern AI Playbook",
    "section": "Examples: AI in Action",
    "text": "Examples: AI in Action\nThe following examples demonstrate how deep learning extracts meaningful patterns from complex data—creative synthesis, high-speed sailing optimization, and scientific discovery.\n\nExample 1 (The Next Rembrandt) In 2016, a “new” Rembrandt painting was unveiled in Amsterdam—not discovered in an attic, but generated by algorithms. The project was the brainchild of Bas Korsten, creative director at J. Walter Thompson Amsterdam.\nThe portrait emerged from 18 months of analysis of 346 paintings and 150 gigabytes of digitally rendered graphics. Everything about the painting—from the subject matter (a Caucasian man between 30 and 40) to his clothes (black, wide-brimmed hat, black shirt and white collar), facial hair (small mustache and goatee), and the way his face is positioned (facing right)—was distilled from Rembrandt’s body of work.\n“A computer learned, with artificial intelligence, how to re-create a new Rembrandt right eye,” Korsten explains. “And we did that for all facial features, and after that, we assembled those facial features using the geometrical dimensions that Rembrandt used to use in his own work.”\nA deep learning model learned the statistical distribution of “Rembrandt-ness,” capturing the artist’s style not as a set of rules, but as a probability distribution over pixel arrangements. The resulting image is a hallucination that fits this distribution perfectly—demonstrating Generative AI’s ability to synthesize high-fidelity creative artifacts.\n\n\n\n\n\n\nFigure 4: Can you guess which image was generated by the algorithm?\n\n\n\n\n\nExample 2 (SailGP: Analytics at 50 Knots) SailGP racing catamarans are floating data centers, instrumented with over 1,000 sensors generating 52 billion data points per race. Oracle co-founder Larry Ellison pioneered this fusion of sailing and analytics through his involvement with Oracle Team USA and the America’s Cup. His approach has shaped modern sailing competitions like SailGP.\nThe power of real-time analytics was dramatically demonstrated in the 2013 America’s Cup. Jimmy Spithill and ORACLE TEAM USA faced Emirates Team New Zealand, falling behind 8-1 in the best-of-17 series—a deficit no team had ever recovered from in America’s Cup history. The turning point came after the 8-1 loss when the team made a critical technological decision: they installed additional sensors throughout the boat to collect more comprehensive data about performance, wind conditions, and boat dynamics. These sensors provided real-time feedback that allowed precise adjustments to sailing strategy and boat configuration. With the enhanced data collection system in place, ORACLE TEAM USA won eight consecutive races to claim the America’s Cup 9-8—one of the greatest comebacks in sporting history.\nModern SailGP boats build on this foundation. Key metrics like Velocity Made Good (VMG)—the speed towards the mark—are continuously recalculated based on wind shifts and currents. Tack and gybe optimization uses statistical modeling to determine optimal timing for direction changes. Layline calculations employ predictive analytics to minimize distance sailed. Pressure sensors combined with flow dynamics models calculate optimal hydrofoil position. Teams use Bayesian inference to update their models in real-time as new data arrives during races, creating a dynamic optimization system that continuously refines strategy.\nThe system performs “inverse inference” on the physical environment: observing the boat’s performance updates beliefs about unseen wind patterns, allowing crews to make optimal decisions in split seconds. Success depends as much on the ability to collect, process, and act on real-time data as on traditional sailing skills.\n\n\n\nEmirates GBR SailGP Team\n\n\n\n\nExample 3 (DeepMind’s Alpha Series: From Games to Science) DeepMind’s journey into reinforcement learning began with a focus on mastering games, not for the sake of play, but to test the limits of artificial intelligence. It started with AlphaGo, which stunned the world in 2016 by defeating 18-time world champion Lee Sedol at the ancient game of Go—a feat previously thought to be a decade away. The system’s creative “Move 37” demonstrated an alien form of intuition, suggesting that machines could transcend human imitation. This evolution continued with AlphaZero, which removed the need for human examples entirely. By playing millions of games against itself, it mastered Go, Chess, and Shogi from scratch in mere hours, discovering novel strategies that had eluded human grandmasters for centuries and proving that AI could learn to solve complex problems through pure trial and error.\nThe true power of these systems, however, lies in their ability to step outside the game board and tackle fundamental scientific challenges. AlphaFold applied the same principles to biology, solving the 50-year-old “protein folding problem” by predicting the 3D structures of nearly all known proteins, a breakthrough that is now accelerating drug discovery and our understanding of life itself. Most recently, AlphaProof has ventured into the realm of abstract reasoning, solving complex mathematical problems from the International Mathematical Olympiad. Together, these models demonstrate a profound shift: we are moving from AI that entertains us to AI that expands the frontiers of human knowledge, turning the intuition learned in games into tools for scientific discovery.\n\n\n\n\n\n\nFigure 5: Alpha GO vs Lee Sedol: Move 37 by AlphaGo in Game Two",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#anything-as-a-vector",
    "href": "00-intro.html#anything-as-a-vector",
    "title": "The Modern AI Playbook",
    "section": "Anything as a Vector",
    "text": "Anything as a Vector\nModern AI treats all information—text, images, audio, biological sequences—as vectors: lists of numbers that represent meaning in a high-dimensional space.\n\nTokenization and Embeddings\nIn Natural Language Processing (NLP), text is first broken into tokens (words or sub-words). Each token is then mapped to a vector. Unlike simple ID numbers (e.g., cat=1, dog=2), these learned vectors capture semantic relationships: the vector for “cat” is mathematically closer to “dog” than to “car.”\n\nExample 4 (Semantic Relations) One of the most intriguing aspects of vector representations is their ability to capture semantic relationships through simple arithmetic operations. In natural language processing, this is famously illustrated by analogies such as “king - man + woman = queen,” where the difference between “king” and “man” encodes the concept of royalty, and adding “woman” shifts the meaning to “queen.” This property emerges because the learned vectors for words, phrases, or even entities are organized in such a way that similar relationships are reflected as consistent directions in the high-dimensional space. The same principle applies beyond language: for example, in sports analytics, we might find that the vector for “Ovechkin” (a star hockey player) plus the vector for “Capitals” (his team) minus the vector for “Gretzky” (another legendary player) yields a vector close to “Oilers” (Gretzky’s team), capturing the underlying relationships between players and their teams. \\[\n\\text{Ovechkin + Capitals - Gretzky = Oilers}\n\\]\nThis ability to perform analogical reasoning with vectors is not limited to words or names—it extends to images, audio, and even structured data like chess positions. In computer vision, for instance, the difference between the vector representations of two images might correspond to a specific transformation, such as changing the background or adding an object. In recommendation systems, the vector difference between a user’s preferences and an item’s features can help identify the best match. These semantic relations, encoded as vector arithmetic, enable AI systems to generalize, reason, and make creative associations across domains. The power of this approach lies in its universality: once information is embedded in a vector space, the same mathematical tools can be used to uncover patterns and relationships, regardless of the original data type.\n\n\n\nVectorizing Chess\nThe vectorization concept becomes particularly clear when we examine how chess positions can be represented numerically. A chess board contains 64 squares, each of which can be empty or occupied by one of 12 different piece types (6 pieces \\(\\times\\) 2 colors). We can represent any chess position as a vector by simply listing the contents of each square in order. For instance, we might use the encoding: empty=0, white pawn=1, white rook=2, …, white king=6, black pawn=7, black rook=8, …, black king=12. A chess position would then become a 64-dimensional vector like [8, 9, 10, 11, 12, 10, 9, 8, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, …] representing the starting position with black pieces on the back rank, black pawns on the second rank, and so forth. More sophisticated representations might include additional dimensions for castling rights, en passant possibilities, or whose turn it is to move, creating vectors of 70 or more dimensions. This numerical representation allows chess engines to use the same mathematical operations that work for language or images. The AI can learn that certain vector patterns (piece configurations) are more advantageous than others, and it can mathematically compute how different moves transform one position vector into another. Modern chess engines like AlphaZero process millions of these position vectors to evaluate potential moves, demonstrating how any complex domain can be reduced to vector operations that computers excel at manipulating.\nModern engines like AlphaZero don’t just “read” the board; they map these board-state vectors into a learned strategy space. Moves that lead to victory are clustered together, allowing the AI to “see” tactical patterns structurally, just as an LLM “sees” grammatical relationships.\n\n\nHuman vs. Machine Representations\nWhile modern AI uses a universal vector language, human cognition relies on distinct, specialized systems. Informal experiments by Richard Feynman and John Tukey at Princeton in 1939 illustrate this contrast (Gleick 1992). They challenged each other to count time intervals while performing other tasks.\nThe results revealed a striking asymmetry: Feynman could read silently while counting but failed if he spoke; Tukey could recite poetry while counting but failed if he read. Unknown to them, they had discovered working memory’s separate channels: Feynman counted “auditorily” (conflicting with speech), while Tukey counted “visually” (conflicting with reading).\nThis underscores a fundamental difference: biology evolved specialized channels (auditory vs. visual), whereas AI unifies all data—text, images, audio—into the same vector operations, allowing a single model to become truly multimodal.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#how-machines-learn-the-loss-function",
    "href": "00-intro.html#how-machines-learn-the-loss-function",
    "title": "The Modern AI Playbook",
    "section": "How Machines Learn: The Loss Function",
    "text": "How Machines Learn: The Loss Function\n\n“Let us suppose we have set up a machine with certain initial instruction tables… One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition… It would be like a pupil who had learnt much from his master, but had added much more by his own work.” – Alan Turing (Turing 1950)\n\nIf vectors are the language of AI, then the loss function is its teacher.\nA traditional program follows fixed rules. An AI model begins with random “instruction tables” (weights). It makes a prediction, compares it to the correct answer, and calculates a “loss”—a single number representing the error (e.g., the distance between the predicted pixel and the real pixel). Optimization algorithms (like Gradient Descent) then work backward, slightly adjusting the billions of weights to reduce this loss.\nThis process, repeated billions of times, essentially “compiles” data into software.\n\nExample 5 (Example: Learning Language) Large Language Models (LLMs) master this through a simple objective: predict the next token. 1. Input: “The cat sat on the…” 2. Prediction: The model computes probabilities for every possible next word. 3. Update: If it predicts “car” (low probability for “mat”), the loss is high. The weights are adjusted to make “mat” more likely next time.\nThis simple mechanism—reducing prediction error—forces the model to internalize grammar, facts, and reasoning. To predict accurately, it must understand that “sat” implies an agent, “on” implies a location, and “mat” fits the context. As Turing predicted, the machine alters its own instructions to become a “pupil” that eventually surpasses its initial programming.\nWhy does this produce seemingly abstract knowledge? Because the cheapest way to predict language at scale is to internalize the latent structure that generated it: human conventions of grammar, stable facts about the world, common-sense regularities, and task patterns (definitions, explanations, step-by-step solutions). The network’s continual self-modification—Turing’s pupil—pushes its internal tables toward representations that make these regularities linearly separable and compositionally usable during generation.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "00-intro.html#book-structure",
    "href": "00-intro.html#book-structure",
    "title": "The Modern AI Playbook",
    "section": "Book Structure",
    "text": "Book Structure\nThis book is organized into three parts that build progressively from foundational concepts to cutting-edge applications:\n\nPart I: Bayesian Foundations — Probability as the language of uncertainty, Bayesian inference for updating beliefs, decision theory and utility for choosing actions, and reinforcement learning as sequential decision-making. This part develops the mathematical framework for reasoning under uncertainty that underlies all modern AI.\nPart II: Statistical Learning — Regression methods from linear to regularized, tree-based methods including random forests and gradient boosting, time series and forecasting, and model selection principles. These chapters cover the classical machine learning toolkit that remains essential for production systems.\nPart III: Deep Learning — Neural network fundamentals, convolutional networks for vision, sequence models and attention mechanisms, large language models, and AI agents. This part explores the architectures powering today’s most capable systems.\n\nThis unified approach brings together ideas from probability and statistics, optimization, scalable linear algebra, and high-performance computing. While deep learning initially excelled in image analysis and natural language processing, it has since expanded into diverse fields—from drug discovery to algorithmic trading—becoming a core tool for modern data scientists. Modern methodologies, software, and cloud computing make these techniques accessible to practitioners who previously relied on traditional models like generalized linear regression or tree-based methods. The ability to learn complex patterns and generate accurate predictions makes this an exciting methodology, and we hope to convey that excitement.\n\nCourse Plans\nThis book is targeted towards students who have completed introductory statistics and high school calculus. Basics of probability, statistics, and linear algebra are revisited as needed. We make extensive use of computational tools: R for statistical modeling and probability chapters, as well as PyTorch and JAX for deep learning. The material can be adapted for different audiences and time constraints:\nIntroduction to AI for MBAs (One Semester)\nChapters (theoretical sections skipped): Probability (1), Bayesian Inference (2), Decision Theory (4), A/B Testing (5), Pattern Recognition (11), Regression (12), Logistic Regression (13), Trees (14), Model Selection (16), Neural Networks up to backpropagation (18), Natural Language Processing (23).\nLearning Outcomes: Evaluate AI claims critically and identify hype versus substance. Understand when machine learning adds value versus traditional analytics. Communicate effectively with data science teams about requirements and limitations. Assess and quantify uncertainty in business decisions. Make informed build-versus-buy decisions for AI capabilities.\nFoundations of AI for Engineering MS (One Semester)\nChapters (theory sections optional): Probability (1), Bayesian Inference (2), Bayesian Learning (3), Decision Theory (4), A/B Testing (5), Stochastic Processes up to Markov chains (7), Reinforcement Learning (9) (brifly inroduce concepts of sequential decision making), Pattern Recognition (11), Regression (12), Logistic Regression (13), Trees (14), Model Selection (16), Neural Networks (18), SGD and Optimization (20)\nLearning Outcomes: Implement machine learning models from scratch using PyTorch and JAX. Derive learning algorithms mathematically and understand their convergence properties. Design AI systems for production deployment including testing and monitoring. Quantify, propagate, and communicate uncertainty throughout the modeling pipeline. Evaluate and extend state-of-the-art methods from research literature.\n\n\n\n\nGleick, James. 1992. Genius: The Life and Science of Richard Feynman. New York: Pantheon Books.\n\n\nKeynes, John Maynard. 1930. “Economic Possibilities for Our Grandchildren.” In Essays in Persuasion, 358–73. W. W. Norton & Company.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nick, Vadim Sokolov, and Jianeng Xu. 2023. “Quantum Bayesian Computation.” Applied Stochastic Models in Business and Industry 39 (6): 869–83.\n\n\nTuring, A. M. 1950. “Computing Machinery and Intelligence.” Mind; a Quarterly Review of Psychology and Philosophy 59 (236): 433–60.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nVeblen, Thorstein. 1899. The Theory of the Leisure Class: An Economic Study of Institutions. New York: Macmillan.\n\n\n———. 1921. The Engineers and the Price System. New York: B. W. Huebsch.\n\n\nWiener, Norbert. 1950. The Human Use of Human Beings: Cybernetics and Society. Boston: Houghton Mifflin.",
    "crumbs": [
      "The Modern AI Playbook"
    ]
  },
  {
    "objectID": "01-prob.html",
    "href": "01-prob.html",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "1.1 Odds as Probabilities\nProbability deals with randomness and provides a language to communicate uncertainty, which is usually associated with our lack of knowledge or information. In the classical coin toss, for instance, if we knew the exact force applied, we could predict the outcome with certainty. However, practically it is never the case and we treat coin toss outcome as random.\nAssigning probabilities to events is a challenging problem. Often, the probability will be applied to analyze results of experiments (observed data). Consider the coin-tossing example. Say event \\(A\\) represents a Head. Then, to empirically estimate probability or event \\(A\\), \\(P(A)\\), we can repeat the tosses experiment \\(N\\) times and count \\(n\\), the number of times \\(A\\) occurred. The plot below shows the proportion of heads after \\(N\\) trials.\nWe can see that as \\(N\\) grows, the curve converges to \\(0.5\\). This is the law of large numbers. When \\(N\\) is large, \\(n/N\\) will be close to \\(P(A)\\). The probability as a limit definition is natural and was proposed by von Mises. \\[\nP(\\text{Heads}) = \\lim_{N \\to \\infty} \\frac{n}{N}.\n\\]\nHowever, this definition is not operational. It requires the notion of a collective, an infinite sequence of reputable trials with random outcomes. This is an untestable assumption and was criticized by Ville (1939). On the other hand, Kolmogorov tried to operationalize probability by proposing tests for randomness in his work on algorithmic complexity theory.\nWe can use a relaxed definition due to Bernoulli, and define probability as simply the ratio of the number of heads to the total number of trials in a given experiment. This definition is operational and can be used to estimate the probability of an event. This definition requires the experiment to be repeated under identical conditions. If we are to repeat this experiment under different conditions, e.g. when an unbalanced coin is used, our estimate of \\(P(A)\\) will change as well.\nAn alternative operational definition of probability was proposed by Frank Ramsey (1926) and later refined by Bruno de Finetti (1937). Rather than relying on long-run frequencies, this approach defines probability through the lens of rational betting behavior. The key insight is that your probability assignment for an event should correspond to the odds at which you would be willing to bet on that event.\nde Finetti and Ramsey school of thought takes probability as subjective, namely personal to the observer. De Finetti famously concluded that “Probability does not exist.” Measuring uncertainty is personal to the observer. It’s not like mass which is a property of an object. If two different observers have differing “news” then there is an opportunity for them to bet (exchange contracts). Thus leading to an assessment of probability.\nFor many events most people will agree on their probabilities, for example \\(P(\\text{Heads}) = 0.5\\). In the subjective view of probability, we can measure or elicit a personal probability as a “willingness to play”. Namely, will you be willing to bet $1 so you can get $2 if the coin lands Tail and $0 if Head occurs? The subjective view of probability also leads to subjective expected utility theory. You cannot separate the two. For more details, see Chapter 4.\nThis book primarily adopts the Bayesian (subjective) interpretation of probability, formalized through coherence (the Dutch book principle here, and de Finetti’s representation in Chapter 3). We still use frequentist language and long-run intuition as operational tools, but the organizing viewpoint is probabilistic modeling and belief updating.\nSuppose you believe the probability of an event \\(A\\) is \\(p\\). According to Ramsey’s definition, this means you should be indifferent between paying \\(p \\cdot S\\) dollars to receive \\(S\\) dollars if event \\(A\\) occurs (and nothing otherwise) and accepting \\(p \\cdot S\\) dollars to pay someone \\(S\\) dollars if event \\(A\\) occurs. However, not all probability assignments lead to rational behavior. Suppose someone assigns probabilities to events in an incoherent way. In that case, a clever adversary could construct a series of bets, called a Dutch book, where the individual is guaranteed to lose money regardless of which events occur. The requirement that probabilities must be assigned in such a way that no Dutch book can be constructed against you is known as coherence. For a detailed derivation of probability rules using Dutch Book arguments, see the Appendix.\nThe principle of coherence for subjective probability is the fundamental rationality constraint, in finance that would be called no-arbitrage condition Bachelier (1900).\nLet us consider a simple example. Suppose you assign probability \\(P(A) = 0.7\\) to event \\(A\\) occurring and probability \\(P(\\bar A) = 0.2\\) to event \\(A\\) not occurring, where \\(\\bar A\\) denotes the complement of \\(A\\). A Dutch book can be constructed as follows:\nYour total payment is \\(\\$0.90\\), but you will receive exactly \\(\\$1\\) regardless of whether \\(A\\) or \\(\\bar A\\) occurs, for a guaranteed loss of \\(\\$0.10\\). The incoherence arises because \\(P(A) + P(\\bar A) = 0.9 \\neq 1\\).\nMore generally, coherence implies that probabilities must satisfy the following basic properties:\nTo see why non-negativity must hold, suppose \\(P(A) = -0.1\\) for some event \\(A\\). According to the betting interpretation, you would receive \\(\\$0.10\\) and then pay \\(\\$1\\) if \\(A\\) occurs. If \\(A\\) does not occur, you keep the \\(\\$0.10\\) but receive nothing. However, this means you are offering to pay someone to take a bet against \\(A\\)—clearly irrational behavior. While standard probability theory requires non-negativity, extensions involving negative probabilities have been explored in fields like physics and quantum computing, as well as in Bayesian modeling as mixing distributions for unobserved latent variables (Polson and Sokolov 2025).\nThe normalization requirement \\(P(\\Omega) = 1\\) ensures that you assign probability one to something that is certain to happen. If \\(P(\\Omega) &lt; 1\\), you would be willing to pay less than \\(\\$1\\) to receive \\(\\$1\\) with certainty, allowing an arbitrageur to make a riskless profit. Conversely, if \\(P(\\Omega) &gt; 1\\), you would pay more than \\(\\$1\\) for a certain payoff of \\(\\$1\\), guaranteeing a loss.\nThe additivity axiom ensures consistency across mutually exclusive events. If \\(A\\) and \\(B\\) cannot both occur, then betting on “\\(A\\) or \\(B\\)” should cost the same as placing separate bets on \\(A\\) and on \\(B\\). Violating this principle again opens the door to Dutch books.\nAnother axiom \\(P(A) + P(\\bar A) = 1\\) just follows from the normalization requirement and the additivity axiom.\nThese are precisely the axioms proposed by Andrey Kolmogorov (1933) in his foundational work on probability theory. They provide mathematical structure to probability. Although, Kolmogorov’s axioms are agnostic to any definition of probability and are purely mathematical in nature, the fact that they can be derived from the Dutch book argument shows their applicability to rational decision-making under uncertainty. Any violation of these axioms opens you to guaranteed losses through carefully constructed bets. However, Kolmogorov’s axiomatic approach helps us to derive results in more complex settings. For examples, Kolmogorov’s framework is applicable in infinite sample spaces and continuous random variables.\nThe axioms provide a number of rules that probabilities must follow. There are several important corollaries that can help us assign probabilities to events. Here are some important corollaries that follow from the Kolmogorov axioms:\nAll of these axioms follow simply from the principle of coherence and the avoidance of Dutch book arbitrage. This includes the Bayes rule itself (de Finetti 1937; Shimony 1955). If there is arbitrage present in the market, it should be “traded”. It often happens when subjective probabilities do not match.\nBayes rule is a fundamental rule of probability that allows us to calculate conditional probabilities. It is a direct consequence of the definition of conditional probability and the normalization axiom. This rule will become central to learning and inference in artificial intelligence.\nBayes rule simply provides a disciplined probability accounting of how probabilities get updated in light of evidence. A rational agent requires that their subjective probabilities must obey the principle of coherence. Namely in announcing the set of probabilities they cannot undergo a sure loss. Interestingly enough, this is enough to provide a similar framework to the axiomatic approach of Kolmogorov.\nThese corollaries and principles help in deriving further results and provide additional tools for analyzing and understanding probability and random processes based on the fundamental principles laid out by Kolmogorov. Arguably the most important rule is Bayes rule for conditional probability.\nThe rise of artificial intelligence has definitively established Bayesian inference as a cornerstone of modern learning algorithms. One of the key properties of probabilities is that they are updated as you learn new information. Conditional means given its personal characteristics or the personal situation. Personalization algorithms used by many online services rely on this concept. One can argue that all probabilities are conditional in some way. The process of Bayesian updating is central to how machines learn from observed data. Rational human behavior ought to adhere to Bayes rule, although there is much literature documenting the contrary.\nAnother way, sometimes more convenient, to talk about uncertainty and to express probabilities via odds, such as 9 to 2 or 3 to 1. Odds express the ratio of favorable to unfavorable outcomes (Success:Failure), while probability is the chance of an event happening out of all possibilities (Success / Total).\nWe assign odds “on \\(A\\)” (or “in favor of \\(A\\)”) versus odds “against \\(A\\)”. For example, if the probability of a Chicago Bears’ Super Bowl win is \\(P(A) = 2/11\\), the odds against them are \\((1 - 2/11) / (2/11) = 9/2\\), or “9 to 2”. This means for every 2 times they win, they lose 9 times. Conventionally, “odds on” often refers to the reverse, but to avoid ambiguity, we will speak of probability or odds against. \\[\nO(A) = \\dfrac{P(\\bar A) }{P(A)} = \\dfrac{1-P(A)}{P(A)}\n\\] Equivalently, probabilities can be determined from odds \\[\nP(A) = \\dfrac{1}{1+O(A)}\n\\] For example, if the odds are one \\(O(A) = 1\\), then for every $1 bet you will pay out $1. This event has probability \\(0.5\\).\nIf \\(O(A) = 2\\), then you are willing to offer \\(2:1\\). For a $1 bet you’ll payback $3. In terms of probability \\(P(A) = 1/3\\).\nOdds are primarily used in betting markets. For example, let’s re-analyze the 2016 election in the US.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#odds-as-probabilities",
    "href": "01-prob.html#odds-as-probabilities",
    "title": "1  Probability and Uncertainty",
    "section": "",
    "text": "Example 1.1 (Odds) One of the main sources of prediction markets is bookmakers who take bets on outcomes of events (mostly sporting) at agreed upon odds. Figure 1.1 shows the odds used by several bookmakers to take bets on the winner of the US presidential election in 2016. At that time the market was predicting that Hillary Clinton would win over Donald Trump, the second favorite, with odds 7/3. The table is generated by the Oddschecker website.\n\n\n\n\n\n\nFigure 1.1: Presidential Odds 2016\n\n\n\nAhead of time we can assign probabilities of winning to each candidate. According to the bookmakers’ odds the candidate with highest chance to win is Hillary Clinton. The best odds on Clinton are \\(1/3\\); this means that you have to risk $3 to win $1 offered by Matchbook. Odds dynamically change as new information arrives. There is also competition between the Bookmakers and the Market is adapting to provide the best possible odds. Ladbrokes is the largest UK bookie and Betfair is an online exchange. A bookmaker sets their odds trying to get equal public action on both sides, otherwise they are risking to stay out of business.\n\n\nExample 1.2 (Kentucky Derby) The Kentucky Derby happens once a year – first Saturday in May. In horse racing the odds are set by the betting public. The racetrack collects all the bets, takes a fee (18%), and then redistributes the pool to the winning tickets. The race is \\(1 \\frac{1}{4}\\) miles (2 kilometers) and is the first time the three-year old horses have raced the distance.\nThere was a long period where favorites rarely won. Only six favorites have won in the 36 year period from 1979 to 2013. Recently favorites have won many times in a row. The market is getting better at predicting who’s going to win. Here’s the data\n\n\n\nHorse Name\nYear\nOdds\n\n\n\n\nSpectacular Bid\n1979\n3/5\n\n\nFusaichi Pegasus\n2000\n2.3/1\n\n\nStreet Sense\n2007\n9/2\n\n\nBig Brown\n2008\n5/2\n\n\n\nRecently, favorites have had a lot more success\n\n\n\nHorse Name\nYear\nOdds\n\n\n\n\nCalifornia Chrome\n2014\n5/2\n\n\nAmerican Pharoah\n2015\n2/1\n\n\nNyquist\n2016\n3.3/1\n\n\nAlways Dreaming\n2017\n5.2/1\n\n\n\nThe most famous favorite to win is Secretariat (1973) who won with odds 3/2 in a record time of 1 minute 59 and 2/5 seconds. Monarchos was the only other horse that in 2001 has broken two minutes at odds 11.5/1.\n\n\nExample 1.3 (Exacta Betting and the Harville Formula) How can probability help you with betting on the race? There are many different types of bets, and probability can help you find fair odds. The Derby is a Grade 1 stakes race for three-year-old thoroughbred horses. Colts and geldings carry 126 pounds and fillies 121. The odds are set by pari-mutuel betting by the public. After all the wagers have been placed, the racetrack takes a fee (18%). After the winning horse passes the finishing line, the pool of money is redistributed to the winning tickets. Random betting therefore loses you 18%, so it’s important to learn some empirical facts to try and tilt the odds in your favor.\nFor example, you can place bets as follows:\n\nWin: “$2 win horse 1”\nStraight Exacta: “$2 exacta 1 with 2”\n\nExacta Box: “$2 exacta box 1 and 2” You win with either order: 2 bets = $4.\n\nConsider a hypothetical race where Sovereignty wins at 9/1 odds and Journalism comes second at 7/2 odds. For a $2 bet on Sovereignty to Win at 9/1, the payout would be \\(2 \\cdot 9 + 2 = \\$20\\) (the 9/1 win plus your initial $2 bet returned).\nLet’s figure out the fair value for an exacta bet given that you know the win odds. This is known as the Harville formula. The exacta is probably one of the most popular bets for many horseplayers, corresponding to predicting the first two horses in the correct order.\nThe Harville formula provides an answer. We use the rule of conditional probability. The probability for the straight exacta of horses \\(A\\) beating horse \\(B\\) is: \\[\nP(A \\text{ beats } B) = P(A \\text{ Wins}) \\cdot P(B \\text{ Second} \\mid A \\text{ Wins})\n\\]\nA reasonable assessment of \\(P(B \\text{ Second} \\mid A \\text{ Wins})\\) can be derived as follows. Renormalizing the probabilities by removing the winner \\(A\\) and distributing the probability mass to the remaining horses gives: \\[\nP(B \\text{ Second} \\mid A \\text{ Wins}) = \\frac{P(B \\text{ Wins})}{1 - P(A \\text{ Wins})}\n\\]\nIn total, the fair price for the exacta is: \\[\nP(A \\text{ beats } B) = P(A \\text{ Wins}) \\cdot \\frac{P(B \\text{ Wins})}{1 - P(A \\text{ Wins})}\n\\]\nTherefore, we have: \\[\np_{12} = p_1 \\cdot \\frac{p_2}{1-p_1} \\text{ where } p_1 = \\frac{1}{1+O_1}, p_2 = \\frac{1}{1+O_2}\n\\]\nSolving for odds, we get the Harville formula: \\[\nO_{12} = O_1(1 + O_2) - 1\n\\]\nUsing our example with 9/1 and 7/2 odds: \\(O_{12} = 9 \\cdot (1 + 3.5) - 1 = 39.5/1\\).\nNotice that the actual payout is determined solely by the volume of money wagered on that combination. There’s no requirement it matches our probabilistic analysis. However, the Harville formula gives us an idea of fair value. Some bettors searching for value try to find significantly undervalued exacta bets relative to the Harville formula.\nThere are many other factors to consider: jockey performance, bloodlines, and post positions can all matter significantly in determining the actual race outcome.\n\n\nExample 1.4 (Boy-Girl Paradox) If a woman has two children and one is a girl, the chance that the other child is also female has to be \\(50-50\\), right? But it’s not. Let’s list the possibilities of girl-girl, girl-boy and boy-girl. So the chance that both children are girls is 33 percent. Once we are told that one child is female, this extra information constrains the odds. (Even weirder, the author demonstrates that the odds change again if we’re told that one of the girls is named Florida.) In terms of conditional probability, the four possible combinations are \\[\nBB \\; \\; BG \\; \\; GB \\; \\; GG\n\\] Conditional on the information that one is a girl means that you know we can’t have the \\(BB\\) scenario. Hence we are left with three possibilities \\[\nBG \\; \\; GB \\; \\; GG\n\\] In one of these is the other a girl. Hence \\(1/3\\).\nIt’s a different question if we say that the first child is a girl. Then the probability that the other is a girl is \\(1/2\\) as there are two possibilities \\[\nGB \\; \\; GG\n\\] This leads to the probability of \\(1/2\\).\n\n\nExample 1.5 (Galton Paradox) You flip three fair coins. What is the \\(P(\\text{all} \\; \\text{alike})\\)?\nAssuming a fair coin (i.e. \\(P(H) = P(T) = 1/2\\)), a formal approach might consist of computing the probability for all heads or all tails, which is\n\\[\\begin{align*}\nP(HHH) &\\equiv P(H \\text{ and } H \\text{ and } H) \\\\\n&= P(H)\\times P(H)\\times P(H) \\\\\n&= \\left(\\frac{1}{2}\\right)^3\n\\end{align*}\\] and, since we’re ultimately interested in the probability of either (mutually exclusive) case, \\[\\begin{align*}\nP(\\text{all alike}) &= P(HHH \\text{ or } TTT) \\\\\n&= P(HHH) + P(TTT) \\\\\n&= 2 \\times \\frac{1}{8}\n\\end{align*}\\]\nOne could arrive at the same conclusion by enumerating the entire sample space and counting the events. Now, what about a simpler argument like the following. In a run of three coin flips, two coins will always share the same result, so the probability that the “remaining/last” coin matches the other two is 1/2; thus, \\[\nP(\\text{all alike}) = 1/2\n\\] There are 8 equally likely outcomes. Two are ‘all alike’ (HHH, TTT). So 2/8 = 1/4. The error in reasoning is assuming that ‘two must correspond’ fixes the first two coins, but ‘two alike’ could be coins 1&2, 2&3, or 1&3.\nFor a real treatment of the subject, we highly recommend reading Galton’s essay at galton.org.\n\n\nExample 1.6 (Three Cards) Suppose that you have three cards: one red/red, one red/blue and one blue/blue. You randomly draw a card and place it face down on a table and then you reveal the top side. You see that it’s red. What’s the probability the other side is red? \\(1/2\\)? No, it’s \\(2/3\\)! By a similar logic there are six initial possibilities \\[\nB_1 B_2 \\; \\; B_2 B_1 \\; \\; B R \\; \\; R B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] where \\(1\\) and \\(2\\) index the sides of the same colored cards.\nIf we now condition on the top side being red we see that there are still three possibilities left \\[\nR B \\; \\; R_1 R_2 \\; \\; R_2 R_1\n\\] Hence the probability is \\(2/3\\) and not the intuitive \\(1/2\\).\n\n\nExample 1.7 (NFL: New England Patriots Coin Toss) Let’s consider another example and calculate the probability of winning 19 coin tosses out of 25. The NFL team New England Patriots won 19 out of 25 coin tosses in the 2014-15 season. What is the probability of this happening?\nLet \\(X\\) be a random variable equal to \\(1\\) if the Patriots win and \\(0\\) otherwise. It’s reasonable to assume \\(P(X = 1) = \\frac{1}{2}\\). The probability of observing the sequence in which there is 1 on the first 19 positions and 0 afterwards is \\((1/2)^{25}\\). We can code a typical sequence as, \\[\n1,1,1,\\ldots,1,0,0,\\ldots,0.\n\\] There are \\(177,100\\) different sequences of 25 games where the Patriots win 19. There are \\(25! = 1\\cdot 2\\cdot \\ldots \\cdot 25\\) ways to re-arrange this sequence of zeroes and ones. Further, all zeroes and ones are interchangeable and there are \\(19!\\) ways to re-arrange the ones and \\(6!\\) ways to rearrange the sequence of zeroes. Thus, the total number of different winning sequences is\n\nfactorial(25)/(factorial(19)*factorial(25-19))\n## 177100\n\nEach potential sequence has probability \\(0.5^{25}\\), thus \\[\nP\\left(\\text{Patriots win 19 out of 25 tosses}\\right) =  177,100 \\times 0.5^{25} = 0.005\n\\]\nOften, it is easier to communicate uncertainties in the form of odds. In terms of betting odds of \\(1:1\\) gives \\(P = \\frac{1}{2}\\), odds of \\(2:1\\) (I give \\(2\\) for each \\(1\\) you bet) is \\(P = \\frac{1}{3}\\).\nRemember, odds, \\(O(A)\\), is the ratio of the probability of happening over not happening, \\[\nO(A) = (1 - P(A))/P(A),\n\\] equivalently, \\[\nP(A) = \\frac{1}{1 + O(A)}.\n\\]\nThe odds of the Patriots winning sequence are then 1 to 199\n\n0.005/(1-0.005)\n## 0.005\n\n\n\nExample 1.8 (Hitting Streak) Pete Rose of the Cincinnati Reds set a National League record of hitting safely in \\(44\\) consecutive games. How likely is such a long sequence of safe hits to be observed? If you were a bookmaker, what odds would you offer on such an event? This means that he safely reached first base after hitting the ball into fair territory, without the benefit of an error or a fielder’s choice at least once in every one of those 44 games. Here are a couple of facts we know about him:\n\nRose was a \\(300\\) hitter, he hits safely 3 times out of 10 attempts\nEach at bat is assumed to be independent, i.e., the current at bat doesn’t affect the outcome of the next.\n\nAssuming he comes to bat \\(4\\) times each game, what probability might reasonably be associated with that hitting streak? First we define notation. We use \\(A_i\\) to denote an event of hitting safely at game \\(i\\), then \\[\n\\begin{aligned}\n& P( \\mathrm{Rose \\; Hits \\; Safely \\; in \\;44 \\; consecutive \\; games} ) = \\\\\n& P ( A_1 \\; \\text{and} \\;  A_2  \\ldots \\text{and} \\;  A_{44} ) = P ( A_1 ) P ( A_2 ) \\ldots P ( A_{44} )\n\\end{aligned}\n\\] We now need to find \\(P(A_i)\\)s where \\(P(A_i) = 1 - P(\\text{not} \\; A_i)\\) \\[\\begin{align*}\nP ( A_1 ) & = 1 - P ( \\mathrm{ not} \\; A_1 ) \\\\\n& = 1 - P ( \\mathrm{ Rose \\; makes \\; 4 \\; outs } ) \\\\\n& = 1 - ( 0.7)^4 = 0.76\n\\end{align*}\\] For the winning streak, then we have \\((0.76)^{44} = 0.0000057\\), a very low probability. In terms of odds, there are three basic inferences\n\nThis means that the odds for a particular player as good as Pete Rose starting a hitting streak today are 175,470 to 1.\nThis doesn’t mean that the run of \\(44\\) won’t be beaten by some player at some time: the Law of Very Large Numbers\nJoe DiMaggio’s record is 56. He is a 325 hitter, thus we have \\((0.792)^{56} = 2.13 \\times 10^{-6}\\) or 455,962 to 1. It’s going to be hard to beat.\n\nThe independence assumption underlying this calculation does not account for the popular belief in the “hot hand”—the idea that a player who has been successful recently is more likely to succeed again.\n\n\nExample 1.9 (Derek Jeter) Sample averages can have paradoxical behavior. This is related to the field of causation and the property of confounding. Let’s compare Derek Jeter and David Justice batting averages. In both 1995 and 1996, Justice had a higher batting average than Jeter did. However, when you combine the two seasons, Jeter shows a higher batting average than Justice! This is just a property of averages and finer subset selection can change your average effects.\n\n\n\n\n1995\n\n1996\n\nCombined\n\n\n\n\n\nDerek Jeter\n12/48\n0.250\n183/582\n0.314\n195/630\n0.310\n\n\nDavid Justice\n104/411\n0.253\n45/140\n0.321\n149/551\n0.270\n\n\n\nThis situation is known as confounding. It occurs when two separate and different populations are aggregated to give misleading conclusions. The example shows that if \\(A,B,C\\) are events it is possible to have the three inequalities \\[\\begin{align*}\n&P( A \\mid B \\text{ and } C ) &gt; P( A \\mid B \\text{ and } \\bar C )\\\\\n&P( A \\mid \\bar  B \\text{ and } C ) &gt; P( A \\mid \\bar  B \\text{ and } \\bar  C )\\\\\n&P( A \\mid C ) &lt; P( A \\mid \\bar C )\n\\end{align*}\\] The three inequalities can’t hold simultaneously when \\(P(B\\mid C) = P(B\\mid \\bar  C)\\).\n\n\nExample 1.10 (Birthday Problem) The birthday problem (Diaconis and and Mosteller 1989) is a classic problem in probability theory that explores the counterintuitive likelihood of shared birthdays within a group. Surprisingly, in a room of 23 people, the probability of shared birthdays is 50%. With 70 people, the probability is 99.9%.\nIn general, given \\(N\\) items (people) randomly distributed into \\(c\\) categories (birthdays), where the number of items is small compared to the number of categories \\(N \\ll c\\), the probability of no match is given by \\[\nP(\\text{no match}) \\approx \\exp\\left(-N^2/2c\\right).\n\\] Given \\(A_i\\) is the event that person \\(i\\) has a matching birthday with someone, we have \\[\nP(\\text{no match})  = \\prod_{i=1}^{N-1}(1-P(A_i)) = \\exp\\left(\\sum_{i=1}^{N-1}\\log (1-P(A_i))\\right).\n\\] Here \\(P(A_i) =\\dfrac{i}{c}\\) Then use the approximation \\(\\log(1-x) \\approx -x\\) for small \\(x\\) to get \\(P(\\text{no match})\\). \\[\n\\sum_{i=1}^{N-1}\\log (1-P(A_i)) \\approx -\\sum_{i=1}^{N-1}\\dfrac{i}{c} = -\\dfrac{N(N-1)}{2c}.\n\\]\nThe probability of at least two people sharing a birthday is then the complement of the probability above: \\[\nP(\\text{At least one shared birthday}) = 1 - P(\\text{no match}).\n\\] Solving for \\(P(\\text{match})=1/2\\), leads to a square root law \\(N=1.2\\sqrt{c}\\), if \\(c=365\\) then \\(N=23\\), and if \\(c=121\\) (near birthday match), then \\(N=13\\).\nThe unintuitive nature of this result is a consequence of the fact that there are many potential pairs of people in the group, and the probability of at least one pair sharing a birthday increases quickly as more people are added. The birthday problem is often used to illustrate concepts in probability, combinatorics, and statistical reasoning. It’s a great example of how our intuitions about probabilities can be quite different from the actual mathematical probabilities.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#random-variables-quantityung-uncertainty",
    "href": "01-prob.html#random-variables-quantityung-uncertainty",
    "title": "1  Probability and Uncertainty",
    "section": "1.2 Random Variables: Quantityung Uncertainty",
    "text": "1.2 Random Variables: Quantityung Uncertainty\nA random variable is a function that maps the outcomes of a random experiment (events) to real numbers. It essentially assigns a numerical value to each outcome in the sample space of a random experiment. In other words, a random variable provides a bridge between the abstract concept of events in a sample space and the concrete calculations involving numerical values and probabilities. Similar to assigning probabilities to events, we can assign respective probabilities to random variables.\nFor example, consider a random experiment of rolling a die. Here, an event could be “the outcome is an even number”, and the random variable could be the actual number that shows up on the die. The probability of the event “the outcome is an even number” is 0.5, and the probability distribution of the random variable is a list of all numbers from 1 to 6 each with a probability of 1/6.\nWhile events and random variables are distinct concepts, they are closely related through the framework of probability theory, with random variables serving as a key tool for calculating and working with probabilities of events.\nRandom variables are quantities that we are not certain about. A random variable that can take a finite or a countable number of values is called a discrete random variable (number of rainy days next week). Otherwise, it will be a continuous random variable (amount of rain tomorrow).\nDiscrete random variables are often constructed by assigning specific values to events such as \\(\\{X=x\\}\\) which corresponds to the outcomes where \\(X\\) equals a specific number \\(x\\). For example\n\nWill a user click-through on a Google ad? (0 or 1)\nWho will win the 2024 elections? (Trump=1, Biden=2, Independent=3)\n\nTo fix notation, we will use \\(P(X=x)\\) to denote the probability that random variable \\(X\\) is equal to \\(x\\). A map from all possible values \\(x\\) of a discrete random variable \\(X\\) to probabilities is called a probability mass function \\(p(x)\\). We will interchangeably use \\(P(X=x)\\) and \\(p(x)\\). An important property of the probability mass function is that (normalization Kolmogorov axiom) \\[\n\\sum_{x\\in S} p(x) = 1.\n\\] Here \\(S\\) denotes the set of all possible values of random variable \\(X\\).\nClearly, all probabilities have to be greater than or equal to zero, so that \\(p(x)\\ge 0\\).\nOften, we are interested in \\[\nF(x) = P(X\\le x) = \\sum_{y\\le x} p(y),\n\\] this is the cumulative distribution function (CDF).\nThe CDF is a monotonically increasing function (never decreases as \\(x\\) increases). In other words, if \\(a \\leq b\\), then \\(F_X(a) \\leq F_X(b)\\). The value of the CDF always lies between 0 and 1, inclusive.\n\nExample 1.11 (Discrete CDF) Suppose \\(X\\) is a discrete random variable that represents the outcome of rolling a six-sided die. The probability mass function (PMF) of \\(X\\) is:\n\\[\nP(X = x) = \\frac{1}{6}\n\\] for \\(x = 1, 2, 3, 4, 5, 6\\)\nThe CDF of \\(X\\), \\(F(x)\\), is calculated as follows:\n\nFor \\(x &lt; 1\\), \\(F(x) = 0\\) (since it’s impossible to roll less than 1).\nFor \\(1 \\leq x &lt; 2\\), \\(F(x) = \\frac{1}{6}\\) (the probability of rolling a 1).\nFor \\(2 \\leq x &lt; 3\\), \\(F(x) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6}\\) (the probability of rolling a 1 or 2).\nThis pattern continues, adding \\(\\frac{1}{6}\\) for each integer interval up to 6.\nFor \\(x \\geq 6\\), \\(F(x) = 1\\) (since it’s certain to roll a number 6 or less).\n\nGraphically, the CDF of a discrete random variable is a step function that increases at the value of each possible outcome. It’s flat between these outcomes because a discrete random variable can only take specific, distinct values.\n\nplot(ecdf(1:6), main=\"\")\n\n\n\n\nCDF of a discrete random variable\n\n\n\n\n\n\nContinuous Random Variables\nIf we want to build a probabilistic model of a stock price or return, we need to use a continuous random variable that can take an interval of values. Instead of a frequency function we will use a density function, \\(p(x)\\) to describe a continuous variable. Unlike the discrete case, \\(p(x)\\) is not the probability that the random variable takes value \\(x\\). Rather, we need to talk about the value being inside an interval. For example, the probability of \\(X\\) with density \\(p(x)\\) being inside any interval \\([a,b]\\), with \\(a&lt;b\\) is given by \\[\nP(a &lt; X &lt; b) = \\int_{a}^{b}p(x)dx.\n\\] The total probability is one as \\(\\int_{-\\infty}^\\infty p(x) dx=1\\). The simplest continuous random variable is the uniform. A uniform distribution describes a variable which takes on any value as likely as any other. For example, if you are asked about what would be the temperature in Chicago on July 4 of next year, you might say anywhere between 20 and 30 C. The density function of the corresponding uniform distribution is then \\[\n  p(x) = \\begin{cases} 1/10, ~~~20 \\le x \\le 30\\\\0, ~~~\\mbox{otherwise}\\end{cases}\n\\]\nUnder this model, the probability of temperature being between 25 and 27 degrees is \\[\nP(25 \\le x \\le 27) = \\int_{25}^{27} p(x)dx = (27-25)/10 = 0.2\n\\]\n\n\n\nUniform Distribution: Probability of temperature being between 25 and 27\n\n\nThe Cumulative Distribution Function for a continuous random variable, it is defined similarly to discrete RV CDF as \\[\nF(x) = P(X \\leq x) = \\int_{-\\infty}^x p(t)dt\n\\] It is a non-decreasing function and takes values in [0,1].\n\nExample 1.12 (Continuous CDF for Uniform Distribution) \\[\np(x) = \\begin{cases}\n1 & \\text{if } 0 \\leq x \\leq 1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe CDF, \\(F(x)\\), is obtained by integrating the PDF:\n\nFor \\(x &lt; 0\\), \\(F(x) = 0\\).\nFor \\(0 \\leq x \\leq 1\\), \\(F(x) = \\int_0^x 1 \\, dt = x\\).\nFor \\(x &gt; 1\\), \\(F(x) = 1\\).\n\nSo, the CDF of this uniform distribution is a linear function that increases from 0 to 1 as \\(x\\) goes from 0 to 1.\nGraphically, the CDF of a continuous random variable is a smooth curve. It starts at 0, increases as \\(x\\) increases, and eventually reaches 1. The exact shape of the curve depends on the distribution of the variable, but the smooth, non-decreasing nature is a common feature. Figure below shows the CDF of a uniform and normal random variable, respectively.\nplot(ecdf(runif(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\nplot(ecdf(rnorm(500)), main=\"\", col=\"lightblue\", pch=21, bg=\"grey\")\n\n\n\n\n\n\nCDF of a uniform random variable\n\n\n\n\n\n\n\nCDF of a normal random variable\n\n\n\n\n\n\n\n\nThe Inverse CDF Method\nThe inverse distribution method uses samples of uniform random variables to generate draws from random variables with a continuous distribution function, \\(F\\). Since \\(F\\left(  x\\right)\\) is uniformly distributed on \\(\\left[ 0,1\\right]\\), draw a uniform random variable and invert the CDF to get a draw from \\(F\\). Thus, to sample from \\(F\\), \\[\\begin{align*}\n&  \\text{Step 1}\\text{: Draw }U\\sim U\\left[  0,1\\right]  \\ \\\\\n&  \\text{Step 2}\\text{: }\\text{Set }X=F^{-1}\\left(  U\\right)  ,\n\\end{align*}\\] where \\(F^{-1}\\left(  U\\right)  =\\inf\\left\\{  x:F\\left(  x\\right)  =U\\right\\}\\).\nThis inversion method provides i.i.d. draws from \\(F\\) provided that \\(F^{-1}\\left(  U\\right)\\) can be exactly calculated. For example, the CDF of an exponential random variable with parameter \\(\\mu\\) is \\(F\\left(  x\\right) =1-\\exp\\left(  -\\mu x\\right)\\), which can easily be inverted. When \\(F^{-1}\\) cannot be analytically calculated, approximate inversions can be used. For example, suppose that the density is a known analytical function. Then, \\(F\\left(  x\\right)\\) can be computed to an arbitrary degree of accuracy on a grid and inversions can be approximately calculated, generating an approximate draw from \\(F\\). With all approximations, there is a natural trade-off between computational speed and accuracy. One example where efficient approximations are possible are inversions involving normal distributions, which is useful for generating truncated normal random variables. Outside of these limited cases, the inverse transform method does not provide a computationally attractive approach for drawing random variables from a given distribution function. In particular, it does not work well in multiple dimensions.\n\n\nFunctional Transformations\nThe second main method uses functional transformations to express the distribution of a random variable that is a known function of another random variable. Suppose that \\(X\\sim F\\), admitting a density \\(f\\), and that \\(y=h\\left(  x\\right)\\) is an increasing continuous function. Thus, we can define \\(x=h^{-1}\\left(  y\\right)\\) as the inverse of the function \\(h\\). The distribution of \\(y\\) is given by \\[\nF_Y\\left(y\\right)  =P\\left(  Y\\leq y\\right)  =\\int_{-\\infty}^{h^{-1}\\left(  y\\right)  }f\\left(  x\\right)  dx=F_X\\left(  X\\leq h^{-1}\\left(y\\right)  \\right).\n\\] Differentiating with respect to \\(y\\) gives the density via Leibnitz’s rule: \\[\nf_{Y}\\left(  y\\right)  =f\\left(  h^{-1}\\left(  y\\right)  \\right)  \\left\\vert\\frac{d}{dy}\\left(  h^{-1}\\left(  y\\right)  \\right)  \\right\\vert,\n\\] where we make explicit that the density is over the random variable \\(Y\\). This result is used widely. For example, if \\(X\\sim\\mathcal{N}\\left(  0,1\\right)\\), then \\(Y=\\mu+\\sigma X\\). Since \\(x=h^{-1}\\left(  y\\right)  =\\frac{y-\\mu}{\\sigma}\\), the distribution function is \\(F\\left(  \\frac{x-\\mu}{\\sigma}\\right)\\) and density \\[\nf_{Y}\\left(  y\\right)  =\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{y-\\mu}{\\sigma}\\right)  ^{2}\\right).\n\\] Transformations are widely used to simulate both univariate and multivariate random variables. As examples, if \\(Y\\sim\\mathcal{X}^{2}\\left(  \\nu\\right)\\) and \\(\\nu\\) is an integer, then \\(Y=\\sum_{i=1}^{\\nu}X_{i}^{2}\\) where each \\(X_{i}\\) is independent standard normal. Exponential random variables can be used to simulate \\(\\mathcal{X}^{2}\\), Gamma, Beta, and Poisson random variables. The famous Box-Muller algorithm simulates normals from uniform and exponential random variables. In the multivariate setting, Wishart (and inverse Wishart) random variables can be simulated via sums of squared vectors of standard normal random variables.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#expectation-and-variance-reward-and-risk",
    "href": "01-prob.html#expectation-and-variance-reward-and-risk",
    "title": "1  Probability and Uncertainty",
    "section": "1.3 Expectation and Variance (Reward and Risk)",
    "text": "1.3 Expectation and Variance (Reward and Risk)\nAn expected value of a random variable, denoted by \\(\\E{X}\\) is a weighted average. Each possible value of a random variable is weighted by its probability. For example, Google Maps uses expected value when calculating travel times. We might compute two different routes by their expected travel time. Typically, a forecast or expected value is all that is required — these expected values can be updated in real time as we travel. Say I am interested in travel time from Washington National airport to Fairfax in Virginia. The histogram below shows the travel times observed for a work day evening and were obtained from Uber.\n\nExample 1.13 (Uber) Let’s look at the histogram of travel times from Fairfax, VA to Washington, DC\n\nd = read.csv(\"../data/dc_travel_time.csv\") \n# use evening travel times (column 18) and convert from seconds to minutes \nevening_tt = d[,18]/60; day_tt = d[,15]/60; \nevening_tt = evening_tt[!is.na(evening_tt)] # remove missing observations \nhist(evening_tt, freq = F,main=\"\", xlab=\"Travel Time [min]\", nclass=20, col=\"lightblue\")\n\n\n\n\nTravel times in the evening\n\n\n\n\nFrom this dataset, we can empirically estimate the probabilities of observing different values of travel times\n\nbins = hist(evening_tt, breaks = 3, plot = F) \nknitr::kable(data.frame(\"tt\" = bins$mids, \"Probability\" = bins$counts/length(evening_tt)),col.names = c(\"Travel Time\",\"Probability\"),digits=2)\n\n\n\n\nTravel Time\nProbability\n\n\n\n\n18\n0.05\n\n\n22\n0.77\n\n\n28\n0.18\n\n\n\n\n\nThere is a small chance (5%) I can get to Washington, DC in 18 minutes, which probably happens on a holiday and a non-trivial chance (18%) to travel for 28 minutes, possibly due to a sports game or bad weather. Most of the time (77%) our travel time is 22 minutes. However, when Uber shows you the travel time, it uses the expected value as a forecast rather than the full distribution. Specifically, you will be given an expected travel time of 23 minutes.\n\n0.05*18 + 0.77*22 + 0.18*28\n## 23\n\nIt is a simple summary that takes into account travel accidents and other events that can affect travel time as best as it can.\n\nThe expected value \\(\\E{X}\\) of discrete random variable \\(X\\) which takes possible values \\(\\{x_1,\\ldots x_n\\}\\) is calculated using\n\\[\n\\E{X} =\\sum_{i=1}^{n}x_i P(X = x_i)\n\\]\nFor example, in a binary scenario, if \\(X\\in \\{0,1\\}\\) and \\(P(X=1)=p\\), then \\(\\E{X} = 0\\times(1-p)+1\\times p = p\\). The expected value of a Bernoulli random variable is simply the probability of success. In many binary scenarios, a probabilistic forecast is sufficient.\nIf \\(X\\) is continuous with probability distribution \\(p(x)\\), then we have to calculate the expectation as an integral \\[\n\\E{X} = \\int xp(x)d x  \\text{ and } \\int p(x)dx = 1.\n\\]\n\nStandard Deviation and Covariance\nVariance measures the spread of a random variable around its expected value \\(\\mu = \\E{X}\\). For a discrete random variable \\(X\\) with possible values \\(\\{x_1,\\ldots x_N\\}\\), we have \\[\n\\Var{X} = \\E{(X-\\mu)^2} =  \\sum_{i=1}^N (x_i-\\mu)^2 P(X=x_i).\n\\] In the continuous case, we have \\[\n\\Var{X} = \\int_{-\\infty}^\\infty (x-\\mu) ^2 p(x)dx,\\text{ where } \\mu = \\mathbb{E}(X)=\\int_{-\\infty}^{\\infty}p_X(x)dx.\n\\]\nThe standard deviation is more convenient and is the square root of variance \\(\\sd{X} = \\sqrt{\\Var{X}}\\). Standard deviation has the desirable property that it is measured in the same units as the random variable \\(X\\) itself and is a more useful measure.\nSuppose that we have two random variables \\(X\\) and \\(Y\\). We need to measure whether they move together or in opposite directions. The covariance is defined by \\[\n\\Cov{X,Y} = \\E{\\left[ (X- \\E{X})(Y- \\E{Y})\\right]}.\n\\]\nWhen \\(X\\) and \\(Y\\) are discrete and we are given the joint probability distribution, we need to calculate \\[\n\\Cov{X,Y} = \\sum_{x,y}  ( x - \\E{X} )(y - \\E{Y})p(x,y).\n\\] Covariance is measured in units of \\(X\\times\\)units of \\(Y\\). This can be inconvenient and makes it hard to compare covariances of different pairs of variables. A more convenient metric is the correlation, which is defined by \\[\n\\Cor{X,Y}= \\frac{ \\Cov{X,Y} }{ \\sd{X} \\sd{Y} }.\n\\] Correlation, \\(\\Cor{X,Y}\\), is unitless and takes values between -1 and 1.\nIn the case of joint continuous distribution it is convenient to use the covariance matrix \\(\\Sigma\\) which is defined as \\[\n\\Sigma = \\begin{bmatrix}\n\\Var{X} & \\Cov{X,Y} \\\\\n\\Cov{X,Y} & \\Var{Y}\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) are independent, then \\(\\Cov{X,Y} = 0\\) and \\(\\Sigma\\) is diagonal. The correlation matrix is defined as \\[\n\\rho = \\begin{bmatrix}\n1 & \\Cor{X,Y} \\\\\n\\Cor{X,Y} & 1\n\\end{bmatrix}.\n\\] If \\(X\\) and \\(Y\\) have an exact linear relationship, then \\(\\Cor{X,Y} = 1\\) and \\(\\Cov{X,Y}\\) is the product of standard deviations. In matrix notation, the relation between the covariance matrix and correlation matrix is given by \\[\n\\rho = \\mathrm{diag}\\left(\\Sigma\\right)^{-1/2} \\Sigma\\mathrm{diag}\\left(\\Sigma\\right)^{-1/2},\n\\] where \\(\\Sigma\\) is a diagonal matrix with standard deviations on the diagonal.\n\n\nPortfolios: linear combinations\nCalculating means and standard deviations of combinations of random variables is a central tool in probability. It is known as the portfolio problem. Let \\(P\\) be your portfolio, which comprises a mix of two assets \\(X\\) and \\(Y\\), typically stocks and bonds, \\[\nP = aX + bY,\n\\] where \\(a\\) and \\(b\\) are the portfolio weights, typically \\(a+b=1\\), as we are allocating our total capital. Imagine that you have placed \\(a\\) dollars on the random outcome \\(X\\), and \\(b\\) dollars on \\(Y\\). The portfolio \\(P\\) measures your total weighted outcome.\nKey portfolio rules: The expected value and variance follow the relations \\[\\begin{align*}\n\\E{aX + bY} = &      a\\E{X}+b\\E{Y}\\\\\n\\Var{ aX + bY }  = & a^2 \\Var{X} + b^2 \\Var{Y} + 2 ab \\Cov{X,Y },\n\\end{align*}\\] with covariance defined by \\[\n\\Cov{X,Y} = \\E{ ( X- \\E{X} )(Y- \\E{Y})}.\n\\] Expectation and variance help us to understand the long-run behavior. When we make long-term decisions, we need to use the expectations to avoid biases.\nThe covariance is related to the correlation by \\(\\Cov{X,Y} = \\text{Corr}(X, Y) \\cdot \\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)}\\).\n\nExample 1.14 (Tortoise and Hare) Tortoise and Hare are selling cars. Say \\(X\\) is the number of cars sold and probability distributions, means and variances are given by the following table\n\n\n\n\n\\(X\\)\n\n\n\nMean\nVariance\nsd\n\n\n\n\n\n0\n1\n2\n3\n\\(\\E{X}\\)\n\\(\\Var{X}\\)\n\\(\\sqrt{\\Var{X}}\\)\n\n\nTortoise\n0\n0.5\n0.5\n0\n1.5\n0.25\n0.5\n\n\nHare\n0.5\n0\n0\n0.5\n1.5\n2.25\n1.5\n\n\n\nLet’s calculate Tortoise’s expectations and variances \\[\\begin{align*}\n\\E{T} & = (1/2) (1) + (1/2)(2) = 1.5 \\\\\n\\Var{T} & = \\E{T^2} - \\E{T}^2 \\\\\n& =  (1/2)(1)^2 + (1/2)(2)^2 - (1.5)^2 = 0.25\n\\end{align*}\\]\nNow the Hare’s \\[\\begin{align*}\n\\E{H} & = (1/2)(0) + (1/2)(3) = 1.5 \\\\\n\\Var{H} & =  (1/2)(0)^2 + (1/2)(3)^2- (1.5)^2 = 2.25\n\\end{align*}\\]\nWhat do these tell us about the long run behavior?\n\nTortoise and Hare have the same expected number of cars sold.\nTortoise is more predictable than Hare. He has a smaller variance.\n\nThe standard deviations \\(\\sqrt{\\Var{X}}\\) are \\(0.5\\) and \\(1.5\\), respectively. Given two equal means, you always want to pick the lower variance. If we are to invest in one of those, we prefer Tortoise.\nWhat about a portfolio of Tortoise and Hare? Suppose I want to evenly split my investment between Tortoise and Hare. What is the expected number of cars sold and the variance of the number of cars sold? \\[\n\\E{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{2} \\E{T} + \\frac{1}{2} \\E{H} = 1.5\n\\] For variance, we need to know \\(\\Cov{T,H}\\). Let’s take \\(\\Cov{T,H} = -1\\) and see what happens. \\[\n\\Var{\\frac{1}{2}T + \\frac{1}{2}H} = \\frac{1}{4} \\Var{T} + \\frac{1}{4} \\Var{H} + \\frac{1}{2} \\Cov{T,H} = 0.0625 + 0.5625 -0.5 = 0.125\n\\]\nNotice that the portfolio variance (0.125) is lower than both individual variances (0.25 and 2.25). This demonstrates the power of diversification - by combining investments with negative covariance, we can reduce overall risk while maintaining the same expected return. The negative covariance indicates that when Tortoise performs well, Hare tends to perform poorly, and vice versa, creating a natural hedge.\nThis example illustrates a fundamental principle in finance and decision theory: diversification can reduce risk without sacrificing expected returns when assets are not perfectly positively correlated. The key insight is that variance depends not only on individual asset volatilities but also on their covariances, making portfolio construction a crucial consideration in risk management.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#limiting-behavior-of-averages",
    "href": "01-prob.html#limiting-behavior-of-averages",
    "title": "1  Probability and Uncertainty",
    "section": "1.4 Limiting Behavior of Averages",
    "text": "1.4 Limiting Behavior of Averages\nThe Tortoise and Hare example illustrates how expectations and variances behave for a single period or a fixed portfolio. However, in many real-world applications—from insurance to machine learning—we are interested in what happens when we repeat an experiment many times. Does the average outcome settle down to a predictable value?\nWhen we observe a stochastic process repeatedly, we naturally ask: what happens to averages as we collect more data? This question lies at the heart of statistical inference and forms the theoretical foundation for learning from experience. The answer is provided by the law of large numbers, one of the most fundamental results in probability theory.\n\nThe Weak Law of Large Numbers\nThe weak law of large numbers, in its simplest form, states that sample averages converge in probability to the expected value. We assume that observation \\(x_1, x_2, \\ldots, x_n\\) are draws (realizations) of independent identically distributed (i.i.d.) random variables \\(X_1, X_2, \\ldots, X_n\\) with finite mean \\(\\mu = \\E{X_i}\\). Then the sample average \\(\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\) satisfies: \\[\n\\lim_{n \\to \\infty} P\\left(\\left|\\bar{X}_n - \\mu\\right| &gt; \\epsilon\\right) = 0,\n\\] for any \\(\\epsilon &gt; 0\\). This form of convergence, known as convergence in probability, means that for large enough \\(n\\), the probability that the sample average deviates from the true mean by more than any fixed amount becomes arbitrarily small.\nThe proof of this result, when the variance \\(\\sigma^2 = \\Var{X_i}\\) exists and is finite, follows elegantly from Chebyshev’s inequality. Since \\(\\E{\\bar{X}_n} = \\mu\\) and \\(\\Var{\\bar{X}_n} = \\sigma^2/n\\), we have:\n\\[\nP\\left(\\left|\\bar{X}_n - \\mu\\right| &gt; \\epsilon\\right) \\leq \\frac{\\Var{\\bar{X}_n}}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2} \\to 0\n\\]\nas \\(n \\to \\infty\\). This simple argument reveals why averages become more reliable as sample sizes grow: the variance of the sample mean shrinks at rate \\(1/n\\).\nThis \\(1/n\\) scaling reappears throughout the book, from confidence intervals and A/B testing (Chapter 5) to the role of sample size in estimation risk and generalization.\n\n\nKolmogorov’s Strong Law of Large Numbers\nWhile the weak law establishes convergence in probability, a stronger form of convergence is possible. The strong law of large numbers states that sample averages converge almost surely (with probability one) to the expected value. This is a fundamentally stronger statement: it means that for almost every realization of the sequence, the sample average actually approaches the true mean, not merely that the probability of large deviations vanishes.\nAndrey Kolmogorov formalized this result in his groundbreaking 1933 monograph Foundations of the Theory of Probability (Kolmogoroff 1933), establishing the conditions under which strong convergence holds. His result revolutionized probability theory by providing a rigorous measure-theoretic foundation for probabilistic reasoning.\nKolmogorov’s Strong Law states that if \\(X_1, X_2, \\ldots\\) are independent random variables (not necessarily identically distributed) with \\(\\E{|X_i|} &lt; \\infty\\), then:\n\\[\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\to \\mu \\quad \\text{almost surely}\n\\]\nwhere \\(\\mu = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^n \\E{X_i}\\), provided this limit exists. For i.i.d. random variables with \\(\\E{|X_i|}  = \\mu &lt; \\infty\\), this simplifies to \\(\\bar{X}_n \\to \\mu\\) almost surely.\nThe distinction between convergence in probability and almost sure convergence is subtle but crucial. Convergence in probability allows for infinitely many large deviations, as long as they become increasingly rare. Almost sure convergence is stronger: it requires that eventually, after some finite (but random) time, all deviations remain small forever.\nFormally, almost sure convergence means:\n\\[\nP\\left(\\lim_{n \\to \\infty} \\bar{X}_n = \\mu\\right) = 1\n\\]\nThis is equivalent to saying that the set of sequences for which the limit fails to equal \\(\\mu\\) has probability zero. For practical purposes, this means we can be confident that the specific sequence we observe will exhibit convergence—not merely that convergence is likely.\nThe independence assumption is central to Kolmogorov’s results, but it can be relaxed in various ways for different applications. For stochastic processes with dependent observations, the key question becomes: how much dependence can we tolerate while still obtaining convergence of averages?\nFor stationary processes—where the joint distribution of \\((X_t, X_{t+1}, \\ldots, X_{t+k})\\) does not depend on \\(t\\)—a law of large numbers holds under considerably weaker conditions than independence. If the process is ergodic (roughly, if it eventually “forgets” its initial conditions), then time averages converge to ensemble averages:\n\\[\n\\frac{1}{n}\\sum_{i=1}^n X_i \\to \\mu \\quad \\text{almost surely}\n\\]\nThis ergodic theorem extends Kolmogorov’s law of large numbers to dependent sequences and has profound implications for statistical inference from time series data. It justifies the common practice of estimating population means from a single long realization of a stochastic process.\nThe law of large numbers is not merely a theoretical curiosity—it forms the bedrock of statistical practice. Every time we estimate a population mean from a sample, test a hypothesis, or train a machine learning model, we implicitly rely on the law of large numbers. The confidence we place in larger samples, the use of cross-validation to assess model performance, and the convergence of stochastic gradient descent in deep learning all trace back to this fundamental result.\nIn the context of stochastic processes, the law of large numbers justifies estimating process parameters from a single long trajectory. When modeling financial returns, climate data, or network traffic, we typically observe one realization over time rather than multiple independent realizations. The ergodic theorem ensures that time averages from this single path converge to the true population moments, enabling inference from the data we actually have.\nThe Kolmogorov’s formalization is the culmination of several decades of work on the foundations of probability theory. The weak law of large numbers was first proved by Jakob Bernoulli in 1713 for the special case of binomial random variables—an achievement that took him over twenty years. The result was later generalized by Poisson (1837) and Chebyshev (1867) to broader classes of random variables.\nThe strong law required deeper mathematical machinery. Émile Borel proved a version for Bernoulli trials in 1909, but the general result awaited the development of measure-theoretic probability. Francesco Cantelli made progress in the 1910s, but it was Kolmogorov who provided the definitive treatment in 1933, unifying diverse results under a single rigorous framework.\nKolmogorov’s work transformed probability theory from a collection of special cases and heuristics into a branch of mathematics with the same rigor as analysis or algebra. His measure-theoretic foundations enabled precise statements about almost sure convergence, clarified the distinction between different modes of convergence, and opened the door to modern probability theory and stochastic processes.\nThe law of large numbers has profound implications for Bayesian inference and computational statistics. For applications to posterior consistency, Monte Carlo simulation, and Markov chain Monte Carlo (MCMC) methods, see Section 3.12 in Chapter 3.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#binomial-poisson-and-normal-distributions",
    "href": "01-prob.html#binomial-poisson-and-normal-distributions",
    "title": "1  Probability and Uncertainty",
    "section": "1.5 Binomial, Poisson, and Normal Distributions",
    "text": "1.5 Binomial, Poisson, and Normal Distributions\nHaving explored specific examples of random variables and their properties through the Tortoise and Hare scenario, we now turn to examining some of the most fundamental and widely-used probability distributions. These distributions form the building blocks of probability theory and statistics, appearing repeatedly across diverse fields from finance and engineering to biology and social sciences. Understanding their properties—including their probability mass or density functions, expected values, and variances—is essential for modeling real-world phenomena and making informed decisions under uncertainty. In this section, we’ll introduce several key distributions, starting with the simple yet powerful Bernoulli distribution and building toward more complex models that capture different types of random behavior.\n\nBernoulli Distribution\nThe formal model of a coin toss was described by Bernoulli. He modeled the notion of probability for a coin toss, now known as the Bernoulli distribution, where \\(X \\in \\{0,1\\}\\) and \\(P(X=1)=p, P(X=0) = 1-p\\). Laplace gave us the principle of insufficient reason: where you would list out the possibilities and then place equal probability on each of the outcomes. Essentially the discrete uniform distribution on the set of possible outcomes.\nA Bernoulli trial relates to an experiment with the following conditions\n\nThe result of each trial is either a success or failure.\nThe probability \\(p\\) of a success is the same for all trials.\nThe trials are assumed to be independent.\n\nThe Bernoulli random variable can take on one of two possible outcomes, typically labeled as “success” and “failure.” It is named after the Swiss mathematician Jacob Bernoulli, who introduced it in the 18th century. The distribution is often denoted by \\(\\text{Bernoulli}(p)\\), where \\(p\\) is the probability of success.\nThe probability mass function (PMF) of a Bernoulli distribution is defined as follows: \\[\nP(X = x) = \\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0\n\\end{cases}\n\\] The expected value (mean) of a Bernoulli distributed random variable \\(X\\) is given by: \\[\\E{X} = p\n\\] Simply speaking, if you are to toss a coin many times, you expect \\(p\\) heads.\nThe variance of \\(X\\) is given by: \\[\n\\Var{X} = p(1-p)\n\\]\n\nExample 1.15 (Coin Toss) The quintessential random variable is an outcome of a coin toss. The set of all possible outcomes, known as the sample space, is \\(S = \\{H,T\\}\\), and \\(P(X = H) = P(X = T) = 1/2\\). On the other hand, a single outcome can be an element of many different events. For example, there are four possible outcomes of two coin tosses, HH, TT, HT, TH, which are equally likely with probabilities 1/4. The probability mass function over the number of heads \\(X\\) out of two coin tosses is\n\n\n\n\\(x\\)\n\\(p(x)\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nGiven the probability mass function we can, for example, calculate the probability of at least one head as \\(P(X \\geq 1) = P(X =1) + P(X =2) = p(1)+p(2) = 3/4\\).\n\nThe Bernoulli distribution serves as the foundation for more complex distributions, such as the binomial distribution (which models the number of successes in a fixed number of independent Bernoulli trials) and the geometric distribution (which models the number of trials needed to achieve the first success). A Binomial distribution arises from a sequence of Bernoulli trials, and assigns probability to \\(X\\), which is the number of successes. Its probability distribution is calculated via: \\[\nP(X=x) = {n \\choose x} p^x(1-p)^{n-x}.\n\\] Here \\({n \\choose x}\\) is the combinatorial function, \\[\n{n \\choose x} = \\frac{n!}{x!(n-x)!},\n\\] Here compbinatorial function \\({n \\choose x}\\) counts the number of ways of getting \\(x\\) successes in \\(n\\) trials and \\(n!=n(n-1)(n-2)\\ldots 2 \\cdot 1\\) counts the number of permutations of \\(n\\) observations without replacement.\nPlot below shows the probability mass function of a Binomial distribution with \\(n = 20\\) and \\(p = 0.3\\).\n\nbarplot(dbinom(0:20, size = 20, prob = 0.3), names.arg = 0:20, col = \"lightblue\",\n xlab = \"Number of Heads\", ylab = \"Probability\")\n\n\n\n\nProbability Mass Function of a Binomial Distribution (n = 20, p = 0.3)\n\n\n\n\nThe table below shows the expected value and variance of a Binomial random variable. Those quantities can be calculated by plugging in the possible outcomes ans corresponding probabilities into the definitions of expected value and variance.\n\nMean and Variance of Binomial\n\n\nBinomial Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = n p\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = n p ( 1 - p )\\)\n\n\n\nFor large sample sizes \\(n\\), this distribution is approximately normal with mean \\(np\\) and variance of \\(np(1-p)\\).\nSuppose we are about to toss two coins. Let \\(X\\) denote the number of heads. Then the following table specifies the probability distribution \\(p(x)\\) for all possible values \\(x\\) of \\(X\\). This leads to the following table\n\nOutcomes of two coin flips\n\n\n\\(x\\)\n\\(P(X=x)\\)\n\n\n\n\n0\n1/4\n\n\n1\n1/2\n\n\n2\n1/4\n\n\n\nThus, most likely we will see one Head after two tosses. Now, let’s look at a more complex example and introduce our first probability distribution, namely the Binomial distribution.\nLet \\(X\\) be the number of heads in three flips. Each possible outcome (“realization”) of \\(X\\) is an event. Now consider the event of getting only two heads \\[\n\\{ X= 2\\} = \\{ HHT, HTH, THH \\} ,\n\\] The probability distribution of \\(X\\) is Binomial with parameters \\(n = 3, p= 1/2\\), where \\(n\\) denotes the sample size (a.k.a. number of trials) and \\(p\\) is the probability of heads; we have a fair coin. The notation is \\(X \\sim \\mathrm{Bin} \\left ( n = 3 , p = \\frac{1}{2} \\right )\\) where the sign \\(\\sim\\) is read as distributed as.\n\nOutcomes of three coin flips\n\n\nResult\n\\(X\\)\n\\(P(X=x)\\)\n\n\n\n\nHHH\n3\n\\(p^3\\)\n\n\nHHT\n2\n\\(p^2 ( 1- p)\\)\n\n\nHTH\n2\n\\(p^2 ( 1 - p)\\)\n\n\nTHH\n2\n\\((1-p)p^2\\)\n\n\nHTT\n1\n\\(p( 1-p)^2\\)\n\n\nTHT\n1\n\\(p ( 1-p)^2\\)\n\n\nTTH\n1\n\\((1-p)^2 p\\)\n\n\nTTT\n0\n\\((1-p)^3\\)\n\n\n\n\n\nPoisson Distribution\nThe Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. You can think of Poisson distribution as a limiting case of the Binomial distribution when the number of trials \\(n\\) is large and the probability of success \\(p\\) is small, such that \\(np = \\lambda\\) remains constant. Think of a soccer game where the goal is the “successful” event of interest. The soccer team does not have a predefined number of attempts to score a goal. Rather they continiously try to score a goal until they do. The number of goals scored in a game is the number of events occurring in a fixed interval of time. The mean number of goals scored in a game is the mean rate of events occurring in a fixed interval of time.\nThere are many examples of Poisson distributed random variables. For example, the number of phone calls received by a call center per hour, the number of emails received per day, the number of customers arriving at a store per hour, the number of defects in a manufactured product, the number of accidents on a highway per month.\nA random variable \\(X\\) follows a Poisson distribution with parameter \\(\\lambda &gt; 0\\) if its probability mass function is given by: \\[\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\] for \\(k = 0, 1, 2, 3, \\ldots\\), where \\(\\lambda\\) is both the mean and variance of the distribution.\nPlot below shows the probability mass function of a Poisson distribution with \\(\\lambda = 1, 5, 10\\).\n\n\nCode\n# Create x values\nx &lt;- 0:20\n\n# Plot the first distribution\nplot(x, dpois(x, lambda = 1), type = \"b\", col = \"blue\", lwd = 2,\n     xlab = \"Number of Events\", ylab = \"Probability\", \n     main = \"Poisson Distribution for Different Lambda Values\",\n     ylim = c(0, 0.4))\n\n# Add the other distributions\nlines(x, dpois(x, lambda = 5), col = \"red\", lwd = 2, type = \"b\")\nlines(x, dpois(x, lambda = 10), col = \"green\", lwd = 2, type = \"b\")\n\n# Add legend\nlegend(\"topright\", legend = c(\"lmb = 1\", \"lmb = 5\", \"lmb = 10\"), \n       col = c(\"blue\", \"red\", \"green\"), lwd = 2, bty = \"n\", cex = 0.8)\n\n\n\n\n\nProbability Mass Function of a Poisson Distribution\n\n\n\n\nWe write \\(X \\sim \\text{Poisson}(\\lambda)\\) to denote that \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\).\n\nMean and Variance of Poisson\n\n\nPoisson Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = \\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = \\lambda\\)\n\n\n\n\nExample 1.16 (Customer Arrivals) Suppose customers arrive at a coffee shop at an average rate of 3 customers per hour. What is the probability that exactly 5 customers will arrive in the next hour?\nUsing the Poisson distribution with \\(\\lambda = 3\\): \\[\nP(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\times e^{-3}}{120} \\approx 0.101\n\\]\nSo there is approximately a 10.1% chance that exactly 5 customers will arrive in the next hour.\n\nThe Poisson distribution can be derived as a limiting case of the binomial distribution when \\(n\\) is large and \\(p\\) is small, such that \\(np = \\lambda\\) remains constant. This connection makes the Poisson distribution particularly useful for modeling rare events in large populations.\n\n\nNormal Distribution\nThe Normal distribution is a continuous probability distribution that is widely used in statistics and probability theory. It is also known as the Gaussian distribution or the bell curve. The Normal distribution is characterized by its symmetric bell-shaped curve and is defined by two parameters: the mean (\\(\\mu\\)) and the variance (\\(\\sigma^2\\)).\nThe probability density function (PDF) of the Normal distribution is given by: \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\] for \\(-\\infty &lt; x &lt; \\infty\\), where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\nThe Normal distribution is often used to model real-world phenomena such as measurement errors, heights, weights, and scores on standardized tests. It is also used in hypothesis testing and confidence interval construction.\n\nExample 1.17 (Heights of Adults) The heights of adult males in a certain population are normally distributed with a mean of 70 inches and a standard deviation of 3 inches.\n\nWhat is the probability that a randomly selected male is between 67 and 73 inches tall?\n\nUsing the Normal distribution with \\(\\mu = 70\\) and \\(\\sigma = 3\\):\n\n\nCode\n# Define parameters\nmu &lt;- 70\nsigma &lt;- 3\n\n# Create x values for the curve\nx &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\n\n# Plot the normal curve\nplot(x, y, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Height (inches)\", ylab = \"Density\",main = \"\")\n\n# Add vertical lines at 67 and 73\nabline(v = 67, col = \"red\", lty = 2, lwd = 2)\nabline(v = 73, col = \"red\", lty = 2, lwd = 2)\n\n# Highlight the area between 67 and 73\nx_area &lt;- seq(67, 73, length.out = 100)\ny_area &lt;- dnorm(x_area, mean = mu, sd = sigma)\npolygon(c(67, x_area, 73), c(0, y_area, 0), \n         col = rgb(0, 0, 1, alpha = 0.3), border = NA)\n\n# Add labels\ntext(66, 0, \"67\", col = \"red\", cex = 1.2)\ntext(74, 0, \"73\", col = \"red\", cex = 1.2)\ntext(70, 0.06, \"P(67 &lt; X &lt; 73)\", cex = 1.2, col = \"darkblue\")\n\n# Add grid for better readability\ngrid(col = \"gray\", lty = 3)\n\n\n\n\n\n\n\n\nFigure 1.2: Normal Distribution (mu = 70, sigma = 3) with Highlighted Area\n\n\n\n\n\nThe bell curve in Figure 1.2 shows the probability density function of a random variable \\(X \\sim N(70,3^2)\\) that follows a Normal distribution with mean 70 and standard deviation 3. As we can see, the Normal distribution is symmetric around the mean, and the mean, median, and mode are all equal. The prrobability of a randomly selected male being between 67 and 73 inches tall is the area under the curve between 67 and 73 inches and is approximately 0.6827.\nNow let’s calculate this probability using R:\n\nprob_between &lt;- pnorm(73, mean = 70, sd = 3) - pnorm(67, mean = 70, sd = 3)\ncat(\"P(67 &lt; X &lt; 73) =\", round(prob_between, 4), \"\\n\")\n## P(67 &lt; X &lt; 73) = 0.68\n\nThe calculation shows that \\(P(67 &lt; X &lt; 73) = P(X \\leq 73) - P(X \\leq 67) \\approx 0.6827\\).\nThis result makes sense because 67 and 73 inches are exactly one standard deviation below and above the mean (70 \\(\\pm\\) 3), respectively. According to the empirical rule (68-95-99.7 rule), approximately 68% of values in a normal distribution fall within one standard deviation of the mean.\nSo approximately 68.27% of adult males are between 67 and 73 inches tall.\n\nWhat height corresponds to the 95th percentile?\n\nWe need to find \\(x\\) such that \\(P(X \\leq x) = 0.95\\). From standard normal tables, \\(\\Phi^{-1}(0.95) \\approx 1.645\\).\n\nx &lt;- qnorm(0.95, mean = 70, sd = 3)\ncat(\"Height corresponding to the 95th percentile:\", round(x, 2), \"inches\\n\")\n## Height corresponding to the 95th percentile: 75 inches\n\nTherefore: \\(x = \\mu + \\sigma \\cdot 1.645 = 70 + 3 \\cdot 1.645 = 74.935\\) inches.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#conditional-marginal-and-joint-distributions",
    "href": "01-prob.html#conditional-marginal-and-joint-distributions",
    "title": "1  Probability and Uncertainty",
    "section": "1.6 Conditional, Marginal and Joint Distributions",
    "text": "1.6 Conditional, Marginal and Joint Distributions\nSuppose that we have two random variables \\(X\\) and \\(Y\\), which can be related to each other. Knowing \\(X\\) would change your belief about \\(Y\\). For example, as a first pass, psychologists who study the phenomenon of happiness can be interested in understanding its relation to income level. Now we need a single probability mass function (a.k.a. probabilistic model) that describes all possible values of those two variables. Joint distributions do exactly that.\nFormally, the joint distribution of two variables \\(X\\) and \\(Y\\) is a function given by \\[\nP(x,y) = P(X=x,Y=y).\n\\] This maps all combinations of possible values of these two variables to a probability on the interval [0,1].\nThe conditional probability is a measure of the probability of a random variable \\(X\\), given that the value of another random variable was observed \\(Y = y\\). \\[\nP(x\\mid y) = P(X = x \\mid Y = y).\n\\]\nThe marginal probability of a subset of a collection of random variables is the probability distribution of the variables contained in the subset without reference to the values of the other variables. Say we have two random variables \\(X\\) and \\(Y\\), the marginal probability \\(P(X)\\) is the probability distribution of \\(X\\) when the values of \\(Y\\) are not taken into consideration. This can be calculated by summing the joint probability distribution over all values of \\(Y\\). The converse is also true: the marginal distribution can be obtained for \\(Y\\) by summing over the separate values of \\(X\\).\nMarginal probability is different from conditional probability. Marginal probability is the probability of a single event occurring, independent of other events. A conditional probability, on the other hand, is the probability that an event occurs given that another specific event has already occurred.\n\nExample 1.18 (Salary-Happiness) Let’s look at an example. Suppose that to model the relationship between two quantities, salary \\(Y\\) and happiness \\(X\\). After running a survey, we summarize our results using the joint distribution, that is described by the following “happiness index” table as a function of salary.\n\n\n\nResults of the Gallup survey. Rows are Salary (\\(Y\\)) and columns are happiness (\\(X\\))\n\n\n\nX = 0 (low)\nX = 1 (medium)\nX = 2 (high)\n\n\n\n\nY = low (0)\n0.03\n0.13\n0.14\n\n\nY = medium (1)\n0.12\n0.11\n0.01\n\n\nY = high (2)\n0.07\n0.01\n0.09\n\n\nY = very high (3)\n0.02\n0.13\n0.14\n\n\n\n\n\nEach cell of the table is the joint probability, e.g. 14% of people have very high income level and are very happy. Those joint probabilities are calculated by simple counting and calculating the proportions.\nNow, if we want to answer the question what is the percent of high earners in the population. For that we need to calculate what is called a marginal probability \\(P(y = 2)\\). We can calculate the proportion of high earners \\(P(y = 2)\\) by summing up the entries in the third row of the table, which is 0.17 in our case.\n\n0.07 + 0.01 + 0.09\n## 0.17\n\nFormally marginal probability over \\(y\\) is calculated by summing the joint probability over the other variable, \\(x\\), \\[\np(y) = \\sum_{x \\in S}p(x,y)\n\\] Where \\(S\\) is the set of all possible values of the random variable \\(X\\).\n\n\n\n\n\n\nAnother question of interest is whether happiness depends on income level. To answer those types of questions, we need to introduce an important concept, which is the conditional probability of \\(X\\) given that the value of variable \\(Y\\) is known. This is denoted by \\(P(X=x\\mid Y=y)\\) or simply \\(p(x\\mid y)\\), where \\(\\mid\\) reads as “given” or “conditional upon”.\nThe conditional probability \\(p(x\\mid y)\\) also has interpretation as updating your probability over \\(X\\) after you have learned the new information about \\(Y\\). In this sense, probability is also the language of how you change opinions in light of new evidence. The proportion of happy people among high earners is given by the conditional probability \\(P(X=2\\mid Y=2)\\) and can be calculated by dividing the proportion of those who are high earners and highly happy by the proportion of high earners \\[\nP(X=2\\mid Y=2) = \\dfrac{P(X=2,Y=2)}{P(Y=2)} = \\dfrac{0.09}{0.17} = 0.5294118.\n\\]\nNow, if we compare it with the proportion of highly happy people \\(P(X = 2) = 0.38\\), we see that on average you are more likely to be happy given your income is high.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#independence",
    "href": "01-prob.html#independence",
    "title": "1  Probability and Uncertainty",
    "section": "1.7 Independence",
    "text": "1.7 Independence\nHistorically, the concept of independence in experiments and random variables has been a defining mathematical characteristic that has uniquely shaped the theory of probability. This concept has been instrumental in distinguishing the theory of probability from other mathematical theories.\nUsing the notion of conditional probability, we can define independence of two variables. Two random variables \\(X\\) and \\(Y\\) are said to be independent if \\[\nP(Y = y \\mid X = x) = P(Y = y),\n\\] for all possible \\(x\\) and \\(y\\) values. That is, learning information \\(X=x\\) doesn’t affect our probabilistic assessment of \\(Y\\) for any value \\(y\\). In the case of independence, \\(p(x \\mid y) = p(x)\\) and \\(p(y \\mid x) = p(y)\\).\nConditional probabilities are counter-intuitive. For example, one of the most important properties is typically \\(p( x \\mid y ) \\neq p( y\\mid x )\\). Confusing these two—specifically equating \\(P(A|B)\\) with \\(P(B|A)\\)—is a common error known as the Prosecutor’s Fallacy.\nWe just derived an important relation that allows us to calculate conditional probability \\(p(x \\mid y)\\) when we know joint probability \\(p(x,y)\\) and marginal probability \\(p(y)\\). The total probability or evidence can be calculated as usual, via \\(p(y) = \\sum_{x}p(x,y)\\).\nWe will see that independence will lead to a different conclusion than the Bayes conditional probability decomposition: specifically, independence yields \\(p( x,y ) = p(x) p(y)\\) whereas Bayes implies \\(p(x ,y) = p(x)p(y \\mid x)\\).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "01-prob.html#sec-dutch-book",
    "href": "01-prob.html#sec-dutch-book",
    "title": "1  Probability and Uncertainty",
    "section": "1.8 Further Notes: Dutch Book Arguments",
    "text": "1.8 Further Notes: Dutch Book Arguments\nIf probabilities are degrees of belief and subjective, where do they come from and what rules must they satisfy? These questions were answered to varying degrees by Ramsey, de Finetti, and Savage. Ramsey and de Finetti, working independently and at roughly the same time, developed the first primitive theories of subjective probability and expected utility, and Savage placed the theories on a more rigorous footing, combining the insights of Ramsey with the expected utility theory of von Neumann and Morgenstern.\nThe starting point for Ramsey’s and de Finetti’s theories is the measurement of one’s subjective probabilities using betting odds, which have been used for centuries to gauge the uncertainty over an event. As noted by de Finetti, “It is a question of simply making mathematically precise the trivial and obvious idea that the degree of probability attributed by an individual to a given event is revealed by the conditions under which he would be disposed to bet on that event” (p. 101). Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.\nFormally, for any event \\(A\\), the identity \\[\nP(A)  =\\frac{1}{1+\\text{odds}(A)}\\text{ or }\\text{odds}(A)=\\frac{1-P(A)}{P(A)},\n\\] where \\(\\bar A\\) is the complement of \\(A\\), links odds and probabilities. Throughout, we use \\(P\\) as a generic term to denote probabilities, when there is no specific reference to an underlying distribution or density. If a horse in a race has odds of 2, commonly expressed as 2:1 (read two to one), then the probability the horse wins is \\(1/3\\). The basic idea of using betting odds to elicit probabilities is simple and intuitive: ask an individual to place odds over various mutually exclusive events, and use these odds to calculate the probabilities. Odds are fair if lower odds would induce a person to take the bet and higher odds would induce the person to take the other side of the bet.\nIn constructing a collection of betting odds over various events, de Finetti and Ramsey argued that not all odds are rational (i.e., consistent or coherent). For example, the sum of the probability of each horse winning a race cannot be greater than one. If a person has inconsistent beliefs, then he “could have a book made against him by a cunning bettor and would then stand to lose in any event” (Ramsey (1931), p. 22). This situation is called a Dutch book arbitrage, and a rational theory of probability should rule out such inconsistencies. By avoiding Dutch books, Ramsey and de Finetti showed that the degrees of beliefs elicited from coherent odds satisfy the standard axioms of probability theory, such as the restriction that probabilities are between zero and one, finite additivity, and the laws of conditional probability. The converse also holds: probabilities satisfying the standard axioms generate odds excluding Dutch-book arbitrages. Absence of arbitrage is natural in finance and economics and is a primary assumption for many foundational results in asset pricing. In fact, the derivations given below have a similar flavor to those used to prove the existence of a state price density assuming discrete states.\nDutch-book arguments are simple to explain. To start, they require an individual to post odds over events. A bettor or bookie can then post stakes or make bets at those odds with a given payoff, \\(S\\). The choice of the stakes is up to the bettor. A Dutch book occurs when a cunning bettor makes money for sure by placing carefully chosen stakes at the given odds. Alternatively, one can view the odds as prices of lottery tickets that pay off $1 when the event occurs, and the stakes as the number of tickets bought. Thus, probabilities are essentially lottery ticket prices. In fact, de Finetti used the notation ‘Pr’ to refer to both prices and probabilities.\nTo derive the rules, consider the first axiom of probability: for any event \\(A\\), \\(0\\leq P(A) \\leq 1\\). Suppose that the odds imply probabilities \\(P(A)\\) for \\(A\\) occurring and \\(P(\\bar A)\\) for other outcomes, with associated payoffs of \\(S_{A}\\) and \\(S_{\\bar A}\\). Then, having bet \\(S_{A}\\) and \\(S_{\\bar A}\\), the gains if \\(A\\) or \\(\\bar A\\) occur, \\(G_{A}\\) and \\(G_{\\bar A}\\), respectively, are \\[\\begin{align*}\nG(A)   &  =S_{A}-P(A) S_{A}-P(\\bar A)  S_{\\bar A}\\\\\nG(\\bar A)   &  =S_{\\bar A}-P(A) S_{A}-P(\\bar A)  S_{\\bar A}.\n\\end{align*}\\] To see this, note that the bettor receives \\(S_{A}\\) and pays \\(P(A) S_{A}\\) for a bet on event \\(A\\). The bookie can always choose to place a zero stake on \\(\\bar A\\) occurring, which implies that \\(G(A) =S_{A}-P(A) S_{A}\\) and \\(G\\left(\\bar A\\right) =-P(A) S_{A}\\). Coherence or the absence of arbitrage implies that you cannot gain or lose in both states, thus \\(G(A) G(\\bar A) \\leq 0\\). Substituting, \\(\\left( 1-P(A) \\right) P(A) \\geq0\\) or \\(0\\leq P(A) \\leq 1\\), which is the first axiom of probability. The second axiom, that the set of all possible outcomes has probability \\(1\\), is similarly straightforward to show.\nThe third axiom is that probabilities add, that is, for two disjoint events \\(A_{1}\\) and \\(A_{2}\\), \\(P(A) =P\\left( A_{1} \\text{ or } A_{2}\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\). Assuming stakes sizes of \\(S_{A}\\), \\(S_{A_{1}},\\) and \\(S_{A_{2}}\\) (and zero stakes on their complements) there are three possible outcomes. If neither \\(A_{1}\\) nor \\(A_{2}\\) occur, the gain is \\[\nG(\\bar A)  =-P(A)  S_{A} -P\\left(  A_{1}\\right)  S_{A_{1}}-P\\left( A_{2}\\right)  S_{A_{2}}.\n\\]\nIf \\(A_{1}\\) occurs, \\(A\\) also occurs, and the gain is \\[\nG\\left(  A_{1}\\right)  =\\left(  1-P(A) \\right)  S_{A}+\\left(  1-P\\left(  A_{1}\\right)  \\right) S_{A_{1}}-P\\left(  A_{2}\\right)  S_{A_{2}},\n\\] and finally if \\(A_{2}\\) occurs, \\(A\\) also occurs, and \\[\nG\\left(  A_{2}\\right)  =\\left(  1-P(A) \\right)  S_{A}-P\\left(  A_{1}\\right)  S_{A_{1}}+\\left( 1-P\\left(  A_{2}\\right)  \\right)  S_{A_{2}}.\n\\] Arranging these into a matrix equation, \\(G=PS\\): \\[\n\\left( \\begin{array}\n[c]{c}%\nG(\\bar A) \\\\\nG\\left(  A_{1}\\right) \\\\\nG\\left(  A_{2}\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P(A)  & -P\\left(  A_{1}\\right)  &\n-P\\left(  A_{2}\\right) \\\\\n1-P(A)  & 1-P\\left(  A_{1}\\right)  &\n-P\\left(  A_{2}\\right) \\\\\n1-P(A)  & -P\\left(  A_{1}\\right)  &\n1-P\\left(  A_{2}\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{A}\\\\\nS_{A_{1}}\\\\\nS_{A_{2}}%\n\\end{array}\n\\right).\n\\]\nThe absence of a Dutch book arbitrage implies that there is no set of stakes, \\(S_{A}\\), \\(S_{A_{1}}\\), and \\(S_{A_{2}}\\), such that the winnings in all three events are positive. If the matrix \\(P\\) is invertible, it is possible to find stakes with positive gains. To rule this out, the determinant of \\(P\\) must be zero, which implies that \\(0=-P(A) +P\\left(A_{1}\\right) +P\\left( A_{2}\\right)\\), or \\(P\\left(A\\right) =P\\left( A_{1}\\right) +P\\left( A_{2}\\right)\\).\nThe fourth axiom is conditional probability. Consider an event \\(B\\), with \\(P\\left( B\\right) &gt;0\\), an event \\(A\\) that occurs conditional on \\(B\\), and the event that both \\(A\\) and \\(B\\) occur. The probabilities or prices of these bets are \\(P\\left( B\\right)\\), \\(P\\left( A \\mid B\\right)\\), and \\(P\\left( A \\text{ and } B\\right)\\). Consider bets with stakes \\(S_{B}\\), \\(S_{A \\mid B}\\) and \\(S_{A \\text{ and } B}\\), with the understanding that if \\(B\\) does not occur, the conditional bet on \\(A\\) is canceled. The payoffs to the events that \\(B\\) does not occur, \\(B\\) occurs but not \\(A\\), and \\(A\\) and \\(B\\) occur, are \\[\n\\left( \\begin{array}\n[c]{c}%\nG\\left(  \\bar B\\right) \\\\\nG\\left(  \\bar A \\text{ and } B\\right) \\\\\nG\\left(  A \\text{ and } B\\right)\n\\end{array}\n\\right)  =\\left( \\begin{array}\n[c]{ccc}%\n-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  & 0\\\\\n1-P\\left(  B\\right)  & -P\\left(  A \\text{ and } B\\right)  &\n-P\\left(  A \\mid B\\right) \\\\\n1-P\\left(  B\\right)  & 1-P\\left(  A \\text{ and } B\\right)  &\n1-P\\left(  A \\mid B\\right)\n\\end{array}\n\\right)  \\left( \\begin{array}\n[c]{c}%\nS_{B}\\\\\nS_{A \\text{ and } B}\\\\\nS_{A \\mid B}%\n\\end{array}\n\\right).\n\\] Similar arguments imply the determinant must be zero, which implies that \\[\nP\\left(  A \\mid B\\right)  =\\frac{P\\left(  A \\text{ and } B\\right) }{P\\left(  B\\right)  },\n\\] which is the law of conditional probability, given \\(P(B)&gt;0\\), of course, otherwise the conditional probability is not defined, and the \\(P\\) matrix has determinant 0.\n\n\n\n\nBachelier, Louis. 1900. “Théorie de La Spéculation.” PhD thesis, Paris: Université de Paris.\n\n\nde Finetti, Bruno. 1937. “Foresight: Its Logical Laws, Its Subjective Sources.” In Studies in Subjective Probability, edited by Henry E. Kyburg and Howard E. Smokler, 93–158. New York: Wiley.\n\n\nDiaconis, Persi, and Frederick and Mosteller. 1989. “Methods for Studying Coincidences.” Journal of the American Statistical Association 84 (408): 853–61.\n\n\nKolmogoroff, Andrei. 1933. Grundbegriffe Der Wahrscheinlichkeitsrechnung. Vol. 2. Ergebnisse Der Mathematik Und Ihrer Grenzgebiete. Berlin: Springer.\n\n\nKolmogorov, Andrey N. 1933. Grundbegriffe Der Wahrscheinlichkeitsrechnung. Berlin: Springer.\n\n\nPoincaré, Henri. 1952. Science and Hypothesis. New York]: Dover Publications.\n\n\nPolson, Nick, and Vadim Sokolov. 2025. “Negative Probability.” Applied Stochastic Models in Business and Industry 41 (1): e2910.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” Histoy of {{Economic Thought Chapters}}. McMaster University Archive for the History of Economic Thought.\n\n\nShimony, Abner. 1955. “Coherence and the Axioms of Confirmation.” The Journal of Symbolic Logic 20 (1): 1–28.\n\n\nVille, Jean. 1939. “Étude Critique de La Notion de Collectif.” Thèses de l’entre-Deux-Guerres. PhD thesis, Université de Paris.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability and Uncertainty</span>"
    ]
  },
  {
    "objectID": "02-bayes.html",
    "href": "02-bayes.html",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "2.1 Law of Total Probability\nOne of the key questions in the theory of learning is: How do you update your beliefs in the presence of new information? Bayes rule provides the answer. Conditional probability can be interpreted as updating your probability of event \\(A\\) after you have learned the new information that \\(B\\) has occurred. In this sense probability is also the language of how you’ll change opinions in the light of new evidence. For example, consider finding the probability that a die roll is odd, given the information that the number is less than 4. The intuitive answer might change once we incorporate this new evidence. \\[\nP(\\text{Odd} \\mid &lt;4) = \\frac{P(\\{1,3\\})}{P(\\{1,2,3\\})} = \\frac{2}{3}.\n\\]\nProbability rules allow us to change our mind if the facts change. For example, suppose that we have evidence \\(E = \\{ E_1 , E_2 \\}\\) consists of two pieces of information and that we are interested in identifying a cause \\(C\\). Bayes rule simply lets you calculate this conditional probability \\(P(C\\mid E_1,E_2)\\) in a sequential fashion. First, conditioning on the information contained in \\(E_1\\), lets us calculate \\[\nP( C| E_1 ) = \\frac{ P(  E_1 \\mid C ) P( C) }{ P( E_1 ) }\n\\] Then, using the posterior probability \\(P( C| E_1 )\\) as the “new” prior for the next piece of information \\(E_2\\) lets us find \\[\nP( C| E_1 , E_2 ) = \\frac{ P(  E_2 \\mid E_1 , C ) P( C \\mid E_1 ) }{ P( E_2 \\mid E_1 ) }\n\\] Hence, we see that we need assessments of the two conditional probabilities \\(P( E_1 \\mid C )\\) and \\(P( E_2 \\mid E_1 , C )\\). In many situations, the latter will be simply \\(P( E_2 \\mid C )\\) and not involve \\(E_1\\). The events \\(( E_1, E_2 )\\) will be said to be conditionally independent given \\(C\\).\nThis concept generalizes to a sequence of events where \\(E = \\{ E_1,\\ldots E_n \\}\\). When learning from data we will use this property all the time. An illustrative example will be the Black Swan problem which we discuss later.\nBayes’ rule is a fundamental concept in probability theory and statistics. It describes how to update our beliefs about an event based on new evidence. We start with an initial belief about the probability of an event (called the prior probability). We then observe some conditional information (e.g. evidence). We use Bayes’ rule to update our initial belief based on the evidence, resulting in a new belief called the posterior probability. Remember, the formula is\nwhere:\nThe ability to use Bayes rule sequentially is key in many applications, when we need to update our beliefs in the presence of new information. For example, Bayesian learning was used by mathematician Alan Turing in England at Bletchley Park to break the German Enigma code - a development that helped the Allies win the Second World War (Simpson 2010). Turing called his algorithm Banburismus, it is a process he invented which used sequential conditional probability to infer information about the likely settings of the Enigma machine.\nDennis Lindley argued that we should all be trained in Bayes rule and conditional probability can be simply viewed as disciplined probability accounting. Akin to how market odds change as evidence changes. However, human intuition is rarely naturally calibrated for Bayesian reasoning; it is a skill that must be learned, much like literacy.\nThe Law of Total Probability is a fundamental rule relating marginal probabilities to conditional probabilities. It’s particularly useful when you’re dealing with a set of mutually exclusive and collectively exhaustive events.\nSuppose you have a set of events \\(B_1, B_2, ..., B_n\\) that are mutually exclusive (i.e., no two events can occur at the same time) and collectively exhaustive (i.e., at least one of the events must occur). The Law of Total Probability states that for any other event \\(A\\), the probability of \\(A\\) occurring can be calculated as the sum of the probabilities of \\(A\\) occurring given each \\(B_i\\), multiplied by the probability of each \\(B_i\\) occurring.\nMathematically, it is expressed as: \\[\nP(A) = \\sum_{i=1}^{n} P(A\\mid  B_i) P(B_i)\n\\]\nThis law is particularly useful in complex probability problems where direct calculation of probability is difficult. By breaking down the problem into conditional probabilities based on relevant events, it simplifies the calculation and helps to derive a solution.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#law-of-total-probability",
    "href": "02-bayes.html#law-of-total-probability",
    "title": "2  Bayes Rule",
    "section": "",
    "text": "Example 2.1 (Total Probability) Let’s consider a simple example to illustrate this. Suppose you have two bags of balls. Bag 1 contains 3 red and 7 blue balls, while Bag 2 contains 6 red and 4 blue balls. You randomly choose one of the bags and then randomly draw a ball from that bag. What is the probability of drawing a red ball?\nHere, the events \\(B_1\\) and \\(B_2\\) can be choosing Bag 1 and Bag 2, respectively. You want to find drawing a red ball (event \\(A\\)).\nApplying the law:\n\n\\(P(A\\mid B_1)\\) is the probability of drawing a red ball from Bag 1, which is \\(\\frac{3}{10}\\).\n\\(P(A\\mid B_2)\\) is the probability of drawing a red ball from Bag 2, which is \\(\\frac{6}{10}\\).\nAssume the probability of choosing either bag is equal, so \\(P(B_1) = P(B_2) = \\frac{1}{2}\\).\n\nUsing the Law of Total Probability: \\[\nP(A) = P(A\\mid B_1) \\times P(B_1) + P(A\\mid B_2) \\times P(B_2)= \\frac{3}{10} \\times \\frac{1}{2} + \\frac{6}{10} \\times \\frac{1}{2} = \\frac{9}{20}\n\\]\nSo, the probability of drawing a red ball in this scenario is \\(\\frac{9}{20}\\).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#intuition-and-simple-examples",
    "href": "02-bayes.html#intuition-and-simple-examples",
    "title": "2  Bayes Rule",
    "section": "2.2 Intuition and Simple Examples",
    "text": "2.2 Intuition and Simple Examples\n\nExample 2.2 (Intuition) Our intuition is not well trained to make use of Bayes rule. Suppose I tell you that Steve was selected at random from a representative sample, and that he is 6 feet 2 inches tall and an excellent basketball player. He goes to the gym every day and practices hard playing basketball. Do you think Steve is a custodian at a factory or an NBA player? Most people assume Steve is an NBA player which is wrong. The ratio of NBA players to custodians is very small, probabilistically Steve is more likely to be a custodian. Let’s look at it graphically. The key is to provide the right conditioning and to consider the prior probability! Even though the ratio of people who practice basketball hard is much higher among NBA players (it is 1) when compared to custodians, the larger number of the population means we still have more custodians in the US than NBA players.\n\\[\\begin{align*}\nP(\\text{Practice hard}  \\mid  \\text{Play in NBA}) \\approx  1\\\\\nP( \\text{Play in NBA}  \\mid  \\text{Practice hard}) \\approx  0.\n\\end{align*}\\]\nEven though you practice hard, the odds of playing in the NBA are low (\\(1000\\) players out of \\(7\\) billion). But given you’re in the NBA, you no doubt practice very hard. To understand this further, let’s look at the conditional probability implication and apply Bayes rule \\[\nP \\left ( \\text{Play in NBA}  \\mid  \\text{Practice hard}  \\right ) = \\dfrac{P  \\left ( \\text{Practice hard}  \\mid  \\text{Play in NBA} \\right )}{P(\\text{Practice hard})}P( \\text{Play in NBA}).\n\\] This is written in the form \\[\n\\text{Posterior} = \\frac{\\text{Likelihood}}{\\text{Marginal}}\\times \\text{Prior} = \\text{Bayes Factor} \\times \\text{Prior}.\n\\] The Likelihood/Marginal ratio is called the Bayes Factor. As we will see in the text, one of the key advantages over classical is the ability to sequentially update our beliefs as new evidence appears. It allows for disciplined probability accounting in “real-time”. With the advent of prediction markets and data science, it has become increasingly important to be able to update our beliefs as new evidence appears.\nThe initial (a.k.a. prior) probability \\(P(\\text{Play in NBA} ) = 450/(8 \\cdot 10^9) = 5.625 \\times 10^{-8}\\). This assumes a global population of around 8 billion, making the conditional (or, so called, posterior) probability also very small. \\[\nP   \\left ( \\text{Play in NBA}  \\mid  \\text{Practice hard}  \\right ) \\approx  0,\n\\] \\(P(\\text{practice hard})\\) is not that small and \\(P(\\text{practice hard} \\mid \\text{play in NBA})=1\\). Hence, when one ‘reverses the conditioning’ one gets a very small probability. This makes sense!\nThe Steve example illustrates how our intuition fails us, but let’s consider an even more striking case that demonstrates the power of Bayes rule with extreme probabilities. Consider the question: what is the probability that a randomly selected 7-foot-tall American male plays in the NBA?\nMost people’s intuition suggests this probability should be quite high - after all, being exceptionally tall seems like the primary qualification for professional basketball. However, Bayes rule reveals a more nuanced picture that depends critically on the base rates involved.\nTo calculate \\(P(\\text{NBA player} \\mid \\text{7 feet tall})\\) using Bayes rule, we need to carefully estimate each component:\n\\[P(\\text{NBA} \\mid \\text{7ft}) = \\frac{P(\\text{7ft} \\mid \\text{NBA}) \\times P(\\text{NBA})}{P(\\text{7ft})}\\]\nThe prior probability \\(P(\\text{NBA})\\) represents the baseline chance of being an NBA player. With approximately 450 active players drawn from roughly 40 million American males of playing age, this gives us \\(P(\\text{NBA}) \\approx 1.1 \\times 10^{-5}\\) - an extraordinarily small number.\nThe likelihood \\(P(\\text{7ft} \\mid \\text{NBA})\\) asks what fraction of NBA players are 7 feet or taller. Historically, this has been around 17% of the league, so \\(P(\\text{7ft} \\mid \\text{NBA}) \\approx 0.17\\).\nThe marginal probability \\(P(\\text{7ft})\\) requires us to estimate how rare 7-foot-tall men are in the general population. Male height follows approximately a normal distribution with mean 69 inches and standard deviation 3 inches. At 84 inches (7 feet), we’re looking at a z-score of 5.0, which corresponds to roughly 1 in 3.5 million men, giving us \\(P(\\text{7ft}) \\approx 2.87 \\times 10^{-7}\\).\nApplying Bayes rule: \\[P(\\text{NBA} \\mid \\text{7ft}) = \\frac{0.17 \\times 1.1 \\times 10^{-5}}{2.87 \\times 10^{-7}} \\approx 0.065\\]\nThis yields approximately 6.5% - a dramatic increase from the baseline probability of 0.001%, yet still surprisingly low given our intuitions.\nLet’s also consider a direct count-based calculation. As of September 2025, there are 39 players in the NBA who are 7 feet tall or taller, out of a total of 450 NBA players. This means the probability that a randomly selected NBA player is at least 7 feet tall is:\n\\[\nP(\\text{7ft} \\mid \\text{NBA}) = \\frac{39}{450} = 0.0867\n\\]\nThis empirical estimate is slightly different from the earlier historical average, but it still highlights the rarity of extreme height even among elite basketball players.\nRegardless of the exact calculation, this example powerfully demonstrates how Bayes rule forces us to account for base rates. Even when height provides enormous predictive value for NBA success (the likelihood ratio is massive), the extreme rarity of both 7-foot-tall individuals and NBA players means that most 7-footers will not be professional basketball players. This counterintuitive result exemplifies why disciplined probabilistic reasoning through Bayes rule is essential for making accurate inferences in the presence of rare events.\n\n\nExample 2.3 (Craps) Craps is a fast-moving dice game with a complex betting layout. It’s highly volatile, but eventually your bankroll will drift towards zero. Let’s look at the pass line bet. The expectation \\(E(X)\\) governs the long run. When 7 or 11 comes up, you win. When 2, 3 or 12 comes up, this is known as “craps”, you lose. When 4, 5, 6, 8, 9 or 10 comes up, this number is called the “point”, the bettor continues to roll until a 7 (you lose) or the point comes up (you win).\nWe need to know the probability of winning. The pay-out, probability and expectation for a $1 bet\n\n\n\nWin\nProb\n\n\n\n\n1\n0.4929\n\n\n-1\n0.5071\n\n\n\nThis leads to an edge in favor of the house as \\[\nE(X) = 1 \\cdot 0.4929 + (- 1) \\cdot  0.5071 = -0.014\n\\] The house has a 1.4% edge.\nTo calculate the probability of winning: \\(P( \\text{Win} )\\) let’s use the law of total probability \\[\nP( \\text{Win} ) = \\sum_{ \\mathrm{Point} } P ( \\text{Win} \\mid \\mathrm{Point} ) P ( \\mathrm{Point} )\n\\] The set of \\(P( \\mathrm{Point} )\\) are given by\n\n\n\n\n\n\nValue\nProbability\nPercentage\n\n\n\n\n2\n1/36\n2.78%\n\n\n3\n2/36\n5.56%\n\n\n4\n3/36\n8.33%\n\n\n5\n4/36\n11.1%\n\n\n6\n5/36\n13.9%\n\n\n7\n6/36\n16.7%\n\n\n\n\n\n\n\n\nValue\nProbability\nPercentage\n\n\n\n\n8\n5/36\n13.9%\n\n\n9\n4/36\n11.1%\n\n\n10\n3/36\n8.33%\n\n\n11\n2/36\n5.56%\n\n\n12\n1/36\n2.78%\n\n\n\n\n\n\nThe conditional probabilities \\(P( \\text{Win} \\mid \\mathrm{Point} )\\) are harder to calculate \\[\nP( \\text{Win} \\mid 7 \\; \\mathrm{or} \\; 11 ) = 1 \\; \\; \\mathrm{and} \\; \\; P( \\text{Win} \\mid 2 ,\n3 \\; \\mathrm{or} \\; 12 ) = 0\n\\] We still have to work out all the probabilities of winning given the point. Suppose the point is \\(4\\) \\[\nP( \\text{Win} \\mid 4 ) = P ( 4 \\; \\mathrm{before} \\; 7 ) = \\dfrac{P(4)}{P(7)+P(4)} = \\frac{3}{9} =\n\\frac{1}{3}\n\\] There are 6 ways of getting a 7, 3 ways of getting a 4 for a total of 9 possibilities. Now do all of them and sum them up. You get \\[\nP( \\text{Win}) = 0.4929\n\\] Hence, the casino has a slight edge of 0.71% in the long run!\n\n\nExample 2.4 (Coin Jar) Large jar containing 1024 fair coins and one two-headed coin. You pick one at random and flip it \\(10\\) times and get all heads. What’s the probability that the coin is the two-headed coin? The probability of initially picking the two headed coin is 1/1025. There is a 1/1024 chance of getting \\(10\\) heads in a row from a fair coin. Therefore, it’s a \\(50/50\\) bet.\nLet’s do the formal Bayes rule math. Let \\(E\\) be the event that you get \\(10\\) Heads in a row, then\n\\[\nP \\left ( \\mathrm{two \\; headed}  \\mid  E \\right ) = \\frac{ P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left (  \\mathrm{ two \\; headed} \\right )}\n{P \\left ( E  \\mid  \\mathrm{ fair}  \\right )P \\left ( \\mathrm{ fair} \\right ) + P \\left ( E  \\mid  \\mathrm{ two \\; headed}  \\right )P \\left ( \\mathrm{ two \\; headed} \\right )}\n\\] Therefore, the posterior probability \\[\nP \\left (  \\mathrm{two \\; headed}  \\mid  E \\right ) = \\frac{ 1 \\times \\frac{1}{1025} }{ \\frac{1}{1024} \\times \\frac{1024}{1025} + 1 \\times \\frac{1}{1025} } = 0.50\n\\] What’s the probability that the next toss is a head? Using the law of total probability gives\n\\[\\begin{align*}\n  P( H ) &= P( H  \\mid  \\mathrm{ two \\; headed} )P( \\mathrm{ two \\; headed}  \\mid E ) +  P( H  \\mid  \\mathrm{ fair} )P( \\mathrm{ fair}  \\mid E) \\\\\n  & = 1 \\times \\frac{1}{2} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{3}{4}\n\\end{align*}\\]\n\n\nExample 2.5 (Monty Hall Problem) Another example of a situation when calculating probabilities is counterintuitive. The Monty Hall problem was named after the host of the long-running TV show Let’s Make a Deal. The original solution was proposed by Marilyn vos Savant, who had a column with the correct answer that many Mathematicians thought was wrong!\nThe game set-up is as follows. A contestant is given the choice of 3 doors. There is a prize (a car, say) behind one of the doors and something worthless behind the other two doors: two goats. The game is as follows:\n\nYou pick a door.\nMonty then opens one of the other two doors, revealing a goat. He can’t open your door or show you a car\nYou have the choice of switching doors.\n\nThe question is, is it advantageous to switch? The answer is yes. The probability of winning if you switch is 2/3 and if you don’t switch is 1/3.\nConditional probabilities allow us to answer this question. Assume you pick door 2 (event \\(A\\)) at random, given that the host opened Door 3 and showed a goat (event B), we need to calculate \\(P(A\\mid B)\\). The prior probability that the car is behind Door 2 is \\(P(A) =  1/3\\) and \\(P(B\\mid A) = 1\\), if the car is behind Door 2, the host has no choice but to open Door 3. The Bayes rule then gives us \\[\nP(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)} = \\frac{1/3}{1/2} = \\frac{2}{3}.\n\\] The overall probability of the host opening Door 3 \\[\nP(B) = (1/3 \\times 1/2) + (1/3 \\times 1) = 1/6 + 1/3 = 1/2.\n\\]\nThe posterior probability that the car is behind Door 2 after the host opens Door 3 is 2/3. It is to your advantage to switch doors.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#real-world-bayes",
    "href": "02-bayes.html#real-world-bayes",
    "title": "2  Bayes Rule",
    "section": "2.3 Real World Bayes",
    "text": "2.3 Real World Bayes\n\nSearch and Rescue\n\nExample 2.6 (USS Scorpion sank 5 June, 1968 in the middle of the Atlantic.) Experts placed bets on each casualty and how each would affect the sinking. Undersea soundings gave a prior on location. Bayes rule: \\(L\\) is location and \\(S\\) is scenario \\[\nP (L \\mid S) = \\frac{ P(S \\mid L) P(L)}{P(S)}\n\\] The Navy spent \\(5\\) months looking and found nothing. Built a probability map: within \\(5\\) days, the submarine was found within \\(220\\) yards of the most likely probability!\nA similar story happened during the search of an Air France plane that flew from Rio to Paris.\n\n\nExample 2.7 (Wald and Airplane Safety) Many lives were saved by analysis of conditional probabilities performed by Abraham Wald during the Second World War. He was analyzing damages on the US planes that came back from bombing missions in Germany. Somebody suggested to analyze the distribution of the hits over different parts of the plane. The idea was to find a pattern in the damages and design a reinforcement strategy.\nAfter examining hundreds of damaged airplanes, researchers came up with the following table\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n53\n\n\nCockpit\n65\n\n\nFuel system\n96\n\n\nWings, fuselage, etc.\n434\n\n\n\nWe can convert those counts to probabilities\n\n\n\nLocation\nNumber of Planes\n\n\n\n\nEngine\n0.08\n\n\nCockpit\n0.1\n\n\nFuel system\n0.15\n\n\nWings, fuselage, etc.\n0.67\n\n\n\nWe can conclude that the most likely area to be damaged on the returned planes was the wings and fuselage. \\[\nP(\\mbox{hit on wings or fuselage } \\mid \\mbox{returns safely}) = 0.67\n\\] Wald realized that analyzing damages only on survived planes is not the right approach. Instead, he suggested that it is essential to calculate the inverse probability \\[\nP(\\mbox{returns safely} \\mid \\mbox{hit on wings or fuselage }) = ?\n\\] To calculate that, he interviewed many engineers and pilots, he performed a lot of field experiments. He analyzed likely attack angles. He studied the properties of a shrapnel cloud from a flak gun. He suggested to the army that they fire thousands of dummy bullets at a plane sitting on the tarmac. Wald constructed a ‘probability model’ carefully to reconstruct an estimate for the joint probabilities. The table below shows the results.\n\n\n\nHit\nReturned\nShot Down\n\n\n\n\nEngine\n53\n57\n\n\nCockpit\n65\n46\n\n\nFuel system\n96\n16\n\n\nWings, fuselage, etc.\n434\n33\n\n\n\nWhich allows us to estimate joint probabilities, for example \\[\nP(\\mbox{outcome = returns safely} , \\mbox{hit  =  engine }) = 53/800 = 0.066\n\\] We also can calculate the conditional probabilities now \\[\nP(\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  wings or fuselage  }) = \\dfrac{434}{434+33} = 0.9293362.\n\\] Should we reinforce wings or fuselage? Which part of the airplane needs to be reinforced? \\[\nP(\\mbox{outcome = returns safely} \\mid  \\mbox{hit  =  engine  }) = \\dfrac{53}{53+57} = 0.48\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#legal-and-forensic-applications",
    "href": "02-bayes.html#legal-and-forensic-applications",
    "title": "2  Bayes Rule",
    "section": "2.4 Legal and Forensic Applications",
    "text": "2.4 Legal and Forensic Applications\n\n\n\n\n\n\nNoteBayes Rule in Odds Form\n\n\n\nA convenient way to perform Bayes rule calculations is using the “odds” form. \\[\n\\underbrace{O(H \\mid E)}_{\\text{Posterior Odds}} = \\underbrace{\\frac{P(E \\mid H)}{P(E \\mid \\neg H)}}_{\\text{Bayes Factor}} \\times \\underbrace{O(H)}_{\\text{Prior Odds}}\n\\] where the odds of an event \\(H\\) are defined as \\(O(H) = \\frac{P(H)}{P(\\neg H)}\\). This separates the contribution of the evidence (Bayes Factor) from the prior belief.\n\n\n\nExample 2.8 (The Prosecutor’s Fallacy) The Prosecutor’s Fallacy is a logical error that occurs when a prosecutor presents evidence or statistical data in a way that suggests a defendant’s guilt, even though the evidence is not as conclusive as it may seem. This fallacy arises from confounding conditional probabilities, specifically equating the probability of evidence given guilt \\(P(E|G)\\) with the probability of guilt given evidence \\(P(G|E)\\). \\[\nP(E\\mid G) \\ne P(G\\mid E)\n\\] A classic example involves DNA evidence. Suppose you’re serving on a jury in a city with a population of 10 million. A defendant is accused of a crime based on a DNA match found at the scene. A forensic scientist testifies that the probability of an innocent person’s DNA matching the sample is one in a million (\\(P(E|\\bar G) = 10^{-6}\\)).\nThe prosecutor argues that this means there is only a one in a million chance the defendant is innocent. This is the fallacy. You are charged with assessing \\(P(G \\mid E)\\) - the probability of guilt given the match.\nLet’s apply Bayes’ rule. - \\(P(G) \\approx 1/10^7\\) (Prior probability, assuming random citizen). - \\(P(E|\\bar G) = 10^{-6}\\) (False positive rate). - \\(P(E|G) \\approx 1\\) (Sensitivity).\n\\[\nP(G\\mid E) = \\frac{P(E\\mid G)P(G)}{P(E\\mid G)P(G) + P(E\\mid \\bar G)P(\\bar G)}\n\\] \\[\nP(G\\mid E) \\approx \\frac{1 \\cdot 10^{-7}}{1 \\cdot 10^{-7} + 10^{-6} \\cdot 1} = \\frac{10^{-7}}{1.1 \\times 10^{-6}} \\approx \\frac{1}{11} \\approx 0.09\n\\]\nDespite the “one in a million” match rarity, the probability of guilt is only about 9%! There are 10 million people, so we expect about 10 innocent matches (\\(10^7 \\times 10^{-6}\\)) and 1 guilty match. Thus, out of 11 matches, only 1 is guilty. The prosecutor’s argument ignores the base rate.\n\n\nExample 2.9 (Island Problem) There are \\(N+1\\) people on the island and one is a criminal. We have probability of a trait of a criminal equal to \\(p\\), which is \\(p = P(E\\mid I)\\), the probability of evidence, given innocence. Then we have a suspect who is matching the trait and we need to find probability of being guilty, given the evidence \\(P(G \\mid E)\\). It is easier to do the Bayes rule in the odds form. There are three components to the calculations: the prior odds of innocence, \\[\nO ( I ) = P (G) / P ( I ),\n\\] the Bayes factor, \\[\n\\frac{P(E\\mid G)}{P(E\\mid I)}.\n\\] and the posterior odds of innocence. \\[\n    O(I\\mid E) = \\dfrac{P(G\\mid E)}{P(I\\mid E)} = \\dfrac{1}{Np}.\n\\]\nCromwell’s rule states that the use of prior probability of 1 or 0 should be avoided except when it is known for certain that the probability is 1 or 0. It is named after Oliver Cromwell who wrote to the General Assembly of the Church of Scotland in 1650 “I beseech you, in the bowels of Christ, think it possible that you may be mistaken”. In other words, using the Bayes rule \\[\nP(G\\mid E) = \\dfrac{P(E\\mid G)}{P(E)}P(G),\n\\] if \\(P(G)\\) is zero, it does not matter what the evidence is. Symmetrically, probability of innocence is zero if the evidence is certain. In other words, if \\(P(E\\mid I) = 0\\), then \\(P(I\\mid E) = 0\\). This is a very strong statement. It is not always true, but it is a good rule of thumb, it is a good way to avoid the prosecutor’s fallacy.\n\n\nExample 2.10 (Nakamura’s Alleged Cheating) In our paper Maharaj, Polson, and Sokolov (2023), we provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex-world champion) and Hikaru Nakamura. Kramnik called into question Nakamura’s 45.5 out of 46 win streak in a 3+0 online blitz contest at chess.com. In this example we reproduce this paper and assess the weight of evidence using an a priori probabilistic assessment of Viswanathan Anand and the streak evidence of Kramnik. Our analysis shows that Nakamura has a 99.6 percent chance of not cheating given Anand’s prior assumptions.\nWe start by addressing the argument of Kramnik which is based on the fact that the probability of such a streak is very small. This falls into precisely the Prosecutor’s Fallacy, as discussed in Example 2.8. We denote by \\(G\\) the event of being guilty and \\(I\\) the event of innocence. We use \\(E\\) to denote evidence (streak of wins). Kramnik’s argument is that because the probability of observing the streak is very low (\\(P(E|I)\\) is small), it implies a high probability of cheating (\\(P(G|E)\\) is high). As we have seen, this reasoning is flawed because it neglects the prior probability of cheating. Kramnik’s calculations neglect other relevant factors, such as the prior probability of cheating. The prosecutor’s fallacy can lead to an overestimation of the strength of the evidence and may result in an unjust conviction. In the cheating problem, at the top level of chess the prior probability of \\(P(G)\\) is small! According to a recent statement by Viswanathan Anand, the probability of cheating is \\(1/10000\\).\n\n\n\nAnand’s Prior\n\n\nGiven the prior ratio of cheaters to not cheaters is \\(1/N\\), meaning out of \\(N+1\\) players, there is one cheater, the Bayes calculations require two main terms. The first one is the prior odds of guilt: \\[\nO ( G ) = P (I) / P ( G ).\n\\] Here \\(P(I)\\) and \\(P(G)\\) are the prior probabilities of innocence and guilt respectively.\nThe second term is the Bayes factor, which is the ratio of the probability of the evidence under the guilt hypothesis to the probability of the evidence under the innocence hypothesis. The Bayes factor is given by \\[\n    L(E\\mid G) = \\frac{P(E\\mid I)}{P(E\\mid G)}.\n\\]\nThe product of the Bayes factor and the prior odds is the posterior odds of guilt, given the evidence. The posterior odds of guilt is given by \\[\n    O(G\\mid E) = O(G) \\times L(E\\mid G).\n\\]\nThe odds of guilt is \\[\n    O ( G )  = \\dfrac{N/(N+1)}{1/(N+1)} = N.\n\\]\nThe Bayes factor is given by \\[\n\\frac{P(E\\mid I)}{P(E\\mid G)} = \\dfrac{p}{1} = p.\n\\] Thus, the posterior odds of guilt are \\[\n    O(G\\mid E) = Np.\n\\] There are two numbers we need to estimate to calculate the odds of cheating given the evidence, namely the prior probability of cheating given via \\(N\\) and the probability of a streak \\(p = P(E\\mid I)\\).\nThere are multiple ways to calculate the probability of a streak. We can use the binomial distribution, the negative binomial distribution, or the Poisson distribution. The binomial distribution is the most natural choice. The probability of a streak of \\(k\\) wins in a row is given by \\[\n    P(E\\mid I) = \\binom{N}{k} q^k (1-q)^{N-k}.\n\\] Here \\(q\\) is the probability of winning a single game. Thus, for a streak of 45 wins in a row, we have \\(k = 45\\) and \\(N = 46\\). We encode the outcome of a game as \\(1\\) for a win and \\(0\\) for a loss or a draw. The probability of a win is \\(q = 0.8916\\) (Nakamura’s Estimate, he reported on his YouTube channel). The probability of a streak is then 0.029. The individual game win probability is calculated from the ELO rating difference between the players.\nThe ELO rating of Hikaru is 3300 and the average ELO rating of his opponents is 2950, according to Kramnik. The difference of 350 corresponds to the odds of winning of \\(wo = 10^{350/400} = 10^{0.875} = 7.2\\). The probability of winning a single game is \\(q = wo/(1+wo) = 0.8916\\).\nThen we use Anand’s prior of \\(N = 10000\\) to get the posterior odds of cheating given the evidence of a streak of 45 wins in a row. The posterior odds of being innocent are 285. The probability of cheating is then \\[\nP(G\\mid E) = 1/(1+O(G\\mid E)) = 0.003491.\n\\] Therefore the probability of innocence \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9965.\n\\]\nFor completeness, we perform sensitivity analysis and also get the odds of not cheating for \\(N = 500\\), which should be a high prior probability given the status of the player and the importance of the event. We get \\[\n    P(I\\mid E) = \\frac{Np}{Np+1} = 0.9445.\n\\]\nThere are several assumptions we made in this analysis.\n\nInstead of calculating game-by-game probability of winning, we used the average probability of winning of 0.8916, provided by Nakamura himself. This is a reasonable assumption given the fact that Nakamura is a much stronger player than his opponents. This assumption slightly shifts posterior odds in favor of not cheating. Due to Jensen’s inequality, we have \\(E(q^{50}) &gt; E(q)^{50}\\). Expected value of the probability of winning a single game is \\(E(q) = 0.8916\\) and the expected value of the probability of a streak of 50 wins is \\(E(q^{50})\\). We consider the difference between the two to be small. Further, there is some correlation between the games, which also shifts the posterior odds in favor of not cheating. For example, some players are on tilt. Given they lost the first game, they are more likely to lose the second game.\nThere are many ways to win 3+0 unlike in classical chess. For example, one can win on time. We argue that the probability of winning calculated from the ELO rating difference is underestimated.\n\nNext, we can use the Bayes analysis to solve an inverse problem and to find what prior you need to assume and how long of a sequence you need to observe to get 0.99 posterior? Small sample size, we have \\(p\\) close to 1. Figure 2.1 shows the combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\nIndeed, the results of the Bayesian analysis contradict the results of a traditional p-value based approach. A p-value is a measure used in frequentist statistical hypothesis testing. It represents the probability of obtaining the observed results, or results more extreme, assuming that the null hypothesis is true. The null hypothesis is a default position that Nakamura is not cheating and we compare the ELO-based expected win probability of \\(q=0.8916\\) to the observed one of \\(s=45/46=0.978\\). Under the null hypothesis, Nakamura should perform at the level predicted by \\(q\\).\n\nq = 0.8916\np = dbinom(45,46,q)\nN = 10000\nodds = p*N\nprint(1-1/(1+odds))\n## 1\nprint(1/(1+odds))\n## 0.0035\nprint(N*p/(N*p+1))\n## 1\n\n\np = seq(from=0.006, to=0.07, length.out=500)\nN = seq(500,10000, by=250)\nplot(99/N,N,xlab=\"p\", ylab=\"N\", type='l', lwd=3, col=\"blue\")\n\n\n\n\n\n\n\nFigure 2.1: The combination of prior (\\(N\\)) and the probability of a streak (\\(p\\)) that gives posterior odds of 0.99.\n\n\n\n\n\n\nDavid Hume discussed the problem similar to the Island problem in his “On Miracles” essay. Hume is making the following argument on miracles:\n\n“…no testimony is sufficient to establish a miracle, unless the testimony be of such a kind, that its falsehood would be more miraculous, than the fact, which it endeavors to establish; and even in that case there is a mutual destruction of arguments, and the superior only gives us an assurance suitable to that degree of force, which remains, after deducting the inferior.”\n\nOne can view this as an application of the Island problem. Assuming the probability of a miracle \\(A\\) is \\(P( A) = p\\) and \\(P( \\text{not } A ) = 1 -p\\). Then Bayes rule gives \\[\nP( A| a ) = \\frac{ P( a| A) p }{  P( a| A) p  +  P( a |  \\text{not } A) (1-p) }  \n\\] Prosecutor’s fallacy, \\(P( a| \\text{not } A)  \\neq 1 -  P( a| A)\\), in general.\nIn Hume’s assessment of miracles (has to be something not in the laws of nature) we have \\(P(A) = 10^{-6}\\). This assessment takes into account background information, \\(I\\). Rare to have a contradiction to the laws of nature. More informative to write \\(P( A | I )\\). Furthermore, we take \\(P( a| A) =0.99\\). The hardest bit is to assess \\(P(a | \\text{not } A )\\). The “frequency” of faked miracles and mankind’s propensity to be marvelous. We assess \\(P(a | \\text{not } A )  = 10^{-3}\\). This yields the chance of a miracle to be unlikely as \\[\nP( A| a ) = \\frac{ 0.99 \\times 10^{-6}  }{  0.99 \\times 10^{-6}    +  10^{-3} (1- 10^{-6}) }   \\approx 10^{-3}.\n\\] Feynman considers the inverse problem: can we learn the laws of nature purely from empirical observation? Uses chess as an example. Is it a miracle that we have two bishops of the same color? No! according to Hume. We just didn’t know the laws of nature (a.k.a. model).\n\nExample 2.11 (Sally Clark Case: Independence or Bayes Rule?) To show that independence can lead to dramatically different results from Bayes conditional probabilities, consider the Sally Clark case. Sally Clark was accused and convicted of killing her two children who could have both died of SIDS. One explanation is that this was a random occurrence, the other one is that they both died of sudden infant death syndrome (SIDS). How can we use conditional probability to figure out a reasonable assessment of the probability that she murdered her children. First, some known probability assessments\n\nThe chance of a family of non-smokers having a SIDS death is \\(1\\) in \\(8,500\\).\nThe chance of a second SIDS death is \\(1\\) in \\(100\\).\nThe chance of a mother killing her two children is around \\(1\\) in \\(1,000,000\\).\n\nUnder Bayes \\[\\begin{align*}\nP(\\mathrm{both} \\; \\; \\mathrm{SIDS})   &  = P(\\mathrm{first} \\; \\mathrm{SIDS}) P(\\mathrm{Second} \\; \\;\\mathrm{SIDS} \\mid \\mathrm{first} \\; \\mathrm{SIDS})\\\\\n&  = \\frac{1}{8500} \\cdot \\frac{1}{100} = \\frac{1}{850,000}.\n\\end{align*}\\]\nThe \\(1/100\\) comes from taking into account the genetic properties of SIDS. Independence, as implemented by the court, gets you to a probabilistic assessment of \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = (1/8500) (1/8500) = (1/73,000,000).\n\\] This is a low probability. It is still not the answer to our question of context. We need a conditional probability, this will come to the Bayes rule.\nFirst, some general comment on the likelihood ratio calculation used to assess the weight of evidence in favor of guilty v.s. innocent evidence. Under Bayes we’ll find that there’s reasonable evidence that she’d be acquitted. We need the relative odds ratio. Let \\(I\\) denote the event that Sally Clark is innocent and \\(G\\) denotes guilty. Let \\(E\\) denote the evidence. In most cases, \\(E\\) contains a sequence \\(E_1, E_2, \\ldots\\) of ‘facts’ and we have to use the likelihood ratios in turn. Bayes rule then tells you to combine via multiplicative fashion. If likelihood ratio \\(&gt;1\\), odds of guilty. If likelihood ratio \\(&lt;1\\), more likelihood to be \\(I\\). By Bayes rule \\[\n\\frac{P(I\\mid E)}{P(G\\mid E)} = \\frac{P( E\\text{ and } I)}{P( E\\text{ and } G)}.\n\\] If we further decompose \\(P(E \\text{ and } I) = P(E\\mid I )P(I)\\) then we have to discuss the prior probability of innocence, namely \\(P(I)\\). Hence this is one subtle advantage of the above decomposition.\nThe underlying intuition that Bayes gives us in this example, is that of the two possible explanations of the data, both of which are unlikely, it is the relative likelihood comparison that should matter. Here is a case where the \\(p\\)-value would be non-sensible (\\(P(E\\mid I) \\neq P(I\\mid E)\\)). Effectively comparing two rare event probabilities from the two possible models or explanations.\nHence putting these two together gives the odds of guilt as \\[\n\\frac{P(I\\mid E)}{P(G\\mid E)} = \\frac{1/850,000}{1/1,000,000} = 1.15.\n\\] Solving for the posterior probability yields \\(46.5\\%\\) for probability of guilty given evidence. \\[\nP( G\\mid E) = \\frac{1}{1 + O(G\\mid E)} = 0.465.\n\\] Basically a \\(50/50\\) bet. Not enough to definitively convict! But remember that our initial prior probability on guilt \\(P(G)\\) was \\(10^{-6}\\). So now there has been a dramatic increase to a posterior probability of \\(0.465\\). So it’s not as if Bayes rule thinks this is evidence in the suspect’s favor – but the magnitude is still not in the \\(0.999\\) range though, where most jurors would have to be to feel comfortable with a guilt verdict.\nIf you use the “wrong” model of independence (as the court did) you get \\[\nP \\left(  \\mathrm{both} \\; \\; \\mathrm{SIDS} \\right)  = \\frac{1}{8500}\n  \\cdot\\frac{1}{8500} = \\frac{1}{73,000,000}.\n\\] With the independence assumption, you make the assessment \\[\n\\frac{P(I\\mid E)}{P(G\\mid E)} = \\frac{1}{73} \\; \\mathrm{ and} \\; P( G\\mid E) \\approx 0.99.\n\\] Given these probability assumptions, the suspect looks guilty with probability 99%.\nExperts also mis-interpret the evidence by saying: 1 in 73 million chance that it is someone else. This is clearly false and misleading to the jury and has led to appeals.\n\n\nExample 2.12 (O. J. Simpson Case: Dershowitz Fallacy)  \n\nThis example is based on I. J. Good’s, “When batterer turns murderer.” Nature, 15 June 1995, p. 541. Alan Dershowitz, on the O. J. Simpson defense team, stated on T.V. and in newspapers that only 1 in 2,500 of men who abuse their wives go on to murder them. He clearly wanted his audience to interpret this to mean that the evidence of abuse by Simpson would only suggest a 0.04% probability of his being guilty of murdering her. He used probability to argue that because so few husbands who batter their wives actually go on to murder their wives. Thus, O.J. is highly likely to be not guilty. This leaves out the most relevant conditioning information that we also know that Nicole Brown Simpson was actually murdered. Both authors believe the jury would be more interested in the probability that the husband is guilty of the murder of his wife given that he abused his wife and his wife was murdered. They both solve this problem by using Bayes’ theorem.\nIn this example, the notation \\(B\\) represents “woman battered by her husband, boyfriend, or lover”, \\(M\\) represents the event “woman murdered”, and \\(G\\) denotes “woman murdered by her batterer”. Our goal is to show that \\[\n% P(M,B \\mid M) \\neq P(M,B \\mid B).\nP(G \\mid M,B) \\neq P(G\\mid B).\n\\]\nIt is not hard to come to a wrong conclusion if you don’t take into account all the relevant conditional information. He intended this information to exonerate O.J. In 1992 the women population of the US was 125 million and 4936 women were murdered, thus \\[\nP(M) = 4936/125,000,000 = 0.00004 = 1/25,000.\n\\] At the same year about 3.5 million women were battered \\[\nP(B) = 3.5/125 = 0.028.\n\\] That same year 1432 women were murdered by their previous batterers, so the marginal probability of that event is \\(P(G) = 1432/125,000,000 = 0.00001 = 1/87,290\\), and the conditional probability, \\(P(G | B)\\) is 1432 divided by 3.5 million, or \\(1/2444\\). These are the numbers Dershowitz used to obtain his estimate that about 1 in 2500 battered women go on to be murdered by their batterers.\nWe need to calculate \\[\nP(G \\mid M,B) = P(M | G,B) P(G) / P(M).\n\\] We know \\(P(M | G,B) = 1\\) and \\(P(G) / P(M) = 0.00001/0.00004 = 0.29\\), or about 1 in 3.5.\nAlan Dershowitz provided the jury with an accurate but irrelevant probability. The fact the woman was murdered increases the probability that she was murdered by her batterer by a factor of 709 (0.29/(1/2444)). \\[\nP(G\\mid M,B)\\approx 709\\times P(G\\mid B).\n\\]\nThe argument used by Dershowitz relating to the Simpson case has been discussed by John Paulos in an op-ed article in the Philadelphia Inquirer (15 Oct. 1995, C7) and his book “Once Upon a Number”, by I.J. Good in an article in Nature (June 15,1995, p 541) and by Jon Merz and Jonathan Caulkins in an article in Chance Magazine, (Spring 1995, p 14).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#first-application-naive-bayes",
    "href": "02-bayes.html#first-application-naive-bayes",
    "title": "2  Bayes Rule",
    "section": "2.5 First Application: Naive Bayes",
    "text": "2.5 First Application: Naive Bayes\nUse of the Bayes rule allows us to build our first predictive model, called Naive Bayes classifier. Naive Bayes is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3” in diameter. A Naive Bayes classifier considers each of these “features” (red, round, 3” in diameter) to contribute independently to the probability that the fruit is an apple, regardless of any correlations between features. Features, however, aren’t always independent which is often seen as a shortcoming of the Naive Bayes algorithm and this is why it’s labeled “naive”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor, Shape and Size of Fruits\n\n\nAlthough it’s a relatively simple idea, Naive Bayes can often outperform other more sophisticated algorithms and is extremely useful in common applications like spam detection and document classification. In a nutshell, the algorithm allows us to predict a class, given a set of features using probability. So in another fruit example, we could predict whether a fruit is an apple, orange or banana (class) based on its colour, shape etc (features). In summary, the advantages are:\n\nIt’s relatively simple to understand and build\nIt’s easily trained, even with a small dataset\nIt’s fast!\nIt’s not sensitive to irrelevant features\n\nThe main disadvantage is that it assumes every feature is independent, which isn’t always the case.\nLet’s say we have data on 1000 pieces of fruit. The fruit being a Banana, Orange or some Other fruit and imagine we know 3 features of each fruit, whether it’s long or not, sweet or not and yellow or not, as displayed in the table below:\n\n\n\nFruit\nLong\nSweet\nYellow\nTotal\n\n\n\n\nBanana\n400\n350\n450\n500\n\n\nOrange\n0\n150\n300\n300\n\n\nOther\n100\n150\n50\n200\n\n\nTotal\n500\n650\n800\n1000\n\n\n\nFrom this data we can calculate marginal probabilities\n\n50% of the fruits are bananas\n30% are oranges\n20% are other fruits\n\nBased on our training set we can also say the following:\n\nFrom 500 bananas 400 (0.8) are Long, 350 (0.7) are Sweet and 450 (0.9) are Yellow\nOut of 300 oranges 0 are Long, 150 (0.5) are Sweet and 300 (1) are Yellow\nFrom the remaining 200 fruits, 100 (0.5) are Long, 150 (0.75) are Sweet and 50 (0.25) are Yellow So let’s say we’re given the features of a piece of fruit and we need to predict the class. If we’re told that the additional fruit is Long, Sweet and Yellow, we can classify it using the following formula and subbing in the values for each outcome, whether it’s a Banana, an Orange or Other Fruit. The one with the highest probability (score) being the winner.\n\nGiven the evidence \\(E\\) (\\(L\\) = Long, \\(S\\) = Sweet and \\(Y\\) = Yellow) we can calculate the probability of each class \\(C\\) (\\(B\\) = Banana, \\(O\\) = Orange or \\(F\\) = Other Fruit) using Bayes’ Theorem: \\[\\begin{align*}\nP(B \\mid E) = & \\frac{P(L \\mid B)P(S \\mid B)P(Y \\mid B)P(B)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.8\\times 0.7\\times 0.9\\times 0.5}{P(E)}=\\frac{0.252}{P(E)}\n\\end{align*}\\]\nOrange: \\[\nP(O\\mid E)=0.\n\\]\nOther Fruit: \\[\\begin{align*}\nP(F \\mid E) & = \\frac{P(L \\mid F)P(S \\mid F)P(Y \\mid F)P(F)}{P(L)P(S)P(Y)}\\\\\n=&\\frac{0.5\\times 0.75\\times 0.25\\times 0.2}{P(E)}=\\frac{0.01875}{P(E)}\n\\end{align*}\\]\nIn this case, based on the higher score, we can assume this Long, Sweet and Yellow fruit is, in fact, a Banana.\nNotice, we did not have to calculate \\(P(E)\\) because it is a normalizing constant and it cancels out when we calculate the ratio\n\\[\n\\dfrac{P(B \\mid E)}{P(F \\mid E)} = 0.252/0.01875 = 13.44 &gt; 1.\n\\]\nNow that we’ve seen a basic example of Naive Bayes in action, you can easily see how it can be applied to Text Classification problems such as spam detection, sentiment analysis and categorization. By looking at documents as a set of words, which would represent features, and labels (e.g. “spam” and “ham” in case of spam detection) as classes we can start to classify documents and text automatically.\n\nExample 2.13 (Spam Filtering) The original spam filtering algorithm was based on Naive Bayes. The “naive” aspect of Naive Bayes comes from the assumption that inputs (words in the case of text classification) are conditionally independent, given the class label. Naive Bayes treats each word independently, and the model doesn’t capture the sequential or structural information inherent in the language. It does not consider grammatical relationships or syntactic structures. The algorithm doesn’t understand the grammatical rules that dictate how words should be combined to form meaningful sentences. Further, it doesn’t understand the context in which words appear. For example, it may treat the word “bank” the same whether it refers to a financial institution or the side of a river bank. Despite its simplicity and the naive assumption, Naive Bayes often performs well in practice, especially in text classification tasks.\nWe start by collecting a dataset of emails labeled as “spam” or “not spam” (ham) and calculate the prior probabilities of spam (\\(P(\\text{spam})\\)) and not spam (\\(P(\\text{ham})\\)) based on the training dataset, by simply counting the proportions of each in the data.\nThen each email gets converted into a bag-of-words representation (ignoring word order and considering only word frequencies). Then, we create a vocabulary of unique words from the entire dataset \\(w_1,w_2,\\ldots,w_N\\) and calculate conditional probabilities \\[\nP(\\mathrm{word}_i  \\mid  \\text{spam}) = \\frac{\\text{Number of spam emails containing }\\mathrm{word}_i}{\\text{Total number of spam emails}}, ~ i=1,\\ldots,n\n\\] \\[\nP(\\mathrm{word}_i  \\mid  \\text{ham}) = \\frac{\\text{Number of ham emails containing }\\mathrm{word}_i}{\\text{Total number of ham emails}}, ~ i=1,\\ldots,n\n\\]\nNow, we are ready to use our model to classify new emails. We do it by calculating the posterior probability using Bayes’ theorem. Say an email has a set of \\(k\\) words \\(\\text{email} = \\{w_{e1},w_{e2},\\ldots, w_{ek}\\}\\), then \\[\nP(\\text{spam}  \\mid  \\text{email}) = \\frac{P(\\text{email}  \\mid  \\text{spam}) \\times P(\\text{spam})}{P(\\text{email})}\n\\] Here \\[\nP(\\text{email}  \\mid  \\text{spam}) = P( w_{e1}  \\mid  \\text{spam})P( w_{e2}  \\mid  \\text{spam})\\ldots P( w_{ek}  \\mid  \\text{spam})\n\\] We calculate \\(P(\\text{ham} \\mid \\text{email})\\) in a similar way.\nFinally, we classify the email as spam or ham based on the class with the highest posterior probability.\nSuppose you have a spam email with the word “discount” appearing. Using Naive Bayes, you’d calculate the probability that an email containing “discount” is spam \\(P(\\text{spam} \\mid \\text{discount})\\) and ham \\(P(\\text{ham} \\mid \\text{discount})\\), and then compare these probabilities to make a classification decision.\nWhile the naive assumption simplifies the model and makes it computationally efficient, it comes at the cost of a more nuanced understanding of language. More sophisticated models, such as transformers, have been developed to address these limitations by considering the sequential nature of language and capturing contextual relationships between words.\nIn summary, naive Bayes, due to its simplicity and the naive assumption of independence, is not capable of understanding the rules of grammar, the order of words, or the intricate context in which words are used. It is a basic algorithm suitable for certain tasks but may lack the complexity needed for tasks that require a deeper understanding of language structure and semantics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#sec-sensitivity",
    "href": "02-bayes.html#sec-sensitivity",
    "title": "2  Bayes Rule",
    "section": "2.6 Sensitivity and Specificity",
    "text": "2.6 Sensitivity and Specificity\nConditional probabilities are used to define two fundamental metrics used for many probabilistic and statistical learning models, namely sensitivity and specificity.\nSensitivity and specificity are two key metrics used to evaluate the performance of diagnostic tests, classification models, or screening tools. These metrics help assess how well a test can correctly identify individuals with a condition (true positives) and those without the condition (true negatives). Let’s break down each term:\n\nSensitivity (true‐positive rate or recall) is the ability of a test \\(T\\) to correctly identify individuals who have a particular condition or disease (\\(D\\)), \\(P ( T=1 \\mid D=1 )\\), the probability of a positive test given that the individual has the disease. It is calculated as the ratio of true positives to the sum of true positives and false negatives. \\[\nP(T=1\\mid D=1) = \\dfrac{P(T=1,D=1)}{P(D=1)}.\n\\] A high sensitivity indicates that the test is good at identifying individuals with the condition, minimizing false negatives.\nSpecificity (true‐negative rate) is the ability of a test to correctly identify individuals who do not have a particular condition or disease, \\(P (T=0 \\mid D=0 )\\). It is calculated as the ratio of true negatives to the sum of true negatives and false positives. \\[\nP(T=0\\mid D=0) = \\dfrac{P(T=0,D=0)}{P(D=0)}\n\\] A high specificity indicates that the test is good at correctly excluding individuals without the condition, minimizing false positives.\n\nSensitivity and specificity are often trade-offs. Increasing sensitivity might decrease specificity, and vice versa. Thus, depending on the application, you might prefer sensitivity over specificity or vice versa, depending on the consequences of false positives and false negatives in a particular application.\nConsider a medical test designed to detect a certain disease. If the test has high sensitivity, it means that it is good at correctly identifying individuals with the disease. On the other hand, if the test has high specificity, it is good at correctly identifying individuals without the disease. The goal is often to strike a balance between sensitivity and specificity based on the specific needs and implications of the test results.\n\nSensitivity is often called the power of a procedure (a.k.a. test). Type I and Type II errors are fundamental concepts in hypothesis testing, serving as the duals to specificity and sensitivity.\n\n\n\n\n\n\nNoteType I error (false positive rate)\n\n\n\nis the percentage of healthy people who tested positive, \\(P(T=1\\mid D=0)\\), it is the mistake of thinking something is true when it is not.\n\n\n\n\n\n\n\n\nNoteType II error (or false negative rate)\n\n\n\nis the percentage of sick people who are tested negative, \\(P(T=0\\mid D=1)\\), it is the mistake of thinking something is not true when in fact it is true.\n\n\nWe would like to control both conditional probabilities with our test. Also if someone tests positive, how likely is it that they actually have the disease. There are two ‘errors’ one can make. Falsely diagnosing someone, or not correctly finding the disease.\nIn the stock market, one can think of type I error as not selling a losing stock quickly enough, and a type II error as failing to buy a growing stock, e.g. Amazon or Google.\n\n\n\n\n\n\n\n\n\n\\(P(T=1\\mid D=1)\\)\nSensitivity\nTrue Positive Rate\n\\(1-\\beta\\)\n\n\n\\(P(T=0\\mid D=0 )\\)\nSpecificity\nTrue Negative Rate\n\\(1-\\alpha\\)\n\n\n\\(P(T=1\\mid D=0)\\)\n1-Specificity\nFalse Positive Rate\n\\(\\alpha\\) (type I error)\n\n\n\\(P(T=0\\mid D =1)\\)\n1-Sensitivity\nFalse Negative Rate\n\\(\\beta\\) (type II error)\n\n\n\nOften it is convenient to write those four values in the form of a two-by-two matrix, called the confusion matrix:\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nwhere: TP: True Positive. FN: False Negative, FP: False Positive, TN: True Negative\nWe will extensively use the concepts of errors, specificity and sensitivity later in the book, when describing AB testing and predictive models. These examples illustrate why people can commonly miscalculate and mis-interpret probabilities. Those quantities can be calculated using the Bayes rule.\n\nMedical Diagnostics\n\nExample 2.14 (Alice Mammogram) Alice is a 40-year-old woman, what is the chance that she really has breast cancer when she gets positive mammogram result, given the conditions:\n\nThe prevalence of breast cancer among people like Alice is 1%.\nThe test has an 80% detection rate.\nThe test has a 10% false-positive rate.\n\nWe want to calculate the posterior probability \\(P(\\text{cancer} \\mid \\text{positive mammogram})\\).\n\n\n\n\n\n\nFigure 2.2: Frequency Tree: Medical Diagnosis Scenario (Mammogram)\n\n\n\nUsing the frequency tree in Figure 2.2, we can see that out of 1000 cases: - Number of actual cancer cases = 10. - Number of healthy cases = 990.\nThe test detects 8 out of the 10 cancer cases (True Positives). The test falsely flags 100 out of the 990 healthy cases (False Positives).\nThe total number of positive mammograms is thus \\(8 + 100 = 108\\). The number of these that are actually cancer is 8.\nTherefore, the posterior probability is: \\[\nP(\\text{cancer} \\mid \\text{positive}) = \\frac{8}{108} \\approx 0.074.\n\\] There is only about a 7.4% chance Alice has cancer, despite the positive test result.\n\n\nExample 2.15 (Apple Watch Series 4 ECG and Bayes’ Theorem) The Apple Watch Series 4 can perform a single-lead ECG and detect atrial fibrillation. The software can correctly identify 98% of cases of atrial fibrillation (true positives) and 99% of cases of non-atrial fibrillation (true negatives) (Kim et al. 2024; Bumgarner et al. 2018).\n\n\n\n\n\n\n\n\n\nPredicted\natrial fibrillation\nno atrial fibrillation\nTotal\n\n\n\n\natrial fibrillation\n1960\n980\n2940\n\n\nno atrial fibrillation\n40\n97020\n97060\n\n\nTotal\n2000\n98000\n100000\n\n\n\nHowever, what is the probability of a person having atrial fibrillation when atrial fibrillation is identified by the Apple Watch Series 4? We use Bayes theorem to answer this question. \\[\nP(\\text{atrial fibrillation}\\mid \\text{atrial fibrillation is identified }) = \\frac{0.01960}{ 0.02940} = 0.6667\n\\]\nThe conditional probability of having atrial fibrillation when the Apple Watch Series 4 detects atrial fibrillation is about 67%.\nApple Watch’s positive predictive value is just 19.6 percent. That means in this group – which constitutes more than 90 percent of users of wearable devices like the Apple Watch – the app incorrectly diagnoses atrial fibrillation 79.4 percent of the time. (You can try the calculation yourself using this Bayesian calculator: enter 0.02 for prevalence, 0.98 for sensitivity, and 0.99 for specificity).\nThe electrocardiogram app becomes more reliable in older individuals: The positive predictive value is 76 percent among users between the ages of 60 and 64, 91 percent among those aged 70 to 74, and 96 percent for those older than 85.\nIn the case of medical diagnostics, the sensitivity is the ratio of people who have disease and tested positive to the total number of positive cases in the population \\[\nP(T=1\\mid D=1) = \\dfrac{P(T=1,D=1)}{P(D=1)} = 0.0196/0.02 = 0.98\n\\] The specificity is given by \\[\nP(T=0\\mid D=0) = \\dfrac{P(T=0,D=0)}{P(D=0)} = 0.9702/0.98 = 0.99.\n\\] As we see the test is highly sensitive and specific. However, only 66% of those who are tested positive will have a disease. This is due to the fact that the number of sick people is much less than the number of healthy and presence of type I error.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#advanced-applications",
    "href": "02-bayes.html#advanced-applications",
    "title": "2  Bayes Rule",
    "section": "2.7 Advanced Applications",
    "text": "2.7 Advanced Applications\n\nExample 2.16 ## Obama Elections This example demonstrates a Bayesian approach to election forecasting using polling data from the 2012 US presidential election. The goal is to predict the probability of Barack Obama winning the election by combining polling data across different states.\nThe data used includes polling data from various pollsters across all 50 states plus DC. Each state has polling percentages for Republican (GOP) and Democratic (Dem) candidates along with their electoral vote counts. The data is aggregated by state, taking the most recent polls available.\nThe techniques applied involve Bayesian simulation using a Dirichlet distribution to model uncertainty in polling percentages.\n\n\n\n\n\n\nNoteDirichlet Distribution\n\n\n\nThe Dirichlet distribution is a probability distribution over probabilities. While a standard distribution like the Normal distribution tells you the likelihood of a variable taking a certain real value (like height), the Dirichlet distribution is used when your variables are a set of probabilities that must sum to 1 (like the vote shares of candidates in an election). In this example, for each state, we have vote shares for Obama, Romney, and Others. These three numbers must sum to 100%. The Dirichlet distribution allows us to sample possible election results that respect this constraint while reflecting the uncertainty inherent in the polling data.\n\n\nMonte Carlo simulation runs 10,000 simulations of the election to estimate win probabilities. The analysis is conducted state-by-state, calculating Obama’s probability of winning each individual state. Electoral college modeling combines state probabilities with electoral vote counts to determine the overall election outcome. The simulation runs the entire election multiple times to account for uncertainty and determines the likelihood of Obama reaching the required 270 electoral votes to win. This approach demonstrates how pattern matching through statistical modeling can be used for prediction, showing how polling data can be transformed into probabilistic forecasts of election outcomes.\nWe start by loading the data and aggregating it by state. We then run the simulation and plot probabilities by state.\n\nlibrary(plyr)\n# Source: \"http://www.electoral-vote.com/evp2012/Pres/pres_polls.csv\"\nelection.2012 = read.csv(\"../data/pres_polls.csv\")\n# Remove a pollster: elect2012 &lt;- election.2012[!grepl('Rasmussen', election.2012$Pollster),]\nelect2012 &lt;- election.2012\n# Aggregrate the data\nelect2012 &lt;- ddply(elect2012, .(state), subset, Day == max(Day))\nelect2012 &lt;- ddply(elect2012, .(state), summarise, R.pct = mean(GOP), O.pct = mean(Dem), EV = mean(EV))\n\n\nCode\nknitr::kable(elect2012[1:5,],longtable=TRUE)\nknitr::kable(elect2012[47:51,],longtable=TRUE)\n\n\n\n\n\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\nAlabama\n61\n38\n9\n\n\nAlaska\n55\n42\n3\n\n\nArizona\n54\n44\n11\n\n\nArkansas\n61\n37\n6\n\n\nCalifornia\n38\n59\n55\n\n\n\n\n\n\n\n\n\nstate\nR.pct\nO.pct\nEV\n\n\n\n\n47\nVirginia\n48\n51\n13\n\n\n48\nWashington\n42\n56\n12\n\n\n49\nWest Virginia\n62\n36\n5\n\n\n50\nWisconsin\n46\n53\n10\n\n\n51\nWyoming\n69\n28\n3\n\n\n\n\n\n\n\nElection 2012 Data (first 5 states and last 5 states)\n\n\n\n\nlibrary(MCMCpack)\nprob.Obama &lt;- function(mydata) {\np &lt;- rdirichlet(1000, 500 * c(mydata$R.pct, mydata$O.pct, 100 - mydata$R.pct - \n    mydata$O.pct)/100 + 1)\nmean(p[, 2] &gt; p[, 1])\n}\nwin.probs &lt;- ddply(elect2012, .(state), prob.Obama)\nwin.probs$Romney &lt;- 1 - win.probs$V1\nnames(win.probs)[2] &lt;- \"Obama\"\nwin.probs$EV &lt;- elect2012$EV\nwin.probs &lt;- win.probs[order(win.probs$EV), ]\nrownames(win.probs) &lt;- win.probs$state\n\nWe then plot the probabilities of Obama winning by state.\n\nlibrary(usmap)\nplot_usmap(data = win.probs, values = \"Obama\") + \n  scale_fill_continuous(low = \"red\", high = \"blue\", name = \"Obama Win Probability\", label = scales::comma) + theme(legend.position = \"right\") + coord_sf(expand = FALSE) \n\n\n\n\nProbabilities of Obama winning by state\n\n\n\n\nWe use those probabilities to simulate the probability of Obama winning the election. First, we calculate the probability of Obama having 270 EV or more\n\nsim.election &lt;- function(win.probs) {\n    winner &lt;- rbinom(51, 1, win.probs$Obama)\n    sum(win.probs$EV * winner)\n}\n\nsim.EV &lt;- replicate(10000, sim.election(win.probs))\noprob &lt;- sum(sim.EV &gt;= 270)/length(sim.EV)\noprob\n## 0.96\n\n\nlibrary(lattice)\n# Lattice Graph\ndensityplot(sim.EV, plot.points = \"rug\", xlab = \"Electoral Votes for Obama\", \n  panel = function(x, ...) {\n      panel.densityplot(x, ...)\n      panel.abline(v = 270)\n      panel.text(x = 285, y = 0.01, \"270 EV to Win\")\n      panel.abline(v = 332)\n      panel.text(x = 347, y = 0.01, \"Actual Obama\")\n}, main = \"Electoral College Results Probability\")\n\n\n\n\n\n\n\n\nResults of recent state polls in the 2008 United States Presidential Election between Barack Obama and John McCain.\n\n##  Dirichlet simulation\nprob.Obama = function(j) {\np=rdirichlet(5000,500*c(M.pct[j],O.pct[j],100-M.pct[j]-O.pct[j])/100+1)\nmean(p[,2]&gt;p[,1])}\n## sapply function to compute Obama win prob for all states\nObama.win.probs=sapply(1:51,prob.Obama)\n##  sim.EV function\nsim.election = function() {\nwinner = rbinom(51,1,Obama.win.probs)\nsum(EV*winner) }\nsim.EV = replicate(1000,sim.election())\n\n\n\nHistogram of simulated election\n## histogram of simulated election\nhist(sim.EV,min(sim.EV):max(sim.EV),col=\"blue\",prob=T)\nabline(v=365,lwd=3)   # Obama received 365 votes\ntext(375,30,\"Actual \\n Obama \\n total\")\n\n\n\n\n\n\n\n\nFigure 2.3: Histogram of simulated election\n\n\n\n\n\nThe analysis of the 2008 U.S. Presidential Election data reveals several key insights about the predictive power of state-level polling and the uncertainty inherent in electoral forecasting. The actual result of 365 electoral votes falls within the simulated range, demonstrating the model’s validity. The 270-vote threshold needed to win the presidency is clearly marked and serves as a critical reference point.\nWe used a relatively simple model to simulate the election outcome. The model uses Dirichlet distributions to capture uncertainty in state-level polling percentages. Obama’s win probabilities vary significantly across states, reflecting the competitive nature of the election. The simulation approach accounts for both sampling uncertainty and the discrete nature of electoral vote allocation. The histogram of simulated results shows the distribution of possible outcomes. The actual Obama total of 365 electoral votes is marked and falls within the reasonable range of simulated outcomes. This validates the probabilistic approach to election forecasting.\nThis analysis demonstrates how Bayesian methods can be effectively applied to complex prediction problems with multiple sources of uncertainty, providing both point estimates and uncertainty around those estimates.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "href": "02-bayes.html#graphical-representation-of-probability-and-conditional-independence.",
    "title": "2  Bayes Rule",
    "section": "2.8 Graphical Representation of Probability and Conditional Independence.",
    "text": "2.8 Graphical Representation of Probability and Conditional Independence.\n\nWe can use the telescoping property of conditional probabilities to write the joint probability distribution as a product of conditional probabilities. This is the essence of the chain rule of probability. It is given by \\[\nP(x_1, x_2, \\ldots, x_n) = P(x_1)P(x_2 \\mid x_1)P(x_3 \\mid x_1, x_2) \\ldots P(x_n \\mid x_1, x_2, \\ldots, x_{n-1}).\n\\] The expression on the right hand side can be simplified if some of the variables are conditionally independent. For example, if \\(x_3\\) is conditionally independent of \\(x_2\\), given \\(x_1\\), then we can write \\[\nP(x_3 \\mid x_1, x_2) =P(x_3 \\mid x_1).\n\\]\nIn a high-dimensional case, when we have a joint distribution over a large number of random variables, we can often simplify the expression by using independence or conditional independence assumptions. Sometimes it is convenient to represent these assumptions in a graphical form. This is the idea behind the concept of a Bayesian network. Essentially, the graph is a compact representation of a set of independencies that hold in the distribution.\nLet’s consider an example of joint distribution with three random variables, we have the following joint distribution: \\[\nP(a,b,c) = P(a\\mid b,c)P(b\\mid c)P(c)\n\\]\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network. Each node represents a random variable and the arrows represent the conditional dependencies between the variables. When two nodes are connected they are not independent. Consider the following three cases:\n\n\n\n\n\n\n\\[\nP(b\\mid c,a) = P(b\\mid c),~ P(a,b,c) = P(a)P(c\\mid a)P(b\\mid c)\n\\]\n\n\n\n\nLine Structure\n\n\n\n\n\n\n\n\n\n\\[\nP(a\\mid b,c) = P(a\\mid c), ~ P(a,b,c) = P(a\\mid c)P(b\\mid c)P(c)\n\\]\n\n\n\n\nLambda Structure\n\n\n\n\n\n\n\n\n\n\\[\nP(a\\mid b) = P(a),~ P(a,b,c) = P(c\\mid a,b)P(a)P(b)\n\\]\n\n\n\n\nV-structure\n\n\n\nAlthough the graph shows us the conditional independence assumptions, we can also derive other independencies from the graph. An interesting question is whether they are connected through a third node. In the first case (a), we have \\(a\\) and \\(b\\) connected through \\(c\\). Thus, \\(a\\) can influence \\(b\\). However, once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In case (b) the logic here is similar, \\(a\\) can influence \\(b\\) through \\(c\\), but once \\(c\\) is known, \\(a\\) and \\(b\\) are independent. In the third case (c), \\(a\\) and \\(b\\) are independent, but once \\(c\\) is known, \\(a\\) and \\(b\\) are not independent. You can formally derive these independencies from the graph by comparing \\(P(a,b\\mid c)\\) and \\(P(a\\mid c)P(b\\mid c)\\).\n\nExample 2.17 (Bayes Home Diagnostics) Suppose that a house alarm system sends me a text notification when some motion inside my house is detected. It detects motion when I have a person inside (burglar) or during an earthquake. Say, from prior data we know that during an earthquake alarm is triggered in 10% of the cases. Once I receive a text message, I start driving back home. While driving I hear on the radio about a small earthquake in our area. Now we want to know \\(P(b \\mid a)\\) and \\(P(b \\mid a,r)\\). Here \\(b\\) = burglary, \\(e\\) = earthquake, \\(a\\) = alarm, and \\(r\\) = radio message about small earthquake.\nThe joint distribution is then given by \\[\n  P(b,e,a,r) = P(r \\mid a,b,e)P(a \\mid b,e)P(b\\mid e)P(e).\n\\] Since we know the causal relations, we can simplify this expression \\[\nP(b,e,a,r) = P(r \\mid e)P(a \\mid b,e)P(b)P(e).\n\\] The \\(P(a \\mid b,e)\\) distribution is defined by\n\n\n\nTable 2.1: Conditional probability of alarm given burglary and earthquake\n\n\n\n\n\n\\(P(a=1 \\mid b,e)\\)\nb\ne\n\n\n\n\n0\n0\n0\n\n\n0.1\n0\n1\n\n\n1\n1\n0\n\n\n1\n1\n1\n\n\n\n\n\n\nGraphically, we can represent the relations between the variables known as a Directed Acyclic Graph (DAG), which is known as a Bayesian network.\n\n\n\n\n\n\nFigure 2.4: Bayesian network for alarm.\n\n\n\nNow we can easily calculate \\(P(a=0 \\mid b,e)\\), from the property of a probability distribution \\(P(a=1 \\mid b,e) + P(a=0 \\mid b,e) = 1\\). In addition, we are given \\(P(r=1 \\mid e=1) = 0.5\\) and \\(P(r=1 \\mid e=0) = 0\\). Further, based on historic data we have \\(P(b) = 2\\cdot10^{-4}\\) and \\(P(e) = 10^{-2}\\). Note that causal relations allowed us to have a more compact representation of the joint probability distribution. The original naive representation requires specifying \\(2^4\\) parameters.\nTo answer our original question, calculate \\[\nP(b \\mid a) = \\dfrac{P(a \\mid b)P(b)}{P(a)},~~P(a) = P(a=1 \\mid b=1)P(b=1) + P(a=1 \\mid b=0)P(b=0).\n\\] We have everything but \\(P(a \\mid b)\\). This is obtained by marginalizing \\(P(a=1 \\mid b,e)\\), to yield \\[\nP(a \\mid b) = P(a \\mid b,e=1)P(e=1) + P(a \\mid b,e=0)P(e=0).\n\\] We can calculate \\[\nP(a=1 \\mid b=1) = 1, ~P(a=1 \\mid b=0) = 0.1*10^{-2} + 0 = 10^{-3}.\n\\] This leads to \\(P(b \\mid a) = 2\\cdot10^{-4}/(2\\cdot10^{-4} + 10^{-3}(1-2\\cdot10^{-4})) = 1/6\\).\nThis result is somewhat counterintuitive. We get such a low probability of burglary because its prior is very low compared to the prior probability of an earthquake. What will happen to the posterior if we live in an area with higher crime rates, say \\(P(b) = 10^{-3}\\). Figure 2.5 shows the relationship between the prior and posterior. \\[\nP(b \\mid a) = \\dfrac{P(b)}{P(b) + 10^{-3}(1-P(b))}\n\\]\n\nprior &lt;- seq(0, .1, length.out = 200)\npost &lt;- prior / (prior + 0.001 * (1 - prior))\nplot(prior, post, type = \"l\", lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nFigure 2.5: Relationship between the prior and posterior\n\n\n\n\n\nNow, suppose that you hear on the radio about a small earthquake while driving. Then, using Bayesian conditioning, \\[\nP(b=1 \\mid a=1,r=1) =  \\dfrac{P(a,r  \\mid  b)P(b)}{P(a,r)}\n\\] and \\[\nP(a,r  \\mid  b)P(b) = \\dfrac{\\sum_e P(b=1,e,a=1,r=1)}{\\sum_b\\sum_eP(b,e,a=1,r=1)}\n\\] \\[\n=\\dfrac{\\sum_eP(r=1 \\mid e)P(a=1 \\mid b=1,e)P(b=1)P(e)}{\\sum_b\\sum_eP(r=1 \\mid e)P(a=1 \\mid b,e)P(b)P(e)}\n\\] which is \\(\\approx 2\\%\\) in our case. This effect is called explaining away, namely when new information explains some previously known fact.\n\n\n\n\n\nBumgarner, John M., Chad T. Lambert, Ayman A. Hussein, Daniel J. Cantillon, Bryan Baranowski, Kathy Wolski, Bruce D. Lindsay, Oussama M. Wazni, and Khaldoun G. Tarakji. 2018. “Smartwatch Algorithm for Automated Detection of Atrial Fibrillation.” Journal of the American College of Cardiology 71 (21): 2381–88.\n\n\nKim, Young-Hoon, Jaehyung Shim, Hyoung-Seob Park, et al. 2024. “Diagnostic Accuracy of Single-Lead Handheld ECG Devices for Atrial Fibrillation Detection.” Journal of Cardiovascular Electrophysiology 35: 614–21.\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs Nakamura or Bayes Vs P-Value.” {{SSRN Scholarly Paper}}. Rochester, NY.\n\n\nSimpson, Edward. 2010. “Edward Simpson: Bayes at Bletchley Park.” Significance 7 (2): 76–80.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Bayes Rule</span>"
    ]
  },
  {
    "objectID": "03-bl.html",
    "href": "03-bl.html",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "3.1 Exchangeability and the Bayesian view of probability models\nStatistics makes use of parametric families of distributions and assumes that observed samples \\(y = (y_1,\\ldots,y_n)\\) are independent and identically distributed observations from a distribution with density function parametrized by \\(\\theta\\), the notation is \\(y\\mid \\theta \\sim p(y \\mid \\theta)\\). The functional form of \\(p(y \\mid \\theta)\\) is assumed to be known, but the value of \\(\\theta\\) is unknown. The goal of statistical inference is to estimate \\(\\theta\\) from the observed data \\[\ny = (y_1,\\ldots,y_n).\n\\] There are several tasks in statistical inference, including estimation, hypothesis testing, and prediction.\nThe main tasks in statistical inference include estimation, hypothesis testing, and prediction. Estimation involves using the observed sample to infer the value of the unknown parameter \\(\\theta\\), either by providing a single best guess (denoted as \\(\\hat{\\theta}\\)) or by constructing an interval \\([a, b]\\) that is likely to contain the true value of \\(\\theta\\) with a specified probability. Hypothesis testing focuses on evaluating specific claims or hypotheses about the value of \\(\\theta\\); for instance, we might be interested in determining whether \\(\\theta\\) is equal to a particular value \\(\\theta_0\\). Prediction, on the other hand, is concerned with forecasting the value of a future observation \\(y_{n+1}\\) based on the data already observed, \\(y_1, \\ldots, y_n\\) via a model \\(p(y_{n+1} \\mid y_1, \\ldots, y_n)\\).\nIn this section we present a general framework for statistical inference, known as Bayesian inference, which is based on the use of probability distributions to represent uncertainty and make inferences about unknown parameters. We will use Bayes rule to update our beliefs about the parameters of a model based on new evidence or data. Bayesian inference provides a principled approach to statistical modeling and decision-making, and is widely used in various fields such as machine learning, econometrics, and engineering.\nIn the context of artificial intelligence and statistical modeling, Bayesian parameter learning is particularly relevant when dealing with models that have uncertain or unknown parameters. The goal is to update the probability distribution over the parameters of the model as new data becomes available. Suppose that you are interested in the values of \\(k\\) unknown quantities \\[\n\\theta = (\\theta_1, \\ldots, \\theta_k)\n\\]\nThe basic steps involved in Bayesian parameter learning include:\nThe key advantage of Bayesian parameter learning is its ability to incorporate prior knowledge and update beliefs based on observed data in a principled manner. It provides a framework for handling uncertainty and expressing the confidence or ambiguity associated with parameter estimates. However, it often requires computational methods, such as Markov Chain Monte Carlo (MCMC) or variational inference, to approximate or sample from the complex posterior distributions.\nOften, Bayesian analysis can be performed without calculating the marginal likelihood, in this case we omit the total probability in the denominator on the right hand side and write Bayes rule as \\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}.\n\\]\nThe choice of prior distribution can significantly impact the ease of computation and the interpretation of the posterior distribution. Conjugate priors are a special type of prior distribution that, when combined with a specific likelihood function, result in a posterior distribution that belongs to the same family as the prior. This property simplifies the computation of the posterior distribution, and allows for analytical solutions.\nCommon examples of conjugate priors include:\nUsing conjugate priors simplifies the Bayesian analysis, especially in cases where analytical solutions are desirable. However, the choice of a conjugate prior is often a modeling assumption, and in some cases, non-conjugate priors may be more appropriate for capturing the true underlying uncertainty in the problem. The blind use of conjugate priors can lead to misleading results. We should never ignore the absence of evidence for use of a specific model.\nAt the basis of all statistical problems is a potential sample of data, \\(y=\\left( y_{1},\\ldots,y_{T}\\right)\\), and assumptions over the data generating process such as independence, a model or models, and parameters. How should one view the relationship between models, parameters, and samples of data? How should one define a model and parameters? These questions have fundamental implications for statistical inference and can be answered from different perspectives. We will discuss de Finetti’s representation theorem which provides a formal connection between data, models, and parameters.\nTo understand the issues, consider the simple example of an experiment consisting of tosses of a simple thumb tack in ideal “laboratory” conditions. The outcome of the experiment can be defined as a random variable \\(y_{i},\\) where \\(y_{i}=1\\) if the \\(i^{th}\\) toss was a heads (the tack lands on the spike portion) and \\(y_{i}=0\\) if the tack lands tails (on its flat portion). How do we model these random variables? The frequentist or objective approach assumes tosses are independent and identically distributed. In this setting, independence implies that \\[\nP\\left(  y_{2}=1,y_{1}=1\\right)  =P\\left(  y_{2}=1\\right)\nP\\left(  y_{1}=1\\right).\n\\]\nGiven this, are thumbtack tosses independent? Surprisingly, the answer is no. Or at least absolutely not under the current assumptions. Independence implies that \\[\nP\\left(  y_{2}=1 \\mid y_{1}=1\\right)  =P\\left(  y_{2}=1\\right),\n\\] which means that observing \\(y_{1}=1\\) does not affect the probability that \\(y_{2}=1\\). To see the implications of this simple fact, suppose that the results of 500 tosses were available. If the tosses were independent, then \\[\nP\\left(  y_{501}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=1\\right)  =P\\left(  y_{501}=1\\mid {\\textstyle\\sum\\nolimits_{t=1}^{500}}y_{t}=499\\right).\n\\] It is hard to imagine that anyone would believe this–nearly every observer would state that the second probability is near zero and the third probability is near 1 as the first 500 tosses contain a lot of information. Thus, the tosses are not independent.\nTo see the resolution of this apparent paradox, introduce a parameter, \\(\\theta\\), which is the probability that a thumb tack toss is heads. If \\(\\theta\\) were known, then it is true that, conditional on the value of this parameter, the tosses are independent and \\[\nP\\left(  y_{2}=1\\mid y_{1}=1,\\theta\\right)  =P\\left(y_{2}=1\\mid \\theta\\right)  =\\theta.\n\\] Thus, the traditional usage of independence, and independent sampling, requires that “true” parameter values are known. With unknown probabilities, statements about future tosses are heavily influenced by previous observations, clearly violating the independence assumption. Ironically, if the data was really independent, we would not need samples in the first place to estimate parameters because the probabilities would already be known! Given this, if you were now presented with a thumb tack from a box that was to be repeatedly tossed, do you think that the tosses are independent?\nThis example highlights the tenuous foundations, an odd circularity, and the internal inconsistency of the frequentist approach that proceeds under the assumption of a fixed “true” parameter. All frequentist procedures are founded on the assumption of known parameter values: sampling distributions of estimators are computed conditional on \\(\\theta\\); confidence intervals consist of calculations of the form: \\(P\\left( f\\left( y_{1}, \\ldots ,y_{T}\\right) \\in\\left( a,b\\right) |\\theta\\right)\\); and asymptotics also all rely on the assumption of known parameter values. None of these calculations are possible without assuming the known parameters.\nIn the frequentist approach, even though the parameter is completely unknown to the researcher, \\(\\theta\\) is not a random variable, does not have a distribution, and therefore inference is not governed by the rules of probability. Given this “fixed, but unknown” definition, it is impossible to discuss concepts like “parameter uncertainty.” This strongly violates our intuition, since things that are not known are typically thought of as random.\nThe Bayesian approach avoids this internal inconsistency by shedding the strong assumption of independence and assumption of a fixed but unknown parameter. Instead it assumes that \\(\\theta\\) is a random variable and describes the uncertainty about \\(\\theta\\) using a probability distribution, \\(p\\left( \\theta\\right)\\) (the prior). The joint distribution of the data is then \\[\np(y_{1}, \\ldots ,y_{T})  = \\int p(y_{1}, \\ldots ,y_{T} \\mid \\theta)  p(\\theta)d\\theta = \\int\\prod_{t=1}^Tp(y_t\\mid \\theta)  p( \\theta)d\\theta.\n\\] Notice, that the right-hand-side does not depend on the order of the data, and the joint distribution of the data is the same for all potential orderings. This is a natural assumption about the symmetry of the data, and is called exchangeability. The Bayesian approach makes no assumptions about the order in which the data may arrive, and each observation has the same marginal distribution, \\(P\\left( y_{i}=1\\right) =P\\left(y_{j}=1\\right)\\) for any \\(i\\) and \\(j\\).\nThus, we replace the independence assumption with a weaker and more natural assumption of exchangeability: a collection of random variables, \\(y_{1}, \\ldots ,y_{T}\\), is exchangeable if the distribution of \\(y_{1}, \\ldots ,y_{T}\\) is the same as the distribution of any permutation \\(y_{\\pi_{1}}, \\ldots ,y_{\\pi_{T}}\\), where \\(\\pi=\\left( \\pi_{1}, \\ldots ,\\pi_{T}\\right)\\) is a permutation of the integers \\(1\\) to \\(T\\). Independent events are always exchangeable, but the converse is not true. Notice the differences between the assumptions in the Bayesian and frequentist approach: the Bayesian makes assumptions over potentially realized data, and there is no need to invent the construct of a fixed but unknown parameter, since exchangeability makes no reference to parameters.\nIn the case of the tack throwing experiment, exchangeability states that the ordering of heads and tails does not matter. Thus, if the experiment of 8 tosses generated 4 heads, it does not matter if the ordering was \\(\\left(1,0,1,0,1,0,1,0\\right)\\) or \\(\\left( 0,1,1,0,1,0,0,1\\right)\\). This is a natural assumption about the symmetry of the tack tosses, capturing the idea that the information in any toss or sequence of tosses is the same as any other–the idea of a truly random sample. It is important to note that exchangeability is a property that applies prior to viewing the data. After observation, data is no longer a random variable, but a realization of a random variable.\nBruno de Finetti introduced the notion of exchangeability, and then asked a simple question: “What do exchangeable sequences of random variables look like?” The answer to this question is given in the famous de Finetti’s theorem, which also defines models, parameters, and provides important linkages between frequentist and classical statistics.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "href": "03-bl.html#exchangeability-and-the-bayesian-view-of-probability-models",
    "title": "3  Bayesian Learning",
    "section": "",
    "text": "de Finetti’s representation theorem\nde Finetti’s representation theorem provides the theoretical connection between data, models, and parameters. It is stated first in the simplest setting, where the observed data takes two values, either zero or one, and then extended below.\n\nTheorem 3.1 (de Finetti’s representation theorem) Let \\(\\left( Y_{1},Y_{2},\\ldots\\right)\\) be an infinite sequence of 0-1 exchangeable random variables with joint density \\(p\\left(Y_{1}, \\ldots ,Y_{n}\\right)\\). Then there exists a distribution \\(P\\) such that \\[\nP(Y_{1}=y_1,\\ldots,Y_{n}=y_n)=\\int\\prod_{i=1}^{n}\\theta^{y_{i}}(1-\\theta)^{1-y_{i}}dP(\\theta)=\\int\\prod_{i=1}^{n}p\\left(  y_{i} \\mid \\theta\\right)  dP(\\theta)\n\\tag{3.1}\\] where \\[\nP(\\theta)=\\lim_{n\\rightarrow\\infty}\\text{Prob}\\left[  \\frac{1}{n}\\sum_{t=1}^{n}Y_{t}\\leq\\theta\\right]  \\text{ and }\\theta=\\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_{t=1}^{n}Y_{t}.\n\\] If the distribution function or measure admits a density with respect to Lebesgue measure, then \\(dP(\\theta)=p\\left( \\theta\\right) d\\theta\\).\nHeath and Sudderth (1976) provide a general version and simple proof. For general spaces \\(\\mathcal{Y}\\), we have \\[\nP(Y_1 \\le y_1, \\ldots, Y_n \\le y_n) = \\int_{\\mathcal{F}} \\prod_{i=1}^{n} F(y_i) \\, \\mu(dF)\n\\] where \\(\\mu(\\cdot)\\) is a measure over the space of distributions, \\(\\mathcal{F}\\). In the parametric case, \\(F_\\theta\\), then \\(\\mu(d\\theta) = p(\\theta) d\\theta\\) and \\(p(\\theta)\\) is the typical prior density. The predictive \\(p(Y_{n+1} \\mid y_1, \\ldots, y_n)\\) is then a marginal over \\(F_\\theta\\) and the posterior \\(p(\\theta \\mid y_1, \\ldots, y_n)\\) which is updated via Bayes theorem.\n\nde Finetti’s representation theorem has profound implications for understanding models from a subjectivist perspective and in relating subjectivist to frequentist theories of inference. The theorem is interpreted as follows:\n\nUnder exchangeability, parameters exist, and one can act as if the \\(Y_{t}\\)’s are drawn independently from a Bernoulli distribution with parameter \\(\\theta\\). That is, they are draws from the model \\(p\\left(Y_{t} \\mid \\theta\\right) =\\theta^{Y_{t}}(1-\\theta)^{1-Y_{t}},\\) generating a likelihood function \\(p\\left( y \\mid \\theta\\right) =\\prod_{t=1}^{T}p\\left(Y_{t} \\mid \\theta\\right)\\). Formally, the likelihood function is defined via the density \\(p\\left( y \\mid \\theta\\right)\\), viewed as a function of \\(\\theta\\) for a fixed sample \\(y=\\left( Y_{1}, \\ldots ,Y_{T}\\right)\\). More \"likely\" parameter values generate higher likelihood values, thus the name. The maximum likelihood estimate (MLE) is \\[\n\\widehat{\\theta}_{\\mathrm{MLE}}=\\arg\\underset{\\theta\\in\\Theta}{\\max}\\text{ }p\\left(y \\mid \\theta\\right)  =\\arg\\underset{\\theta\\in\\Theta}{\\max}\\ln p\\left(y \\mid \\theta\\right),   \n\\] where \\(\\Theta\\) is the parameter space.\nParameters are random variables. The limit \\(\\theta=\\underset {T\\rightarrow\\infty}{\\lim}T^{-1}\\sum_{t=1}^{T}Y_{t}\\) exists but is a random variable. This can be contrasted with the strong law of large numbers that requires independence and implies that \\(T^{-1}\\sum_{t=1}^{T}Y_{t}\\) converges almost surely to a fixed value, \\(\\theta_{0}\\). From this, one can interpret a parameter as a limit of observables and justifies the frequentist interpretation of \\(\\theta\\) as a limiting frequency of 1’s.\nThe distribution \\(P\\left( \\theta\\right)\\) or density \\(p\\left(\\theta\\right)\\) can be interpreted as beliefs about the limiting frequency \\(\\theta\\) prior to viewing the data. After viewing the data, beliefs are updated via Bayes rule resulting in the posterior distribution, \\(p(\\theta \\mid Y_{1},\\ldots,Y_{T})\\).\n\nSince the likelihood function is fixed in this case, any distribution of observed data can be generated by varying the prior distribution.\nThe main implication of de Finetti’s theorem is a complete justification for Bayesian practice of treating the parameters as random variables and specifying a likelihood and parameter distribution. Stated differently, a “model” consists of both a likelihood and a prior distribution over the parameters. Thus, parameters as random variables and priors are a necessity for statistical inference, and not some extraneous component motivated by philosophical concerns.\nMore general versions of de Finetti’s theorem are available. A general version is as follows. If \\(\\left\\{ Y_{t}\\right\\} _{t\\geq1}\\), \\(Y_{t}\\in\\mathbb{R}\\), is a sequence of infinitely exchangeable random variables, then there exists a probability measure \\(P\\) on the space of all distribution functions, such that \\[\np(Y_{1},\\ldots,Y_{T})=\\int\\prod_{t=1}^{T}F\\left(  Y_{t}\\right)\nP(dF)\n\\] with mixing measure \\[\nP\\left(  F\\right)  =\\underset{T\\rightarrow\\infty}{\\lim}P(F_{T}),\n\\] where \\(F_{T}\\) is the empirical distribution of the data. At this level of generality, the distribution function is infinite-dimensional. In practice, additional subjective assumptions are needed that usually restrict the distribution function to finite dimensional spaces, which implies that the distribution function is indexed by a parameter vector \\(\\theta\\): \\[\np(Y_{1},\\ldots,Y_{T})=\\int\\prod_{t=1}^{T}p\\left(  Y_{t} \\mid \\theta\\right)\ndP\\left(  \\theta\\right).\n\\] To operationalize this result, the researcher needs to choose the likelihood function and the prior distribution of the parameters.\n\n\nPosterior Empirical CDF\nLet \\(m = \\{ f_\\theta ( y ) : y \\in \\mathcal{Y} \\}\\) be a model. When necessary we index the parameters in model \\(m\\), as \\(\\theta_m\\). Let \\(y = ( y_1 , \\ldots , y_n )\\) be a vector of signals. The conditional likelihood, under \\(m\\), is given by \\(f_\\theta(y) =  \\prod_{i=1}^n f_\\theta ( y_i )\\). We also allow for the possibility that the data is generated from a model \\(f\\) that does not belong to the family of models \\(f_\\theta\\).\nGiven a prior measure, \\(\\Pi ( d F )\\), over \\(\\mathcal{F}\\) the set of distributions, we can calculate the predictive density \\[\nf_n  ( y_{n+1} | y_1 , \\ldots , y_n ) = \\int f (y) \\Pi_n ( d F ) \\; \\text{where}\\; \\Pi_n ( d f ) = \\frac{ \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }{  \\int  \\prod_{i=1}^n f( y_i ) \\Pi( d f ) }\n\\] Under the family, \\(f_\\theta\\), we can calculate the parameter posterior as \\[\np( \\theta | y ) =  \\frac{ \\prod_{i=1}^n f_\\theta ( y_i ) p(\\theta)  d \\theta }{  m(y) } \\;  \\text{where}\\; m(y) = \\int f_\\theta (y) p( \\theta ) d \\theta\n\\] Here \\(p(\\theta)\\) is a prior distribution over parameters and \\(m(y)\\) is the marginal distribution of the data implied by the model. There are many applications in Bayesian non-parametric statistics.\nAt first glance, de Finetti’s theorem may seem to suggest that there is a single model or likelihood function. This is not the case however, as models can be viewed in the same manner as parameters. Denoting a model specification by \\(\\mathcal{M}\\), then de Finetti’s theorem would imply that \\[\\begin{align*}\np(y_{1},\\ldots,y_{T})  &  =\\int\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta ,\\mathcal{M}\\right)  p\\left(  \\theta \\mid \\mathcal{M}\\right)  p\\left(\\mathcal{M}\\right)  d\\theta d\\mathcal{M}\\\\\n&  =\\int p(y_{1},\\ldots,y_{T} \\mid \\mathcal{M})p\\left(  \\mathcal{M}\\right)\nd\\mathcal{M},\n\\end{align*}\\] in the case of a continuum of models. Thus, under the mild assumption of exchangeability, it is as if the \\(y_{t}\\)’s are generated from \\(p\\left( y_{t} \\mid \\theta,\\mathcal{M}\\right)\\), conditional on the random variables \\(\\theta\\) and \\(\\mathcal{M}\\), where \\(p\\left( \\theta \\mid \\mathcal{M}\\right)\\) are the beliefs over \\(\\theta\\) in model \\(\\mathcal{M}\\), and \\(p\\left(\\mathcal{M}_{j}\\right)\\) are the beliefs over model specifications.\nSubjective probability is a more general definition of probability than the frequentist definition, as it can be used for all types of events, both repeatable and unrepeatable events. A subjectivist has no issues discussing the probability of a lection result, even though the underlying conditions has not been observed before. As Ramsey (1926) puts it, “the probability is simply the willingness to bet on an event with a counterparty”.\nThe event does not even have to be uncertain in nature. For example, the probability of me having coins in my pocket will depend on who is asked to make the assessment. I, knowing the contents of my pocket, will say the probability is 0. However, if you are asked to make the assessment, you will say the probability is 1/2, as you do not know the contents of my pocket. This is a classic example of subjectivist probability.\nSimilarly, consider the number of people currently in Antarctica. This number is fixed and deterministic at any given moment, yet different individuals will assign different probability distributions to this quantity based on their knowledge. A researcher who recently reviewed Antarctic population statistics might have a tight distribution centered around the correct value, while someone with no such knowledge might have a much wider distribution. A logistics coordinator for a polar research station would have precise information about personnel at their specific facility but uncertainty about other stations. Each of these represents valid subjective probabilities over the same underlying fixed quantity, illustrating how probability in the Bayesian sense quantifies personal uncertainty rather than intrinsic randomness.\nThe main difficulty in operationalizing subjective probability is the process of actually quantifying subjective beliefs into numeric probabilities. One practical approach is to elicit probabilities through a sequence of carefully designed bets.\nConsider eliciting someone’s probability distribution over the number of people in Antarctica. We could start by asking: “Would you accept a bet that pays $100 if the number is below 5,000, and you pay $50 if it’s above 5,000?” If they accept, this suggests they believe \\(P(\\text{population} &lt; 5000) &gt; 1/3\\). We then adjust the threshold and payoffs systematically. For instance, we might ask about betting on the population being below 2,000, or below 1,000, gradually narrowing down probability mass at different intervals.\nFor continuous quantities, we can elicit a full distribution through a sequence of binary bets about quantiles. By asking someone to specify values \\(q_{0.25}, q_{0.5}, q_{0.75}\\) such that they are indifferent between bets paying equal amounts if the true value falls below or above each threshold, we construct their 25th, 50th, and 75th percentiles. This process, known as probability elicitation, transforms abstract beliefs into concrete probability distributions by observing revealed preferences through betting behavior.\nThe betting framework provides two key advantages. First, it forces coherence: if someone states inconsistent probabilities (such as \\(P(A) + P(\\neg A) \\neq 1\\)), an adversary could construct a Dutch book—a set of bets that guarantees a loss regardless of the outcome. The threat of sure loss incentivizes rational probability assignments. Second, betting naturally handles non-repeatable events. We can elicit probabilities about tomorrow’s Supreme Court decision or next quarter’s GDP growth, neither of which has a frequentist interpretation.\nInstead of using repetitive experiments, subjective probabilities can be measured using betting odds, which have been used for centuries to gauge the uncertainty over an event. The probability attributed to winning a coin toss is revealed by the type of odds one would accept to bet. Notice the difference between the frequentist and Bayesian approach. Instead of defining the probabilities via an infinite repeated experiment, the Bayesian approach elicits probabilities from an individual’s observed behavior.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sufficient-statistic",
    "href": "03-bl.html#sufficient-statistic",
    "title": "3  Bayesian Learning",
    "section": "3.2 Sufficient Statistic",
    "text": "3.2 Sufficient Statistic\nIn Bayesian inference, we need to compute the posterior over unknown model parameters \\(\\theta\\), given data \\(y\\). The posterior density is denoted by \\(p(\\theta \\mid y)\\). A map from data \\(y = (y_1, \\ldots, y_n)\\) to a statistic \\(S(y)\\) is called a sufficient statistic for \\(\\theta\\) if the conditional distribution of \\(y\\) given \\(S(y)\\) is independent of \\(\\theta\\): \\[\np(y\\mid S(y),\\theta) = p(y\\mid S(y)).\n\\] This implies that \\(S(y)\\) captures all the information in the data relevant to \\(\\theta\\). Because the statistic is a deterministic function of the data, the likelihood factorizes as: \\[\np(y\\mid \\theta) = p(S(y)\\mid \\theta)p(y\\mid S(y)).\n\\]\nThere is a powerful connection between the posterior mean and sufficient statistics in the exponential family. Kolmogorov (1942) showed that if \\(S^*(y)\\) is a minimal sufficient statistic, then the posterior expectation \\(E[\\theta \\mid y]\\) is a function of \\(S^*(y)\\). This provides a theoretical foundation for using summary statistics in simplified models.\n\nExample 3.1 (Posterior Distribution for Coin Toss) What if we gamble against unfair coin flips or the person who performs the flips is trained to get the side he wants? In this case, we need to estimate the probability of heads \\(\\theta\\) from the data. Suppose we have observed 10 flips \\[\n\\{H, T, H, H, H, T, H, T, H, H\\},\n\\] and only three of them were tails. What is the probability that the next flip will be tail? The frequency-based answer would be \\(3/10 = 0.3\\). However, the Bayes approach gives us more flexibility. Suppose we have a prior belief that the coin is fair, but we are not sure. We can model this belief by a prior distribution. Let’s discretize the variable \\(\\theta\\) and assign prior probabilities to each value of \\(\\theta\\). The p as follows\nWe put most of the mass to the fair assumption (\\(\\theta = 0.5\\)) and zero mass to the extreme values \\(\\theta = 0\\) and \\(\\theta = 1\\). Our mass is exponentially decaying as we move away from 0.5. This is a reasonable assumption, since we are not sure about the fairness of the coin. Now, we can use Bayes rule to update our prior belief. The posterior distribution is shown in fig-coinposterior (left panel). \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}.\n\\] The denominator is the marginal likelihood, which is given by \\[\np(y) = \\sum_{\\theta} p(y \\mid \\theta) p(\\theta).\n\\] The likelihood is given by the Binomial distribution \\[\np(y \\mid \\theta) \\propto \\theta^3 (1 - \\theta)^7.\n\\] Notice, that the posterior distribution depends only on the number of positive and negative cases. Those numbers are sufficient for the inference about \\(\\theta\\). The posterior distribution shown in fig-coinposterior (middle panel).\ntheta &lt;- seq(0, 1, by = 0.1)\nprior = c(0, 0.024, 0.077, 0.132, 0.173, 0.188, 0.173, 0.132, 0.077, 0.024, 0)\nbarplot(prior, names.arg = theta, xlab = \"theta\", ylab = \"prior\", col = \"lightblue\")\n\nlikelihood &lt;- function(theta, n, Y) {theta^Y * (1 - theta)^(n - Y)}\nposterior &lt;- likelihood(theta, 10,3) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\", col = \"lightblue\")\n\nposterior &lt;- likelihood(theta, 100,30) * prior\nposterior &lt;- posterior / sum(posterior) # normalize\nbarplot(posterior, names.arg = theta, xlab = \"theta\", ylab = \"posterior\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Prior\n\n\n\n\n\n\n\n\n\n\n\n(b) Posterior for 10 flips\n\n\n\n\n\n\n\n\n\n\n\n(c) Posterior for 100 flips\n\n\n\n\n\n\n\nFigure 3.1: Priot and Posterior distribution\n\n\n\nIf you are to keep collecting more observations and say observe a sequence of 100 flips and 30 of them were heads, then the posterior distribution will be more concentrated around the value of \\(\\theta = 0.3\\) as shown in fig-coinposterior (right panel).\nThis demonstrates that for large sample sizes, the frequentist approach and the Bayesian approach agree.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sec-betabinomial",
    "href": "03-bl.html#sec-betabinomial",
    "title": "3  Bayesian Learning",
    "section": "3.3 Beta-Binomial Model",
    "text": "3.3 Beta-Binomial Model\nThe Beta-Binomial Bayesian model is a statistical model that is used when we are interested in learning about a proportion or probability of success, denoted by \\(p\\). This model is used, for example, when dealing with binary data in A/B testing.\nIn the Beta-Binomial model, we assume that the the observed data is generated from a Binomial distribution with parameter \\(\\theta\\) and \\(m\\) trials. The probability of success \\(\\theta\\) in each of \\(m\\) Bernoulli trials is not fixed but randomly drawn from a Beta distribution. Thus, the model is given by \\[\ny_i \\mid \\theta \\sim Binomial(m,\\theta) ~\\text{(likelihood)}, \\quad \\theta \\sim Beta(\\alpha,\\beta) ~\\text{(prior)}.\n\\]\nThe Beta distribution is a family of continuous probability distributions defined on the interval [0,1] and has two parameters alpha (\\(\\alpha\\)) and beta (\\(\\beta\\)), that appear as exponents of the variable and its complement to 1, respectively, and control the shape of the distribution. The Beta distribution is frequently used in Bayesian statistics, empirical Bayes methods, and classical statistics to model random variables with values falling inside a finite interval.\nThe probability density function (PDF) of the Beta distribution is given by: \\[\nBeta(y; \\alpha, \\beta) = \\frac{y^{\\alpha - 1}(1 - y)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\] where \\(y \\in [0, 1]\\), \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\), and \\(B(\\alpha, \\beta)\\) is the beta function. It is simply a normalizing constant \\[\nB\\left( \\alpha,\\beta\\right)  =\\int_{0}^{1}y^{\\alpha-1}\\left(  1-y\\right)^{\\beta-1}dy .\n\\]\nThe mean and variance of the Beta distribution are given by: \\[\n\\begin{aligned}\n\\mu &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\sigma^2 &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{aligned}\n\\] where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance.\nFigure 3.2 illustrates the Beta distribution for different values of \\(\\alpha\\) and \\(\\beta\\).\n\n\nPlot Beta distributions\nx &lt;- seq(0, 1, length.out = 1000)\n\n# Define Beta distribution parameters for each curve\nbeta_params &lt;- list(\n  red = c(0.5, 0.5),      # U-shaped\n  blue = c(2, 1),         # Increasing\n  green = c(1, 2),        # Decreasing\n  purple = c(2, 2),       # Bell-shaped\n  orange = c(5, 2)        # Asymmetric bell (skewed left)\n)\n\n# Calculate density values for each Beta distribution\ndensities &lt;- lapply(beta_params, function(params) {\n  dbeta(x, shape1 = params[1], shape2 = params[2])\n})\n\n\n# Create empty plot\nplot(1, type = \"n\", xlim = c(0, 1), ylim = c(0, 2.5),xlab = \"\", ylab = \"\")\n# Plot each curve\ncolors &lt;- c(\"red\", \"blue\", \"green\", \"purple\", \"orange\")\nfor (i in seq_along(densities)) {\n  lines(x, densities[[i]], col = colors[i], lwd = 2)\n}\n\n# Add legend\nlegend(\"top\", \n       legend = c(expression(paste(alpha, \" = \", beta, \" = 0.5\")), \n                  expression(paste(alpha, \" = 5, \", beta, \" = 1\")),\n                  expression(paste(alpha, \" = 1, \", beta, \" = 3\")),\n                  expression(paste(alpha, \" = 2, \", beta, \" = 2\")),\n                  expression(paste(alpha, \" = 2, \", beta, \" = 5\"))),\n       col = colors, lty = 1, lwd = 2, bty='n')\n\n\n\n\n\n\n\n\nFigure 3.2: Beta distributions\n\n\n\n\n\nThe Beta-Binomial model is one of the simplest Bayesian models and is widely used in various fields including epidemiology, intelligence testing, and marketing. It provides the tools we need to study the proportion of interest, \\(\\theta\\), in a variety of settings.\nThe nice property of the Beta-Binomial model is that the posterior \\[\np(\\theta \\mid y) = \\dfrac{p(y \\mid \\theta)p(\\theta)}{p(y)}\n\\] \\(p(\\theta\\mid y)\\) is yet another Beta distribution. Beta is called a conjugate prior for the Binomial likelihood and is a very useful property.\n\n\n\n\n\n\nNoteConjugate prior\n\n\n\nA prior is called a conjugate prior for a likelihood function if the posterior distribution is of the same family as the prior distribution.\n\n\nWhen \\(m=1\\) (observations follow the Bernoulli distribution), the posterior is given by \\[\np(\\theta\\mid Y) = Beta(Y+\\alpha, 1-Y+\\beta)\n\\] where \\(Y\\) is the number of successful outcomes \\[\nY = \\sum_{i=1}^n y_i,\n\\] where \\(y_i \\mid \\theta \\sim Bernoulli(\\theta)\\).\nHere the count of successful outcome \\(Y\\) acts as a sufficient statistic for the parameter \\(\\theta\\). This means that the posterior distribution depends on the data only through the sufficient statistic \\(Y\\). This is a very useful property and is a consequence of the conjugacy of the Beta prior and Binomial likelihood.\nIn the case of \\(n&gt;1\\) (observations follow the Binomial distribution), the posterior is given by \\[\n\\theta\\mid Y \\sim Beta(Y+\\alpha, n-Y+\\beta)\n\\] where \\(n\\) is the number of observations and \\(Y\\) is the number of successful outcomes as before. \\[\nY = \\sum_{i=1}^n y_i,\n\\] where \\(y_i \\mid \\theta \\sim Binomial(n,\\theta)\\).\nThe posterior mean and variance are \\[\n\\mathbb{E}\\left[ \\theta\\mid Y\\right]  =\\frac{\\alpha_{n}}{\\alpha_{n}+\\beta_{n}} \\;\\text{ and }\\; \\Var{\n\\theta\\mid Y}  =\\frac{\\alpha_{n}\\beta_{n}}{\\left(  \\alpha_{n}+\\beta_{n}\\right)  ^{2}\\left(   \\alpha_{n}+\\beta_{n}+1\\right)  }\\text{,}\n\\] where \\(\\alpha_{n} = \\alpha + Y\\) and \\(\\beta_{n} = \\beta + n - Y\\).\n\nExample 3.2 (Black Swans) A related problem is the Black Swan inference problem. Suppose that after \\(n\\) trials where \\(n\\) is large you have only seen successes and that you assess the probability of the next trial being a success as \\((T+1)/(T+2)\\) that is, almost certain. This is a model of observing White Swans and having never seen a Black Swan. Taleb (2007) makes it sound as if the rules of probability are not rich enough to be able to handle Black Swan events. There is a related class of problems in finance known as Peso problems where countries decide to devalue their currencies and there is little prior evidence from recent history that such an event is going to happen.\nTo obtain such a probability assessment we use a Binomial/Beta conjugate Bayes updating model. The key point is that it can also explain that there is still a large probability of a Black Swan event to happen sometime in the future. An independence model has difficulty doing this.\nThe Bayes Learning Beta-Binomial model will have no problem. We model with \\(y_{t}=0\\) or \\(1\\), with probability \\(P\\left( y_{t}=1\\mid \\theta\\right) =\\theta\\). This is the classic Bernoulli “coin-flipping” model and is a component of more general specifications such as regime switching or outlier-type models.\nLet \\(Y = \\sum_{t=1}^{T}y_{t}\\) be the number of observed successful outcomes. The likelihood for a sequence of Bernoulli observations is then \\[\np\\left(  y\\mid \\theta\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t}\\mid \\theta\\right)\n=\\theta^{Y}\\left(  1-\\theta\\right)^{T-Y}.\n\\] The maximum likelihood estimator is the sample mean, \\(\\widehat{\\theta} = T^{-1}Y\\). This makes little sense when you just observe white swans. It predicts \\(\\hat{\\theta} = 1\\) and gets shocked when it sees a black swan (zero probability event). Bayes, on the other hand, allows for “learning”.\nBayes rule then tells us how to combine the likelihood and prior to obtain a posterior distribution, namely \\(\\theta \\mid Y=y\\). What do we believe about \\(\\theta\\) given a sequence of observations? Our predictor rule is then \\(P(y_{t+1} =1 \\mid Y=y ) = \\mathbb{E}(\\theta \\mid y)\\) and it is straightforward to show that the posterior distribution is again a Beta distribution with \\[\np\\left( \\theta\\mid y\\right)  \\sim Beta\\left(  \\alpha_{n},\\beta_{n}\\right)  \\; \\mathrm{ and} \\;  \\alpha_{n}=\\alpha+k , \\beta_{n}=\\beta+T-k.\n\\]\n\n\nExample 3.3 (Clinical Trials) Consider a problem of designing clinical trials in which \\(K\\) possible drugs \\(a\\in 1,\\dots,K\\) need to be tested. The outcome of the treatment with drug \\(a\\) is binary \\(y(a) \\in \\{0,1\\}\\). We use Bernoulli distribution with mean \\(f(a)\\) to model the outcome. Thus, the full probabilistic model is described by \\(w = f(1),\\dots,f(K)\\). Say we have observed a sample \\(D = \\{y_1,\\dots,y_n\\}\\). We would like to compute posterior distribution over \\(w\\). We start with a Beta prior \\[\np(w\\mid \\alpha,\\beta) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha,\\beta)    \n\\] Then the posterior distribution is given by \\[\np(w\\mid D) = \\prod_{a=1}^K Beta(w_a\\mid \\alpha + n_{a,1},\\beta + n_{a,0})   \n\\]\nThis setup allows us to perform sequential design of experiments. The simplest version of it is called Thompson sampling. After observing \\(n\\) patients, we draw a single sample \\(\\tilde w\\) from the posterior and then maximize the resulting surrogate \\[\na_{n+1} = \\arg\\max_{a} f_{\\tilde w}(a), ~~~ \\tilde{w} \\sim p(w\\mid D)\n\\]\n\n\nExample 3.4 (Shrinkage and Baseball Batting Averages) The batter-pitcher match-up is a fundamental element of a baseball game. There are detailed baseball records that are examined regularly by fans and professionals. This data provides a good illustration of Bayesian hierarchical methods. There is a great deal of prior information concerning the overall ability of a player. However, we only see a small amount of data about a particular batter-pitcher match-up. Given the relatively small sample size, to determine our optimal estimator we build a hierarchical model taking into account the within pitcher variation.\nLet’s analyze the variability in Jeter’s \\(2006\\) season. Let \\(p_{i}\\) denote Jeter’s ability against pitcher \\(i\\) and assume that \\(p_{i}\\) varies across the population of pitchers according to a particular probability distribution \\((p_{i} \\mid \\alpha,\\beta)\\sim Be(\\alpha,\\beta)\\). To account for extra-binomial variation we use a hierarchical model for the observed number of hits \\(y_{i}\\) of the form \\[\n(y_{i} \\mid p_{i})\\sim Bin(T_{i},p_{i})\\;\\;\\mathrm{with}\\;\\;p_{i}\\sim\nBe(\\alpha,\\beta)\n\\] where \\(T_{i}\\) is the number of at-bats against pitcher \\(i\\). A priori we have a prior mean given by \\(E(p_{i})=\\alpha/(\\alpha+\\beta)=\\bar{p}\\). The extra heterogeneity leads to a prior variance \\(Var(p_{i})=\\bar{p}(1-\\bar{p})\\phi\\) where \\(\\phi=(\\alpha+\\beta+1)^{-1}\\). Hence \\(\\phi\\) measures how concentrated the beta distribution is around its mean, \\(\\phi=0\\) means highly concentrated and \\(\\phi=1\\) means widely dispersed. \nThis model assumes that each player \\(i\\) has a true ability \\(p_{i}\\) that is drawn from a common distribution. The model is hierarchical in the sense that the parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from the data. The model is also a shrinkage model in the sense that the estimates of \\(p_{i}\\) are shrunk towards the overall mean \\(\\bar{p}_{i}\\). In reality, we don’t know that each \\(p_i\\) exists. We also don’t know if it follows a Binomial distribution with the Beta prior. We are making a model assumption. However, the model is a good approximation to the data and is a good way to estimate the parameters.\nStern et al. (2007) estimates the parameter \\(\\hat{\\phi} = 0.002\\) for Derek Jeter, showing that his ability varies a bit but not very much across the population of pitchers. The effect of the shrinkage is not surprising. The extremes are shrunk the most with the highest degree of shrinkage occurring for the match-ups that have the smallest sample sizes. The amount of shrinkage is related to the large amount of prior information concerning Jeter’s overall batting average. Overall Jeter’s performance is extremely consistent across pitchers as seen from his estimates. Jeter had a season \\(.308\\) average. We see that his Bayes estimates vary from\\(.311\\) to\\(.327\\) and that he is very consistent. If all players had a similar record then the assumption of a constant batting average would make sense.\n\n\n\nPitcher\nAt-bats\nHits\nObsAvg\nEstAvg\n95% Int\n\n\n\n\nR. Mendoza\n6\n5\n.833\n.322\n(.282, .394)\n\n\nH. Nomo\n20\n12\n.600\n.326\n(.289, .407)\n\n\nA.J.Burnett\n5\n3\n.600\n.320\n(.275, .381)\n\n\nE. Milton\n28\n14\n.500\n.324\n(.291, .397)\n\n\nD. Cone\n8\n4\n.500\n.320\n(.218, .381)\n\n\nR. Lopez\n45\n21\n.467\n.326\n(.291, .401)\n\n\nK. Escobar\n39\n16\n.410\n.322\n(.281, .386)\n\n\nJ. Wettland\n5\n2\n.400\n.318\n(.275, .375)\n\n\nT. Wakefield\n81\n26\n.321\n.318\n(.279, .364)\n\n\nP. Martinez\n83\n21\n.253\n.312\n(.254, .347)\n\n\nK. Benson\n8\n2\n.250\n.317\n(.264, .368)\n\n\nT. Hudson\n24\n6\n.250\n.315\n(.260, .362)\n\n\nJ. Smoltz\n5\n1\n.200\n.314\n(.253, .355)\n\n\nF. Garcia\n25\n5\n.200\n.314\n(.253, .355)\n\n\nB. Radke\n41\n8\n.195\n.311\n(.247, .347)\n\n\nD. Kolb\n5\n0\n.000\n.316\n(.258, .363)\n\n\nJ. Julio\n13\n0\n.000\n.312\n(.243, .350 )\n\n\nTotal\n6530\n2061\n.316\n\n\n\n\n\nSome major league managers believe strongly in the importance of such data (Tony La Russa, Three days in August). One interesting example is the following. On Aug 29, 2006, Kenny Lofton (career \\(.299\\) average, and current \\(.308\\) average for \\(2006\\) season) was facing the pitcher Milton (current record \\(1\\) for \\(19\\)). He was rested and replaced by a \\(.273\\) hitter. Is putting in a weaker player really a better bet? Was this just an over-reaction to bad luck in the Lofton-Milton match-up? Statistically, from Lofton’s record against Milton we have \\(P\\left( \\leq 1\\;\\mathrm{hit\\;in}\\ 19\\;\\mathrm{attempts} \\mid p=0.3\\right) =0.01\\) an unlikely \\(1\\)-in-\\(100\\) event. However, we have not taken into account the multiplicity of different batter-pitcher match-ups. We know that Lofton’s batting percentage will vary across different pitchers, it’s just a question of how much? A hierarchical analysis of Lofton’s variability gave a \\(\\phi=0.008\\) – four times larger than Jeter’s \\(\\phi=0.002\\). Lofton has batting estimates that vary from \\(.265\\) to \\(.340\\) with the lowest being against Milton. Hence, the optimal estimate for a pitch against Milton is \\(.265&lt;.275\\) and resting Lofton against Milton is justified by this analysis.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-model-for-count-data",
    "href": "03-bl.html#poisson-model-for-count-data",
    "title": "3  Bayesian Learning",
    "section": "3.4 Poisson Model for Count Data",
    "text": "3.4 Poisson Model for Count Data\nThe Poisson distribution is obtained as a result of the Binomial when \\(p\\) is small and \\(n\\) is large. In applications, the Poisson models count data. Suppose we want to model the arrival rate of users to one of our stores. Let \\(\\lambda = np\\), which is fixed and take the limit as \\(n \\rightarrow \\infty\\). There is a relationship between \\(p(x)\\) and \\(p(x+1)\\) given by \\[\n\\dfrac{p(x+1)}{p(x)}= \\dfrac{\\left(\\dfrac{n}{x+1}\\right)p^{x+1}(1-p)^{n-x-1}}{\\left(\\dfrac{n}{x}\\right)p^{x}(1-p)^{n-x}} \\approx \\dfrac{np}{x+1}\n\\] If we approximate \\(p(x+1)\\approx \\lambda p(x)/(x+1)\\) with \\(\\lambda=np\\), then we obtain the Poisson pdf given by \\(p(x) = p(0)\\lambda^x/x!\\). To ensure that \\(\\sum_{x=0}^\\infty p(x) = 1\\), we set \\[\nf(0) = \\dfrac{1}{\\sum_{x=0}^{\\infty}\\lambda^x/x!} = e^{-\\lambda}.\n\\] The above equality follows from the power series property of the exponent function \\[\ne^{\\lambda} = \\sum_{x=0}^{\\infty}\\dfrac{\\lambda^x}{x!}\n\\] The Poisson distribution counts the occurrence of events. Given a rate parameter, denoted by \\(\\lambda\\), we calculate probabilities as follows \\[\np( X = x ) = \\frac{ e^{-\\lambda} \\lambda^x }{x!} \\; \\mathrm{ where} \\; x=0,1,2,3, \\ldots\n\\] For \\(n\\) independent Poisson observations \\(x_1,\\ldots,x_n\\), the sufficient statistic for \\(\\lambda\\) is the sum \\(\\sum_{i=1}^n x_i\\). The mean and variance of the Poisson are given by:\n\n\n\nPoisson Distribution\nParameters\n\n\n\n\nExpected value\n\\(\\mu = \\E{X} = \\lambda\\)\n\n\nVariance\n\\(\\sigma^2 = \\Var{X} = \\lambda\\)\n\n\n\nHere \\(\\lambda\\) denotes the rate of occurrence of an event.\nConsider the problem of modeling soccer scores in the English Premier League (EPL) games. We use data from Betfair, a website, which posts odds on many football games. The goal is to calculate odds for the possible scores in a match. \\[\n0-0, \\; 1-0, \\; 0-1, \\; 1-1, \\; 2-0, \\ldots\n\\]\nAnother question we might ask, is what’s the odds of a team winning? This is given by \\(P\\left ( X&gt; Y \\right )\\). The odds of a draw are given by \\(P \\left ( X = Y \\right )\\). Here \\(X\\) is the number of goals scored by the home team and \\(Y\\) is the number of goals scored by the away team.\nProfessional sports bettors rely on sophisticated statistical models to predict the outcomes. Instead, we present a simple, but useful model for predicting outcomes of EPL games. We follow the methodology given in Spiegelhalter and Ng (2009).\nTo make the discussion more concrete, we will use the data from the English Premier League (EPL) games from the 2014/2015 season. and will model the game between Manchester United and Hull City.\nFirst, load the data and then model the number of goals scored using Poisson distribution for each team.\n\ndf = read.csv(\"../data/epl.csv\")\nknitr::kable(head(df[,c(\"home_team_name\",\"away_team_name\",\"home_score\",\"guest_score\")]))\n\n\n\n\nhome_team_name\naway_team_name\nhome_score\nguest_score\n\n\n\n\nArsenal\nLiverpool\n3\n4\n\n\nBournemouth\nManchester United\n1\n3\n\n\nBurnley\nSwansea\n0\n1\n\n\nChelsea\nWest Ham\n2\n1\n\n\nCrystal Palace\nWest Bromwich Albion\n0\n1\n\n\nEverton\nTottenham\n1\n1\n\n\n\n\n\nLet’s compare the empirical distribution across the number of goals scored by Manchester United to the Poisson distribution.\n\n\nCode\n# Extract all goals scored by Manchester United (home and away)\nteam_for &lt;- c(\n  df$home_score[df$home_team_name == \"Manchester United\"],\n  df$guest_score[df$away_team_name == \"Manchester United\"]\n)\n# Calculate empirical distribution\nfor_byscore &lt;- table(factor(team_for, levels = 0:4)) / length(team_for)\n\n\n\n\n#| fig-cap: \"Histogram vs Poisson Model Prediction of Goals Scored by MU\"\nlambda_for = mean(team_for) \nbarplot(rbind(dpois(0:4, lambda = lambda_for),for_byscore),beside = T, col=c(\"aquamarine3\",\"coral\"), xlab=\"Goals\", ylab=\"probability\", main=\"\") \nlegend(\"topright\", c(\"Poisson\",\"MU\"), pch=15, col=c(\"aquamarine3\", \"coral\"), bty=\"n\")\n\n\n\n\n\n\n\n\nHence the historical data fits closely to a Poisson distribution, the parameter \\(\\lambda\\) describes the average number of goals scored and we calculate it by calculating the sample mean, the maximum likelihood estimate. A Bayesian method where we assume that \\(\\lambda\\) has a Gamma prior is also available. This lets you incorporate outside information into the predictive model.\nNow we will use Poisson model and Monte Carlo simulations to predict possible outcomes of the MU vs Hull games. First we estimate the rate parameter for goals by MU lmb_mu and goals by Hull lmb_h. Each team played a home and away game with every other team, thus 38 total games was played by all teams. We calculate the average by dividing total number of goals scored by the number of games\n\n\nSummarizing the data\nsumdf = df %&gt;% \n  group_by(home_team_name) %&gt;% \n  summarise(Goals_For_Home = sum(home_score)) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;% \n              summarise(Goals_For_Away = sum(guest_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(home_team_name) %&gt;% \n              summarise(Goals_Against_Home = sum(guest_score))\n            ) %&gt;%\n  full_join(df %&gt;% \n              group_by(away_team_name) %&gt;%\n              summarise(Goals_Against_Away = sum(home_score)), by = c(\"home_team_name\" = \"away_team_name\")\n            ) %&gt;%\n  rename(Team=home_team_name)\nsumdf[sumdf$Team %in% c(\"Manchester United\", \"Hull\"),] %&gt;%\n  rename(GF_H = Goals_For_Home, GF_A = Goals_For_Away, \n         GA_H = Goals_Against_Home, GA_A = Goals_Against_Away) %&gt;%\n  knitr::kable()\n\n\n\n\n\nTeam\nGF_H\nGF_A\nGA_H\nGA_A\n\n\n\n\nHull\n28\n9\n35\n45\n\n\nManchester United\n26\n28\n12\n17\n\n\n\nSummarizing the data\n\n\n\nlmb_mu = (26+28)/38\nlmb_h = (28+9)/38\n\nNow we simulate 100 games between the teams\n\nx = rpois(100,lmb_mu)\ny = rpois(100,lmb_h)\nknitr::kable(table(x,y))\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n9\n7\n2\n3\n1\n1\n\n\n1\n7\n15\n15\n3\n1\n0\n\n\n2\n8\n4\n1\n1\n0\n0\n\n\n3\n7\n5\n2\n1\n0\n0\n\n\n4\n0\n5\n2\n0\n0\n0\n\n\n\n\n\nFrom our simulation that sum(x&gt;y): 40 number of times MU wins and sum(x==y): 26 there is a draw. The actual outcome was 0-0 (Hull at MU) and 0-1 (Mu at Hull). Thus our model gives a reasonable prediction.\nThe model can be improved by calculating different averages for home and away games. For example, Hull does much better at home games compared to away games. Further, we can include the characteristics of the opponent team to account for interactions between attack strength (number of scored) and defense weakness of the opponent. Now we modify our value of expected goals for each of the teams by calculating \\[\n\\hat \\lambda = \\lambda \\times  \\text{Defense weakness}\n\\]\nLet’s model the MU at Hull game. The average away goals for MU \\(28/19 = 1.4736842\\) and the defense weakness of Hull is \\(35/19 = 1.8421053\\), thus the adjusted expected number of goals to be scored by MU is 2.7146814. Similarly, the adjusted number of goals Hull is expected to score is \\(28/19 \\times 17/19 = 1.3185596\\)\nAs a result of the simulation, we obtain the following count matrix of possible outcomes, shown in Figure 3.3.\n\nset.seed(1)\nx &lt;- rpois(100, 28 / 19 * 35 / 19)\ny &lt;- rpois(100, 28 / 19 * 17 / 19)\nimage(z = table(x, y), x = 0:7, y = 0:5, xlab = \"MU Score\", ylab = \"Hull Score\")\n\n\n\n\n\n\n\nFigure 3.3: Simulation of MU vs Hull game\n\n\n\n\n\nNow we can calculate the number of times MU wins:\n\nsum(x &gt; y)\n## 67\n\n\n\n\n\nA model is only as good as its predictions. Let’s see how well our model predicted the outcome of the MU vs Hull game. The actual outcome was 0-1 (MU at Hull). The model predicted that most likely MU would score 1-2 (16 games out of 100). In our simulation 0-1 was the third most probable outcome (8 games out of 100). Man U wins 67 games out of 100, we should bet when odds ratio is below 67 to 100.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#poisson-gamma-learning-about-an-intensity",
    "href": "03-bl.html#poisson-gamma-learning-about-an-intensity",
    "title": "3  Bayesian Learning",
    "section": "3.5 Poisson-Gamma: Learning about an Intensity",
    "text": "3.5 Poisson-Gamma: Learning about an Intensity\nConsider a continuous-time stochastic process, \\(\\left\\{ N_{t}\\right\\} _{t\\geq0}\\), with \\(N_{0}=0\\), counting the number of events that have occurred up to time \\(t\\). The process is constant between event times, and jumps by one at event times: \\(\\Delta N_{t}=N_{t}-N_{t-}=1,\\) where \\(N_{t-}\\) is the limit from the left. The probability of an event over the next short time interval, \\(\\Delta t\\) is \\(\\lambda\\Delta t\\), and \\(N_{t}\\) is called a Poisson process because \\[\nP\\left[  N_{t}=k\\right]  =\\frac{e^{-\\lambda t}\\left(  \\lambda\nt\\right)  ^{k}}{k!}\\text{ for }k=1,\\ldots\n\\] which is the Poisson distribution, thus \\(N_{t}\\sim Poi\\left(\\lambda t\\right)\\). A more general version of the Poisson process is a Cox process, or doubly stochastic point process.\nHere, there is additional conditioning information in the form of state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\). The process now has two sources of randomness, one associated with the discontinuous jumps and another in the form of random state variables, \\(\\left\\{X_{t}\\right\\}_{t&gt;0}\\), that drive the intensity of the process. The intensity of the Cox process is \\(\\lambda_{t}=\\int_{0}^{t}\\lambda\\left( X_{s}\\right) ds\\), which is formally defined as \\[\nP\\left[  N_{t}-N_{s}=k \\mid \\left\\{  X_{u}\\right\\}  _{s\\leq u\\leq\nt}\\right]  =\\frac{\\left(  \\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)\n^{k}\\exp\\left(  -\\int_{s}^{t}\\lambda\\left(  X_{s}\\right)  ds\\right)}{k!}, ~ k=0,1,\\ldots\n\\] Cox processes are very useful extensions to Poisson processes and are the basic building blocks of reduced form models of defaultable bonds.\nThe inference problem is to learn about \\(\\lambda\\) from a continuous-record of observation up to time \\(t\\). The likelihood function is given by \\[\np\\left(  N_{t}=k \\mid \\lambda\\right)  =\\frac{\\left(  \\lambda t\\right)  ^{k}%\n\\exp\\left(  -\\lambda t\\right)  }{k!},\n\\] and the MLE is \\(\\widehat{\\lambda}=N_{t}/t\\). Notice that the total count \\(N_t\\) and elapsed time \\(t\\) together form the sufficient statistic for \\(\\lambda\\), since the likelihood depends on the data only through these two quantities. The MLE has the unattractive property that prior to the first event \\(\\left\\{ t:N_{t}=0\\right\\}\\), the MLE is 0, despite the fact that the model explicitly assumes that events are possible. This problem often arises in credit risk contexts, where it would seem odd to assume that the probability of default is zero just because a default has not yet occurred.\nA natural prior for this model is the Gamma distribution, which has the following pdf \\[\np\\left(  \\lambda \\mid a,A\\right)  =\\frac{A^{a}}{\\Gamma(a)  }\\lambda^{a-1}\\exp\\left(  -A\\lambda\\right)  \\text{.}\n\\tag{3.2}\\] Like the beta distribution, a Gamma prior distribution allows for a variety of prior shapes and is parameterized by two hyperparameters. Combining the prior and likelihood, the posterior is also Gamma: \\[\np\\left(  \\lambda \\mid N_{t}\\right)  \\propto\\frac{\\left(  \\lambda\\right)\n^{N_{t}+a-1}\\exp\\left(  -\\lambda\\left(  t+A\\right)  \\right)  }{N_{t}!}%\n\\sim\\mathcal{G}\\left(  a_{t},A_{t}\\right)  ,\n\\] where \\(a_{t}=N_{t}+a\\) and \\(A_{t}=t+A\\). The expected intensity, based on information up to time \\(t\\), is \\[\n\\mathbb{E}\\left[  \\lambda \\mid N_{t}\\right]  =\\frac{a_{t}}{A_{t}}=\\frac{N_{t}%\n+a}{t+A}=w_{t}\\frac{N_{t}}{t}+\\left(  1-w_{t}\\right)  \\frac{a}{A},\n\\] where the second line expresses the posterior mean in shrinkage form as a weighted average of the MLE and the prior mean where \\(w_{t}=t/(t+A)\\). In large samples, \\(w_{t}\\rightarrow1\\) and \\(E\\left( \\lambda \\mid N_{t}\\right) \\approx N_{t}/t=\\widehat{\\lambda}\\).\nTo understand the updating mechanics, Figure 3.4 (right column) displays a simulated sample path, posterior means, and (5%,95%) posterior quantiles for various prior configurations. In this case, time is measured in years and the intensity used to simulate the data is \\(\\lambda=1\\), implying on average one event per year. The four prior configurations embody different beliefs. In the first case, in the middle left panel, \\(a=4\\) and \\(A=1\\), captures a high-activity prior, that posits that jumps occur, on average, four times per year, and there is substantial prior uncertainty over the arrival rate as the (5%,95%) prior quantiles are (1.75,6.7). In the second case, captures a prior that is centered over the true value with modest prior uncertainty. The third case captures a low-activity prior, with a prior mean of 0.2 jumps/year. The fourth case captures a dogmatic prior, that posits that jumps occur three times per year, with high confidence in these beliefs.\nThe priors were chosen to highlight different potential paths for Bayesian learning. The first thing to note from the priors is the discontinuity upward at event times, and the exponential decrease during periods of no events, both of which are generic properties of Bayesian learning in this model. If one thinks of the events as rare, this implies rapid revisions in beliefs at event times and a constant drop in estimates of the intensity in periods of no events. For the high-activity prior and the sample path observed, the posterior begins well above \\(\\lambda=1\\), and slowly decreases, getting close to \\(\\lambda=1\\) at the end of the sample. This can be somewhat contrasted with the low-activity prior, which has drastic revisions upward at jump times. In the dogmatic case, there is little updating at event times. The prior parameters control how rapidly beliefs change, with noticeable differences across the priors.\nIn all cases, the orange line shows the cumulative event count \\(N_t\\), the blue dashed line represents the posterior mean of \\(\\lambda\\), and the grey dashed lines indicate the 5% and 95% posterior quantiles. The discontinuous upward jumps at event times and exponential decay during quiet periods are characteristic features of Bayesian learning in Poisson processes.\n\nSensitivity of Gamma Prior for Poisson Process\nset.seed(8) # Ovi\nt = 1:5\nlmb = 1\nN = rpois(5,t*lmb)\n\n# A: rate (beta), a: shape (alpha)\nplotgamma = function(a,A,N) {\n    x = seq(0,10,0.01)\n    plot(x,dgamma(x,a,A),type=\"l\",xlab=\"t\",ylab=\"Gamma(t)\",lwd=2)\n    at = N+a\n    At = t+A\n    mean = at/At\n    plot(N, type='l', col=\"orange\", ylim=c(0,5), xlab=\"t\", ylab=\"N(t)\", lwd=2)\n    lines(mean, col=\"blue\", lwd=3, lty=2)\n    lines(qgamma(0.05,at,At), col=\"grey\", lwd=2, lty=2)\n    lines(qgamma(0.95,at,At), col=\"grey\", lwd=2, lty=2)\n}\nplotgamma(a=4,A=1, N)\nplotgamma(a=1,A=1, N)\nplotgamma(a=1,A=5, N)\nplotgamma(a=30,A=10, N)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) High-activity: a = 4, A = 1; 4 events/year, Substantial prior uncertainty\n\n\n\n\n\n\n\n\n\n\n\n(b) Posterior: Starts high, gradually decreases toward true value\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Diffuse: a = 1, A = 1; 1 event/year, Minimal prior information\n\n\n\n\n\n\n\n\n\n\n\n(d) Posterior: Data dominates inference relatively quickly\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Low-activity: a = 1, A = 5; 0.2 events/year, Rare events expected\n\n\n\n\n\n\n\n\n\n\n\n(f) Posterior: The low prior count dominates the posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) Dogmatic: a = 30, A = 10; 3 events/year, Strong prior beliefs\n\n\n\n\n\n\n\n\n\n\n\n(h) Posterior: Updates slowly, resistant to contradictory evidence\n\n\n\n\n\n\n\nFigure 3.4: Sensitivity of Gamma Prior for Poisson Process\n\n\n\nPoisson event models are often embedded as portion of more complicated model to capture rare events such as stock market crashes, volatility surges, currency revaluations, or defaults. In these cases, prior distributions are often important–even essential–since it is common to build models with events that could, but have not yet occurred. These events are often called ‘Peso’ events. For example, in the case of modeling corporate defaults a researcher wants to allow for a jump to default. This requires positing a prior distribution that places non-zero probability on an event occurring. Classical statistical methods have difficulties dealing with these situations since the MLE of the jump probability is zero, until the first event occurs.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#exponential-gamma-model",
    "href": "03-bl.html#exponential-gamma-model",
    "title": "3  Bayesian Learning",
    "section": "3.6 Exponential-Gamma Model",
    "text": "3.6 Exponential-Gamma Model\nThe Exponential distribution is often used to model waiting times between events, such as the time between independent arrivals in a Poisson process. The probability density function (PDF) is defined as: \\[\np(x \\mid \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n\\] where \\(\\lambda &gt; 0\\) is the rate parameter (inverse of the mean).\nThe Exponential-Gamma model assumes that the data follows an exponential distribution, and the rate parameter \\(\\lambda\\) follows a Gamma prior distribution. \\[\\begin{align*}\n    \\lambda &\\sim \\text{Gamma}(\\alpha, \\beta) \\\\\n    y_i \\mid \\lambda &\\sim \\text{Exponential}(\\lambda)\n\\end{align*}\\]\nThe probability density function of the Gamma prior is: \\[\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda}\n\\]\nGiven \\(n\\) observations \\(y = (y_1, \\ldots, y_n)\\), the likelihood depends on the data only through the count \\(n\\) and the sum \\(\\sum_{i=1}^n y_i\\), which form the sufficient statistics for \\(\\lambda\\). The posterior distribution of \\(\\lambda\\) is: \\[\np(\\lambda\\mid y) \\propto p(y \\mid \\lambda) p(\\lambda) \\propto \\left( \\prod_{i=1}^n \\lambda e^{-\\lambda y_i} \\right) \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} = \\lambda^{\\alpha + n - 1} e^{-(\\beta + \\sum y_i)\\lambda}\n\\] This is a Gamma distribution with updated parameters: \\[\n\\lambda \\mid y \\sim \\text{Gamma}\\left(\\alpha + n, \\beta + \\sum_{i=1}^n y_i\\right).\n\\] The posterior mean is: \\[\n\\mathbb{E}[\\lambda \\mid y] = \\frac{\\alpha + n}{\\beta + \\sum y_i}.\n\\]\nThis model is widely used in reliability engineering (failure rates) and survival analysis, where the rate of events is uncertain and varies across populations.\nThe Normal or Gaussian distribution is central to probability and statistical inference. Suppose that we are trying to predict tomorrow’s return on the S&P500. There’s a number of questions that come to mind\n\nWhat is the random variable of interest?\nHow can we describe our uncertainty about tomorrow’s outcome?\nInstead of listing all possible values we’ll work with intervals instead. The probability of an interval is defined by the area under the probability density function.\n\nReturns are continuous (as opposed to discrete) random variables. Hence a normal distribution would be appropriate - but on what scale? We will see that on the log-scale a Normal distribution provides a good approximation.\nThe most widely used model for a continuous random variable is the normal distribution. Standard normal random variable \\(Z\\) has the following properties\nThe standard Normal has mean \\(0\\) and has a variance \\(1\\), and is written as \\[\nZ \\sim N(0,1)\n\\] Then, we have the probability statements of interest \\[\\begin{align*}  \nP(-1 &lt;Z&lt; 1) &=0.68\\\\\nP(-1.96 &lt;Z&lt; 1.96) &=0.95\\\\\n\\end{align*}\\]\nIn R, we can find probabilities pnorm(1.96): 0.9750021 and quantiles qnorm(0.9750): 1.959964. The quantile function qnorm is the inverse of pnorm.\nA random variable that follows normal distribution with general mean and variance \\(X \\sim \\mbox{N}(\\mu, \\sigma^2)\\), has the following properties \\[\\begin{align*}\n  p(\\mu - 2.58 \\sigma &lt; X &lt; \\mu + 2.58 \\sigma) &=0.99 \\\\\n  p(\\mu - 1.96 \\sigma &lt; X &lt; \\mu + 1.96 \\sigma) &=0.95 \\, .\n\\end{align*}\\] The chance that \\(X\\) will be within \\(2.58 \\sigma\\) of its mean is \\(99\\%\\), and the chance that it will be within \\(2\\sigma\\) of its mean is about \\(95\\%\\).\nThe probability model is written \\(X \\sim N(\\mu,\\sigma^2)\\), where \\(\\mu\\) is the mean, \\(\\sigma^2\\) is the variance. This can be transformed to a standardized normal via \\[\nZ =\\frac{X-\\mu}{\\sigma} \\sim N(0,1).\n\\] For a Normal distribution, we know that \\(X \\in [\\mu-1.96\\sigma,\\mu+1.96\\sigma]\\) with probability 95%. This is a specific property of the Normal curve (the “Empirical Rule”). For any distribution, regardless of shape, we can use Chebyshev’s Inequality to bound the probability mass. It states that at least \\(100(1-1/k^2)\\)% of values lie within \\(k\\) standard deviations of the mean:\n\nAt least 75% probability lies within \\(2\\sigma\\) (\\(k=2\\)).\nAt least 89% probability lies within \\(3\\sigma\\) (\\(k=3\\)).\nAt least \\(100(1-1/m^2)\\)% lies within \\(m\\times \\sigma\\) of the mean \\(\\mu\\).\n\nWhile the Normal distribution guarantees 95% within \\(2\\sigma\\), Chebyshev guarantees only 75%, reflecting the uncertainty given a lack of distributional assumptions.\n\nExample 3.5 (Google Stock 2019) Consider observations of daily log-returns of a Google stock for 2019 Daily log-return on day \\(t\\) is calculated by taking a logarithm of the ratio of price at close of day \\(t\\) and at close of day \\(t-1\\) \\[\n  y_t = \\log\\left(\\dfrac{P_t}{P_{t-1}}\\right)\n\\] For example on January 3 of 2017, the open price is 778.81 and close price was 786.140, then the log-return is \\(\\log(786.140/778.81) =  -0.0094\\). It was empirically observed that log-returns follow a Normal distribution. This observation is a basis for Black-Scholes model with is used to evaluate future returns of a stock.\n\np = read.csv(\"../data/GOOG2019.csv\")$Adj.Close; n = length(p) \nr = log(p[2:n]/p[1:(n-1)]) \nhist(r, breaks=30, col=\"lightblue\", main=\"\")\n\n\n\n\n\n\n\n\nObservations on the far right correspond to the days when positive news was released and on the far left correspond to bad news. Typically, those are days when the quarterly earnings reports are released.\nTo estimate the expected value \\(\\mu\\) (return) and standard deviation \\(\\sigma\\) (a measure of risk), we simply calculate their sample counterparts \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, ~\\mathrm{ and }~    s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x} )^2\n\\] The empirical (or sample) values \\(\\bar x\\) and \\(s^2\\) are called sample mean and sample variance. Here simply vie them as our best guess about the mean and variance of the normal distribution model then our probabilistic model for next day’s return is then given by \\[\nR \\sim N(\\bar x, s^2).\n\\]\nSay we are interested in investing into Google and would like to calculated the expected return of our investment as well as risk associated with this investment We assume that behavior of the returns in the future will be the same as in 2019.\n\nn = length(r) \nrbar = sum(r)/n; print(rbar) \n## 0.00098\ns2 = sum((r-rbar)^2)/(n-1); print(s2) \n## 0.00023\nx = seq(-0.08,0.08, length.out = 200) \nhist(r, breaks=30, col=\"lightblue\", freq = F, main=\"\", xlab=\"\") \nlines(x,dnorm(x,rbar,sqrt(s2)), col=\"red\", lwd=3)\n\n\n\n\nHistogram (blue) and fitted normal curve (red) of for the Google stock daily return data.\n\n\n\n\nNow, assume, I invest all my portfolio into Google. I can predict my annual return to be \\(251 \\times 0.0009798\\) = 0.2459348 and risk (volatility) of my investment is \\(\\sqrt{s^2}\\) = 1.5198424% a year.\nI can predict the risk of losing 3% or more in one day using my model is 1.93%.\n\npnorm(log(1-0.03), rbar, sqrt(s2))*100\n## 1.9\n\n\nsp = read.csv(\"../data/SPMonthly.csv\")$Adj.Close; n = length(sp) \nspret = sp[602:n]/sp[601:(n-1)]-1 # Calculate  1977-1987 returns \n\n\nmean(spret) \n## 0.012\nsd(spret)\n## 0.043",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-mean",
    "href": "03-bl.html#normal-with-unknown-mean",
    "title": "3  Bayesian Learning",
    "section": "3.7 Normal With Unknown Mean",
    "text": "3.7 Normal With Unknown Mean\nLet \\(Y\\) be a random variable with a normal distribution, \\(Y \\sim N(\\mu, \\sigma^2)\\). The mean \\(\\mu\\) is unknown, but the variance \\(\\sigma^2\\) is known. The likelihood function is given by \\[\np(y \\mid \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(y-\\mu)^2\\right)\n\\] The MLE of \\(\\mu\\) is \\(\\hat{\\mu} = \\bar{y}\\), the sample mean. Normal prior for the mean parameter \\(\\mu\\) is conjugate to the normal likelihood. \\[\n\\mu \\sim N(\\mu_0, \\sigma_0^2)\n\\] The posterior distribution is also normal. \\[\np(\\mu \\mid y) \\sim N(\\mu_n, \\sigma_n^2)\n\\]\nwhere \\[\n\\mu_n = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\bar{y}\n\\] and \\[\n\\sigma_n^2 = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\n\\] The posterior mean is a weighted average of the prior mean and the sample mean, with the weights being proportional to the precision of the prior and the likelihood. The posterior variance is smaller than the prior variance, and the sample size \\(n\\) appears in the denominator. The posterior mean is a shrinkage estimator of the sample mean, and the amount of shrinkage is controlled by the prior variance \\(\\sigma_0^2\\). A couple of observations \\[\n\\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ and } \\frac{n\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\rightarrow 1, \\text{ as } n \\rightarrow \\infty.\n\\] Further, \\[\n\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2} \\rightarrow 0 \\text{ as } n \\rightarrow \\infty.\n\\]\n\nExample 3.6 (Stylized Example) Assuming the prior distribution \\(\\mu \\sim N(-1,1)\\), say we observed \\(y=2\\) and we want to update our beliefs about \\(\\mu\\). The likelihood function is \\(p(y \\mid \\mu) = N(\\mu,2)\\), and the posterior distribution is \\[\np(\\mu \\mid y) \\propto p(y \\mid \\mu) p(\\mu) = N(y\\mid \\mu,2) N(\\mu\\mid -1,1) = N(-0.4,0.9).\n\\]\n\nmu0 = -1; sigma0 = 1\nybar = 2; sigma = 2\nmu1 = (mu0/sigma0^2 + ybar/sigma^2)/(1/sigma0^2 + 1/sigma^2)\nsigma1 = sqrt(1/(1/sigma0^2 + 1/sigma^2))\nsprintf(\"Posterior mean: %f, Posterior variance: %f\", mu1, sigma1)\n## \"Posterior mean: -0.400000, Posterior variance: 0.894427\"\n\nGraphically we can represent this as follows\n\n\nNorm-Norm Updating\n# The prior distribution \nmu = seq(-4,10,0.01)\ny = seq(-4,10,0.01)\n# Prior\nplot(mu,dnorm(mu,mu0,sigma0),type=\"l\",xlab=\"x\",ylab=\"p(x)\",lwd=3,col=\"blue\",ylim=c(0,0.5))\n# The likelihood function\nlines(y,dnorm(y,ybar,sigma),type=\"l\",lwd=3,col=\"red\")\n# The posterior distribution\nlines(y,dnorm(y,mu1,sigma1),type=\"l\",lwd=3,col=\"green\")\n# legend\nlegend(\"topright\", c(\"Prior\",\" Data (Likelihood)\",\"Posterior\"), pch=15, col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\n\nFigure 3.5: Norm-Norm Updating\n\n\n\n\n\nNote, the posterior mean is in between those of prior and likelihood and posterior variance is lower than variance of both prior and likelihood, this is effect of combining information from data and prior!\n\nMore generally, when we observe \\(n\\) independent and identically distributed (i.i.d.) data points \\(y_1,\\ldots,y_n\\) from a normal distribution with known variance \\(\\sigma^2\\), the likelihood function is given by \\[\np(y \\mid \\mu) = N(\\bar y\\mid \\mu,\\sigma^2/n),~ \\text{where}~ \\bar y = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] Note, that average over the observed data \\(\\bar y = \\mathrm{Ave}(y_1,\\ldots,y_n)\\) is the sufficient statistics for the mean \\(\\mu\\). The prior distribution is given by \\[\np(\\mu) = N(\\mu\\mid \\mu_0,\\sigma_0^2)\n\\] The posterior distribution is given by \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0-\\mu_0^2}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}-\\bar{y}^2}{2\\sigma^2/n}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\mu_0}{2\\sigma_0^2}}\\Bigg]\\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\bar{y}}{2\\sigma^2/n}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we combine the terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{(-\\mu^2+2\\mu\\mu_0)\\sigma^2 +(-\\mu^2+2\\mu\\bar{y})n\\sigma_0^2}{2\\sigma_0^2\\sigma^2}}\\Bigg]. \\\\\n\\end{split}\n\\] Now re-arrange and combine \\(\\mu^2\\) and \\(\\mu\\) terms \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2(n\\sigma_0^2+\\sigma^2)+2\\mu(\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2) }{2\\sigma_0^2\\sigma^2}}\\Bigg] \\\\\n& \\propto  \\exp\\Bigg[{\\frac{-\\mu^2+2\\mu\\left(\\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\right) }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Now we add constants which do not depend upon \\(\\mu\\) to complete the square in the numerator: \\[\n\\begin{split}\np(\\mu\\mid y)\n& \\propto  \\exp\\Bigg[{\\frac{-\\bigg(\\mu - \\frac{\\mu_0\\sigma^2 + \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\bigg)^2 }{2(\\sigma_0^2\\sigma^2) /(n\\sigma_0^2+\\sigma^2)}}\\Bigg]. \\\\\n\\end{split}\n\\] Finally we get the posterior mean \\[\n\\mu_n = \\frac{\\mu_0\\sigma^2+ \\bar{y}n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2} = \\mu_0\\frac{\\sigma^2}{n\\sigma_0^2+\\sigma^2} + \\bar{y}\\frac{n\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\n\\] and the posterior variance \\[\n\\sigma_n^2 = \\frac{\\sigma_0^2\\sigma^2}{n\\sigma_0^2+\\sigma^2}.\n\\]\n\nExample 3.7 (Chicago Bears 2014-2015 Season) The Chicago Bears are a professional American football team based in Chicago, Illinois. The Bears were a young team in 2014-2015, an were last in the their division. This season the Chicago Bears suffered back-to-back \\(50\\)-points defeats and lost to Patriots and Packers.\n\nPatriots-Bears \\(51-23\\)\nPackers-Bears \\(55-14\\)\n\nTheir next game was at home against the Minnesota Vikings. Current line against the Vikings was \\(-3.5\\) points. Slightly over a field goal. What’s the Bayes approach to learning the line? We use hierarchical data and Bayes learning to update our beliefs in light of new information. The current average win/lose this year can be modeled as a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We assume that \\(\\mu\\) is normally distributed with mean \\(\\mu_0\\) and standard deviation \\(\\tau\\). \\[\\begin{align*}\n\\bar{y} \\mid \\mu & \\sim N \\left ( \\mu , \\frac{\\sigma^2}{n} \\right ) \\sim N \\left ( \\mu , \\frac{18.34^2}{9} \\right )\\\\\n\\mu & \\sim N( 0 , \\tau^2 )\n\\end{align*}\\] Here \\(n =9\\) games so far. With \\(s = 18.34\\) points. We assume the pre-season prior mean \\(\\mu_0 = 0\\), standard deviation \\(\\tau = 4\\). Base on the observed data so-far: \\(\\bar{y} = -9.22\\).\nThe Bayes Shrinkage estimator is then \\[\n\\mathbb{E} \\left( \\mu \\mid \\tau, \\bar y  \\right) = \\frac{ \\tau^2 }{ \\tau^2 + \\frac{\\sigma^2}{n} }\\bar{y} .\n\\]\nThe shrinkage factor is \\(0.3\\)! That’s quite a bit of shrinkage. Why? Our updated estimator is \\[\n\\mathbb{E} \\left ( \\mu | \\bar{y} , \\tau \\right ) = - 2.75 &gt; -.3.5\n\\] where current line is \\(-3.5\\).\n\nBased on our hierarchical model this is an over-reaction. One point change on the line is about \\(3\\)% on a probability scale.\nAlternatively, calculate a market-based \\(\\tau\\) given line \\(=-3.5\\). \\[\n\\tau^2 = \\frac{\\sigma^2}{n} \\frac{1}{0.3^2} = 18.34^2 \\frac{1}{0.3^2} = 180.\n\\]\nThe market-based \\(\\tau\\) is \\(13.4\\) points.\n\n\nbears=c(-3,8,8,-21,-7,14,-13,-28,-41)\nprint(mean(bears)); print(sd(bears))\n## -9.2\n## 18\ntau=4\nsig2=sd(bears)*sd(bears)/9\nprint(tau^2/(sig2+tau^2)); print(0.29997*-9.22)\n## 0.3\n## -2.8\nprint(pnorm(-2.76/18))\n## 0.44\n\nHome advantage is worth \\(3\\) points. The actual result of the game is Bears 21, Vikings 13.\n\n\nPosterior Predictive\nAfter estimating the parameters, using the posterior distribution, we often want to predict future observations. This is done using the posterior predictive distribution. The posterior predictive distribution is the distribution of a new observation \\(y_{n+1}\\) given the observed data \\(y_1,\\ldots,y_n\\). The posterior predictive distribution is given by\n\\[\\begin{align*}\np(y_{n+1} \\mid & y_1,\\ldots,y_n) = \\int p(y_{n+1} \\mid \\mu) p(\\mu \\mid y_1,\\ldots,y_n) d\\mu \\\\\n&  = \\int N(y_{n+1} \\mid \\mu, \\sigma^2) N(\\mu \\mid \\mu_n, \\sigma_n^2) d\\mu = N(y_{n+1} \\mid \\mu_n, \\sigma_n^2 + \\sigma^2).\n\\end{align*}\\] This follows from the general properties of the Gaussian distribution",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#normal-with-unknown-variance",
    "href": "03-bl.html#normal-with-unknown-variance",
    "title": "3  Bayesian Learning",
    "section": "3.8 Normal With Unknown Variance",
    "text": "3.8 Normal With Unknown Variance\nConsider, another example, when mean \\(\\mu\\) is fixed and variance is a random variable which follows some distribution \\(\\sigma^2 \\sim p(\\sigma^2)\\). Given an observed sample \\(y\\), we can update the distribution over variance using the Bayes rule \\[\np(\\sigma^2 \\mid  y) = \\dfrac{p(y\\mid \\sigma^2 )p(\\sigma^2)}{p(y)}.\n\\] Now, the total probability in the denominator can be calculated as \\[\np(y) = \\int p(y\\mid \\sigma^2 )p(\\sigma^2) d\\sigma^2.\n\\]\nA conjugate prior that leads to analytically calculable integral for variance under the normal likelihood is the inverse Gamma. When the mean \\(\\mu\\) is known, the sufficient statistic for \\(\\sigma^2\\) is the sum of squared deviations \\(\\sum_{i=1}^n(y_i - \\mu)^2\\). Thus, if \\[\n\\sigma^2 \\mid  \\alpha,\\beta \\sim IG(\\alpha,\\beta) = \\dfrac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\sigma^{2(-\\alpha-1)}\\exp\\left(-\\dfrac{\\beta}{\\sigma^2}\\right)\n\\] and \\[\ny \\mid \\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\n\\] Then the posterior distribution is another inverse Gamma \\(IG(\\alpha_{\\mathrm{posterior}},\\beta_{\\mathrm{posterior}})\\), with \\[\n\\alpha_{\\mathrm{posterior}} = \\alpha + \\dfrac{1}{2}, ~~\\beta_{\\mathrm{posterior}} = \\beta + \\dfrac{y-\\mu}{2}.\n\\]\nNow, the predictive distribution over \\(y\\) can be calculated by \\[\np(y_{new}\\mid y) = \\int p(y_{new},\\sigma^2\\mid y)p(\\sigma^2\\mid y)d\\sigma^2.\n\\] Which happens to be a \\(t\\)-distribution with \\(2\\alpha_{\\mathrm{posterior}}\\) degrees of freedom, mean \\(\\mu\\) and variance \\(\\alpha_{\\mathrm{posterior}}/\\beta_{\\mathrm{posterior}}\\).\n\nThe Normal-Gamma Model\nNow, consider the case when both mean and variance are unknown. To simplify the formulas, we work with precision \\(\\tau = 1/\\sigma^2\\). The Normal-Gamma distribution is a conjugate prior for a Normal likelihood with unknown mean and precision. Given data \\(y = \\{y_1,\\ldots,y_n\\}\\), we assume: \\[\ny_i \\mid \\theta, \\tau \\sim N(\\theta, 1/\\tau)\n\\]\nFor this model, the sufficient statistics are the sample mean \\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\), the sample size \\(n\\), and the sum of squared deviations \\(\\sum_{i=1}^n(y_i - \\bar{y})^2\\). The Normal-Gamma prior distribution is defined as: \\[\n\\theta\\mid \\mu,\\tau,\\nu \\sim N(\\mu, 1/(\\tau \\nu)), \\quad \\tau \\mid \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta).\n\\] Conditional on precision \\(\\tau\\), the mean \\(\\theta\\) is Normal with precision \\(\\nu\\tau\\). The marginal distribution of \\(\\tau\\) is Gamma. Note that \\(\\theta\\) and \\(\\tau\\) are not independent in the joint prior.\nGiven the likelihood: \\[\np(y\\mid \\theta, \\tau) \\propto \\tau^{n/2}\\exp\\left(-\\frac{\\tau}{2}\\sum_{i=1}^n(y_i-\\theta)^2\\right)\n\\] and the prior, the posterior distribution is also Normal-Gamma with parameters: \\[\n\\begin{aligned}\n\\mu_n &= \\dfrac{\\nu\\mu + n\\bar{y}}{\\nu+n},\\\\\n\\nu_n &= \\nu+n,\\\\\n\\alpha_n &= \\alpha + \\dfrac{n}{2},\\\\\n\\beta_n &= \\beta + \\dfrac{1}{2}\\sum_{i=1}^n(y_i-\\bar{y})^2 + \\dfrac{n\\nu}{2(\\nu+n)}(\\bar{y}-\\mu)^2.\n\\end{aligned}\n\\] where \\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\) is the sample mean and \\(n\\) is the sample size. The posterior distribution is a normal-Gamma distribution with parameters \\(\\mu_n, \\nu_n, \\alpha_n, \\beta_n\\).\n\n\nCredible Intervals for Normal-Gamma Model Posterior Parameters\nThe precision posterior follows a Gamma distribution with parameters \\(\\alpha_n, \\beta_n\\), thus we can use quantiles of the Gamma distribution to calculate credible intervals. A symmetric \\(100(1-c)%\\) credible interval \\([g_{c/2},g_{1-c/2}]\\) is given by \\(c/2\\) and \\(1-c/2\\) quantiles of the gamma distribution. To find credible interval for the variance \\(v = 1/\\tau\\), we simply use \\[\n[1/g_{1-c/2},1/g_{c/2}].\n\\] and for standard deviation \\(s = \\sqrt{v}\\) we use \\[\n[\\sqrt{1/g_{1-c/2}},\\sqrt{1/g_{c/2}}].\n\\] To find credible interval over the mean \\(\\theta\\), we need to integrate out the precision \\(\\tau^{-2}\\) from the posterior distribution. The marginal distribution of \\(\\theta\\) is a Student’s t-distribution with parameters center at \\(\\mu_n\\), variance \\(\\beta_n/(\\nu_n\\alpha_n)\\) and degrees of freedom \\(2\\alpha_n\\).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#multivariate-normal",
    "href": "03-bl.html#multivariate-normal",
    "title": "3  Bayesian Learning",
    "section": "3.9 Multivariate Normal",
    "text": "3.9 Multivariate Normal\nWe write \\(X \\sim N(\\mu,\\Sigma)\\) for a \\(d\\)-dimensional multivariate normal random vector with mean vector \\(\\mu \\in \\mathbb{R}^d\\) and covariance matrix \\(\\Sigma \\in \\mathbb{R}^{d\\times d}\\), where \\(\\Sigma\\) is symmetric and positive definite. If the linear algebra below is unfamiliar (transpose, inverse, determinant, positive definiteness), see Appendix Chapter 26. Its density is \\[\np(x\\mid \\mu,\\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\right), \\qquad x\\in\\mathbb{R}^d.\n\\] The multivariate normal is the workhorse distribution for joint modeling, conditioning, and linear transformations.\nIn the multivariate case, the normal-normal model is \\[\n\\theta \\sim N(\\mu_0,\\Sigma_0), \\quad y \\mid \\theta \\sim N(\\theta,\\Sigma).\n\\] For a single multivariate observation, \\(y\\) itself serves as the sufficient statistic for \\(\\theta\\). More generally, for \\(n\\) independent observations \\(y_1,\\ldots,y_n\\), the sample mean \\(\\bar{y} = n^{-1}\\sum_{i=1}^n y_i\\) is the sufficient statistic. The posterior distribution is \\[\n\\theta \\mid y \\sim N(\\mu_1,\\Sigma_1),\n\\] where \\[\n\\Sigma_1 = (\\Sigma_0^{-1} + \\Sigma^{-1})^{-1}, \\quad \\mu_1 = \\Sigma_1(\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}y).\n\\] The predictive distribution is \\[\ny_{new} \\mid y \\sim N(\\mu_1,\\Sigma_1 + \\Sigma).\n\\]\n\nExample 3.8 (Satya Nadella: CEO of Microsoft) In 2014, Satya Nadella became the CEO of Microsoft. The stock price of Microsoft has been on a steady rise since then. Suppose that you are a portfolio manager and you are interested in analyzing the returns of Microsoft stock compared to the market.\nSuppose you are managing a portfolio with two positions stock of Microsoft (MSFT) and an index fund that follows S&P500 index and tracks overall market performance. We are interested in estimating the mean returns of the positions in our portfolio. You believe that the returns are normally distributed and are related to each other. You have prior beliefs about these returns, which are also normally distributed. We will use what is called the empirical prior for the mean returns. This is a prior that is based on historical data. The empirical prior is a good choice when you have a lot of historical data and you believe that the future mean returns will be similar to the historical mean returns. We assume the prior for the mean returns is a bivariate normal distribution, let \\(\\mu_0 = (\\mu_{M}, \\mu_{S})\\) represent the prior mean returns for the stocks. The covariance matrix \\(\\Sigma_0\\) captures your beliefs about the variability and the relationship between these stocks’ returns in the prior. We will use the sample mean and covariance matrix of the historical returns as the prior mean and covariance matrix. The prior covariance matrix is given by \\[\n\\Sigma_0 = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the historical returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the historical returns of MSFT and SPY. The prior mean is given by \\[\n\\mu_0 = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the historical returns of MSFT and SPY, respectively. The likelihood of observing the data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns \\(\\mu = [\\mu_A, \\mu_B]\\). The covariance matrix \\(\\Sigma\\) of the likelihood represents the uncertainty in your data. We will use the sample mean and covariance matrix of the observed returns as the likelihood mean and covariance matrix. The likelihood covariance matrix is given by \\[\n\\Sigma = \\begin{bmatrix} \\sigma_{M}^2 & \\sigma_{MS} \\\\ \\sigma_{MS} & \\sigma_{S}^2 \\end{bmatrix},\n\\] where \\(\\sigma_{M}^2\\) and \\(\\sigma_{S}^2\\) are the sample variances of the observed returns of MSFT and SPY, respectively, and \\(\\sigma_{MS}\\) is the sample covariance of the observed returns of MSFT and SPY. The likelihood mean is given by \\[\n\\mu = \\begin{bmatrix} \\mu_{M} \\\\ \\mu_{S} \\end{bmatrix},\n\\] where \\(\\mu_{M}\\) and \\(\\mu_{S}\\) are the sample means of the observed returns of MSFT and SPY, respectively. In a Bayesian framework, you update your beliefs (prior) about the mean returns using the observed data (likelihood). The posterior distribution, which combines your prior beliefs and the new information from the data, is also a bivariate normal distribution. The mean \\(\\mu_{\\text{post}}\\) and covariance \\(\\Sigma_{\\text{post}}\\) of the posterior are calculated using Bayesian updating formulas, which involve \\(\\mu_0\\), \\(\\Sigma_0\\), \\(\\mu\\), and \\(\\Sigma\\).\nWe use observed returns prior to Nadella’s becoming CEO as our prior and analyze the returns post 2014. Thus, our observed data includes July 2015 - Dec 2023 period. We assume the likelihood of observing this data, given the mean returns, is also a bivariate normal distribution. The mean of this distribution is the true (but unknown) mean returns. The covariance matrix \\(Sigma\\) of the likelihood represents the uncertainty in your data and is calculated from the overall observed returns data 2001-2023.\n\ngetSymbols(c(\"MSFT\", \"SPY\"), from = \"2001-01-01\", to = \"2023-12-31\")\n## \"MSFT\" \"SPY\"\ns = 3666 # 2015-07-30\nprior = 1:s\nobs = s:nrow(MSFT) # post covid\n# obs = 5476:nrow(MSFT) # 2022-10-06 bull run if 22-23\na = as.numeric(dailyReturn(MSFT))\nc = as.numeric(dailyReturn(SPY))\n# Prior\nmu0 = c(mean(a[prior]), mean(c[prior]))\nSigma0 = cov(data.frame(a=a[prior],c=c[prior]))\n# Data\nmu = c(mean(a[obs]), mean(c[obs]))\nSigma = cov(data.frame(a=a,c=c))\n# Posterior\nSigmaPost = solve(solve(Sigma0) + solve(Sigma))\nmuPost = SigmaPost %*% (solve(Sigma0) %*% mu0 + solve(Sigma) %*% mu)\n\n\n\nPlotting Portfolio Updating with MSFT and SPY\n# Plot\nplot(a[obs], c[obs], xlab=\"MSFT\", ylab=\"SPY\", xlim=c(-0.005,0.005), \n    ylim=c(-0.005,0.005), pch=16, cex=0.5)\nabline(v=0, h=0, col=\"grey\")\nabline(v=mu0[1], h=mu0[2], col=\"blue\",lwd=3) #prior\nabline(v=mu[1], h=mu[2], col=\"red\",lwd=3) #data\nabline(v=muPost[1], h=muPost[2], col=\"green\",lwd=3) #posterior\nlegend(\"bottomright\", c(\"Prior\", \"Likelihood\", \"Posterior\"), pch=15, \n    col=c(\"blue\", \"red\", \"green\"), bty=\"n\")\n\n\n\n\n\n\n\n\nFigure 3.6: Plotting Portfolio Updating with MSFT and SPY\n\n\n\n\n\nWe can see the posterior mean for SPY is close to the prior mean, while the posterior mean for MSFT is further away. The performance of MSFT was significantly better past 2015 compared to SPY. The posterior mean (green) represents mean reversion value. We can think of it a expected mean return if the performance of MSFT starts reverting to its historical averages.\nThis model is particularly powerful because it can be extended to more dimensions (more stocks) and can include more complex relationships between the variables. It’s often used in finance, econometrics, and other fields where understanding the joint behavior of multiple normally-distributed variables is important.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#kelly-criterion-and-optimal-portfolio-allocation",
    "href": "03-bl.html#kelly-criterion-and-optimal-portfolio-allocation",
    "title": "3  Bayesian Learning",
    "section": "3.10 Kelly Criterion and Optimal Portfolio Allocation",
    "text": "3.10 Kelly Criterion and Optimal Portfolio Allocation\nThe Kelly criterion provides a principled approach to determining optimal bet sizes or portfolio allocations when facing uncertain outcomes. The fundamental question is: how should we allocate capital across risky assets to maximize long-run wealth growth?\nSuppose for the moment that \\(\\mathbf{p}\\) is known to the gambler. How should \\(\\mathbf{f}\\) be chosen? A criterion, due to Kelly (1956) (cf. Breiman (1961); Finkelstein and Whitley (1981)), is to choose \\(\\mathbf{f}\\) so as to maximize \\(E[\\log(1 + \\mathbf{f} \\cdot \\mathbf{G})]\\), the expected geometric rate of growth of the gambler’s fortune. Denoting by \\(p_{(1)} \\geq \\cdots \\geq p_{(k)}\\) the parameters \\(p_1, \\ldots, p_k\\) arranged in descending order, the optimal choice of \\(\\mathbf{f}\\) is \\[\nf_i = \\max\\{p_{(1)} + \\cdots + p_{(j_0)} + (k_0 - j_0)p_i - 1, 0\\}/(k_0 - j_0)\n\\tag{3.3}\\] for \\(i = 1, \\ldots, k\\), where \\[\nj_0 = \\min\\{0 \\leq j \\leq k_0 - 1: p_{(1)} + \\cdots + p_{(j)} + (k_0 - j)p_{(j+1)} \\leq 1\\}\n\\tag{3.4}\\] (Kelly 1956). (Note the similarity between Equation 3.3 and Equation 3.4.)\nOf course, \\(\\mathbf{p}\\) is typically unknown. The following Bayesian approach was proposed by Thomas G. Kurtz. Let \\(\\mathbf{p}\\) have a symmetric Dirichlet prior distribution with parameter \\(\\theta\\). Then, to maximize the expected geometric rate of growth of the gambler’s fortune at trial \\(n + 1\\) over all \\(\\mathbf{f}\\) depending only on the first \\(n\\) trials, it suffices to choose \\(\\mathbf{f}\\) as in Equation 3.3 and Equation 3.4, where \\(\\mathbf{p}\\) is replaced by the posterior mean of \\(\\mathbf{p}\\) based on the first \\(n\\) trials, that is, by \\[\n\\left(\\frac{X_1 + \\theta}{n + k\\theta}, \\ldots, \\frac{X_k + \\theta}{n + k\\theta}\\right).\n\\]\nThis result connects Bayesian learning to optimal decision-making under uncertainty. The Dirichlet-Multinomial conjugate pair allows us to update our beliefs about success probabilities and immediately compute the optimal allocation strategy. As more data arrives, the posterior mean converges to the empirical frequencies, and the Kelly allocation adapts accordingly.\n\nWater-Filling Interpretation\nThe Kelly criterion has an elegant geometric interpretation known as water-filling. Imagine pouring water into containers of different heights, where the height represents \\(1 - p_i\\). The water fills from the bottom up, and the depth of water in each container corresponds to the optimal allocation \\(f_i\\).\nMathematically, the water-filling algorithm works as follows:\n\nSort the probabilities in descending order: \\(p_{(1)} \\geq p_{(2)} \\geq \\cdots \\geq p_{(k)}\\)\nFind the “water level” \\(\\lambda\\) such that \\(\\sum_{i: p_i &gt; \\lambda} (p_i - \\lambda) = 1\\)\nThe optimal allocation is \\(f_i = \\max(p_i - \\lambda, 0)\\)\n\nThis ensures that: - Assets with higher expected returns receive more allocation - The total allocation sums to 1 (or the available capital) - Assets below the water level receive zero allocation\nThe water-filling perspective reveals why Kelly betting is aggressive yet prudent: it concentrates capital on the most favorable opportunities while respecting the constraint that probabilities sum to one. The Bayesian version replaces unknown probabilities with their posterior means, naturally incorporating parameter uncertainty into the allocation decision.\nThis framework extends beyond gambling to portfolio optimization, bandwidth allocation in communications, and resource distribution in machine learning. The key insight is that sequential decision-making under uncertainty requires both learning (updating beliefs via Bayes rule) and optimization (maximizing expected growth via Kelly). The two processes work in concert: better estimates lead to better decisions, and observing outcomes from decisions provides data for better estimates.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#mixtures-of-conjugate-priors",
    "href": "03-bl.html#mixtures-of-conjugate-priors",
    "title": "3  Bayesian Learning",
    "section": "3.11 Mixtures of Conjugate Priors",
    "text": "3.11 Mixtures of Conjugate Priors\nThe mixture of conjugate priors is a powerful tool for modeling complex data. It allows us to combine multiple conjugate priors to create a more flexible model that can capture a wider range of data patterns. The mixture of conjugate priors is particularly useful when the data is generated from a mixture of distributions, where each component of the mixture is generated from a different distribution. \nIf \\(p_1(x),\\ldots,p_k(x)\\) are proper density functions and \\(\\pi_1,\\ldots,\\pi_k\\) are non-negative weights that sum to 1, then the mixture distribution is given by \\[\np(x) = \\sum_{i=1}^k \\pi_i p_i(x).\n\\] It is easy to show that \\(p(x)\\) is a proper density. Indeed, given domain \\(x\\in A\\subset \\mathbb{R}\\) we have \\[\n\\int_A p(x)dx = \\sum_{i=1}^k \\pi_i \\int_A p_i(x)dx  = \\sum_{i=1}^k \\pi_i = 1.\n\\]\nAssume our prior is a mixture of distributions, that is \\[\n\\theta \\sim p(\\theta) = \\sum_{k=1}^K \\pi_k p_k(\\theta).\n\\] Then the posterior is also a mixture of normal distributions, that is \\[\np(\\theta\\mid y) = p(y\\mid \\theta)\\sum_{k=1}^K \\pi_k p_k(\\theta)/Z.\n\\] We introduce a normalizing constant for each component \\[\nZ_k = \\int p(y\\mid \\theta)p_k(\\theta)d\\theta.\n\\] then \\[\np_k(\\theta\\mid y)  = p_k(\\theta)p(y\\mid \\theta)/Z_k\n\\] is a proper distribution and our posterior is a mixture of these distributions \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\pi_k Z_k p_k(\\theta\\mid y)/Z.\n\\] Meaning that we need to require \\[\n\\dfrac{\\sum_{k=1}^K \\pi_k Z_k}{Z} = 1, \\quad \\text{or} \\quad Z = \\sum_{k=1}^K \\pi_k Z_k.\n\\] Then the posterior density is a mixture \\[\np(\\theta\\mid y) = \\sum_{k=1}^K \\hat \\pi_k p_k(\\theta \\mid y).\n\\]\nConsider an example of a mixture of two normal distributions. The prior distribution is a mixture of two normal distributions, that is \\[\n\\mu \\sim 0.5 N(0,1) + 0.5 N(5,1).\n\\] The likelihood is a normal distribution with mean \\(\\mu\\) and variance 1, that is \\[\ny \\mid \\mu \\sim N(\\mu,1).\n\\] The posterior distribution is a mixture of two normal distributions, that is \\[\np(\\mu \\mid y) \\propto \\phi(y\\mid \\mu,1) \\left(0.5 \\phi(\\mu\\mid 0,1) + 0.5 \\phi(\\mu\\mid 5,1)\\right),\n\\] where \\(\\phi(x\\mid \\mu,\\sigma^2)\\) is the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). We can calculate it using property of a normal distribution \\[\n\\phi(x\\mid \\mu_1,\\sigma_1^2)\\phi(x\\mid \\mu_2,\\sigma_2^2) = \\phi(x\\mid \\mu_3,\\sigma_3^2)\\phi(\\mu_1-\\mu_2\\mid 0,\\sigma_1^2+\\sigma_2^2)\n\\] where \\[\n\\mu_3 = \\dfrac{\\mu_1/\\sigma_2^2 + \\mu_2/\\sigma_1^2}{1/\\sigma_1^2 + 1/\\sigma_2^2}, \\quad \\sigma_3^2 = \\dfrac{1}{1/\\sigma_1^2 + 1/\\sigma_2^2}.\n\\]\nGiven, we observed \\(y = 2\\), we can calculate the posterior distribution for \\(\\mu\\)\n\nmu0 = c(0,5)\nsigma02 = c(1,1)\npi = c(0.5,0.5)\ny = 2\nmu3 = (mu0/sigma02 + y) / (1/sigma02 + 1)\nsigma3 = 1/(1/sigma02 + 1)\nZ = dnorm(y-mu0,0,1+sigma02)*pi\nw = Z/sum(Z)\n# To add a new line in sprintf, use \"\\n\" inside the format string.\nprint(\"Component parameters:\")\n## \"Component parameters:\"\nsprintf(\"Mean = (%1.1f,%2.1f)  Var = (%1.1f,%1.1f)  weights = (%1.2f,%1.2f)\", \n        mu3[1], mu3[2], sigma3[1], sigma3[2], w[1], w[2])\n## \"Mean = (1.0,3.5)  Var = (0.5,0.5)  weights = (0.65,0.35)\"\n\nSummary table of random variables\n\n\n\nTable 3.1: Summary table of commonly used random variables\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\n\\(\\theta\\)\nPDF\nMean\nVariance\nSupport\n\n\n\n\nNormal\n\\(\\mu, \\sigma^2\\)\n\\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\)\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\\(\\frac{1}{\\lambda}\\)\n\\(\\frac{1}{\\lambda^2}\\)\n\\(x \\geq 0\\)\n\n\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\)\n\\(\\frac{\\alpha}{\\beta}\\)\n\\(\\frac{\\alpha}{\\beta^2}\\)\n\\(x \\geq 0\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{e^{-\\lambda}\\lambda^x}{x!}\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(x \\in \\mathbb{N}\\)\n\n\nBinomial\n\\(n, p\\)\n\\(\\binom{n}{x}p^x(1-p)^{n-x}\\)\n\\(np\\)\n\\(np(1-p)\\)\n\\(x \\in \\{0, 1, \\ldots, n\\}\\)\n\n\nBernoulli\n\\(p\\)\n\\(p^x(1-p)^{1-x}\\)\n\\(p\\)\n\\(p(1-p)\\)\n\\(x \\in \\{0, 1\\}\\)\n\n\nMultinomial\n\\(n, \\boldsymbol{p}\\)\n\\(\\frac{n!}{x_1!x_2!\\cdots x_k!}p_1^{x_1}p_2^{x_2}\\cdots p_k^{x_k}\\)\n\\(np_i\\)\n\\(np_i(1-p_i)\\)\n\\(\\sum x_i = n, x_i \\in \\mathbb{R}^+\\)\n\n\nBeta\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\)\n\\(\\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\\(x \\in [0, 1]\\)\n\n\nInverse Gamma\n\\(\\alpha, \\beta\\)\n\\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{-\\alpha-1}e^{-\\frac{\\beta}{x}}\\)\n\\(\\frac{\\beta}{\\alpha-1}\\)\n\\(\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\)\n\\(x &gt; 0\\)\n\n\n\n\n\n\nTable 3.2 summarizes the conjugate prior distributions for common likelihoods. Thus far, we’ve considered the Normal-Normal model with both known and unknown variance as well as Poisson-Gamma and Beta Binomial. The other pairs are left as an exercise. Given observed data \\(x = (x_1,\\ldots,x_n)\\) and \\(s = \\sum_{i=1}^nx_i\\), \\(\\bar x = s/n\\). For each conjugate pair, the table lists the sufficient statistics that summarize all information in the data relevant to the parameter. The posterior depends on the data only through these sufficient statistics, which is a key property of conjugate families.\n\n\n\nTable 3.2: Conjugate prior table for common likelihoods\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nPrior\nPrior Parameters\nModel Parameters\nSufficient Statistics\nPosterior Parameters\n\n\n\n\nNormal (known \\(\\sigma^2\\))\nNormal\n\\(\\mu_0, \\sigma^2_0\\)\n\\(\\mu\\)\n\\(\\bar x, n\\)\n\\(\\frac{n\\sigma^2_0 \\bar x + \\sigma^2 \\mu_0}{\\sigma^2 + n\\sigma^2_0},~\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2+\\sigma^2}\\)\n\n\nNormal (known \\(\\mu\\))\nInverse Gamma\n\\(\\alpha,\\beta\\)\n\\(\\sigma^2\\)\n\\(\\sum(x_i-\\mu)^2, n\\)\n\\(\\alpha+n/2, \\beta + \\frac{1}{2}\\sum(x_i-\\mu)^2\\)\n\n\nBinomial (\\(m\\) trials)\nBeta\n\\(\\alpha, \\beta\\)\n\\(p\\)\n\\(s, n\\)\n\\(\\alpha + s, \\beta + nm - s\\)\n\n\nPoisson\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(s, n\\)\n\\(\\alpha + s, \\beta + n\\)\n\n\nExponential\nGamma\n\\(\\alpha, \\beta\\)\n\\(\\lambda\\)\n\\(s, n\\)\n\\(\\alpha + n, \\beta + s\\)\n\n\nMultinomial\nDirichlet\n\\(\\alpha \\in \\mathbb{R}^k\\)\n\\(p \\in \\mathbb{R}^k\\)\n\\(s_j\\) for each category \\(j\\)\n\\(\\alpha+s\\)\n\n\nNormal\nNormal-inverse gamma\n\\(\\mu_0, \\nu, \\alpha, \\beta\\)\n\\(\\mu, \\sigma\\)\n\\(\\bar x, \\sum(x_i-\\bar x)^2, n\\)\n\\(\\frac{\\nu\\mu_0+n\\bar{x}}{\\nu+n} ,\\, \\nu+n,\\, \\alpha+\\frac{n}{2} ,\\,\\)  \\(\\beta + \\tfrac{1}{2} \\sum_{i=1}^n (x_i - \\bar{x})^2 + \\frac{n\\nu}{\\nu+n}\\frac{(\\bar{x}-\\mu_0)^2}{2}\\)\n\n\n\n\n\n\nThese conjugate relationships simplify Bayesian calculations by ensuring that the posterior distributions are in the same family as the priors.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sec-computational-bridge",
    "href": "03-bl.html#sec-computational-bridge",
    "title": "3  Bayesian Learning",
    "section": "3.12 Bayesian Computation: when conjugacy breaks",
    "text": "3.12 Bayesian Computation: when conjugacy breaks\nConjugate priors are powerful because they keep the posterior in a familiar family and allow analytic updating. But most realistic Bayesian models are not conjugate: likelihoods can be non-Gaussian, priors can encode structure (sparsity, hierarchy), and predictors can enter through nonlinear functions. In those settings, the posterior still exists, but we cannot usually write it down in closed form.\nThe default remedy is approximation by simulation. The unifying idea is simple: if we can draw samples \\(\\theta^{(1)},\\ldots,\\theta^{(N)} \\sim p(\\theta\\mid y)\\), then posterior summaries become Monte Carlo averages. For example, for any function \\(g(\\theta)\\), \\[\n\\E{g(\\theta)\\mid y} \\approx \\frac{1}{N}\\sum_{n=1}^N g(\\theta^{(n)}).\n\\] This is the same law-of-large-numbers logic discussed in Chapter 1, but applied to posterior distributions rather than to a known data-generating distribution.\n\nPosterior Consistency and the Law of Large Numbers\nThe law of large numbers provides theoretical justification for Bayesian learning. As we collect more data, the posterior distribution concentrates around the true parameter value, regardless of the prior. This phenomenon, known as posterior consistency, follows from the fact that the likelihood function—being a product of many terms—is dominated by the data for large samples.\nConsider estimating the mean \\(\\mu\\) of a normal distribution from i.i.d. observations \\(x_1, \\ldots, x_n \\sim N(\\mu, \\sigma^2)\\) with a prior \\(\\mu \\sim N(\\mu_0, \\tau_0^2)\\). The posterior mean is:\n\\[\n\\E{\\mu \\mid x_1, \\ldots, x_n} = \\frac{\\tau_0^{-2}\\mu_0 + n\\sigma^{-2}\\bar{x}_n}{\\tau_0^{-2} + n\\sigma^{-2}}\n\\]\nAs \\(n \\to \\infty\\), the data term \\(n\\sigma^{-2}\\bar{x}_n\\) dominates, and by the law of large numbers, \\(\\bar{x}_n \\to \\mu\\) almost surely. Thus:\n\\[\n\\E{\\mu \\mid x_1, \\ldots, x_n} \\to \\mu \\quad \\text{almost surely}.\n\\]\nThe posterior concentrates at \\(\\mu\\), regardless of the prior \\(\\mu_0\\). The prior matters for small samples but becomes negligible for large samples—a reassuring property that ensures different researchers with different priors eventually reach consensus as evidence accumulates.\n\n\nMonte Carlo Methods\nMonte Carlo simulation is also justified by the law of large numbers. To approximate an expectation \\(\\E{f(x)}\\) where \\(x \\sim p(x)\\), we draw i.i.d. samples \\(x_1, \\ldots, x_n \\sim p(x)\\) and compute: \\[\n\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n f(x_i) \\approx \\E{f(x)}\n\\]\nBy the strong law of large numbers, \\(\\hat{\\mu}_n \\to \\E{f(X)}\\) almost surely as \\(n \\to \\infty\\), provided \\(\\E{|f(X)|} &lt; \\infty\\). The approximation error decreases at rate \\(O_p(n^{-1/2})\\) by the central limit theorem, giving us both convergence guarantees and quantifiable uncertainty.\nThe formal development of Monte Carlo methods emerged from the Manhattan Project during World War II, when Stanislaw Ulam, recovering from illness in 1946, realized that complex probability problems could be solved by simulating random processes rather than through analytical calculations. Ulam shared this insight with John von Neumann, who recognized its potential for solving neutron diffusion problems critical to nuclear weapons design and implemented the algorithms on early electronic computers like ENIAC. Nicholas Metropolis coined the term “Monte Carlo” as a reference to the Monaco casino, and the first unclassified paper appeared in 1949 (Metropolis and Ulam 1949). The 1953 Metropolis algorithm (Metropolis et al. 1953) extended the method beyond simple averaging to sampling from complex distributions—precisely the situation in Bayesian inference—laying the groundwork for modern MCMC. The law of large numbers had existed for centuries, but only the combination of electronic computing and wartime urgency transformed this theoretical principle into the practical computational tool that underpins contemporary Bayesian statistics (Metropolis 1987).\n\n\nMarkov Chain Monte Carlo\nMarkov chain Monte Carlo (MCMC) methods extend Monte Carlo simulation to settings where samples are dependent but ergodic. For an ergodic Markov chain with stationary distribution \\(\\pi(x)\\), the ergodic theorem guarantees:\n\\[\n\\frac{1}{n}\\sum_{i=1}^n f(x_i) \\to \\E[\\pi]{f(x)} \\quad \\text{almost surely}\n\\]\nwhere \\(x_i\\) are states visited by the chain. This justifies using MCMC to approximate posterior expectations in Bayesian inference, even though consecutive samples are correlated, see Polson (1996) for a formal analysis.\nMCMC methods provide a powerful framework for sampling from complex distributions that cannot be sampled directly. The key insight is to construct a Markov chain \\(P(x, y)\\) describing the transition probability from state \\(x\\) to state \\(y\\), such that the desired posterior distribution \\(\\pi\\) is the equilibrium (stationary) distribution of the chain.\nFormally, a distribution \\(\\pi\\) is stationary for a Markov chain with transition kernel \\(P(x, y)\\) if: \\[\n\\pi(y) = \\int \\pi(x) P(x, y) dx\n\\]\nOnce we establish that \\(\\pi\\) is stationary, we can generate samples \\(x^{(1)}, x^{(2)}, \\ldots\\) by simulating the Markov chain for sufficiently many iterations. By the ergodic theorem, time averages along the chain converge to expectations under \\(\\pi\\): \\[\n\\frac{1}{n}\\sum_{i=1}^n f(x^{(i)}) \\to \\E[\\pi]{f(x)} \\quad \\text{almost surely}\n\\]\nThe remarkable feature of MCMC is that we only need to know the ratio of densities \\(\\pi(y)/\\pi(x)\\), making it perfect for Bayesian inference where normalizing constants are often intractable. Two fundamental MCMC algorithms illustrate this principle:\n\nMetropolis-Hastings Algorithm\nThe Metropolis-Hastings algorithm constructs a reversible Markov chain by proposing a candidate point \\(y\\) and accepting it with probability: \\[\nP(x, y) = \\min\\left(\\frac{\\pi(y)}{\\pi(x)}, 1\\right)\n\\]\nIt is straightforward to verify that this chain is time-reversible (satisfies detailed balance): \\[\n\\pi(x)P(x, y) = \\min(\\pi(y), \\pi(x)) = \\pi(y)P(y, x)\n\\]\nThis implies that \\(\\pi\\) satisfies the stationarity condition: \\[\n\\sum_x \\pi(x)P(x, y) = \\pi(y)\n\\]\nand is therefore the equilibrium distribution of the chain.\n\n\nGibbs Sampler\nThe Gibbs sampler is a special case of the Metropolis-Hastings algorithm where the acceptance probability is always one. It is particularly useful for multivariate distributions where we can easily sample from conditional distributions. The Gibbs sampler cycles through coordinates, sampling each variable conditional on the current values of all others.\nFor a state vector \\(x = (x_1, \\ldots, x_p)\\), one iteration of the Gibbs sampler updates: \\[\nx_j^{(t+1)} \\sim \\pi(x_j \\mid x_1^{(t+1)}, \\ldots, x_{j-1}^{(t+1)}, x_{j+1}^{(t)}, \\ldots, x_p^{(t)})\n\\]\nThe Gibbs sampler is time-reversible due to the Clifford-Hammersley theorem, which guarantees that the joint distribution is the unique stationary distribution. This makes the Gibbs sampler a special case of the multivariate coordinate-by-coordinate sampler where the acceptance probability is always one.\nBoth algorithms transform the problem of computing complex integrals into a problem of simulating a carefully constructed stochastic process. The variance-mean mixture representations discussed in Chapter 17 leverage exactly this principle: by introducing auxiliary variables, they convert intractable posterior distributions into forms amenable to Gibbs sampling with conjugate conditional distributions.\nPosterior predictive simulation is the recurring workflow that connects computation back to modeling. To forecast or to check fit, we first draw parameters from the posterior and then simulate replicated or future data: \\[\n\\theta^{(n)} \\sim p(\\theta\\mid y),\\qquad \\tilde y^{(n)} \\sim p(\\tilde y\\mid \\theta^{(n)}).\n\\] Repeating this produces an empirical approximation to the posterior predictive distribution \\(p(\\tilde y\\mid y)\\) and makes uncertainty propagation concrete: parameter uncertainty becomes predictive uncertainty.\nLater chapters return to this pattern in different guises: sampling-based Bayesian inference (e.g., for logistic regression), approximate inference for scalable models, and simulation-based decision making. The key takeaway here is that conjugacy is a convenience, not a requirement; when the algebra ends, simulation provides the continuation.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "03-bl.html#sec-inference-to-decisions",
    "href": "03-bl.html#sec-inference-to-decisions",
    "title": "3  Bayesian Learning",
    "section": "3.13 From inference to decisions",
    "text": "3.13 From inference to decisions\nThe examples in this chapter focused on learning about unknown quantities from data, expressed through posterior distributions. In many applications, however, we must turn a posterior into an action: approve a loan, deploy a model, choose a medical treatment, or ship a product variant. This step requires an additional ingredient beyond probability modeling: a loss function that formalizes what we mean by a good or bad decision.\nFor a parameter \\(\\theta\\) and a decision (or estimate) \\(a\\), a loss \\(\\mathcal{L}(\\theta, a)\\) assigns a numerical penalty to choosing \\(a\\) when the truth is \\(\\theta\\). Given data \\(y\\), Bayesian decision making compares actions using posterior expected loss, \\[\n\\E{\\mathcal{L}(\\theta, a)\\mid y}.\n\\] The optimal Bayesian action is the posterior Bayes rule \\[\na^*(y) = \\arg\\min_a \\E{\\mathcal{L}(\\theta, a)\\mid y}.\n\\]\nSeveral estimators that are common in statistics can be interpreted as Bayes rules under simple choices of loss. For example, the posterior mean minimizes squared error loss \\(\\mathcal{L}(\\theta,a)=(\\theta-a)^2\\), while the posterior median minimizes absolute error loss \\(\\mathcal{L}(\\theta,a)=|\\theta-a|\\). Under 0–1 loss for point classification, the posterior mode becomes optimal. This perspective makes point estimation a special case of a more general principle: choose the action that is optimal under the posterior distribution and the costs of being wrong.\nChapter 4 develops this decision-theoretic framework systematically, connecting loss functions to utility, risk, and optimal actions under uncertainty.\n\n\n\n\nBreiman, Leo. 1961. “Optimal Gambling Systems for Favorable Games.” Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability 1: 65–78.\n\n\nFinkelstein, Mark, and Robert Whitley. 1981. “The Application of an Entropy Maximizing Gambling Objective to a Production Problem.” Management Science 27 (9): 1023–33.\n\n\nHeath, David, and William Sudderth. 1976. “De Finetti’s Theorem on Exchangeable Variables.” The American Statistician 30 (4): 188–89.\n\n\nKelly, J. L. 1956. “A New Interpretation of Information Rate.” Bell System Technical Journal 35 (4): 917–26.\n\n\nMetropolis, Nicholas. 1987. “The Beginning of the Monte Carlo Method.” Los Alamos Science 15: 125–30.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92.\n\n\nMetropolis, Nicholas, and Stanislaw Ulam. 1949. “The Monte Carlo Method.” Journal of the American Statistical Association 44 (247): 335–41.\n\n\nPolson, Nicholas. 1996. “Convergence of Markov Chain Monte Carlo Algorithms (with Discussion).” Bayesian Statistics 5: 297–321.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to Go!” Significance 6 (4): 151–53.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference about Batter-Pitcher Matchups in Baseball from Small Samples.” Statistical Thinking in Sports, 153–65.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The Impact of the Highly Improbable. Annotated edition. New York. N.Y: Random House.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bayesian Learning</span>"
    ]
  },
  {
    "objectID": "04-dec.html",
    "href": "04-dec.html",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "4.1 Expected Utility\nStatistical decision theory asks: given what the data tells us, what should we do? The problem typically splits in two. First, learn something from data—estimate parameters, fit a model. Second, use that learning to choose an action. In finance, this means estimating means and covariances from historical data, then constructing a portfolio. In statistics, it means evaluating which estimator or hypothesis test performs best under a given criterion.\nLet \\(P,Q\\) be two possible risky gambles or probability bets. An agent’s preferences can then be specified as an ordering on probability bets where we write \\(P\\) is preferred to \\(Q\\) as \\(P \\succeq Q\\) and indifference as \\(P \\sim Q\\). A compound or mixture bet is defined by the probability assignment \\(p P + (1 - p ) Q\\) for a prospect weight \\(0 \\leq p \\leq 1\\).\nRamsey-de Finetti-Savage show that if an agent’s preferences satisfy a number of plausible axioms – completeness, transitivity, continuity and independence – then they can be represented by the expectation of a utility function. The theory is a normative one and not necessarily descriptive. It suggests how a rational agent should formulate beliefs and preferences and not how they actually behave.\nThis representation of preferences in terms of expected utility \\(U(P)\\) of a risky gamble is then equivalent to \\[\nP \\succeq Q \\; \\; \\iff \\; \\; U (P) \\geq U (Q )\n\\] Therefore, the higher the value taken by the utility function the more the gamble is preferred. Specifically, the axioms lead to existence of expected utility and uniqueness of probability.\nThe two key facts then are uniqueness of probability and existence of expected utility. Formally,\nThis implies that \\(U\\) is additive and it is also unique up to affine transformation.\nProof: If \\(w\\) is not unique then \\(\\exists w_1\\) such that \\(w_1 P + (1 - w_1 ) Q \\sim R\\). Without loss of generality assume that \\(w_1 &lt; w\\) and so \\(0 &lt; w - w_1 &lt; 1 - w_1\\). However, we can write the bet \\(Q\\) as \\[\nQ = \\left ( \\frac{w-w_1}{1-w_1} \\right ) Q + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\] By transitivity, as \\(P \\succeq Q\\) we have \\[\n\\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q \\succeq Q\n\\] However, \\[\nw P + ( 1 - w) Q = w_1 P + (1 - w_1 ) \\left (  \\left ( \\frac{w-w_1}{1-w_1} \\right ) P + \\left ( \\frac{1-w}{1-w_1} \\right ) Q\n\\right )\n\\] implying by transitivity that \\[\nw P + (1 - w ) Q \\succeq w_1 P + (1 - w_1 ) Q\n\\] which is a contradiction.\nThis can be used together with the axioms to then prove the existence and uniqueness of a utility function.\nOther interesting extensions: how do people come to a consensus (DeGroot 1974; Morris 1994, 1996). Ramsey (1926) observation that if someone is willing to offer you a bet then that’s conditioning information for you. All probabilities are conditional probabilities.\nIf the bet outcome \\(x\\) is a monetary value, then the utility functions \\(x, x^2, \\sqrt{x}, \\ln x\\) are all monotonically increasing (the more the better). However, the utility function \\(x^2\\) is convex and the utility function \\(\\ln x\\) is concave. The concavity of the utility function implies that the agent is risk averse and the convexity implies that the agent is risk seeking.\nNow, consider a more general situation, when you have two gambles 1: get \\(P_1\\) for sure, 2: get \\(P_2 = P_1+k\\) and \\(P_3 = P_1-k\\) with probability 1/2. Then we will compare the utility of those gambles \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) \\text{ and } U(P_1).\n\\] If the utility function is linear then we should be indifferent between the two gambles. However, if the utility function is concave then we should prefer the sure thing. This is known as the certainty effect. \\[\n\\dfrac{1}{2}U(P_2) + \\dfrac{1}{2}U(P_3) &lt; U(P_1).\n\\]\nThe usual situation can be described as follows. Let \\(\\Omega\\) be a finite set of possible outcomes with \\(\\Omega = \\{ \\omega_1 , \\ldots , \\omega_n \\}\\). Let \\(P_i\\) be the consequence that assigns one to outcome \\(\\omega_i\\) and zero otherwise and let \\(P = ( p_1 , \\ldots , p_n )\\) assign probability \\(p_i\\) to outcome \\(\\omega_i\\). Then we can write the expected utility, \\(U(P)\\), of the gamble \\(P\\) as \\[\nU(P) = \\sum_{i=1}^n p_i U( P_i ).\n\\] That is, the utility of \\(P\\) is the expected value of a random variable \\(W\\) (wealth) that takes the value \\(U(P_i)\\) if the outcome is \\(\\omega_i\\). Therefore, we can write \\(U(P) = \\mathbb{E}_P \\left ( U( W ) \\right)\\).\nThis leads us to the notion of risk aversion and a categorization of agents according to their risk tolerance: the agent is said to be\nHere we assume that these hold for all probabilities and random variables. Risk aversion is equivalent to the agent having concave utility and risk seeking convex.\nPower utility and log-utilities allow us to model constant relative risk aversion (CRRA). The main advantage is that the optimal rule is unaffected by wealth effects. The CRRA utility of wealth takes the form \\[\nU_\\gamma (W) = \\frac{ W^{1-\\gamma} -1 }{1-\\gamma}\n\\]\nThe special case \\(U(W) = \\log (W )\\) for \\(\\gamma = 1\\). This leads to a myopic Kelly criterion rule.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#expected-utility",
    "href": "04-dec.html#expected-utility",
    "title": "4  Utility, Risk and Decisions",
    "section": "",
    "text": "If \\(P \\succeq R \\succeq Q\\) and \\(w P + (1 - w ) Q \\sim R\\) then \\(w\\) is unique.\nThere exists an expected utility \\(U(\\cdot )\\) such that \\(P \\succeq Q ~ \\iff ~ U (P) \\geq U (Q)\\). Furthermore \\[\nU \\left (w P + (1 - w ) Q \\right ) = wU (P) +(1 - w ) U(Q)\n\\] for any \\(P, Q\\) and \\(0 \\leq w \\leq 1\\).\n\n\n\n\n\nTheorem 4.1 (Uniqueness of Utility) If \\(V\\) is any other function satisfying these results then \\(V\\) is an affine function of \\(U\\).\n\n\nProof. If \\(\\forall P , Q\\) we have \\(P \\sim Q\\), then define \\(u(P) \\equiv 0\\). Hence suppose that there exists \\(S \\succ T\\). Define \\(U(S) =1\\) and \\(U(T)=0\\). For any \\(P \\in \\mathcal{P}\\) there are five possibilities: \\(P \\succ T\\) or \\(P \\sim S\\) or \\(S \\succ P \\succ T\\) or \\(P \\sim T\\) or \\(T \\succ P\\).\nIn the first case define \\(1/U(P)\\) to be the unique \\(p\\) (see previous theorem) defined by \\(p P + ( 1 -p )T \\sim S\\). In the second case, define \\(U(P) =1\\). In the third, there exists a unique \\(q\\) with \\(q S + ( 1 -q )T \\sim P\\) and then define \\(U(P)=q\\). In the fourth case, define \\(U(P)=0\\) and finally when \\(T \\succ P\\) there exists a unique \\(r\\) with \\(r S + ( 1-r )P \\sim T\\) and then we define \\(U(P) = - r / (1 - r)\\).\nThen check that \\(U(P)\\) satisfies the conditions. See Savage (1954), Ramsey (1927) and de Finetti (1931)\n\n\n\n\nExample 4.1 (Saint Petersburg Paradox) The Saint Petersburg paradox is a concept in probability and decision theory that was first introduced by Daniel Bernoulli in 1738. It revolves around the idea of how individuals value risky propositions and how those valuations may not align with classical expected utility theory.\nThe paradox is named after the city of Saint Petersburg, where the problem was formulated. Here’s a simplified version of the paradox:\nImagine a gambling game where a fair coin is flipped repeatedly until it lands on heads. The payoff for the game is \\(2^N\\), where \\(N\\) is the number of tosses needed for the coin to land on heads. The expected value of this game, calculated by multiplying each possible payoff by its probability and summing the results, is infinite:\n\\[\nE(X) = \\frac{1}{2} \\cdot 2 + \\frac{1}{4} \\cdot 4 + \\frac{1}{8} \\cdot 8 + \\ldots = \\infty\n\\]\nThis means that, in theory, a rational person should be willing to pay any finite amount to play this game, as the expected value is infinite. However, in reality, most people would be unwilling to pay a large amount to play such a game.\nThe paradox arises because traditional expected utility theory assumes that individuals make decisions based on maximizing their expected gain. Bernoulli argued that people do not maximize expected monetary value but rather expected utility, where utility is a subjective measure of satisfaction or happiness. He proposed that individuals exhibit diminishing marginal utility for wealth, meaning that the additional satisfaction gained from an extra unit of wealth decreases as total wealth increases.\nIn the case of the Saint Petersburg paradox, although the expected monetary value is infinite, the utility gained from each additional dollar diminishes rapidly, leading to a reluctance to pay large amounts to play the game.\nIn modern decision theory and economics, concepts like diminishing marginal utility and expected utility are fundamental in understanding how individuals make choices under uncertainty and risk. The Saint Petersburg paradox highlights the limitations of relying solely on expected monetary value in explaining human behavior in such situations.\nOne common approach is to consider aspects of potential players, such as their possible risk aversion, available funds, etc., through a utility function \\(U(x)\\). Applying a utility function in this situation means changing our focus to the quantity \\[\nE[U(X)] = \\sum^\\infty_{k=1} 2^{-k} U(2^k).   \n\\]\nSome examples of utility functions are,\n\n\\(U(x) = W_0 (1-x^{-\\alpha})\\), \\(\\alpha &gt; 0\\), which gives an expected utility of \\(W_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\)\nLog utility, \\(U(x) = \\log(x)\\), with expected value \\(2 \\log(2)\\).\n\nNotice that after obtaining an expected utility value, you’ll have to find the corresponding reward/dollar amount.\nFor the log utility case, we need to find the certain dollar amount \\(x^*\\) that provides the same utility as playing the game. Setting \\(U(x^*) = 2\\log(2)\\), we solve: \\[\n\\log(x^*) = 2\\log(2) = \\log(2^2) = \\log(4)\n\\] which gives \\(x^* = 4\\). Therefore, under log utility, a rational player would be willing to pay at most $4 to play the Saint Petersburg game, despite its infinite expected monetary value. This is a dramatic reduction from infinity and demonstrates how risk aversion (captured by the concave log utility function) resolves the paradox.\nSimilarly, for the power utility \\(U(x) = W_0 (1-x^{-\\alpha})\\) with \\(\\alpha &gt; 0\\), we have an expected utility of \\(W_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\\). To find the certainty equivalent \\(x^*\\), we solve: \\[\nW_0 (1-(x^*)^{-\\alpha}) = W_0 \\left(1-\\frac{1}{2^{\\alpha+1}-1}\\right)\n\\] which simplifies to \\((x^*)^{-\\alpha} = \\frac{1}{2^{\\alpha+1}-1}\\), giving: \\[\nx^* = \\left(2^{\\alpha+1}-1\\right)^{1/\\alpha}\n\\] For example, with \\(\\alpha = 1\\), we get \\(x^* = (2^2-1)^1 = 3\\) dollars. As risk aversion increases (larger \\(\\alpha\\)), the certainty equivalent further decreases.\nBernoulli’s resolution of this paradox was a watershed moment: it shifted the focus from objective expected value (which is infinite here) to subjective expected utility. This idea—that people value money non-linearly—became the cornerstone of the modern economic theory of risk.\n\n\n\n\n\nRisk Averse if \\(\\mathbb{E}_P \\left ( U(W) \\right ) \\leq U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Neutral if \\(\\mathbb{E}_P \\left ( U(W) \\right ) =    U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\nRisk Seeking if \\(\\mathbb{E}_P \\left ( U(W) \\right ) \\geq U \\left (  \\mathbb{E}_P  (W)  \\right )\\)\n\n\n\nExample 4.2 (Risk Aversion) Consider the family of Constant Relative Risk Aversion (CRRA) utility functions. This family includes the log-utility (Kelly) \\[\nU(W)=\\log(W),\n\\] and the power utility \\[\nU(W)=\\dfrac{W^{1-\\gamma} - 1}{1-\\gamma}.\n\\] The log-utility is a limiting case of the power utility when \\(\\gamma \\to 1\\). The parameter \\(\\gamma\\) controls the curvature of the function and is known as the coefficient of relative risk aversion: \\[\nR(W) = -W \\frac{U''(W)}{U'(W)} = \\gamma.\n\\] Higher values of \\(\\gamma\\) imply greater concavity, meaning the agent is more risk-averse and requires a higher risk premium to accept a gamble.\n\n\\(\\gamma \\to 0\\): Risk neutral (linear utility).\n\\(\\gamma = 0.5\\): Moderate risk aversion (less than log).\n\\(\\gamma = 1\\): Log utility (Kelly criterion).\n\\(\\gamma &gt; 1\\): High risk aversion (e.g., fractional Kelly).\n\nFigure 4.1 demonstrates the effect of \\(\\gamma\\). As \\(\\gamma\\) increases, the utility function becomes more curved (“bends” more). This means that losses (moving left from \\(W=1\\)) hurt much more in utility terms than equivalent gains (moving right) help.\n\n\nPlot utility functions\nw &lt;- seq(0.2, 2, length.out = 500)\ngamma=2\n # Plot this data frame\ndf &lt;- data.frame(w = w, \n                 logu = log(w), \n                 kelly = (w^(1-gamma) - 1) / (1-gamma))\n# 3. Plot use different y-scales for \nggplot(df, aes(x = w)) +\n  geom_line(aes(y = logu, color = \"log utility\"), size = 1.3) +\n  geom_line(aes(y = kelly, color = \"Kelly criterion\"), size = 1.3) +\n  geom_vline(xintercept = c(0.5, 1, 1.5), linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"log utility\" = \"blue\", \"Kelly criterion\" = \"red\")) +\n  labs(title = \"Convex Utility Functions\",\n       x = \"Wealth (w)\",\n       y = \"Utility\",\n       color = \"Utility Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFigure 4.1: Utility functions for different risk aversion parameters.\n\n\n\n\n\nTo see this concretely, consider a simple gamble with two equally likely outcomes: wealth \\(w_1 = 0.5\\) and \\(w_2 = 1.5\\). The expected wealth is \\(E[W] = 0.5(0.5) + 0.5(1.5) = 1.0\\).\nFor a risk-neutral agent (\\(\\gamma=0\\)), the utility is linear, so the value of the gamble is exactly the expected wealth (\\(1.0\\)). However, for a risk-averse agent (\\(\\gamma &gt; 0\\)), the utility of the expected wealth is greater than the expected utility of the gamble. The Certainty Equivalent (CE) is the guaranteed amount of wealth that provides the same utility as the risky gamble. The difference between the expected wealth and the certainty equivalent is the Risk Premium—the amount the agent is willing to “pay” (forgo in expected value) to avoid the risk.\nThe table below shows how the Certainty Equivalent decreases and the Risk Premium increases as the risk aversion parameter \\(\\gamma\\) grows.\n\n\n\nImpact of risk aversion on the valuation of a gamble with outcomes {0.5, 1.5}.\n\n\nRisk Aversion (\\(\\gamma\\))\nCertainty Equivalent\nRisk Premium\n\n\n\n\n0.0\n1.00\n0.000\n\n\n0.5\n0.93\n0.067\n\n\n1.0\n0.87\n0.134\n\n\n2.0\n0.75\n0.250\n\n\n4.0\n0.62\n0.378\n\n\n\n\n\n\n\nExample 4.3 (Kelly Criterion) The Kelly criterion has been used effectively by many practitioners. Ed Thorp, in his book Beat the Dealer, pioneered its use in blackjack and later applied it to investing in financial markets. Since then, many market participants, such as Jim Simons, have stressed the importance of this money management approach. The criterion’s application extends to other domains: Phil Laak described its use for bet sizing in a game-theoretic approach to poker, and Bill Benter applied it to horse racing. Stewart Ethier provided a mathematical framework for multiple outcomes and analyzed a “play the winner” rule in roulette. Claude Shannon also developed a system to detect and exploit unintentionally biased roulette wheels, an endeavor chronicled in the book The Eudaemonic Pie.\nSuppose you have $1000 to invest. With probability \\(0.55\\) you will win whatever you wager and with probability \\(0.45\\) you lose whatever you wager. What’s the proportion of capital that leads to the fastest compounded growth rate?\nQuoting Kelly (1956), the exponential rate of growth, \\(G\\), of a gambler’s capital is \\[\nG = \\lim_{T\\to \\infty} \\frac{1}{T} \\log_2 \\frac{W_T}{W_0}\n\\] for initial capital \\(W_0\\) and capital after \\(T\\) bets \\(W_T\\).\nUnder the assumption that a gambler bets a fraction of his capital, \\(\\omega\\), each time, we use \\[\nW_T = (1+\\omega)^W (1-\\omega)^L W_0\n\\] where \\(W\\) and \\(L\\) are the number of wins and losses in \\(N\\) bets. We get \\[\nG = p \\log_2(1+\\omega)+ q \\log_2(1-\\omega)\n\\] in which the limit(s) of \\(\\frac{W}{N}\\) and \\(\\frac{L}{N}\\) are the probabilities \\(p\\) and \\(q\\), respectively.\nThis also comes about by considering the sequence of i.i.d. bets with \\[\np ( X_t = 1 ) = p \\; \\; \\text{ and} \\; \\; p ( X_t = -1 ) = q=1-p\n\\] We want to find an optimal allocation \\(\\omega^*\\) that maximizes the expected long-run growth rate: \\[\\begin{align*}\n\\max_\\omega \\mathbb{E} \\left ( \\ln ( 1 + \\omega W_T ) \\right )\n& = p \\ln ( 1 + \\omega ) + (1 -p) \\ln (1 - \\omega ) \\\\\n& \\leq p \\ln p + q \\ln q + \\ln 2 \\; \\text{ and} \\; \\omega^\\star = p - q\n\\end{align*}\\]\nThe solution is \\(w^* = 0.55 - 0.45 = 0.1\\).\nBoth approaches give the same optimization problem, which, when solved, give the optimal fraction rate \\(\\omega^* = p-q\\), thus, with \\(p=0.55\\), the optimal allocation is 10% of capital.\nWe can generalize the rule to the case of asymmetric payouts \\((a,b)\\). Then the expected utility function is \\[\np \\ln ( 1 + b \\omega ) + (1 -p) \\ln (1 - a \\omega )\n\\] The optimal solution is \\[\n\\omega^\\star = \\frac{bp - a q}{ab}\n\\]\nIf \\(a=b=1\\) this reduces to the pure Kelly criterion.\nA common case occurs when \\(a=1\\) and market odds \\(b=O\\). The rule becomes \\[\n\\omega^* = \\frac{p \\cdot O  -q }{O}.\n\\]\nLet’s consider another scenario. You have two possible market opportunities: one where it offers you \\(4/1\\) when you have personal odds of \\(3/1\\) and a second one when it offers you \\(12/1\\) while you think the odds are \\(9/1\\).\nIn expected return these two scenarios are identical both offering a 33% gain. In terms of maximizing long-run growth, however, they are not identical.\nTable 4.1 shows the Kelly criterion advises an allocation that is twice as much capital to the lower odds proposition: \\(1/16\\) weight versus \\(1/40\\).\n\n\n\nTable 4.1: Kelly rule\n\n\n\n\n\nMarket\nYou\n\\(\\Delta\\)\n\\(\\omega^\\star\\)\n\n\n\n\n\\(4/1\\)\n\\(3/1\\)\n\\(1/4\\)\n\\(1/16\\)\n\n\n\\(12/1\\)\n\\(9/1\\)\n\\(1/10\\)\n\\(1/40\\)\n\n\n\n\n\n\nThe optimal allocation \\(\\omega^\\star = ( p O - q ) / O\\) is \\[\n\\frac{ (1/4) \\times 4 - (3/4) }{4} = \\frac{1}{16} \\; \\text{ and} \\;\n\\frac{ (1/10) \\times 12 - (9/10) }{12} = \\frac{1}{40}.\n\\]\nNote, that although the expected return is the same, the risk is different. The first gamble has a higher variance than the second gamble.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#statistical-decisions-and-risk",
    "href": "04-dec.html#statistical-decisions-and-risk",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.2 Statistical Decisions and Risk",
    "text": "4.2 Statistical Decisions and Risk\nWhile Bernoulli’s work laid the foundation for expected utility, it was in the mid-20th century that L.J. Savage and John von Neumann/Oskar Morgenstern formalized these concepts into a rigorous mathematical decision theory. This framework provides the “rational” standard against which human behavior is often measured.\nThe statistical decision making problem can be posed as follows. A decision maker (you) has to choose from a set of decisions or acts. The consequences of these decisions depend on an unknown state of the world. Let \\(d\\in\\mathcal{D}\\) denote the decision and \\(\\theta\\in\\Theta\\) the state of the world. As an example, think of \\(\\theta\\) as the unknown parameter and the decision as choosing a parameter estimation or hypothesis testing procedure. To provide information about the parameter, the decision maker obtains a sample \\(y\\in\\mathcal{Y}\\) that is generated from the likelihood function \\(p\\left(y|\\theta\\right)\\). The resulting decision depends on the observed data, is denoted as \\(d\\left(  y\\right)\\), and is commonly called the decision rule.\nTo make the decision, the decision maker uses a “loss” function as a quantitative metric to assesses the consequences or performance of different decisions. For each state of the world \\(\\theta\\), and decision \\(d\\), \\(\\mathcal{L}\\left(  \\theta,d\\right)\\) quantifies the “loss” made by choosing \\(d\\) when the state of the world is \\(\\theta.\\) Common loss functions include a quadratic loss, \\(\\mathcal{L}(\\theta,d)=(\\theta-d)^{2},\\) an absolute loss, \\(\\mathcal{L}(\\theta,d)=|\\theta-d|\\), and a \\(0-1\\) loss, \\[\n\\mathcal{L}(\\theta,d)=L_{0}1_{\\left[  \\theta\\in\\Theta_{0}\\right]  }+L_{1}1_{\\left[  \\theta\\in\\Theta_{1}\\right]  }.\n\\] For Bayesians, the utility function provides a natural loss function. Historically, decision theory was developed by classical statisticians, thus the development in terms of “objective” loss functions instead of “subjective” utility.\nClassical decision theory takes a frequentist approach, treating parameters as “fixed but unknown” and evaluating decisions based on their population properties. Intuitively, this thought experiment entails drawing a dataset \\(y\\) of given length and applying the same decision rule in a large number of repeated trials and averaging the resulting loss across those hypothetical samples. Formally, the classical risk function is defined as \\[\nR(\\theta,d)=\\int_{\\mathcal{Y}}\\mathcal{L}\\left[  \\theta,d(y)\\right]  p(y|\\theta )dy=\\mathbb{E}\\left[  \\mathcal{L}\\left[  \\theta,d(y)\\right]  |\\theta\\right]  .\n\\] Since the risk function integrates over the data, it does not depend on a given observed sample and is therefore an ex-ante or a-priori metric. In the case of quadratic loss, the risk function is the mean-squared error (MSE) and is \\[\\begin{align*}\nR(\\theta,d)  &  =\\int_{\\mathcal{Y}}\\left[  \\theta-d\\left(  y\\right)  \\right]\n^{2}p(y|\\theta)dy\\\\\n&  =\\mathbb{E}\\left[  \\left(  d\\left(  y\\right)  -E\\left[  d\\left(  y\\right)\n|\\theta\\right]  \\right)  ^{2}|\\theta\\right]  +\\mathbb{E}\\left[  \\left(\nE\\left[  d\\left(  y\\right)  |\\theta\\right]  -\\theta\\right)  ^{2}|\\theta\\right]\n\\\\\n&  =Var\\left(  d\\left(  y\\right)  |\\theta\\right)  +\\left[  bias\\left(\nd\\left(  y\\right)  -\\theta\\right)  \\right]  ^{2}%\n\\end{align*}\\] which can be interpreted as the bias of the decision/estimator plus the variance of the decision/estimator. Common frequentist estimators choose unbiased estimators so that the bias term is zero, which in most settings leads to unique estimators.\nThe goal of the decision maker is to minimize risk. Unfortunately, rarely is there a decision that minimizes risk uniformly for all parameter values. To see this, consider a simple example of \\(y\\sim N\\left(  \\theta,1\\right)\\), a quadratic loss, and two decision rules, \\(d_{1}\\left(  y\\right)  =0\\) or \\(d_{2}\\left(  y\\right)  =y\\). Then, \\(R\\left(  \\theta,d_{1}\\right)  =\\theta^{2}\\) and \\(R\\left(  \\theta,d_{2}\\right)  =1\\). If \\(\\left\\vert \\theta\\right\\vert &lt;1\\), then \\(R\\left(  \\theta,d_{1}\\right)  &lt;R\\left(  \\theta,d_{2}\\right)\\), with the ordering reversed for \\(\\left\\vert \\theta\\right\\vert &gt;1\\). Thus, neither rule uniformly dominates the other.\nOne way to deal with the lack of uniform domination is to use the minimax principle: first maximize risk as function of \\(\\theta\\), \\[\n\\theta^{\\ast}=\\underset{\\theta\\in\\Theta}{\\arg\\max}R(\\theta,d)\\text{,}%\n\\] and then minimize the resulting risk by choosing a decision:\n\\[\nd_{m}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\left[  R(\\theta^{\\ast },d)\\right]  \\text{.}%\n\\] The resulting decision is known as a minimax decision rule. The motivation for minimax is game theory, with the idea that the statistician chooses the best decision rule against the other player, mother nature, who chooses the worst parameter.\nThe Bayesian approach treats parameters as random and specifies both a likelihood and prior distribution, denoted here by \\(\\pi\\left(  \\theta\\right)\\). The Bayesian decision maker recognizes that both the data and parameters are random, and accounts for both sources of uncertainty when calculating risk. The Bayes risk is defined as\n\\[\\begin{align*}\nr(\\pi,d)  &  =\\int_{\\mathcal{\\Theta}}\\int_{\\mathcal{Y}}\\mathcal{L}\\left[  \\theta ,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  dyd\\theta\\\\\n&  =\\int_{\\mathcal{\\Theta}}R(\\theta,d)\\pi\\left(  \\theta\\right)  d\\theta =\\mathbb{E}_{\\pi}\\left[  R(\\theta,d)\\right]  ,\n\\end{align*}\\] and thus the Bayes risk is an average of the classical risk, with the expectation taken under the prior distribution. The Bayes decision rule minimizes expected risk:\n\\[\nd_{\\pi}^{\\ast}=\\underset{d\\in\\mathcal{D}}{\\arg\\min}\\text{ }r(\\pi,d)\\text{.}%\n\\] The classical risk of a Bayes decision rule is defined as \\(R\\left(\n\\theta,d_{\\pi}^{\\ast}\\right)\\), where \\(d_{\\pi}^{\\ast}\\) does not depend on \\(\\theta\\) or \\(y\\). Minimizing expected risk is consistent with maximizing posterior expected utility or, in this case, minimizing expected loss. Expected posterior risk is \\[\nr(\\pi,d)=\\int_{\\mathcal{Y}}\\left[  \\int_{\\mathcal{\\Theta}}\\mathcal{L}\\left[\n\\theta,d(y)\\right]  p(y|\\theta)\\pi\\left(  \\theta\\right)  d\\theta\\right]  dy,\n\\] where the term in the brackets is posterior expected loss. Minimizing posterior expected loss for every \\(y\\in\\mathcal{Y},\\) is clearly equivalent to minimizing posterior expected risk, provided it is possibility to interchange the order of integration.\nThe previous definitions did not explicitly state that the prior distribution was proper, that is, that \\(\\int_{\\mathcal{\\Theta}}\\pi\\left(  \\theta\\right)d\\theta=1\\). In some applications and for some parameters, researchers may use priors that do not integrate, \\(\\int_{\\Theta}\\pi\\left(  \\theta\\right)d\\theta=\\infty\\), commonly called improper priors. A generalized Bayes rule is one that minimizes \\(r(\\pi,d),\\) where \\(\\pi\\) is not necessarily a distribution, if such a rule exists. If \\(r(\\pi,d)&lt;\\infty\\), then the mechanics of this rule is clear, although its meaning is less clear.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#unintuitive-nature-of-decision-making",
    "href": "04-dec.html#unintuitive-nature-of-decision-making",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.3 Unintuitive Nature of Decision Making",
    "text": "4.3 Unintuitive Nature of Decision Making\nDespite the elegant mathematical framework of Utility Theory and Statistical Decision Theory established by Savage and von Neumann, actual human decision-making often deviates from these normative axioms. Experiments by Ellsberg and Allais demonstrated that people frequently violate the axioms of independence and consistency when faced with ambiguity or specific risk profiles.\n\nExample 4.4 (Ellsberg Paradox: Ambiguity Aversion) The Ellsberg paradox is a thought experiment that was first proposed by Daniel Ellsberg in 1961. It is a classic example of a situation where individuals exhibit ambiguity aversion, meaning that they prefer known risks over unknown risks. The paradox highlights the importance of considering ambiguity when making decisions under uncertainty.\nThere are two urns each containing 100 balls. It is known that urn A contains 50 red and 50 black, but urn B contains an unknown mix of red and black balls. The following bets are offered to a participant:\n\n\n\nBet\nCondition\nPayoff if True\nPayoff if False\n\n\n\n\n1A\nRed drawn from urn A\n$1\n$0\n\n\n2A\nBlack drawn from urn A\n$1\n$0\n\n\n1B\nRed drawn from urn B\n$1\n$0\n\n\n2B\nBlack drawn from urn B\n$1\n$0\n\n\n\nMost participants prefer bet 1A over 1B (preferring the known 50% chance over the unknown probability of drawing red from urn B), and they also prefer bet 2A over 2B (again preferring the known 50% chance over the unknown probability of drawing black from urn B).\nThis pattern of preferences violates the axioms of expected utility theory. If we denote the probability of drawing a red ball from urn B as \\(p\\), then:\n\nPreferring 1A over 1B implies: \\(0.5 &gt; p\\)\nPreferring 2A over 2B implies: \\(0.5 &gt; (1-p)\\), which means \\(p &gt; 0.5\\)\n\nThese two inequalities are contradictory, yet this preference pattern is commonly observed. The paradox demonstrates that people exhibit ambiguity aversion: they prefer known probabilities (risk) over unknown probabilities (ambiguity), even when the expected values might be similar. This behavior cannot be explained by standard expected utility theory, which treats all probabilities symmetrically regardless of whether they are known or unknown.\nThe Ellsberg paradox has important implications for decision-making in real-world situations where probabilities are often ambiguous or unknown, such as in financial markets, insurance, or strategic business decisions. It vividly demonstrates that human decision-making is sensitive not just to risk (known probabilities) but to ambiguity (unknown probabilities). Standard expected utility theory fails to capture this “ambiguity aversion,” leading to the development of more generalized decision theories that incorporate confidence in probability estimates.\n\n\nExample 4.5 (Allais Paradox: Independence Axiom) The Allais paradox is a choice problem designed by Maurice Allais to show an inconsistency of actual observed choices with the predictions of expected utility theory. The paradox is that the choices made in the second problem seem irrational, although they can be explained by the fact that the independence axiom of expected utility theory is violated.\nWe run two experiments. In each experiment a participant has to make a choice between two gambles.\n\n\n\nExperiment 1\n\n\n\nGamble \\({\\cal G}_1\\)\n\nGamble \\({\\cal G}_2\\)\n\n\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n1\n$5m\n0.89\n\n\n$0m\n0\n$0m\n0.01\n\n\n\n\n\nExperiment 2\n\n\n\nGamble \\({\\cal G}_3\\)\n\nGamble \\({\\cal G}_4\\)\n\n\n\n\n\nWin\nChance\nWin\nChance\n\n\n$25m\n0\n$25m\n0.1\n\n\n$5m\n0.11\n$5m\n0\n\n\n$0m\n0.89\n$0m\n0.9\n\n\n\n\n\n\nThe difference in expected gains is identical in two experiments\n\nE1 = 5*1 \nE2 = 25*0.1 + 5*0.89 + 0*0.01\nE3 = 5*0.11 + 0*0.89\nE4 = 25*0.1 + 0*0.9\nprint(c(E1-E2,E3-E4))\n## -2 -2\n\nHowever, typically a person prefers \\({\\cal G}_1\\) to \\({\\cal G}_2\\) and \\({\\cal G}_4\\) to \\({\\cal G}_3\\), we can conclude that the expected utilities of the preferred are greater than the expected utilities of the second choices. The fact is that if \\({\\cal G}_1 \\geq {\\cal G}_2\\) then \\({\\cal G}_3 \\geq {\\cal G}_4\\) and vice-versa.\nAssuming the subjective probabilities \\(P = ( p_1 , p_2 , p_3)\\). The expected utility \\(E ( U | P )\\) is \\(u ( 0 ) = 0\\) and for the high prize set \\(u ( \\$ 25 \\; \\text{million} ) = 1\\), which leaves one free parameter \\(u = u(\\$ 5 \\; \\text{million})\\).\nHence to compare gambles with probabilities \\(P\\) and \\(Q\\) we look at the difference \\[\nE ( u | P ) - E ( u | Q ) = ( p_2 - q_2 ) u + ( p_3 - q_3 )\n\\]\nFor comparing \\({\\cal G}_1\\) and \\({\\cal G}_2\\) we get \\[\\begin{align*}\nE ( u | {\\cal G}_1 ) - E ( u | {\\cal G}_2 ) &= 0.11 u - 0.1 \\\\\nE ( u | {\\cal G}_3 ) - E ( u | {\\cal G}_4 ) &= 0.11 u - 0.1\n\\end{align*}\\] The order is the same, given your \\(u\\). If your utility satisfies \\(u &lt; 0.1/0.11 = 0.909\\) you take the “riskier” gamble.\nThe Allais paradox is particularly damaging to the Independence Axiom, which states that if you prefer A to B, you should also prefer A+C to B+C. Here, adding a common consequence flips the preference. This finding suggests that people value “certainty” disproportionately, a phenomenon famously captured by Prospect Theory.\n\n\nExample 4.6 (Winner’s Curse) One of the interesting facts about expectation is that when you are in a competitive auctioning game then you shouldn’t value things based on pure expected value. You should take into consideration the event that you win \\(W\\). Really you should be calculating \\(E(X\\mid W)\\) rather than \\(E(X)\\).\nThe winner’s curse: given that you win, you should feel regret: \\(E(X\\mid W) &lt; E(X)\\).\nA good example is claiming racehorse whose value is uncertain.\n\n\n\nValue\nOutcome\n\n\n\n\n0\nhorse never wins\n\n\n50,000\nhorse improves\n\n\n\nSimple expected value tells you \\[\nE(X) = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 50,000 = \\$25,000.\n\\] In a $20,000 claiming race (you can buy the horse for this fixed fee ahead of time from the owner) it looks like a simple decision to claim the horse.\nIt’s not so simple! We need to calculate a conditional expectation. What’s \\(E( X\\mid W )\\), given you win event (\\(W\\))? This is the expected value of the horse given that you win that is relevant to assessing your bid. In most situations \\(E(X\\mid W) &lt; 20,000\\).\nAnother related feature of this problem is asymmetric information. The owner or trainer of the horse may know something that you don’t know. There’s a reason why they are entering the horse into a claiming race in the first place.\nWinner’s curse implies that immediately after you have won, you should feel a little regret, as the object is less valuable to you after you have won! Or put another way, in an auction nobody else in the room is willing to offer more than you at that time.\n\n\nExample 4.7 (The Hat Problem) There are \\(N\\) prisoners in a forward facing line. Each guy is wearing a blue or red hat. Everyone can see all the hats in front of him, but cannot see his own hat. The hats can be in any combination of red and blue, from all red to all blue and every combination in between. The first guy doesn’t know his own hat.\nA guard is going to walk down the line, starting in the back, and ask each prisoner what color hat they have on. They can only answer “blue” or “red.” If they answer incorrectly, or say anything else, they will be shot dead on the spot. If they answer correctly, they will be set free. Each prisoner can hear all of the other prisoners’ responses, as well as any gunshots that indicate an incorrect response. They can remember all of this information.\nThere is a rule that all can agree to follow such that the first guy makes a choice (“My hat is …”) and everyone after that, including the last guy, will get their color right with probability \\(1\\).\nHere is the strategy:\n\nThe last prisoner (Prisoner 100) counts the number of blue hats among the 99 people in front of him.\nIf he sees an even number of blue hats, he yells “Blue”. If odd, he yells “Red”. This yell conveys the parity of blue hats to everyone else.\nPrisoner 99 hears the yell. Now knowing the total parity of blue hats (for 1..99), he counts the blue hats he sees (on 1..98).\n\nIf the total parity (from 100) matches the parity he sees, his own hat must be Red (contributing 0 to the count).\nIf the parities differ, his own hat must be Blue (changing the parity).\n\nHe yells his calculated color, saving himself and passing the parity information down to Prisoner 98.\nThis induction continues, allowing every prisoner except the first to determine their hat color with certainty. The first prisoner survives with probability 0.5 (random guess that conveys the bit).\n\nOne hundred prisoners are too many to work with. Suppose there are two (1 and 2, where 2 is the back). Prisoner 2 sees prisoner 1. If he sees Blue, he yells “Red” (odd). Prisoner 1 hears “Red”, sees 0 blue hats (even). Mismatch -&gt; Blue. This logic extends to any \\(N\\). By using parity, the group collectively solves the problem with only 1 bit of uncertainty for the entire group.\n\n\nExample 4.8 (Lemon’s Problem) The lemon problem is an interesting conditional probability puzzle and is a classic example of asymmetric information in economics. It was first proposed by George Akerlof in his 1970 paper “The Market for Lemons: Quality Uncertainty and the Market Mechanism.” The problem highlights the importance of information in markets and how it can lead to adverse selection, where the quality of goods or services is lower than expected.\nThe basic tenet of the lemons principle is that low-value cars force high-value cars out of the market because of the asymmetrical information available to the buyer and seller of a used car. This is primarily due to the fact that a seller does not know what the true value of a used car is and, therefore, is not willing to pay a premium on the chance that the car might be a lemon. Premium-car sellers are not willing to sell below the premium price so this results in only lemons being sold.\nSuppose that a dealer pays $20K for a car and wants to sell for $25K. Some cars on the market are Lemons. The dealer knows whether a car is a lemon. A lemon is only worth $5K. There is asymmetric information as the customer doesn’t know if the particular new car is a lemon. S/he estimates the probability of lemons on the road by using the observed frequency of lemons. We will consider two separate cases:\n\nLet’s first suppose only 10% of cars are lemons.\nWe’ll then see what happens if 50% are lemons.\n\nThe question is how does the market clear (i.e. at what price do car’s sell). Or put another way does the customer buy the car and if so what price is agreed on? This is very similar to winner’s curse: when computing an expected value what conditioning information should I be taking into account?\nIn the case where the customer thinks that \\(p=0.10\\) of the car’s are lemons, they are willing to pay \\[\nE (X)= \\frac{9}{10} \\cdot 25 + \\frac{1}{10} \\cdot 5 = \\$ 23 K\n\\] This is greater than the initial $20 that the dealer paid. The car then sells at $23K \\(&lt;\\) $25K.\nOf course, the dealer is disappointed that there are lemons on the road as he is not achieving the full value – missing $2000. Therefore, they should try and persuade the customer its not a lemon by offering a warranty for example.\nThe more interesting case is when \\(p=0.5\\). The customer now values the car at \\[\nE (X)  = \\frac{1}{2} \\cdot 25 + \\frac{1}{2} \\cdot 5 = \\$ 15K\n\\] This is lower than the $20K – the reservation price that the dealer would have for a good car. Now what type of car and at what price do they sell?\nThe key point in asymmetric information is that the customer must condition on the fact that if the dealer still wants to sell the car, the customer must update his probability of the type of the car. We already know that if the car is not a lemon, the dealer won’t sell under his initial cost of $20K. So at $15K he is only willing to sell a lemon. But then if the customer computes a conditional expectation \\(E( X \\mid \\mathrm{Lemon})\\) – conditioning on new information that the car is a lemon \\(L\\) we get the valuation \\[\nE ( X \\mid L ) = 1 \\cdot  5 = \\$ 5K\n\\] Therefore only lemons sell, at $ 5K, even if the dealer has a perfectly good car the customer is not willing to buy!\nAgain what should the dealer do? Try to raise the quality and decrease the frequency of lemons in the observable market. This type of modeling has all been used to understand credit markets and rationing in periods of loss of confidence.\n\n\nExample 4.9 (Envelope Paradox) The envelope paradox is a thought experiment or puzzle related to decision-making under uncertainty. It is also known as the “exchange paradox” or the “two-envelope paradox.” The paradox highlights the importance of carefully considering the information available when making decisions under uncertainty and the potential pitfalls of making assumptions about unknown quantities.\nA swami puts \\(m\\) dollars in one envelope and \\(2 m\\) in another. He hands on envelope to you and one to your opponent. The amounts are placed randomly and so there is a probability of \\(\\frac{1}{2}\\) that you get either envelope.\nYou open your envelope and find \\(x\\) dollars. Let \\(y\\) be the amount in your opponent’s envelope. You know that \\(y = \\frac{1}{2} x\\) or \\(y = 2 x\\). You are thinking about whether you should switch your opened envelope for the unopened envelope of your friend. It is tempting to do an expected value calculation as follows \\[\nE( y) = \\frac{1}{2} \\cdot  \\frac{1}{2} x + \\frac{1}{2} \\cdot 2 x = \\frac{5}{4} x &gt; x\n\\] Therefore, it looks as if you should switch no matter what value of \\(x\\) you see. A consequence of this, following the logic of backwards induction, that even if you didn’t open your envelope that you would want to switch! Where’s the flaw in this argument?\nThis problem permits multiple interpretations, yet it serves as an excellent case study for distinguishing between frequentist and Bayesian reasoning. Rather than addressing every possible condition, we will focus on the most instructive cases. First, assume we are risk-neutral (note that we can simply substitute “money” with “utility” without loss of generality). We will compare frequentist vs. Bayesian, and open vs. closed envelope scenarios.\nIf I DO NOT look in my envelope, in this case, even from a frequentist viewpoint, we can find a fallacy in this naive expectation reasoning \\(E[trade] = 5X/4\\) . First, the right answer from a frequentist view is, loosely, as follows. If we switch the envelope, we can obtain \\(m\\) (when \\(X = m\\)) or lose \\(m\\) (when \\(X = 2m\\)) with the same probability \\(1/2\\). Thus, the value of a trade is zero, so that trading matters not for my expected wealth.\nInstead, naive reasoning is confusing the property of variables \\(x\\) and \\(m\\), \\(x\\) is a random variable and \\(m\\) is a fixed parameter which is constant (again, from a frequentist viewpoint). By trading, we can obtain \\(x\\) or lose \\(x/2\\) with the same probability. Here, the former \\(x=m\\) is different from the latter \\(X= 2m\\). Thus, \\(X \\frac{1}{2} - \\frac{X}{2} \\frac{1}{2} = \\frac{X}{4}\\) is the wrong expected value of trading. On the other hand, from a bayesian view, since we have no information, we are indifferent to either trading or not.\nThe second scenario is if I do look in my envelope. As the Christensen & Utts (1992) article said, the classical view cannot provide a completely reasonable resolution to this case. It is just ignoring the information revealed. Also, the arbitrary decision rule introduced at the end of the paper or the extension of it commented by Ross (1996) are not the results of reasoning from a classical approach. However, the bayesian approach provides a systematic way of finding an optimal decision rule using the given information.\nWe can use the Bayes rule to update the probabilities of which envelope your opponent has! Assume \\(p(m)\\) of dollars to be placed in the envelope by the swami. Such an assumption then allows us to calculate an odds ratio \\[\n\\frac{ p \\left ( y = \\frac{1}{2} x | x \\right ) }{ p \\left ( y = 2 x | x \\right ) }\n\\] concerning the likelihood of which envelope your opponent has.\nThen, the expected value is given by \\[\nE(y\\mid x) =  p \\left ( y = \\frac{1}{2} x \\mid  x \\right ) \\cdot  \\frac{1}{2} x +\n  p \\left ( y = 2 x | x \\right ) \\cdot 2 x\n\\] and the condition \\(E(y) &gt; x\\) becomes a decision rule.\nLet \\(g(m)\\) be the prior distribution of \\(m\\). Applying Bayes’ theorem, we have \\[\np(m = x \\mid X = x) = \\frac{p(X = x \\mid m = x) g(x)}{p(X = x)} = \\frac{g(x)}{g(x)+g(x/2)}.\n\\] Similarly, we have \\[\np(m = x/2 \\mid X = x) = \\frac{p(X = x \\mid m = x/2) g(x/2)}{p(X = x/2)} = \\frac{g(x/2)}{g(x)+g(x/2)}.\n\\] The Bayesian can now compute his expected winnings from the two actions. If he keeps the envelope he has, he wins \\(x\\) dollars. If he trades envelopes, he wins \\(x/2\\) if he currently has the envelope with \\(2m\\) dollars, i.e., if \\(m = x/2\\) and he wins \\(2\\)x if he currently has the envelope with \\(m\\) dollars, i.e., \\(m = x\\). His expected winnings from a trade are \\[\nE(W\\mid Trade) = E(Y\\mid X = x) = \\frac{g(x/2)}{g(x)+g(x/2)} \\frac{x}{2} + \\frac{g(x)}{g(x)+g(x/2)} 2x.\n\\] It is easily seen that when \\(g(x/2) = 2g(x)\\), \\(E(W\\mid Trade) = x\\). Therefore, if \\(g(x/2) &gt; 2g(x)\\) it is optimal to keep the envelope and if \\(g(x/2) &lt; 2g(x)\\) it is optimal to trade envelopes. For example, if your prior distribution on \\(m\\) is exponential \\(\\lambda\\), so that \\(g(m) = \\lambda e^{-\\lambda m}\\), then it is easily seen that it is optimal to keep your envelope if \\(x &gt; 2\\log(2)/\\lambda\\).\nThe intuitive value of the expected winnings when trading envelopes was shown to be \\(5x/4\\). This value can be obtained by assuming that \\(g(x)/[g(x) + g(x/2)] =\n1/2\\) for all \\(x\\). In particular, this implies that \\(g(x) = g(x/2)\\) for all x, i.e., \\(g(x)\\) is a constant function. In other words, the intuitive expected winnings assumes an improper “noninformative” uniform density on \\([0, \\infty)\\). It is of interest to note that the improper noninformative prior for this problem gives a truly noninformative (maximum entropy) posterior distribution.\nMost of the arguments in the Christensen & Utts (1992) paper are right, but there is one serious error in the article which is corrected in Bachman-Christensen-Utts (1996) and discussed in Brams & Kilgour (1995). The paper calculated the marginal density of \\(X\\) like below. \\[\\begin{align*}\np(X = x) &= p(m = x)g(x) + p(2m = x)g(x/2) \\\\\n&= \\frac{1}{2} g(x) + \\frac{1}{2} g(x/2)\n\\end{align*}\\] where \\(g(x)\\) is the prior distribution of \\(m\\). However, integrating \\(p(X = x)\\) with respect to \\(x\\) from \\(0\\) to \\(\\infty\\) gives \\(3/2\\) instead of \\(1\\). In fact, their calculation of \\(p(X = x)\\) can hold only when the prior distribution \\(g(x)\\) is discrete and \\(p(X = x)\\), \\(g(m)\\), \\(g(m/2)\\) represent the probabilities that \\(X = x\\), \\(m = m\\), \\(m = m/2\\), respectively.\nFor the correct calculation of the continuous \\(X\\) case, one needs to properly transform the distribution. That can be done by remembering to include the Jacobian term alongside the transformed PDF, or by working with the CDF of \\(X\\) instead. The latter forces one to properly consider the transform, and we proceed with that method.\nLet \\(G(x)\\) be the CDF of the prior distribution of \\(m\\) corresponding to \\(g(x)\\). \\[\\begin{align*}\np(x &lt; X \\leq x+dx) &= p(m = x)dG(x)+ p(2m = x)dG(x/2) \\\\\n&= \\frac{1}{2} \\left( dG(x)+ dG(x/2) \\right)\n\\end{align*}\\] where \\(g(x) = dG(x)/dx\\). Now, the PDF of \\(X\\) is \\[\\begin{align*}\nf_X(x) &= \\frac{d}{dx} p(x &lt; X \\leq x + dx) \\\\\n&= \\frac{1}{2} \\left(g(x) + \\frac{1}{2} g(x/2) \\right)\n\\end{align*}\\] We have an additional \\(1/2\\) in the last term due to the chain rule, or the Jacobian in the change-in-variable formula. (Recall that when transforming a probability density \\(f_X(x)\\) to \\(f_Y(y)\\) where \\(y=g(x)\\), we must scale by \\(|dx/dy|\\) to preserve the total probability mass of 1). Therefore, the expected amount of a trade is \\[\\begin{align*}\nE(Y\\mid X = x) &= \\frac{x}{2} p(2m = x\\mid X = x) + 2 x \\, p(m = x\\mid X = x) \\\\\n&= \\frac{x}{2} \\frac{g(x)}{g(x) + g(x/2)/2} + 2 x \\frac{g(x/2)/2}{g(x) + g(x/2)/2} \\\\\n&=  \\frac{\\frac{x}{2}g(x) + x g(x/2)}{g(x) + g(x/2)/2}\n\\end{align*}\\]\nThus, for the continuous case, trading is advantageous whenever \\(g(x/2) &lt; 4g(x)\\), instead of the decision rule for the discrete case \\(g(x/2) &lt; 2g(x)\\).\nNow, think about which prior will give you the same decision rule as the frequentist result. In the discrete case, \\(g(x)\\) such that \\(g(x/2) = 2g(x)\\), and in the continuous case \\(g(x)\\) such that \\(g(x/2) = 4g(x)\\). However, both do not look like useful, non-informative priors. Therefore, the frequentist approach does not always equal the Bayes approach with a non-informative prior. At the moment you start to treat \\(x\\) as a given number, and consider \\(p(m \\mid X = x)\\) (or \\(p(Y \\mid X = x)\\)), you are thinking in a bayesian way, and need to understand the implications and assumptions in that context.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#decision-trees",
    "href": "04-dec.html#decision-trees",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.4 Decision Trees",
    "text": "4.4 Decision Trees\nDecision trees can effectively model and visualize conditional probabilities. They provide a structured way to break down complex scenarios into smaller, more manageable steps, allowing for clear calculations and interpretations of conditional probabilities.\nEach node in a decision tree, including the root, represents an event or condition. The branches represent the possible outcomes of that condition. Along each branch, you’ll often see a probability. This is the chance of that outcome happening, given the condition at the node. As you move down the tree, you’re looking at more specific conditions and their probabilities. The leaves of the tree show the final probabilities of various outcomes, considering all the conditions along the path to that leaf. Thus, the probabilities of the leaves need to sum to 1.\n\nExample 4.10 (Medical Testing) A patient goes to see a doctor. The doctor performs a test which is 95% sensitive – that is 95 percent of people who are sick test positive and 99% specific – that is 99 percent of the healthy people test negative. The doctor also knows that only 2 percent of the people in the country are sick. Now the question is: if the patient tests positive, what are the chances the patient is sick? The intuitive answer is 99 percent, but the correct answer is 66 percent.\nFormally, we have two binary variables, \\(D=1\\) that indicates you have a disease and \\(T=1\\) that indicates that you test positive for it. The estimates we know already are given by \\(P(D) = 0.02\\), \\(P(T\\mid D) = 0.95\\), and \\(P(\\bar T \\mid \\bar D) = 0.99\\). Here we used shortcut notations, instead of writing \\(P(D=1)\\) we used \\(P(D)\\) and instead of \\(P(D=0)\\) we wrote \\(P(\\bar D)\\).\nSometimes it is more intuitive to describe probabilities using a tree rather than tables. The tree below shows the conditional distribution of \\(D\\) and \\(T\\).\n\n\n\n\n\n\n\n\ngraph LR\n    D --0.02--&gt; d1(D=1)\n    D --0.98--&gt; d0(D=0)\n    d1--0.95--&gt;t1(T=1)\n    d1 --0.05--&gt; t0(T=0)\n    d0--0.01--&gt;t10(T=1)\n    d0 --0.99--&gt; t00(T=0)\n\n\n\n\nFigure 4.2\n\n\n\n\n\nThe result is counter-intuitive. Let’s think about this intuitively. Rather than relying on Bayesian math to help us with this, let us consider another illustration. Imagine that the above story takes place in a small town, with \\(1,000\\) people. From the prior \\(P(D)=0.02\\), we know that 2 percent, or 20 people, are sick, and \\(980\\) are healthy. If we administer the test to everyone, the most probable result is that 19 of the 20 sick people test positive. Since the test has a 1 percent error rate, however, it is also probable that 9.8 of the healthy people test positive, we round it to 10.\nNow if the doctor sends everyone who tests positive to the national hospital, there will be 10 healthy and 19 sick patients. If you meet one, even though you are armed with the information that the patient tested positive, there is only a 66 percent chance this person is sick.\nLet’s extend the example and add the utility of the test and the utility of the treatment. Then the decision problem is to treat \\(a_T\\) or not to treat \\(a_N\\). The Q-function is the function of the state \\(S \\in \\{D_0,D_1\\}\\) and the action \\(A \\in \\{a_T,a_N\\}\\)\n\nUtility of the test and the treatment.\n\n\nA/S\n\\(a_T\\)\n\\(a_N\\)\n\n\n\n\n\\(D_0\\)\n90\n100\n\n\n\\(D_1\\)\n90\n0\n\n\n\nThen expected utility of the treatment is 90 and no treatment is 98. A huge difference. Given our prior knowledge, we should not treat everyone.\n\n0.02*90 + 0.98*90  # treat\n## 90\n0.02*0 + (1-0.02)*100 # do not treat\n## 98\n\nHowever, the expected utility will change when our probability of disease changes. Let’s say that we are in a country where the probability of disease is 0.1 or we performed a test and updated our prior probability of disease to some number \\(p\\). Then the expected utility of the treatment is \\(E\\left[U(a_T)\\right] = 90\\) and no treatment is \\[\nE\\left[U(a_N)\\right] = 0\\cdot p + 100 \\cdot (1-p) = 100(1-p)\n\\] When we are unsure about the value of \\(p\\) we may want to explore how the optimal decision changes as we vary \\(p\\)\n\np = seq(0,1,0.01)\nplot(p, 100*(1-p), type = \"l\", xlab = \"p\", ylab = \"E[U(a)]\")\nabline(h=90, col=\"red\")\nlegend(\"bottomleft\", legend = c(TeX(\"$E[U(a_N)]$\"), \n      TeX(\"$E[U(a_T)]$\")), col = c(\"black\", \"red\"), lty = 1, bty='n')\n\n\n\n\nExpected utility of the treatment and no treatment as a function of the prior probability of disease.\n\n\n\n\nIf our estimate is at the crossover point, then we should be indifferent between treatment and no treatment, if on the left of the crossover point, we should treat, and if on the right, we should not treat. The crossover point is. \\[\n100(1-p) = 90, ~p = 0.1\n\\]\nThe gap of \\(90-100(1-p)\\) is the expected gain from treatment.\nNow, let us calculate the value of test, e.g. the change in expected utility from the test. We will need to calculate the posterior probabilities\n\n# P(D | T = 0) = P(T = 0 | D) P(D) / P(T = 0)\npdt0 = 0.05*0.02/(0.05*0.02 + 0.99*0.98) \n# Expected utility given the test is negative \n# E[U(a_N | T=0)]\nUN0 = pdt0*0 + (1-pdt0)*100\n# E[U(a_T | T=0)]\nUT0 = pdt0*90 + (1-pdt0)*90\n\nsprintf(\"P(D | T = 0) : %.4f, E[U(a_N | T=0)] : %.1f, E[U(a_T | T=0)] : %.1f\", pdt0, UN0, UT0)\n## \"P(D | T = 0) : 0.0010, E[U(a_N | T=0)] : 99.9, E[U(a_T | T=0)] : 90.0\"\n\nGiven test is negative, our best action is not to treat. Our utility is 100. What if the test is positive?\n\n# P(D | T = 1) = P(T = 1 | D) P(D) / P(T = 1)\npdt = 0.95*0.02/(0.95*0.02 + 0.01*0.98)\n# E[U(a_N | T=1)]\nUN1 = pdt*0 + (1-pdt)*100\n# E[U(a_T | T=1)]\nUT1 = pdt*90 + (1-pdt)*90\n\n\n## \"P(D | T = 1) : 0.6597, E[U(a_N | T=1)] : 34.0, E[U(a_T | T=1)] : 90.0\"\n\nThe best option is to treat now! Given the test our strategy is to treat if the test is positive and not treat if the test is negative. Let’s calculate the expected utility of this strategy.\n\n# P(T=1) = P(T=1 | D) P(D) + P(T=1 | D=0) P(D=0)\npt = 0.95*0.02 + 0.01*0.98\n# P(T=0) = P(T=0 | D) P(D) + P(T=0 | D=0) P(D=0)\npt0 = 0.05*0.02 + 0.99*0.98\n# Expected utility of the strategy\nsprintf(\"P(T=1) : %.4f, P(T=0) : %.4f, E[U(a)] : %.1f\", pt, pt0, pt*UT1 + pt0*UN0)\n## \"P(T=1) : 0.0288, P(T=0) : 0.9712, E[U(a)] : 99.6\"\n\nThe utility of our strategy of 100 is above the strategy prior to testing (98), this difference of 2 is called the value of information.\n\n\nExample 4.11 (Mudslide) I live in a house that is at risk of being damaged by a mudslide. I can build a wall to protect it. The wall costs $10,000. If there is a mudslide, the wall will protect the house with probability \\(0.95\\). If there is no mudslide, the wall will not cause any damage. The prior probability of a mudslide is \\(0.01\\). If there is a mudslide and the wall does not protect the house, the damage will cost $100,0000. Should I build the wall?\nLet’s formally solve this as follows:\n\nBuild a decision tree.\nThe tree will list the probabilities at each node. It will also list any costs there are you going down a particular branch.\nFinally, it will list the expected cost of going down each branch, so we can see which one has the better risk/reward characteristics.\n\n\n\n\n\n\n\n\n\ngraph LR\n    B--\"Build: $40, $40.5\"--&gt;Y\n    B--\"Don't Build: $0, $10\"--&gt;N\n    Y--\"Slide: $0, $90\"--&gt;yy[Y]\n    Y--\"No slide $0, $40\"--&gt;40\n    N--\"Slide: $1000, $1000\"--&gt;1000\n    N--\"No slide $0, $0\"--&gt;0\n    yy --\"Hold: $0, $40\"--&gt;401[40]\n    yy --\"Not Hold: $1000, $1040\"--&gt;1040\n\n\n\n\nFigure 4.3: Mudslide Decision Tree\n\n\n\n\n\nThe first dollar value is the cost of the edge, e.g. the cost of building the wall is $10,000. The second dollar value is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide, the expected cost is $15,000. If you build the wall and there is no mudslide, the expected cost is $10,000. The expected cost of building the wall is $10,050. The expected cost of not building the wall is $1,000. The expected cost of building the wall is greater than the expected cost of not building the wall, so you should not build the wall. The dollar value at the leaf nodes is the expected cost of going down that branch. For example, if you build the wall and there is a mudslide and the wall does not hold, the expected cost is $110,000.\nThere’s also the possibility of a further test to see if the wall will hold. Let’s include the geological testing option. The test costs $3000 and has the following accuracies. \\[\nP( T  \\mid  \\mathrm{Slide} ) = 0.90 \\; \\; \\mathrm{and } \\; \\; P( \\mathrm{not~}T  \\mid\n\\mathrm{No \\; Slide} ) = 0.85\n\\] If you choose the test, then should you build the wall?\nLet’s use the Bayes rule. The initial prior probabilities are \\[\nP( Slide ) = 0.01  \\; \\; \\mathrm{and} \\; \\; P ( \\mathrm{No \\; Slide} ) = 0.99\n\\]\n\\[\\begin{align*}\nP( T) & = P( T  \\mid  \\mathrm{Slide} ) P( \\mathrm{Slide} ) +\nP( T  \\mid  \\mathrm{No \\;  Slide} ) P( \\mathrm{No \\; Slide} ) \\\\\nP(T)& = 0.90 \\times 0.01 + 0.15 \\times 0.99 = 0.1575\n\\end{align*}\\] We’ll use this to find our optimal course of action.\nThe posterior probability given a positive test is \\[\\begin{align*}\nP ( Slide  \\mid  T ) & = \\frac{ P ( T  \\mid  Slide ) P ( Slide )}{P(T)} \\\\\n& = \\frac{ 0.90 \\times 0.01}{ 0.1575} = 0.0571\n\\end{align*}\\]\nThe posterior probability given a negative test is \\[\\begin{align*}\nP \\left ( \\mathrm{Slide}  \\mid  \\mathrm{not~}T \\right ) & = \\frac{ P ( \\mathrm{not~}T  \\mid  \\mathrm{Slide} ) P ( \\mathrm{Slide} )}{P(\\mathrm{not~}T)} \\\\\n& = \\frac{0.1 \\times 0.01 }{0.8425} \\\\\n& =0.001187\n\\end{align*}\\]\nCompare this to the initial base rate of a \\(1\\)% chance of having a mud slide.\nGiven that you build the wall without testing, what is the probability that you’ll lose everything? With the given situation, there is one path (or sequence of events and decisions) that leads to losing everything:\n\nBuild without testing (given) Slide (\\(0.01\\))\nDoesn’t hold (\\(0.05\\)) \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{build} \\; \\mathrm{w/o} \\;\n\\mathrm{testing} ) = 0.01 \\times 0.05 = 0.0005\n\\]\n\nGiven that you choose the test, what is the probability that you’ll lose everything? There are two paths that lead to losing everything:\n\nThere are three things that have to happen to lose everything. Test +ve (\\(P=0.1575\\)), Build, Slide (\\(P= 0.0571\\)), Doesn’t Hold (\\(P=0.05\\))\nNow you lose everything if Test -ve (\\(P=0.8425\\)), Don’t Build, Slide given negative (\\(P=0.001187\\)).\n\nThe conditional probabilities for the first path \\[\nP ( \\mathrm{first} \\; \\mathrm{path} ) = 0.1575 \\times 0.0571 \\times 0.05\n= 0.00045\n\\]\nFor the second path \\[\nP ( \\mathrm{second} \\; \\mathrm{path} ) = 0.8425 \\times 0.001187 = 0.00101\n\\]\nHence putting it all together \\[\nP ( \\mathrm{losing} \\; \\mathrm{everything}  \\mid  \\mathrm{testing} ) = 0.00045 + 0.00101 = 0.00146\n\\]\nPutting these three cases together we can build a risk/reward table\n\n\n\nChoice\nExpected Cost\nRisk\nP\n\n\n\n\nDon’t Build\n$1,000\n0.01\n1 in 100\n\n\nBuild w/o testing\n$10,050\n0.0005\n1 in 2000\n\n\nTest\n$4,693\n0.00146\n1 in 700\n\n\n\nThe expected cost with the test is \\(3000+10000\\times 0.1575+100000\\times 0.001187 = 4693\\)\nWhat do you choose?",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "04-dec.html#nash-equilibrium",
    "href": "04-dec.html#nash-equilibrium",
    "title": "4  Utility, Risk and Decisions",
    "section": "4.5 Nash Equilibrium",
    "text": "4.5 Nash Equilibrium\nWhen multiple decision makers interact with each other, meaning the decision of one player changes the state of the “world” and thus affects the decision of another player, then we need to consider the notion of equilibrium. It is a central concept in economics and game theory. The most widely used type of equilibrium is the Nash equilibrium, named after John Nash, who introduced it in his 1950 paper “Equilibrium Points in N-Person Games.” It was popularized by the 1994 film “A Beautiful Mind,” which depicted Nash’s life and work.\nIt is defined as a set of strategies where no player can improve their payoff by unilaterally changing their strategy, assuming others keep their strategies constant. In other words, a Nash equilibrium is a set of strategies where no player has an incentive to deviate from their current strategy, given the strategies of the other players.\nHere are a few examples of Nash equilibria:\n\nPrisoner’s Dilemma: Two prisoners must decide whether to cooperate with each other or defect. The Nash equilibrium is for both to defect, even though they would be better off if they both cooperated.\nPricing Strategies: Firms in a market choose prices to maximize profits, taking into account their competitors’ pricing decisions. The equilibrium is the set of prices where no firm can increase profits by changing its price unilaterally.\nTraffic Flow: Drivers choose routes to minimize travel time, based on their expectations of other drivers’ choices. The equilibrium is the pattern of traffic flow where no driver can reduce their travel time by choosing a different route.\n\n\nExample 4.12 (Marble Game) Here is a subtle marble game where players have to call out (or present) either red or blue with different payoffs according to how things match. Two players \\(A\\) and \\(B\\) have both a red and a blue marble. They present one marble to each other. The payoff table is as follows:\n\nIf both present red, \\(A\\) wins $3.\nIf both present blue, \\(A\\) wins $1.\nIf the colors do not match, \\(B\\) wins $2\n\nThe question is whether it is better to be \\(A\\) or \\(B\\) or does it matter? Moreover, what kind of strategy should you play? A lot depends on how much credit you give your opponent. A lot of empirical research was done on the tit-for-tat strategy, where you cooperate until your opponent defects. Then you match his last response.\nNash equilibrium will also allow us to study the concept of a randomized strategy (ie. picking a choice with a certain probability) which turns out to be optimal in many game theory problems.\nFirst, assume that the players have a \\(\\frac{1}{2}\\) probability of playing Red or Blue. Thus each player has the same expected payoff \\(E(A) = \\$1\\) \\[\\begin{align*}\n    E(A) &= \\frac{1}{4} \\cdot 3 + \\frac{1}{4} \\cdot 1 =1 \\\\\n    E(B) &= \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 2 =1\n\\end{align*}\\] We might go one step further and look at the risk (and measured by a standard deviation) and calculate the variances of each player’s payouts. \\[\\begin{align*}\n    Var (A) & = (1-1)^2 \\cdot \\frac{1}{4} +(3-1)^2 \\cdot \\frac{1}{4} + (0-1)^2 \\cdot \\frac{1}{2} = 1.5 \\\\\n    Var(B) & = 1^2 \\cdot \\frac{1}{2} + (2-1)^2 \\cdot \\frac{1}{2} = 1\n\\end{align*}\\] Therefore, under this scenario, if you are risk averse, player \\(B\\) position is favored.\nThe matrix of probabilities with equally likely choices is given by\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( red, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, red )\\)\n(1/2)(1/2)=1/4\n\n\n\\(P( blue, blue )\\)\n(1/2)(1/2)=1/4\n\n\n\nNow there is no reason to assume ahead of time that the players will decide to play \\(50/50\\). We will show that there’s a mixed strategy (randomized) that is a Nash equilibrium that is, both players won’t deviate from the strategy. We’ll prove that the following equilibrium happens:\n\n\\(A\\) plays Red with probability 1/2 and blue 1/2\n\\(B\\) plays Red with probability 1/4 and blue 3/4\n\nIn this case the expected payoff to playing Red equals that of playing Blue for each player. We can simply calculate: \\(A\\)’s expected payoff is 3/4 and \\(B\\)’s is $1 \\[\nE(A) = \\frac{1}{8} \\cdot 3 + \\frac{3}{8} \\cdot 1 = \\frac{3}{4}\n\\] Moreover, \\(E(B) =1\\), thus \\(E(B) &gt; E(A)\\). We see that \\(B\\) is the favored position. It is clear that if I know that you are going to play this strategy and vice-versa, neither of us will deviate from this strategy – hence the Nash equilibrium concept.\nNash equilibrium probabilities are: \\(p=P( A \\; red )= 1/2, p_1 = P( B \\; red ) = 1/4\\) with payout matrix\n\n\n\n\\(A,B\\)\nProbability\n\n\n\n\n\\(P( red, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( red, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\\(P( blue, red )\\)\n(1/2)(1/4)=1/8\n\n\n\\(P( blue, blue )\\)\n(1/2)(3/4)=3/8\n\n\n\nWe have general payoff probabilities: \\(p=P( A \\; red ), p_1 = P( B \\; red )\\)\n\\[\\begin{align*}\n    f_A ( p , p_1 ) =& 3 p p_1 + ( 1 -p ) ( 1 - p_1 ) \\\\\n    f_B ( p , p_1 ) =& 2 \\{ p(1 - p_1) + ( 1 -p ) p_1 \\}\n\\end{align*}\\]\nTo find the equilibrium point \\[\\begin{align*}\n    ( \\partial / \\partial p ) f_A ( p , p_1 ) =& 3 p_1 - ( 1 - p_1 ) = 4 p_1 -1 \\; \\; \\mathrm{so} \\; \\; p_1= 1/4 \\\\\n    ( \\partial / \\partial p_1 ) f_B ( p , p_1 ) =& 2 ( 1 - 2p ) \\; \\; \\mathrm{so} \\; \\; p= 1/2\n\\end{align*}\\]\nMuch research has been directed to repeated games versus the one-shot game and is too large a topic to discuss further. However, we can analyze one particularly famous strategy: tit-for-tat.\nIn a repeated version of the marble game (Camerer 2003), player \\(A\\) can employ the tit-for-tat strategy: start by playing Red in the first round, and thereafter simply repeat the opponent’s previous move. This strategy was famously shown by Robert Axelrod in his computer tournaments to be remarkably effective in iterated prisoner’s dilemma games, despite its simplicity.\nLet us analyze how this strategy performs against different opponent behaviors. Consider player \\(A\\) using tit-for-tat against various strategies employed by \\(B\\).\nIf \\(B\\) always plays Red, the game locks into a mutually beneficial pattern: both players present Red every round, and \\(A\\) wins $3 each time. The per-round expected payoff for \\(A\\) is simply $3.\nIf \\(B\\) always plays Blue, the dynamics are more interesting. In round 1, \\(A\\) plays Red while \\(B\\) plays Blue, so \\(B\\) wins $2. From round 2 onward, \\(A\\) copies \\(B\\)’s Blue, resulting in both playing Blue and \\(A\\) winning $1 per round. Over \\(n\\) rounds, \\(A\\)’s total payoff is \\(-2 + (n-1) \\cdot 1 = n - 3\\), giving an average of \\((n-3)/n\\) which approaches $1 as \\(n \\to \\infty\\).\nIf both players use tit-for-tat starting with Red, they remain synchronized forever, each playing Red in every round, yielding $3 per round for \\(A\\). This mutual cooperation is stable because neither player ever defects.\nThe more challenging case arises when \\(B\\) employs an alternating strategy. Suppose \\(B\\) plays Red, Blue, Red, Blue, and so on. Then the sequence unfolds as follows: in round 1, \\((A,B) = (R,R)\\) so \\(A\\) wins $3; in round 2, \\(A\\) copies \\(B\\)’s Red while \\(B\\) switches to Blue, giving \\((R,B)\\) and \\(B\\) wins $2; in round 3, \\(A\\) copies Blue while \\(B\\) plays Red, giving \\((B,R)\\) and \\(B\\) wins $2; this pattern of \\(B\\) winning $2 continues indefinitely. After the first round, tit-for-tat becomes permanently out of phase with the alternating opponent, resulting in perpetual losses for \\(A\\).\n\n\nSimulate tit-for-tat against different opponent strategies\nset.seed(17)\nn_rounds &lt;- 50\n\n# Payoff function: positive means A wins, negative means B wins\npayoff &lt;- function(a, b) {\n  if (a == b) {\n    if (a == \"R\") return(3) else return(1)\n  } else {\n    return(-2)  # B wins\n  }\n}\n\n# Tit-for-tat: start with R, then copy opponent's last move\ntit_for_tat &lt;- function(opponent_history) {\n  if (length(opponent_history) == 0) return(\"R\")\n  return(tail(opponent_history, 1))\n}\n\n# Simulate against different strategies\nsimulate_game &lt;- function(b_strategy, n = n_rounds) {\n  a_history &lt;- c()\n  b_history &lt;- c()\n  payoffs &lt;- c()\n  \n  for (i in 1:n) {\n    a_move &lt;- tit_for_tat(b_history)\n    b_move &lt;- b_strategy(i, a_history)\n    \n    a_history &lt;- c(a_history, a_move)\n    b_history &lt;- c(b_history, b_move)\n    payoffs &lt;- c(payoffs, payoff(a_move, b_move))\n  }\n  return(cumsum(payoffs))\n}\n\n# B strategies\nb_always_red &lt;- function(round, a_hist) \"R\"\nb_always_blue &lt;- function(round, a_hist) \"B\"\nb_alternating &lt;- function(round, a_hist) if (round %% 2 == 1) \"R\" else \"B\"\nb_random &lt;- function(round, a_hist) sample(c(\"R\", \"B\"), 1)\nb_tit_for_tat &lt;- function(round, a_hist) {\n  if (length(a_hist) == 0) return(\"R\")\n  return(tail(a_hist, 1))\n}\n\n# Run simulations\nresults &lt;- data.frame(\n  round = 1:n_rounds,\n  vs_always_red = simulate_game(b_always_red),\n  vs_always_blue = simulate_game(b_always_blue),\n  vs_alternating = simulate_game(b_alternating),\n  vs_random = simulate_game(b_random),\n  vs_tit_for_tat = simulate_game(b_tit_for_tat)\n)\n\n# Plot\nlibrary(tidyr)\nresults_long &lt;- pivot_longer(results, -round, \n                             names_to = \"opponent\", values_to = \"cumulative_payoff\")\nresults_long$opponent &lt;- factor(results_long$opponent,\n  levels = c(\"vs_always_red\", \"vs_tit_for_tat\", \"vs_random\", \n             \"vs_always_blue\", \"vs_alternating\"),\n  labels = c(\"B: Always Red\", \"B: Tit-for-Tat\", \"B: Random\", \n             \"B: Always Blue\", \"B: Alternating\"))\n\nggplot(results_long, aes(x = round, y = cumulative_payoff, color = opponent)) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"Round\", y = \"Cumulative Payoff for A\", color = \"Opponent Strategy\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\nCumulative payoff for player A using tit-for-tat against different opponent strategies over 50 rounds.\n\n\n\n\nThe simulation reveals that tit-for-tat performs excellently against cooperative or consistent opponents (always Red, or another tit-for-tat player), reasonably well against random play, but poorly against the alternating strategy which exploits the one-round lag inherent in the copying mechanism.\n\n\nCode\n# Average per-round payoff in the last 20 rounds\n# Calculate average payoffs for last 20 rounds\npayoff_summary &lt;- data.frame(\n  Opponent = c(\"Always Red\", \"Tit-for-Tat\", \"Random\", \"Always Blue\", \"Alternating\"),\n  `Average Payoff` = c(\n    mean(diff(c(0, results$vs_always_red[(n_rounds-19):n_rounds]))),\n    mean(diff(c(0, results$vs_tit_for_tat[(n_rounds-19):n_rounds]))),\n    mean(diff(c(0, results$vs_random[(n_rounds-19):n_rounds]))),\n    mean(diff(c(0, results$vs_always_blue[(n_rounds-19):n_rounds]))),\n    mean(diff(c(0, results$vs_alternating[(n_rounds-19):n_rounds])))\n  ),\n  check.names = FALSE\n)\n\nkable(payoff_summary, \n      caption = \"Average payoff per round (last 20 rounds)\",\n      digits = 2)\n\n\n\nAverage payoff per round (last 20 rounds)\n\n\nOpponent\nAverage Payoff\n\n\n\n\nAlways Red\n7.50\n\n\nTit-for-Tat\n7.50\n\n\nRandom\n-0.35\n\n\nAlways Blue\n2.35\n\n\nAlternating\n-4.75\n\n\n\nAverage payoff per round (last 20 rounds)\n\n\nThe key insight from Axelrod’s tournaments was that tit-for-tat succeeds not by exploiting opponents, but by fostering cooperation. It is nice (never defects first), retaliatory (punishes defection immediately), forgiving (returns to cooperation if the opponent does), and clear (easy for opponents to understand and predict). These properties make it robust in environments where opponents can learn and adapt, even though it can be exploited by adversaries who know the strategy and deliberately play to keep it out of phase.\nThomas Schelling, in his influential essay The Manipulation of Risk, extends these game-theoretic insights to international relations, arguing that conflicts between major powers often resemble what he calls competitions in risk taking or tests of nerve rather than direct tests of force. In this framing, the marble game captures something essential about diplomacy: outcomes are determined not by who can bring the most force to bear, but by who is willing to escalate further or who can make it appear that escalation is forthcoming.\nSchelling emphasizes the central role of face in such interactions, though he uses the term carefully. Face is not mere pride or status; it is a country’s (or player’s) reputation for action, the expectations others hold about how it will behave in future encounters. As Schelling puts it, face is the interdependence of a country’s commitments. When player \\(A\\) in our marble game employs tit-for-tat and retaliates against defection, \\(A\\) is not merely responding to the current round but preserving expectations about future behavior. If \\(A\\) were to let defections pass unpunished, \\(B\\) would rationally update beliefs about what \\(A\\) will tolerate, potentially inviting further exploitation.\nThis perspective illuminates why tit-for-tat’s retaliatory property is so important: it maintains credibility. Schelling notes that it would be hard to persuade an adversary, if one yielded repeatedly on minor issues, that one would stand firm on a vital issue. The same logic applies in repeated games: a player who fails to punish defection signals weakness, inviting further defection. Yet Schelling also cautions that face should not attach itself to unworthy enterprises, and that it is sometimes wise to help an adversary save face by providing an exit that does not appear to be capitulation. In game-theoretic terms, this suggests that optimal strategies in repeated interactions must balance retaliation with opportunities for returning to cooperation, precisely the forgiving quality that makes tit-for-tat effective.\n\nEquilibrium analysis helps predict the likely outcomes of strategic interactions, even when individuals are acting in their own self-interest. Further, we can use it to understand how markets function and how firms make pricing and production decisions or to design mechanisms (e.g., auctions, voting systems) that incentivize desired behavior and achieve efficient outcomes.\nOne major drawback is that equilibrium analysis relies on assumptions about rationality and common knowledge of preferences and strategies, which may not always hold in real-world situations. Furthermore, some games may have multiple equilibria, making it difficult to predict which one will be reached. The problem of dynamic strategies, when individuals may learn and adjust their strategies as they gain experience, is hard.\n\n\n\n\nCamerer, Colin F. 2003. Behavioral Game Theory: Experiments in Strategic Interaction. The Roundtable Series in Behavioral Economics. New York Princeton: Russell sage foundation Princeton university press.\n\n\nDeGroot, Morris H. 1974. “Reaching a Consensus.” Journal of the American Statistical Association 69 (345): 118–21.\n\n\nKelly, J. L. 1956. “A New Interpretation of Information Rate.” Bell System Technical Journal 35 (4): 917–26.\n\n\nMorris, Stephen. 1994. “Trade with Heterogeneous Prior Beliefs and No-Trade Theorems.” Econometrica : Journal of the Econometric Society 62 (6): 1327–47.\n\n\n———. 1996. “Speculative Trade with Rational Beliefs.” Journal of Economic Theory 70 (2): 445–72.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” Histoy of {{Economic Thought Chapters}}. McMaster University Archive for the History of Economic Thought.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utility, Risk and Decisions</span>"
    ]
  },
  {
    "objectID": "05-ab.html",
    "href": "05-ab.html",
    "title": "5  A/B Testing",
    "section": "",
    "text": "5.1 Hypothesis Testing\nThe Internet age opened the door to enormous data collection on personal preferences, behaviors, and actions. The data collected are observational rather than data collected from designed experiments—where we can control the environment to find the effects of interventions. User interface design is a prime example of this. Companies like Google, Amazon, and Netflix run thousands of experiments daily to optimize user experience and revenue. A/B testing is essentially a randomized controlled trial at scale, where two variants (A and B) are compared to determine which one performs better on a specific metric. The statistical machinery remains the same—we formulate a null hypothesis (no difference between variants), collect data, and compute a statistic to see if the observed difference is significant or just noise. The A/B variations can be physical, such as different colors or layouts, or they can be algorithmic, such as different search algorithms or recommendation systems.\nHowever, a key distinction exists between traditional scientific hypothesis testing and modern A/B testing: the objective. In scientific research, the primary goal is often truth discovery—establishing a reproducible fact about the world. This necessitates a conservative approach with strict control over False Positives (Type I errors). In contrast, the goal of A/B testing in industry is often decision making—choosing the best option to maximize a business metric like revenue or engagement. Here, the cost of a missed opportunity (Type II error) can be just as detrimental as a false alarm. Consequently, industry practitioners may focus more on the magnitude of the effect (effect size) and the expected value of the decision, rather than relying solely on a rigid p-value threshold.\nThroughout this chapter we use the classical testing language as an operational planning tool: Type I and Type II errors, power, and significance levels. In particular, \\(\\alpha\\) denotes a frequentist Type I error rate (test size). Chapter 6 revisits the same problems from a Bayesian decision-theoretic viewpoint, where posterior probabilities, Bayes factors, and explicit losses take center stage; when we discuss credibility or posterior uncertainty, we will avoid overloading \\(\\alpha\\) with a second meaning.\nHow do we know if the difference between variants is real or just noise? Consider a coin flip: if it comes up heads twice in a row, is the coin biased? Obviously two tosses aren’t enough to tell. Hypothesis testing formalizes this intuition—it tells us whether we have enough evidence to draw a conclusion, or whether we need more data. Let’s work through a concrete example.\nYou work as a quant for a trading firm and you have developed a new algorithm to trade stocks. You tested your algorithm on historical data and it outperformed the state-of-the-art algorithm used in your company. Now, the important question is whether your trading strategy can truly outperform the market or it just got lucky. We need to analyze the performance of the algorithm after it was created and decide whether we have truly discovered a dominant strategy. The effect we try to measure is usually present in some statistics that we calculate from data, for example, sample mean, proportion, or difference in means.\nCentral Limit Theorem\nCLT states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution. The theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nThe Central Limit Theorem originates from the De Moivre-Laplace theorem, published by de Moivre in 1738, which established the normal approximation to the binomial distribution. According to this theorem the standard normal distribution arises as the limit of scaled and centered Binomial distributions, in the following sense. Let \\(x_1,\\ldots,x_n\\) be independent, identically distributed Rademacher random variables, that is, independent random variables with distribution \\[\nP(X_i = 1) = P(X_i = -1) = \\frac{1}{2}.\n\\] Then, the distribution of the sum of these random variables converges to the standard normal distribution as \\(n\\) tends to infinity. That is, for any \\(a&lt;b\\), we have \\[\n\\lim_{n\\to\\infty} P\\left(a \\le \\frac{X_1+\\cdots+X_n}{\\sqrt{n}} \\le b\\right) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx.\n\\] In this case, the sum \\(X_1+\\cdots+X_n\\) has mean \\(n\\mu\\) and variance \\(n\\sigma^2\\), so that the standardized sum \\((X_1+\\cdots+X_n - n\\mu)/\\sqrt{n\\sigma^2}\\) has mean \\(0\\) and variance \\(1\\). The theorem then states that the distribution of this standardized sum converges to the standard normal distribution as \\(n\\) tends to infinity.\nIn 1889 Francis Galton published a paper where he described what we now call the Galton Board. The Galton Board is a vertical board with interleaved rows of pins. Balls are dropped from the top, and bounce left and right as they hit the pins. Eventually, they are collected into one of several bins at the bottom. The distribution of balls in the bins approximates the normal distribution. Each pin is a physical realization of the binomial draw and each row is a summand. The location at the bottom is a sum of the binomial draws. The galton-ball.r script simulates the Galton board experiment. The script is available in the R folder of the book repository. Figure 5.1 shows the result of the simulation. The distribution of the balls in the bins approximates the normal distribution.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#hypothesis-testing",
    "href": "05-ab.html#hypothesis-testing",
    "title": "5  A/B Testing",
    "section": "",
    "text": "Example 5.1 (Pyx Trial) The “Pyx Trial” refers to an ancient ceremony held in the United Kingdom’s Royal Mint. This tradition, dating back to the 12th century, is a method of testing the quality of minted coins to ensure it meets the standards of weight and purity set by law. The term “Pyx” comes from the Greek word “pyxis,” meaning a small box, which is used to hold the sample coins that are to be tested.\nSir Isaac Newton became Warden of the Mint in 1696 and later the Master of the Mint. His role was crucial in reforming the coinage and improving its quality. Newton was rigorous in enforcing standards and combating counterfeiting and clipping (the practice of shaving off small amounts of precious metal from coins). Newton applied his scientific mind to the problems of minting, including refining assays (the testing of the purity of metals), improving the design of coins to prevent clipping, and introducing milled edges on coins.\nThe trial starts by selecting \\(n\\) coins from each batch produced by the Royal Mint. These coins are placed in a box called the Pyx. The number of coins used in the Trial of the Pyx can vary each year. This number depends on several factors, including the variety and quantity of coins produced by the Royal Mint in that particular year. Typically, a representative sample of each type of coin minted is selected for testing. Then, for each coin attribute (weight, size, composition), the mean (average) value of the sample is calculated as well as the variance of the mean.\nSuppose we have minted one million of coins and collected the sample of \\(n=100\\) coins, the legal weight tolerance for a certain coin is 0.05 grams.\nWe will use the simulated data for our analysis. Let’s simulate the weights of all of the coins produced\n\nset.seed(17) # Kharlamov\nallcoins = runif(1000000, 4.95, 5.05)\n\nNow, we survey 100 randomly selected coins\n\nxbar = mean(survey_sample)\n\nThe sample mean of 4.9950764 is very close to the true mean of 5. However, if we were to collect a different sample, the sample mean would be slightly different\n\nxbar = mean(survey_sample)\n\nNow, we simulate 2000 surveys and calculate the sample mean for each survey.\n\nhist(prep, breaks = 30, freq = F, main=\"\", col=\"lightblue\")\np = seq(4.9,5.1,length.out = 500)\nlines(p, dnorm(p,mean(prep),sd(prep)), col=\"red\",lwd=3)\n\n\n\n\nHistogram of voting proportions\n\n\n\n\nWe see that the red bell-curve (normal density) is a good model for the distribution over the means calculated from samples. In fact, the central limit theorem says that sample means follow a normal distribution. We need to estimate the mean and standard deviation of this bell curve. It is natural to use the sample mean as the estimate of the mean of the bell curve mean(prep): 4.9998448. The mean \\[\n\\bar x = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n\\] is close to the true population mean of 5. We sometimes use the notation \\(\\hat \\mu\\) to denote an estimate. So we have \\(\\hat \\mu = \\bar x\\).\nHowever, the standard deviation is much lower compared to the standard deviation of the population\n\nsd(prep)\n## 0.0028\nsd(allcoins)\n## 0.029\n\nThe variance of the mean measures how much the sample mean is expected to vary from one sample to another, if you were to take multiple samples from the same population.\nAssuming that samples are uncorrelated (correct sampling procedure is important!), the variance of the mean is given by \\[\n\\Var{\\bar X} = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_i\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\Var{X_i} = \\frac{\\sigma^2}{n}.\n\\]\nTherefore, the variance of the mean formula is \\[\n\\Var{\\bar X} = \\frac{\\sigma^2}{n}.\n\\] If we know the population variance \\(\\Var{X_i} = \\sigma^2\\), then we can calculate the variance of the mean. However, in practice, we do not know the population variance. Instead, we estimate it using the sample variance \\(s^2\\). The estimated variance of the mean is then \\[\n\\Var{\\bar X} = \\frac{s^2}{n}.\n\\] The standard deviation of the mean is called the standard error and is given by \\[\ns_{\\bar X} = \\sqrt{\\Var{\\bar X}} = \\frac{s}{\\sqrt{n}}.\n\\]\nLet’s compare the standard error of the mean and standard deviation calculated from the simulations\n\nsd(prep)\n## 0.0028\nsd(allcoins)/sqrt(100)\n## 0.0029\n\nThey are very close!\nThis statistical property allows us to quantify uncertainty about the sample mean and say something about the true value of the mean \\(\\mu\\), in terms of a probabilistic interval statement.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Galton Board\n\n\n\n\nExample 5.2 (Android Earthquake Alerts) A fantastic modern example of the Central Limit Theorem in action is Google’s Android Earthquake Alerts System. The core idea is to use the accelerometers present in billions of Android smartphones to create a global earthquake detection network.\nEach individual phone’s accelerometer is a “noisy sensor.” It can be triggered by many events that are not earthquakes, such as the phone being dropped, vibrations from a passing truck, or even loud music. A signal from a single phone is therefore a “weak signal” — on its own, it’s not reliable enough to issue an earthquake alert.\nHowever, Google’s system aggregates signals from a vast number of phones in a specific geographic area. When a real earthquake occurs, thousands or even millions of phones in the affected region will detect the seismic waves (P-waves and S-waves) at roughly the same time.\nBy averaging the readings from this massive number of sensors, the system can effectively cancel out the random noise from individual phones. A single phone dropping is a random, isolated event. But thousands of phones vibrating in a synchronized pattern is a clear, strong signal that is highly unlikely to be due to chance.\nThis is a direct application of the principle that the standard error of the mean is inversely proportional to the square root of the sample size (\\(s_{\\bar x} = s/\\sqrt{n}\\)). Here, \\(n\\) is the number of phones. With a massive \\(n\\), the standard error of the average measurement becomes incredibly small. This allows the system to have a very high level of confidence that the detected event is a real earthquake, enabling it to send out timely alerts to people who may be in danger. It’s a powerful demonstration of how aggregating many weak, unreliable signals can produce a single, highly reliable and actionable insight.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#confidence-intervals",
    "href": "05-ab.html#confidence-intervals",
    "title": "5  A/B Testing",
    "section": "5.2 Confidence Intervals",
    "text": "5.2 Confidence Intervals\nThe fact that the distribution of the simulated means from the Pyx example can be described well by a normal bell curve, in fact has a theoretical justification. It is called the Central Limit Theorem. The Central Limit Theorem states that, given a sufficiently large sample size, the distribution of the sample means will be approximately normally distributed, regardless of the shape of the population distribution. This normal distribution is also known as the Gaussian distribution.\nThere are a few conditions. The sampled observations must be independent. In practice, this means that the sampling should be random, and one observation should not influence another. Further, the sample size should be sufficiently large. While there is no strict rule for what constitutes ‘large enough,’ a common guideline is a sample size of 30 or more. However, if the population distribution is far from normal, a larger sample size may be required.\nWe can estimate the mean of this bell curve using \\(\\bar x\\) and the standard deviation (standard error) using \\(s/\\sqrt{n}\\).\nThe square-root nature of this relation is somewhat unfortunate. To double your certainty about the population mean, you need to quadruple the sample size.\nOne of the main applications of this result is the construction of confidence intervals. A confidence interval is a range of values that is likely to contain the true value of the population mean. It is a plausible range for the quantity we are trying to estimate. The confidence interval is calculated using the sample mean \\(\\bar x\\) and the standard error \\(s/\\sqrt{n}\\). The confidence interval is calculated as follows \\[\n\\bar x \\pm 1.96s_{\\bar x}, ~ s_{\\bar x} = \\frac{s}{\\sqrt{n}}.\n\\]\nThe theorem applies to a wide range of population distributions, including distributions that are not normal. This universality makes it one of the most powerful and widely-used theorems in statistics.\nHere are a few conclusions we can make thus far\n\nMean estimates are based on random samples and therefore random (uncertain) themselves\nWe need to account for this uncertainty!\nStandard Error measures the uncertainty of an estimate\nUsing properties of the Normal distribution, we can construct \\(95\\)% Confidence Intervals\nThis provides us with a plausible range for the quantity we are trying to estimate.\n\nRecall the Patriots coin toss example from Chapter 3, we know that they won 19 out of 25 tosses during the 2014-2015 season. In this example, our observations are values 0 (lost toss) and 1 (won toss) and the average over those 0-1 observations is called the proportion and is denoted by \\(\\hat p\\) instead of \\(\\bar x\\). When we deal with proportions, we can calculate the sample variance from its mean \\(\\hat p\\) as follows \\[\ns^2_{\\hat p} = \\frac{\\hat p(1-\\hat p)}{n}.\n\\] Thus, we know that given our observations and CLT, the sampling distribution of our estimator \\(\\hat{p}\\) is normal. Our best guess at the mean \\(\\hat p\\) is \\(19/25 = 0.76\\) and variance \\(s^2 = 0.76(1-0.76)/25 = 0.0073\\) \\[\n\\hat p \\sim N(0.76, 0.0073).\n\\] Then a \\(95\\%\\) Confidence Interval is calculated by\n\n0.76 + c(-1.96,1.96)*sqrt(0.0073)\n## 0.59 0.93\n\nSince 0.5 is outside the confidence interval, we say that we do not have enough evidence to say that the coin tosses were fair.\nThen we formulate a hypothesis that we are to test. Our status-quo assumption (there is no effect) is called the null hypothesis and is typically denoted by \\(H_0\\).\n\nMythbusters Example\nWhile A/B testing is industry-standard for software, the same methodology applies to testing physical phenomena or human behavior, dealing with small sample sizes where uncertainty is higher. In 2006 the creators of Mythbusters TV show on Discovery channel wanted to test whether yawning is contagious in humans. They recruited 50 participants and each of those went through an interview. At the end of 34 randomly selected interviews the interviewer did yawn. Then participants were asked to wait in a next door room. Out of 34 participants from the experimental group, 10 did yawn (29.4%) and only 4 out of 16 (25%) in the control group did yawn. The difference in the proportion of those who did yawn was 4.4%. The show hosts Kari Byron, Tory Belleci and Scottie Chapman concluded that yawn is indeed contagious.\nTo translate the question from this experiment into language of hypothesis testing, we say that our null hypothesis is that proportion of yawning participants in control (\\(\\hat p_c\\)) and experimental group (\\(\\hat p_e\\)) is the same \\(H_0: \\hat p_c - \\hat p_e = 0\\), and the alternative hypothesis is \\(H_a: \\hat p_c &lt; \\hat p_e\\). The goal is to use the data to tell us if the hypothesis is correct or not.\nA key statistical fact behind the hypothesis testing is the Central Limit Theorem. It states that if we have a sample \\(\\{x_1,\\ldots,x_n\\}\\) with \\(n\\) observations from any distribution \\(x_i \\sim p(x)\\), then the average of the sample follows a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) \\[\n\\bar X = \\frac{1}{n}\\sum_{i=1}^{n}X_i \\sim N(\\mu, \\sigma^2/n)\n\\]\nLet us use a simple simulated data set to demonstrate the central limit theorem. We generate 100 outcomes of a Bernoulli trial with \\(p=0.3\\) and calculate the mean of this sample \\(\\hat p\\). We repeat it 2000 times and compare the empirical distribution of \\(\\hat p\\) with \\(N(0.3, 0.046)\\).\n\nset.seed(1)\na = replicate(2000,mean(rbinom(100,1,0.3)))\nplot(density(a), main=\"\")\nse = sqrt(0.3*(1-0.3)/100)  #  0.046\nx = seq(0,0.5,length.out = 300)\nlines(x,dnorm(x,mean = 0.3,sd = se), col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nThere are three ways to quantify uncertainty in hypothesis testing. The first approach relies on calculating confidence intervals, as we did for the yawn example. There are two complementary approaches. One is to calculate what is called a \\(p\\)-value, that is the probability of getting the result observed in the data, assuming null-hypothesis is true. If \\(p\\)-value is low, then we reject the null-hypothesis. For the yawn example, the conditional probability that the observed difference in proportions is greater than 0.044, given null hypothesis is true is given by\n\\[\np\\text{-value} = P(\\hat p_e - \\hat p_c \\ge 0.044 \\mid H_0),\n\\] which can be calculated using pnorm function\n\n1 - pnorm(0.044,0,sqrt(0.0177))     \n## 0.37\n\nThe \\(p\\)-value of 0.37 means that there is a 37% chance to observe the difference to be greater than 0.044 assuming the null-hypothesis. It is quite high! We want the \\(p\\)-value to be low, only then we can claim that we have discovered a new fact, i.e. that yawning is contagious. In many applications we require this number to be at most 0.005. The smallest acceptable \\(p\\)-value is called the significance level and is typically denoted as \\(\\alpha\\). We can test the hypothesis at different levels of significance \\(\\alpha\\). Further we assume that the statistic we are analyzing follows the sampling distribution. The probability distribution of the statistics values is either Normal, or \\(t\\)-distribution for continuous variable.\nFundamentally, a statistical hypothesis is a testable statement about a population parameter that can be evaluated using observed data. To summarize the process of testing a significance of our discovery for proportions, we perform the hypothesis testing following the 5-step process.\n\nStep 1: Formulate the Null Hypothesis (\\(H_0\\)), which we assume to be true unless there is sufficient evidence to the contrary. Then, alternative Hypothesis (\\(H_1\\)): test against the null, e.g. \\(H_0: p_e - p_c = 0\\), and \\(H_a: p_e - p_c &gt; 0\\). If there is evidence that \\(H_0\\) is false, we accept \\(H_1\\).\nStep 2: Select the significance level \\(\\alpha\\). While \\(\\alpha = 0.05\\) (the 5% level) is the most commonly used, \\(\\alpha = 0.01\\) (the 1% level) is prevalent in medical and quality assurance examples.\nStep 3: Compute the Test Statistic (\\(Z\\) or \\(T\\))\nStep 4: Formulate the Decision Rule. For example, reject the Null hypothesis if \\(|Z| &gt; 1.96\\)\nStep 5: Make a Decision, Compute the p-value. p-value is the smallest significance level at which a null hypothesis can be rejected. If \\(p\\)-value \\(&lt;\\alpha\\), we have evidence that \\(H_1\\) is true, we accept \\(H_1\\) and claim we have a discovery. If \\(p\\)-value is \\(\\ge \\alpha\\), then we cannot reject the null-hypothesis.\n\nIn Steps 1-2 we formulate the hypothesis. In steps 3-5 we make a decision.\nIn the context of hypothesis testing, we come back to the type I and type II errors we already discussed. They can be used to describe two types of errors you can make when testing\n\n\n\nError Type\nDescription\n\n\n\n\nType I Error\nRejecting a true \\(H_0\\)\n\n\nType II Error\nNot rejecting a false \\(H_0\\)\n\n\n\nAnd the significance level is then \\[\nP(\\mbox{reject } H_0 \\mid H_0 \\; \\mbox{ true}) =\nP(\\mbox{type I error}).\n\\]\nHypothesis testing is often used in scientific reporting. For example, the discovery of Higgs Boson was announced as a result of hypothesis testing. Scientists used the five-sigma concept to test the Higgs-Boson hypothesis. This concept, however, is somewhat counter-intuitive. If the particle doesn’t exist, one in 3.5 million is the chance an experiment just like the one announced would nevertheless come up with a result appearing to confirm it does exist. In other words, one in 3.5 million is the likelihood of finding a false positive—a fluke produced by random statistical fluctuation that seems as definitive as the findings released by two teams of researchers at the CERN laboratory in Geneva. So we can talk about the significance level as \\(p\\)-value to be one-in-3.5-million and then the \\(Z\\)-score is five.\nThe test statistic (\\(T\\) or \\(Z\\)) quantifies uncertainty between the null-hypothesis value and the observed one and is equal to the number of standard deviations they are apart from each other. This value is called the \\(Z\\)-score, and is calculated as \\[\nZ = \\frac{ \\bar{x} - \\mu_0 }{s/\\sqrt{n}},\n\\] where \\(\\mu_0\\) is the mean assumed under null-hypothesis. The square root of the statistic’s variance \\(s/\\sqrt{n}\\) is called standard error and is denoted by \\(se(\\bar X)\\).\nLet’s calculate the \\(Z\\)-score for the yawning example. Let \\(\\mu_0 = 0\\), \\(\\hat{p} = \\hat p_e - \\hat p_c = 0.044\\), \\(\\Var{\\hat p }=\\Var{\\hat p_e - \\hat p_c} = 0.0177\\), we get \\(Z\\) statistic to be 0.33. Thus, our observed difference is very close to 0.\nTo summarize the duality of confidence interval, \\(p\\)-value and \\(Z\\)-score, the following statements are equivalent:\n\n\n\n\n\n\n\nStatement\nCondition\n\n\n\n\n0 is inside the 95% confidence interval\n\\(p\\)-value is greater than 0.05\n\n\n\\(p\\)-value is greater than 0.05\n\\(Z\\)-statistic is less than 1.96\n\n\n\\(Z\\)-statistic is less than 1.96\n0 is inside the 95% confidence interval\n\n\n\n\nExample 5.3 (Stock market crash 1987 (Z-score)) Prior to the October, 1987 crash, SP500 monthly returns were 1.2% with a risk/volatility of 4.3%. The question is how extreme was the 1987 crash of \\(-21.76\\)%? \\[\nX \\sim N \\left(1.2, 4.3^2 \\right )\n\\] This probability distribution can be standardized to yield \\[\nZ =\\frac{X-\\mu}{\\sigma} = \\frac{X - 1.2}{4.3} \\sim N(0,1) .\n\\] Now, we calculate the observed \\(Z\\), given the outcome of the crash event \\[\nZ = \\frac{-0.2176 - 0.012}{0.043} = -5.27\n\\] That is a \\(5\\)-sigma event in terms of the distribution of \\(X\\). Meaning that -0.2176 is 5 standard deviations away from the mean. Under a normal model that is equivalent to \\(P(X &lt; -0.2176) = 4.6593\\times 10^{-8}\\).\n\n\n\nOther Applications\nHere we consider three examples of hypothesis testing in action: the famous “Coke vs Pepsi” challenge, the dispute over the drug Avonex, and safety statistics for Viagra.\n\nExample 5.4 (Coke vs Pepsi) The most famous hypothesis test in history in whether people can decide the difference between Coke and Pepsi. We run a double blind experiment, neither the experimenter or subject know the allocation. Pepsi claimed that more than half of Diet Coke drinkers said they preferred to drink Diet Pepsi. That is our null hypothesis. The data comes from a random sample of \\(100\\) drinkers. We find that \\(56\\) favor Pepsi.\nThis is a hypothesis test about the proportion of drinkers who prefer Pepsi \\[H_0 : p = \\frac{1}{2} \\; \\; \\mathrm{ and} \\; \\; H_1 : p &gt; \\frac{1}{2}\\] Let’s estimate our statistics form data: \\[\\hat{p} = X/n = 56/100 = 0.56\\]\nThis is my best estimate of the true \\(p\\). The standard error of my statistic \\[se(\\hat{p}) =  \\sqrt{\\hat{p}(1-\\hat{p})/n} = 0.0496 .\\]\nThe \\(95\\)% is then \\[0.56 \\pm 1.96 (0.0496) = 0.56 \\pm 0.098 = ( 0.463, 0.657 )\\] \\(p=0.5\\) lies inside the confidence interval. Pepsi was lying!\nThe \\(Z\\)-score now with \\(s_{ \\hat{p} }= \\sqrt{ p_0(1-p_0)/n} = 0.05\\) \\[Z = \\frac{ \\hat{p} - p_0 }{ s_{\\hat{p} }} = \\frac{ 0.56-0.5}{0.05} = 1.2 &lt; 1.64\\] Let’s take the usual \\(\\alpha = 0.05\\).\n\nprop.test(56,100,alternative='greater', conf.level = 0.95) %&gt;% fmt_prop_test()\n\n\n\n\nest\nstat\np\nparam\nlow\nhigh\nalt\n\n\n\n\n0.56\n1.2\n0.14\n1\n0.47\n1\ngreater\n\n\n\n\n\nDon’t reject \\(H_0\\) for a one-sided test at \\(5\\)% level (p-value: 0.14). We need a larger \\(n\\) to come to a more definitive conclusion. We might come to a different conclusion with a larger sample size. One of the downsides of hypothesis testing is that it generates a yes/no answer without having any uncertainty associated with it.\n\n\nExample 5.5 (Avonex) Biogen made the following assertion: “Avonex delivers the highest rate of satisfaction: 95% among patients” In response to that statement, the U.S. Food and Drug Administration (FDA) on October 30th, 2002 informed the biotech company Biogen to stop publishing misleading promotions for its multiple sclerosis drug Avonex. To clarify the issue, FDA did run an experiment. The FDA found that in a random sample of \\(75\\) patients surveyed, only 60% said they were satisfied with Avonex. The question is: Who is right?\nLet’s use hypothesis testing to get an answer. Following our five-step process to set up a Hypothesis Test: 1. Formulate the Null Hypothesis: \\(H_0 : p = 0.95 = p_0\\). The alternative hypothesis: \\(H_1 : p &lt; 0.95\\). A 1-sided alternative. 2. We’ll use a small significance level, 1%. 3. The appropriate test statistic is \\(Z = -14\\).\nTo complete our analysis, we need to determine the critical region and calculate the \\(p\\)-value for this test. For a one-sided test at the 1% significance level, the critical region is \\(Z &lt; -2.32\\). Since our observed test statistic of \\(Z = -14\\) falls well within this rejection region, we reject the null hypothesis.\nThis procedure can be implemented in R in the prop.test function.\n\nprop.test(45,75,0.95) %&gt;% fmt_prop_test()\n\n\n\n\nest\nstat\np\nparam\nlow\nhigh\nalt\n\n\n\n\n0.6\n186\n0\n1\n0.48\n0.71\ntwo.sided\n\n\n\n\n\nThe p-value is less than \\(2.2 \\times 10^{-16}\\), which is essentially zero. This extremely small \\(p\\)-value provides overwhelming statistical evidence that the FDA is correct and Biogen’s claim is false.\n\n\nExample 5.6 (Pfizer) Pfizer introduced Viagra in early 1998. During \\(1998\\) of the \\(6\\) million Viagra users \\(77\\) died from coronary problems such as heart attacks. Pfizer claimed that this rate is no more than the general population. A clinical study found \\(11\\) out of \\(1,500,000\\) men who were not on Viagra died of coronary problems during the same length of time as the \\(77\\) Viagra users who died in \\(1998\\). The question is whether the drug is safe. Let’s calculate the confidence interval. A 95% confidence interval for a difference in proportions \\(p_1 - p_2\\) is \\[\n( \\hat{p}_1 - \\hat{p}_2 ) \\pm 1.96\n\\sqrt{ \\frac{ \\hat{p}_1 ( 1 - \\hat{p}_1 ) }{ n_1 }  +\n    \\frac{ \\hat{p}_2 ( 1 - \\hat{p}_2 ) }{ n_2 } }\n\\]\nWe can do a confidence interval or a \\(Z\\)-score test. With Viagra, \\(\\hat{p}_1 = 77/6000000 = 0.00001283\\) and without Viagra \\(\\hat{p}_2 = 11/1500000 = 0.00000733\\). We need to test whether these are equal. With a \\(95\\)% confidence interval for \\((p_1 - p_2)\\) you get an interval \\(( 0.00000549 , 0.0000055).\\)\nSince this confidence interval does not contain zero, we have evidence that the proportion of deaths from coronary problems is significantly higher among Viagra users compared to the general population. Despite the small proportions involved, the measurement is highly accurate due to the large sample sizes in both groups. For hypothesis testing, we would use a one-sided test with a significance level of \\(\\alpha = 0.01\\) to test whether the proportion of deaths is higher in the Viagra group. The difference in proportions can be analyzed as follows:\n\nprop.test(x=c(11,77), n=c(1500000,6000000), alternative='greater',conf.level=.95) %&gt;% \nfmt_prop_test()\n\n\n\n\nstat\np\nparam\nlow\nhigh\nalt\n\n\n\n\n2.6\n0.95\n1\n0\n1\ngreater\n\n\n\n\n\nThe p-value for the Null is \\(1-0.948 =0.052\\). This p-value is greater than 0.01, so we cannot reject the null hypothesis.\n\n\nExample 5.7 (Lord Rayleigh’s Argon Discovery) Lord Rayleigh won the Nobel Prize for discovery of Argon. This discovery occurred when he noticed a small discrepancy between two sets of measurements on nitrogen gas that he had extracted from the air and one he had made in the lab.\nFirst, he removed all oxygen from a sample of air. He measured the density of the remaining gas in a fixed volume at constant temperature and pressure. Second, he prepared the same volume of pure nitrogen by the chemical decomposition of nitrous oxide (\\(N_2 O\\)) and nitric oxide \\(NO\\). Here’s the results. Table on the left shows the measurements. Boxplot on the right shows the distribution of the measurements.\nair = c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, NA) # Missing data point\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\nd = data.frame(\"Air\"=air,\"Chemical Decomposition\"=decomp)\nknitr::kable(d, booktabs = TRUE)\nboxplot(d, col = c(\"red\", \"green\"))\n\n\n\n\n\n\nAir\nChemical.Decomposition\n\n\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\n2.3\n2.3\n\n\nNA\n2.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough the averages are very close 2.31 vs 2.30, the small standard deviations (\\(1.43 \\times 10^{-4}\\) and \\(1.38 \\times 10^{-3}\\)) suggest that the measurements are very precise.\n\nt.test(air,decomp,var.equal=T)\n## \n##  Two Sample t-test\n## \n## data:  air and decomp\n## t = 20, df = 13, p-value = 0.00000000003\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  0.0095 0.0118\n## sample estimates:\n## mean of x mean of y \n##       2.3       2.3\n\nThe t-test confirms that with the t-statistic of 20, the difference is statistically significant. It is a 20-sigma event and we’ve found Argon!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#ab-testing-applications",
    "href": "05-ab.html#ab-testing-applications",
    "title": "5  A/B Testing",
    "section": "5.3 A/B Testing Applications",
    "text": "5.3 A/B Testing Applications\nWhile we have established the foundations of hypothesis testing, let’s look at specific applications in industry.\n\nExample 5.8 (SimCity) In the gaming industry, A/B testing is crucial for optimizing revenue. Electronic Arts is a company that makes video games. SimCity 5, one of EA’s most popular video games, sold 1.1 million copies in the first two weeks of its launch. 50% of sales were digital downloads, thanks to a strong A/B testing strategy designed to maximize the conversion rate of pre-orders.\nAs EA prepared to release the new version of SimCity, they released a promotional offer to drive more game pre-orders. The offer was displayed as a banner across the top of the pre-order page – front-and-center for shoppers. But according to the team, the promotion was not driving the increase in pre-orders they had expected.\nThey decided to test some other options to see what design or layout would drive more revenue.\n\n\n\n\n\n\n\n\nOriginal\n\n\n\n\n\n\n\nVariation4\n\n\n\n\n\n\nFigure 5.2: SimCity banner\n\n\n\nOne variation removed the promotional offer from the page altogether. The test led to some very surprising results: The variation with no offer messaging whatsoever drove 43.4% more purchases. Data revealed that customers preferred a direct purchase path, indicating that additional incentives were unnecessary.\nMost people believe that direct promotions drive purchases, but for EA, this turned out to be totally false. Testing gave them the information needed to maximize revenue in a way that would not have been otherwise possible.\nWe define the abtestfunc function to examine whether a black or pink background results in more purchases. Run experiment for one week:\n\nPink background: \\(40\\)% purchase rate with \\(500\\) visitors\nBlack background: \\(30\\)% purchase rate with \\(550\\) visitors\n\nLet’s run the AB test to see which is more effective, we will calculate the confidence intervals for conversion rates for each variation of site using the abtestfunc function.\nThe abtestfuncfunction below calculates CIs (80% significance, \\(Z=1.28\\)).\n\nsite1 = c(.40, 500) # pink\nsite2 = c(.30, 550) # black\nabtestfunc &lt;- function(ad1, ad2){\nsterror1 = sqrt( ad1[1] * (1-ad1[1]) / ad1[2] )\nsterror2 = sqrt( ad2[1] * (1-ad2[1]) / ad2[2] )\nminmax1 = c((ad1[1] - 1.28*sterror1) * 100, (ad1[1] + 1.28*sterror1) * 100)\nminmax2 = c((ad2[1] - 1.28*sterror2) * 100, (ad2[1] + 1.28*sterror2) * 100)\nprint( round(minmax1,2) )\nprint( round(minmax2,2) )\n}\nabtestfunc(site1, site2)\n## 37 43\n## 28 32\n\nThe confidence intervals are [37, 42] for the pink background and [28, 32] for the black background. Since these intervals do not overlap, we can conclude that the purchase rate for the pink background is significantly higher than for the black background at the 80% confidence level.\n\n\nExample 5.9 (Mythbusters) We now return to the Mythbusters yawning experiment to analyze it using the standard error of the difference in proportions.\nThe question is what happens if we are to re-run this experiment several times with different groups of participants, will we see the same difference of 4.4%? The fact is that from one experiment to another calculated proportions of yawners in both groups will be different.\nIn our example, the proportion of yawners in the experimental group is \\(\\hat p_e = 0.294\\) and in the control group is \\(\\hat p_c = 0.25\\). Thus, \\[\n\\hat \\sigma^2_e = 0.294(1-0.294) = 0.208,~~~\\hat \\sigma^2_c = 0.25(1-0.25) = 0.188\n\\]\nWe can apply CLT and calculate the uncertainty about \\(\\hat p_{e}\\) and \\(\\hat p_{c}\\) \\[\n\\hat p_e\\sim N(0.294, 0.208/34),~~~ \\hat p_c\\sim N(0.25, 0.188/16).\n\\] Now, instead of comparing proportions (numbers), we can compare their distributions and thus quantify uncertainties. If we plot density functions of those two Normal variables, we can see that although means are different, there is a large overlap of the two density functions.\n\np = seq(0.0,0.6, length.out = 200)\nplot(p,dnorm(p,0.25, sqrt(0.188/16)), col=2, type='l', lwd=3, ylab=\"Density\", ylim = c(0,5))\nlines(p,dnorm(p,0.294, sqrt(0.208/34)), col=3, lwd=3, ylim = c(0, 6))\nlegend(\"topright\", c(\"control\", \"experiment\"), col=c(3,2), lwd=3, bty='n')\n\n\n\n\n\n\n\n\nThe amount of overlap is the measure of how certain we are that \\(p_e\\) and \\(p_c\\) are different. Large overlap means we are not very certain if proportions are truly different. For example, both \\(p_e\\) and \\(p_c\\) have a high probability of being between 0.2 and 0.4. We can use properties of normal distribution to say specifically what is the amount of this overlap by calculating the corresponding 95% confidence interval of the difference between the proportions. Note that the difference of two Normal random variables is another Normal \\[\n\\hat p_e - \\hat p_c \\sim N(0.294 - 0.25, 0.208/34 + 0.188/16) = N(0.044, 0.0177)\n\\] Now we can calculate 95% confidence interval for \\(\\hat p_e - \\hat p_c\\), again using properties of Normal\n\n0.044 + c(-1.96,1.96)*sqrt(0.0177)\n## -0.22  0.30\n\nThe interval is wide and most importantly, it does contain 0. Thus, we cannot say for sure that the proportions are different. They might just appear to be different due to chance (sampling error). Meaning, that if we are to re-run the experiment we should expect the difference to be anywhere between -0.22 and 0.31 in 95% of the cases.\nThus, statistical analysis does not confirm the conclusion made by the show hosts and indicates that there is no evidence that the proportion of yawners is different between the control and experimental groups.\n\n\nExample 5.10 (Search algorithm) For tech giants, even small improvements in metrics can lead to massive gains. Let’s look at another example and test effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\nGoogle Search Algorithm\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1828\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nThe probability of success is estimated to be \\(\\hat{p}_1 = 1755/2500 = 0.702\\) for the current algorithm and \\(\\hat{p}_2 = 1828/2500 = 0.731\\) for the new algorithm. We can calculate the 95% confidence interval or 95% Bayesian credible region for both estimated proportions.\n\np1 = 1755/2500; p2 = 1828/2500\nAlgo1 = round(p1 + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500), 3)\nAlgo2 = round(p2 + c(-1.96,1.96)*sqrt(p2*(1-p2)/2500), 3)\nkable(rbind(Algo1, Algo2), col.names = c(\"Lower\", \"Upper\"), caption = \"95% Confidence Interval for the Proportions\")\n\n\n95% Confidence Interval for the Proportions\n\n\n\nLower\nUpper\n\n\n\n\nAlgo1\n0.68\n0.72\n\n\nAlgo2\n0.71\n0.75\n\n\n\n\n\nGiven that the intervals do slightly overlap, there is not enough evidence to say that algorithms are different, and the new Algo 2 is not necessarily more efficient.\nWe will get a slightly more precise estimation of uncertainty if we calculate confidence interval for the difference of the proportions. Since \\(p_1\\) and \\(p_2\\) both follow Normal distribution, their difference is also normally distributed \\[\np_1 - p_2 \\sim N(\\hat p_1 - \\hat p_2, s_1^2/n + s_2^2/n).\n\\] Applying this formula for the Google search algorithm experiment, we calculate the 95% confidence interval for the difference\n\ndiff = p1 - p2  + c(-1.96,1.96)*sqrt(p1*(1-p1)/2500 + p2*(1-p2)/2500)\nprint(diff, digits = 5)\n## -0.0541696 -0.0042304\n\nThe confidence interval for the difference does not contain 0, and thus we can say that we are confident that algorithms are different!\nMore generally, if the number of observations in two groups are different, say \\(n_1\\) and \\(n_2\\) then the \\[\ns_{ \\bar{X}_1 - \\bar{X}_2 } = \\sqrt{ \\frac{ s^2_{ \\bar{X}_1 }}{n_1} + \\frac{ s^2_{ \\bar{X}_2 }}{n_2} }\n\\] or for proportions, we compute \\[\ns_{ \\hat{p}_1 - \\hat{p}_2 } = \\sqrt{ \\frac{ \\hat{p}_1 (1- \\hat{p}_1)}{n_1} + \\frac{ \\hat{p}_2 (1- \\hat{p}_2)}{n_2} }.\n\\]\n\n\nExample 5.11 (Search Algorithm with Unequal Sample Sizes) Let’s consider a variation of the previous example where the sample sizes are different. Suppose we have the following data:\n\nGoogle Search Algorithm with Unequal Sample Sizes\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1824\n1867\n\n\nfailure\n776\n683\n\n\ntotal\n2600\n2550\n\n\n\nThe probability of success is estimated to be \\(\\hat{p}_1 = 1824/2600 = 0.702\\) for Algo1 and \\(\\hat{p}_2 = 1867/2550 = 0.732\\) for Algo2. Now we calculate the 95% confidence interval for the difference in proportions using the formula for unequal sample sizes:\n\np1 = 1824/2600; p2 = 1867/2550\ndiff = p1 - p2  + c(-1.96,1.96)*sqrt(p1*(1-p1)/2600 + p2*(1-p2)/2550)\nprint(diff, digits = 5)\n## -0.0552111 -0.0060257\n\nThe confidence interval for the difference does not contain 0, and thus we can say that we are confident that algorithms are different!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#challenges-in-ab-testing",
    "href": "05-ab.html#challenges-in-ab-testing",
    "title": "5  A/B Testing",
    "section": "5.4 Challenges in A/B Testing",
    "text": "5.4 Challenges in A/B Testing\n\nMultiple Testing\nConsider this simple multiple-testing scenario. If we want to test 1000 hypotheses and we test each hypothesis one-by-one, say the ground truth is that only 10% (100) of those hypotheses are true. Using \\(\\alpha=0.05\\) rule, we assume that out of 900 false hypotheses \\(0.05 \\cdot 900 = 45\\) will show up as positive (false positives). Now we run our one-by-one hypothesis tests and our procedure correctly identified 80 out of 100 true positives and incorrectly identified 45 false positives and 20 false negatives. Now, among 125 hypotheses identified as positives 45 in fact are not! Another way to look at it is to calculate the probability of at least one false positive \\(P(\\mbox{at least one false positive}) = 1 - (1-0.05)^{1000} = 1\\). We are almost guaranteed to see at least one false positive.\n\nplot(1:100,1 - (1-0.05)^{1:100}, type='l', ylab=\"False Positive Rate\", xlab=\"Number of Tests\", col=\"red\", lwd=2)\n\n\n\n\nProbability of At Least 1 False Positive\n\n\n\n\nOne way to deal with the problem is to lower the cut-off to \\(\\alpha/n\\). This approach is called the Bonferroni correction. For the case of 1000 hypotheses we set \\(\\alpha = 0.00005\\). However this conservative approach will lead to many false negatives. The probability of identifying at least one significant result is then \\(1 - (1-0.00005)^{1000} = 0.049\\)\n\nClassification of results for a testing procedure. T/F = True/False, D/N = Discovery/Non-discovery. We observe \\(m\\), \\(D\\) and \\(N\\).\n\n\n\n\\(H_0\\) Accepted\n\\(H_0\\) Rejected\nTotal\n\n\n\n\n\\(H_0\\) True\nTN\nFD\n\\(T_0\\)\n\n\n\\(H_0\\) False\nFN\nTD\n\\(T_1\\)\n\n\nTotal\n\\(N\\)\n\\(D\\)\n\\(m\\)\n\n\n\nA more practical approach is to use the False Discovery Rate \\[\n\\text{FDR} = \\E{\\frac{FD}{D}}\n\\] which is the proportion of false positives among all significant results. We aim to set a cutoff so that FDR \\(&lt; Q\\). The FDR approach allows us to increase the power while maintaining some principled bound on error.\nBenjamini and Hochberg developed a procedure based on FDR to perform multiple testing. Under their procedure, we put individual \\(p\\)-values in order from smallest to largest. Then we choose the largest \\(p_k\\) value that is smaller than \\((k/m)/Q\\) where \\(Q\\) is the false discovery rate you choose. Then all hypotheses with index \\(i&lt;k\\) are significant. Benjamini and Hochberg showed that under this procedure the FDR \\(&lt;Q\\).\nAs an example, García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women. They found the following results:\n\nd = read.csv(\"../data/cancer-diet.csv\")\nknitr::kable(d[1:5,], booktabs = TRUE)\n\n\nDietary Risk Factors of Cancer\n\n\nLabel\np.value\nRank\nBH\n\n\n\n\nTotal calories\n0.00\n1\n0.01\n\n\nOlive oil\n0.01\n2\n0.02\n\n\nWhole milk\n0.04\n3\n0.03\n\n\nWhite meat\n0.04\n4\n0.04\n\n\nProteins\n0.04\n5\n0.05\n\n\n\n\n\nIf we choose \\(Q = 0.25\\), then \\(k=5\\) (Proteins) is our cut-off rank. Thus we reject \\(H_0\\) for the first five tests. Note that traditional hypothesis testing procedure only controls for Type 1 error and FDR-based procedure controls for both error types.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "05-ab.html#sec-ab-rct",
    "href": "05-ab.html#sec-ab-rct",
    "title": "5  A/B Testing",
    "section": "5.5 Randomized experiments and Causality",
    "text": "5.5 Randomized experiments and Causality\nFlorence Nightingale (1820-1910), widely known for her role in the Crimean War, was also a pioneering statistician and a champion of evidence-based medicine. She can be considered the mother of observational studies. To her, every piece of legislation was an experiment in the laboratory of society deserving study and demanding evaluation. Nightingale recognized the importance of collecting accurate and reliable data to understand healthcare outcomes. She developed standardized methods for collecting data on hospital admissions, deaths, causes of death, and other relevant factors. This systematic data collection allowed for more rigorous and reliable analysis of healthcare practices and their impact on patient outcomes. During the Crimean War (1853-1856), she collected and analyzed data on mortality rates among soldiers. She created statistical diagrams, such as the famous polar area diagram or “coxcomb,” to illustrate the causes of mortality. These visual representations helped to convey complex information in a clear and understandable way. Nightingale’s observations and statistical analyses led her to emphasize the importance of sanitation and hygiene in healthcare settings. She advocated for improvements in cleanliness, ventilation, and sanitation in hospitals, recognizing the impact of these factors on the health and well-being of patients. Beyond the battlefield, Nightingale continued her work in public health. She used statistical evidence to advocate for healthcare reforms and improvements in public health infrastructure. Her efforts played a crucial role in shaping public health policies and practices.\nThe work of Nightingale would nowadays be classified as an observational study. An observational study is a research design where researchers observe and collect data on existing groups of people or phenomena without intervening or manipulating any variables. Unlike randomized controlled trials, researchers do not assign participants to different groups and do not directly influence the outcome.\nIn contrast, George Washington (1732-1799) advanced agriculture through rigorous experimentation, employing methods remarkably similar to modern controlled experiments. He was deeply interested in improving agricultural techniques and conducted numerous experiments at his Mount Vernon estate. One of his most notable experiments involved dividing his land into plots and testing different crop rotations and fertilization methods. Washington recognized the importance of sustainable agriculture and the detrimental effects of monoculture (growing the same crop year after year) on soil fertility. He observed how tobacco, his primary cash crop at the time, depleted the soil nutrients, leading to diminishing yields. To address this issue and improve the long-term health of his land, he began experimenting with crop rotation and soil management techniques.\nWashington divided his land into several plots, each receiving different treatments. He experimented with various crop rotations, including wheat-fallow, wheat-rye-fallow, and corn-wheat-fallow. These rotations aimed to prevent soil depletion and promote its natural restoration by planting nitrogen-fixing crops like rye and clover. He also tested different fertilizer applications on various plots. He used manure, compost, and even imported materials like gypsum and marl to improve soil fertility and crop yields.\nWashington meticulously documented his experiments in his agricultural diaries. He recorded planting dates, yields, weather conditions, and observations on crop growth and soil health. This meticulous record-keeping allowed him to analyze the effectiveness of different treatments and compare their impact on crop yields and soil quality.\nWashington’s experiments yielded valuable insights into sustainable agricultural practices. He discovered that crop rotation and fertilization improved soil health and increased crop yields over time. He abandoned tobacco as his primary crop and shifted towards wheat, which was less soil-depleting and offered a more stable income source.\nThe historic trades staff at Mount Vernon have recreated Washington’s experiment at the Pioneer Farm, using the same plot layout, crops, and fertilization methods described in his diaries. This allows visitors to learn about his innovative farming techniques and their impact on the land. Figure 5.3 shows the plot layout at the Pioneer Farm.\n\n\n\n\n\n\n\nFigure 5.3: Plot layout at the Mount Vernon’s Pioneer Farm\n\n\n\nGeorge Washington’s commitment to experimentation and innovation made him a pioneer in American agriculture. His plot-based experiments demonstrated the effectiveness of crop rotation and soil management in promoting sustainable farming practices. His work continues to inspire farmers today and serves as a valuable resource for understanding agricultural history and best practices.\nLater, at the turn of the 20th century, Ronald Fisher (1890-1962) developed the theory of experimental design which allowed for controlled experiments, known as randomized controlled trials (RCT). Fisher’s work laid the foundation for modern experimental design and analysis, providing a rigorous statistical framework for conducting randomized controlled trials. His contributions to experimental design and ANOVA were crucial in establishing the importance of randomized trials in research. He emphasized the importance of randomization and control groups in experimental design, recognizing their crucial role in establishing causal relationships.\n\nThe question of causation\nRandomized controlled trials (RCTs) and field experiments are widely considered the gold standard for establishing causation because they allow researchers to isolate the effect of a specific intervention or treatment from other confounding factors. The main principle of RCTs and field experiments is randomization, which ensures that the treatment and control groups are similar in all respects except for the treatment. This allows researchers to attribute any differences in outcomes between the two groups to the treatment, rather than to other factors.\nRandomization helps to control for confounding variables, which are factors that are associated with both the treatment and the outcome variable. By randomly assigning participants to groups, researchers can ensure that any confounding variables are evenly distributed between the groups. The control group serves as a baseline for comparison. It is a group that is not exposed to the treatment or intervention being studied. By comparing the outcomes of the treatment group and the control group, researchers can isolate the effect of the treatment. Any differences in the outcomes between the two groups can be attributed to the treatment.\nFrom a Bayesian modeling perspective, randomization is a practical route to approximate exchangeability between groups, making comparisons less sensitive to unobserved confounding (Chapter 3).\nThe modern randomized controlled trial (RCT) in medicine is most often attributed to Sir Austin Bradford Hill. In 1948, Hill published a landmark paper titled “Streptomycin Treatment of Pulmonary Tuberculosis” in the British Medical Journal, which described the first fully randomized, double-blind clinical trial. This study is considered a turning point in the history of medical research and established the RCT as the gold standard for evaluating the effectiveness of medical treatments.\nRandomized trials and observational studies are two distinct approaches to gathering and analyzing data in research studies. Here’s a breakdown of their key differences:\n\n\n\n\nRandomized trials\n\nDefinition: Participants are randomly assigned to different groups, with one group receiving the intervention being studied and the other group receiving a control intervention or placebo.\nPurpose: Determine whether the intervention causes the observed outcome by controlling for other factors that might influence the results.\nStrengths: High internal validity, strong causal inference due to randomization, allows for isolating the effect of the intervention, minimizes selection bias through random assignment.\nWeaknesses: Can be expensive and time-consuming to conduct, may not be ethical or feasible for all interventions, may not be generalizable to real-world settings.\n\n\n\n\n\nObservational data\n\nDefinition: Data is collected on existing groups of people without any intervention being implemented. Researchers observe and analyze the data to identify relationships between variables.\nPurpose: Explore potential associations between variables, generate hypotheses for further research, and investigate the natural course of a phenomenon.\nStrengths: Often less expensive and time-consuming, can provide insights into real-world settings, can investigate interventions that are not ethically feasible to test in randomized trials.\nWeaknesses: Lower internal validity, susceptibility to confounding variables, cannot establish causal relationships conclusively.\n\n\n\n\n\nIf you happen to have a choice between randomized trials and observational data (often you do not have that choice), which one should you choose? Consider the following:\n\nResearch question: If the research question aims to establish causation, a randomized trial is generally preferred. However, if the goal is to explore associations or generate hypotheses, observational data may be sufficient.\nAvailable resources: Randomized trials require significant resources, while observational studies can be less expensive and time-consuming.\nEthical considerations: Randomizing individuals to certain interventions may be unethical, making observational data the only option.\nGeneralizability: Randomized trials often involve carefully controlled environments, which may limit their generalizability to real-world settings. Observational data can provide insights into how interventions work in real-world situations.\n\nUltimately, both randomized trials and observational data play crucial roles in research. Combining these two approaches provides a more comprehensive understanding of the relationship between interventions and outcomes.\n\nExample 5.12 (Russian election fraud: a field experiment) Enikolopov et al. (2013) show how a field experiment can be used to estimate electoral fraud in Russian parliamentary elections held on December 4, 2011. They randomly assigned independent observers to 156 of 3,164 polling stations in the city of Moscow. The observers were trained by the nongovernmental organization Citizen Observer. The authors compared the vote shares of the incumbent United Russia party at polling stations with and without observers. They found that the presence of observers decreased the reported vote share of United Russia by almost 11 percentage points. This suggests that the extent of the fraud was sufficient to have changed the outcome of the elections.\n\n\nCode\nd = read.csv(\"../data//PNAS_data_2011.csv\")\n\nd %&gt;% ggplot(aes(x=er_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \nd %&gt;% ggplot(aes(x=turnout_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \nd %&gt;% ggplot(aes(x=apple_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \nd %&gt;% ggplot(aes(x=sr_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \nd %&gt;% ggplot(aes(x=ldpr_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \nd %&gt;% ggplot(aes(x=com_share, fill=as.factor(s))) + \n  geom_density(alpha=0.6, color=\"#e9ecef\") + \n  labs(fill=\"\") + \n  xlab(\"beta\") + \n  scale_fill_manual(values=c(\"#69b3a2\", \"#404080\")) \n\n\n\n\n\n\n\n\n\n\n\n\n(a) United Russia Share\n\n\n\n\n\n\n\n\n\n\n\n(b) Turnout Share\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Yabloko Share\n\n\n\n\n\n\n\n\n\n\n\n(d) Just Russia Share\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) LDPR Share\n\n\n\n\n\n\n\n\n\n\n\n(f) Communists Share\n\n\n\n\n\n\n\nFigure 5.4: Histogram comparison of the share of votes received by different parties and the share of those eligible voters who actually voted (turnout)\n\n\n\n\n\n\nCode\ndl = d %&gt;% dplyr::select(ends_with(\"_share\"),s,uik)\ndl = dl %&gt;% pivot_longer(cols = c(er_share,apple_share,turnout_share,sr_share,ldpr_share,com_share),names_to = \"party\",values_to = \"share\")\ndf2 = dl %&gt;% group_by(party,s) %&gt;% summarise(mu = mean(share),sd = sd(share)/sqrt(n()))\nggplot(df2, aes(x=party, y=mu, fill=as.factor(s)))+\n  geom_bar(stat=\"identity\", color = \"#e9ecef\", position=position_dodge()) +\n  geom_errorbar(aes(ymin=mu-2.58*sd, ymax=mu+2.58*sd), width=.2,position=position_dodge(.9)) + \n  scale_fill_manual(values = c(\"#69b3a2\", \"#404080\"),name = \"\", labels = c(\"Control group\", \"Treatment Group\")) +\n  scale_x_discrete(labels=c(\"Yabloko\", \"Communist\", \"United Russia\", \"LDPR\", \"Just Russia\", \"Turnout\")) +\n  ylab(\"Mean Share\") + xlab(\"\") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  theme(legend.position = \"right\", legend.margin = margin(r = -5), plot.margin = margin(b = -10))\n\n\n\n\n\n\n\n\nFigure 5.5: Bar plot comparison of the share of votes received by different parties and the share of those eligible voters who actually voted (turnout)\n\n\n\n\n\nFigure 5.4 and Figure 5.5 show the results of the experiments and plot histograms of the vote shares. The first histogram compares the share of the ruling United Russia party at the polling stations without observers (treatment = 0) and with observers (treatment = 1). On average, United Russia vote share is decreased by 11 percent when observers were present. The calculations made by Enikolopov et al. (2013) showed that this amount of manipulation was enough to preserve the majority of United Russia in the parliament; it would have lost it without manipulations. While “adding” votes for UR, the results indicate that all other parties were hurt by electoral fraud. The Liberal Democratic Party of Russia (LDPR) was hurt the least and is believed to be the most loyal to the ruling party.\n\n\nExample 5.13 (Pollution in India) Randomized Controlled Trials (RCTs) have revolutionized economic research and policy-making by providing a rigorous methodology to establish causal relationships between interventions and outcomes. The 2019 Nobel Prize in Economics awarded to Esther Duflo, Abhijit Banerjee, and Michael Kremer recognized their pioneering work in applying experimental approaches to alleviating global poverty and transforming development economics. Their experimental approach has fundamentally changed how economists tackle complex social problems by breaking them down into smaller, more manageable questions that can be answered through carefully designed experiments.\nThe paper “Truth-telling by Third-party Auditors and the Response of Polluting Firms: Experimental Evidence from India” by Duflo, Greenstone, Pande, and Ryan exemplifies this experimental approach. This two-year field experiment conducted in Gujarat, India, examined how altering the market structure for environmental audits could improve the accuracy of pollution reporting and ultimately reduce industrial pollution. The study demonstrates how RCTs can identify causal mechanisms in complex regulatory environments and provide evidence for effective policy reforms.\nThe researchers randomly assigned 473 industrial plants to either a treatment or control group. In the treatment group, they implemented a package of reforms to the environmental audit system:\n\n\n\n\n\n\n\nReform\nDescription\n\n\n\n\nRandom Assignment\nAuditors were randomly assigned to plants (rather than plants choosing their auditors)\n\n\nCentral Payment\nAuditors were paid from a central pool at a fixed rate\n\n\nRandom Backchecks\nIndependent technical agencies conducted random backchecks of auditor reports\n\n\nIncentive Pay\nIn the second year, incentive pay was provided for accurate reporting\n\n\n\nIn the control group, auditors were paid by the plants they audited. This created a blatant conflict of interest: plants could simply hire auditors who would provide favorable reports, leading to systematic misreporting of pollution levels. In contrast, the treatment group broke this client-auditor relationship by paying auditors from a central pool and independently verifying their work. The random backchecks served as a robust deterrent against misreporting, while the incentive pay further encouraged honesty.\nFurther, researchers performed backchecks, which are independent verification. Backchecks are follow-up visits conducted by independent technical agencies to verify the accuracy of pollution readings reported by third-party auditors. They serve as a quality control mechanism to monitor whether auditors are truthfully reporting actual pollution levels or manipulating data to show false compliance with regulatory standards.\nThe figure below (copied from the original paper) displays the distribution of Suspended Particulate Matter (SPM) concentrations measured in boiler-stack samples during the midline survey. Left plot presents the distributions of readings from both audits and backchecks at control plants, while right plot presents the corresponding distributions for treatment plants. A vertical line indicates the regulatory maximum concentration limit of 150 mg/N m3 for SPM, with the region between 75% and 100% of this limit highlighted in gray shading.\n\n\n\n\n\n\nControl plants\n\n\n\n\n\n\n\nTreatment plants\n\n\n\n\n\nThis figure clearly demonstrates how the RCT revealed systematic misreporting in the status quo audit system and how the treatment intervention improved reporting accuracy. The stark difference between audit reports and backcheck readings in the control group (Panel A) provides visual evidence of corruption that would have been difficult to establish through observational methods alone.\nThe experiment yielded three main results. First, regarding status quo corruption, under the existing system, auditors systematically reported plant emissions just below the regulatory standard, even though true emissions were typically higher. Second, the treatment improved reporting accuracy, causing auditors to report pollution levels more truthfully, with treatment auditors reporting pollution readings 50-70% higher than control auditors. Third, plants in the treatment group reduced their actual emissions by 0.21 standard deviations, with reductions concentrated among the highest-polluting plants.\nThe work of Duflo, Banerjee, Kremer, and their collaborators has fundamentally changed how economists approach questions of causality and policy effectiveness. By adapting experimental methods from medical research to address economic and social questions, they have created a powerful toolkit for identifying effective interventions to address poverty and other global challenges.\nThe Gujarat environmental audit experiment exemplifies how RCTs can uncover hidden mechanisms—in this case, corruption in regulatory reporting—and test solutions that might not have been evident from observational data alone. The study’s findings demonstrate that reformed incentives for third-party auditors can improve their reporting and make regulation more effective, with tangible benefits for environmental quality.\nAs RCTs continue to evolve and spread across different domains of economics, they promise to further strengthen the evidence base for policy decisions, ultimately leading to more effective interventions and better outcomes for society.\n\n\n\nSplit-plot designs and Rothamsted\nWhile Fisher championed randomization, practical constraints in agriculture often made complete randomization difficult. At the Rothamsted Experimental Station, where Fisher worked, researchers frequently encountered situations where some factors were difficult or expensive to randomize across small individual plots, while others were easy.\nFor example, consider an experiment testing the effects of two factors: irrigation methods (Factor A) and fertilizer types (Factor B). Applying different irrigation methods (e.g., flood vs. drip) typically requires large equipment and infrastructure, making it impractical to switch methods for every small plot of land. In contrast, changing fertilizer types is relatively easy and can be done by hand on smaller patches.\nIf we were to use a completely randomized design, we would have to randomly assign irrigation and fertilizer combinations to every small plot, which might require an impossible amount of plumbing/setup changes. Fisher and his colleagues formalized the split-plot design to handle this.\nIn a split-plot design, the experimental units are nested: 1. Whole Plots: Large areas of land to which the “hard-to-change” factor (e.g., irrigation) is applied. 2. Subplots: Smaller divisions within each whole plot to which the “easy-to-change” factor (e.g., fertilizer) is applied.\nThis design is a compromise. By restricting randomization of the main factor to larger blocks, we sacrifice some precision in estimating the main effect of Factor A (irrigation). However, we gain higher precision in estimating the effect of Factor B (fertilizer) and the interaction between A and B, because comparisons between fertilizers are made within the same whole plot, holding the irrigation method constant.\n\n\n\nSplit-plot design\n\n\nThis hierarchical structure is not limited to agriculture. In modern industrial and medical experiments, we often see similar structures. For instance, in a hospital trial, a new sanitation protocol (Factor A) might be applied to entire hospital wings (Whole Plots) because it’s hard to have different protocols for adjacent rooms, while individual patients within those wings (Subplots) might be randomized to receive different medications (Factor B). Understanding these constraints is crucial ensuring that the statistical analysis accounts for the correlations among subplots within the same whole plot; ignoring this nesting leads to underestimated standard errors and overconfident conclusions.\n\n\nDigital experimentation (A/B testing)\nFor many years, the main areas of application of randomized trials were medicine and agriculture. However, with the rise of the internet, randomized trials have become increasingly popular for testing the effectiveness of online interventions, such as email campaigns, website designs, and social media ads. When applied to user experience and marketing, randomized trials are often called A/B tests. The idea of A/B testing is the same as in traditional RCTs: randomly assign users to different versions of a website or an email campaign and compare the outcomes.\nHowever, digital A/B tests often operate with fewer constraints on randomization protocols and sample selection than clinical or agricultural trials. There are fewer strict rules about ethics, randomization, sample size, and statistical analysis. For example, randomization is sometimes completely ignored in A/B testing. Instead of assigning users randomly to groups, they are divided into groups based on factors like time of day, location, or browsing history. This can introduce bias into the results, as the groups may not be comparable. As a result, A/B testing is cheap and quick to conduct, as it can be done online without the need for IRB approval or recruitment of participants. A/B testing is primarily focused on measuring the comparative performance of variations without necessarily establishing a causal relationship (e.g., “Which version of our web page leads to more clicks?”).\n\n\nThe era of observational data\nYet, not all digital data comes from structured experiments. We rely heavily on observational data streams. Diane Lambert was a pioneer in promoting proper statistical methods for observational data in the tech industry. She highlighted how to detect selection bias in data streams drawn from transaction logs and ad systems, and how to use diagnostics to judge when confounding bias is severe enough to overwhelm the causal signal. She advocated for practical fixes such as propensity-score weighting and simulation to ensure that credible causal conclusions can still be drawn from field data.\nThe advent of digital data has fundamentally transformed the practice of statistics, shifting the field from a discipline focused on small, carefully collected samples to one that must grapple with massive, often messy datasets generated as byproducts of digital systems. In the pre-digital era, statisticians worked primarily with structured, purposefully collected data through surveys, experiments, and clinical trials. Today, organizations routinely collect terabytes of data from web traffic, sensor networks, financial transactions, and social media interactions. This shift has necessitated new statistical approaches that can handle high-dimensional data and complex dependencies. Machine learning algorithms have become essential tools for extracting patterns from these vast datasets. However, this transition has also introduced new challenges: the need to distinguish correlation from causation in observational data, the importance of addressing selection bias in non-random samples, and the ethical considerations of privacy and algorithmic fairness.\n\n\nFrom experiments to observational data\nWhile randomized controlled trials remain the gold standard for establishing causality, conducting them is not always feasible. In many real-world scenarios—especially in policy, economics, and social science—we cannot ethically or practically assign treatments randomly. We are often left with observational data, where the “treatment” (e.g., a job training program, a medical procedure, or a new law) was assigned based on complex, often unobserved, human decisions. This brings us back to the fundamental problem of causal inference: how do we estimate the effect of a treatment when the treated and control groups are systematically different? To solve this, we need rigorous statistical frameworks that can adjust for these differences and impute the missing counterfactual outcomes.\nTree-based Bayesian methods are one modern approach to this problem. In particular, BART-based causal inference is developed in the tree chapter; see Section 14.5.\n\n\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and Alexei Zakharov. 2013. “Field Experiment Estimate of Electoral Fraud in Russian Parliamentary Elections.” Proceedings of the National Academy of Sciences 110 (2): 448–52.\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope, Pilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al. 2014. “Calorie Intake, Olive Oil Consumption and Mammographic Density Among Spanish Women.” International Journal of Cancer 134 (8): 1916–25.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A/B Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html",
    "href": "06-hyp.html",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "6.1 Likelihood Principle\nThe hypothesis testing problem is as follows. Based on a sample of data, \\(y\\), generated from \\(p\\left( y \\mid \\theta\\right)\\) for \\(\\theta\\in\\Theta\\), the goal is to determine if \\(\\theta\\) lies in \\(\\Theta_{0}\\) or in \\(\\Theta_{1}\\), two disjoint subsets of \\(\\Theta\\). In general, the hypothesis testing problem involves an action: accepting or rejecting a hypothesis. The problem is described in terms of a null, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), which are defined as \\[\nH_{0}:\\theta\\in\\Theta_{0}\\;\\;\\mathrm{and}\\;\\;H_{1}%\n:\\theta\\in\\Theta_{1}\\text{.}%\n\\]\nAs a scope note, we will be explicit about notation: in this chapter \\(\\alpha\\) and \\(\\beta\\) refer to Type I and Type II error probabilities in the classical sense, defined through repeated-sampling performance of a decision rule. When we discuss Bayesian evidence, we instead emphasize posterior probabilities and Bayes factors, and when we talk about posterior uncertainty we use credible intervals and posterior summaries without reusing \\(\\alpha\\) as a credibility-level parameter.\nDifferent types of regions generate different types of hypothesis tests. If the null hypothesis assumes that \\(\\Theta_{0}\\) is a single point, \\(\\Theta _{0}=\\theta_{0}\\), this is known as a simple or “sharp” null hypothesis. If the region consists of multiple points, the hypothesis is called composite; this occurs when the space is unconstrained or corresponds to an interval of the real line. In the case of a single parameter, typical one-sided tests are of the form \\(H_{0}:\\theta&lt;\\theta_{0}\\) and \\(H_{1}:\\theta&gt;\\theta_{0}\\).\nThere are two correct decisions and two possible types of errors. The correct decisions are accepting a null or an alternative that is true, whereas a Type I error incorrectly rejects a true null and a Type II error incorrectly accepts a false null.\nFormally, the probabilities of Type I (\\(\\alpha\\)) and Type II (\\(\\beta\\)) errors are defined as: \\[\n\\alpha=P \\left[  \\text{reject }H_{0} \\mid H_{0}\\text{\nis true }\\right]  \\text{ and }\\beta=P \\left[  \\text{accept\n}H_{0} \\mid H_{1}\\text{ is true }\\right]  \\text{.}%\n\\]\nIt is useful to think of the decision to accept or reject as a decision rule, \\(d\\left( y\\right)\\). In many cases, the decision rules form a critical region \\(R\\), such that \\(d\\left( y\\right) =d_{1}\\) if \\(y\\in R\\). These regions often take the form of simple inequalities. Next, defining the decision to accept the null as \\(d\\left( y\\right) =d_{0}\\), and the decision to accept the alternative as \\(d_{1},\\) the error types are \\[\\begin{align*}\n\\alpha_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{1} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{0}\\text{ }(H_{0}\\text{ is true})\\\\\n\\beta_{\\theta}\\left(  d\\right)   &  =P \\left[  d\\left(  y\\right)\n=d_{0} \\mid \\theta\\right]  \\text{ if }\\theta\\in\\Theta_{1}\\text{ }(H_{1}\\text{ is true})\\text{.}%\n\\end{align*}\\] where both types of errors explicitly depend on the decision and the true parameter value. Notice that both of these quantities are determined by the population properties of the data. In the case of a composite null hypothesis, the size of the test (the probability of making a type I error) is defined as \\[\n\\alpha = \\underset{\\theta\\in\\Theta_{0}}{\\sup}~\\alpha_{\\theta}\\left( d\\right)\n\\] The supremum (\\(\\sup\\)) is the least upper bound of a set. For finite sets, \\(\\sup=\\max\\). For a standard reference, see Billingsley (1995). and the power is defined as \\(1-\\beta_{\\theta}\\left( d\\right)\\). It is always possible to set either \\(\\alpha_{\\theta}\\left( d\\right)\\) or \\(\\beta_{\\theta }\\left( d\\right)\\) equal to zero, by finding a test that always rejects the alternative or null, respectively.\nThe total probability of making an error is \\(\\alpha_{\\theta}\\left(d\\right) +\\beta_{\\theta}\\left(d\\right)\\), and ideally one would seek to minimize the total error probability, absent additional information. The optimal action \\(d^*\\) minimizes the posterior expected loss; \\(d^* = d_0 = 0\\) if the posterior probability of hypothesis \\(H_0\\) exceeds 1/2, and \\(d^* = d_1=1\\) otherwise \\[\nd^* = 1\\left(  P \\left(  \\theta \\in \\Theta_0 \\mid y\\right) &lt; P \\left(  \\theta \\in \\Theta_1 \\mid y\\right)\\right)  = 1\\left(P \\left(  \\theta \\in \\Theta_0 \\mid y\\right)&lt;1/2\\right).\n\\] Formally, a decision rule selects the hypothesis with higher posterior probability.\nThe easiest way to reduce the error probability is to gather more data, as the additional evidence should lead to more accurate decisions. In some cases, it is easy to characterize optimal tests, those that minimize the sum of the errors. Simple hypothesis tests of the form \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta=\\theta_{1}\\), are one such case admitting optimal tests. Defining \\(d^{\\ast}\\) as a test accepting \\(H_{0}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &gt;a_{1}f\\left( y \\mid \\theta_{1}\\right)\\) and \\(H_{1}\\) if \\(a_{0}f\\left( y \\mid \\theta_{0}\\right) &lt;a_{1}f\\left( y \\mid \\theta _{1}\\right)\\), for some \\(a_{0}\\) and \\(a_{1}\\). Either \\(H_{0}\\) or \\(H_{1}\\) can be accepted if \\(a_{0}f\\left(y \\mid \\theta_{0}\\right) =a_{1}f\\left( y \\mid \\theta_{1}\\right)\\). Then, for any other test \\(d\\), it is not hard to show that \\[\na_{0}\\alpha\\left(  d^{\\ast}\\right)  +a_{1}\\beta\\left(  d^{\\ast}\\right)  \\leq\na_{0}\\alpha\\left(  d\\right)  +a_{1}\\beta\\left(  d\\right),\n\\] where \\(\\alpha_{d}=\\alpha_{d}\\left( \\theta\\right)\\) and \\(\\beta_{d}=\\beta_{d}\\left( \\theta\\right)\\). This result highlights the optimality of tests defining rejection regions in terms of the likelihood ratio statistic, \\(f\\left( y \\mid \\theta_{0}\\right)/f\\left( y \\mid \\theta_{1}\\right)\\). It turns out that the results are in fact stronger. In terms of decision theoretic properties, tests that define rejection regions based on likelihood ratios are not only admissible decisions, but form a minimal complete class, the strongest property possible.\nOne of the main problems in hypothesis testing is that there is often a tradeoff between the two goals of reducing type I and type II errors: decreasing \\(\\alpha\\) leads to an increase in \\(\\beta\\), and vice-versa. Because of this, it is common to fix \\(\\alpha_{\\theta}\\left( d\\right)\\), or \\(\\sup~\\alpha_{\\theta}\\left( d\\right)\\), and then find a test to minimize \\(\\beta_{d}\\left( \\theta\\right)\\). This leads to “most powerful” tests. There is an important result from decision theory: test procedures that use the same size level of \\(\\alpha\\) in problems with different sample sizes are inadmissible. This is commonly done where significance is indicated by a fixed size, say 5%. The implications of this will be clearer below in examples.\nGiven observed data \\(y\\) and likelihood function \\(l(\\theta) = p(y\\mid \\theta)\\), the likelihood principle states that all relevant experimental information is contained in the likelihood function for the observed \\(y\\). Furthermore, two likelihood functions contain the same information about \\(\\theta\\) if they are proportional to each other. For example, the widely used maximum-likelihood estimation does satisfy the likelihood principle. However, frequentist hypothesis testing procedures often violate this principle. The likelihood principle is a fundamental principle in statistical inference, and it is a key reason why Bayesian procedures are often preferred.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#likelihood-principle",
    "href": "06-hyp.html#likelihood-principle",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "",
    "text": "NoteDecision Theoretic Concepts\n\n\n\n\nAdmissibility: A decision rule \\(\\delta\\) is admissible if there exists no other rule \\(\\delta^{\\prime}\\) such that \\(R\\left( \\theta,\\delta^{\\prime}\\right) \\leq R\\left( \\theta,\\delta\\right)\\) for all \\(\\theta\\), with strict inequality for at least one \\(\\theta\\). In other words, an admissible rule cannot be uniformly improved upon.\nComplete Class: A class of rules \\(\\mathcal{C}\\) is essentially complete if for any rule \\(\\delta \\notin \\mathcal{C}\\), there exists a rule \\(\\delta^{\\prime} \\in \\mathcal{C}\\) that dominates it. A minimal complete class is the smallest such set of rules containing all admissible rules.\n\n\n\n\nExample 6.1 (Testing fairness) Suppose we are interested in testing \\(\\theta\\), the unknown probability of heads for a possibly biased coin. Suppose, \\[\nH_0 :~\\theta=1/2 \\quad\\text{v.s.} \\quad  H_1 :~\\theta&gt;1/2.\n\\] An experiment is conducted and 9 heads and 3 tails are observed. This information is not sufficient to fully specify the model \\(p(y\\mid \\theta)\\). There are two approaches.\nScenario 1: Number of flips, \\(n = 12\\) is predetermined. Then number of heads \\(Y \\mid \\theta\\) is binomial \\(B(n, \\theta)\\), with probability mass function \\[\np(y\\mid \\theta)= {n \\choose y} \\theta^{y}(1-\\theta)^{n-y} = 220 \\cdot \\theta^9(1-\\theta)^3\n\\] For a frequentist, the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{12} {12 \\choose y} (1/2)^y(1-1/2)^{12-y} = (1+12+66+220)/2^{12} =0.073,\n\\] and if you recall the classical testing, \\(H_0\\) is not rejected at level \\(\\alpha = 0.05\\).\nScenario 2: The number of tails (successes) \\(\\alpha = 3\\) is predetermined; that is, flipping continues until 3 tails are observed. Then, \\(Y\\), the number of heads (failures) observed until 3 tails appear, follows a Negative Binomial distribution \\(NB(3, 1- \\theta)\\), \\[\np(y\\mid \\theta)= {\\alpha+y-1 \\choose \\alpha-1} \\theta^{y}(1-\\theta)^{\\alpha} = {3+9-1 \\choose 3-1} \\theta^9(1-\\theta)^3 = 55\\cdot \\theta^9(1-\\theta)^3.\n\\] For a frequentist, large values of \\(Y\\) are critical and the p-value of the test is \\[\nP(Y \\geq 9\\mid H_0)=\\sum_{y=9}^{\\infty} {3+y-1 \\choose 2} (1/2)^{y}(1/2)^{3} = 0.0327.\n\\] We used the following identity here \\[\n\\sum_{x=k}^{\\infty} {2+x \\choose 2}\\dfrac{1}{2^x} = \\dfrac{8+5k+k^2}{2^k}.\n\\] The hypothesis \\(H_0\\) is rejected, and this change in decision is not caused by observations.\nAccording to the Likelihood Principle, all relevant information is in the likelihood \\(l(\\theta) \\propto \\theta^9(1 - \\theta)^3\\), and Bayesians could not agree more!\nEdwards, Lindman, and Savage (1963, 193) note: The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#the-bayesian-approach",
    "href": "06-hyp.html#the-bayesian-approach",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.2 The Bayesian Approach",
    "text": "6.2 The Bayesian Approach\nFormally, the Bayesian approach to hypothesis testing is a special case of the model comparison results to be discussed later. The Bayesian approach just computes the posterior distribution of each hypothesis. By Bayes \\[\nP \\left(  H_{i} \\mid y\\right)  =\\frac{p\\left(  y \\mid H_{i}\\right)  P \\left(  H_{i}\\right)  }{p\\left(  y\\right)}    , ~\\text{for} ~ i=0,1\n\\] where \\(P \\left( H_{i}\\right)\\) is the prior probability of \\(H_{i}\\), \\[\np\\left( y \\mid H_{i}\\right) =\\int_{\\theta \\in \\Theta_i} p\\left( y \\mid \\theta\\right) p\\left( \\theta \\mid H_{i}\\right) d\\theta\n\\] is the marginal likelihood under \\(H_{i}\\), \\(p\\left( \\theta \\mid H_{i}\\right)\\) is the parameter prior under \\(H_{i}\\), and \\[\np\\left(  y\\right)  = \\sum_{i=0,1} p\\left(  y \\mid H_{i}\\right)  P \\left( H_{i}\\right).\n\\]\nIf the hypotheses are mutually exclusive, \\(P \\left( H_{0}\\right) =1-P \\left( H_{1}\\right)\\).\nThe posterior odds of the null to the alternative is \\[\n\\text{Odds}_{0,1}=\\frac{P \\left(  H_{0} \\mid y\\right)  }{P %\n\\left(  H_{1} \\mid y\\right)  }=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{p\\left(  y \\mid H_{1}\\right)  }\\frac{P \\left(  H_{0}\\right)  }{P \\left(  H_{1}\\right)  }\\text{.}%\n\\]\nThe odds ratio updates the prior odds, \\(P \\left( H_{0}\\right) /P \\left( H_{1}\\right)\\), using the Bayes Factor, \\[\n\\mathrm{BF}_{0,1}=\\dfrac{p\\left(y \\mid H_{0}\\right)}{p\\left( y \\mid H_{1}\\right)}.\n\\] With exhaustive competing hypotheses, \\(P \\left( H_{0} \\mid y\\right)\\) simplifies to \\[\nP \\left(  H_{0} \\mid y\\right)  =\\left(  1+\\left(  \\mathrm{BF}_{0,1}\\right)  ^{-1}\\frac{\\left(  1-P \\left(  H_{0}\\right)\n\\right)  }{P \\left(  H_{0}\\right)  }\\right)  ^{-1}\\text{,}%\n\\] and with equal prior probability, \\(P\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathrm{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Both Bayes factors and posterior probabilities can be used for comparing hypotheses. Jeffreys (1961) advocated using Bayes factors, and provided a scale for measuring the strength of evidence that was given earlier. Bayes factors merely indicate that the null hypothesis is more likely if \\(\\mathrm{BF}_{0,1}&gt;1\\), \\(p\\left( y \\mid H_{0}\\right) &gt;p\\left( y \\mid H_{1}\\right)\\). The Bayesian approach merely compares density ordinates of \\(p\\left( y \\mid H_{0}\\right)\\) and \\(p\\left( y \\mid H_{1}\\right)\\), which mechanically involves plugging in the observed data into the functional form of the marginal likelihood.\nFor a point null, \\(H_{0}:\\theta=\\theta_{0}\\), the parameter prior is \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{\\theta_{0}}\\left( \\theta\\right)\\) (a Dirac mass at \\(\\theta_{0}\\)), which implies that \\[\np\\left( y \\mid H_{0}\\right) =\\int p\\left( y \\mid \\theta_{0}\\right) p\\left( \\theta \\mid H_{0}\\right) d\\theta=p\\left( y \\mid \\theta_{0}\\right).\n\\] With a general alternative, \\(H_{1}:\\theta\\neq\\theta_{0}\\), the probability of the null is \\[\nP \\left(  \\theta=\\theta_{0} \\mid y\\right)  =\\frac{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  }{p\\left(  y \\mid \\theta\n_{0}\\right)  P \\left(  H_{0}\\right)  +\\left(  1-P\\left( H_{0}\\right)  \\right)  \\int_{\\Theta}p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta},\n\\] where \\(p\\left( \\theta \\mid H_{1}\\right)\\) is the parameter prior under the alternative. This formula will be used below.\nBayes factors and posterior null probabilities measure the relative weight of evidence of the hypotheses. Traditional hypothesis testing involves an additional decision or action: to accept or reject the null hypothesis. For Bayesians, this typically requires some statement of the utility/loss that codifies the benefits/costs of making a correct or incorrect decision. The simplest situation occurs if one assumes a zero loss of making a correct decision. The loss incurred when accepting the null (alternative) when the alternative is true (false) is \\(L\\left( d_{0} \\mid H_{1}\\right)\\) and \\(L\\left( d_{1} \\mid H_{0}\\right)\\), respectively.\nThe Bayesian will accept or reject based on the posterior expected loss. If the expected loss of accepting the null is less than the alternative, the rational decision maker will accept the null. The posterior loss of accepting the null is \\[\n\\mathbb{E}\\left[  \\mathcal{L}\\mid d_{0},y\\right]  =L\\left(  d_{0} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{0} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  ,\n\\] since the loss of making a correct decision, \\(L\\left( d_{0} \\mid H_{0}\\right)\\), is zero. Similarly, \\[\n\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{1},y\\right]  =L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  +L\\left(  d_{1} \\mid H_{1}\\right)  P \\left(  H_{1} \\mid y\\right)  =L\\left( d_{1} \\mid H_{0}\\right)  P \\left(  H_{0} \\mid y\\right)  .\n\\] Thus, the null is accepted if \\[\n\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{0},y\\right]  &lt;\\mathbb{E}\\left[  \\mathcal{L} \\mid d_{1},y\\right]\n\\Longleftrightarrow L\\left(  d_{0} \\mid H_{1}\\right)  P \\left( H_{1} \\mid y\\right)  &lt;L\\left(  d_{1} \\mid H_{0}\\right)\nP \\left(  H_{0} \\mid y\\right)  ,\n\\] which further simplifies to \\[\n\\frac{L\\left(  d_{0} \\mid H_{1}\\right)  }{L\\left(  d_{1} \\mid H_{0}\\right)  }&lt;\\frac{P \\left(  H_{0} \\mid y\\right)  }{P \\left(  H_{1} \\mid y\\right)  }.\n\\] In the case of equal losses, this simplifies to accept the null if \\(P \\left( H_{1} \\mid y\\right) &lt;P \\left( H_{0} \\mid y\\right)\\). One advantage of Bayes procedures is that the resulting estimators and decisions are always admissible.\n\nExample 6.2 (Enigma Code Breaking) Consider an alphabet of \\(A=26\\) letters. Let \\(x\\) and \\(y\\) be two transmitted messages of length \\(T\\). We want to determine if they were encoded by the same Enigma machine setting (\\(H_1\\)) or by different/random settings (\\(H_0\\)).\nTo compute the Bayes factor, we compare the likelihood of the observed pair \\((x, y)\\) under each hypothesis: \\[\nP( x,y\\mid  H_0 ) \\; \\; \\mathrm{ and} \\; \\; P( x,y\\mid  H_1 ).\n\\] Under \\(H_0\\) (different settings), the two messages are effectively independent random sequences. The probability of any specific pair of letters is \\((1/A)^2\\), so for length \\(T\\): \\[\nP(x, y \\mid H_0) = \\prod_{i=1}^T \\left(\\frac{1}{A}\\right)^2 = \\left(\\frac{1}{A}\\right)^{2T}.\n\\]\nUnder \\(H_1\\) (same setting), the messages are correlated. Specifically, if the letters at position \\(i\\) are the same (\\(x_i = y_i\\)), it implies a ‘match’. The probability of a match, denoted by \\(m\\), depends on the language’s letter frequencies \\(p_t\\) (for English, \\(m = \\sum p_t^2 \\approx 0.066\\) or about \\(2/26\\)). If they don’t match, the probability is distributed among the remaining pairs. Thus: \\[\nP( x_i , y_i \\mid H_1 ) = \\begin{cases}\n\\frac{m}{A} & \\text{if } x_i = y_i \\text{ (match)} \\\\\n\\frac{1-m}{A(A-1)} & \\text{if } x_i \\neq y_i \\text{ (mismatch)}\n\\end{cases}\n\\] The term \\(1/A\\) appears because we approximate the marginal probability of \\(x_i\\) as uniform, but the conditional probability \\(P(y_i|x_i)\\) is boosted to \\(m\\) if \\(x_i=y_i\\).\nThe log Bayes factor is the sum of contributions from matches (\\(M\\)) and mismatches (\\(N\\)): \\[\\begin{align*}\n\\ln \\frac{P( x,y\\mid  H_1 )}{P( x,y\\mid  H_0 )} & = M \\ln \\frac{ m/A}{1/A^2} +N \\ln \\frac{ ( 1-m ) / A(A-1) }{ 1/ A^2} \\\\\n& = M \\ln (mA)  + N \\ln \\frac{ ( 1-m )A }{A-1 }\n\\end{align*}\\] Substituting values for English (\\(A=26, m \\approx 0.066\\)): The first term (match) adds \\(\\ln(0.066 \\times 26) \\approx 0.54\\). The second term (mismatch) subtracts \\(\\ln(\\frac{0.934 \\times 26}{25}) \\approx -0.01\\). In base 10 (decibans), a match provides roughly 2.3 decibans of evidence, while a mismatch provides a slight penalty.\nExample: With \\(T=51\\) letters, suppose we observe \\(M=4\\) matches and \\(N=47\\) mismatches. This yields: \\[\n4 \\times 2.3 - 47 \\times 0.03 \\approx 9.2 - 1.41 = 7.79 \\text{ decibans}.\n\\] This corresponds to a Bayes factor of roughly \\(10^{0.78} \\approx 6\\), providing evidence for \\(H_1\\).\nHow long a sequence do you need to look at? Calculate the expected log odds. Turing and Good figured you needed sequences of about length \\(400\\). Can also look at doubles and triples.\n\n\nExample 6.3 (Dice and Odds Updating.) Suppose that you wish to assess whether a die is loaded or not. $ H_0 : p= 1/6$ vs $ H_1 : p = 1 / 5 $. How will the evidence accumulate in each case?\nLet $ x = # 6$’s and $ y = # $ non-\\(6\\)’s. Then $ x+ y = n $. The posterior odds will update via the likelihood ratio (a.k.a. Bayes factor) as \\[\\begin{align*}\nO ( H_0 | D ) & = \\left ( \\frac{1/6}{1/5} \\right )^x \\left ( \\frac{5/6}{4/5} \\right )^y O ( H_0 )  \\\\\n  & = \\left ( \\frac{5}{6} \\right )^x \\left ( \\frac{25}{24} \\right )^y O ( H_0 )\n\\end{align*}\\] Under $ H_0 : p= 1/6 $ we can replace the data with the empirical cdf (a.k.a. $ x/n = 1/6 $) and similarly under $ H_1 $ we have $ x/n = 1/5 $.\nHence, we have \\[\n\\frac{ O ( H_0 | D ) }{O(H_0)}  \\approx \\left \\{ \\left ( \\frac{5}{6} \\right )^{1/6} \\left ( \\frac{25}{24} \\right )^{5/6}  \\right \\}^n   = ( 1.00364 )^n = 10^{0.00158 n }\n\\] Hence, on a deciban scale (ten times the log-base-10 likelihood ratio, a term coined by I.J. Good), evidence accumulates at rate \\(0.00158\\) in favor of \\(H_0\\).\nUnder \\(H_1\\), we have \\[\n\\frac{ O ( H_0 | D )}{O(H_0) }   \\approx \\left \\{ \\left ( \\frac{5}{6} \\right )^{1/5} \\left ( \\frac{25}{24} \\right )^{4/5}  \\right \\}^n   = ( 0.9962 )^n  = 10^{- 0.00165 n }\n\\] Hence, on a deciban scale (IJ Good), evidence accumulates at rate \\(0.00165\\) against $H_0 $.\nThe Chernoff-Stein information lemma formalises this (Cover and Thomas 2006).\n\n\nExample 6.4 (Signal Transmission) Suppose that the random variable \\(X\\) is transmitted over a noisy communication channel. Assume that the received signal is given by \\[\nY=X+W,\n\\] where \\(W\\sim N(0,\\sigma^2)\\) is independent of \\(X\\). Suppose that \\(X=1\\) with probability \\(p\\), and \\(X=-1\\) with probability \\(1-p\\). The goal is to decide between \\(X=1\\) and \\(X=-1\\) by observing the random variable \\(Y\\). We will assume symmetric loss and will accept the hypothesis with the higher posterior probability. This is also sometimes called the maximum a posteriori (MAP) test.\nWe assume that \\(H_0: ~ X = 1\\), thus \\(Y\\mid H_0 \\sim N(1,\\sigma^2)\\), and \\(Y\\mid H_1 \\sim N(-1,\\sigma^2)\\). The Bayes factor is simply the likelihood ratio \\[\n\\dfrac{p(y\\mid H_0)}{p(y \\mid H_1)} =  \\exp\\left( \\frac{2y}{\\sigma^2}\\right).\n\\] The prior odds are \\(p/(1-p)\\), thus the posterior odds are \\[\n\\exp\\left( \\frac{2y}{\\sigma^2}\\right)\\dfrac{p}{1-p}.\n\\] We choose \\(H_0\\) (true \\(X\\) is 1), if the posterior odds are greater than 1, i.e., \\[\ny &gt; \\frac{\\sigma^2}{2} \\log\\left( \\frac{1-p}{p}\\right) = c.\n\\]\nFurther, we can calculate the error probabilities of our test. \\[\np(d_1\\mid H_0) = P(Y&lt;c\\mid X=1) = \\Phi\\left( \\frac{c-1}{\\sigma}\\right),\n\\] and \\[\np(d_0\\mid H_1) = P(Y&gt;c\\mid X=-1) = 1- \\Phi\\left( \\frac{c+1}{\\sigma}\\right).\n\\] Let’s plot the total error rate as a function of \\(p\\) and assuming \\(\\sigma=0.2\\) \\[\nP_e = p(d_1\\mid H_0) (1-p) + p(d_0\\mid H_1) p\n\\]\n\nsigma &lt;- 0.2\np &lt;- seq(0.01,0.99,0.01)\nc &lt;- sigma^2/2*log((1-p)/p)\nPe &lt;- pnorm((c-1)/sigma)*(1-p) + (1-pnorm((c+1)/sigma))*p\nplot(p,Pe,type=\"l\",xlab=\"p\",ylab=\"Total Error Rate\")\n\n\n\n\n\n\n\nFigure 6.1: Total error rate as a function of \\(p\\) for \\(\\sigma=0.2\\)\n\n\n\n\n\nFigure 6.1 shows the resulting Bayes (MAP) error rate as the prior probability \\(p\\) varies. As expected from the symmetry of the model, the curve is symmetric around \\(p=1/2\\): when the two hypotheses are equally likely, the decision threshold \\(c\\) is closest to 0 and the total probability of error is minimized. As \\(p\\) approaches 0 or 1, the threshold shifts to favor the more likely hypothesis, which reduces one type of mistake but increases the other, leading to a higher overall error rate.\n\n\nExample 6.5 (Hockey: Hypothesis Testing for Normal Mean) The general manager of Washington Capitals (an NHL hockey team) thinks that their star center player Evgeny Kuznetsov is underperforming and is thinking of trading him to a different team. He uses the number of goals per season as a metric of performance. He knows that historically, a top forward scores on average 30 goals per season with a standard deviation of 5, \\(\\theta \\sim N(30,25)\\). In the 2022-2023 season Kuznetsov scored 12 goals. For the number of goals \\(X\\mid \\theta\\) he uses normal likelihood \\(N(\\theta, 36)\\). Kuznetsov’s performance was not stable over the years, thus the high variance in the likelihood. Thus, the posterior is \\(N(23,15)\\).\n\nsigma2 = 36\nsigma02 = 25\nmu=30\ny=12\nk = sigma02 + sigma2\nmu1 = sigma2/k*mu + sigma02/k*y\nsigma21 = sigma2*sigma02/k\nmu1\n## 23\nsigma21\n## 15\n\nThe manager thinks that Kuznetsov simply had a bad year and his true performance is at least 24 goals per season \\(H_0: \\theta \\geq 24\\), \\(H_1: \\theta&lt;24\\). The posterior probability of the \\(H_0\\) hypothesis is\n\na = 1-pnorm(24,mu1,sqrt(sigma21))\na\n## 0.36\n\nIt is less than 1/2, only 36%. Thus, we should reject the null hypothesis. The posterior odds in favor of the null hypothesis are\n\na/(1-a)\n## 0.56\n\nIf underestimating (and trading) Kuznetsov is two times more costly than overestimating him (fans will be upset and team spirit might be affected), that is \\(L(d_1\\mid H_0) = 2L(d_0\\mid H_1)\\), then we should accept the null when posterior odds are greater than 1/2. This is the case here, 0.55 is greater than 1/2. The posterior odds are in favor of the null hypothesis. Thus, the manager should not trade Kuznetsov.\nKuznetsov was traded to Carolina Hurricanes towards the end of the 2023-2024 season.\nNotice, when we try to evaluate a newcomer to the league, we use the prior probability of \\(\\theta \\geq 24\\):\n\na = 1-pnorm(24,mu,sqrt(sigma02))\nprint(a)\n## 0.88\na/(1-a)\n## 7.7\n\nThus, the prior odds in favor of \\(H_0\\) are 7.7.\n\n\nExample 6.6 (Hypothesis Testing for Normal Mean: Two-Sided Test) In the case of two sided test, we are interested in testing\n\n\\(H_0: \\theta = \\theta_0\\), \\(p\\left( \\theta \\mid H_{0}\\right) =\\delta_{\\theta_0}\\left( \\theta\\right)\\)\n\\(H_1: \\theta \\neq \\theta_0\\), \\(p\\left( \\theta \\mid H_{1}\\right) = N\\left( \\theta_0,\\sigma^{2}/n_0\\right)\\)\n\nWhere \\(n\\) is the sample size and \\(\\sigma^2\\) is the variance (known) of the population. Observed samples are \\(Y = (y_1, y_2, \\ldots, y_n)\\) with \\[\ny_i \\mid \\theta,\\sigma^2 \\sim N(\\theta, \\sigma^2).\n\\]\nThe Bayes factor can be calculated analytically \\[\nBF_{0,1} = \\frac{p(Y\\mid \\theta = \\theta_0, \\sigma^2 )}\n{\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid \\theta_0, n_0, \\sigma^2)\\, d \\theta}\n\\] \\[\n\\int p(Y\\mid \\theta, \\sigma^2) p(\\theta \\mid \\theta_0, n_0, \\sigma^2)\\, d \\theta = \\frac{\\sqrt{n_0}\\exp\\left\\{-\\frac{n_0(\\theta_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}\\right\\}}{\\sqrt{2\\pi}\\sigma^2\\sqrt{\\frac{n_0+n}{\\sigma^2}}}\n\\] \\[\np(Y\\mid \\theta = \\theta_0, \\sigma^2 ) = \\frac{\\exp\\left\\{-\\frac{(\\bar y-\\theta_0)^2}{2 \\sigma ^2}\\right\\}}{\\sqrt{2 \\pi } \\sigma }\n\\] Thus, the Bayes factor is \\[\nBF_{0,1} = \\frac{\\sigma\\sqrt{\\frac{n_0+n}{\\sigma^2}}e^{-\\frac{(\\theta_0-\\bar y)^2}{2\\left(n_0+n\\right)\\sigma^2}}}{\\sqrt{n_0}}\n\\]\n\\[\nBF_{0,1} =\\left(\\frac{n + n_0}{n_0} \\right)^{1/2} \\exp\\left\\{-\\frac{1}{2} \\frac{n }{n + n_0} Z^2 \\right\\}\n\\]\n\\[\nZ =  \\frac{(\\bar{Y} - \\theta_0)}{\\sigma/\\sqrt{n}}\n\\]\nOne way to interpret the scaling factor \\(n_0\\) is to look at the standard effect size \\[\n\\delta = \\frac{\\theta - \\theta_0}{\\sigma}.\n\\] The prior of the standard effect size is \\[\n\\delta \\mid H_1 \\sim N(0, 1/n_0).\n\\] This allows us to think about a standardized effect independent of the units of the problem.\nLet’s consider now example of Argon discovery.\n\nair =    c(2.31017, 2.30986, 2.31010, 2.31001, 2.31024, 2.31010, 2.31028, 2.31028)\ndecomp = c(2.30143, 2.29890, 2.29816, 2.30182, 2.29869, 2.29940, 2.29849, 2.29889)\n\nOur null hypothesis is that the mean of the difference equals to zero. We assume that measurements made in the lab have normal errors, this the normal likelihood. We empirically calculate the standard deviation of our likelihood. The Bayes factor is\n\ny = air - decomp\nn = length(y); m0 = 0\nsigma = sqrt(var(air) + var(decomp))\nn0 = 1\nZ = (mean(y) - m0)/(sigma/sqrt(n))\nBF = sqrt((n + n0)/n0)*exp(-0.5*n/(n + n0)*Z^2)\nBF\n## 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000019\n\nWe have extremely strong evidence in favor \\(H_1: \\theta \\ne 0\\) hypothesis. The posterior probability of the alternative hypothesis is numerically 1!\n\na = 1/(1+BF)\na\n## 1\n\n\n\nExample 6.7 (Hypothesis Testing for Proportions) Let’s look at again at the effectiveness of Google’s new search algorithm. We measure effectiveness by the number of users who clicked on one of the search results. As users send the search requests, they will be randomly processed with Algo 1 or Algo 2. We wait until 2500 search requests were processed by each of the algorithms and calculate the following table based on how often people clicked through\n\n\n\n\nAlgo1\nAlgo2\n\n\n\n\nsuccess\n1755\n1818\n\n\nfailure\n745\n682\n\n\ntotal\n2500\n2500\n\n\n\nHere we assume binomial likelihood and use conjugate beta prior, for mathematical convenience. We are putting independent beta priors on the click-through rates of the two algorithms, \\(p_1\\sim Beta(\\alpha_1,\\beta_1)\\) and \\(p_2\\sim Beta(\\alpha_2,\\beta_2)\\). The posterior for \\(p_1\\) and \\(p_2\\) are independent Beta distributions \\[\np(p_1, p_1 \\mid y) \\propto p_1^{\\alpha_1 + 1755 - 1} (1-p_1)^{\\beta_1 + 745 - 1}\\times p_2^{\\alpha_2 + 1818 - 1} (1-p_2)^{\\beta_2 + 682 - 1}.\n\\]\nThe easiest way to explore this posterior is via Monte Carlo simulation of the posterior.\n\nset.seed(92) #Kuzy\ny1 &lt;- 1755; n1 &lt;- 2500; alpha1 &lt;- 1; beta1 &lt;- 1\ny2 &lt;- 1818; n2 &lt;- 2500; alpha2 &lt;- 1; beta2 &lt;- 1\nm = 10000\np1 &lt;- rbeta(m, y1 + alpha1, n1 - y1 + beta1)\np2 &lt;- rbeta(m, y2 + alpha2, n2 - y2 + beta2)\nrd &lt;- p2 - p1\nplot(density(rd),xlab=\"p2 - p1\", ylab=\"Density\", lwd=3)\nq = quantile(rd, c(.05, .95))\nprint(q)\n##     5%    95% \n## 0.0037 0.0465\nabline(v=q,col=\"red\", lwd=2)\n\n\n\n\nPosterior Difference in Click-Through Rates",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#interval-estimation-credible-sets",
    "href": "06-hyp.html#interval-estimation-credible-sets",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.3 Interval Estimation: Credible Sets",
    "text": "6.3 Interval Estimation: Credible Sets\nThe interval estimators of model parameters are called credible sets. If we use the posterior measure to assess the credibility, the credible set is a set of parameter values that are consistent with the data and gives us is a natural way to measure the uncertainty of the parameter estimate.\nThose who are familiar with the concept of classical confidence intervals (CI’s) often make an error by stating that the probability that the CI interval \\([L, U ]\\) contains parameter \\(\\theta\\) is \\(1 - \\alpha\\). The right statement seems convoluted, one needs to generate data from such model many times and for each data set to exhibit the CI. Now, the proportion of CI’s covering the unknown parameter is “tends to” \\(1 - \\alpha\\). Bayesian interpretation of a credible set \\(C\\) is natural: The probability of a parameter belonging to the set \\(C\\) is \\(1 - \\alpha\\). A formal definition follows. Assume the set \\(C\\) is a subset of domain of the parameter \\(\\Theta\\). Then, \\(C\\) is credible set with credibility \\((1 - \\alpha)\\cdot 100\\%\\) if \\[\np(\\theta \\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] If the posterior is discrete, then the integral becomes sum (counting measure) and \\[\np(\\theta \\in C \\mid y) = \\sum_{\\theta_i\\in C}p(\\theta_i\\mid y) \\ge 1 - \\alpha.\n\\] This is the definition of a \\((1 - \\alpha)100\\%\\) credible set, and of course for a given posterior function such set is not unique.\nFor a given credibility level \\((1 - \\alpha)100\\%\\), the shortest credible set is of interest. To minimize size the sets should correspond to highest posterior probability (density) areas. Thus the acronym HPD.\n\nDefinition 6.1 (Highest Posterior Density (HPD) Credible Set) The \\((1 - \\alpha)100\\%\\) HPD credible set for parameter \\(\\theta\\) is a set \\(C \\subset \\Theta\\) of the form \\[\nC = \\{ \\theta \\in \\Theta : p(\\theta \\mid y) \\ge k(\\alpha) \\},\n\\] where \\(k(\\alpha)\\) is the largest value such that \\[\nP(\\theta\\in C \\mid y) = \\int_{C}p(\\theta\\mid y)d\\theta \\ge 1 - \\alpha.\n\\] Geometrically, if the posterior density is cut by a horizontal line at the height \\(k(\\alpha)\\), the set \\(C\\) is the projection on the \\(\\theta\\) axis of the region where the posterior density lies above the line.\n\n\n\n\n\n\n\n\n\n\n\nLemma 6.1 The HPD set \\(C\\) minimizes the size among all sets \\(D \\subset \\Theta\\) for which \\[\nP(\\theta \\in D) = 1 - \\alpha.\n\\]\n\n\nProof. The proof is essentially a special case of Neyman-Pearson lemma. If \\(I_C(\\theta) = 1(\\theta \\in C)\\) and \\(I_D(\\theta) = 1(\\theta \\in D)\\), then the key observation is \\[\n\\left(p(\\theta\\mid y) - k(\\alpha)\\right)(I_C(\\theta) - I_D(\\theta)) \\ge 0.\n\\] Indeed, for \\(\\theta\\)’s in \\(C\\cap D\\) and \\((C\\cup D)^c\\), the factor \\(I_C(\\theta)-I_D(\\theta) = 0\\). If \\(\\theta \\in C\\cap D^c\\), then \\(I_C(\\theta)-I_D(\\theta) = 1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\ge 0\\). If, on the other hand, \\(\\theta \\in D\\cap C^c\\), then \\(I_C(\\theta)-I_D(\\theta) = -1\\) and \\(p(\\theta\\mid y)-k(\\alpha) \\le 0\\). Thus, \\[\n\\int_{\\Theta}(p(\\theta\\mid y) - k(\\alpha))(I_C(\\theta) - I_D(\\theta))d\\theta \\ge 0.\n\\] The statement of the theorem now follows from the chain of inequalities, \\[\n\\int_{C}(p(\\theta\\mid y) - k(\\alpha))d\\theta \\ge \\int_{D}(p(\\theta\\mid y) - k(\\alpha))d\\theta\n\\] \\[\n(1-\\alpha) - k(\\alpha)\\text{size}(C) \\ge (1-\\alpha) - k(\\alpha)\\text{size}(D)\n\\] \\[\nsize(C) \\le size(D).\n\\] The size of a set is simply its total length if the parameter space \\(\\theta\\) is one dimensional, total area, if \\(\\theta\\) is two dimensional, and so on.\n\nNote, when the distribution \\(p(\\theta \\mid y)\\) is unimodal and symmetric using quantiles of the posterior distribution is a good way to obtain the HPD set.\nAn equal-tailed interval (also called a central interval) of confidence level\n\\[\nI_{\\alpha} = [q_{\\alpha/2}, q_{1-\\alpha/2}],\n\\] here \\(q\\)’s are the quantiles of the posterior distribution. This is an interval on whose both right and left side lies \\((1-\\alpha/2)100\\%\\) of the probability mass of the posterior distribution; hence the name equal-tailed interval.\nUsually, when a credible interval is mentioned without specifying which type of the credible interval it is, an equal-tailed interval is meant.\nHowever, unless the posterior distribution is unimodal and symmetric, there are points outside of the equal-tailed credible interval having a higher posterior density than some points of the interval. If we want to choose the credible interval so that this not happen, we can do it by using the highest posterior density criterion for choosing it.\n\nExample 6.8 (Cauchy.) Assume that the observed samples\n\ny = c(2,-7,4,-6)\n\ncome from Cauchy distribution. The likelihood is \\[\np(y\\mid \\theta, \\gamma) = \\frac{1}{\\pi\\gamma} \\prod_{i=1}^{4} \\frac{1}{1+\\left(\\dfrac{y_i-\\theta}{\\gamma}\\right)^2}.\n\\] We assume unknown location parameter \\(\\theta\\) and scale parameter \\(\\gamma=1\\). For the flat prior \\(\\pi(\\theta) = 1\\), the posterior is proportional to the likelihood.\n\npar(mar = c(4, 4, 0, 0), bty=\"n\")\nlhood = function(theta) 1/prod(1+(y-theta)^2)\ntheta &lt;- seq(-10,10,0.1)\npost &lt;- sapply(theta,lhood)\npost = 10*post/sum(post)\nplot(theta,post,type=\"l\",xlab=expression(theta),ylab=\"Posterior Density\")\nabline(h=c(0.008475, 0.0159, 0.1, 0.2),col=\"red\")\n\n\n\n\n\n\n\n\nThe four horizontal lines correspond to four credible sets\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(C\\)\n\\(P(\\theta \\in C \\mid y)\\)\n\n\n\n\n0.008475\n[-8.498, 5.077]\n99%\n\n\n0.0159\n[-8.189, -3.022] \\(\\cup\\) [-0.615, 4.755]\n95%\n\n\n0.1\n[-7.328, -5.124] \\(\\cup\\) [1.591, 3.120]\n64.2%\n\n\n0.2\n[-6.893, -5.667]\n31.2%\n\n\n\nNotice that for \\(k = 0.0159\\) and \\(k = 0.1\\) the credible set is not a compact. This shows that two separate intervals “clash” for the ownership of \\(\\theta\\) and this is a useful information. This non-compactness can also point out that the prior is not agreeing with the data. There is no frequentist counterpart for the CI for \\(\\theta\\) in the above model.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#alternative-approaches",
    "href": "06-hyp.html#alternative-approaches",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.4 Alternative Approaches",
    "text": "6.4 Alternative Approaches\nThe two main alternatives to the Bayesian approach are significance testing using \\(p-\\)values, developed by Ronald Fisher, and the Neyman-Pearson approach.\n\nSignificance testing using p-values\nFisher’s approach posits a test statistic, \\(T\\left( y\\right)\\), based on the observed data. In Fisher’s mind, if the value of the statistic was highly unlikely to have occured under \\(H_{0}\\), then the \\(H_{0}\\) should be rejected. Formally, the \\(p-\\)value is defined as \\[\np=P \\left[  T\\left(  Y\\right)  &gt;T\\left(  y\\right)   \\mid H_{0}\\right]  ,\n\\] where \\(y\\) is the observed sample and \\(Y=\\left( Y_{1}, \\ldots ,Y_{T}\\right)\\) is a random sample generated from model \\(p\\left( Y \\mid H_{0}\\right)\\), that is, the null distribution of the test-statistic in repeated samples. Thus, the \\(p-\\)value is the probability that a data set would generate a more extreme statistic under the null hypothesis, and not the probability of the null, conditional on the data.\nThe testing procedure is simple. Fisher (1946, p. 80) argues that: If P (the p-value) is between* \\(0.1\\) and \\(0.9\\), there is certainly no reason to suspect the hypothesis tested. If it is below \\(0.02\\), it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not be astray if we draw a line at 0.05 and consider that higher values of \\(\\mathcal{X}^{2}\\) indicate a real discrepancy. Defining \\(\\alpha\\) as the significance level, the tests rejects \\(H_{0}\\) if \\(p&lt;\\alpha\\). Fisher advocated a fixed significance level of \\(5\\%\\), based largely that \\(5\\%\\) is roughly the tail area of a mean zero normal distribution more than two standard deviations from \\(0\\), indicating a statistically significant departure. In practice, testing with \\(p-\\)values involves identifying a critical value, \\(t_{\\alpha}\\), and rejecting the null if the observed statistic \\(t\\left( y\\right)\\) is more extreme than \\(t_{\\alpha}\\). For example, for a significance test of the sample mean, \\(t\\left( y\\right) =\\left( \\overline{y}-\\theta_{0}\\right) /se\\left( \\overline{y}\\right)\\), where \\(se\\left( \\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\); the \\(5\\%\\) critical value is 1.96; and Fisher would reject the null if \\(t\\left( y\\right) &gt;t_{\\alpha}\\).\nFisher interpreted the \\(p-value\\) as the weight or measure of evidence of the null hypothesis. The alternative hypothesis is noticeable in its absence in Fisher’s approach. Fisher largely rejected the consideration of alternatives, believing that researchers should weigh the evidence or draw conclusions about the observed data rather than making decisions such as accepting or rejecting hypotheses based on it.\nThere are a number of issues with Fisher’s approach. The first and most obvious criticism is that it is possible to reject the null, when the alternative hypothesis is less likely. This is an inherent problem in using population tail probabilities–essentially rare events. Just because a rare event has occurred does not mean the null is incorrect, unless there is a more likely alternative. This situation often arises in court cases, where a rare event like a murder has occurred. Decisions based on p-values generates a problem called prosecutor’s Fallacy, which is discussed below. Second, Fisher’s approach relies on population properties (the distribution of the statistic under the null) that would only be revealed in repeated samples or asymptotically. Thus, the testing procedure relies on data that is not yet seen, a violation of what is known as the likelihood principle. As noted by Jeffreys’ (1939, pp. 315-316): “What the use of P implies, therefore, is that a hypothesis that may be true may be rejected because it has not predicted observable data that have not occurred. This seems a remarkable procedure” \nThird, Fisher is agnostic regarding the source of the test statistics, providing no discussion of how the researcher decides to focus on one test statistic over another. In some simple models, the distribution of properly scaled sufficient statistics provides natural test statistics (e.g., the \\(t-\\)test). In more complicated models, Fisher is silent on the sources. In many cases, there are numerous test statistics (e.g., testing for normality), and test choice is clearly subjective. For example, in Generalized Method of Moments (GMM) tests, the choice of test moments is clearly a subjective choice. Finally, from a practical perspective, \\(p-\\)values have a serious deficiency: tests using \\(p\\)-values often appear to give the wrong answer, in the sense that they provide a highly misleading impression of the weight of evidence in many samples. A number of examples of this will be given below, but in all cases, Fisher’s approach tends to over-reject the null hypotheses.\n\n\nBayes vs P-value\nThe fundamental difference between the Bayesian and frequentist approaches can be summarized by how they quantify evidence. As R.A. Fisher famously wrote: “… for the one chance in a million will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us …”. This quote highlights that rare events do happen under the null hypothesis.\nThe Bayesian evidence is quantified by the Bayes Factor: \\[\nBF = \\frac{P(D|H_0)}{P(D|H_1)}\n\\] If we consider the data definition used in frequentist testing, where \\(D = \\{T(x) &gt; t\\}\\), then the numerator \\(P(D|H_0)\\) corresponds precisely to the p-value. The crucial limitation of the p-value is that it considers only the numerator. Even if this probability is small (suggesting the data is rare under the null), the denominator \\(P(D|H_1)\\)—the probability of observing such data under the alternative—might be even smaller!\nIn a case with mutually exhaustive hypotheses (\\(P(H_0) + P(H_1) = 1\\)), it is perfectly possible for the p-value to be small (e.g., 0.05), yet for the Bayes Factor to be greater than 1 (\\(BF &gt; 1\\)), indicating that the evidence actually favors \\(H_0\\). This highlights the danger of assessing hypotheses in isolation rather than comparing them relative to one another.\n\nExample 6.9 (Ulcer Treatment Clinical Trial) One of the earliest applications of Bayesian methods to clinical trials was presented by Novick and Grizzle (1965), who analyzed data from an ongoing experiment comparing four operative treatments for duodenal ulcers. Doctors assessed patient outcomes as Excellent, Fair, or Death. The data, collected sequentially over the course of the trial, are shown below:\n\n\n\nTable 6.1: Outcomes by treatment for duodenal ulcer surgery\n\n\n\n\n\nTreatment\nExcellent\nFair\nDeath\nTotal\n\n\n\n\nA\n76\n17\n7\n100\n\n\nB\n89\n10\n1\n100\n\n\nC\n86\n13\n1\n100\n\n\nD\n88\n9\n3\n100\n\n\n\n\n\n\nWhen I present this data to students and ask which treatment they would prefer, most choose Treatment B. The reasoning is intuitive: Treatment B has the highest number of excellent outcomes (89) and the lowest death rate (only 1 death, tied with C). Treatment A, despite being listed first, has only 76 excellent outcomes and 7 deaths, the worst performance on both metrics. The students’ intuition is correct, but can we quantify how confident we should be that B is truly better than A?\nA classical chi-square test of homogeneity across treatments yields a p-value greater than 0.05, leading to the conclusion that we cannot reject the null hypothesis that all treatments are equally effective at the 5% significance level. This non-rejection is often misinterpreted as evidence that the treatments are equivalent.\nBut consider what the p-value actually measures: \\(p = P(D \\mid H_0)\\), the probability of observing data at least as extreme as what we saw, assuming the null hypothesis is true. The critical insight is that \\(P(D \\mid H_1)\\), the probability of the data under any specific alternative, can be much smaller than \\(P(D \\mid H_0)\\). A small p-value does not mean the alternative is more likely; it only means the observed data would be rare under the null. This is the essence of the Bayesian critique: inference should be relative, comparing the evidence for different hypotheses, not absolute.\nThe Bayesian approach directly addresses the question of interest: given the observed data, what is the probability that one treatment is better than another? Let \\(p_i\\) denote the death rate under treatment \\(i\\). Using independent Beta priors for each treatment’s death rate, say \\(p_i \\sim \\text{Beta}(1, 1)\\) (uniform), the posteriors after observing the data are: \\[\np_A \\mid \\text{data} \\sim \\text{Beta}(1 + 7, 1 + 93) = \\text{Beta}(8, 94)\n\\] \\[\np_B \\mid \\text{data} \\sim \\text{Beta}(1 + 1, 1 + 99) = \\text{Beta}(2, 100)\n\\]\n\n# Posterior distributions for death rates\n# Treatment A: 7 deaths out of 100\n# Treatment B: 1 death out of 100\nalpha_A &lt;- 1 + 7; beta_A &lt;- 1 + 93\nalpha_B &lt;- 1 + 1; beta_B &lt;- 1 + 99\n\n# Monte Carlo estimate of P(p_A &gt; p_B | data)\nset.seed(123)\nn_sims &lt;- 100000\np_A_samples &lt;- rbeta(n_sims, alpha_A, beta_A)\np_B_samples &lt;- rbeta(n_sims, alpha_B, beta_B)\n\nprob_A_worse &lt;- mean(p_A_samples &gt; p_B_samples)\ncat(\"P(death rate A &gt; death rate B | data) =\", round(prob_A_worse, 3), \"\\n\")\n## P(death rate A &gt; death rate B | data) = 0.98\n\nThe posterior probability that Treatment A has a higher death rate than Treatment B is approximately 0.98. This is a direct, interpretable answer to the clinical question: there is a 98% probability that patients receiving Treatment A face higher mortality risk than those receiving Treatment B.\nThe contrast with the frequentist conclusion is striking. The chi-square test fails to reject equality at the 5% level, which many would interpret as “no difference.” The Bayesian analysis reveals that we can be 98% confident that Treatment A is worse than Treatment B in terms of mortality. The difference arises because the Bayesian approach compares hypotheses directly, while the p-value only measures how surprising the data would be under the null.\n\n# Frequentist chi-square test of homogeneity\n# Construct the contingency table (Excellent, Fair, Death) x (Treatment A, B, C, D)\noutcome_matrix &lt;- matrix(c(\n  76, 17, 7,   # Treatment A\n  89, 10, 1,   # Treatment B\n  86, 13, 1,   # Treatment C\n  88, 9, 3     # Treatment D\n), nrow = 4, byrow = TRUE)\nrownames(outcome_matrix) &lt;- c(\"A\", \"B\", \"C\", \"D\")\ncolnames(outcome_matrix) &lt;- c(\"Excellent\", \"Fair\", \"Death\")\n\n# Chi-square test\nchisq_result &lt;- suppressWarnings(chisq.test(outcome_matrix))\ncat(\"Chi-square statistic:\", round(chisq_result$statistic, 2), \"\\n\")\n## Chi-square statistic: 12\ncat(\"Degrees of freedom:\", chisq_result$parameter, \"\\n\")\n## Degrees of freedom: 6\ncat(\"P-value:\", round(chisq_result$p.value, 3), \"\\n\")\n## P-value: 0.053\n\n# Fisher's exact test for just the death column (A vs B)\ndeath_table &lt;- matrix(c(7, 93, 1, 99), nrow = 2, byrow = TRUE)\nrownames(death_table) &lt;- c(\"A\", \"B\")\ncolnames(death_table) &lt;- c(\"Death\", \"Survival\")\nfisher_result &lt;- fisher.test(death_table, alternative = \"greater\")\ncat(\"\\nFisher's exact test (A vs B, deaths only):\\n\")\n## \n## Fisher's exact test (A vs B, deaths only):\ncat(\"P-value:\", round(fisher_result$p.value, 3), \"\\n\")\n## P-value: 0.032\n\nThe chi-square test yields a p-value of 0.053, which exceeds the conventional 0.05 threshold. Even Fisher’s exact test comparing only the death rates between Treatments A and B gives a p-value around 0.03, which is only marginally significant and provides no sense of the magnitude of the difference or our confidence in it. The Bayesian approach, by contrast, tells us directly that there is a 98% probability that Treatment A has a higher death rate.\nThis example illustrates a fundamental principle: Bayesian inference is relative. We do not ask whether the data are unlikely in some absolute sense; we ask which hypothesis better explains the data. There are no absolutes in Bayesian inference, only comparisons. A treatment is not declared “effective” or “ineffective” in isolation; it is compared to alternatives, with uncertainty fully quantified.\n\np_grid &lt;- seq(0, 0.2, length.out = 500)\nplot(p_grid, dbeta(p_grid, alpha_A, beta_A), type = \"l\", col = \"red\", lwd = 2,\n     xlab = \"Death rate\", ylab = \"Posterior density\", ylim = c(0, 50))\nlines(p_grid, dbeta(p_grid, alpha_B, beta_B), col = \"blue\", lwd = 2)\nlegend(\"topright\", legend = c(\"Treatment A\", \"Treatment B\"),\n       col = c(\"red\", \"blue\"), lwd = 2, bty = \"n\")\n\n# Add posterior means\nabline(v = alpha_A/(alpha_A + beta_A), col = \"red\", lty = 2)\nabline(v = alpha_B/(alpha_B + beta_B), col = \"blue\", lty = 2)\n\n\n\n\nPosterior distributions of death rates\n\n\n\n\nThe posterior distributions show clear separation between the two treatments. Treatment A’s death rate is concentrated around 7-8%, while Treatment B’s is concentrated around 1-2%. The overlap is minimal, corresponding to our finding that \\(P(p_A &gt; p_B \\mid \\text{data}) \\approx 0.98\\).\nPrior Sensitivity Analysis\nA natural concern with Bayesian analysis is whether the conclusions depend heavily on the choice of prior. We used a uniform Beta(1,1) prior, but what if we had used different priors? The table below shows how \\(P(p_A &gt; p_B \\mid \\text{data})\\) varies across several reasonable prior specifications:\n\n\nPrior Sensitivity Analysis\n# Prior sensitivity analysis\nset.seed(456)\nn_sims &lt;- 100000\n\n# Data: Treatment A: 7 deaths, 93 survivals; Treatment B: 1 death, 99 survivals\ndeaths_A &lt;- 7; survivals_A &lt;- 93\ndeaths_B &lt;- 1; survivals_B &lt;- 99\n\n# Different priors to consider\npriors &lt;- data.frame(\n  Prior = c(\"Uniform Beta(1,1)\", \n            \"Jeffreys Beta(0.5,0.5)\", \n            \"Skeptical Beta(2,20)\",\n            \"Optimistic Beta(1,10)\",\n            \"Strong Beta(5,50)\"),\n  alpha = c(1, 0.5, 2, 1, 5),\n  beta = c(1, 0.5, 20, 10, 50)\n)\n\n# Calculate P(p_A &gt; p_B | data) for each prior\nresults &lt;- sapply(1:nrow(priors), function(i) {\n  a &lt;- priors$alpha[i]\n  b &lt;- priors$beta[i]\n  \n  # Posterior parameters\n  alpha_A_post &lt;- a + deaths_A\n  beta_A_post &lt;- b + survivals_A\n  alpha_B_post &lt;- a + deaths_B\n  beta_B_post &lt;- b + survivals_B\n  \n  # Monte Carlo\n  p_A &lt;- rbeta(n_sims, alpha_A_post, beta_A_post)\n  p_B &lt;- rbeta(n_sims, alpha_B_post, beta_B_post)\n  mean(p_A &gt; p_B)\n})\n\npriors$`Prior Mean` &lt;- round(priors$alpha / (priors$alpha + priors$beta), 3)\npriors$`P(p_A &gt; p_B | data)` &lt;- round(results, 3)\nknitr::kable(priors[, c(\"Prior\", \"Prior Mean\", \"P(p_A &gt; p_B | data)\")],\n             caption = \"Sensitivity of posterior probability to prior specification\")\n\n\n\nSensitivity of posterior probability to prior specification\n\n\nPrior\nPrior Mean\nP(p_A &gt; p_B | data)\n\n\n\n\nUniform Beta(1,1)\n0.50\n0.98\n\n\nJeffreys Beta(0.5,0.5)\n0.50\n0.99\n\n\nSkeptical Beta(2,20)\n0.09\n0.97\n\n\nOptimistic Beta(1,10)\n0.09\n0.98\n\n\nStrong Beta(5,50)\n0.09\n0.94\n\n\n\n\n\nThe results demonstrate remarkable robustness. Regardless of whether we use a uniform prior, the Jeffreys prior, or informative priors centered on different mortality rates, the posterior probability that Treatment A has a higher death rate than Treatment B remains above 0.93. This robustness occurs because the sample size (100 patients per treatment) is large enough that the likelihood generally dominates the prior. The data speak loudly, and the conclusion is not an artifact of our prior beliefs.\nThis sensitivity analysis is particularly important in clinical settings, where different stakeholders may have different prior beliefs about treatment efficacy. The fact that all reasonable priors lead to the same qualitative conclusion (strong evidence that A is worse than B) strengthens the case for preferring Treatment B.\n\n\n\nNeyman-Pearson\nThe motivation for the Neyman-Pearson (NP) approach was W.S. Gosset, the famous Student who invented the \\(t-\\)test. In analyzing a hypothesis, Student argued that a hypothesis is not rejected unless an alternative is available that provides a more plausible explanation of the data, in which case. Mathematically, this suggests analyzing the likelihood ratio, \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{,}%\n\\] and rejecting the null in favor of the alternative when the likelihood ratio is small enough, \\(\\mathcal{LR}_{0,1}&lt;k\\). This procedures conforms in spirit with the Bayesian approach.\nThe main problem was one of finding a value of the cut off parameter \\(k.\\) From the discussion above, by varying \\(k\\), one varies the probabilities of type one and type two errors in the testing procedure. Neyman and Pearson (1933a) argued that the balance between Type I and II errors is subjective: “how the balance (between the type I and II errors) should be struck must be left to the investigator”. This approach, however, was not “objective”, and they then advocated fixing \\(\\alpha\\), the probability of a type I error, in order to determine \\(k\\). This led to their famous lemma:\n\nLemma 6.2 (Neyman-Pearson Lemma) Consider the simple hypothesis test of \\(H_{0}:\\theta=\\theta_{0}\\) versus \\(H_{1}:\\theta =\\theta_{1}\\) and suppose that the null is rejected if \\(\\mathcal{LR}_{0,1}&lt;k_{\\alpha}\\), where \\(k_{\\alpha}\\) is chosen to fix the probability of a type I error at \\(\\alpha:\\)% \\[\n\\alpha=P \\left[  y:\\mathcal{LR}_{0,1}&lt;k_{\\alpha} \\mid H_{0}\\right]  \\text{.}%\n\\] Then, this test is the most powerful test of size \\(\\alpha\\) in the sense that any other test with greater power, must have a higher size.\n\nIn the case of composite hypothesis tests, parameter estimation is required under the alternative, which can be done via maximum likelihood, leading to the likelihood ratio \\[\n\\mathcal{LR}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{\\underset\n{\\theta\\in\\Theta}{\\sup}p\\left(  y \\mid \\Theta\\right)  }=\\frac{p\\left( y \\mid H_{0}\\right)  }{p\\left(  y \\mid \\widehat{\\theta}\\right)  }\\text{,}%\n\\] where \\(\\widehat{\\theta}\\) is the MLE. Because of this, \\(0\\leq\\mathcal{LR}_{0,1}\\leq 1\\) for composite hypotheses. In multi-parameter cases, finding the distribution of the likelihood ratio is more difficult, requiring asymptotic approximations to calibrate \\(k_{\\alpha}.\\)\nAt first glance, the NP approach appears similar to the Bayesian approach, as it takes into account the likelihood ratio. However, like the \\(p-\\)value, the NP approach has a critical flaw. Neyman and Pearson fix the Type I error, and then minimizes the type II error. In many practical cases, \\(\\alpha\\) is set at \\(5\\%\\) and the resulting \\(\\beta\\) is often very small, close to 0. Why is this a reasonable procedure? Given the previous discussion, this is essentially a very strong prior over the relative benefits/costs of different types of errors. While these assumptions may be warranted in certain settings, it is difficult to a priori understand why this procedure would generically make sense. The next section highlights how the \\(p-\\)value and NP approaches can generate counterintuitive and even absurd results in standard settings.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#sequential-analysis",
    "href": "06-hyp.html#sequential-analysis",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.5 Sequential Analysis",
    "text": "6.5 Sequential Analysis\nSequential analysis represents one of the most natural applications of Bayesian reasoning, allowing researchers to evaluate evidence as it accumulates and make principled decisions about when sufficient information has been gathered. Unlike fixed-sample designs that predetermine the number of observations, sequential methods continuously update beliefs about hypotheses and can terminate data collection once a satisfactory conclusion is reached.\nThe foundations of sequential analysis were laid by Abraham Wald during World War II, with his development of the Sequential Probability Ratio Test (SPRT) (Wald 1945). Wald’s work, later expanded in his book Sequential Analysis (Wald 1947), demonstrated that sequential procedures could reduce the expected sample size by up to 50% compared to fixed-sample tests while maintaining the same error rates. The key insight was that rather than collecting a predetermined number of observations and then analyzing them, one could examine the data after each observation and stop as soon as the evidence was sufficiently strong in either direction.\n\nThe Bayesian Framework for Sequential Testing\nThe Bayesian approach to sequential testing provides a coherent framework for deciding when to stop collecting data. Consider testing \\(H_0: \\theta \\in \\Theta_0\\) versus \\(H_1: \\theta \\in \\Theta_1\\). After observing data \\(y_1, y_2, \\ldots, y_n\\), the posterior odds are \\[\n\\frac{P(H_0 \\mid y_{1:n})}{P(H_1 \\mid y_{1:n})} = \\frac{p(y_{1:n} \\mid H_0)}{p(y_{1:n} \\mid H_1)} \\cdot \\frac{P(H_0)}{P(H_1)}.\n\\]\nA natural stopping rule is to continue sampling until the posterior probability of one hypothesis exceeds some threshold. For example, stop and accept \\(H_0\\) if \\(P(H_0 \\mid y_{1:n}) &gt; 1 - \\alpha\\), or stop and accept \\(H_1\\) if \\(P(H_1 \\mid y_{1:n}) &gt; 1 - \\beta\\). This approach has a compelling interpretation: we continue gathering evidence until we are sufficiently confident in our conclusion.\nThe Bayesian perspective offers a key advantage over frequentist sequential procedures: the posterior probability is always valid, regardless of when or why sampling stopped. As Edwards, Lindman, and Savage (1963) noted, the likelihood principle implies that the rules governing when data collection stops are irrelevant to data interpretation. This property, sometimes called optional stopping, means that Bayesian inference is immune to the criticism that plagues frequentist sequential analysis, where p-values become invalid if the stopping rule is not prespecified.\n\n\nWald’s Sequential Probability Ratio Test\nWhile Wald’s SPRT was developed from a frequentist perspective, it has deep connections to Bayesian testing. For simple hypotheses \\(H_0: \\theta = \\theta_0\\) versus \\(H_1: \\theta = \\theta_1\\), the SPRT accumulates the likelihood ratio \\[\n\\Lambda_n = \\frac{p(y_{1:n} \\mid \\theta_1)}{p(y_{1:n} \\mid \\theta_0)} = \\prod_{i=1}^{n} \\frac{p(y_i \\mid \\theta_1)}{p(y_i \\mid \\theta_0)}\n\\] and stops when \\(\\Lambda_n \\leq A\\) (accept \\(H_0\\)) or \\(\\Lambda_n \\geq B\\) (accept \\(H_1\\)), where the boundaries \\(A\\) and \\(B\\) are chosen to achieve desired error rates.\nThis is precisely the Bayes factor for simple hypotheses. With equal prior probabilities, the SPRT stopping rule becomes: stop and accept \\(H_0\\) when the posterior probability exceeds \\(B/(1+B)\\), or stop and accept \\(H_1\\) when it exceeds \\(1/(1+A)\\). Wald proved that among all tests with the same error rates, the SPRT minimizes the expected sample size under both hypotheses, a remarkable optimality property.\n\n\nApplications in Clinical Trials\nClinical trials represent perhaps the most important application of sequential analysis, where the ethical imperative to minimize patient exposure to inferior treatments aligns with the statistical goal of efficient inference. Peter Armitage pioneered the application of sequential methods to clinical trials in the 1950s and 1960s (Armitage 1975), demonstrating how these methods could reduce trial duration and the number of patients receiving suboptimal treatments.\nModern clinical trial design frequently employs group sequential designs, which allow interim analyses at predetermined points during patient accrual. The key challenge is controlling the overall Type I error rate when multiple looks at the data are permitted. Frequentist approaches use spending functions to allocate alpha across interim analyses (DeMets and Lan 1994), while Bayesian approaches naturally handle multiple looks through the posterior probability framework.\nThe Bayesian approach to clinical trial design has been championed by Donald Berry, whose seminal work (D. A. Berry 1985) critiqued traditional hypothesis testing and advocated for a framework where sampling stops when the posterior probability that one treatment is superior exceeds a specified threshold. Berry’s methodology, later expanded in S. M. Berry et al. (2010), integrates Bayesian decision theory into trial design and monitoring, enabling continuous assessment of accumulating data and facilitating decisions on early termination for efficacy or futility.\n\nBerry’s Bayesian Framework for Clinical Trials\nConsider a clinical trial comparing a new treatment to a control, where the primary outcome is binary (success or failure). Let \\(p_T\\) denote the success probability under treatment and \\(p_C\\) under control. The quantity of interest is typically the treatment effect, which can be parameterized as the risk difference \\(\\delta = p_T - p_C\\), the relative risk \\(p_T/p_C\\), or the odds ratio.\nBerry’s approach specifies conjugate beta priors for each success probability: \\[\np_C \\sim \\text{Beta}(\\alpha_C, \\beta_C) \\quad \\text{and} \\quad p_T \\sim \\text{Beta}(\\alpha_T, \\beta_T).\n\\] Non-informative priors correspond to \\(\\alpha = \\beta = 1\\) (uniform) or \\(\\alpha = \\beta = 0.5\\) (Jeffreys). After observing \\(x_C\\) successes in \\(n_C\\) control patients and \\(x_T\\) successes in \\(n_T\\) treated patients, the posteriors are \\[\np_C \\mid \\text{data} \\sim \\text{Beta}(\\alpha_C + x_C, \\beta_C + n_C - x_C)\n\\] \\[\np_T \\mid \\text{data} \\sim \\text{Beta}(\\alpha_T + x_T, \\beta_T + n_T - x_T).\n\\]\nThe key quantity for decision-making is the posterior probability that treatment is superior: \\[\nP(p_T &gt; p_C \\mid \\text{data}) = \\int_0^1 \\int_{p_C}^{1} f(p_C \\mid \\text{data}) f(p_T \\mid \\text{data}) \\, dp_T \\, dp_C,\n\\] where \\(f(\\cdot \\mid \\text{data})\\) denotes the posterior density. This integral can be computed via Monte Carlo simulation by drawing samples from each posterior and computing the proportion where \\(p_T^{(s)} &gt; p_C^{(s)}\\).\nBerry’s stopping rules are defined in terms of posterior probability thresholds:\n\nEfficacy stopping: Stop and declare treatment effective if \\(P(p_T &gt; p_C + \\delta_{\\min} \\mid \\text{data}) &gt; \\theta_E\\), where \\(\\delta_{\\min}\\) is the minimum clinically meaningful difference and \\(\\theta_E\\) is the efficacy threshold (e.g., 0.95 or 0.99).\nFutility stopping: Stop for futility if \\(P(p_T &gt; p_C \\mid \\text{data}) &lt; \\theta_F\\), where \\(\\theta_F\\) is the futility threshold (e.g., 0.05 or 0.10).\n\nA more sophisticated approach uses predictive probability of success (PPoS), which accounts for future data that might be collected. The PPoS at interim analysis is the probability, given current data, that the trial will demonstrate efficacy if continued to the planned maximum sample size: \\[\n\\text{PPoS} = P\\left( P(p_T &gt; p_C \\mid \\text{all data}) &gt; \\theta_E \\mid \\text{current data} \\right).\n\\] This is computed by averaging over the predictive distribution of future outcomes. If PPoS falls below a threshold (e.g., 0.05), continuing the trial is unlikely to yield a positive result, making futility stopping appropriate.\n\n\nOperating Characteristics\nThe operating characteristics of a Bayesian sequential design, including the Type I error rate, power, and expected sample size, are determined through simulation. For a given set of true parameter values \\((\\pi_C, \\pi_T)\\), one simulates many trials, applies the stopping rules at each interim analysis, and tabulates the outcomes. This approach allows calibration of the thresholds \\(\\theta_E\\) and \\(\\theta_F\\) to achieve desired frequentist properties if regulatory requirements demand it.\nBerry emphasized that Bayesian methods do not require such calibration for logical validity, the posterior probability is always a coherent measure of evidence regardless of the stopping rule. However, demonstrating acceptable operating characteristics facilitates regulatory acceptance and provides assurance that the design performs well across plausible scenarios.\nThe FDA has increasingly recognized the value of Bayesian methods in drug development, issuing guidance documents on their use (U.S. Food and Drug Administration 2010). Berry Consultants has designed hundreds of adaptive trials using these methods, demonstrating their practical viability across therapeutic areas including oncology, cardiovascular disease, and rare diseases.\n\nExample 6.10 (Bayesian Sequential Trial for a Rare Disease) Consider a Phase II trial for a rare autoimmune condition where the standard of care has a 30% response rate. A new therapy is hypothesized to improve response to 50%. Due to the rarity of the condition, we plan for a maximum of 60 patients (30 per arm) with interim analyses every 10 patients per arm.\nWe use weakly informative Beta(1,1) priors for both response rates. The efficacy threshold is \\(\\theta_E = 0.95\\) for declaring \\(P(p_T &gt; p_C \\mid \\text{data}) &gt; 0.95\\), and the futility threshold is \\(\\theta_F = 0.10\\).\n\n\nSimulate Bayesian Sequential Trial\nset.seed(2024)\n\n# True parameters (unknown to trialists)\np_control_true &lt;- 0.30\np_treatment_true &lt;- 0.50\n\n# Prior parameters (Beta(1,1) = uniform)\nalpha_prior &lt;- 1\nbeta_prior &lt;- 1\n\n# Trial parameters\nn_max_per_arm &lt;- 30\ninterim_points &lt;- c(10, 20, 30)  # Patients per arm at each look\nn_sims &lt;- 10000  # Monte Carlo samples for posterior probability\n\n# Efficacy and futility thresholds\ntheta_E &lt;- 0.95\ntheta_F &lt;- 0.10\n\n\n\n# Simulate patient outcomes\nn_control &lt;- n_max_per_arm\nn_treatment &lt;- n_max_per_arm\ncontrol_outcomes &lt;- rbinom(n_control, 1, p_control_true)\ntreatment_outcomes &lt;- rbinom(n_treatment, 1, p_treatment_true)\n\nNow we perform the sequential analysis according to the rules we have set up. We stop the trial if the posterior probability of the treatment being superior to the control is greater than 0.95 or less than 0.10.\nThe sequential analysis proceeds as follows. At each interim analysis, we compute the posterior distributions for both response rates using the accumulated data. For the control arm, the posterior is \\(\\text{Beta}(\\alpha_{\\text{prior}} + x_C, \\beta_{\\text{prior}} + n - x_C)\\), where \\(x_C\\) is the number of responders observed. Similarly, for the treatment arm, the posterior is \\(\\text{Beta}(\\alpha_{\\text{prior}} + x_T, \\beta_{\\text{prior}} + n - x_T)\\).\nTo evaluate the probability that the treatment response rate exceeds the control rate, \\(P(p_T &gt; p_C \\mid \\text{data})\\), we draw Monte Carlo samples from both posterior distributions and compute the proportion of samples where \\(p_T &gt; p_C\\). This probability is then compared against our decision thresholds: if it exceeds \\(\\theta_E = 0.95\\), we stop for efficacy; if it falls below \\(\\theta_F = 0.10\\), we stop for futility; otherwise, we continue enrolling patients.\nThe table below shows the results at each interim analysis, including the accumulated number of responders in each arm and the posterior probability of treatment superiority:\n\n\nSequential Analysis\n# Storage for results\nresults &lt;- data.frame(\n  interim = integer(),\n  n_per_arm = integer(),\n  x_control = integer(),\n  x_treatment = integer(),\n  p_superior = numeric(),\n  decision = character()\n)\n\n# Conduct sequential analysis\nfinal_decision &lt;- \"Continue\"\nfor (k in seq_along(interim_points)) {\n  n_current &lt;- interim_points[k]\n  \n  # Accumulated data\n  x_C &lt;- sum(control_outcomes[1:n_current])\n  x_T &lt;- sum(treatment_outcomes[1:n_current])\n  \n  # Posterior parameters\n  alpha_C_post &lt;- alpha_prior + x_C\n  beta_C_post &lt;- beta_prior + n_current - x_C\n  alpha_T_post &lt;- alpha_prior + x_T\n  beta_T_post &lt;- beta_prior + n_current - x_T\n  \n  # Monte Carlo estimate of P(p_T &gt; p_C | data)\n  p_C_samples &lt;- rbeta(n_sims, alpha_C_post, beta_C_post)\n  p_T_samples &lt;- rbeta(n_sims, alpha_T_post, beta_T_post)\n  p_superior &lt;- mean(p_T_samples &gt; p_C_samples)\n  \n  # Decision\n  if (p_superior &gt; theta_E) {\n    decision &lt;- \"Stop for Efficacy\"\n    final_decision &lt;- decision\n  } else if (p_superior &lt; theta_F) {\n    decision &lt;- \"Stop for Futility\"\n    final_decision &lt;- decision\n  } else {\n    decision &lt;- \"Continue\"\n  }\n  \n  results &lt;- rbind(results, data.frame(\n    interim = k,\n    n_per_arm = n_current,\n    x_control = x_C,\n    x_treatment = x_T,\n    p_superior = round(p_superior, 4),\n    decision = decision\n  ))\n  \n  if (decision != \"Continue\") break\n}\n\n# Display results\nknitr::kable(results, \n             col.names = c(\"Interim\", \"N per arm\", \"Control successes\", \n                          \"Treatment successes\", \"P(Treatment &gt; Control)\", \"Decision\"),\n             caption = \"Sequential analysis results\")\n\n\n\nSequential analysis results\n\n\n\n\n\n\n\n\n\n\nInterim\nN per arm\nControl successes\nTreatment successes\nP(Treatment &gt; Control)\nDecision\n\n\n\n\n1\n10\n3\n5\n0.81\nContinue\n\n\n2\n20\n8\n11\n0.83\nContinue\n\n\n3\n30\n11\n14\n0.78\nContinue\n\n\n\n\n\nThe trial data shows the accumulation of evidence across interim analyses. Let us visualize the posterior distributions at the final interim analysis:\n\n\nPlot Posterior Distributions at Final Analysis\n# Plot posterior distributions at final analysis\nfinal_n &lt;- results$n_per_arm[nrow(results)]\nfinal_x_C &lt;- results$x_control[nrow(results)]\nfinal_x_T &lt;- results$x_treatment[nrow(results)]\n\n# Posterior parameters\nalpha_C_final &lt;- alpha_prior + final_x_C\nbeta_C_final &lt;- beta_prior + final_n - final_x_C\nalpha_T_final &lt;- alpha_prior + final_x_T\nbeta_T_final &lt;- beta_prior + final_n - final_x_T\n\n# Create plot\np_grid &lt;- seq(0, 1, length.out = 500)\npar(mar = c(4, 4, 2, 1))\nplot(p_grid, dbeta(p_grid, alpha_T_final, beta_T_final), type = \"l\", \n     col = \"blue\", lwd = 2, ylim = c(0, 8),\n     xlab = \"Response probability\", ylab = \"Posterior density\",\n     main = \"Posterior distributions at final analysis\")\nlines(p_grid, dbeta(p_grid, alpha_C_final, beta_C_final), col = \"red\", lwd = 2)\nabline(v = p_control_true, col = \"red\", lty = 2)\nabline(v = p_treatment_true, col = \"blue\", lty = 2)\nlegend(\"topright\", legend = c(\"Treatment\", \"Control\", \"True values\"),\n       col = c(\"blue\", \"red\", \"gray\"), lwd = c(2, 2, 1), lty = c(1, 1, 2), bty=\"n\", cex=0.8)\n\n\n\n\n\n\n\n\n\nWe can also examine the posterior distribution of the treatment effect (risk difference):\n\n# Posterior of risk difference via Monte Carlo\nset.seed(123)\np_C_samples &lt;- rbeta(50000, alpha_C_final, beta_C_final)\np_T_samples &lt;- rbeta(50000, alpha_T_final, beta_T_final)\nrisk_diff &lt;- p_T_samples - p_C_samples\n\n\n## Posterior summary for risk difference (p_T - p_C):\n##   Mean: 0.095\n##   95% Credible Interval: [ -0.14 , 0.33 ]\n##   P(p_T &gt; p_C): 0.78\n##   P(p_T &gt; p_C + 0.10): 0.49\n\n\n\n\n\n\n\n\n\n\nExample 6.11 (Operating Characteristics via Simulation) To understand how the Bayesian sequential design performs across different scenarios, we simulate its operating characteristics. This involves running many hypothetical trials under various true parameter values and recording the outcomes. The simulation proceeds by generating a large number of hypothetical trials under each scenario. For each trial, patient outcomes are generated according to the true response probabilities, and the sequential monitoring procedure is applied at each interim analysis point. At each look, posterior distributions are updated and the probability that treatment is superior to control is computed via Monte Carlo sampling. The trial stops early for efficacy if this probability exceeds the efficacy threshold \\(\\theta_E = 0.95\\), or for futility if it falls below the futility threshold \\(\\theta_F = 0.10\\). If neither stopping criterion is met at any interim analysis, the trial continues to the maximum sample size.\nThe simulation tracks three key operating characteristics: the decision reached (efficacy, futility, or maximum sample size), the final sample size at which the trial stopped, and the posterior probability of treatment superiority at that point. By replicating this process across many trials under different true parameter values—ranging from no treatment effect to large effects—we can assess the design’s ability to correctly identify effective treatments, avoid false positives, and efficiently use resources by stopping early when appropriate.\nThe simulation code implements this procedure by defining a simulate_trial() function that generates patient outcomes under specified true response probabilities, then applies the sequential monitoring rules at interim points (\\(n = 10, 20, 30\\) per arm). For each interim analysis, it draws Monte Carlo samples from the posterior distributions of \\(p_C\\) and \\(p_T\\) and computes the probability of treatment superiority. The trial stops early for efficacy if this probability exceeds \\(\\theta_E = 0.95\\), for futility if it falls below \\(\\theta_F = 0.10\\), or continues to the maximum sample size otherwise.\nFour scenarios are evaluated by running 1,000 simulated trials under each: no treatment effect (\\(p_T = 0.30 = p_C\\)), small effect (\\(p_T = 0.40\\) vs \\(p_C = 0.30\\)), moderate effect (\\(p_T = 0.50\\)), and large effect (\\(p_T = 0.60\\)). The operating characteristics—probability of declaring efficacy, probability of stopping for futility, and expected sample size per arm—are computed by aggregating outcomes across the replicated trials.\n\n\n\nTable 6.2: Operating characteristics of the Bayesian sequential design\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenario\nTrue p_C\nTrue p_T\nP(Efficacy)\nP(Futility)\nE[N per arm]\n\n\n\n\nNull (no effect)\n0.3\n0.3\n0.095\n0.181\n26.28\n\n\nSmall effect\n0.3\n0.4\n0.281\n0.071\n25.53\n\n\nModerate effect\n0.3\n0.5\n0.543\n0.028\n22.91\n\n\nLarge effect\n0.3\n0.6\n0.807\n0.002\n19.59\n\n\n\n\n\n\nThe results in Table 6.2 reveal several important properties of the sequential design. Under the null hypothesis (no treatment effect), the probability of declaring efficacy is 9.5%, which represents the Type I error rate—the false positive rate when there is truly no benefit. This exceeds the traditional 5% threshold, reflecting the fact that the sequential procedure with its multiple interim looks increases the chance of a spurious efficacy declaration. If stricter Type I error control were required, the efficacy threshold \\(\\theta_E\\) could be raised above 0.95.\nThe power of the design—the probability of correctly identifying an effective treatment—increases substantially as the true treatment effect grows: 28.1% for the small effect, 54.3% for moderate, and 80.7% for large effects. The relatively modest power for small effects (10 percentage point difference in response rates) reflects the inherent difficulty of detecting subtle improvements with limited sample sizes. More patients would be needed to reliably identify small but clinically meaningful benefits.\nPerhaps most striking is the expected sample size, which demonstrates the efficiency gains from sequential stopping. Under the null hypothesis, the average trial uses only 26.3 patients per arm rather than the maximum of 30, as 18.1% of trials stop early for futility. When the treatment effect is large, the average sample size drops to just 19.6 per arm, with 80.7% of trials stopping early for efficacy. This adaptive efficiency means fewer patients are exposed to inferior treatments and trial results become available sooner, accelerating the translation of effective therapies into clinical practice.\n\n\n\n\nSequential Analysis for Rare Diseases\nSequential and adaptive designs are particularly valuable for clinical trials in rare diseases, where small patient populations make traditional fixed-sample designs impractical. With potentially only hundreds or even dozens of patients worldwide having a particular condition, every enrolled patient provides precious information that must be used efficiently.\nSeveral innovative approaches have emerged for rare disease trials. The small n Sequential Multiple Assignment Randomized Trial (snSMART) design (Wei et al. 2018) provides a framework for testing multiple treatments within a single trial, re-randomizing non-responding patients to alternative therapies in subsequent stages. This design achieves increased statistical power over traditional single-stage designs by leveraging information from all treatment sequences.\nHilgers, Roes, and Stallard (2016) provided an overview of design options for achieving valid randomized clinical trials in rare diseases, emphasizing that sequential procedures can substantially reduce the expected sample size while maintaining statistical validity. Their work demonstrated that group sequential designs and adaptive approaches can cut average trial sizes by 30-50% compared to fixed designs.\nThe application of Bayesian methods specifically to rare disease trials has been systematically reviewed by Chen et al. (2022), who identified both the opportunities and hurdles in this setting. The Bayesian framework naturally accommodates informative priors based on historical data, external controls, or expert elicitation, which is particularly valuable when patient populations are too small for purely data-driven inference. However, the authors caution that prior sensitivity analyses are essential when sample sizes are small, as the prior can dominate posterior conclusions.\nBenda et al. (2016) evaluated the performance of sequential methods specifically in small-sample settings with normally distributed responses. They found that careful attention to the distribution of test statistics is necessary to maintain nominal Type I error rates, as large-sample approximations may not hold. Their recommendations provide practical guidance for implementing sequential designs in trials with limited participants.\n\n\nMulti-Armed Bandit Experiments\nA closely related approach to sequential analysis is the multi-armed bandit, a framework that has revolutionized how online experiments are conducted in the digital economy. While classical A/B tests require predetermining sample sizes and waiting for experiments to conclude before making decisions, multi-armed bandits adaptively allocate traffic based on accumulating evidence. This approach, implemented in platforms like Google Analytics Content Experiments (Steven L. Scott 2013; Steven L. Scott 2015), represents a practical application of Bayesian sequential analysis at massive scale.\nA multi-armed bandit is a type of experiment characterized by two key features: the goal is to find the best or most profitable action (or arm), and the randomization distribution can be updated as the experiment progresses. The colorful name originates from a hypothetical scenario where a gambler faces several slot machines (colloquially called one-armed bandits) with potentially different, unknown payoff rates. The gambler wants to find the machine with the best payout, but also wants to maximize winnings during the search. This creates the fundamental tension between exploiting arms that have performed well in the past and exploring new or seemingly inferior arms that might perform even better.\n\nThe Bayesian Approach to Bandits\nThe Bayesian framework provides an elegant solution to the bandit problem. Suppose we have \\(K\\) arms (variations), each with an unknown success probability \\(\\theta_k\\) for \\(k = 1, \\ldots, K\\). Using Bayes’ theorem, we can compute the probability that each arm is the best based on observed data. Let \\(x_k\\) denote the number of successes and \\(n_k\\) the number of trials for arm \\(k\\). With a Beta prior \\(\\theta_k \\sim \\text{Beta}(\\alpha, \\beta)\\), the posterior after observing data is \\[\n\\theta_k \\mid x_k, n_k \\sim \\text{Beta}(\\alpha + x_k, \\beta + n_k - x_k).\n\\]\nThe probability that arm \\(k\\) is optimal is \\[\nP(\\theta_k &gt; \\theta_j \\text{ for all } j \\neq k \\mid \\text{data}),\n\\] which can be computed via Monte Carlo integration by sampling from the posterior distributions and counting how often arm \\(k\\) produces the largest sample.\nThe key insight from Steven L. Scott (2015) is that these posterior probabilities of optimality can serve directly as allocation weights. An arm that appears to be doing well receives more traffic, while an arm that is clearly underperforming receives less. The adjustments consider sample size and performance metrics together, providing confidence that we are adjusting for real performance differences rather than random chance.\n\n\nThompson Sampling\nThe algorithm that implements this Bayesian allocation is called Thompson sampling (Thompson 1933), one of the oldest heuristics for the bandit problem, dating to 1933. For binary outcomes with Beta priors, the algorithm proceeds as follows:\n\nInitialize \\(\\alpha_k = \\beta_k = 1\\) for each arm (uniform prior).\nAt each decision point, sample \\(\\tilde{\\theta}_k\\) from the current posterior \\(\\text{Beta}(\\alpha_k, \\beta_k)\\) for each arm.\nSelect the arm with the highest sampled value: \\(a_t = \\arg\\max_k \\tilde{\\theta}_k\\).\nObserve the outcome \\(Y_t \\in \\{0, 1\\}\\) and update: \\(\\alpha_{a_t} \\leftarrow \\alpha_{a_t} + Y_t\\), \\(\\beta_{a_t} \\leftarrow \\beta_{a_t} + (1 - Y_t)\\).\n\nThompson sampling automatically balances exploration and exploitation. Arms with high posterior means are selected frequently (exploitation), but arms with high posterior variance, which indicates uncertainty, also have a chance of producing high samples (exploration). As data accumulates, posteriors concentrate around true values, and the algorithm increasingly exploits the best arm.\n\n\nBenefits Over Classical A/B Testing\nExperiments based on multi-armed bandits are typically much more efficient than classical A/B experiments based on hypothesis testing. As Steven L. Scott (2013) explains, they are just as statistically valid but can produce answers far more quickly. The efficiency gains arise from two sources: traffic moves toward winning variations gradually rather than waiting for a final answer, and samples that would have gone to obviously inferior variations can be assigned to potential winners.\nConsider a concrete example from Steven L. Scott (2013). Suppose a website has a 4% conversion rate, and a new variation actually converts at 5%. A standard power calculation for detecting this difference at 95% confidence requires 22,330 observations (11,165 per arm). At 100 visits per day, this experiment would take 223 days to complete. In a classical experiment, you wait 223 days, run the hypothesis test, and get your answer.\nWith a multi-armed bandit, the experiment can finish much sooner. In simulations, the bandit found the correct arm in 96.4% of cases (about the same error rate as the classical test), but the average experiment duration was only 66 days, saving 157 days of testing. The savings compound when experiments have more arms, because the classical approach requires Bonferroni-type corrections for multiple comparisons while the bandit naturally handles multiple arms through the posterior probability framework.\n\nExample 6.12 (Reproducing Scott’s Bandit Experiment) We reproduce the simulation from Steven L. Scott (2013). The setup involves an original page with 4% conversion rate and a variation with 5% conversion rate, with 100 visits per day. A classical power calculation requires 223 days; we will see how the bandit performs.\n\n\nSingle Trial Simulation\nset.seed(42)\n\n# True conversion rates\ntheta_true &lt;- c(0.04, 0.05)  # Original, Variation\nK &lt;- 2\narm_names &lt;- c(\"Original\", \"Variation\")\n\n# Experiment parameters\nvisits_per_day &lt;- 100\nmax_days &lt;- 300\nclassical_days &lt;- 223  # From power calculation\n\n# Initialize Beta(1,1) priors\nalpha_params &lt;- c(1, 1)\nbeta_params &lt;- c(1, 1)\n\n# Storage\nprob_optimal_history &lt;- matrix(0, max_days, K)\nn_mc &lt;- 10000\n\n# Run single trial\nend_day &lt;- max_days\nfor (day in 1:max_days) {\n  # Compute probability each arm is optimal\n  theta_samples &lt;- cbind(\n    rbeta(n_mc, alpha_params[1], beta_params[1]),\n    rbeta(n_mc, alpha_params[2], beta_params[2])\n  )\n  prob_optimal &lt;- colMeans(theta_samples == apply(theta_samples, 1, max))\n  prob_optimal_history[day, ] &lt;- prob_optimal\n  \n  # Allocate today's traffic using Thompson sampling\n  for (v in 1:visits_per_day) {\n    theta_sampled &lt;- rbeta(K, alpha_params, beta_params)\n    a &lt;- which.max(theta_sampled)\n    y &lt;- rbinom(1, 1, theta_true[a])\n    alpha_params[a] &lt;- alpha_params[a] + y\n    beta_params[a] &lt;- beta_params[a] + (1 - y)\n  }\n  \n  # Check stopping: 95% confident one arm is best\n  if (max(prob_optimal) &gt; 0.95) {\n    end_day &lt;- day\n    break\n  }\n}\n\n# Plot: Optimal arm probabilities over time\npar(mfrow = c(1, 2))\nplot(1:end_day, prob_optimal_history[1:end_day, 1], type = \"l\", \n     col = \"black\", lwd = 2, ylim = c(0, 1),\n     xlab = \"Time period\", ylab = \"Probability of being optimal\",\n     main = \"Optimal Arm Probabilities\")\nlines(1:end_day, prob_optimal_history[1:end_day, 2], col = \"red\", lwd = 2, lty = 2)\nabline(h = c(0.05, 0.95), lty = 3, col = \"gray40\")\nlegend(\"right\", legend = arm_names, col = c(\"black\", \"red\"), \n       lwd = 2, lty = c(1, 2), cex = 0.8, bty = \"n\")\n\n# True success rates\nbarplot(theta_true, names.arg = 1:2, col = c(\"black\", \"red\"),\n        xlab = \"Arm\", ylab = \"\", main = \"True success rate\",\n        ylim = c(0, 0.06))\n\n\n\n\n\n\n\n\n\nSingle Trial Simulation\n\ncat(\"Experiment ended on day:\", end_day, \"\\n\")\n## Experiment ended on day: 46\ncat(\"Days saved vs classical:\", classical_days - end_day, \"\\n\")\n## Days saved vs classical: 177\n\n\nThe left panel shows the probability that each arm is optimal over time. The two curves are complementary (summing to 1) and fluctuate until eventually one crosses the 95% threshold. The right panel shows the true success rates that are unknown to the algorithm.\nTo understand the distribution of outcomes, we run 500 simulations and compare to the classical experiment:\n\n\n500 Simulation Runs\nset.seed(2024)\n\n# Parameters\ntheta_true &lt;- c(0.04, 0.05)\nK &lt;- 2\nvisits_per_day &lt;- 100\nclassical_days &lt;- 223\nn_simulations &lt;- 500\nn_mc &lt;- 5000\n\n# Function to run one bandit trial\nrun_bandit_trial &lt;- function(theta_true, visits_per_day, max_days = 300) {\n  alpha_params &lt;- c(1, 1)\n  beta_params &lt;- c(1, 1)\n  total_conversions &lt;- 0\n  \n  for (day in 1:max_days) {\n    # Compute probability each arm is optimal\n    theta_samples &lt;- cbind(\n      rbeta(n_mc, alpha_params[1], beta_params[1]),\n      rbeta(n_mc, alpha_params[2], beta_params[2])\n    )\n    prob_optimal &lt;- colMeans(theta_samples == apply(theta_samples, 1, max))\n    \n    # Allocate traffic\n    for (v in 1:visits_per_day) {\n      theta_sampled &lt;- rbeta(2, alpha_params, beta_params)\n      a &lt;- which.max(theta_sampled)\n      y &lt;- rbinom(1, 1, theta_true[a])\n      alpha_params[a] &lt;- alpha_params[a] + y\n      beta_params[a] &lt;- beta_params[a] + (1 - y)\n      total_conversions &lt;- total_conversions + y\n    }\n    \n    # Check stopping\n    if (max(prob_optimal) &gt; 0.95) {\n      winner &lt;- which.max(prob_optimal)\n      return(list(day = day, conversions = total_conversions, \n                  winner = winner, correct = (winner == 2)))\n    }\n  }\n  winner &lt;- which.max(prob_optimal)\n  return(list(day = max_days, conversions = total_conversions, \n              winner = winner, correct = (winner == 2)))\n}\n\n# Run simulations\nresults &lt;- lapply(1:n_simulations, function(i) run_bandit_trial(theta_true, visits_per_day))\n\n# Extract results\nbandit_days &lt;- sapply(results, function(x) x$day)\nbandit_conversions &lt;- sapply(results, function(x) x$conversions)\ncorrect_arm &lt;- sapply(results, function(x) x$correct)\n\n# Classical experiment: fixed allocation for 223 days\n# Expected conversions per day: 50 * 0.04 + 50 * 0.05 = 4.5\nclassical_conversions_per_day &lt;- visits_per_day * mean(theta_true)\nclassical_total_conversions &lt;- classical_days * classical_conversions_per_day\n\n# For bandit, compute expected conversions if we ran to classical_days\n# with optimal allocation after stopping\nexpected_bandit_conversions &lt;- bandit_conversions + \n  (classical_days - bandit_days) * visits_per_day * max(theta_true)\n\n# Days saved and conversions saved\ndays_saved &lt;- classical_days - bandit_days\nconversions_saved &lt;- expected_bandit_conversions - classical_total_conversions\n\n# Plot histograms\npar(mfrow = c(1, 2), mar = c(4, 4, 3, 1))\n\nhist(days_saved, breaks = 20, col = \"lightgreen\", border = \"white\",\n     main = \"Days of Testing Saved\", xlab = \"Number of Days\",\n     ylab = \"Frequency\", xlim = c(-50, 250))\nabline(v = mean(days_saved), col = \"darkgreen\", lwd = 2, lty = 2)\n\nhist(conversions_saved, breaks = 20, col = \"lightgreen\", border = \"white\",\n     main = \"Conversions Saved\", xlab = \"Number of Conversions\",\n     ylab = \"Frequency\")\nabline(v = mean(conversions_saved), col = \"darkgreen\", lwd = 2, lty = 2)\n\n# Summary statistics\ncat(\"Simulation results (n =\", n_simulations, \"):\\n\")\ncat(\"  Correct arm identified:\", sum(correct_arm), \"/\", n_simulations, \n    \"(\", round(100*mean(correct_arm), 1), \"%)\\n\")\ncat(\"  Average days to finish:\", round(mean(bandit_days), 1), \"\\n\")\ncat(\"  Average days saved:\", round(mean(days_saved), 1), \"\\n\")\ncat(\"  Average conversions saved:\", round(mean(conversions_saved), 1), \"\\n\")\n\n\n\n\n\n\n\n\nFigure 6.2: Distribution of days saved and conversions saved\n\n\n\nThe histograms in Figure 6.2 show the distribution of days saved (left) and conversions saved (right) compared to the classical 223-day experiment. The key results from 500 simulations are summarized in Table 6.3:\n\n\n\nTable 6.3: Simulation results for multi-armed bandit vs. classical experiment (n = 500)\n\n\n\n\n\nMetric\nValue\n\n\n\n\nCorrect arm identified\n469 / 500 (93.8%)\n\n\nAverage days to finish\n46.9\n\n\nAverage days saved\n176.1\n\n\nAverage conversions saved\n99.1\n\n\n\n\n\n\nOn average, the bandit saves approximately 176 days of testing (about 79% reduction) while achieving the same statistical validity (finding the correct arm 93.8% of the time, comparable to the 95% confidence level of the classical test). The conversion savings come from two sources: ending the experiment earlier and allocating more traffic to the better-performing variation during the experiment.\n\n\n\nStopping Rules, Value Remaining, and Regret\nSeveral questions arise in practice: when should we stop the experiment? How do we quantify the potential benefit of continuing? Steven L. Scott (2015) describes two complementary stopping criteria. The first is the probability that each variation beats the original; if we are 95% confident that a variation is best, a winner can be declared. The second criterion is the value remaining in the experiment, which measures the expected improvement from switching away from the current champion. When there is at least a 95% probability that the value remaining is less than 1% of the champion’s conversion rate, the experiment can end.\nAnother perspective is regret, the cumulative cost of not always selecting the optimal arm. These concepts, along with detailed mathematical treatment and code examples for computing stopping criteria, are covered in Chapter 9 (see the section on Multi-Armed Bandits, specifically “When to End Experiments”).\nThis framework also extends to clinical trials, where Villar, Bowden, and Wason (2015) reviewed how response-adaptive randomization can reduce the number of patients receiving inferior treatments. For contextual bandits, design considerations, and extensions to reinforcement learning, see Chapter 9.\n\n\n\nPractical Considerations\nSequential analysis requires careful consideration of several practical issues. First, the definition of sufficient evidence must be specified in advance, whether through posterior probability thresholds, Bayes factor bounds, or expected utility calculations. Second, the frequency of interim analyses affects operational aspects of trials, including regulatory interactions and data monitoring committee responsibilities. Third, the potential for early stopping must be balanced against the need for long-term safety data and secondary endpoint analyses.\nThe choice between Bayesian and frequentist sequential methods often depends on regulatory requirements and institutional preferences. However, the coherence of Bayesian inference under optional stopping, the natural incorporation of prior information, and the interpretability of posterior probabilities make Bayesian sequential analysis an increasingly attractive option, particularly in challenging settings like rare disease trials where traditional approaches are impractical.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#examples-and-paradoxes",
    "href": "06-hyp.html#examples-and-paradoxes",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.6 Examples and Paradoxes",
    "text": "6.6 Examples and Paradoxes\nThis section provides a number of paradoxes arising when using different hypothesis testing procedures. The common strands of the examples will be discussed at the end of the section.\n\nExample 6.13 (Neyman-Pearson tests) Consider testing \\(H_{0}:\\mu=\\mu_{0}\\) versus \\(H_{1}:\\mu=\\mu_{1}\\), \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,\\sigma^{2}\\right)\\) and \\(\\mu_{1}&gt;\\mu_{0}\\). For this simple test, the likelihood ratio is given by \\[\n\\mathcal{LR}_{0,1}=\\frac{\\exp\\left(  -\\frac{1}{2\\sigma^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{0}\\right)  ^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2\\sigma\n^{2}}%\n%TCIMACRO{\\tsum \\nolimits_{t=1}^{T}}%\n%BeginExpansion\n{\\textstyle\\sum\\nolimits_{t=1}^{T}}\n%EndExpansion\n\\left(  y_{t}-\\mu_{1}\\right)  ^{2}\\right)  }=\\exp\\left(  -\\frac{T}{\\sigma^{2}%\n}\\left(  \\mu_{1}-\\mu_{0}\\right)  \\left(  \\overline{y}-\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)  \\right)  \\right)  \\text{.}%\n\\] Since \\(\\mathrm{BF}_{0,1}=\\mathcal{LR}_{0,1}\\), assuming equal prior probabilities and symmetric losses, the Bayesian accepts \\(H_{0}\\) if \\(\\mathrm{BF}_{0,1}&gt;1\\). Thus, the Bayes procedure rejects \\(H_{0}\\) if \\(\\overline{y}&gt;\\frac{1}{2}\\left( \\mu_{0}+\\mu_{1}\\right)\\) for any \\(T\\) and \\(\\sigma^{2}\\), with \\(\\mu_{0}\\),\\(\\mu_{1}\\), \\(T,\\)and \\(\\sigma^{2}\\) determining the strength of the rejection. If \\(\\mathrm{BF}_{0,1}=1\\), there is equal evidence for the two hypotheses.\nThe NP procedure proceeds by first setting \\(\\alpha=0.05,\\) and rejects when \\(\\mathcal{LR}_{0,1}\\) is large. This is equivalent to rejecting when \\(\\overline{y}\\) is large, generating an `optimal’ rejection region of the form \\(\\overline{y}&gt;c\\). The cutoff value \\(c\\) is calibrated via the size of the test, \\[\nP \\left[  reject\\text{ }H_{0} \\mid H_{0}\\right]\n=P \\left[  \\overline{y}&gt;c \\mid \\mu_{0}\\right]  =P \\left[\n\\frac{\\left(  \\overline{y}-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}}&gt;\\frac{\\left( c-\\mu_{0}\\right)  }{\\sigma/\\sqrt{T}} \\mid H_{0}\\right] .\n\\] The size equals \\(\\alpha\\) if \\(\\sqrt{T}\\left( c-\\mu_{0}\\right) /\\sigma =z_{\\alpha}\\). Thus, the NP test rejects if then if \\(\\overline{y}&gt;\\mu _{0}+\\sigma z_{\\alpha}/\\sqrt{T}\\). Notice that the test rejects regardless of the value of \\(\\mu_{1}\\), which is rather odd, since \\(\\mu_{1}\\) does not enter into the size of the test only the power. The probability of a type II error is \\[\n\\beta=P \\left[  \\text{accept }H_{0} \\mid H_{1}\\right]\n=P \\left[  \\overline{y}\\leq\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}}z_{\\alpha\n} \\mid H_{1}\\right]  =\\int_{-\\infty}^{\\mu_{0}+\\frac{\\sigma}{\\sqrt{T}%\n}z_{\\alpha}}p\\left(  \\overline{y} \\mid \\mu_{1}\\right)  d\\overline{y}\\text{,}%\n\\] where \\(p\\left( \\overline{y} \\mid \\mu_{1}\\right) \\sim\\mathcal{N}\\left( \\mu _{1},\\sigma^{2}/T\\right)\\).\nThese tests can generate strikingly different conclusions. Consider a test of \\(H_{0}:\\mu=0\\) versus \\(H_{1}:\\mu=5\\), based on \\(T=100\\) observations drawn from \\(y_{t}\\sim\\mathcal{N}\\left( \\mu,10^{2}\\right)\\) with \\(\\overline{y}=2\\). For NP, since \\(\\sigma/\\sqrt{T}=1\\), \\(\\overline{y}\\) is two standard errors away from \\(0\\), thus \\(H_{0}\\) is rejected at the 5% level (the same conclusion holds for \\(p-\\)values). Since \\(p(\\overline {y}=2 \\mid H_{0})=0.054\\) and \\(p(\\overline{y}=2 \\mid H_{1})=0.0044\\), the Bayes factor is \\(\\mathrm{BF}_{0,1}=12.18\\) and \\(P \\left( H_{0} \\mid y\\right) =92.41\\%\\). Thus, the Bayesian is quite sure the null is true, while Neyman-Pearson reject the null.\nThe paradox can be seen in two different ways. First, although \\(\\overline{y}\\) is actually closer to \\(\\mu_{0}\\) than \\(\\mu_{1}\\), the NP test rejects \\(H_{0}\\). This is counterintuitive and makes little sense. The problem is one of calibration. The classical approach develops a test such that 5% of the time, a correct null would be rejected. The power of the test is easy to compute and implies that \\(\\beta=0.0012\\). Thus, this testing procedure will virtually never accept the null if the alternative is correct. For Bayesian procedure, assuming the prior odds is \\(1\\) and \\(L_{0}=L_{1}\\), then \\(\\alpha=\\beta=0.0062\\). Notice that the overall probability of making an error is 1.24% in the Bayesian procedure compared to 5.12% in the classical procedure. It should seem clear that the Bayesian approach is more reasonably, absent a specific motivation for inflating \\(\\alpha\\). Second, suppose the null and alternative were reversed, testing \\(H_{0}:\\mu=\\mu_{1}\\) versus \\(H_{1}:\\mu=\\mu_{0}\\) In the previous example, the Bayes approach gives the same answer, while NP once again rejects the null hypothesis! Again, this result is counterintuitive and nonsensical, but is common when arbitrarily fixing \\(\\alpha\\), which essentially hardwires the test to over-reject the null.\n\n\nExample 6.14 (Lindley’s paradox) Consider the case of testing whether or not a coin is fair, based on observed coin flips, \\[\nH_{0}:\\theta=\\frac{1}{2}\\text{ versus }H_{1}:\\theta\n\\neq\\frac{1}{2}\\text{,}%\n\\] based on \\(T\\) observations from \\(y_{t}\\sim Ber\\left( \\theta\\right)\\). As an example, Table 6.4 provides 4 datasets of differing lengths. Prior to considering the formal hypothesis tests, form your own opinion on the strength of evidence regarding the hypothesis in each data set. It is common for individuals, when confronted with this data to conclude that the fourth sample provides the strongest of evidence for the null and the first sample the weakest.\n\n\n\nTable 6.4: Lindley’s paradox\n\n\n\n\n\n\n#1\n#2\n#3\n#4\n\n\n\n\n# Flips\n50\n100\n400\n10,000\n\n\n# Heads\n32\n60\n220\n5098\n\n\nPercentage of heads\n64\n60\n55\n50.98\n\n\n\n\n\n\nFisher’s solution to the problem posits an unbiased estimator, the sample mean, and computes the \\(t-\\)statistic, which is calculated under \\(H_{0}\\): \\[\nt\\left(  y\\right)  =\\frac{\\overline{y}-E\\left[  \\overline{y} \\mid \\theta\n_{0}\\right]  }{se\\left(  \\overline{y}\\right)  }=\\sqrt{T}\\left(  2\\widehat\n{\\theta}-1\\right)  \\text{,}%\n\\] where \\(se\\left(\\overline{y}\\right)\\) is the standard error of \\(\\overline{y}\\). The Bayesian solution requires marginal likelihood under the null and alternative, which are \\[\np\\left(  y \\mid \\theta_{0}=1/2\\right)  =\\prod_{t=1}^{T}p\\left(  y_{t} \\mid \\theta\n_{0}\\right)  =\\left(  \\frac{1}{2}\\right)  ^{\\sum_{t=1}^{T}y_{t}}\\left( \\frac{1}{2}\\right)  ^{T-\\sum_{t=1}^{T}y_{t}}=\\left(  \\frac{1}{2}\\right)  ^{T},\n\\tag{6.1}\\] and, from Equation 6.1, \\(p\\left( y \\mid H_{1}\\right) =B\\left( \\alpha_{T},\\beta_{T}\\right) /B\\left(\\alpha,\\beta\\right)\\) assuming a beta prior distribution.\nTo compare the results, note first that in the datasets given above, \\(\\widehat{\\theta}\\) and \\(T\\) generate \\(t_{\\alpha}=1.96\\) in each case. Thus, for a significance level of \\(\\alpha=5\\%\\), the null is rejected for each sample size. Assuming a flat prior distribution, the Bayes factors are \\[\n\\mathrm{BF}_{0,1}=\\left\\{\n\\begin{array}\n[c]{l}%\n0.8178\\text{ for }N=50\\text{ }\\\\\n1.0952\\text{ for }N=100\\\\\n2.1673\\text{ for }N=400\\\\\n11.689\\text{ for }N=10000\n\\end{array}\n\\right.  ,\n\\] showing increasingly strong evidence in favor of \\(H_{0}\\). Assuming equal prior weight for the hypotheses, the posterior probabilities are 0.45, 0.523, 0.684, and 0.921, respectively. For the smallest samples, the Bayes factor implies roughly equal odds of the null and alternative. As the sample size increase, the weight of evidence favors the null, with a 92% probability for \\(N=10K\\).\nNext, consider testing \\(H_{0}:\\theta_{0}=0\\) vs. \\(H_{1}:\\theta_{0}\\neq0,\\) based on \\(T\\) observations from \\(y_{t}\\sim \\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), where \\(\\sigma^{2}\\) is known. This is the formal example used by Lindley to generate his paradox. Using \\(p-\\)values, the hypothesis is rejected if the \\(t-\\)statistic is greater than \\(t_{\\alpha}\\). To generate the paradox, consider datasets that are exactly \\(t_{\\alpha}\\) standard errors away from \\(\\overline{y}\\), that is, \\(\\overline {y}^{\\ast}=\\theta_{0}+\\sigma t_{\\alpha}/\\sqrt{n}\\), and a uniform prior over the interval \\(\\left( \\theta_{0}-I/2,\\theta_{0}+I/2\\right)\\). If \\(p_{0}\\) is the probability of the null, then, \\[\\begin{align*}\nP \\left(  \\theta=\\theta_{0} \\mid \\overline{y}^{\\ast}\\right)   &\n=\\frac{\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\n_{0}\\right)  ^{2}}{\\sigma^{2}}\\right)  p_{0}}{\\exp\\left(  -\\frac{1}{2}%\n\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta_{0}\\right)  ^{2}}{\\sigma^{2}%\n}\\right)  p_{0}+\\left(  1-p_{0}\\right)  \\int_{\\theta_{0}-I/2}^{\\theta_{0}%\n+I/2}\\exp\\left(  -\\frac{1}{2}\\frac{T\\left(  \\overline{y}^{\\ast}-\\theta\\right)\n^{2}}{\\sigma^{2}}\\right)  I^{-1}d\\theta}\\\\\n&  =\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\int_{\\theta_{0}-I/2}^{\\theta_{0}+I/2}\\exp\\left(  -\\frac{1}{2}\\left( \\frac{\\left(  \\overline{y}^{\\ast}-\\theta\\right)  }{\\sigma/\\sqrt{T}}\\right)^2\n\\right)  d\\theta}\\\\\n&  \\geq\\frac{\\exp\\left(  -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}}{\\exp\\left( -\\frac{1}{2}t_{\\alpha}^{2}\\right)  p_{0}+\\frac{\\left(  1-p_{0}\\right)  }%\n{I}\\sqrt{2\\pi\\sigma^{2}/T}}\\rightarrow1\\text{ as }T\\rightarrow\\infty\\text{.}%\n\\end{align*}\\] In large samples, the posterior probability of the null approaches 1, whereas Fisher always reject the null. It is important to note that this holds for any \\(t_{\\alpha}\\), thus even if the test were performed at the 1% level or lower, the posterior probability would eventually reject the null.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#prior-sensitivity",
    "href": "06-hyp.html#prior-sensitivity",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.7 Prior Sensitivity",
    "text": "6.7 Prior Sensitivity\nThe paradoxes discussed above, particularly Lindley’s Paradox, often invite criticism regarding the choice of the prior distribution. A common frequentist counter-argument is that the Bayesian result is driven by a specific, perhaps “biased”, prior. How do we know that the chosen prior is not artificially inflating the probability of the null? One elegant way to address this is to search over classes of priors to find the one that minimizes the evidence for the null hypothesis, effectively constructing a “worst-case” Bayesian scenario.\nTo see this, consider the case of testing \\(H_{0}:\\mu_{0}=0\\) vs. \\(H_{1}:\\mu_{0}\\neq0\\) with observations drawn from \\(y_{t} \\sim\\mathcal{N}\\left( \\theta_{0},\\sigma^{2}\\right)\\), with \\(\\sigma\\) known. With equal prior null and alternative probability, the probability of the null is \\(p\\left( H_{0} \\mid y\\right) =\\left( 1+\\left( \\mathrm{BF}_{0,1}\\right) ^{-1}\\right) ^{-1}\\). Under the null, \\[\np\\left(  y \\mid H_{0}\\right)  =\\left(  \\frac{1}{2\\pi\\sigma^{2}}\\right)\n^{\\frac{T}{2}}\\exp\\left(  -\\frac{1}{2}\\left(  \\frac{\\left(  \\overline\n{y}-\\theta_{0}\\right)  }{\\sigma/\\sqrt{T}}\\right)  ^{2}\\right)  \\text{.}%\n\\] The criticism applies to the priors under the alternative. To analyze the sensitivity, consider four classes of priors under the alternative: (a) the class of normal priors, \\(p\\left( \\theta \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( a,A\\right)\\); (b) the class of all symmetric unimodal prior distributions; (c) the class of all symmetric prior distributions; and (d) the class of all proper prior distributions. These classes provide varying degrees of prior information, allowing a thorough examination of the strength of evidence.\nIn the first case, consider the standard conjugate prior distribution, \\(p\\left( \\mu \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A\\right)\\). Under the alternative, \\[\\begin{align*}\np\\left(  y \\mid H_{1}\\right)   &  =\\int p\\left(  y \\mid \\mu,H_{1}\\right)  p\\left(  \\mu \\mid H_{1}\\right)  d\\mu\\\\\n&  =\\int p\\left(  \\overline{y} \\mid \\mu,H_{1}\\right)  p\\left( \\mu \\mid H_{1}\\right)  d\\mu\\text{,}%\n\\end{align*}\\] using the fact that \\(\\overline{y}\\) is a sufficient statistic. Noting that \\(p\\left( \\overline{y} \\mid \\mu,H_{1}\\right) \\sim N\\left( \\mu ,\\sigma^{2}/T\\right)\\) and \\(p\\left( \\mu \\mid H_{1}\\right) \\sim N\\left( \\mu_{0},A\\right)\\), we can use the “substitute” instead of integrate trick to assert that \\[\n\\overline{y}=\\mu_{0}+\\sqrt{A}\\eta+\\sqrt{\\sigma^{2}/T}\\varepsilon\\text{,}%\n\\] where \\(\\eta\\) and \\(\\varepsilon\\) are standard normal. Then, \\(p\\left( \\overline{y} \\mid H_{1}\\right) \\sim\\mathcal{N}\\left( \\mu_{0},A+\\sigma^{2}/T\\right)\\). Thus, \\[\n\\mathrm{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }=\\frac{p\\left(  \\overline{y} \\mid H_{0}\\right)\n}{p\\left(  \\overline{y} \\mid H_{1}\\right)  }=\\frac{\\left(  \\sigma^{2}/T\\right)\n^{-\\frac{1}{2}}}{\\left(  \\sigma^{2}/T+A\\right)  ^{-\\frac{1}{2}}}\\frac\n{\\exp\\left(  -\\frac{1}{2}t^{2}\\right)  }{\\exp\\left(  -\\frac{1}{2}\\frac\n{z^{2}\\sigma^{2}/T}{A+\\sigma^{2}/T}\\right)  }\\text{.} \\label{BF_normal}%\n\\] To operationalize the test, \\(A\\) must be selected. \\(A\\) is chosen to minimizing the posterior probabilities of the null, with \\(P_{norm}\\left( H_{0} \\mid y\\right)\\) being the resulting lower bound on the posterior probability of the null. For \\(z\\geq1\\), the lower bound on the posterior probability of the null is \\[\nP_{norm}\\left(  H_{0} \\mid y\\right)  =\\left[\n1+\\sqrt{e}\\exp\\left(  -.5t^{2}\\right)  \\right]  ^{-1},\n\\] which is derived in a reference cited in the notes. This choice provides a maximal bias of the Bayesian approach toward rejecting the null. It is important to note that this is not a reasonable prior, as it was intentionally constructed to bias the null toward rejection.\nFor the class of all proper prior distributions, it is also easy to derive the bound. From equation above, minimizing the posterior probability is equivalent to minimizing the Bayes factor, \\[\n\\mathrm{BF}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)  }{p\\left( y \\mid H_{1}\\right)  }\\text{.}%\n\\] Since \\[\np\\left(  y \\mid H_{1}\\right)  =\\int p\\left(  y \\mid \\theta,H_{1}\\right)  p\\left(  \\theta \\mid H_{1}\\right)  d\\theta\\leq p\\left( y \\mid \\widehat{\\theta}_{MLE},H_{1}\\right)  \\text{,}%\n\\] where \\(\\widehat{\\theta}_{MLE}=\\arg\\underset{\\theta\\neq0}{\\max}p\\left( y \\mid \\theta\\right)\\). The maximum likelihood estimator, maximizes the probability of the alternative, and provides a lower bound on the Bayes factor, \\[\n\\underline{\\mathrm{BF}}_{0,1}=\\frac{p\\left(  y \\mid H_{0}\\right)\n}{\\underset{\\theta\\neq0}{\\sup}p\\left(  y \\mid \\theta\\right)  }\\text{.}%\n\\] In this case, the bound is particularly easy to calculate and is given by \\[\nP_{all}\\left(  H_{0} \\mid y\\right)  =\\left( 1+\\exp\\left(  -\\frac{t^{2}}{2}\\right)  \\right)  ^{-1}\\text{.}%\n\\] A reference cited in the notes provides the bounds for the second and third cases, generating \\(P_{s,u}\\left( H_{0} \\mid y\\right)\\) and \\(P_{s}\\left( H_{0} \\mid y\\right)\\), respectively. All of the bounds only depend on the \\(t-\\)statistic and constants.\nTable 6.5 reports the \\(t-\\)statistics and associated \\(p-\\)values, with the remaining columns provide the posterior probability bounds. For the normal prior and choosing the prior parameter \\(A\\) to minimize the probability of the null, the posterior probability of the null is much larger than the \\(p-\\)value, in every case. For the standard case of a \\(t-\\)statistic of 1.96, \\(P\\left( H_{0} \\mid y\\right)\\) is more than six times greater than the \\(p-\\)value. For \\(t=2.576\\), \\(P\\left( H_{0} \\mid y\\right)\\) is almost 13 times greater than the \\(p-\\)value. These probabilities fall slightly for more general priors. For example, for the class of all priors, a t-statistic of 1.96/2.576 generates a lower bound for the posterior probability of 0.128/0.035\\(,\\) more than 2/3 times the \\(p-\\)value.\n\n\n\nTable 6.5: Comparison of strength of evidence against the point null hypothesis. The numbers are reproduced from Berger (1986).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-stat\n\\(p\\)-value\n\\(P_{norm}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{s,u}\\left( H_{0} \\mid y\\right)\\)\n\\(P_{s}\\left(H_{0} \\mid y\\right)\\)\n\\(P_{all}\\left(H_{0} \\mid y\\right)\\)\n\n\n\n\n1.645\n0.100\n0.412\n0.39\n0.34\n0.205\n\n\n1.960\n0.050\n0.321\n0.29\n0.227\n0.128\n\n\n2.576\n0.010\n0.133\n0.11\n0.068\n0.035\n\n\n3.291\n0.001\n0.0235\n0.018\n0.0088\n0.0044",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#the-difference-between-p-values-and-bayesian-evidence",
    "href": "06-hyp.html#the-difference-between-p-values-and-bayesian-evidence",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.8 The difference between p-values and Bayesian evidence",
    "text": "6.8 The difference between p-values and Bayesian evidence\nSuppose that you routinely reject two-sided hypotheses at a fixed level of significance, \\(\\alpha = 0.05\\). Furthermore, suppose that half the experiments under the null are actually true: \\(p(H_0) = p(H_1) = \\frac{1}{2}\\).\nThe observed p-value is not a probability in any real sense. The observed t-value is a realization of a statistic that happens to be \\(N(0,1)\\) under the null hypothesis. Suppose that we observe \\(t = 1.96\\).\nThen the maximal evidence against the null hypothesis, which corresponds to \\(t = 0\\), will be achieved by evaluating the likelihood ratio at the observed t-ratio. We get \\[\n\\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\geq \\frac{p(y \\mid \\theta = \\theta_0)}{p(y \\mid \\theta = \\hat{\\theta})}\n\\]\nTechnically, \\(p(y \\mid H_1) = \\int p(y \\mid \\theta) p(\\theta \\mid H_1) d\\theta \\leq p(y \\mid \\hat{\\theta})\\).\nFor testing, this gives: \\[\n\\frac{p(y \\mid H_0)}{p(y \\mid H_1)} \\geq \\frac{\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\cdot 1.96^2}}{\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\cdot 0^2}} = 0.146\n\\]\nIn terms of probabilities, with \\(p(H_0) = p(H_1)\\), we have: \\[\np(H_0 \\mid y) = \\frac{1}{1 + \\frac{p(y \\mid H_1)}{p(y \\mid H_0)} \\frac{p(H_1)}{p(H_0)}} \\geq 0.128\n\\]\nHence, there’s still a 12.8% chance that the null is true! That’s very different from the p-value of 5%.\nMoreover, among experiments with p-values of 0.05, at least 28.8% will actually turn out to be true nulls (Sellke, Bayarri, and Berger 2001)! Put another way, the probability of rejecting a true null conditional on the observed \\(p = 0.05\\) is at least 30%. You are throwing away good null hypotheses and claiming you have found effects!",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#jeffreys-decision-rule",
    "href": "06-hyp.html#jeffreys-decision-rule",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.9 Jeffreys’ Decision Rule",
    "text": "6.9 Jeffreys’ Decision Rule\nJeffreys (1998) provided a famous rule for hypothesis testing. Consider testing \\(H_0: \\beta = 0\\) versus \\(H_1: \\beta \\neq 0\\) with a t-statistic \\(\\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}}\\). Jeffreys proposed the rule: \\[\n\\frac{\\hat{\\beta}}{s_{\\hat{\\beta}}} &gt; \\log\\left(\\frac{2n}{\\pi}\\right)\n\\]\nThis result can be understood through the Dickey-Savage density ratio, which states that for a sharp null hypothesis \\(H_0: \\beta=0\\) nested within an alternative \\(H_1\\), the Bayes Factor is the ratio of the posterior density to the prior density evaluated at the null value: \\[\nBF_{01} = \\frac{p(\\beta = 0 \\mid y, H_1)}{p(\\beta = 0 \\mid H_1)}.\n\\] Assuming a Cauchy(\\(0, \\sigma\\)) prior for \\(\\beta\\) under \\(H_1\\), the prior density at the null is \\(p(0 \\mid H_1) = 1/(\\pi\\sigma)\\). For the posterior, we can approximate it using a normal distribution centered at the MLE \\(\\hat{\\beta}\\) with standard error \\(s_{\\hat{\\beta}}\\). Evaluating this approximate posterior at 0 gives: \\[\np(0 \\mid y, H_1) \\approx \\frac{1}{\\sqrt{2\\pi}s_{\\hat{\\beta}}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{0 - \\hat{\\beta}}{s_{\\hat{\\beta}}}\\right)^2\\right).\n\\] Substituting these into the ratio, the condition for evidence neutrality (\\(BF=1\\)) becomes: \\[\n\\frac{\\frac{1}{\\sqrt{2\\pi}s_{\\hat{\\beta}}} e^{-t^2/2}}{1/(\\pi\\sigma)} = 1 \\implies \\sqrt{\\frac{n\\pi}{2}} e^{-t^2/2} \\approx 1,\n\\] where we’ve used the approximation \\(s_{\\hat{\\beta}} \\approx \\sigma/\\sqrt{n}\\). Jeffreys’ rule \\(\\hat{\\beta}/s_{\\hat{\\beta}} &gt; \\log(2n/\\pi)\\) provides a heuristic approximation to this boundary.\nThese critical values differ substantially from the fixed frequentist thresholds:\n\nJeffreys’ decision rule critical values Jeffreys (1998) (p. 379)\n\n\n\\(n\\)\n\\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}}\\)\n\n\n\n\n5\n1.16\n\n\n10\n1.85\n\n\n100\n4.15\n\n\n100,000\n11.06\n\n\n\nFor instance, when \\(n = 10\\), we have \\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}} = \\log(20/\\pi) = 1.85\\), and for \\(n = 100\\), we have \\(\\hat{\\beta}/\\sigma_{\\hat{\\beta}} = \\log(200/\\pi) = 4.15\\).\nJeffreys then explains the consequences of this sample-size dependence: traditional fixed critical values like 1.96 do not properly account for the evidence provided by larger sample sizes.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "06-hyp.html#cromwells-rule",
    "href": "06-hyp.html#cromwells-rule",
    "title": "6  Bayesian Hypothesis Testing",
    "section": "6.10 Cromwell’s Rule",
    "text": "6.10 Cromwell’s Rule\nThe discussion of hypothesis testing throughout this chapter reveals a fundamental tension between the desire for certainty and the reality of uncertainty in statistical inference. This tension is captured by Cromwell’s Rule, a principle that serves as a philosophical foundation for Bayesian hypothesis testing.\nWe can write Bayes rule for updating models as follows: \\[\np(M \\mid D) = \\frac{p(D \\mid M)}{p(D)} p(M)\n\\]\nThus, if \\(p(M) = 0\\), then \\(p(M \\mid D) = 0\\) for all \\(D\\).\nThis mathematical result has profound implications: if you assign zero prior probability to a hypothesis, no amount of evidence can ever change your mind. The posterior probability remains zero regardless of how strongly the data might support that hypothesis.\nThis principle is named after Oliver Cromwell’s famous plea to the Church of Scotland in 1650:\n\nI beseech you, in the bowels of Christ, think it possible you may be mistaken\n\nCromwell’s appeal for intellectual humility resonates deeply with the Bayesian approach to hypothesis testing. The rule suggests that we should never assign zero probability to hypotheses that could plausibly be true, as doing so makes us unable to learn from any amount of contradictory evidence.\nThe rule emphasizes the importance of careful prior specification. As we saw in the section on prior sensitivity, even when we try to bias our priors against the null hypothesis, the resulting posterior probabilities often remain substantially higher than corresponding p-values. Cromwell’s Rule reminds us that assigning zero probability to any reasonable hypothesis is not just mathematically problematic—it’s epistemologically unsound.\nCromwell’s Rule further aligns with the likelihood principle discussed earlier. Just as the likelihood principle states that all relevant experimental information is contained in the likelihood function, Cromwell’s Rule ensures that we remain open to learning from all possible evidence. By avoiding zero prior probabilities, we maintain the ability to update our beliefs based on observed data.\nThis principle serves as a philosophical foundation that unifies the various approaches to hypothesis testing discussed in this chapter, emphasizing the importance of intellectual humility and the willingness to learn from evidence in statistical inference.\n\n\n\n\nArmitage, Peter. 1975. Sequential Medical Trials. 2nd ed. Oxford: Blackwell Scientific Publications.\n\n\nBenda, Norbert, Michael Branson, Willi Maurer, and Tim Friede. 2016. “Sequential Designs with Small Samples: Evaluation and Recommendations for Normal Responses.” Statistics in Medicine 35 (19): 3215–30.\n\n\nBerry, Donald A. 1985. “Interim Analyses in Clinical Trials: Classical Vs. Bayesian Approaches.” Statistics in Medicine 4 (4): 521–26.\n\n\nBerry, Scott M., Bradley P. Carlin, J. Jack Lee, and Peter Müller. 2010. Bayesian Adaptive Methods for Clinical Trials. Boca Raton: CRC Press.\n\n\nBillingsley, Patrick. 1995. Probability and Measure. 3rd ed. New York: John Wiley & Sons.\n\n\nChen, Cong, Naitee Li, Shuai Yuan, Zoran Antonijevic, Wei Guo, et al. 2022. “Application of Bayesian Methods to Accelerate Rare Disease Drug Development: Scopes and Hurdles.” Orphanet Journal of Rare Diseases 17: 186.\n\n\nCover, Thomas M., and Joy A. Thomas. 2006. Elements of Information Theory. John Wiley & Sons.\n\n\nDeMets, David L., and K. K. Gordon Lan. 1994. “Interim Analysis: The Alpha Spending Function Approach.” Statistics in Medicine 13 (13-14): 1341–52.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J. Savage. 1963. “Bayesian Statistical Inference for Psychological Research.” Psychological Review 70 (3): 193–242.\n\n\nHilgers, Ralf-Dieter, Kit Roes, and Nigel Stallard. 2016. “Efficient Ways Exist to Obtain the Optimal Sample Size in Clinical Trials in Rare Diseases.” Journal of Clinical Epidemiology 80: 68–76.\n\n\nJeffreys, Harold. 1998. Theory of Probability. Third Edition, Third Edition. Oxford Classic Texts in the Physical Sciences. Oxford, New York: Oxford University Press.\n\n\nNovick, Melvin R., and James E. Grizzle. 1965. “A Bayesian Approach to the Analysis of Data from Clinical Trials.” Journal of the American Statistical Association 60 (309): 81–96.\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the Online Service Economy.” Applied Stochastic Models in Business and Industry 31 (1): 37–45.\n\n\nScott, Steven L. 2013. “Multi-Armed Bandit Experiments.”\n\n\nSellke, Thomas, M. J Bayarri, and James O Berger. 2001. “Calibration of \\(\\rho\\) Values for Testing Precise Null Hypotheses.” The American Statistician 55 (1): 62–71.\n\n\nThompson, William R. 1933. “On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.” Biometrika 25 (3/4): 285–94.\n\n\nU.S. Food and Drug Administration. 2010. “Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials.”\n\n\nVillar, Sofía S., Jack Bowden, and James Wason. 2015. “Multi-Armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges.” Statistical Science 30 (2): 199–215.\n\n\nWald, Abraham. 1945. “Sequential Tests of Statistical Hypotheses.” The Annals of Mathematical Statistics 16 (2): 117–86.\n\n\n———. 1947. Sequential Analysis. New York: John Wiley & Sons.\n\n\nWei, Bo, Thomas M. Braun, Roy N. Tamura, and Kelley Kidwell. 2018. “A Small n Sequential Multiple Assignment Randomized Trial Design for Use in Rare Disease Research.” Statistics in Medicine 37 (26): 3836–52.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayesian Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "07-sp.html",
    "href": "07-sp.html",
    "title": "7  Stochastic Processes",
    "section": "",
    "text": "7.1 Brownian Motion\nYet another fundamental concept that is useful for probabilistic reasoning is a stochastic process. An instance of a process is a function \\(X\\colon \\Omega \\rightarrow S\\) from an index set \\(\\Omega\\) to a set of possible values \\(S\\), called the state space. The state space of a stochastic process is the set of all possible states that the process can be in. Each state in the state space represents a possible outcome or condition of the system being modeled. The process then is the distribution over the space of functions from \\(\\Omega\\) to \\(S\\). The term process is used because the function \\(X\\) is often thought of as a time-varying quantity, and the index set \\(\\Omega\\) is often interpreted as time. While time is the most common index, the index set \\(\\Omega\\) is general; for example, in geostatistics, the process is indexed by spatial coordinates rather than time. Both the index set and the state space can be discrete or continuous.\nA key concept in many stochastic processes is the Markov property, which states that the future state of the process depends only on the current state, not on the sequence of events that preceded it. Formally, \\(P(X_{t+1}|X_t, X_{t-1}, \\ldots) = P(X_{t+1}|X_t)\\).\nA stochastic process is a family of random variables that describes the evolution through time of some (physical) process. We denote this by \\(X = \\{X_t,~t\\in T\\}\\), with \\(t\\) representing time and \\(X_t = \\omega\\) is the state of the process at time \\(t\\). We will get a realization (a.k.a. sample path). In the case when time is discrete, the realization is a sequence of observed \\(X = \\Omega = \\{\\omega_1,\\omega_2,\\ldots\\}\\). Common discrete time processes are Markov chains. Brownian motion is a central process in continuous time and state with almost surely continuous but nowhere differentiable paths. Poisson processes are commonly used to account for jumps in the process.\nHere are some widely used stochastic processes:\nData variability can be modeled as i.i.d. processes or as stochastic processes with serial, spatial, or other dependencies. Modeling these dependencies moves beyond descriptive statistics to predictive frameworks that quantify uncertainty. The early sections of Davison (2003) offer a primer on developing and applying stochastic models.\nBrownian Motion, named after botanist Robert Brown who observed pollen particles following irregular random trajectories in water, is fundamental to stochastic processes. A one-dimensional Brownian Motion (Wiener process) \\(B_t,~t\\ge 0\\) has the following properties:\nFormally, Brownian motion is a continuous-time stochastic process \\(B_t\\) for \\(t \\ge 0\\) used to model random continuous evolution.\nFigure 7.1 below shows three sample paths of Brownian Motion.\n# Brownian Motion\nset.seed(92)\nt = seq(0, 1, 0.001)\nplot(t, cumsum(rnorm(1001, 0, sqrt(0.001))), type=\"l\", xlab=\"t\", \nylab=\"B_t\", lwd=2, ylim=c(-1.2, 2))\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))), lwd=2, col=2)\nlines(t, cumsum(rnorm(1001, 0, sqrt(0.001))),lwd=2, col=3)\n\n\n\n\n\n\n\nFigure 7.1: Brownian Motion\nThus, for any times \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\(B(t_2) - B(t_1)\\), \\(B(t_3) - B(t_2)\\), , \\(B(t_n) - B(t_{n-1})\\) are independent and the function \\(t \\mapsto B_t\\) is continuous almost surely.\nSome properties of Brownian Motion are:\nThe crash was precipitated by ‘portfolio insurance,’ an automated strategy that sold equities as markets declined, creating a self-reinforcing downward spiral.\nThe normal assumption of asset returns was first proposed in 1900 in the PhD thesis of Louis Bachelier, who was a student of Henri Poincare. Bachelier was interested in developing statistical tools for pricing options (predicting asset returns) on the Paris stock exchange. Although Bachelier’s work laid the foundation for the modern theory of stochastic processes, he was never given credit by his contemporaries, including Einstein, Levy and Borel.\nIn 1905 Einstein published a paper which used the same statistical model as Bachelier to describe the 1827 discovery by botanist Robert Brown, who observed that pollen particles suspended in water followed irregular random trajectories. Thus, we call the stochastic process that describes these phenomena Brownian motion. Einstein’s advisor at the University of Zurich was Hermann Minkowski who was a friend and collaborator of Poincare. Thus, it is likely Einstein knew about the work of Bachelier, but he never mentioned it in his paper. Einstein’s omission of references was not without precedent; his 1905 relativity paper similarly lacked citations, famously omitting Poincaré’s 1898 contributions. Poincare published a paper Poincaré (1898) on relativity theory in 1898, seven years before Einstein. This paper was published in a philosophy journal and thus Poincare avoided using any mathematical formulas except for the famous \\(E=mc^2\\). Poincare discussed his results on relativity theory with Minkowski. Minkowski asked Einstein to read Poincare’s work Arnol’d (2006). However, Einstein never referenced the work of Poincare until 1945. One of the reviewers for the 1905 paper on relativity by Einstein was Poincare and he wrote a very positive review mentioning it as a breakthrough. When Minkowski asked Poincare why he did not claim his priority on the theory, Poincare replied that our mission is to support young scientists. More about why credit is mistakenly given to Einstein for relativity theory is discussed by Logunov Logunov (2004).\nEinstein was not the only one who ignored the work of Bachelier; Paul Levy did so as well. Paul Levy was considered a pioneer and authority on stochastic processes during Bachelier’s time, although Bruno de Finetti introduced a dual concept of infinite divisibility in 1929, before the works of Levy in the early 1930s on this topic. Levy never mentioned the work of the obscure and little known mathematician Bachelier. The first to give credit to Bachelier was Kolmogorov in his 1931 paper Kolmogoroff (1931) (Russian translation A. N. Kolmogorov (1938) and English translation Shiryayev (1992)). Later L.J. Savage translated Bachelier’s work to English and showed it to Paul Samuelson. Samuelson extended the work of Bachelier by considering the log-returns rather than absolute numbers, popularized the work of Bachelier among economists and the translation of Bachelier’s thesis was finally published in English in 1964 Cootner (1967). Many economists who extended the work of Bachelier won Nobel prizes, including Eugene Fama known for work on the efficient markets hypothesis, Paul Samuelson, and Myron Scholes for the Black-Scholes model, as well as Robert Merton.\nOriginally developed to model financial markets by Louis Bachelier in 1900, Brownian Motion applies to biology (biomolecule movement), environmental science (diffusion of pollutants), and mathematics (stochastic calculus).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#brownian-motion",
    "href": "07-sp.html#brownian-motion",
    "title": "7  Stochastic Processes",
    "section": "",
    "text": "\\(B_0 = 0\\) almost surely\n\\(B_t\\) has stationary independent increments: \\(B_t - B_s \\sim N(0, t-s)\\) for \\(0 \\le s &lt; t\\)\n\\(B_t\\) is a continuous function of \\(t\\)\nFor each time \\(t &gt; 0\\), the random variable \\(B_t\\) is normally distributed with mean 0 and variance \\(t\\), i.e., \\(B_t \\sim N(0, t)\\).\n\n\n\n\n\n\n\nScale Invariance: If \\(B_t\\) is a Brownian motion, then for any \\(a &gt; 0\\), the process \\(aB(t/a^2)\\) is also a Brownian motion.\nFractal Nature: Brownian motion paths are nowhere differentiable but continuous everywhere, reflecting a fractal-like nature.\n\n\n\n\n\n\n\nExample 7.1 (Yahoo Stock Price Simulation) Investing in volatile stocks can be very risky. The Internet stocks during the late 1990’s were notorious for their volatility. For example, the leading Internet stock Yahoo! started 1999 at $62,rose to $122, then fell back to $55 in August, only to end the year at $216. Even more remarkable is the fact that by January 2000, Yahoo! has risen more than 100-fold from its offering price of $1.32 on April 15, 1996. In comparison, theNasdaq 100, a benchmark market index, was up about 5-fold during the same period.\nStock prices fluctuate somewhat randomly. Maurice Kendall, in his seminal 1953 paper on the random walk nature of stock and commodity prices, observed that “The series looks like a wandering one, almost as if once a week the Demon of Chance drew a random number from a symmetrical population of fixed dispersion and added to it the current price to determine next week’s price (p. 87).” While a pure random walk model for Yahoo!’s stock price is in fact not reasonable since its price cannot fall below zero, an alternative model that appears to provide reasonable results assumes that the logarithms of price changes, or returns, follow a random walk. This alternative model is the basis for the results in this example.\nTo evaluate a stock investment, we take the initial price as \\(X_0\\) and then we need to determine what the stock price might be in year \\(T\\), namely \\(X_T\\). Our approach draws from the Black-Scholes Model for valuing stock options. Technically, the Black-Scholes Model assumes that \\(X_T\\) is determined by the solution to a stochastic differential equation. This leads to the Geometric Brownian Motion \\[\nX_T = X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T + \\sigma B_T  \\right),\n\\] where \\(B_T\\) is a standard Brownian motion; that is, \\(B_0 = 0\\), \\(B_t - B_s\\) is independent of \\(B_s\\), and its distribution depends only on \\(t-s\\) with \\(B_t \\sim N(0,t)\\). Hence, \\(B_t = \\sqrt{t}Z\\), where \\(Z \\sim N(0,1)\\).\nThen, the expected value is \\[\\begin{align*}\nE(X_T) = &X_0 \\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma B_T))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) E(\\exp(\\sigma \\sqrt{T}Z)) \\\\\n&= X_0\\exp\\left( (\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\frac{1}{2}\\sigma^2T \\right) = X_0\\exp\\left( \\mu T \\right).\n\\end{align*}\\] The \\(E(\\exp(\\sigma \\sqrt{T}Z)) = \\exp\\left( 1/2\\sigma^2T \\right)\\) is due to the moment property of the log-normal distribution. This follows from the property that the expectation of a log-normal variable \\(e^Y\\) where \\(Y \\sim N(\\mu, \\sigma^2)\\) is \\(e^{\\mu + \\sigma^2/2}\\). We can interpret \\(\\mu\\) as the expected rate of return \\[\n\\hat \\mu = \\frac{1}{T}\\log\\left( \\frac{X_T}{X_0} \\right).\n\\] This provides a way to estimate the expected rate of return from the expected value of the stock price at time \\(T\\), by plugging in the observed values of \\(X_0\\) and \\(X_T\\).\nThe variance is \\[\\begin{align*}\n\\text{Var}(X_T) = &X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma B_T))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\text{Var}(\\exp(\\sigma \\sqrt{T}Z))\\\\\n& = X_0^2 \\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right) \\exp\\left( \\sigma^2T \\right) - X_0^2\\exp\\left( 2(\\mu - 1/2\\sigma^2)T \\right)\\\\\n& = X_0^2\\exp\\left( 2\\mu T \\right)\\left( \\exp\\left( \\sigma^2T \\right) - 1 \\right).\n\\end{align*}\\]\nThe important consequence of the model for predicting future prices is that \\(\\log(X_T/X_0)\\) has a normal distribution with mean \\((\\mu-\\frac{1}{2} \\sigma^2)T\\) and variance \\(\\sigma^2 T\\) which is equivalent to saying that the ratio \\(X_T/X_0\\) has a log-normal distribution. It is interesting that although the Black-Scholes result is a standard tool for valuing options in finance the log-normal predictive distribution that follows from its assumptions is not commonly studied. In order to forecast \\(X_T\\) we need to estimate the unknowns \\(\\mu\\) and \\(\\sigma\\) (recall \\(X_0\\) is known). The unknown parameters \\(\\mu\\) and \\(\\sigma\\) can be interpreted as the instantaneous expected rate of return and the volatility, respectively. The mean parameter \\(\\mu\\) is known as the expected rate of return because the expected value of \\(X_T\\) is \\(X_0e^{\\mu T}\\). There are a number of ways of estimating the unknown parameters. One approach is to use an equilibrium model for returns, such as the Capital Asset Pricing Model or CAPM. We will discuss this model later. Another approach is to use historical data to estimate the parameters. For example, the expected rate of return can be estimated as the average historical return. The volatility can be estimated as the standard deviation of historical returns. The Black-Scholes model is a continuous time model, but in practice we use discrete time data. The Black-Scholes model can be adapted to discrete time by replacing the continuous time Brownian motion with a discrete time random walk.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#black-scholes-model-for-sports-betting",
    "href": "07-sp.html#black-scholes-model-for-sports-betting",
    "title": "7  Stochastic Processes",
    "section": "7.2 Black-Scholes Model for Sports Betting",
    "text": "7.2 Black-Scholes Model for Sports Betting\nSports betting involves wagering on the outcome of athletic events. Bettors’ assessments of these outcomes are aggregated in markets that provide key metrics like the point spread, which is the expected margin of victory, and moneyline odds, which imply the probability of a team winning. These market-based measures can be used to analyze the uncertainty, or volatility, inherent in a sports game.\nTo quantify the uncertainty in a game’s outcome, the score difference between two teams over time can be modeled as a stochastic process. Specifically, we use a Brownian motion model, first proposed by Stern (1994), to represent the evolution of a team’s lead. In this framework, the score difference at time \\(t\\), denoted as \\(X_t\\), is assumed to follow a normal distribution with a mean (or “drift”) that grows over time and a variance that also increases with time.\nThis can be expressed mathematically as: \\[\nX_t = \\mu t + \\sigma B_t \\sim N(\\mu t, \\sigma^2 t)\n\\] where \\(\\mu\\) is the drift parameter, representing the favored team’s point advantage over the whole game (derived from the point spread), \\(\\sigma\\) is the volatility parameter, representing the standard deviation of the final outcome, and \\(t\\) is the time elapsed in the game, scaled from 0 to 1. We normalize the game duration to \\(T=1\\).\nThis model allows for the calculation of a team’s win probability at any point in the game and provides a formal way to measure the uncertainty of the final score.\nThe concept of deriving a game’s volatility from betting markets is directly analogous to the Black-Scholes model in finance. In finance, the Black-Scholes formula is used to price options. If the market price of an option is known, one can work backward to solve for the volatility of the underlying stock; this is called implied volatility. The model in sports betting does the same: it uses the market-set point spread (\\(\\mu\\)) and win probability (\\(p\\)) to solve for the game’s implied volatility (\\(\\sigma\\)).\nBoth models use a Brownian motion framework to describe how a variable changes over time. However, there is a key difference. The sports model uses a standard Brownian motion, where the score changes additively. In contrast, the Black-Scholes model uses a geometric Brownian motion, which assumes that a stock price changes by a certain percentage, not by a fixed amount.\nEssentially, this approach applies the financial concept of implied volatility to the sports world, creating a lens through which betting market data can be interpreted to measure the expected uncertainty of a game.\n\nImplied Volatility for Sports Games\nThe concept of implied volatility is central to understanding how market prices reflect uncertainty. In the context of sports betting, implied volatility represents the market’s assessment of the uncertainty in a game’s final outcome, derived from observable betting market data.\nGiven the point spread \\(\\mu\\) (which represents the expected margin of victory) and the win probability \\(p\\) (derived from moneyline odds), we can solve for the implied volatility \\(\\sigma\\) using the relationship:\n\\[\np = \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)\n\\]\nRearranging this equation, the implied volatility is given by:\n\\[\n\\sigma = \\frac{\\mu}{\\Phi^{-1}(p)}\n\\]\nwhere \\(\\Phi^{-1}\\) is the inverse of the standard normal cumulative distribution function (the quantile function).\nThis approach mirrors the methodology used in financial markets, where option prices are used to infer the market’s expectation of future stock price volatility. In sports betting, the “option price” is effectively the betting odds, and the “underlying asset” is the game outcome. Just as financial implied volatility reflects market sentiment about future price movements, sports implied volatility captures the market’s view of how uncertain or “volatile” a particular game is likely to be.\nFor example, a game between two closely matched teams might have high implied volatility, reflecting greater uncertainty in the outcome, while a game featuring a heavily favored team against a significant underdog would typically exhibit lower implied volatility, as the outcome is more predictable.\n\nExample 7.2 (Black-Scholes Model for Super Bowl) In order to define the implied volatility of a sports game we begin with a distributional model for the evolution of the outcome in a sports game which we develop from Stern (1994). The model specifies the distribution of the lead of team A over team B, \\(X_t\\) for any \\(t\\) as a Brownian motion process. If \\(B_t\\) denotes a standard Brownian motion with distributional property \\(B_t \\sim N(0,t)\\) and we incorporate drift, \\(\\mu\\), and volatility, \\(\\sigma\\), terms, then the evolution of the outcome \\(X_t\\) that is given by: \\[\nX_t=\\mu t + \\sigma B_t \\sim N( \\mu t , \\sigma^2 t).\n\\] This distribution of the game outcome is similar to the Black-Scholes model of the distribution of a stock price.\nThis specification results in several useful measures (or, this specification results in closed-form solutions for a number of measures of interest). The distribution of the final score follows a normal distribution, \\(X_1\\sim N(\\mu, \\sigma^2)\\). We can calculate the probability of team A winning, denoted \\(p=\\mathbb{P}(X_1&gt;0)\\), from the spread and probability distribution. Given the normality assumption, \\(X_1 \\sim N(\\mu, \\sigma^2)\\), we have \\[\np = \\mathbb{P}(X_1&gt;0) = \\Phi \\left ( \\frac{\\mu}{\\sigma} \\right )\n\\] where \\(\\Phi\\) is the standard normal cdf. Table 7.1 uses \\(\\Phi\\) to convert team A’s advantage \\(\\mu\\) to a probability scale using the information ratio \\(\\mu/\\sigma\\).\n\n\n\nTable 7.1: Probability of Winning \\(p\\) versus the Sharpe Ratio \\(\\mu/\\sigma\\) (measure of risk-adjusted return)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu/\\sigma\\)\n0\n0.25\n0.5\n0.75\n1\n1.25\n1.5\n2\n\n\n\n\n\\(p=\\Phi(\\mu/\\sigma)\\)\n0.5\n0.60\n0.69\n0.77\n0.84\n0.89\n0.93\n0.977\n\n\n\n\n\n\nIf teams are evenly matched and \\(\\mu/\\sigma =0\\) then \\(p=0.5\\). Table 7.1 provides a list of probabilities as a function of \\(\\mu/\\sigma\\). For example, if the point spread \\(\\mu=-4\\) and volatility is \\(\\sigma=10.6\\), then the team has a \\(\\mu/\\sigma = -4/10.6 = - 0.38\\) volatility point disadvantage. The probability of winning is \\(\\Phi(-0.38) = 0.353 &lt; 0.5\\). A common scenario is that team A has an edge equal to half a volatility, so that \\(\\mu/\\sigma =0.5\\) and then \\(p= 0.69\\).\nOf particular interest here are conditional probability assessments made as the game progresses. For example, suppose that the current lead at time \\(t\\) is \\(l\\) points and so \\(X_t = l\\). The model can then be used to update your assessment of the distribution of the final score with the conditional distribution \\((X_1 | X_t=l )\\). To see this, we can re-write the distribution of \\(X_1\\) given \\(X_t\\) by noting that \\(X_1 = X_t+ X_1 - X_t\\). Using the formula above and substituting \\(t\\) for \\(1\\) where appropriate and noting that \\(X_t = l\\) by assumption, this simplifies to \\[\nX_1= l + \\mu(1- t) + \\sigma (B(1) - B_t).\n\\] Here \\(B(1) - B_t  \\stackrel{D}{=} B(1-t)\\) which is independent of \\(X_t\\) with distribution \\(N(0,1-t)\\). The mean and variance of \\(X_1|X_t=l\\) decay to zero as \\(t \\rightarrow 1\\) and the outcome becomes certain at the realised value of \\(X_1\\). We leave open the possibility of a tied game and overtime to determine the outcome.\nTo determine this conditional distribution, we note that there are \\(1-t\\) time units left together with a drift \\(\\mu\\) and as shown above in this case the uncertainty can be modeled as \\(\\sigma^2(1-t)\\). Therefore, we can write the distribution of the final outcome after \\(t\\) periods with a current lead of \\(l\\) for team A as the conditional distribution: \\[\n( X_1 | X_t=l) =  (X_1-X_t) + l   \\sim N( l + \\mu(1 - t) , \\sigma^2 (1 - t) )\n\\] From the conditional distribution \\((X_1 | X_t=l) \\sim N(l+\\mu(1-t), \\sigma^2 (1-t))\\), we can calculate the conditional probability of winning as the game evolves. The probability of team A winning at time \\(t\\) given a current lead of \\(l\\) point is: \\[\np_t = P ( X_1 &gt; 0 | X_t = l) = \\Phi \\left ( \\frac{ l + \\mu ( 1 - t)  }{ \\sigma \\sqrt{ ( 1-t) } } \\right )\n\\]\n\n\n\n\n\n\nFigure 7.2: Score Evolution on a Discretized Grid\n\n\n\nFigure 7.2 A and B illustrate our methodology with an example. Suppose we are analyzing data for a Superbowl game between teams A and B with team A favored. Figure A presents the information available at the beginning of the game from the perspective of the underdog team B. If the initial point spread—or the market’s expectation of the expected outcome—is \\(-4\\) and the volatility is \\(10.6\\) (assumed given for the moment; more on this below) then the probability that the underdog team wins is \\(p = \\Phi ( \\mu /\\sigma ) = \\Phi ( - 4/ 10.6) = 35.3\\)%. This result relies on our assumption of a normal outcome distribution on the outcome as previously explained. Another way of saying this is \\(\\mathbb{P}(X(1)&gt;0)=0.353\\) for an outcome distribution \\(X(1) \\sim N(-4, 10.6^2)\\). Figure A illustrates this with the shaded red area under the curve.\nFigure 7.2 B illustrates the information and potential outcomes at half-time. Here we show the evolution of the actual score until half time as the solid black line. From half-time onwards we simulate a set of possible Monte Carlo paths to the end of the game.\nSpecifically, we discretise the model with time interval \\(\\Delta =1/200\\) and simulate possible outcomes given the score at half time. The volatility plays a key role in turning the point spread into a probability of winning as the greater the volatility of the distribution of the outcome, \\(X_1\\), the greater the range of outcomes projected in the Monte Carlo simulation. Essentially the volatility provides a scale which calibrates the advantage implied by a given point spread.\nWe can use this relationship to determine how volatility decays over the course of the game. The conditional distribution of the outcome given the score at time \\(t\\), is \\((X_1|X_t=l)\\) with a variance of \\(\\sigma^2(1-t)\\) and volatility of \\(\\sigma \\sqrt{1-t}\\). The volatility is a decreasing function of \\(t\\), illustrating that the volatility dissipates over the course of a game. For example, if there is an initial volatility of \\(\\sigma = 10.6\\), then at half-time when \\(t=\\frac{1}{2}\\), the volatility is \\(10.6 / \\sqrt{2} = 7.5\\) volatility points left. Table 7.2, below, illustrates this relationship for additional points over the game.\n\n\n\nTable 7.2: Volatility Decay over Time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\n\\(\\sigma \\sqrt{1-t}\\)\n10.6\n9.18\n7.50\n5.3\n0\n\n\n\n\n\n\nTo provide insight into the final outcome given the current score, Table 7.1 and Table 7.2 can be combined to measure the current outcome, \\(l\\), in terms of standard deviations of the outcome.\nFor example, suppose that you have Team B, an underdog, so from their perspective \\(\\mu = -4\\) and at half-time team B has a lead of 15, \\(l= 15\\). Team B’s expected outcome as presented earlier is \\(l + \\mu (1-t)\\) or \\(15 - 4 \\times \\frac{1}{2} = 13\\). If initial volatility is \\(\\sigma = 10.60\\) then the remaining volatility at half-time is \\(10.6/\\sqrt{2} = 7.50\\) and team B’s expected outcome of \\(13\\) in terms of standard deviations is \\(13/7.5 = 1.73\\). Thus team B’s expected outcome is at the 99th percentile of the distribution, \\(\\Phi ( 1.73 ) = 0.96\\), implying a 96% chance of winning.\nImplied Volatility\nThe previous discussion assumed that the variance (or volatility) parameter \\(\\sigma\\) was a known constant. We return to this important quantity now. We are now in a position to define the implied volatility implicit in the two betting lines that are available. Given our model, we will use the money-line odds to provide a market assessment of the probability of winning, \\(p\\), and the point spread to assess the expected margin of victory, \\(\\mu\\). The money line odds are shown for each team A and B and provide information on the payoff from a bet on the team winning. This calculation will also typically require an adjustment for the bookmaker’s spread. With these we can infer the implied volatility, \\(\\sigma_{IV}\\), by solving \\[\n\\sigma_{IV}: \\; \\; \\;  \\; \\; p = \\Phi \\left ( \\frac{\\mu}{\\sigma_{IV}} \\right ) \\; \\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV} = \\frac{ \\mu }{ \\Phi^{-1} ( p ) } \\; .\n\\] Here \\(\\Phi^{-1}(p)\\) denotes the standard normal quantile function such that the area under the standard normal curve to the left of \\(\\Phi^{-1}(p)\\) is equal to \\(p\\). In our example we calculate this using the qnorm in R. Note that when \\(\\mu =0\\) and \\(p= \\frac{1}{2}\\) there’s no market information about the volatility as \\(\\mu / \\Phi^{-1} (p)\\) is undefined. This is the special case where the teams are seen as evenly matched- the expected outcome has a zero point spread and there is an equal probability that either team wins.\nTime Varying Implied Volatility\nUp to this point the volatility rate has been assumed constant through the course of the game, i.e., that the same value of \\(\\sigma\\) is relevant. The amount of volatility remaining in the game is not constant but the basic underlying parameters has been assumed constant. This need not be true and more importantly the betting markets may provide some information about the best estimate of the volatility parameter at a given point of time. This is important because time-varying volatility provides an interpretable quantity that can allow one to assess the value of a betting opportunity.\nWith the advent of online betting there is a virtually continuous traded contract available to assess implied expectations of the probability of team A winning at any time \\(t\\). The additional information available from the continuous contract allows for further update of the implied conditional volatility. We assume that the online betting market gives us a current assessment of \\(p_t\\), that is the current probability that team A will win. We will then solve for \\(\\sigma^2\\) and in turn define resulting time-varying volatility, as \\(\\sigma_{IV,t}\\), using the resulting equation to solve for \\(\\sigma_{IV,t}\\) with \\[\np_t = \\Phi \\left ( \\frac{ l + \\mu(1-t)  }{\\sigma_{IV,t} \\sqrt{1-t}} \\right )\n\\; \\text{ which \\; gives} \\; \\;\n\\sigma_{IV,t} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}}\n\\] We will use our methodology to find evidence of time-varying volatility in the SuperBowl XLVII probabilities.\nSuper Bowl XLVII: Ravens vs San Francisco 49ers\nSuper Bowl XLVII was held at the Superdome in New Orleans on February 3, 2013 and featured the San Francisco 49ers against the Baltimore Ravens. Going into Super Bowl XLVII the San Francisco 49ers were favorites to win which was not surprising following their impressive season. It was a fairly bizarre Super Bowl with a \\(34\\) minute power outage affecting the game by ultimately an exciting finish with the Ravens causing an upset victory \\(34-31\\). We will build our model from the viewpoint of the Ravens. Hence \\(X_t\\) will correspond to the Raven’s score minus the San Francisco 49ers. Table 7.3 provides the score at the end of each quarter.\n\n\n\nTable 7.3: SuperBowl XLVII by Quarter\n\n\n\n\n\n\\(t\\)\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{3}{4}\\)\n1\n\n\n\n\nRavens\n0\n7\n21\n28\n34\n\n\n49ers\n0\n3\n6\n23\n31\n\n\n\\(X_t\\)\n0\n4\n15\n5\n3\n\n\n\n\n\n\nTo determine the parameters of our model we first use the point spread which was set at the Ravens being a four point underdog, i.e. \\(\\mu=-4\\). This sets the mean of our outcome, \\(X_1\\), as \\[\n\\mu = \\mathbb{E} \\left (X_1 \\right )=-4 .\n\\] In reality, it was an exciting game with the Ravens upsetting the 49ers by \\(34-31\\). Hence, the realised outcome is \\(X_1= 34-31=3\\) with the point spread being beaten by \\(7\\) points or the equivalent of a touchdown.\n\n\n\n\n\n\nFigure 7.3: Superbowl XLVII: Ravens vs 49ers: TradeSports contracts traded and dynamic probability of the Ravens winning\n\n\n\nTo determine the markets’ assessment of the probability that the Ravens would win at the beginning of the game we use the money-line odds. These odds were quoted as San Francisco \\(-175\\) and Baltimore Ravens \\(+155\\). This implies that a bettor would have to place $175 to win $100 on the 49ers and a bet of $100 on the Ravens would lead to a win of $155. We can convert both of these money-lines to implied probabilities of the each team winning, by the equations \\[\np_{SF} = \\frac{175}{100+175} = 0.686 \\; \\; \\text{ and} \\; \\; p_{Ravens} = \\frac{100}{100+155} = 0.392\n\\] The probability sum to one plus the market overround: \\[\np_{SF} + p_{Ravens} = 0.686+0.392 = 1.078\n\\] namely a \\(7.8\\)% edge for the bookmakers. Put differently, if bettors place money proportionally across both teams then the bookies vig will be \\[\n\\text{Vig} = \\dfrac{0.078}{0.078+1} = 0.072\n\\] This means that the bookmaker is expected to make a profit of 7.2% of the total amount staked, no matter what happens to the outcome of the game.\nTo account for this edge in our model, we use the mid-point of the spread to determine \\(p\\) implying that \\[\np = \\frac{1}{2} p_{Ravens} + \\frac{1}{2} (1 - p_{SF} ) = 0.353\n\\] From the Ravens perspective we have \\(p = \\mathbb{P}(X_1&gt;0) =0.353\\).\nFigure 7.3 shows the evolution of the markets conditional probability of winning \\(p_t\\) for the Ravens. The data are from the online betting website TradeSports.com. Starting at \\(p=0.353\\) we see how dramatically the markets assessment of the Ravens winning can fluctuate. Given their commanding lead at half time, the probability has as high as \\(0.90\\). At the end of the four quarter when the 49ers nearly went into the lead with a touchdown, at one point the probability had dropped to \\(30\\)%.\nOur main question of interest is then: What implied volatility is consistent with market expectations?\nTo calculate the implied volatility of the Superbowl we substitute the pair \\((\\mu,p)\\) into our definition and solve for \\(\\sigma_{IV}\\). We obtain \\[\n\\sigma_{IV} = \\frac{\\mu}{\\Phi^{-1}(p)} = \\frac{-4}{-0.377}  = 10.60\n\\] where we have used \\(\\Phi^{-1} ( p) = qnorm(0.353) = -0.377\\). So on a volatility scale the \\(4\\) point advantage assessed for the 49ers is under a \\(\\frac{1}{2} \\sigma\\) favorite. From Table 2, this is consistent with a win probability of \\(p=\\Phi(\\frac{1}{2})=0.69\\). Another feature is that a \\(\\sigma=10.6\\) is historically low, as a typical volatility of an NFL game is \\(14\\) (see Stern, 1991). This implies a standard deviation of the final score differential of approximately 10.6 points. However, the more competitive the game one might expect a lower volatility. In reality, the outcome \\(X_1=3\\) was within one standard deviation of the model, which had an expectation of \\(\\mu=-4\\) and volatility \\(\\sigma=10.6\\). Another question of interest is\nWhat’s the probability of the Ravens winning given their lead at half time?\n\nAt half-time the Ravens were leading \\(21\\) to \\(6\\). This gives us \\(X(\\frac{1}{2})=21-6=15\\). From the online betting market we also have traded contracts on TradeSports.com that yield a current probability of \\(p_{\\frac{1}{2}} = 0.90\\). \nAn alternative view is to assume that the market assesses time varying volatility and the prices fully reflect the underlying probability. Here we ask the question\nWhat’s the implied volatility for the second half of the game?\nWe now have an implied volatility \\[\n\\sigma_{IV,t=\\frac{1}{2}} = \\frac{ l + \\mu ( 1-t ) }{ \\Phi^{-1} ( p_t )  \\sqrt{1-t}} = \\frac{15-2}{ \\Phi^{-1}(0.9) / \\sqrt{2} } = 14\n\\] where qnorm(0.9)=1.28. Notice that \\(14&gt; 10.6\\), our assessment of the implied volatility at the beginning of the game.\nWhat’s a valid betting strategy?\nAn alternative approach is to assume that the initial moneyline and point spread set the volatility and this stays constant throughout the game. This market is much larger than the online market and this is a reasonable assumption unless there has been material information as the game progresses such as a key injury.\nHence the market was expected a more typical volatility in the second half. If a bettor believed that the volatility was closer to the historical average of \\(\\sigma=14\\) (Stern, 1991), rather than the implied \\(10.6\\), then their assessment of the Ravens win probability would be lower. The remaining volatility would be \\(14/\\sqrt{2} \\approx 9.9\\), yielding a Z-score of \\(13/9.9 \\approx 1.31\\). This corresponds to a win probability of \\(\\Phi(1.31) \\approx 0.905\\). Compared to the market probability of \\(0.90\\), this represents a small edge.\nThe Kelly criterion (Kelly,1956) yields the optimal betting fraction \\(f^*\\): \\[\nf^* = p - \\dfrac{q}{b} = 0.905 - \\frac{0.095}{1/9} \\approx 0.05\n\\] where \\(q = 1-p\\) is the probability of losing, and \\(b\\) is the odds received (net fractional odds). That is, \\(5\\)% of capital. A more realistic strategy is to use the fractional Kelly criterion, which scales the bet by a risk-aversion parameter \\(\\gamma\\). For example, in this case if \\(\\gamma =3\\), we would bet \\(0.05/3 \\approx 0.017\\), or \\(1.7\\)% of our capital on this betting opportunity.\nFinally, odds changes can be dramatic at the end of the fourth quarter, and this Super Bowl was no exception. With the score at \\(34\\)–\\(29\\) and only a few minutes remaining, the 49ers were at first-and-goal. A few minutes after this, the probability of the Ravens winning had dropped precipitously from over \\(90\\)% to \\(30\\)%, see Figure 7.3. On San Francisco’s final offensive play of the game, Kaepernick threw a pass on fourth down to Michael Crabtree, but Ravens cornerback Jimmy Smith appeared to hold the wide receiver during the incompletion, No call was given and the final result was a Ravens win.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#poisson-process",
    "href": "07-sp.html#poisson-process",
    "title": "7  Stochastic Processes",
    "section": "7.3 Poisson Process",
    "text": "7.3 Poisson Process\nA Poisson process models random events occurring independently at a constant average rate—customer arrivals, call center traffic, or goals in a soccer match. A counting process \\(\\{N_t, t \\geq 0\\}\\) is a Poisson process with rate \\(\\lambda &gt; 0\\) if:\n\n\\(N(0) = 0\\) (the process starts at zero)\nThe process has independent increments: for any \\(0 \\leq t_1 &lt; t_2 &lt; \\ldots &lt; t_n\\), the random variables \\(N(t_2) - N(t_1), N(t_3) - N(t_2), \\ldots, N(t_n) - N(t_{n-1})\\) are independent\nThe process has stationary increments: for any \\(s &lt; t\\), the distribution of \\(N_t - N(s)\\) depends only on the length of the interval \\(t - s\\)\nFor any interval of length \\(t\\), the number of events follows a Poisson distribution: \\[\nP(N_t = k) = \\frac{e^{-\\lambda t}(\\lambda t)^k}{k!}, \\quad k = 0, 1, 2, \\ldots\n\\]\n\nThe parameter \\(\\lambda\\) represents the rate at which events occur per unit time. The expected number of events in an interval of length \\(t\\) is \\(\\E{N_t} = \\lambda t\\), and the variance is \\(\\Var{N_t} = \\lambda t\\).\nFigure 7.4 below shows three sample paths of a Poisson process with rate \\(\\lambda = 5\\) events per unit time.\n\n\n\n\n\n\n\n\nFigure 7.4: Poisson Process Trajectories\n\n\n\n\n\nAn equivalent characterization of the Poisson process is through the inter-arrival times between consecutive events. If \\(T_1, T_2, \\ldots\\) denote the times between successive events, then these are independent and identically distributed exponential random variables with mean \\(1/\\lambda\\). This connection between the Poisson and exponential distributions is fundamental: the Poisson process counts events while the exponential distribution models the waiting time between events.\nThe Poisson process can be viewed from two complementary perspectives. From a continuous-time viewpoint, we track the evolution of the counting process \\(N_t\\) as time progresses, asking questions about the probability of observing a certain number of events by time \\(t\\) or the distribution of event times. From a discrete count data perspective, we observe the number of events that occurred during a fixed time interval and use this to make inferences about the underlying rate parameter \\(\\lambda\\).\nChapter 3 introduced Poisson models in the context of count data and Bayesian inference. The Poisson distribution (discussed in the section on Poisson Model for Count Data) emerges naturally when we observe a Poisson process over a fixed time interval. For instance, when modeling the number of goals scored by a soccer team in a match, we implicitly assume that goals occur according to a Poisson process with some rate \\(\\lambda\\), and we observe the total count at the end of the match.\nThe Bayesian approach to learning about the rate parameter \\(\\lambda\\) (covered in the section on Poisson-Gamma: Learning about a Poisson Intensity in Chapter 3) becomes particularly powerful in the continuous-time setting. When we observe a Poisson process over time, we can update our beliefs about \\(\\lambda\\) as new events occur. The Gamma distribution serves as a conjugate prior for \\(\\lambda\\), meaning that if we start with a Gamma prior and observe events from a Poisson process, the posterior distribution remains in the Gamma family with updated parameters. This elegant updating mechanism allows us to refine our estimates of the event rate as we gather more data, balancing prior beliefs with observed evidence.\nThe connection between these perspectives is crucial for applications. In many real-world scenarios, we observe event counts over fixed intervals (discrete perspective) but need to make predictions about future events or the timing of the next event (continuous perspective). The Poisson process framework unifies these views, allowing us to seamlessly move between counting events and modeling their temporal dynamics.\n\nExample 7.3 (EPL Betting) Feng, Polson, and Xu (2016) employ a Skellam process (a difference of Poisson random variables) to model real-time betting odds for English Premier League (EPL) soccer games. Given a matrix of market odds on all possible score outcomes, we estimate the expected scoring rates for each team. The expected scoring rates then define the implied volatility of an EPL game. As events in the game evolve, they re-estimate the expected scoring rates and our implied volatility measure to provide a dynamic representation of the market’s expectation of the game outcome. They use real-time market odds data for a game between Everton and West Ham in the 2015-2016 season. We show how the implied volatility for the outcome evolves as goals, red cards, and corner kicks occur.\nGambling on soccer is a global industry with revenues of over $1 trillion a year (see “Football Betting - the Global Gambling Industry worth Billions,” BBC Sport). Betting on the result of a soccer match is a rapidly growing market, and online real-time odds exist (Betfair, Bet365, Ladbrokes). Market odds for all possible score outcomes (\\(0-0, 1-0, 0-1, 2-0, \\ldots\\)) as well as outright win, lose, and draw are available in real time. In this paper, we employ a two-parameter probability model based on a Skellam process and a non-linear objective function to extract the expected scoring rates for each team from the odds matrix. The expected scoring rates then define the implied volatility of the game.\nSkellam Process\nTo model the outcome of a soccer game between team A and team B, we let the difference in scores, \\(N_t = N_{A,t} - N_{B,t}\\), where \\(N_{A,t}\\) and \\(N_{B,t}\\) are the team scores at time point \\(t\\). Negative values of \\(N_t\\) indicate that team A is behind. We begin at \\(N(0) = 0\\) and end at time one with \\(N_1\\) representing the final score difference. The probability \\(\\mathbb{P}(N_1 &gt; 0)\\) represents the ex-ante odds of team A winning. Half-time score betting, which is common in Europe, is available for the distribution of \\(N_{1/2}\\).\nThen we find a probabilistic model for the distribution of \\(N_1\\) given \\(N_t = \\ell\\), where \\(\\ell\\) is the current lead. This model, together with the current market odds, can be used to infer the expected scoring rates of the two teams and then to define the implied volatility of the outcome of the match. We let \\(\\lambda^A\\) and \\(\\lambda^B\\) denote the expected scoring rates for the whole game. We allow for the possibility that the scoring abilities (and their market expectations) are time-varying, in which case we denote the expected scoring rates after time \\(t\\) by \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), respectively, instead of \\(\\lambda^A(1-t)\\) and \\(\\lambda^B(1-t)\\).\nThe Skellam distribution is defined as the difference between two independent Poisson variables given by:\n\\[\n\\begin{aligned}\nN_{A,t} &= W_{A,t} + W_t \\\\\nN_{B,t} &= W_{B,t} + W_t\n\\end{aligned}\n\\]\nwhere \\(W_{A,t}\\), \\(W_{B,t}\\), and \\(W_t\\) are independent processes with:\n\\[\nW_{A,t} \\sim \\text{Poisson}(\\lambda^A t), \\quad W_{B,t} \\sim \\text{Poisson}(\\lambda^B t).\n\\]\nHere \\(W_t\\) is a Poisson process used to induce a correlation between the numbers of goals scored.\n\\[\nN_t = N_{A,t} - N_{B,t} \\sim \\text{Skellam}(\\lambda^A t, \\lambda^B t).\n\\tag{7.1}\\]\nAt time \\(t\\), we have the conditional distributions:\n\\[\n\\begin{aligned}\nW_A(1) - W_{A,t} &\\sim \\text{Poisson}(\\lambda^A(1-t)) \\\\\nW_B(1) - W_{B,t} &\\sim \\text{Poisson}(\\lambda^B(1-t)).\n\\end{aligned}\n\\]\nNow letting \\(N^*(1-t)\\), the score difference of the sub-game which starts at time \\(t\\) and ends at time 1 and the duration is \\((1-t)\\). By construction, \\(N_1 = N_t + N^*(1-t)\\). Since \\(N^*(1-t)\\) and \\(N_t\\) are differences of two Poisson process on two disjoint time periods, by the property of Poisson process, \\(N^*(1-t)\\) and \\(N_t\\) are independent. Hence, we can re-express equation (Equation 7.1) in terms of \\(N^*(1-t)\\), and deduce\n\\[\n%N^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim Skellam(\\lambda^A (1-t),\\lambda^B (1-t) )\nN^*(1-t) = W^*_A(1-t) - W^*_B(1-t) \\sim \\text{Skellam}(\\lambda^A_t,\\lambda^B_t)\n\\]\nwhere \\(W^*_A(1-t) = W_A(1) - W_{A,t}\\), \\(\\lambda^A = \\lambda^A_0\\) and \\(\\lambda^A_t=\\lambda^A(1-t)\\). A natural interpretation of the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), is that they reflect the “net” scoring ability of each team from time \\(t\\) to the end of the game. The term \\(W_t\\) models a common strength due to external factors, such as weather. The “net” scoring abilities of the two teams are assumed to be independent of each other as well as the common strength factor. We can calculate the probability of any particular score difference, given by \\(\\mathbb{P}(N_1=x|\\lambda^A,\\lambda^B)\\), at the end of the game where the \\(\\lambda\\)’s are estimated from the matrix of market odds. Team strength and “net” scoring ability can be influenced by various underlying factors, such as the offensive and defensive abilities of the two teams. The goal of our analysis is to only represent these parameters at every instant as a function of the market odds matrix for all scores.\nAnother quantity of interest is the conditional probability of winning as the game progresses. If the current lead at time \\(t\\) is \\(\\ell\\), and \\(N_t=\\ell=N_{A,t}-N_{B,t}\\), the Poisson property implied that the final score difference \\((N_1|N_t=\\ell)\\) can be calculated by using the fact that \\(N_1=N_t+N^*(1-t)\\) and \\(N_t\\) and \\(N^*(1-t)\\) are independent. Specifically, conditioning on \\(N_t=\\ell\\), we have the identity\n\\[ N_1=N_t+N^*(1-t)=\\ell+\\text{Skellam}(\\lambda^A_t,\\lambda^B_t). \\]\nWe are now in a position to find the conditional distribution (\\(N_1=x|N_t=\\ell\\)) for every time point \\(t\\) of the game given the current score. Simply put, we have the time homogeneous condition\n\\[\n\\begin{aligned}\n\\mathbb{P}(N_1=x|\\lambda^A_t,\\lambda^B_t,N_t=\\ell) &= \\mathbb{P}(N_1-N_t=x-\\ell |\\lambda^A_t,\\lambda^B_t,N_t=\\ell) \\\\\n&= \\mathbb{P}(N^* (1-t)=x-\\ell |\\lambda^A_t,\\lambda^B_t)\n\\end{aligned}\n\\]\nwhere \\(\\lambda^A_t\\), \\(\\lambda^B_t\\), \\(\\ell\\) are given by market expectations at time \\(t\\). See Feng et al. for details.\nMarket Calibration\nOur information set at time \\(t\\) includes the current lead \\(N_t = \\ell\\) and the market odds for \\(\\{Win, Lose, Draw, Score\\}_t\\), where \\(Score_t = \\{ ( i - j ) : i, j = 0, 1, 2, ....\\}\\). These market odds can be used to calibrate a Skellam distribution which has only two parameters \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\). The best fitting Skellam model with parameters \\(\\{\\hat\\lambda^A_t,\\hat\\lambda^B_t\\}\\) will then provide a better estimate of the market’s information concerning the outcome of the game than any individual market (such as win odds) as they are subject to a “vig” and liquidity.\nSuppose that the fractional odds for all possible final score outcomes are given by a bookmaker. Fractional odds, commonly used in the UK, express the ratio of profit to stake. For example, odds of \\(3:1\\) (read as “three-to-one”) mean that for every $1 wagered, the bettor receives $3 in profit if the bet wins, plus the original $1 stake returned, for a total payout of $4. In this case, if the bookmaker offers \\(3:1\\) odds on a 2-1 final score, the bookmaker pays out three times the amount staked by the bettor if the outcome is indeed 2-1. This contrasts with American money-line odds, where positive numbers indicate the profit on a $100 stake (e.g., +300 means $300 profit on $100 wagered), and negative numbers indicate the stake needed to win $100.\nThe market implied probability makes the expected winning amount of a bet equal to 0. For fractional odds of \\(3:1\\), the implied probability is calculated as \\(p = \\frac{1}{1+3} = \\frac{1}{4} = 0.25\\) or 25%. We can verify this creates a fair bet: the expected winning amount is \\(\\mu = -1 \\times (1-1/4) + 3 \\times (1/4) = -0.75 + 0.75 = 0\\). We denote these odds as \\(odds(2,1) = 3\\). To convert all the available odds to implied probabilities, we use the identity\n\\[ \\mathbb{P}(N_A(1) = i, N_B(1) = j)=\\frac{1}{1+odds(i,j)}. \\]\nThe market odds matrix, \\(O\\), with elements \\(o_{ij}=odds(i-1,j-1)\\), \\(i,j=1,2,3...\\) provides all possible combinations of final scores. Odds on extreme outcomes are not offered by the bookmakers. Since the probabilities are tiny, we set them equal to 0. The sum of the possible probabilities is still larger than 1 (see Dixon and Coles (1997) and Dixon and Coles (1997)). This “excess” probability corresponds to a quantity known as the “market vig.” For example, if the sum of all the implied probabilities is 1.1, then the expected profit of the bookmaker is 10%. To account for this phenomenon, we scale the probabilities to sum to 1 before estimation.\nTo estimate the expected scoring rates, \\(\\lambda^A_t\\) and \\(\\lambda^B_t\\), for the sub-game \\(N^*(1-t)\\), the odds from a bookmaker should be adjusted by \\(N_{A,t}\\) and \\(N_{B,t}\\). For example, if \\(N_A(0.5)=1\\), \\(N_B(0.5)=0\\) and \\(odds(2,1)=3\\) at half time, these observations actually says that the odds for the second half score being 1-1 is 3 (the outcomes for the whole game and the first half are 2-1 and 1-0 respectively, thus the outcome for the second half is 1-1). The adjusted \\({odds}^*\\) for \\(N^*(1-t)\\) is calculated using the original odds as well as the current scores and given by\n\\[\n{odds}^*(x,y)=odds(x+N_{A,t},y+N_{B,t}).\n\\]\nAt time \\(t\\) \\((0\\leq t\\leq 1)\\), we calculate the implied conditional probabilities of score differences using odds information\n\\[\n\\mathbb{P}(N_1=k|N_t=\\ell)=\\mathbb{P}(N^*(1-t)=k-\\ell)=\\frac{1}{c}\\sum_{i-j=k-\\ell}\\frac{1}{1+{odds}^*(i,j)}\n\\]\nwhere \\(c=\\sum_{i,j} \\frac{1}{1+{odds}^*(i,j)}\\) is a scale factor, \\(\\ell=N_{A,t}-N_{B,t}\\), \\(i,j\\geq 0\\) and \\(k=0,\\pm 1,\\pm 2\\ldots\\).\nExample: Everton vs West Ham (3/5/2016)\nTable below shows the implied Skellam probabilities.\n\n\n\nTable 7.4: Table: Original odds data from Ladbrokes before the game started.\n\n\n\n\n\nEverton  West Ham\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n11/1\n12/1\n28/1\n66/1\n200/1\n450/1\n\n\n1\n13/2\n6/1\n14/1\n40/1\n100/1\n350/1\n\n\n2\n7/1\n7/1\n14/1\n40/1\n125/1\n225/1\n\n\n3\n11/1\n11/1\n20/1\n50/1\n125/1\n275/1\n\n\n4\n22/1\n22/1\n40/1\n100/1\n250/1\n500/1\n\n\n5\n50/1\n50/1\n90/1\n150/1\n400/1\n\n\n\n6\n100/1\n100/1\n200/1\n250/1\n\n\n\n\n7\n250/1\n275/1\n375/1\n\n\n\n\n\n8\n325/1\n475/1\n\n\n\n\n\n\n\n\n\n\nTable 7.4 shows the raw data of odds right the game. We need to transform odds data into probabilities. For example, for the outcome 0-0, 11/1 is equivalent to a probability of 1/12. Then we can calculate the marginal probability of every score difference from -4 to 5. We neglect those extreme scores with small probabilities and rescale the sum of event probabilities to one.\n\n\n\nTable 7.5: Market implied probabilities for the score differences versus Skellam implied probabilities at different time points. The estimated parameters \\(\\hat\\lambda^A=2.33\\), \\(\\hat\\lambda^B=1.44\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore difference\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n\n\n\n\nMarket Prob. (%)\n1.70\n2.03\n4.88\n12.33\n21.93\n22.06\n16.58\n9.82\n4.72\n2.23\n\n\nSkellam Prob. (%)\n0.78\n2.50\n6.47\n13.02\n19.50\n21.08\n16.96\n10.61\n5.37\n2.27\n\n\n\n\n\n\nTable 7.5 shows the model implied probability for the outcome of score differences before the game, compared with the market implied probability. As we see, the Skellam model appears to have longer tails. Different from independent Poisson modeling in Dixon and Coles (1997), our model is more flexible with the correlation between two teams. However, the trade-off of flexibility is that we only know the probability of score difference instead of the exact scores.\n\n\n\n\n\n\nFigure 7.5: The betting market data for Everton and West Ham is from ladbrokes.com. Market implied probabilities (expressed as percentages) for three different results (Everton wins, West Ham wins and draw) are marked by three distinct colors, which vary dynamically as the game proceeds. The solid black line shows the evolution of the implied volatility. The dashed line shows significant events in the game, such as goals and red cards. Five goals in this game are 13’ Everton, 56’ Everton, 78’ West Ham, 81’ West Ham and 90’ West Ham.\n\n\n\nFigure 7.5 examines the behavior of the two teams and represent the market predictions on the final result. Notably, we see the probability change of win/draw/loss for important events during the game: goals scoring and a red card penalty. In such a dramatic game, the winning probability of Everton gets raised to 90% before the first goal of West Ham in 78th minutes. The first two goals scored by West Ham in the space of 3 minutes completely reverses the probability of winning. The probability of draw gets raised to 90% until we see the last-gasp goal of West Ham that decides the game.\nFigure 7.5 plots the path of implied volatility throughout the course of the game. Instead of a downward sloping line, we see changes in the implied volatility as critical moments occur in the game. The implied volatility path provides a visualization of the conditional variation of the market prediction for the score difference. For example, when Everton lost a player by a red card penalty at 34th minute, our estimates \\(\\hat\\lambda^A_t\\) and \\(\\hat\\lambda^B_t\\) change accordingly. There is a jump in implied volatility and our model captures the market expectation adjustment about the game prediction. The change in \\(\\hat\\lambda_A\\) and \\(\\hat\\lambda_B\\) are consistent with the findings of Vecer, Kopriva, and Ichiba (2009) where the scoring intensity of the penalized team drops while the scoring intensity of the opposing team increases. When a goal is scored in the 13th minute, we see the increase of \\(\\hat\\lambda^B_t\\) and the market expects that the underdog team is pressing to come back into the game, an effect that has been well-documented in the literature. Another important effect that we observe at the end of the game is that as goals are scored (in the 78th and 81st minutes), the markets expectation is that the implied volatility increases again as one might expect.\n\n\n\n\n\n\nFigure 7.6: Red line: the path of implied volatility throughout the game, i.e., \\(\\sigma_{t}^{red} = \\sqrt{\\hat\\lambda^A_t+\\hat\\lambda^B_t}\\). Blue lines: the path of implied volatility with constant \\(\\lambda^A+\\lambda^B\\), i.e., \\(\\sigma_{t}^{blue} = \\sqrt{(\\lambda^A+\\lambda^B)*(1-t)}\\). Here \\((\\lambda^A+\\lambda^B) = 1, 2, ..., 8\\).\n\n\n\n\n\n\nTable 7.6: The calibrated \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) divided by \\((1-t)\\) and the implied volatility during the game. \\(\\{\\lambda^A_t, \\lambda^B_t\\}\\) are expected goals scored for rest of the game. The less the remaining time, the less likely to score goals. Thus \\(\\{\\hat\\lambda^A_t, \\hat\\lambda^B_t\\}\\) decrease as \\(t\\) increases to 1. Dividing them by \\((1-t)\\) produces an updated version of \\(\\hat\\lambda_{0}\\)’s for the whole game, which are in general time-varying (but not decreasing necessarily).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt\n0\n0.11\n0.22\n0.33\n0.44\n0.50\n0.61\n0.72\n0.83\n0.94\n1\n\n\n\n\n\\(\\hat\\lambda^A_t/(1-t)\\)\n2.33\n2.51\n2.53\n2.46\n1.89\n1.85\n2.12\n2.12\n2.61\n4.61\n0\n\n\n\\(\\hat\\lambda^B_t/(1-t)\\)\n1.44\n1.47\n1.59\n1.85\n2.17\n2.17\n2.56\n2.90\n3.67\n5.92\n0\n\n\n\\((\\hat\\lambda^A_t+\\hat\\lambda^B_t)/(1-t)\\)\n3.78\n3.98\n4.12\n4.31\n4.06\n4.02\n4.68\n5.03\n6.28\n10.52\n0\n\n\n\\(\\sigma_{IV,t}\\)\n1.94\n1.88\n1.79\n1.70\n1.50\n1.42\n1.35\n1.18\n1.02\n0.76\n0\n\n\n\n\n\n\nFigure 7.6 compares the updating implied volatility of the game with implied volatilities of fixed \\((\\lambda^A+\\lambda^B)\\). At the beginning of the game, the red line (updating implied volatility) is under the “(\\(\\lambda^A+\\lambda^B=4)\\)”-blue line; while at the end of the game, it’s above the “(\\(\\lambda^A+\\lambda^B=8)\\)”-blue line. As we expect, the value of \\((\\hat\\lambda^A_t + \\hat\\lambda^B_t)/(1-t)\\) in Table 7.6 increases throughout the game, implying that the game became more and more intense and the market continuously updates its belief in the odds.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#the-lévy-itô-decomposition-in-finance",
    "href": "07-sp.html#the-lévy-itô-decomposition-in-finance",
    "title": "7  Stochastic Processes",
    "section": "7.4 The Lévy-Itô Decomposition in Finance",
    "text": "7.4 The Lévy-Itô Decomposition in Finance\nOne of the most profound results in the theory of stochastic processes is the Lévy-Itô decomposition theorem, which provides a universal framework for understanding how randomness evolves over time. The theorem states that any Lévy process (a stochastic process with stationary and independent increments) can be uniquely decomposed into three fundamental components:\n\nA deterministic drift term (linear trend)\nA continuous Gaussian component (Brownian motion)\nA pure jump component (compound Poisson process)\n\nMathematically, any Lévy process \\(X_t\\) can be written as: \\[\nX_t = \\mu t + \\sigma B_t + J_t\n\\] where \\(\\mu\\) is the drift coefficient, \\(B_t\\) is standard Brownian motion with volatility \\(\\sigma\\), and \\(J_t\\) represents the jump component that can be expressed as: \\[\nJ_t = \\sum_{i=1}^{N_t} Z_i\n\\] where \\(N_t\\) is a Poisson process counting the number of jumps up to time \\(t\\), and \\(Z_i\\) are the jump sizes.\nThis decomposition is remarkable because it tells us that no matter how complex a stochastic process might appear, if it has independent and stationary increments, it can always be broken down into these three intuitive building blocks: a predictable trend, continuous random fluctuations, and discrete jumps.\nThe Lévy-Itô decomposition provides a natural motivation for studying Brownian motion and Poisson processes as fundamental objects. Brownian motion captures the continuous, infinitesimal random perturbations that accumulate over time, while the Poisson process models rare, discrete events that cause sudden changes in the system state. Together with a deterministic drift, these components form a complete toolkit for modeling virtually any phenomenon with independent increments.\nThe practical importance of this decomposition cannot be overstated. In finance, asset returns exhibit both continuous price movements (modeled by Brownian motion) and sudden jumps due to news announcements or market shocks (modeled by Poisson processes). In telecommunications, network traffic consists of a steady baseline load (drift) plus continuous fluctuations (Brownian component) and sudden spikes from large file transfers (jumps). In insurance, claim amounts follow a baseline trend with continuous variation and occasional catastrophic events.\n\nExample 7.4 (Financial Asset Prices) Consider modeling the logarithm of a stock price. The Lévy-Itô decomposition suggests we should account for:\n\nDrift: The expected return on the asset, reflecting long-term growth trends in the economy\nBrownian component: Day-to-day price fluctuations driven by the continuous arrival of market information and trading activity\nJump component: Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks\n\nFor instance, during the 2008 financial crisis, stock prices exhibited massive downward jumps that could not be explained by a pure Brownian motion model. The Lehman Brothers bankruptcy on September 15, 2008 caused the S&P 500 to drop by 4.7% in a single day—an event that would have probability essentially zero under a Gaussian model but is naturally accommodated by the jump component in the Lévy-Itô framework.\nThe practical application of this decomposition led to the development of jump-diffusion models in quantitative finance, where option prices are calculated by accounting for both continuous price movements and discrete jumps. This approach provides more realistic pricing and risk assessment compared to the classical Black-Scholes model, which assumes only continuous price movements.\nThe Lévy-Itô decomposition thus provides both theoretical insight and practical tools. It explains why Brownian motion and Poisson processes are the fundamental building blocks for continuous-time stochastic modeling, and it gives practitioners a principled framework for decomposing complex random phenomena into interpretable components that can be estimated, simulated, and managed separately.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#newton-and-the-south-sea-bubble",
    "href": "07-sp.html#newton-and-the-south-sea-bubble",
    "title": "7  Stochastic Processes",
    "section": "7.5 Newton and the South Sea Bubble",
    "text": "7.5 Newton and the South Sea Bubble\nThe South Sea Bubble of 1720 stands as one of history’s most spectacular financial disasters, demonstrating that even the greatest scientific minds can fall victim to speculative mania. The South Sea Company, established in 1711 ostensibly to trade with South America, proposed an audacious scheme: it would assume England’s national debt in exchange for company shares and exclusive trading privileges. By early 1720, the company’s directors launched an unprecedented campaign of stock manipulation, spreading rumors of fabulous wealth, bribing politicians and royalty, and offering generous credit terms that allowed investors to purchase shares with only a small down payment. The stock price soared from £128 at the start of 1720 to over £1,000 by June—an eightfold increase in six months (Figure 7.7). Sir Isaac Newton, then Master of the Royal Mint, initially profited by selling his holdings in April 1720 for £7,000, but he could not resist re-entering the market after watching shares continue to climb. When the bubble burst in September, Newton lost £20,000—several years of his salary—leading him to famously remark, “I can calculate the movement of stars, but not the madness of men.” His experience reveals how market psychology and the fear of missing out can overwhelm even the most disciplined rational minds.\n\n\n\n\n\n\nFigure 7.7: South Sea Bubble\n\n\n\nIn the midst of this frenzy, Parliament passed the Bubble Act in June 1720, ironically just as South Sea stock reached its peak. While ostensibly designed to protect investors from fraudulent schemes, the Act was actually lobbied for by South Sea Company directors seeking to eliminate competition from rival ventures attracting investor capital. The Act required all joint-stock companies to obtain expensive royal charters and imposed severe penalties on unauthorized companies, effectively giving the South Sea Company a monopoly on investor enthusiasm. Paradoxically, by forcing the shutdown of smaller speculative ventures and drying up alternative investments, the Act may have hastened the bubble’s collapse by causing investors to question the sustainability of the broader market euphoria. The Act’s unintended consequences proved profound and long-lasting—it severely restricted the development of joint-stock companies in Britain for over a century until its repeal in 1825, arguably impeding industrialization by creating legal obstacles for large-scale ventures requiring significant capital. The episode offers enduring lessons about financial markets: price bubbles exhibit the characteristics of non-stationary stochastic processes with time-varying volatility and jump risk, leverage amplifies both gains and losses, regulatory interventions can create unintended consequences, and even rational agents can behave irrationally when caught in speculative manias—all phenomena that modern stochastic models attempt to capture.\nThe plot of the prices reveals the interconnected nature of early 18th-century financial manias and demonstrates the stochastic features that modern models attempt to capture. The South Sea Bubble exhibits the classic pattern of explosive growth followed by catastrophic collapse—a dramatic jump discontinuity in September 1720 that cannot be explained by continuous Brownian motion alone. Remarkably, the contagion spread across markets: the Bank of England, Royal African Company, and Old East India Company all show synchronized price movements during 1720, rising in sympathy with the South Sea speculation before experiencing their own sharp corrections. Most striking is the Mississippi Company plot, which tracks John Law’s concurrent bubble in France—it peaked slightly earlier than the South Sea Bubble and collapsed even more precipitously, suggesting that speculative manias can propagate across national borders. The synchronization across these four series illustrates volatility clustering and correlation jumps, phenomena that motivate the stochastic volatility models with correlated jumps discussed later in this chapter. These price paths exhibit all three components of the Lévy-Itô decomposition: drift during the accumulation phase, continuous Brownian fluctuations throughout, and sudden Poisson jumps at the moment of collapse.\n\n\n\n\n\nHistorical Bubbles",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "07-sp.html#stochastic-volatility-financial-economics",
    "href": "07-sp.html#stochastic-volatility-financial-economics",
    "title": "7  Stochastic Processes",
    "section": "7.6 Stochastic Volatility: Financial Economics",
    "text": "7.6 Stochastic Volatility: Financial Economics\nFinancial markets exhibit time-varying volatility—periods of calm trading alternate with episodes of extreme price movements. The October 1987 crash dramatically illustrated the limitations of constant volatility models: the Dow Jones index fell 23% in a single day, an event that had probability essentially zero under the lognormal model assumed by the Black-Scholes framework. This observation motivated the development of stochastic volatility models that allow uncertainty itself to evolve randomly over time.\nRobert Merton, who was a student of Samuelson, proposed a major extension to the work of Bachelier by introducing jumps to the model. The additive jump term addresses the issues of asymmetry and heavy tails in the distribution. Merton’s Jump Stochastic volatility model has a discrete-time version for log-returns, \\(y_t\\), with jump times, \\(J_t\\), jump sizes, \\(Z_t\\), and spot stochastic volatility, \\(V_t\\), given by the dynamics \\[\\begin{align*}\n    y_{t} & \\equiv \\log \\left( S_{t}/S_{t-1}\\right) =\\mu + V_t \\varepsilon_{t}+J_{t}Z_{t} \\\\V_{t+1} & = \\alpha_v + \\beta_v V_t + \\sigma_v \\sqrt{V_t} \\varepsilon_{t}^v\n\\end{align*}\\] where \\(\\mathbb{P} \\left ( J_t =1 \\right ) = \\lambda\\), \\(S_t\\) denotes a stock or asset price and log-returns \\(y^t = (y_1,\\ldots,y_t)\\) are the log-returns. The errors \\((\\varepsilon_{t},\\varepsilon_{t}^v)\\) are possibly correlated bivariate normals. The investor must obtain optimal filters for \\((V_t,J_t,Z_t)\\), and learn the posterior densities of the parameters \\((\\mu, \\alpha_v, \\beta_v, \\sigma_v^2 , \\lambda )\\). These estimates will be conditional on the information available at each time.\n\nMotivation: Combining Brownian Motion and Jumps\nThe Lévy-Itô decomposition provides the theoretical foundation for modeling asset prices. Recall that any Lévy process can be decomposed into three fundamental components: deterministic drift, continuous Brownian fluctuations, and discrete jumps. In finance, this decomposition maps naturally to observed price dynamics:\n\nDrift (\\(\\mu t\\)): The expected return on the asset, reflecting long-term growth trends\nBrownian component (\\(\\sigma B_t\\)): Continuous price fluctuations driven by the steady arrival of market information\n\nJump component (\\(\\sum_{i=1}^{N_t} Z_i\\)): Sudden price movements triggered by earnings announcements, regulatory changes, or macroeconomic shocks\n\nTraditional models like Black-Scholes use only the first two components, assuming constant volatility \\(\\sigma\\). However, empirical evidence overwhelmingly shows that volatility itself is stochastic and exhibits its own patterns: it clusters (high volatility follows high volatility), it mean-reverts to long-run averages, and it can experience sudden jumps during crises.\nStochastic volatility models extend the Lévy-Itô framework by allowing the volatility parameter to follow its own stochastic process. The most general formulation combines:\n\nBrownian motion for continuous price and volatility fluctuations\nPoisson processes for rare but important jump events in both prices and volatility\nCorrelation structure to capture the leverage effect—the empirical observation that volatility tends to rise when prices fall\n\nThis integration of the two fundamental stochastic processes creates a flexible modeling framework capable of capturing the rich dynamics observed in financial markets.\n\nExample 7.5 (Financial Crashes and the Need for Jumps) Consider the distribution of daily S&P 500 returns. Under a Gaussian model with annualized volatility of 15%, a one-day drop of 5% should occur roughly once every 10,000 years. Yet such events occurred multiple times in recent decades: October 1987 (-20.5%), October 2008 (-9.0%), March 2020 (-12.0%). The empirical distribution exhibits heavy tails—extreme events occur far more frequently than predicted by the normal distribution.\nJump-diffusion models accommodate these events naturally. Instead of treating crashes as impossible outliers, they model them as rare but expected occurrences from the Poisson jump component. This provides more realistic risk assessment and option pricing, particularly for out-of-the-money puts that protect against market crashes.\n\n\n\nThe Stochastic Volatility Model\nThe basic stochastic volatility (SV) model extends the geometric Brownian motion of Black-Scholes by allowing volatility to evolve as a latent stochastic process. In continuous time, the log-price \\(\\log S_t\\) and its variance \\(v_t\\) jointly evolve as:\n\\[\\begin{align*}\nd\\log S_{t} &= \\mu dt + \\sqrt{v_t} dB_{t}^{s}  \\\\\nd\\log v_{t} &= \\kappa_{v}(\\theta_{v} - \\log v_t) dt + \\sigma_{v} dB_{t}^{v}\n\\end{align*}\\]\nwhere \\(B_{t}^{s}\\) and \\(B_{t}^{v}\\) are (potentially correlated) Brownian motions. The variance follows a mean-reverting process in logs with:\n\n\\(\\theta_v\\): Long-run average log-variance\n\\(\\kappa_v\\): Speed of mean reversion (how quickly volatility returns to its average)\n\\(\\sigma_v\\): Volatility of volatility (how much randomness in the volatility process)\n\nDiscretizing this model at daily or weekly intervals yields the discrete-time specification:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s}  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v} \\varepsilon_{t}^{v}\n\\end{align*}\\]\nwhere \\(y_t = \\log(S_t/S_{t-1})\\) are log-returns, \\(\\varepsilon_{t}^{s}, \\varepsilon_{t}^{v} \\sim N(0,1)\\) are standard normal innovations, and the parameters relate to the continuous-time specification via \\(\\alpha_v = \\kappa_v \\theta_v \\Delta\\) and \\(\\beta_v = 1 - \\kappa_v \\Delta\\) for time interval \\(\\Delta\\).\nThis model exhibits several desirable features:\n\nVolatility clustering: Since \\(\\log v_t\\) follows an AR(1), periods of high volatility tend to persist\nStationarity: The mean-reverting specification ensures volatility doesn’t explode or collapse to zero\nFlexibility: The correlation between \\(\\varepsilon_t^s\\) and \\(\\varepsilon_t^v\\) allows for leverage effects\n\nAn important empirical regularity in equity markets is that volatility tends to increase when prices fall—a phenomenon known as the leverage effect. While originally attributed to changing debt-to-equity ratios as stock prices move, it is now understood as a more general feature of risk dynamics.\nTo incorporate the leverage effect, we allow the innovations in returns and volatility to be correlated: \\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s}  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v}\\left[\\rho \\varepsilon_{t}^{s} + \\sqrt{1-\\rho^2} \\varepsilon_{t}^{v}\\right]\n\\end{align*}\\] where \\(\\rho &lt; 0\\) for equity returns. A negative return shock (\\(\\varepsilon_t^s &lt; 0\\)) directly increases log-volatility through the \\(\\rho\\) term, generating the observed inverse relationship between prices and volatility.\nWhile stochastic volatility captures the time-varying nature of market uncertainty, it still relies on continuous Brownian motion for price movements. To accommodate the extreme events and heavy tails observed in returns, we augment the model with jump components—invoking the full Lévy-Itô decomposition.\nThe stochastic volatility with jumps (SVJ) model extends the basic SV specification by adding a Poisson-driven jump process to returns: \\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s} + J_t Z_t  \\\\\n\\log v_{t} &= \\alpha_{v} + \\beta_{v} \\log v_{t-1} + \\sigma_{v} \\varepsilon_{t}^{v}\n\\end{align*}\\] where:\n\n\\(J_t \\sim \\text{Bernoulli}(\\lambda)\\) indicates whether a jump occurs at time \\(t\\)\n\\(Z_t \\sim N(\\mu_Z, \\sigma_Z^2)\\) is the jump size when a jump occurs\n\\(\\lambda\\) is the jump intensity (probability of a jump per period)\n\nThe total variance of returns now decomposes into two sources: \\[\n\\text{Var}(y_t) = \\E{v_{t-1}} + \\lambda E[Z_t^2]\n\\]\nThe first term captures diffusive volatility from continuous fluctuations, while the second captures jump variance from discrete events. This allows the model to simultaneously fit the day-to-day variations (through \\(v_t\\)) and occasional crashes (through jumps).\nThe 2008 financial crisis revealed another important feature: volatility itself experiences sudden jumps. The VIX index (a measure of market volatility expectations) more than doubled in a matter of days during the Lehman Brothers collapse. To capture this, we extend the model to allow jumps in both returns and volatility.\n\n\n\n\n\n\nFigure 7.8: Stochastic Volatility Model\n\n\n\nThe graphical structure of this model is shown in Figure 7.8, illustrating how the latent volatility process \\(v_t\\) evolves over time and influences the observed returns \\(y_t\\), with jumps \\(J_t\\) affecting both processes simultaneously.\nThe stochastic volatility with correlated jumps (SVCJ) model specifies:\n\\[\\begin{align*}\ny_{t} &= \\mu + \\sqrt{v_{t-1}} \\varepsilon_{t}^{s} + J_t Z_t  \\\\\nv_{t} &= \\alpha_{v} + \\beta_{v} v_{t-1} + \\sigma_{v}\\sqrt{v_{t-1}} \\varepsilon_{t}^{v} + J_t W_t\n\\end{align*}\\]\nwhere:\n\nThe same Bernoulli \\(J_t\\) triggers jumps in both returns and volatility (correlated jumps)\n\\(Z_t | W_t \\sim N(\\mu_Z + \\rho_J W_t, \\sigma_Z^2)\\) allows jump sizes to be correlated\n\\(W_t \\sim \\text{Exponential}(\\mu_W)\\) ensures volatility jumps are positive\n\nThe correlation parameter \\(\\rho_J &lt; 0\\) captures the empirical finding that large negative return jumps are typically accompanied by large positive volatility jumps. For example, during the March 2020 COVID-19 crash, the S&P 500 fell sharply while the VIX spiked to record levels.\nAn even more flexible specification, the stochastic volatility with independent jumps (SVIJ) model, allows jumps in returns and volatility to occur independently, governed by separate Poisson processes with intensities \\(\\lambda_Z\\) and \\(\\lambda_W\\). This provides maximum flexibility but requires more data to estimate reliably.\nTo understand the empirical importance of these model features, consider parameter estimates from S&P 500 daily returns (1980-1999):\n\n\n\nTable 7.7: Model features\n\n\n\n\n\nFeature\nSV\nSVJ\nSVCJ\nSVIJ\n\n\n\n\nStochastic volatility\n+\n+\n+\n+\n\n\nReturn jumps\n–\n+\n+\n+\n\n\nVolatility jumps\n–\n–\n+\n+\n\n\nIndependent jumps\n–\n–\n–\n+\n\n\n\n\n\n\nThe estimated average annualized volatility across models is remarkably stable (around 15%), closely matching the sample standard deviation of 16%. However, the decomposition of variance sources differs:\n\nSV model: All variation comes from the stochastic volatility component\nSVJ model: 85% from stochastic volatility, 15% from return jumps\nSVCJ model: 90% from stochastic volatility, 10% from return jumps\nSVIJ model: 92% from stochastic volatility, 8% from return jumps\n\nThe diminishing role of return jumps as we add volatility jumps reflects an important finding: much of what appears as “jumps in returns” in simpler models is actually driven by jumps in volatility. When volatility suddenly spikes, even Brownian motion can generate large price movements that might be misidentified as jumps.\nThe mean reversion parameter \\(\\kappa_v\\) also varies across specifications. In the SVCJ and SVIJ models, \\(\\kappa_v\\) roughly doubles compared to the SV model, indicating that volatility reverts more quickly when jumps account for sudden large moves. The volatility-of-volatility parameter \\(\\sigma_v\\) correspondingly falls, as jumps handle the extreme variations.\n\n\nBayesian Inference for Stochastic Volatility Models\nEstimating stochastic volatility models presents a significant challenge: the volatility \\(v_t\\) is never directly observed, appearing as a latent state variable. Classical maximum likelihood approaches require integrating out the entire volatility path, which is computationally intractable for nonlinear models with jumps.\nThe Bayesian approach via MCMC provides an elegant solution by treating the latent volatilities and jump indicators as parameters to be sampled alongside model parameters. The Clifford-Hammersley theorem structures the algorithm efficiently.\nFor the basic SV model with parameter vector \\(\\theta = (\\alpha_v, \\beta_v, \\sigma_v^2)\\) and latent volatilities \\(v = (v_1, \\ldots, v_T)\\), the joint posterior factors as:\n\\[\np(\\theta, v | y) \\propto p(y | v) p(v | \\theta) p(\\theta)\n\\]\nThe MCMC algorithm alternates between:\n\nParameter update: \\(p(\\theta | v, y)\\)\n\nGiven volatilities, returns are conditionally normal: \\(y_t | v_{t-1} \\sim N(\\mu, v_{t-1})\\)\nLog-volatilities follow AR(1): \\(\\log v_t | \\log v_{t-1} \\sim N(\\alpha_v + \\beta_v \\log v_{t-1}, \\sigma_v^2)\\)\nWith conjugate priors, conditional posteriors are standard (Normal-Inverse-Gamma)\n\nVolatility update: \\(p(v_t | v_{t-1}, v_{t+1}, \\theta, y)\\)\n\nThe conditional posterior for each \\(v_t\\) combines information from:\n\nThe likelihood \\(p(y_{t+1} | v_t)\\) (observed return depends on current volatility)\nThe state evolution \\(p(v_t | v_{t-1})\\) (Markov dynamics from previous period)\nThe forward evolution \\(p(v_{t+1} | v_t)\\) (Markov dynamics to next period)\n\nThis distribution is non-standard and requires Metropolis-Hastings sampling\n\n\nFor the jump-augmented models (SVJ, SVCJ, SVIJ), we additionally sample:\n\nJump indicators: \\(p(J_t | v, Z, \\theta, y)\\)\n\nEach \\(J_t \\in \\{0,1\\}\\) follows a Bernoulli posterior\nLarge observed returns increase the probability of \\(J_t = 1\\)\n\nJump sizes: \\(p(Z_t | J_t = 1, v, \\theta, y)\\)\n\nConditional on a jump occurring, the jump size has a Normal posterior\nThe posterior mean balances the jump prior and the size needed to explain the observed return\n\n\nThis modular structure allows us to build up from simpler models (SV) to more complex specifications (SVIJ) by adding components one at a time, reusing the same basic algorithmic building blocks.\nStochastic volatility models with jumps have become standard tools in quantitative finance for several applications:\nOption Pricing: The Black-Scholes model systematically misprices options, particularly out-of-the-money puts. The volatility smile—the observation that implied volatilities increase for strikes far from the current price—reflects the market’s recognition of jump risk and stochastic volatility. Jump-diffusion models with stochastic volatility can reproduce these patterns, providing more accurate prices and hedging strategies.\nRisk Management: Value-at-Risk (VaR) and Expected Shortfall calculations based on constant-volatility Gaussian models dramatically underestimate tail risks. By properly accounting for stochastic volatility and jumps, firms can better quantify their exposure to extreme market movements. During the 2008 crisis, many institutions discovered their VaR models had severely underestimated potential losses.\nPortfolio Allocation: The presence of stochastic volatility creates hedging demands even for long-horizon investors. An investor who correctly anticipates that volatility is mean-reverting will reduce equity exposure when volatility is high (because expected returns are temporarily compressed) and increase exposure when volatility is low. This generates countercyclical trading strategies.\nMarket Timing: The predictable component of volatility can be exploited for tactical asset allocation. Since volatility tends to mean-revert, unusually high volatility signals elevated future returns (as compensation for risk), making it an opportune time to increase risky asset exposure. Conversely, unusually low volatility may warrant defensive positioning.\nThe integration of Brownian motion and Poisson processes through stochastic volatility models exemplifies how the Lévy-Itô decomposition provides not just mathematical elegance, but practical power for understanding and managing financial risk in modern markets. The integration of Brownian motion and Poisson processes through stochastic volatility models exemplifies how the Lévy-Itô decomposition provides not just mathematical elegance, but practical power for understanding and managing financial risk in modern markets.\n\n\n\n\nA. N. Kolmogorov. 1938. “On the Analytic Methods of Probability Theory.” Rossíiskaya Akademiya Nauk, no. 5: 5–41.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of Poincaré.” Russian Mathematical Surveys 61 (1): 1.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market Prices. MIT press.\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol. 11. Cambridge university press.\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling Association Football Scores and Inefficiencies in the Football Betting Market.” Journal of the Royal Statistical Society Series C: Applied Statistics 46 (2): 265–80.\n\n\nFeng, Guanhao, Nicholas G. Polson, and Jianeng Xu. 2016. “The Market for English Premier League (EPL) Odds.” Journal of Quantitative Analysis in Sports 12 (4). https://arxiv.org/abs/1604.03614.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen Methoden in Der Wahrscheinlichkeitsrechnung.” Mathematische Annalen 104 (1): 415–58.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity Theory.” https://arxiv.org/abs/physics/0408077.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de Métaphysique Et de Morale 6 (1): 1–13.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability Theory.” In Selected Works of a. N. Kolmogorov: Volume II Probability Theory and Mathematical Statistics, edited by A. N. Shiryayev, 62–108. Dordrecht: Springer Netherlands.\n\n\nStern, Hal S. 1994. “A Brownian Motion Model for the Progress of Sports Scores.” Journal of the American Statistical Association 89 (427): 1128–34.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009. “Estimating the Effect of the Red Card in Soccer: When to Commit an Offense in Exchange for Preventing a Goal Opportunity.” Journal of Quantitative Analysis in Sports 5 (1).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Stochastic Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html",
    "href": "08-gp.html",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "8.1 Making Predictions with Gaussian Processes\nIn traditional regression, we often find a single “best” line that fits the data (e.g., \\(y = mx + b\\)). But reality is rarely so simple. What if we want to model not just the trend, but our uncertainty about it? What if, instead of committing to a single function, we could consider an infinite number of possible functions consistent with our data?\nA Gaussian Process (GP) allows us to do exactly that. Instead of estimating parameters for a specific function, we define a probability distribution over all possible functions that fit the data. It is a powerful, non-parametric tool used extensively in machine learning for tasks ranging from robotic control to geospatial analysis.\nFormally, a GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. A finite collection of \\(n\\) points drawn from a GP is completely specified by its \\(n\\)-dimensional mean vector \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\mathbf{\\Sigma}\\). We assume the process is indexed by a real variable \\(x\\in\\mathbb{R}\\) (e.g., time or space) and has real-valued outputs. The GP is defined by: 1. Mean function \\(m(x) = \\mathbb{E}[f(x)]\\): The expected value of the function at point \\(x\\). 2. Covariance (Kernel) function \\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\): A measure of similarity between values at \\(x\\) and \\(x'\\).\nWe denote this as: \\[\nf(x) \\sim \\mathcal{GP}(m(x), k(x, x')).\n\\]\nIntuitively, the kernel function determines the “shape” and “smoothness” of the functions we expect to see. If \\(x\\) and \\(x'\\) are close, \\(k(x, x')\\) should be high, implying \\(f(x)\\) and \\(f(x')\\) are likely similar.\nIn practice, we often assume a zero mean, \\(m(x)=0\\), and focus on the covariance kernel. The choice of kernel encodes our prior beliefs about the data. The most common choice is the Squared Exponential (SE) kernel (also known as the Radial Basis Function or RBF):\n\\[\nk(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2l^2}\\right)\n\\]\nHere, we have two hyperparameters:\nObserve that \\(k(x,x) = \\sigma^2\\) and \\(k(x,x') \\rightarrow 0\\) as the distance \\(|x-x'| \\rightarrow \\infty\\).\nWe can illustrate a GP with a simulated example. First generate a sequence of 100 input points (indices)\nand then define the mean function and the covariance function\nThe covariance function depends only on the distance between two points, not on their absolute values. The squared exponential kernel is infinitely differentiable, which means that the GP is a very smooth function. The squared exponential kernel is also called the radial basis function (RBF) kernel. The covariance matrix is then defined as\nand we can generate a sample from the GP using the mvrnorm function from the MASS package and plot a sample.\nFigure 8.1 displays 100 values of a function \\(f(x)\\) drawn from a GP with zero mean and a squared-exponential kernel at inputs \\(x=(0,0.1,0.2,\\ldots,10)\\). The realisation is smooth, with most values lying between -2 and 2. Because each diagonal element of the covariance matrix equals \\(\\sigma^2=1\\), the marginal variance is one. By properties of the normal distribution, approximately 95 percent of the points of \\(Y\\) should therefore fall within 1.96 standard deviations of the mean. The mild oscillations arise because values with neighbouring indices are highly correlated.\nWe can generate a few more samples from the same GP and plot them together\nEach finite sample path differs from the next, yet all share a similar range, a comparable number of bumps, and overall smoothness. That’s what it means to have function realizations under a GP prior: \\(Y = f(x) \\sim \\mathcal{GP}(0, k(x, x'))\\)\nSimulating from the prior shows us the richness of possible functions we can model. However, our goal is not just to generate random curves, but to learn. We want to constrain these possibilities using actual observations.\nSuppose our observed data consists of \\(n\\) inputs \\(\\mathbf{X}=(x_1,\\ldots,x_n)^T\\) and outputs \\(\\mathbf{y}=(y_1,\\ldots,y_n)^T\\). We assume these are a realization of a GP. Our goal is to predict the outputs \\(\\mathbf{y}_*\\) at new inputs \\(\\mathbf{X}_*\\).\nBy definition of a GP, the joint distribution of the observed data \\(\\mathbf{y}\\) and the predictions \\(\\mathbf{y}_*\\) is a multivariate Gaussian:\n\\[\n\\begin{bmatrix} \\mathbf{y} \\\\ \\mathbf{y}_* \\end{bmatrix} \\sim \\mathcal{N} \\left ( \\begin{bmatrix} \\boldsymbol{\\mu} \\\\ \\boldsymbol{\\mu}_* \\end{bmatrix}, \\begin{bmatrix} \\mathbf{K} & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**} \\end{bmatrix} \\right )\n\\]\nwhere: * \\(\\mathbf{K} = k(\\mathbf{X}, \\mathbf{X})\\in \\mathbb{R}^{n\\times n}\\) is the covariance of the training data. * \\(\\mathbf{K}_* = k(\\mathbf{X}, \\mathbf{X}_*)\\in \\mathbb{R}^{n\\times q}\\) is the covariance between training and test data. * \\(\\mathbf{K}_{**} = k(\\mathbf{X}_*, \\mathbf{X}_*) \\in \\mathbb{R}^{q\\times q}\\) is the covariance of the test data. * \\(\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{y}]\\) and \\(\\boldsymbol{\\mu}_* = \\mathbb{E}[\\mathbf{y}_*]\\).\nThe beauty of Gaussians is that conditioning on observed data is closed-form. The conditional distribution of \\(\\mathbf{y}_*\\) given \\(\\mathbf{y}\\) is:\n\\[\n\\mathbf{y}_* \\mid \\mathbf{y}, \\mathbf{X}, \\mathbf{X}_* \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathrm{post}}, \\boldsymbol{\\Sigma}_{\\mathrm{post}})\n\\]\nThe posterior mean \\(\\boldsymbol{\\mu}_{\\mathrm{post}}\\) serves as our prediction, and the posterior covariance \\(\\boldsymbol{\\Sigma}_{\\mathrm{post}}\\) quantifies our uncertainty:\n\\[\n\\boldsymbol{\\mu}_{\\mathrm{post}} = \\boldsymbol{\\mu}_* + \\mathbf{K}_*^T\\mathbf{K}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu})\n\\tag{8.1}\\]\n\\[\n\\boldsymbol{\\Sigma}_{\\mathrm{post}} = \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}^{-1} \\mathbf{K}_*\n\\tag{8.2}\\]\nEquation 8.1 and Equation 8.2 are standard properties of the multivariate normal distribution (see Chapter 3 and Appendix Chapter 26). Intuitively, the posterior variance \\(\\boldsymbol{\\Sigma}_{\\mathrm{post}}\\) is equal to the prior variance \\(\\mathbf{K}_{**}\\) minus a term representing the information gained from the observed data. This structure reflects a fundamental Bayesian principle: data acts to reduce our prior uncertainty (represented by the second term in Equation 8.2).\nNow, instead of using GP to fit a known function (\\(\\sin\\)), we will apply it to a real-world data set. We will use the motorcycle accident data set from the MASS package. The data set contains accelerator readings taken through time in a simulated experiment on the efficacy of crash helmets. This data is non-linear and exhibits varying curvature, making it an excellent candidate for GP regression where linear models would fail.\nIn summary, Gaussian Processes provide a robust and flexible framework for modeling functions where uncertainty is key. By defining a prior over functions and updating it with data, we obtain a posterior distribution that captures both predictions and the confidence in those predictions. The key features of GPs are:",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "08-gp.html#making-predictions-with-gaussian-processes",
    "href": "08-gp.html#making-predictions-with-gaussian-processes",
    "title": "8  Gaussian Processes",
    "section": "",
    "text": "Example 8.1 (Gaussian Process for \\(\\sin\\) function) We can use the GP to make predictions about the output values at new inputs \\(x_*\\). We use \\(x\\) in the [0,\\(2\\pi\\)] range and \\(y\\) to be the \\(y = \\sin(x)\\). We start by simulating the observed \\(x\\)-\\(y\\) pairs.\n\nn = 8; eps=1e-6\nX = matrix(seq(0, 2*pi, length=n), ncol=1)\nY = sin(X)\nK = outer(X[,1], X[,1], sqexpcov) + diag(eps, n)\n\nThe additive term diag(eps, n) corresponds to adding \\(\\epsilon \\mathbf{I}\\), which stabilizes the matrix inversion by ensuring strict positive definiteness; in machine learning practice, this is known as ‘jitter’. Now we implement a function that calculates the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\nNow we generate a new set of inputs \\(x_*\\) and calculate the covariance matrices \\(K_*\\) and \\(K_{**}\\).\n\nq = 100\nXX = matrix(seq(-0.5, 2*pi + 0.5, length=q), ncol=1)\nKX = outer(X[,1], XX[,1],sqexpcov)\nKXX = outer(XX[,1],XX[,1], sqexpcov) + diag(eps, q)\n\nNotice that we did not add \\(\\epsilon I\\) to \\(K_*\\) = KX matrix, but we add it to \\(K_{**}\\) = KXX to ensure the posterior covariance is invertible. We do not add it to \\(K_*\\) (KX) as it represents cross-covariance, which does not need to be positive definite. Now we can calculate the mean and covariance of the posterior distribution of \\(y_*\\) given \\(Y\\).\n\nSi = solve(K)\nmup = t(KX) %*% Si %*% Y # we assume mu is 0\nSigmap = KXX - t(KX) %*% Si %*% KX\n\nNow, we can generate a sample from the posterior distribution over \\(y_*\\), given \\(Y\\)\n\nYY = mvrnorm(100, mup, Sigmap)\n\nUsing our convenience function plot_gp we can plot the posterior distribution over \\(y_*\\), given \\(Y\\).\n\nplot_gp = function(mup, Sigmap, X, Y, XX, YY){\n  q1 = mup + qnorm(0.05, 0, sqrt(diag(Sigmap)))\n  q2 = mup + qnorm(0.95, 0, sqrt(diag(Sigmap)))\n  matplot(XX, t(YY), type=\"l\", col=\"gray\", lty=1, xlab=\"x\", ylab=\"y\")\n  points(X, Y, pch=20, cex=2)\n  lines(XX, sin(XX), col=\"blue\")\n  lines(XX, mup, lwd=2)\n  lines(XX, q1, lwd=2, lty=2, col=2)\n  lines(XX, q2, lwd=2, lty=2, col=2)\n}\n\n\nplot_gp(mup, Sigmap, X, Y, XX, YY)\n\n\n\n\n\n\n\nFigure 8.3: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\n\n\n\n\nExample 8.2 (Gaussian Process for Simulated Data using MLE) In the previous example, we assumed fixed values for the hyperparameters: \\(\\sigma^2 = 1\\) and \\(l^2 = 0.5\\) (since \\(2l^2=1\\)). In real applications, we don’t know these values; we must estimate them from the data.\nWe use Maximum Likelihood Estimation (MLE) to find the parameters that maximize the probability of observing our data. This parallels the likelihood-to-loss framing in Chapter 11: we write down a (marginal) likelihood for the data and then optimize it, often via its log-likelihood. This section relies on basic matrix operations (inverse, determinant); Appendix Chapter 26 provides a short refresher. If you have not seen gradient-based optimization, the core intuition appears in Chapter 20; here we use it only as a practical tool to fit GP hyperparameters.\nThe marginal likelihood of the data \\(\\mathbf{y}\\) (integrating out the function values \\(f\\)) is:\n\\[\np(\\mathbf{y} \\mid \\mathbf{X}, \\sigma, l) = \\frac{1}{(2\\pi)^{n/2} |\\mathbf{K}|^{1/2}} \\exp \\left ( -\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}^{-1} \\mathbf{y} \\right )\n\\]\nwhere \\(\\mathbf{K}\\) is the covariance matrix computed with hyperparameters \\(\\sigma\\) and \\(l\\). For numerical stability, we typically maximize the log-likelihood:\n\\[\n\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\sigma, l) = -\\frac{1}{2} \\log |\\mathbf{K}| - \\frac{1}{2} \\mathbf{y}^T \\mathbf{K}^{-1} \\mathbf{y} - \\frac{n}{2} \\log 2\\pi.\n\\]\nThis equation encapsulates Occam’s Razor. The term \\(-\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}^{-1} \\mathbf{y}\\) rewards the model for fitting the data well. The term \\(-\\frac{1}{2} \\log |\\mathbf{K}|\\) penalizes model complexity; a more flexible kernel (e.g., smaller length scale) leads to a simpler determinant term that exacts a cost. MLE automatically balances these two competing objectives to prevent overfitting.\nWe can implement a function that calculates the log-likelihood of the data given the hyperparameters \\(\\sigma\\) and \\(l\\) and use optim function to find the maximum of the log-likelihood function.\n\nloglik = function(par, X, Y) {\n  sigma = par[1]\n  l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  return(-(-0.5 * log(det(K)) - 0.5 * t(Y) %*% Si %*% Y - (n/2)* log(2*pi)))\n}\npar = optim(c(1,1), loglik, X=X, Y=Y)$par\nprint(par)\n## 1.5 2.4\n\nThe optim function returns the hyperparameters that maximize the log-likelihood function. We can now use those hyperparameters to make predictions about the output values at new inputs \\(x_*\\).\n\nl = par[2]; sigma = par[1]\npredplot = function(X, Y, XX, YY, l, sigma) {\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  KX = outer(X[,1], XX[,1],sqexpcov,l,sigma)\n  KXX = outer(XX[,1],XX[,1], sqexpcov,l,sigma) + diag(eps, q)\n  Si = solve(K)\n  mup = t(KX) %*% Si %*% Y # we assume mu is 0\n  Sigmap = KXX - t(KX) %*% Si %*% KX\n  YY = mvrnorm(100, mup, Sigmap)\n  plot_gp(mup, Sigmap, X, Y, XX, YY)\n}\npredplot(X, Y, XX, YY, l, sigma)\n\n\n\n\n\n\n\n\nWe can see that our uncertainty is much narrower—the posterior distribution is considerably tighter. This is because we used the observed data to estimate the hyperparameters. We can also see that the posterior mean is closer to the true function \\(y = \\sin(x)\\). Although our initial guess of \\(\\sigma^2 = 1\\) and \\(2l^2 = 1\\) was not too far off, the model fits the data much better when we use the estimated hyperparameters.\nThe default optim function uses numerical approximations for derivatives. While convenient, this can be slow and less precise. For GPs, we can calculate the analytical gradients of the log-likelihood with respect to the hyperparameters, significantly speeding up optimization.\n\n\n\n\n\n\nNoteMathematical Details: Gradient Derivation\n\n\n\n\n\nTo optimize efficiently, we rely on two matrix calculus identities: \\[\n\\frac{\\partial \\mathbf{K}^{-1}}{\\partial \\theta} = -\\mathbf{K}^{-1} \\frac{\\partial \\mathbf{K}}{\\partial \\theta} \\mathbf{K}^{-1} \\quad \\text{and} \\quad \\frac{\\partial \\log |\\mathbf{K}|}{\\partial \\theta} = \\mathrm{tr} \\left ( \\mathbf{K}^{-1} \\frac{\\partial \\mathbf{K}}{\\partial \\theta} \\right )\n\\] Using these, the gradient of the log-likelihood is: \\[\n\\frac{\\partial \\log p(\\mathbf{y} \\mid \\mathbf{X},\\theta)}{\\partial \\theta} = -\\frac{1}{2} \\mathrm{tr} \\left ( \\mathbf{K}^{-1} \\frac{\\partial \\mathbf{K}}{\\partial \\theta} \\right ) + \\frac{1}{2} \\mathbf{y}^T \\mathbf{K}^{-1} \\frac{\\partial \\mathbf{K}}{\\partial \\theta} \\mathbf{K}^{-1} \\mathbf{y}\n\\] In the case of squared exponential kernel, the elements of the covariance matrix \\(K\\) are given by \\[\nK_{ij} = k(x_i, x_j) = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ).\n\\] The derivative of the covariance matrix with respect to \\(\\sigma\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial \\sigma} = 2\\sigma \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right );~\\frac{\\partial K}{\\partial \\sigma} = \\dfrac{2}{\\sigma}K.\n\\] The derivative of the covariance matrix with respect to \\(l\\) is given by \\[\n\\frac{\\partial K_{ij}}{\\partial l} = \\sigma^2 \\exp \\left ( -\\frac{1}{2} \\frac{(x_i - x_j)^2}{l^2} \\right ) \\frac{(x_i - x_j)^2}{l^3};~ \\frac{\\partial K}{\\partial l}  = \\frac{(x_i - x_j)^2}{l^3} K.\n\\]\n\n\n\nNow we can implement a function that calculates the derivative of the log-likelihood function with respect to \\(\\sigma\\) and \\(l\\).\n\n# Derivative of the log-likelihood function with respect to sigma\ndloglik_sigma = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK = 2*K/sigma\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Derivative of the log-likelihood function with respect to l\ndloglik_l = function(par, X, Y) {\n  sigma = par[1]; l = par[2]\n  K = outer(X[,1],X[,1], sqexpcov ,l,sigma) + diag(eps, n)\n  Si = solve(K)\n  dK =   outer(X[,1],X[,1], function(x, x1) (x - x1)^2)/l^3 * K\n  tr = sum(diag(Si %*% dK))\n  return(-(-0.5 * tr + 0.5 * t(Y) %*% Si %*% dK %*% Si %*% Y))\n}\n# Gradient function that returns a vector of derivatives\ngnlg = function(par,X,Y) {\n  return(c(dloglik_sigma(par, X, Y), dloglik_l(par, X, Y)))\n}\n\nNow we can use the optim function to find the maximum of the log-likelihood function and provide the derivative function we just implemented.\n\npar1 = optim(c(1,1), fn=loglik, gr=gnlg ,X=X, Y=Y,method=\"BFGS\")$par\nl = par1[2]; sigma = par1[1]\nprint(par1)\n## 1.5 2.4\n\nThe result is the same compared to when we called optim without the derivative function. Even execution time is the same for our small problem. However, at larger scale, the derivative-based optimization algorithm will be much faster.\nFurthermore, instead of coding our own derivative functions, we can use an existing package, such as the laGP package, developed by Bobby Gramacy to estimate the hyperparameters. The laGP package uses the same optimization algorithm we used above, but it also provides better selection of the covariance functions and implements approximate GP inference algorithms for large scale problems, when \\(n\\) becomes large and inversion of the covariance matrix \\(K\\) is prohibitively expensive.\n\nlibrary(laGP)\ngp = newGP(X, Y, 1, 0, dK = TRUE)\nres = mleGP(gp, tmax=20)\nl.laGP = sqrt(res$d/2)\nprint(l.laGP)\n## 2.4\n\nIn the newGP function defines a Gaussian process with square exponential covariance function and assumes \\(\\sigma^2 = 1\\), then mleGP function uses optimization algorithm to maximize the log-likelihood and returns the estimated hyperparameters d = \\(2l^2\\), we can see that the length scale is close to the one we estimated above. We will use the predplot convenience function to calculate the predictions and plot the data vs fit.\npredplot(X, Y, XX, YY, l, sigma)\npredplot(X, Y, XX, YY, l.laGP, 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) MLE Fit\n\n\n\n\n\n\n\n\n\n\n\n(b) laGP Fit\n\n\n\n\n\n\n\nFigure 8.4: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that there is visually no difference between the two fits. Thus, it seems irrelevant whether we keep sigma fixed \\(\\sigma=1\\) or estimate it using MLE. However, in other applications when uncertainty is larger, the choice of \\(\\sigma\\) is important when we use GP for regression and classification tasks. Even for our example, if we ask our model to extrapolate\nXX1 = matrix(seq(-4*pi, 6*pi + 0.5, length=q), ncol=1)\npredplot(X, Y, XX1, YY, l, sigma)\npredplot(X, Y, XX1, YY, l.laGP, 1)\n\n\n\n\n\n\nMLE Fit\n\n\n\n\n\n\n\nlaGP Fit\n\n\n\n\n\n\nExtrapolation: Posterior distribution over \\(y_*\\), given \\(Y\\)\n\n\n\nWe can see that outside of the range of the observed data, the model with \\(\\sigma=1\\) is more confident in its predictions.\n\n\n\nExample 8.3 (Gaussian Process for Motorcycle Accident Data) We first estimate the length scale parameter \\(l\\) using the laGP package.\n\nlibrary(MASS)\nX = mcycle$times\nY = mcycle$accel\ngp = newGP(matrix(X), Y, 2, 1e-6, dK = TRUE);\nmleGP(gp, tmax=10);\n\nNow we plot the data and the fit using the estimated length scale parameter \\(l\\).\n\nXX = matrix(seq(2.4, 55, length = 499), ncol=1)\np = predGP(gp, XX)\nN = 499\nq1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nq3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\nggplot() + \n  geom_point(aes(x=X,y=Y)) + \n  geom_line(aes(x=XX,y=q3)) + \n  geom_ribbon(aes(x=XX,ymin=q1, ymax=q2), alpha=0.2)\n\n\n\n\nMotorcycle Accident Data. Black line is the mean of the posterior distribution over \\(y_*\\), given \\(Y\\). Blue lines are the 95% confidence interval.\n\n\n\n\nWe can see that our model is more confident for time values between 10 and 30. The confidence interval is wider for time values between 0 and 10 and between 30 and 60, and less confident at the end close to the 60 mark. For some reason the acceleration values were not measured evenly. If we look at the histogram of time values, we can see that there are more data points in the middle of the time range.\n\nhist(X)\n\n\n\n\nHistogram of time values\n\n\n\n\nThe widening of the confidence intervals in regions with fewer data points (e.g., between 30 and 40) is a natural property of the GP; where data is sparse, the model reverts to the prior covariance, resulting in higher uncertainty.\n\n\n\nNon-parametric: GPs can model functions of arbitrary complexity without a fixed number of parameters.\nData-efficient: They work well with small datasets and provide uncertainty estimates that are crucial for decision-making.\nVersatile: Through the choice of the kernel function, GPs can capture various structures (smoothness, periodicity, etc.) and are used in fields ranging from environmental modeling to hyperparameter optimization in deep learning (“Bayesian Optimization”).",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gaussian Processes</span>"
    ]
  },
  {
    "objectID": "09-rl.html",
    "href": "09-rl.html",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "9.1 Multi-Armed Bandits\n“Information is the resolution of uncertainty.” Claude Shannon, 1948\nThus far we have discussed making decisions under uncertainty (Chapter 4 and Chapter 5) and two modes of data collection: field experiments and observational studies. In a field experiment we control the data‐generation process before the study begins, whereas in an observational study we have no such control and must work with whatever data are produced.\nWhat if we can choose which data to collect while the experiment is running? This leads to sequential (or adaptive) experimental design, in which each new observation is selected on the basis of the data gathered so far, creating a feedback loop between data generation and decision-making. The idea is illustrated in the following diagram:\nThis framework supports a wide range of applications and is implemented through several key algorithms. In this section, we will consider the most widely used among them: (i) Multi-Armed Bandits, (ii) Q-Learning, (iii) Active Learning, and (iv) Bayesian Optimization.\nOne of the first practical demonstrations of reinforcement learning (then called trial-and-error learning) was Claude Shannon’s 1950s mechanical mouse Theseus, which learned to find its way through a maze.\nWhen the number of alternatives is large and the testing budget limited, a different approach to A/B testing—the multi-armed bandit (MAB)—can be more sample-efficient. MABs allow us to balance exploration (trying new things) and exploitation (sticking with what works) simultaneously, minimizing regret (the difference between our choice and the optimal one). MABs require that the outcome of each experiment is available immediately or with a small delay to decide on the next experiment (Scott 2015). When time is of the essence and there is an opportunity cost associated with delaying the decision, MABs are a better choice than traditional A/B testing.\nThe mathematical framework, the foundational Bayesian solution—Thompson sampling—and a comparison with classical A/B testing are covered in Chapter 6 (see Section 6.5.5). Here we focus on practical aspects: when to end experiments, how to handle contextual information, and design considerations.\nFormally, there are \\(K\\) alternatives (arms) and each arm \\(a\\) is associated with a reward distribution \\(v_a\\), the value of this arm. The goal is to find the arm with the highest expected reward and accumulate the highest total reward in doing so. The reward distribution is unknown and we can only observe the reward after we select an arm \\(a\\), but we assume that we know the distribution \\(f_a(y\\mid \\theta)\\), where \\(a\\) is the arm index, \\(y\\) is the reward, and \\(\\theta\\) is the set of unknown parameters to be learned. Here are a few examples:\nVariations include controlling for background variables (covariates not under our control, such as time of day or user location) or replacing the binary outcome with a continuous variable (time spent on the website, money spent) using linear regression or another appropriate generalized linear model.\nThe key challenge is that acting greedily—always choosing the arm with the highest estimated reward \\(\\hat a = \\arg\\max_a v_a(\\hat \\theta)\\)—may cause us to miss better alternatives. We are not sure about our estimates \\(\\hat \\theta\\) and need to explore other options. Thompson sampling, the oldest and most elegant Bayesian solution, addresses this by sampling from the posterior distribution of each arm’s success probability and selecting the arm with the highest sample. This naturally balances exploration and exploitation: arms with high uncertainty have a chance to produce high samples and get selected, while arms with high posterior means are selected frequently. The algorithm achieves optimal regret bounds of \\(O(\\log T)\\) (meaning the regret scales logarithmically with time \\(T\\)) over \\(T\\) time steps.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#multi-armed-bandits",
    "href": "09-rl.html#multi-armed-bandits",
    "title": "9  Reinforcement Learning",
    "section": "",
    "text": "In online advertisements, we have \\(K\\) possible ads to be shown and probability of user clicking is given by vector \\(\\theta = (\\theta_1,\\ldots,\\theta_K)\\) of success probabilities for \\(K\\) independent binomial models, with \\(v_a(\\theta) = \\theta_a\\). The goal is to find the ad with the highest click-through rate.\nIn website design, we have two design variables, the color of a button (red or blue) and its pixel size (27 or 40), we introduce two dummy variables \\(x_c\\) for color, \\(x_s\\) for size and the probability of user clicking is given by \\[\n\\mathrm{logit} P(\\text{click} \\mid \\theta) = \\theta_0 + \\theta_x x_c + \\theta_s x_s + \\theta_{cs}x_cx_s,\n\\] with \\(v_a(\\theta) = P(\\text{click} \\mid \\theta)\\).\n\n\n\n\nWhen to End Experiments\nStep 2 of the TS algorithm can be replaced by calculating probability of an arm \\(a\\) being the best \\(w_{at}\\) and then choose the arm by sampling from the discrete distribution \\(w_{1t},\\ldots,w_{Kt}\\). The probability of an arm \\(a\\) being the best is given by \\[\nw_{at} = P(a \\text{ is optimal } \\mid y^t) = \\int P(a \\text{ is optimal } \\mid \\theta) P(\\theta \\mid y^t) d\\theta,\n\\] where \\(y^t = (y_1,\\ldots,y_t)\\) is the history of observations up to time \\(t\\). We can calculate the probabilities \\(w_{at}\\) using Monte Carlo. We can sample \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculate the probability as \\[\nw_{at}\\approx \\dfrac{1}{G}\\sum_{g=1}^G I(a = \\arg\\max_i v_i(\\theta^{(g)})),\n\\] where \\(I(\\cdot)\\) is the indicator function. This is simply the proportion of times the arm was the best in the \\(G\\) samples.\nAlthough using a single draw from posterior \\(p(\\theta \\mid y^t)\\) (as in the original algorithm) is equivalent to sampling proportional to \\(w_{at}\\), the explicitly calculated \\(w_{at}\\) yields a useful statistic that can be used to decide on when to end the experiment.\nWe will use the regret statistic to decide when to stop. Regret is the difference in values between the truly optimal arm and the arm that is apparently optimal at time \\(t\\). Although we cannot know the regret (it is unobservable), we can compute samples from its posterior distribution. We simulate the posterior distribution of the regret by sampling \\(\\theta^{(1)},\\ldots,\\theta^{(G)}\\) from the posterior distribution \\(p(\\theta \\mid y^t)\\) and calculating the regret as \\[\nr^{(g)} =  (v_a^*(\\theta^{(g)}) - v_{a^*_t}(\\theta^{(g)})),\n\\] Here \\(a^*\\) is the arm deemed best across all Monte Carlo draws and the first term is the value of the best arm within draw \\(g\\). We choose \\(a^*_t\\) as \\[\na^*_t = \\arg\\max_a w_{at}.\n\\]\nOften, it is convenient to measure the regret on the percent scale and then we use \\[\nr^{(g)} \\leftarrow r^{(g)}/v_{a^*_t}(\\theta^{(g)})\n\\]\nWe can demonstrate using simulated data. The function below generates samples \\(\\theta^{(g)}\\)\n\nbandit = function(x,n,alpha = 1,beta = 1,ndraws = 5000) {\n  set.seed(17) # Kharlamov\n  K &lt;- length(x) # number of bandits\n  prob &lt;- matrix(nrow = ndraws,ncol = K)\n  no = n - x\n  for (a in 1:K) {# posterior draws for each arm\n    prob[, a] = rbeta(ndraws,x[a] + alpha,no[a] + beta)\n  }\n  prob\n}\n\nSay we have three arms with 20, 30, and 40 sessions that have generated 12, 20, and 30 conversions. We assume a uniform prior for each arm \\(\\theta_i \\sim Beta(1,1)\\) and generate 6 samples from the posterior of \\(\\theta \\mid y^t\\).\n\nx = c(12,20,30)\nn = c(20, 30,40)\nprob = bandit(x, n,ndraws=6)\n\n\n\n\n\n\\(\\theta_1\\)\n\\(\\theta_2\\)\n\\(\\theta_3\\)\n\n\n\n\n1\n0.60\n0.63\n0.58\n\n\n2\n0.62\n0.62\n0.74\n\n\n3\n0.69\n0.53\n0.67\n\n\n4\n0.49\n0.59\n0.73\n\n\n5\n0.61\n0.51\n0.69\n\n\n6\n0.47\n0.64\n0.69\n\n\n\n\n\nNow, we calculate the posterior probabilities \\(w_{at} = P(a \\text{ is optimal } \\mid y^t)\\) for each of the three arms\n\nwat = table(factor(max.col(prob),levels = 1:3))/6\n\n\n\n\n1\n2\n3\n\n\n\n\n0.17\n0.17\n0.67\n\n\n\n\n\nThus far, the third arm is the most likely to be optimal, with probability 67%. Now, we calculate the regret for each of the six draws from the posterior of \\(\\theta \\mid y^t\\).\n\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0.09\n0\n0.03\n0\n0\n0\n\n\n\n\n\nWe compute value row by row by subtracting the largest element of that row from the element in column 3 (because arm 3 has the highest chance of being the optimal arm). All rows but 1 and 3 are zero. In the first row, the value is (0.63-0.58)/0.58 because column 2 is 0.05 larger than column 3. If we keep going down each row we get a distribution of values that we could plot in a histogram. We can generate one for a larger number of draws (10000).\n\nprob = bandit(x, n,ndraws=10000)\nregret = (apply(prob,1,max) - prob[,3])/prob[,3]\n\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\n\nwat = table(factor(max.col(prob),levels = 1:3))/10000\n\n\n\n\n1\n2\n3\n\n\n\n\n0.08\n0.2\n0.72\n\n\n\nThe histogram of the value remaining in an experiment (regret). The vertical line is the 95th percentile, or the potential value remaining.\n\n\nArm 3 has a 72% probability of being the best arm, so the value of switching away from arm 3 is zero in 72% of the cases. The 95th percentile of the value distribution is the potential value remaining (CvR) in the experiment, which in this case works out to be about 16%.\n\nquantile(regret,0.95)\n##  95% \n## 0.17\n\nYou interpret this number as “We’re still unsure about the CvR for arm 3, but whatever it is, one of the other arms might beat it by as much as 16%.”\nGoogle Analytics, for example, “ends the experiment when there’s at least a 95% probability that the value remaining in the experiment is less than 1% of the champion’s conversion rate. That’s a 1% improvement, not a one percentage point improvement. So if the best arm has a conversion rate of 4%, then we end the experiment if the value remaining in the experiment is less than .04 percentage points of CvR”.\n\n\nContextual Bandits\nTraditional multi-armed bandit models, like the binomial model, assume independent observations with fixed reward probabilities. This works well when rewards are consistent across different groups and times. However, for situations with diverse user bases or fluctuating activity patterns, such as international audiences or browsing behavior, this assumption can be misleading.\nFor instance, companies with a global web presence may experience temporal effects as markets in Asia, Europe, and the Americas become active at different times of the day. Additionally, user behavior can change based on the day of the week, with people engaging in different browsing patterns and purchase behaviors. For example, individuals may research expensive purchases during work hours but make actual purchases on weekends.\nConsider an experiment with two arms, A and B. Arm A performs slightly better during the weekdays when users browse but don’t make purchases, while Arm B excels during the weekends when users are more likely to make purchases. High traffic volume might lead a binomial model to declare Arm A the winner before observing any weekend behavior. This risk exists regardless of whether the experiment is conducted as a bandit or a traditional experiment. Bandit experiments, however, are particularly vulnerable to this bias due to their typically shorter durations.\nTo mitigate the risk of being misled by distinct sub-populations, two methods can be employed. If the specific sub-populations are known in advance or if there is a proxy for them, such as geographically induced temporal patterns, the binomial model can be adapted to a logistic regression model. This modification allows for a more nuanced understanding of the impact of different factors on arm performance, helping to account for variations in sub-population behavior and temporal effects. \\[\n\\mathrm{logit} P(\\text{click}_a \\mid \\theta, x) = \\theta_{0a} + \\beta^Tx,\n\\] where \\(x\\) describes the circumstances or context of the observation. The success probability for selecting arm \\(a\\) under the context \\(x\\) is represented as \\(P(\\text{click}_a \\mid \\theta, x)\\). Each arm \\(a\\) has its own specific coefficient denoted as \\(\\beta_{0a}\\) with one arm’s coefficient set to zero as a reference point. Additionally, there is another set of coefficients represented as \\(\\beta\\) that are associated with the contextual data and are learned as part of the model. The value function can then be \\[\nv_a(\\theta) = \\mathrm{logit}^{-1}(\\beta_{0a}).\n\\]\nIf we lack knowledge about the crucial contexts, one option is to make the assumption that contexts are generated randomly from a context distribution. This approach is often exemplified by the use of a hierarchical model like the beta-binomial model. \\[\\begin{align*}\n\\theta_{at} &\\sim Beta(\\alpha_a,\\beta_a)\\\\\n\\text{click}_a \\mid \\theta &\\sim Binomial(\\theta_{at}),\n\\end{align*}\\] where \\(\\theta = \\{\\alpha_a,\\beta_a ~:~ a = 1,\\ldots,K \\}\\), with value function \\(v_a(\\theta) = \\alpha_a/\\beta_a\\)\n\n\nSummary of MAB Experimentation\nThe design phase begins with defining your arms by identifying the different options you want to evaluate, such as different website layouts, pricing strategies, or marketing campaigns. Next, choose a bandit algorithm that balances exploration and exploitation in various ways. Popular choices include Epsilon-greedy, Thompson Sampling, and Upper Confidence Bound (UCB). Then set your parameters by configuring the algorithm parameters based on your priorities and expected uncertainty. For example, a higher exploration rate encourages trying new arms earlier. Finally, randomize allocation by assigning users to arms randomly, ensuring unbiased data collection.\nDuring the analysis phase, track rewards by defining and measuring the reward metric for each arm, such as clicks, conversions, or profit. Monitor performance by regularly analyzing the cumulative reward and arm selection probabilities to see which arms are performing well and how the allocation strategy is adapting. Use statistical tools like confidence intervals or Bayesian methods to compare performance between arms and assess the significance of findings. Make adaptive adjustments by modifying the experiment based on ongoing analysis. You might adjust algorithm parameters, stop arms with demonstrably poor performance, or introduce new arms.\nStart with a small pool of arms to avoid information overload by testing a manageable number of options initially. Set a clear stopping criterion by deciding when to end the experiment based on a predetermined budget, time limit, or desired level of confidence in the results. Consider ethical considerations by ensuring user privacy and informed consent if the experiment involves personal data or user experience changes. Interpret results in context by remembering that MAB results are specific to the tested conditions and might not generalize perfectly to other contexts.\nBy following these steps and utilizing available resources, you can design and analyze effective MAB experiments to optimize your decision-making in various scenarios. Remember to adapt your approach based on your specific goals and context to maximize the benefits of this powerful technique.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bellman-principle-of-optimality",
    "href": "09-rl.html#bellman-principle-of-optimality",
    "title": "9  Reinforcement Learning",
    "section": "9.2 Bellman Principle of Optimality",
    "text": "9.2 Bellman Principle of Optimality\n\n“An optimum policy has the property that whatever the initial state and initial decision are, the remaining decision sequence must be optimum for the state resulting from the first decision.” – Richard Bellman\n\nTo solve sequential decision problems like the one above, we rely on a fundamental concept in dynamic programming.\n\nExample 9.1 (Secretary Problem) The Secretary Problem, also known as the marriage problem or sultan’s dowry problem, is a classic problem in decision theory and probability theory. The scenario involves making a decision on selecting the best option from a sequence of candidates or options. The problem is often framed as hiring a secretary, but it can be applied to various situations such as choosing a house, a spouse, or any other scenario where you sequentially evaluate options and must make a decision.\nIn this problem, you receive \\(T\\) offers and must either accept or reject the offer “on the spot”. You cannot return to a previous offer once you have moved on to the next one. Offers are in random order and can be ranked against those previously seen. The aim is to maximize the probability of choosing the offer with the greatest rank. There is an optimal \\(r\\) (\\(1 \\le r &lt; T\\)) to be determined such that we examine and reject the first \\(r\\) offers. Then of the remaining \\(T - r\\) offers we choose the first one that is best seen to date.\nA decision strategy involves setting a threshold such that the first candidate above this threshold is hired, and all candidates below the threshold are rejected. The optimal strategy, known as the 37% rule, suggests that one should reject the first \\(r=T/e\\) candidates and then select the first candidate who is better than all those seen so far.\nThe reasoning behind the 37% rule is based on the idea of balancing exploration and exploitation. By rejecting the first \\(T/e\\) candidates, you gain a sense of the quality of the candidates but avoid committing too early. After that point, you select the first candidate who is better than the best among the initial \\(r\\) candidates.\nIt’s important to note that the 37% rule provides a probabilistic guarantee of selecting the best candidate with a probability close to 1/e (approximately 37%) as \\(T\\) becomes large.\nTo solve the secretary problem, we will use the principle of optimality due to Richard Bellman. The principle states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. In other words, the policy is optimal from the first decision onwards.\nThe solution to the secretary problem can be found via dynamic programming. Given an agent with utility function \\(u(x,d)\\), with current state \\(x\\), and decision \\(d\\). The law of motion of \\(x_t\\) is given by \\(x_{t+1} = p(x_t,d_t)\\). Bellman principle of optimality states that the optimal policy is given by the following recursion \\[\nV(x_t) = \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\E{V(x_{t+1})} \\right \\}\n\\] where \\(\\gamma\\) is the discount factor. The optimal policy is given by \\[\nd_t^* = \\arg \\max_{d_t} \\left \\{ u(x_t,d_t) + \\gamma \\E{V(x_{t+1})} \\right \\}.\n\\]\nNow, back to the secretary problem. Let \\(y^t = (y_1,\\ldots,y_t)\\) denote the history of observations up to time \\(t\\). State \\(x_t=1\\) if the \\(t\\)th candidate is the best seen so far and \\(x_t=0\\) otherwise. The decision \\(d_t=1\\) if the \\(t\\)th candidate is hired and \\(d_t=0\\) otherwise. The utility function is given by \\(u(x_t,d_t) = x_t d_t\\).  The Bellman equation is given by \\[\nP(\\text{best of T}\\mid x_t=1) = \\dfrac{P(\\text{best of T})}{P(x_t=1)} = \\dfrac{1/T}{1/t} = \\dfrac{t}{T}.\n\\] The \\(t\\)th offer is the best seen so far places no restriction on the relative ranks of the first \\(t-1\\) offers. Therefore, \\[\np(x_t=1,y^{t-1}) = p(x_t=1)p(y^{t-1})\n\\] by the independence assumption. Hence, we have \\[\np(x_t=1 \\mid y^{t-1}) = p(x_t=1) = \\dfrac{1}{t}.\n\\]\nLet \\(p^*(x_{t-1}=0)\\) be the probability under the optimal strategy. Now we have to select the best candidate, given we have seen \\(t-1\\) offers so far and the last one was not the best or worse. The probability satisfies the Bellman equation \\[\np^*(x_{t-1}=0) = \\frac{t-1}{t} p^*(x_{t}=0) + \\frac{1}{t}\\max\\left(t/T, p^*(x_{t}=0)\\right).\n\\] This leads to \\[\np^*(x_{t-1}=0) = \\frac{t-1}{T} \\sum_{\\tau=t-1}^{T-1}\\dfrac{1}{\\tau}.\n\\]\nRemember, the strategy is to reject the first \\(r\\) candidates and then select the first. The probability of selecting the best candidate is given by \\[\nP(\\text{success}) = \\dfrac{1}{T}\\sum_{a=r+1}^T \\dfrac{r}{a} \\approx  \\dfrac{1}{T}\\int_{r}^{T}\\dfrac{r}{a} da = \\dfrac{r}{T} \\log \\left ( \\dfrac{T}{r} \\right ).\n\\] We optimize over \\(r\\) by setting the derivative to zero: \\[\n\\frac{\\log \\left(\\frac{T}{r}\\right)}{T}-\\frac{1}{T} = 0,\n\\] which gives the optimal \\(r=T/e\\).\nIf we plug in \\(r=T/e\\) back to the probability of success, we get \\[\nP(\\text{success}) \\approx \\dfrac{1}{e} \\log \\left ( e \\right ) = \\dfrac{1}{e}.\n\\]\nMonte Carlo Simulations Simulations are a powerful tool for making decisions when we deal with a complex system, which is difficult or impossible to analyze mathematically. They are used in many fields, including finance, economics, and engineering. They can also be used to test hypotheses about how a system works and to generate data for statistical analysis.\nWe start by showing how the secretary problem can be analyzed using simulations rather than analytical derivations provided above.\n\nset.seed(17) # Kharlamov\nnmc = 1000\nn = 1000\nsz = 300\nrules = round(n*seq(0.002,0.8,length.out = sz))\nrules = unique(rules[rules&gt;0])\nsz = length(rules)\ncnt = rep(0,sz)\nquality = rep(0,sz)\nfor (i in 1:sz)\n{\n  for (j in 1:nmc){\n    x = sample(1:n,n)\n    screen = x[1:(rules[i]-1)]\n    best_screen = max(screen)\n    xchoice = x[(rules[i]):n]\n    better_candidates = which(xchoice &gt; best_screen)\n    if (length(better_candidates)==0)\n      choice = x[n]\n    else\n      choice = xchoice[min(better_candidates)]\n    cnt[i] = cnt[i] + (choice == max(x))\n    quality[i] = quality[i] + choice\n  }\n}\nd = data.frame(cnt=cnt, quality=quality,nmc=nmc, rules=rules)\n\nplot(d$rules, d$cnt/d$nmc, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screened\", \n     ylab=\"Probability of Picking the Best\")\nplot(d$rules, d$quality/1000, type='l', col=3, lwd=3, xlab=\"Number of Candidates Screened\", \n     ylab=\"Average Quality of Candidate\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left panel confirms the theoretical result: the probability of selecting the best candidate peaks at approximately 37% when we screen roughly 370 out of 1000 candidates (close to \\(T/e \\approx 368\\)). The right panel reveals an interesting trade-off: average quality peaks much earlier, around 50–100 candidates screened, then steadily declines. This happens because being too selective (screening too many) increases the risk of rejecting all strong candidates and being forced to accept the last one, regardless of quality. The two plots together illustrate the tension between maximizing the chance of finding the absolute best (left) and maximizing expected quality (right)—objectives that lead to different optimal stopping rules.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#markov-decision-processes",
    "href": "09-rl.html#markov-decision-processes",
    "title": "9  Reinforcement Learning",
    "section": "9.3 Markov Decision Processes",
    "text": "9.3 Markov Decision Processes\nA Markov Decision Process (MDP) is a discrete time stochastic control process which provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. Almost all dynamic programming and reinforcement learning problems are formulated using the formalism of MDPs. MDPs were known at least as early as the 1950s; a core body of research resulted from Ronald Howard’s 1960 book, Dynamic Programming and Markov Processes. In fact, the multi-armed bandit problem considered before is a special case of MDP with one state.\nAn MDP is defined by:\n\nStates (\\(S\\)): A set of states representing different scenarios or configurations. A key assumption is the Markov property: the future depends only on the current state and action, not on the history.\nActions (\\(A\\)): A set of actions available in each state.\nTransition Probability (\\(P\\)): \\(P(s', r \\mid s, a)\\) is the probability of transitioning to state \\(s'\\), receiving reward \\(r\\), given that action \\(a\\) is taken in state \\(s\\).\nReward (\\(R\\)): A reward function \\(R(s, a, s')\\) that gives the feedback signal immediately after transitioning from state \\(s\\) to state \\(s'\\), due to action \\(a\\).\nDiscount Factor (\\(\\gamma\\)): A factor between 0 and 1, which reduces the value of future rewards.\n\n\nMathematical Representation\nThe states \\(s_t\\) and rewards \\(R_t\\) in MDP are indexed by time \\(t\\). The state at time \\(t+1\\) is distributed according to the transition probability \\[\nP(s_{t+1}\\mid s_t,a_t).\n\\] The reward function is \\(R_s^a = \\E{R_{t+1} \\mid s, a}\\).\nThe Markov property of the state is that the transition probability depends only on the current state and action and not on the history of states and actions. \\[\nP(s_{t+1}\\mid s_t,a_t) = P(s_{t+1}\\mid s_t,a_t,s_{t-1},a_{t-1},\\ldots,s_0,a_0).\n\\] In other words, the future only depends on the present and not on the past history. The state is a sufficient statistic for the future.\nIn the case when the number of states is finite, we can represent the transition probability as a matrix \\(P_{ss'}^a = P(s_{t+1} = s' \\mid s_t = s, a_t = a)\\), where \\(s,s' \\in S\\) and \\(a \\in A\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1 \\[\nP^a = \\begin{bmatrix}\nP_{11}^a & P_{12}^a & \\cdots & P_{1|S|}^a \\\\\nP_{21}^a & P_{22}^a & \\cdots & P_{2|S|}^a \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP_{|S|1}^a & P_{|S|2}^a & \\cdots & P_{|S||S|}^a\n\\end{bmatrix}\n\\] The reward function is also a matrix \\(R_s^a = E[R_{t+1} \\mid s_t = s, a_t = a]\\).\nMarkov Reward Process\nWe can consider a simpler example of Markov Process. This is a special case of MDP when there is no action and the transition probability is simply a matrix \\(P_{ss'} = P(s_{t+1} = s' \\mid s_t = s)\\), where \\(s,s' \\in S\\). For a given action \\(a\\), the transition probability matrix \\(P^a\\) is a square matrix of size \\(|S| \\times |S|\\), where each row sums to 1.\n\nExample 9.2 (Student Example) The graph below represents possible states (nodes) and transitions (links). Each node has reward assigned to it which corresponds to the reward function \\(R(s)\\). The transition probabilities are shown on the links. The graph represents a Markov Chain where transitions are probabilistic and not controlled by an agent.\n\n\n\n\n\n\n\n\ngraph LR\n    fb(\"Facebook; R=1\") --0.9--&gt; fb\n    fb--0.1--&gt;c1(\"Class 1; R=-2\")\n    c1--0.5--&gt;fb\n    c1--0.5--&gt;c2(\"Class 2; R=-2\")\n    c2--0.8--&gt;c3(\"Class 3; R=-2\")\n    c3--0.6--&gt;p(\"Pass; R=10\")\n    p--1.0--&gt;s(\"Sleep; R=0\")\n    c3--0.4--&gt;pub(\"Pub; R=3\")\n    pub--0.2--&gt;f(\"Fail; R=-20\")\n    f--1.0--&gt;s\n    pub--0.3--&gt;c3\n    pub--0.2--&gt;c1\n    pub--0.3--&gt;c2\n    c2--0.2--&gt;s\n\n\n\n\nFigure 9.2\n\n\n\n\n\nIf we are to pick an initial state and sample a trajectory (path on the graph above) by picking a random action at each state, we will get a random walk on the graph. The reward for each state is shown in the graph. The discounted value of the trajectory is then \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1},\n\\] where \\(\\gamma\\) is the discount factor. The discount factor is a number between 0 and 1 that determines the present value of future rewards. A discount factor of 0 makes the agent myopic and only concerned about immediate rewards. A discount factor of 1 makes the agent strive for a long-term high reward. The discount factor is usually denoted by \\(\\gamma\\) and is a parameter of the MDP. The discount of less than 1 is used to avoid infinite returns in cyclic Markov chains and allows us to discount less certain future rewards. The value of \\(\\gamma\\) is usually close to 1, for example 0.9 or 0.99. The value of \\(\\gamma\\) can be interpreted as the probability of the agent surviving from one time step to the next.\nWe can calculate sample returns \\(G_t\\) for this Markov Chain. We first read in the reward matrix\n\n# Reward function\nR = read.csv(\"../data/student-reward.tab\", header = T, sep = \"\\t\", row.names=1)\nt(R) %&gt;% knitr::kable()\n\n\nRewards\n\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\nReward\n-1\n-2\n-2\n-2\n3\n10\n-20\n0\n\n\n\n\ngetR = function(s) R[s,]\n\nand the transition probability matrix and the reward matrix\n\n# Read transition probability matrix\np = read.csv(\"../data/student-mdp.tab\", header = T, sep = \"\\t\", row.names=1)\nkbp = knitr::kable(p)\ngsub(0, ' ', kbp) # replace 0 with blank\n\n\n\n\n\nFacebook\nClass.1\nClass.2\nClass.3\nPub\nPass\nFail\nSleep\n\n\n\n\nFacebook\n.9\n.1\n.\n.\n.\n.\n.\n.\n\n\nClass 1\n.5\n.\n.5\n.\n.\n.\n.\n.\n\n\nClass 2\n.\n.\n.\n.8\n.\n.\n.\n.2\n\n\nClass 3\n.\n.\n.\n.\n.4\n.6\n.\n.\n\n\nPub\n.\n.2\n.3\n.3\n.\n.\n.2\n.\n\n\nPass\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\nFail\n.\n.\n.\n.\n.\n.\n1.\n.\n\n\nSleep\n.\n.\n.\n.\n.\n.\n.\n1.\n\n\n\n\n\nNow we check that all of the rows sum to 1\n\np = as.matrix(p)\nrowSums(p) %&gt;% t() %&gt;% knitr::kable()\n\n\nTransition probability matrix row sums\n\n\nFacebook\nClass 1\nClass 2\nClass 3\nPub\nPass\nFail\nSleep\n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\nGiven the transition probability matrix, we can sample possible trajectories. First, we define a tr(s,m) convenience function that generates a trajectory of length m starting from state s\n\nset.seed(17)\n# Sample column s' using probabilities in row s \njump = function(s) sample(rownames(p), 1, prob = p[s,])\n# Function to generate a trajectory\ntr = function(s,m) {\n  traj = c(s)\n  for (i in 1:m) {\n    traj = c(traj, jump(traj[i]))\n  }\n  return(traj)\n}\n\nNow, we generate 6 trajectories of length 5 starting from state “Pub”\n\ntraj = t(replicate(6,tr(\"Pub\",5)))\nknitr::kable(traj)\n\n\n\n\nPub\nClass 3\nPub\nClass 2\nClass 3\nPass\n\n\nPub\nClass 2\nClass 3\nPass\nSleep\nSleep\n\n\nPub\nClass 2\nClass 3\nPub\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nFail\nFail\nFail\nFail\nFail\n\n\nPub\nClass 3\nPass\nSleep\nSleep\nSleep\n\n\n\n\n\nNow we can calculate the discounted value \\(G_t\\) of each of the trajectories\n\ntrajR = apply(traj,1:2, getR)\ndisc = 0.5^(0:5)\ntrajR %*% disc %&gt;% t() %&gt;% knitr::kable()\n\n\nDiscounted value of each trajectory\n\n\n2.7\n2.8\n0\n-16\n-16\n4.5\n\n\n\n\n\nWe can calculate the discounted value for 1000 trajectories\n\n# Value function of a trajectory\nvalue = function(s,m, gamma=0.5) {\n  traj = tr(s,m)\n  disc = gamma^(0:m)\n  return(sum(sapply(traj,getR) * disc))\n}\nvpub = replicate(1000,value(\"Pub\",6))\nhist(vpub)\n\n\n\n\n\n\n\nmean(vpub)\n## -1.2\n\nWe can see that the distribution of discounted rewards is bimodal and depends on whether you get to “Fail” state or not.\n\nThe value of a state is the expected discounted reward starting from that state \\[\nV(s) = \\E{G_t \\mid s_t = s}.\n\\] It evaluates the long-term value of state \\(s\\) (the goodness of a state). It can be drastically different from the reward value associated with the state. In our student example, the reward for the “Pub” state is 3, but the value is -1.2.\nThe value of a state can be calculated recursively using the Bellman equation \\[\\begin{align*}\nV(s) &= \\E{G_t \\mid s_t = s} \\\\\n&= \\E{R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid s_t = s} \\\\\n&= \\E{R_{t+1} + \\gamma G_{t+1} \\mid s_t = s} \\\\\n&= \\E{R_{t+1} + \\gamma V(s_{t+1}) \\mid s_t = s} \\\\\n&= \\sum_{s'} P(s' \\mid s) \\left[R(s) + \\gamma V(s')\\right].\n\\end{align*}\\]\n\nExample 9.3 (MDP for Forest Management) We can consider one of the classic examples of a Markov Decision Process (MDP). Imagine you need to calculate an optimal policy to manage a forest to prevent possible fires. The goal is to decide between two possible actions to either ‘Wait’ or ‘Cut’. They correspond to balancing between ecological preservation and economic gain, considering the random event of a fire. We can break down the elements of this model.\n\nStates: Represent the age of the forest. The states are denoted as \\(\\{0, 1,\\ldots, S-1\\}\\), where 0 is the youngest state (just after a fire or cutting) and \\(S-1\\) is the oldest state of the forest.\nActions: There are two actions available:\n\n‘Wait’ (Action 0): Do nothing and let the forest grow for another year.\n‘Cut’ (Action 1): Harvest the forest, which brings immediate economic benefit but resets its state to the youngest.\n\nProbabilities: There’s a probability ‘p’ each year that a fire occurs, regardless of the action taken. If a fire occurs, the forest returns to state 0.\nTransition Matrix (P): This matrix defines the probabilities of moving from one state to another, given a specific action.\n\nWe will use mdp_example_forest function from the MDPtoolbox package to generate the transition probability matrix and reward matrix for the Forest example.\n\nlibrary(MDPtoolbox)\n# Define the transition and reward matrices for the Forest example\nres = mdp_example_forest(S=4,r1=10,r2=1,p=0.01)\n\nThis function generates a transition probability \\(P\\) of size \\((|A| \\times |S| \\times |S|)\\). There are four states by default \\(S = \\{1,2,3,4\\}\\) and two actions \\(A = \\{1,2\\}\\). The transition probability matrices for each action are: \\[\nP(\\cdot \\mid s, a=1) = \\begin{bmatrix}\n0.01 & 0.99 & 0.00 & 0.00 \\\\\n0.01 & 0.00 & 0.99 & 0.00 \\\\\n0.01 & 0.00 & 0.00 & 0.99 \\\\\n0.01 & 0.00 & 0.00 & 0.99\n\\end{bmatrix}, \\quad\nP(\\cdot \\mid s, a=2) = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nThe reward matrix \\(R\\) of size \\(|S| \\times |A|\\) specifies the immediate reward for taking each action in each state:\n\\[\nR = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 1 \\\\\n0 & 1 \\\\\n10 & 1\n\\end{bmatrix}\n\\]\nWith these transition probabilities and rewards defined, we can solve for the optimal policy using either value iteration or policy iteration. Value iteration works by iteratively updating the value function until convergence, while policy iteration alternates between policy evaluation and policy improvement steps. Both algorithms converge to the same optimal policy, though they may differ in computational efficiency depending on the problem structure. For this simple MDP with a discount factor of \\(\\gamma = 0.9\\), both methods quickly converge to the optimal policy \\(\\pi^* = (1, 2, 2, 1)\\), meaning action 1 is optimal in states 1 and 4, while action 2 is optimal in states 2 and 3. This policy maximizes the expected discounted cumulative reward.\n\n\nExample 9.4 (Game of Chess as an MDP) We can consider a simple example of a game of chess.\nIn chess, a state \\(s\\) represents the configuration of the chessboard at any given time. This includes the positions of all the pieces (pawns, knights, bishops, rooks, queen, and king) for both players (white and black). Each possible arrangement of these pieces on the chessboard is a unique state. The game starts in a standard initial state (the standard chess setup) and progresses through a series of states as moves are made. If the game is played to completion, it ends in a terminal state (checkmate, stalemate, or draw). Additionally, the game may be declared a draw if the same board state occurs multiple times, requiring the state representation to track repetition history. Thus, we need to remember the states we have seen before and essentially expand the state space to include the number of times we have seen the state. In a timed game, the game can also end when a player runs out of time.\nActions \\(a\\) in chess are the legal moves that can be made by the player whose turn it is to move. This includes moving pieces according to their allowed movements, capturing opponent pieces, and special moves like castling or en passant. The set of actions available changes with each state, depending on the position of the pieces on the board.\nIn chess, the transition probability is deterministic for the most part, meaning that the outcome of a specific action (move) is certain and leads to a predictable next state. For example, moving a knight from one position to another (assuming it’s a legal move) will always result in the same new configuration of the chessboard. However, in the context of playing against an opponent, there is uncertainty in predicting the opponent’s response, which can be seen as introducing a probabilistic element in the larger view of the game.\nDefining a reward function \\(R\\) in chess can be complex. In the simplest form, the reward could be associated with the game’s outcome: a win, loss, or draw. Wins could have positive rewards, losses negative, and draws could be neutral or have a small positive/negative value. Alternatively, more sophisticated reward functions can be designed to encourage certain strategies or positions, like controlling the center of the board, protecting the king, or capturing opponent pieces.\nChess is a game of perfect information, meaning all information about the game state is always available to both players. While the number of states in chess is finite, it is extremely large, making exhaustive state analysis (like traditional MDP methods) computationally impractical.\nIn practice, solving chess as an MDP, especially using traditional methods like value iteration or policy iteration, is not feasible due to the enormous state space. Modern approaches involve heuristic methods, machine learning, and deep learning techniques. For instance, advanced chess engines and AI systems like AlphaZero use deep neural networks and reinforcement learning to evaluate board positions and determine optimal moves, but they do not solve the MDP in the classical sense.\n\nThe goal in an MDP is to find a policy \\(a = \\pi(s)\\) (a function from states to actions) that maximizes the sum of discounted rewards: \\[\nV^\\pi(s) = \\E[\\pi]{G_t \\mid S_t = s},\n\\] where \\[\nG_t = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t), s_{t+1})\n\\]\nFunction \\(V^\\pi(s)\\) is the value of state s under policy \\(\\pi\\). Similarly we can define the action-value function \\(Q^\\pi(s,a)\\) as the value of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\): \\[\nQ^\\pi(s,a) = \\E[\\pi]{G_t \\mid S_t = s, A_t = a}.\n\\]\nBellman Equations for MDP simply state that the value of a state is the sum of the immediate reward and the discounted value of the next state \\[\nV^\\pi(s) = \\E[\\pi]{R_{t+1} + \\gamma V^{\\pi}(S_{t+1})\\mid S_t = s} = \\sum_{a\\in A}\\pi(a\\mid s)\\left(R_s^a + \\gamma \\sum_{s'\\in S}P^a_{ss'}V^\\pi(s') \\right).\n\\] The action-value function satisfies the following Bellman equation \\[\nQ^\\pi(s,a) = \\E[\\pi]{R_{t+1} + \\gamma Q^{\\pi}(S_{t+1}, A_{t+1})\\mid S_t = s, A_t = a}.\n\\] The value function can be defined as an expectation over the action-value function \\[\nV^\\pi(s) = \\E[\\pi]{Q^\\pi(s,a)\\mid S_t = s} = \\sum_{a\\in A}\\pi(a\\mid s)Q^\\pi(s,a).\n\\] In matrix form, we have \\[\nQ^\\pi(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^\\pi(s') = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a\\sum_{a'\\in A}\\pi(a'\\mid s')Q^\\pi(s',a').\n\\] Now we can define the Bellman equation in the matrix form \\[\nV^\\pi = R^\\pi + \\gamma P^\\pi V^\\pi.\n\\] The direct solution is then \\[\nV^\\pi = (I - \\gamma P^\\pi)^{-1}R^\\pi.\n\\] The optimal value function \\(V^*(s)\\) is the value function for the optimal policy \\(\\pi^*(s)\\) \\[\nV^*(s) = \\max_\\pi V^\\pi(s).\n\\] The optimal action-value function \\(Q^*(s,a)\\) is the action-value function for the optimal policy \\(\\pi^*(s)\\) \\[\nQ^*(s,a) = \\max_\\pi Q^\\pi(s,a).\n\\] The optimal policy \\(\\pi^*(s)\\) is the policy that maximizes the value function \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a).\n\\] The optimal value function satisfies the Bellman optimality equation \\[\nV^*(s) = \\max_a Q^*(s,a).\n\\] and vice-versa \\[\nQ^*(s,a) = R_s^a + \\gamma \\sum_{s'\\in S}P_{ss'}^a V^*(s').\n\\]\nThe Bellman optimality equation is non-linear and is typically solved iteratively using Value Iteration, Policy Iteration, or Q-learning. Q-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example.\n:\n\nExample 9.5 (Q-Learning and Deal or No Deal) Deal or No Deal is a popular TV show where a contestant is presented with a number of sealed boxes, each containing a prize. The contestant selects a box and then proceeds to open the remaining boxes one by one. After a certain number of boxes have been opened, the banker makes an offer to buy the contestant’s box. The contestant can either accept the offer and sell the box or reject the offer and continue opening boxes. The game continues until the contestant either accepts an offer or opens all the boxes. The goal is to maximize the expected value of the prize in the contestant’s box. The rule of thumb is to continue as long as there are two large prizes left. Continuation value is large. For example, with three prizes and two large ones, risk averse people will naively choose deal, when if they incorporated the continuation value they would choose no deal.\nLet \\(s\\) denote the current state of the system and \\(a\\) an action. The \\(Q\\)-value, \\(Q_t(s,a)\\), is the value of using action \\(a\\) today and then proceeding optimally in the future. We use \\(a=1\\) to mean no deal and \\(a=0\\) means deal. The Bellman equation for \\(Q\\)-values becomes \\[\nQ_{t} ( s , a) = u( s , a  ) + \\sum_{ s^\\star } P( s^\\star | s ,a ) \\max_{ a } Q_{t+1} ( s^\\star , a )\n\\] where \\(u(s,a)\\) is the immediate utility of taking action \\(a\\) in state \\(s\\). The value function and optimal action are given by \\[\nV(s) = \\max_a Q ( s , a ) \\; \\; \\text{and} \\; \\;  a^\\star = \\text{argmax}  Q ( s , a )\n\\]\nTransition Matrix: Consider the problem where you have three prizes left. Now \\(s\\) is the current state of three prizes. \\[\ns^\\star = \\{ \\text{all \\; sets \\; of \\; two \\; prizes} \\} \\; \\; \\text{and} \\; \\; P( s^\\star | s, a =1) = \\frac{1}{3}\n\\] where the transition matrix is uniform to the next state. There’s no continuation for \\(P( s^\\star | s, a =0)\\).\nUtility: The utility of the next state depends on the contestant’s value for money and the bidding function of the banker \\[\nu( B ( s^\\star ) ) = \\frac{ B ( s^\\star )^{1-\\gamma} -1 }{1 - \\gamma }\n\\] in power utility case.\nExpected value implies \\(B( s ) = \\bar{s}\\) where \\(s\\) are the remaining prizes.\nThe website uses the following criteria: with three prizes left: \\[\nB( s) = 0.305 \\times \\text{big} + 0.5 \\times \\text{small}\n\\] and with two prizes left \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small}   \n\\]\nThree prizes left: \\(s = \\{ 750 , 500 , 25 \\}\\).\nAssume the contestant is risk averse with log-utility \\(U(x) = \\log x\\). Banker offers the expected value we get \\[\nu( B( s = \\{ 750 , 500 , 25 \\}) ) = \\log ( 1275/3  ) = 6.052\n\\] and so \\(Q_t ( s , a= 0 ) = 6.052\\).\nIn the continuation problem, \\(s^\\star = \\{ s_1^\\star , s_2^\\star , s_3^\\star  \\}\\) where \\(s_1^\\star = \\{750,500 \\}\\) and \\(s_2^\\star = \\{ 750,25 \\}\\) and \\(s_3^\\star = \\{ 500,25 \\}\\).\nWe’ll have offers \\(625 , 387.5 , 137.5\\) under the expected value. As the banker offers expected value the optimal action at time \\(t+1\\) is to take the deal \\(a=0\\) with Q-values given by \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  \\log (625) + \\log (387.5) + \\log (262.5) \\right ) = 5.989\n\\end{align*}\\] as immediate utility \\(u( s,a ) = 0\\). Hence as \\[\nQ_{t} ( s , a=1)=5.989 &lt;  6.052 = Q_{t} ( s , a=0)\n\\] the optimal action is \\(a^\\star = 0\\), deal. Continuation value is not large enough to overcome the generous (expected value) offered by the banker.\nSensitivity analysis: we perform it by assuming different Banker’s bidding function. If we use the function from the website (2 prizes): \\[\nB( s) = 0.355 \\times \\text{big} + 0.5 \\times \\text{small},\n\\] Hence \\[\\begin{align*}\nB( s_1^\\star = \\{750,500 \\}) &  = 516.25 \\\\\nB( s_2^\\star = \\{ 750,25 \\}) & =  278.75 \\\\\nB( s_3^\\star = \\{ 500,25 \\}) &  = 190  \n\\end{align*}\\]\nThe optimal action with two prizes left for the contestant is \\[\\begin{align*}\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\log (750) + \\log (500) \\right ) = 6.415 \\\\\n& &gt; 6.246 = Q_{t+1} ( s_1^\\star , a=0) = \\log \\left ( 516.25 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\log (750) + \\log (25) \\right ) = 4.9194 \\\\\n& &lt; 5.63 = Q_{t+1} ( s_1^\\star , a=0)  = \\log \\left ( 278.75 \\right ) \\\\\nQ_{t+1} ( s_1^\\star , a=1) & = \\frac{1}{2} \\left (  \\log (500) + \\log (25) \\right ) = 4.716 \\\\\n& &lt; 5.247 = Q_{t+1} ( s_1^\\star , a=0)  = \\left ( 516.25 \\right ) \\\\\n\\end{align*}\\] Hence future optimal policy will be no deal under \\(s_1^\\star\\), and deal under \\(s_2^\\star , s_3^\\star\\).\nTherefore solving for \\(Q\\)-values at the previous step gives \\[\\begin{align*}\nQ_{t} ( s , a=1) & = \\sum_{ s^\\star } P( s^\\star | s ,a =1) \\max_{ a } Q_{t+1} ( s^\\star , a ) \\\\\n& = \\frac{1}{3} \\left (  6.415+ 5.63 + 5.247 \\right ) = 5.764\n\\end{align*}\\] with a monetary equivalent as \\(\\exp(5.764  ) = 318.62\\).\nWith three prizes we have \\[\\begin{align*}\nQ_{t} ( s , a=0) & = u( B( s = \\{ 750 , 500 , 25 \\}) ) \\\\\n& = \\log \\left ( 0.305 \\times 750 + 0.5 \\times 25 \\right ) \\\\\n& = \\log ( 241.25 ) = 5.48.\n\\end{align*}\\] The contestant is offered $ 241.\nNow we have \\(Q_{t} ( s , a=1)= 5.7079  &gt; 5.48 = Q_{t} ( s , a=0)\\) and the optimal action is \\(a^\\star = 1\\), no deal. The continuation value is large. The premium is $ 241 compared to $319, a 33% premium.\n\n\n\nMDP Solvers\nThe underlying approach behind all MDP solvers is to iteratively apply the Bellman equations until convergence. The main difference between the solvers is how they update the value function. All of them use dynamic programming approach to find optimal policy. Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems. It is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure. If a problem can be solved by combining optimal solutions to non-overlapping subproblems, the strategy is called divide and conquer instead. This is why dynamic programming is applicable to solving MDPs.\nFirst, we consider how to find the values of states under a given policy \\(\\pi\\). We can iteratively apply Bellman expectation backup. We update the values using the following update rule \\[\nV_{k+1}(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a)[R(s, a, s') + \\gamma V_k(s')].\n\\] We will introduce the short-cut notation \\[  \nP_{ss'}^a = P(s' \\mid s, a), \\quad R_s^a = \\sum_{s'} P(s' \\mid s, a)R(s, a, s').\n\\] Then in matrix form the update rule becomes \\[\nV_{k+1} = R^{\\pi} + \\gamma P^{\\pi} V_k.\n\\]\n\nPolicy Iteration\nThe policy iteration algorithm involves two main steps: policy evaluation and policy improvement, which are iteratively applied until convergence. We start with an arbitrary value function, often initialized to zero for all states. \\[\\begin{align*}\nV_0(s) &= 0 \\\\\nV_{k+1} = & R^{\\pi} + \\gamma P^{\\pi} V_k\\\\\n\\pi_{k+1} &= \\arg\\max_a R^a + \\gamma P^a V_{k+1} = \\arg\\max_a Q^\\pi(s,a)\n\\end{align*}\\] The last step is to simply choose the action that maximizes the expected return in each state. Although it can be slow in practice, the convergence is guaranteed because the value function is a contraction mapping. We typically stop the iterations when the maximum change in the value function is below a threshold.\nIt can be used for calculating the optimal policy. The Bellman optimality equation is a fundamental part of finding the best policy in MDPs. \\[\nV^*(s) = \\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is then \\[\n\\pi^*(s) = \\arg\\max_a \\sum_{s', r} P(s', r | s, a)[r + \\gamma V^*(s')]\n\\] The optimal policy is the one that maximizes the value function. The optimal value function is the value function for the optimal policy. The optimal value function satisfies the Bellman optimality equation. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation.\nGiven an optimal policy, we can subdivide it into two parts: the optimal first action \\(A^*\\) and the optimal policy from the next state \\(S'\\). The optimal value \\(V^*\\) can be found using one-step lookahead \\[\nV^*(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s')\n\\] #### Value Iteration This allows us to define another approach to solving MDPs, called value iteration. The value iteration algorithm starts with an arbitrary value function and iteratively applies the Bellman optimality backup. The algorithm updates the value function using the following update rule \\[\nV_{k+1}(s) = \\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V_k(s').\n\\] In matrix form, the update rule becomes \\[\nV_{k+1} = \\max_a R^a + \\gamma P^a V_k.\n\\] The algorithm stops when the maximum change in the value function is below a threshold. The optimal policy can be found by maximizing the right hand side of the Bellman optimality equation \\[\n\\pi^*(s) = \\arg\\max_a R_s^a +  \\gamma \\sum_{s'\\in S} P_{ss'}^a V^*(s').\n\\]\nIn practice, exactly solving the Bellman Expectation Equation in the policy evaluation step can be computationally expensive for large state spaces. Approximate methods may be used. Policy Iteration is particularly effective when the optimal policy needs to be very precise, as in high-stakes decision-making environments.\n\nExample 9.6 (MDP for a Maze) We use a mazemdp archive by Sally Gao, Duncan Rule, Yi Hao to demonstrate the value and policy iterations. Both are applied to the problem of finding an optimal policy for a maze. The maze is represented as a grid, with each cell either being a wall or empty. Agent (decision maker) does not know the maze structure and needs to find the optimal path from the start to the goal state. The agent starts in the bottom right corner and needs to reach the top left corner (marked as red). The agent can move up, down, left, or right, but not diagonally (actions). Moving into a wall keeps the agent in the same cell. Reaching the goal state gives a reward of +1, and all other transitions give a reward of 0. The goal is to find the optimal policy that maximizes the expected return (sum of discounted rewards) for the agent. In other words, the agent needs to find the shortest path to the exit.\nFigures below show the snapshot from policy (top row) and value (bottom row) iterations.\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=12\\)\n\n\n\n\n\n\n\n\\(t=24\\)\n\n\n\n\n\n\nFigure 9.3: Policy Iteration Solver\n\n\n\n\n\n\n\n\n\n\n\n\\(t=0\\)\n\n\n\n\n\n\n\n\\(t=15\\)\n\n\n\n\n\n\n\n\\(t=30\\)\n\n\n\n\n\n\nFigure 9.4: Value Iteration Solver\n\n\n\nThe policy iterations converged after 24 iterations.\n\nA more general form of a value function is the action-value function \\(Q^\\pi(s,a)\\), which represents the expected return when starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter. \\[\nQ^\\pi(s,a) = \\E[\\pi]{G_t \\mid s_t = s, a_t = a}.\n\\] We can derive both the value and optimal policy functions from the action-value function: \\[\nV^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) Q^\\pi(s,a)\n\\] \\[\n\\pi^*(s) = \\arg\\max_a Q^*(s,a)\n\\]\n\n\n\nModel-Free Methods\nBoth policy and value iterations we’ve considered thus far assume that transition probabilities between states given actions are known. However, this is often not the case in many real-world problems. Model-free methods learn through trial and error, by interacting with the environment and observing the rewards. The first method we consider is Monte Carlo methods. Monte Carlo methods for Markov Decision Processes (MDPs) are a class of algorithms used for finding optimal policies when the model of the environment (i.e., the transition probabilities and rewards) is unknown or too complex to model explicitly. These methods rely on learning from experience, specifically from complete episodes of interaction with the environment. Here’s a detailed look at how Monte Carlo methods work in the context of MDPs:\n\nGenerate Episodes: An episode is a sequence of states, actions, and rewards, from the start state to a terminal state. \\[\nS_0, A_0, R_1, S_1, A_1, R_2, \\ldots, S_{T-1}, A_{T-1}, R_T \\sim \\pi.\n\\] In Monte Carlo methods, these episodes are generated through actual or simulated interaction with the environment, based on a certain policy.\nEstimate Value Functions: Unlike dynamic programming methods, which update value estimates based on other estimated values, Monte Carlo methods update estimates based on actual returns received over complete episodes. This involves averaging the returns received after visits to each state. We use empirical mean to estimate the expected value.\nPolicy Improvement: After a sufficient number of episodes have been generated and value functions estimated, the policy is improved based on these value function estimates.\n\nMonte Carlo methods require complete episodes to update value estimates. This makes them particularly suitable for episodic tasks, where interactions naturally break down into separate episodes with clear starting and ending points. MC methods require sufficient exploration of the state space. This can be achieved through various strategies, like \\(\\epsilon\\)-greedy policies, where there’s a small chance of taking a random action instead of the current best-known action. In this case, the policy is given by \\[\n\\pi(a \\mid s) = \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|A|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n\\frac{\\epsilon}{|A|} & \\text{otherwise}\n\\end{cases}\n\\] where \\(\\epsilon\\) is the probability of taking a random action and \\(|A|\\) is the number of actions. The \\(\\epsilon\\)-greedy policy is an example of an exploration-exploitation strategy, where the agent explores the environment by taking random actions (exploration) while also exploiting the current knowledge of the environment by taking the best-known action (exploitation). The value of \\(\\epsilon\\) is typically decayed over time, so that the agent explores more in the beginning and exploits more later on.\nMonte Carlo methods are model-free, meaning they do not require a model of the environment (transition probabilities and rewards). They are also effective in dealing with high variance in returns, which can be an issue in some environments. However, they can be inefficient due to high variance and the need for many episodes to achieve accurate value estimates. They also require careful handling of the exploration-exploitation trade-off. The two main approaches for Monte Carlo methods are first-visit and every-visit methods.\n\nFirst-Visit MC: In this approach, the return for a state is averaged over all first visits to that state in each episode.\nEvery-Visit Monte Carlo: Here, the return is averaged over every visit to the state, not just the first visit in each episode.\n\nMonte Carlo Policy Iteration involves alternating between policy evaluation (estimating the value function of the current policy using Monte Carlo methods) and policy improvement (improving the policy based on the current value function estimate). This process is repeated until the policy converges to the optimal policy.\nTo find the optimal policy, a balance between exploration and exploitation must be maintained. This is achieved through strategies like \\(\\epsilon\\)-greedy exploration. In Monte Carlo Control, the policy is often improved in a greedy manner based on the current value function estimate.\nRecall that an arithmetic average can be updated recursively \\[\n\\bar{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i = \\frac{1}{n}\\left(x_n + \\sum_{i=1}^{n-1} x_i\\right) = \\frac{1}{n}\\left(x_n + (n-1)\\bar{x}_{n-1}\\right) = \\bar{x}_{n-1} + \\frac{1}{n}(x_n - \\bar{x}_{n-1}).\n\\] This is called a running average. We can use this recursion to update the value function \\(V(s)\\) incrementally, each time we visit state \\(s\\) at time \\(t\\). \\[\nV(s_t) = V(s_t) + \\frac{1}{N(s_t)}(G_t - V(s_t)),\n\\] where \\(N(s_t)\\) is the number of times we visited state \\(s_t\\) before time \\(t\\) and \\(G_t\\) is the return at time \\(t\\). This is called first-visit Monte Carlo method. Alternatively, we can use every-visit Monte Carlo method, where we update the value function each time we visit state \\(s\\).\nAlternatively, we can use a learning rate \\(\\alpha\\) \\[\nV_{n+1} = V_n + \\alpha(G_n - V_n).\n\\] This is called constant step size update. The learning rate is a hyperparameter that needs to be tuned. The constant step size update is more convenient because it does not require keeping track of the number of visits to each state. The constant step size update is also more robust to non-stationary problems.\nTemporal Difference Learning (TD Learning) Similar to MC, TD methods learn directly from raw experience without a model of the environment. However, unlike MC methods, TD methods update value estimates based on other learned estimates, without waiting for the end of an episode. This is called bootstrapping. TD methods combine the sampling efficiency of Monte Carlo methods with the low variance of dynamic programming methods. They are also model-free and can learn directly from raw experience. However, they are more complex than MC methods and require careful tuning of the learning rate.\nA simple TD method is TD(0), which updates value estimates based on the current reward and the estimated value of the next state. The update rule is \\[\nV(S_t) = V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)),\n\\] where \\(\\alpha\\) is the learning rate. The TD(0) method is also called one-step TD because it only looks one step ahead. The \\(R_{t+1} + \\gamma V(S_{t+1})\\) term is called the TD target and is a biased estimate of \\(V(S_t)\\). The difference \\(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error. The TD target is an estimate of the return \\(G_t\\) and the TD error is the difference between the TD target and the current estimate \\(V(S_t)\\). Although TD algorithms have lower variance than MC methods, they have higher bias. In practice TD methods are more efficient than MC methods.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#q-learning",
    "href": "09-rl.html#q-learning",
    "title": "9  Reinforcement Learning",
    "section": "9.4 Q-Learning",
    "text": "9.4 Q-Learning\nQ-learning is an off-policy algorithm that learns the optimal policy by directly estimating the optimal action-value function \\(Q^*(s,a)\\). The algorithm iteratively updates the action-value function using the Bellman optimality backup. The off-policy means that the algorithm learns the optimal policy while following a different policy. The algorithm can learn the optimal policy while following a random policy, for example. The algorithm can be summarized as follows: \\[\nQ(S_t,A_t) = Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a) - Q(S_t,A_t)),\n\\] where \\(\\alpha\\) is the learning rate. The algorithm can be summarized as follows:\n\nInitialize \\(Q(s,a)\\) arbitrarily\nRepeat for each episode:\n\nInitialize \\(S\\)\nRepeat for each step of the episode:\n\nChoose \\(A\\) from \\(S\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy)\nTake action \\(A\\), observe \\(R\\), \\(S'\\)\n\\(Q(S,A) = Q(S,A) + \\alpha(R + \\gamma \\max_a Q(S',a) - Q(S,A))\\)\n\\(S = S'\\)\n\nUntil \\(S\\) is terminal\n\n\nThen we can simplify the update rule to \\[\nQ(S_t,A_t) = (1-\\alpha)Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma \\max_a Q(S_{t+1},a)).\n\\]",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#bayesian-optimization",
    "href": "09-rl.html#bayesian-optimization",
    "title": "9  Reinforcement Learning",
    "section": "9.5 Bayesian Optimization",
    "text": "9.5 Bayesian Optimization\nBayesian Optimization solves an optimization problem using sequential design of experiments, which is the same in spirit as reinforcement learning but focuses on finding the optimum of a static function rather than a policy.\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. It is particularly useful when the objective function is expensive to evaluate. Bayesian optimization uses a surrogate model to approximate the objective function and an acquisition function to decide where to sample next. The surrogate model is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. Bayesian optimization is a global optimization method, meaning it does not require derivatives and can find the global optimum of the objective function. It is also sample-efficient, meaning it can find the optimum with fewer samples than other methods. However, it can be slow in practice and is not suitable for high-dimensional problems.\nGiven a function \\(f(x)\\) that is not known analytically (it can represent, for example, output of a complex computer program), the goal is to optimize \\[\nx^* = \\arg\\min_x f(x).\n\\]\nThe Bayesian approach to this problem is the following:\n\nDefine a prior distribution over \\(f(x)\\)\nCalculate \\(f\\) at a few points \\(x_1, \\ldots, x_n\\)\nRepeat until convergence:\n\nUpdate the prior to get the posterior distribution over \\(f(x)\\)\nChoose the next point \\(x^+\\) to evaluate \\(f(x)\\)\nCalculate \\(f(x^+)\\)\n\nPick \\(x^*\\) that corresponds to the smallest value of \\(f(x)\\) among evaluated points\n\nThe prior distribution is typically a Gaussian process (GP) model, which is a probabilistic model that defines a distribution over functions. The GP model is defined by a mean function \\(m(x)\\) and a covariance function \\(k(x,x')\\). The mean function is typically set to zero. The covariance function is typically a squared exponential function \\[\nk(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2l^2}\\right),\n\\] where \\(\\sigma_f^2\\) is the signal variance and \\(l\\) is the length scale. The covariance function defines the similarity between two points \\(x\\) and \\(x'\\). The covariance function is also called a kernel function. The kernel function is a measure of similarity between inputs \\(x\\) and \\(x'\\).\nNow we need to decide where to sample next. We can use the acquisition function to decide where to sample next. The acquisition function is a heuristic that trades off exploration and exploitation to decide where to sample next. The expected improvement (EI) function is a popular acquisition function. Suppose \\[\nf^* = \\min y\n\\] is the minimum value of \\(f(x)\\) among evaluated points. At a given point \\(x\\) and function value \\(y = f(x)\\), the expected improvement function is defined as \\[\na(x) = \\E{\\max(0, f^* - y)},\n\\] The function that we calculate expectation of \\[\nu(x) = \\max(0, f^* - y)\n\\] is the utility function. Thus, the acquisition function is the expected value of the utility function.\nThe acquisition function is high when \\(y\\) is likely to be lower than \\(f^*\\), and low when \\(y\\) is likely to be higher than \\(f^*\\). Given the GP prior, we can calculate the acquisition function analytically. The posterior distribution of Normal \\(y \\sim N(\\mu,\\sigma^2)\\), then the acquisition function is \\[\\begin{align*}\na(x) &= \\E{\\max(0, f^* - y)} \\\\\n&= \\int_{-\\infty}^{\\infty} \\max(0, f^* - y) \\phi(y,\\mu,\\sigma^2) dy \\\\\n& \\text{Since we are interested in improvement } f^* - y &gt; 0 \\text{, i.e., } y &lt; f^* \\text{, the integral is from } -\\infty \\text{ to } f^* \\\\\n&= \\int_{-\\infty}^{f^*} (f^* - y) \\phi(y,\\mu,\\sigma^2) dy\n\\end{align*}\\] where \\(\\phi(y,\\mu,\\sigma^2)\\) is the probability density function of the normal distribution. A useful identity is \\[\n\\int y \\phi(y,\\mu,\\sigma^2) dy =\\frac{1}{2} \\mu ~  \\text{erf}\\left(\\frac{y-\\mu }{\\sqrt{2} \\sigma }\\right)-\\frac{\\sigma\n   e^{-\\frac{(y-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi }},\n\\] where \\(\\Phi(y,\\mu,\\sigma^2)\\) is the cumulative distribution function of the normal distribution. Thus, \\[\n\\int_{-\\infty}^{f^*} y \\phi(y,\\mu,\\sigma^2) dy = \\frac{1}{2} \\mu (1+\\text{erf}\\left(\\frac{f^*-\\mu }{\\sqrt{2} \\sigma\n   }\\right))-\\frac{\\sigma  e^{-\\frac{(f^*-\\mu )^2}{2 \\sigma ^2}}}{\\sqrt{2 \\pi}} = \\mu \\Phi(f^*,\\mu,\\sigma^2) + \\sigma^2 \\phi(f^*,\\mu,\\sigma^2).\n\\]\nwe can write the acquisition function as \\[\na(x) = \\dfrac{1}{2}\\left(\\sigma^2 \\phi(f^*,\\mu,\\sigma^2) + (f^*-\\mu)\\Phi(f^*,\\mu,\\sigma^2)\\right)\n\\]\nWe can implement it\n\nacq &lt;- function(xx,p, fstar) {\n  x = matrix(xx, ncol=1)\n  d = fstar - p$mean\n  s = sqrt(diag(p$Sigma))\n  return(s*dnorm(d) + d*pnorm(d))\n}\n\n\nExample 9.7 (Taxi Fleet Optimisation) We will use the taxi fleet simulator from Emukit project. For a given demand (the frequency of trip requests) and the number of taxis in the fleet, it simulates the taxi fleet operations and calculates the profit. The simulator is a black-box function, meaning it does not have an analytical form and can only be evaluated at specific points. The goal is to find the optimal number of taxis in the fleet that maximizes the profit. We will use Bayesian optimization to solve this problem.\n\n\n\nTaxi Simulator Visualization\n\n\nWe start with initial set of three designs \\(x = (10,30,90)\\), where \\(x\\) is the number of the taxis in the fleet and observe the corresponding profits profit=(3.1,3.6,6.6). When \\(x=10\\), the demand for taxis exceeds the supply and passengers need to wait for their rides, leading to missed profit opportunities. At another extreme when we have 90 taxis, the profit is slightly better. However, there are many empty taxis, which is not profitable. The optimal number of taxis must be somewhere in the middle. Finally, we try 30 taxis and observe that the profit is higher than both of our previous attempts. However, should we increase or decrease the number of taxis from here? We can use Bayesian optimization to answer this question. First we define a convenience function to plot the GP emulator.\n\nplotgp = function(x,y,XX,p) {\n  q1 = qnorm(0.05, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q2 = qnorm(0.95, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  q3 = qnorm(0.5, mean = p$mean, sd = sqrt(diag(p$Sigma)))\n  ggplot() + \n    geom_point(aes(x=x,y=y)) + \n    geom_line(aes(x=XX,y=q3), col=2, lwd=2) + \n    geom_ribbon(aes(x=XX, ymin=q1, ymax=q2), fill=\"blue\", alpha=0.2)\n}\n\nNow, we fit the GP emulator using our initial set of observed taxi-profit pairs, the result is shown in Figure 9.5.\n\nlibrary(laGP)\nlibrary(mvtnorm)\nx = matrix(c(10,90,30), ncol=1)\nxx = seq(1,100, length=500)\nXX &lt;- matrix(xx, ncol = ncol(x))\nprofit = -c(3.1,3.6,6.6)\ngp &lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\np &lt;- predGP(gp, XX)\nplotgp(x,profit,XX,p)\n\n\n\n\n\n\n\nFigure 9.5: Initial GP emulator fit to the taxi-profit pairs.\n\n\n\n\n\nInstead of maximizing the profit, we minimize the negative profit. We see that there is potentially a better value at around 50 taxis. We can use the acquisition function to decide where to sample next. We define two functions: nextsample that uses the acquisition function to decide where to sample next and updgp that updates the GP emulator with the new sample. Then we call those two functions twice. First time, EI suggests 44 and second time it suggests 42. We update the GP emulator with the new samples and plot the updated emulator. We see that the GP emulator is updated to reflect the new samples, shown in figure below.\nnextsample = function(){\n  ei = acq(xx,p,min(profit))\n  plot(xx,ei, type='l', col=2, lwd=2)\n  xnext = as.integer(xx[which.max(ei)])\n  return(xnext)\n}\nupdgp = function(xnext,f){\n  profit &lt;&lt;- c(profit, f)\n  x &lt;&lt;- rbind(x, xnext)\n  gp &lt;&lt;- newGP(x, profit, 1000, 1e-6, dK = TRUE)\n  p &lt;&lt;- predGP(gp, XX)\n  plotgp(x,profit,XX,p)\n}\nnextsample(); #44\nnextsample(); # 57\nnextsample(); # 45\nnextsample(); # 100\n\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n## 44\nupdgp(44, -8.4);\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n## 57\nupdgp(57, -7.1);\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n## 45\nupdgp(45, -8.5);\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n## 100\nupdgp(100, -3.3);\n\n\n\n\n\nUpdated GP emulator fit to the taxi-profit pairs.\n\n\n\n\n\nIf we run nextsample one more time, we get 47, close to our current best of 45. Further, the model is confident at this location. It means that we can stop the algorithm and declare victory, shown in Figure 9.6.\n\nnextsample()\n## 47\n\n\n\n\n\n\n\nFigure 9.6: The acquisition function after 5 samples. The maximum is at 47.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "09-rl.html#concluding-remarks",
    "href": "09-rl.html#concluding-remarks",
    "title": "9  Reinforcement Learning",
    "section": "9.6 Concluding Remarks",
    "text": "9.6 Concluding Remarks\nIn this chapter, we explored the foundational concepts of Markov Decision Processes (MDPs) and their central role in reinforcement learning. We saw how MDPs provide a flexible mathematical framework for modeling sequential decision-making under uncertainty, with applications ranging from robotics and operations research to online recommendation systems and resource allocation in business.\nThrough both analytical derivations and Monte Carlo simulations, we examined classic problems such as the secretary problem and taxi fleet optimization, illustrating how simulation and Bayesian optimization can be used to make effective decisions in complex, uncertain environments. The use of Gaussian Process (GP) emulators and acquisition functions like Expected Improvement (EI) demonstrates the power of combining probabilistic modeling with principled exploration strategies—a hallmark of modern reinforcement learning.\nAs you continue your study of reinforcement learning, remember that the real world rarely presents us with simple, fully known models. The techniques introduced here—modeling uncertainty, simulating outcomes, and iteratively improving decisions—are essential tools for tackling the challenges of real-world AI and data-driven decision making. Whether you are optimizing ad placements, managing supply chains, or designing intelligent agents, the principles of MDPs and Bayesian optimization will serve as a strong foundation for your work.\n\n\n\n\nScott, Steven L. 2015. “Multi-Armed Bandit Experiments in the Online Service Economy.” Applied Stochastic Models in Business and Industry 31 (1): 37–45.",
    "crumbs": [
      "Bayes",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Reinforcement Learning</span>"
    ]
  },
  {
    "objectID": "10-data.html",
    "href": "10-data.html",
    "title": "10  Unreasonable Effectiveness of Data",
    "section": "",
    "text": "10.1 The Wisdom and Madness of Crowds\nChapters in Part I developed a Bayesian language for uncertainty, learning, and decisions, including sequential decisions. We now shift emphasis from inference about parameters to prediction of future or unseen outcomes. This shift changes what matters: scalability, out-of-sample performance, and how representation choices influence generalization. One useful distinction for Part II is between pattern matching, understood broadly as function approximation, and statistical learning, where the emphasis remains on inference, uncertainty quantification, and the consequences of modeling assumptions. The next chapters use classical statistical models as stepping stones toward modern machine learning, while keeping uncertainty and decision-making as recurring themes rather than afterthoughts.\nData collected by early telescopes played a crucial role in the development of statistical techniques during the 18th century, just as Internet and mobile devices do in the 21st. Massive astronomical data sets inspired scientists such as Carl Friedrich Gauss, Pierre-Simon Laplace, and Simeon Denis Poisson to devise data-driven methods, including the method of least squares and the Poisson distribution. These methods fundamentally transformed science, shifting the focus from purely theoretical derivation to the rigorous, quantitative interrogation of observational data. The integration of data and statistical methods laid the foundation for modern data science and statistics, demonstrating the power and versatility of data-driven approaches.\nIn the 18th and 19th centuries data collection was often limited to manual measurements or observations, and the amount of available data was typically much smaller compared to the massive datasets encountered in modern data science. Scientists like Gauss and Poisson often conducted carefully designed experiments, collected their own data, and performed manual calculations without the aid of computers or advanced statistical software. The focus of their work was often on theoretical developments in mathematics, physics and astronomy, and the data was used to test and validate their theories. We can consider one of those studies from the early 18th century.\nMotivated by the analysis of planetary orbits and determining the shape of the Earth, later in 1805, Adrien-Marie Legendre (1752-1833) published the first clear and concise explanation of the least squares method in his book “Nouvelles methodes pour la determination des orbites des cometes”. The method of least squares is a powerful statistical technique used today to fit a mathematical model to a set of data points. Its goal is to find the best-fitting curve that minimizes the sum of the squared distances (also known as residuals) between the curve and the actual data points. Compared to the approach proposed by Boscovich, the least squares method is less robust to measurement errors and inconsistencies. However, from a computational point of view, it is more efficient and various algorithms exist for efficient calculation of curve parameters. This computational efficiency is crucial for modern data analysis, where datasets can be massive and complex, making least squares a fundamental tool in statistics and data analysis, offering a powerful and widely applicable approach to data fitting and model building.\nLegendre provided a rigorous mathematical foundation for the least squares method, demonstrating its theoretical underpinnings and proving its optimality under certain conditions. This mathematical basis helped establish the credibility and legitimacy of the method, paving the way for its wider acceptance and application. Legendre actively communicated his ideas and collaborated with other mathematicians, such as Carl Friedrich Gauss (1777-1855), who also contributed significantly to the development of the least squares method. While evidence suggests Gauss used the least squares method as early as 1795, his formal publication came later than Legendre’s in 1809. Despite the delay in publication, Gauss independently discovered the method and applied it to various problems, including celestial mechanics and geodesy. He developed efficient computational methods for implementing the least squares method, making it accessible for practical use by scientists and engineers. While Legendre’s clear exposition and early publication brought the least squares method to the forefront, Gauss’s independent discovery, theoretical development, practical applications, and contributions to computational methods were equally crucial in establishing the method’s significance and impact. Both mathematicians played vital roles in shaping the least squares method into the powerful statistical tool it is today.\nAnother French polymath Pierre-Simon Laplace (1749-1827) extended the methods of Boscovich and showed that the curve fitting problem could be solved by ordering the candidate slopes and finding the weighted median. Besides that, Laplace made fundamental contributions to probability theory, developing the Bayesian approach to inference. Most of Laplace’s work was in the field of celestial mechanics, where he used data from astronomical observations to develop mathematical models and equations describing the gravitational interactions between celestial bodies. His analytical methods and use of observational data were pioneering in the field of celestial mechanics. Furthermore, he developed methods for estimating population parameters from samples, such as the mean and variance and pioneered the use of random sampling techniques, which are essential for ensuring the validity and generalizability of statistical inferences. These contributions helped lay the foundation for modern sampling theory and survey design, which are crucial for conducting reliable and representative studies. Overall, Laplace’s contributions to data analysis were profound and enduring. His work in probability theory, error analysis, sampling methods, and applications significantly advanced the field and laid the groundwork for modern statistical techniques. He also played a crucial role in promoting statistical education and communication, ensuring that these valuable tools were accessible and utilized across various disciplines.\nBoscovich used what we call today a linear regression analysis. This type of analysis relies on the assumption that the relationship between the independent (sine squared of the latitude) and dependent (arc length) variables is linear. Francis Galton was the person who coined the term “regression” in the context of statistics. One of the phenomena he studied was the relationship between the heights of parents and their children. He found that the heights of children tended to regress towards the average height of the population, which led him to use the term “regression” to describe this phenomenon. Galton promoted a data-driven approach to research that continues to shape statistical practice today. Furthermore, he introduced the concept of quantiles, which divide a population into equal-sized subpopulations based on a specific variable. This allowed for a more nuanced analysis of data compared to simply considering the mean and median. He also popularized the use of percentiles, which are specific quantiles used to express the proportion of a population below a certain value.\nGalton used regression analysis to show that the offspring of exceptionally large or small seed sizes of sweet peas tended to be closer to the average size. He also used it for family studies and investigated the inheritance of traits such as intelligence and talent. He used regression analysis to assess the degree to which these traits are passed down from parents to offspring.\nGalton’s overall approach to statistics was highly influential. He emphasized the importance of quantitative analysis, data-driven decision-making, and empirical research, which paved the way for modern statistical methods and helped to establish statistics as a recognized scientific discipline.\nWhile Galton used data to understand heredity and regression to the mean in biological systems, modern applications leverage the same principles at a scale and speed he could scarcely have imagined. The evolution of statistical methods—from Boscovich’s manual calculations to Galton’s regression—has culminated in systems that perform these optimizations millions of times per second. In high-stakes environments, the “unreasonable effectiveness of data” transforms from a tool for scientific discovery into a competitive necessity.\nThe Formula One example illustrates the power of data quantity and velocity—millions of data points processed instantly to optimize a single objective. However, the effectiveness of data depends just as heavily on how it is aggregated. Having massive amounts of data is useless if the method of combination introduces bias or fails to extract the signal. This brings us to a fundamental paradox in data science: under similar conditions, aggregating independent judgments can lead to either supernatural accuracy or catastrophic delusion.\n“Men, it has been well said, think in herds; it will be seen that they go mad in herds, while they only recover their senses slowly, and one by one.” This observation opens Charles Mackay’s 1841 work Extraordinary Popular Delusions and the Madness of Crowds, one of the earliest systematic examinations of collective human behavior. Yet just sixty-six years later, Francis Galton would document the opposite phenomenon: a crowd of nearly 800 people accurately estimating the weight of an ox to within 1% of its true value. How can crowds be both mad and wise? And what does this duality tell us about aggregating data and building intelligent systems?\nThe tension between collective wisdom and collective madness cuts to the heart of modern data science and artificial intelligence. When we combine multiple predictions, aggregate diverse data sources, or build ensemble models, we implicitly trust in the wisdom of aggregation. Yet history—from the Dutch tulip mania of 1637 to the dot-com bubble of the late 1990s—reminds us that crowds can be spectacularly, catastrophically wrong. Understanding when and why crowds exhibit wisdom versus madness has profound implications for how we design AI systems, interpret market prices, and aggregate human judgment.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unreasonable Effectiveness of Data</span>"
    ]
  },
  {
    "objectID": "10-data.html#the-wisdom-and-madness-of-crowds",
    "href": "10-data.html#the-wisdom-and-madness-of-crowds",
    "title": "10  Unreasonable Effectiveness of Data",
    "section": "",
    "text": "Historical Delusions and Economic Bubbles\nCharles Mackay’s Extraordinary Popular Delusions and the Madness of Crowds (Mackay 1841) chronicled three major categories of collective folly: economic manias, alchemical and prophetic delusions, and the social dynamics of witch hunts and fortune-telling. His most enduring contribution lies in documenting economic bubbles, particularly three spectacular episodes from the 17th and 18th centuries.\nThe Dutch tulip mania of 1636-1637 represents perhaps the purest example of speculative madness. At the height of the mania, single tulip bulbs sold for more than ten times the annual income of a skilled craftsman. A Semper Augustus bulb reportedly sold for 6,000 florins—enough to purchase one of the grandest houses on the most fashionable canal in Amsterdam. The market wasn’t driven by the intrinsic value of tulips or even their aesthetic beauty, but by the expectation that prices would continue rising. When the bubble inevitably collapsed in February 1637, fortunes vanished overnight, leaving behind ruined merchants and a cautionary tale about the dangers of collective speculation.\nThe South Sea Bubble of 1720 in England followed a similar trajectory but on a larger scale. The South Sea Company, granted a monopoly on trade with South America, saw its stock price rise from £128 in January to over £1,000 by August—despite generating minimal actual revenue from trade. The company’s success spawned a wave of similarly dubious ventures, including companies “for carrying on an undertaking of great advantage, but nobody to know what it is.” When confidence finally broke, the collapse devastated the British economy and ruined thousands of investors, including Isaac Newton, who reportedly lamented, “I can calculate the motion of heavenly bodies, but not the madness of people.”\nThe Mississippi Scheme in France, orchestrated by Scottish financier John Law, created the third great bubble Mackay chronicled. Law convinced the French government to establish a national bank and a trading company with monopoly rights to develop French Louisiana. Through a complex scheme of debt conversion and money printing, Law inflated both the currency and company shares to unsustainable levels. When the bubble burst in 1720, it destroyed the French economy and created such trauma that France wouldn’t establish another national bank for decades.\nMackay’s analysis identified common patterns across these episodes:\n\nGradual inception: Bubbles begin with a kernel of truth—tulips were genuinely valuable, the South Sea Company did have a trade monopoly, Louisiana held economic potential.\nSocial contagion: As early investors profit, others join not from fundamental analysis but from observing their neighbors’ gains.\nSuspension of skepticism: Normal risk assessment breaks down as stories of easy wealth dominate rational calculation.\nNew era thinking: Participants convince themselves “this time is different,” that old rules no longer apply.\nCatastrophic collapse: Once confidence breaks, the crowd rushes for the exits, and prices collapse faster than they rose.\n\nWhile modern historians have questioned some of Mackay’s details—the tulip mania may have been less extreme and more localized than he portrayed—his central insight endures: crowds can synchronize on beliefs wildly divorced from reality, sustaining collective delusions until the inevitable reckoning.\n\n\nGalton’s Ox: The Wisdom of Aggregation\nFrancis Galton, whom we met earlier pioneering regression and correlation, approached crowd behavior from a different angle. In 1907, he attended a livestock fair in Plymouth where nearly 800 people paid sixpence each to guess the dressed weight of an ox. The crowd included expert butchers and farmers as well as complete novices. After the competition, Galton obtained all the tickets and analyzed the distribution of guesses (Galton 1907).\nThe ox weighed 1,198 pounds when dressed. The median estimate from the crowd: 1,207 pounds—an error of less than 1%, or just 9 pounds. This remarkable accuracy led Galton to conclude that the result was “more creditable to the trustworthiness of a democratic judgment than might have been expected.”\nGalton’s statistical analysis revealed several fascinating patterns. The probable error of a single randomly selected estimate was $$37 pounds (about 3.1% of the true weight). Yet the median captured the true weight with far greater precision. The distribution wasn’t symmetric: estimates above the median had a quartile deviation of 45 pounds (3.7%), while estimates below deviated by only 29 pounds (2.4%). This asymmetry suggested systematic cognitive bias—people were more likely to overestimate than underestimate weight.\nThe middlemost 50% of estimates ranged from 1,178 to 1,252 pounds, a spread of 74 pounds around the true value. Galton observed that competitors appeared to “have erred normally in the first instance, and then to have magnified all errors that were negative and to have minified all those that were positive”—a remarkably prescient description of what modern psychologists would call anchoring and adjustment biases.\nWhat made the crowd wise? Several conditions aligned:\nIndependence: Each person wrote their estimate privately, without conferring. There was no oratory, no group discussion, no opportunity for social influence to create correlated errors.\nDiversity: The crowd mixed genuine experts (butchers who judged livestock daily) with laypeople relying on crude heuristics. This heterogeneity ensured errors weren’t systematically biased in the same direction.\nIncentive alignment: The sixpence fee deterred frivolous guesses (as Galton noted, it was “sufficiently high to prevent much ‘practical joking’”), while prizes motivated genuine effort.\nAppropriate aggregation: Galton chose the median rather than the mean, making the result robust to outliers and extreme estimates.\nThe experiment demonstrated a fundamental principle: aggregating diverse, independent estimates can produce accuracy exceeding individual expertise. No single person—not even the most experienced butcher—was as accurate as the median of the crowd. The collective judgment extracted signal from the noise of individual errors.\n\n\nSmart Money, Dumb Money: Learning from Crowds\nIf Galton showed how aggregation creates wisdom, Heaton and Polson’s 2012 paper reveals how it can perpetuate madness (Heaton and Polson 2012). Their work examines financial markets where two types of traders coexist: “smart money” who know true probabilities, and “dumb money” who hold systematically incorrect beliefs. Like in Galton’s ox-guessing crowd, market participants report their estimates by actively betting against each other, and prices reflect the proportion of money on each side—not a probability-weighted average of beliefs.\nSuppose an asset can default or not default. Smart money knows the true default probability is 30%, while dumb money believes it’s only 10%. If dumb money constitutes 45% of the market, they’ll bet heavily against default, placing 45% of total market capital on “no default.” Smart money, knowing default is more likely, bets the remaining 55% on default. The equilibrium price for “default” becomes 0.55—simply the proportion betting on that outcome—which overestimates the true 30% probability.\nThis creates systematic inefficiency. Dumb money consistently loses because they bet at unfavorable odds, while smart money earns predictable profits. Yet the market doesn’t naturally correct this imbalance. Smart money can’t arbitrage away all the mispricing because they have limited capital and betting against a bubble is fraught with risk; as the famous saying goes, “the market can remain irrational longer than you can remain solvent.” The inefficiency persists. The central question is then Can dumb money learn from market prices that they are the dumb money?\nThis proves surprisingly difficult due to the identification problem. When a trader observes a market price, they see the aggregate result of all bets, but they cannot inherently distinguish their own contribution from the “smart” contribution.\nConsider a market split 45-55. A trader in the 45% group knows there is disagreement. However, they face a symmetric ambiguity:\n\nScenario A: They are the “Smart Money” (minority insiders), and the 55% are the “Dumb Money” (overconfident herd).\nScenario B: They are the “Dumb Money” (contrarian fools), and the 55% are the “Smart Money” (rational majority).\n\nThe market price alone (\\(p=0.55\\)) is often consistent with both scenarios, rendering the signal uninformative. A price of 0.55 could arise from Smart Money knowing the truth, or Dumb Money pushing the price up. You cannot facilitate the distinction just by looking at the price. Heaton and Polson formalize this using Bayesian learning. To learn they are “dumb,” a trader must update their posterior odds:\n\\[\n\\frac{P(\\text{Dumb} \\mid \\text{Price})}{P(\\text{Smart} \\mid \\text{Price})} = \\underbrace{\\frac{P(\\text{Price} \\mid \\text{Dumb})}{P(\\text{Price} \\mid \\text{Smart})}}_{\\text{Likelihood Ratio}} \\times \\underbrace{\\frac{P(\\text{Dumb})}{P(\\text{Smart})}}_{\\text{Prior Odds}}\n\\]\nIf the Likelihood Ratio is 1, no learning occurs. This happens when the observed price is equally probable whether you are smart or dumb.\nIn the worst-case scenario, the distributions of expected prices conditional on your type overlap perfectly.\nLearning requires two conditions: (1) the likelihood ratio must favor being dumb money given the observed price, and (2) your prior must assign non-zero probability to being dumb. When smart and dumb money are equally balanced, the likelihood ratio equals 1. No amount of observation can update beliefs about one’s type. Paradoxically, increasing smart money toward this 50/50 balance might reduce market efficiency by making it harder for dumb money to learn.\nAs the authors note, “there is a sense in which the essence of being the dumb money is thinking too strongly that one is the smart money.” Learning requires assigning non-zero prior probability to being systematically wrong—a “psychologically difficult self-evaluation” that may be precisely what defines dumb money. Overconfidence isn’t just a symptom of being dumb money; it’s what prevents learning that one is dumb money.\nEven if dumb money suspects they might be wrong, they must accurately estimate the proportion of dumb money in the market. But identifying market composition is notoriously difficult. During the dot-com bubble, for instance, dumb money “had been laughing all the way to the bank”—their short-term success reinforced confidence that they were actually the smart money.\nEven if dumb money concludes they’re probably dumb money, learning must be strong enough to reverse their position. Merely adjusting beliefs slightly won’t change which side of the bet appears attractive. The paper illuminates why certain markets remain persistently inefficient. It’s not just that dumb money exists, but that dumb money cannot learn it is dumb money. The very characteristics that make someone dumb money—overconfidence, poor base rate estimation, unwillingness to question fundamental assumptions—are precisely those that prevent self-correction.\nThe authors cite the 2008 financial crisis as a real-world example. A handful of hedge funds bet against subprime mortgages at extremely favorable odds. As Michael Lewis documented in The Big Short, the constraint wasn’t demand for the bet—smart money was willing—but supply. Finding enough counterparties willing to take the other side proved difficult. Yet even as the crisis unfolded, many “dumb money” participants couldn’t learn their type until catastrophic losses forced the realization.\nThe contrast between Galton’s wise crowd and Heaton and Polson’s mad market reveals critical lessons for building intelligent systems. Galton’s ox-guessing competition succeeded because it created ideal conditions for wisdom:\nStatistical independence: Errors weren’t correlated. When one person overestimated, another’s underestimate balanced it out. This is why ensemble machine learning methods work: combining independent models reduces variance while preserving low bias. Random forests, for instance, decorrelate decision trees by training on random subsets of data and features, ensuring individual tree errors don’t reinforce each other.\nDiversity of approach: The crowd used heterogeneous methods—some people estimated volume then converted to weight, others compared to familiar animals, butchers relied on professional experience. This diversity ensured systematic biases in one approach were offset by different biases in another. Similarly, ensemble methods benefit from combining fundamentally different model types (e.g., neural networks, tree-based models, and linear models) rather than multiple instances of the same architecture.\nAppropriate aggregation: Galton used the median, which is robust to outliers. In modern ensemble methods, we similarly choose aggregation schemes carefully: majority voting for classification, median or trimmed mean for regression, weighted combinations based on confidence. The aggregation method matters as much as the individual models.\nNo strategic interaction: Participants weren’t betting against each other or trying to exploit others’ mistakes. Each estimate represented genuine belief. This differs fundamentally from adversarial settings where agents game the system.\nHeaton and Polson’s market differs on crucial dimensions:\nSystematic subgroup bias: Dumb money isn’t randomly wrong—they’re systematically wrong in the same direction. Aggregating their bets doesn’t cancel errors; it embeds bias in prices. In machine learning, if multiple models share the same systematic bias (say, all trained on biased data), ensembling won’t fix the problem. Voting among biased models just entrenches the bias.\nStrategic interaction: Market participants bet against each other. Prices reflect not just beliefs but capital constraints and risk appetite. Smart money can’t arbitrage away all inefficiency. Similarly, in adversarial machine learning settings (spam detection, fraud detection, adversarial attacks on neural networks), the presence of strategic agents fundamentally changes aggregation dynamics.\nCircular inference: Prices reflect participants’ own bets, creating a circularity: dumb money observes prices that partially reflect their own behavior and must infer whether they’re on the right or wrong side. In machine learning, this resembles the challenge of training on data that includes your model’s own predictions—a form of feedback loop that can amplify rather than correct errors.\nBarriers to self-correction: Dumb money cannot learn its type without assigning prior probability to being wrong and accurately estimating market composition. In machine learning, this parallels the challenge of model selection uncertainty: an algorithm must know which class of model is appropriate before it can learn parameters. Choosing the wrong model class can be more damaging than getting parameters slightly wrong.\n\nDesigning Robust AI Systems\nThese lessons suggest several principles for building intelligent systems. Independence is important. Training on different data subsets through bagging creates independence by ensuring each model sees a different sample of the data. Using different features, as in random forests, prevents models from making identical mistakes based on the same input patterns. Employing different algorithms through stacking combines fundamentally different approaches to the same problem. Adding noise through techniques like dropout and data augmentation decorrelates errors by introducing controlled randomness that prevents models from overfitting to identical patterns.\nCalibrate confidence: Overconfidence is as dangerous in AI as in markets. Dumb money thinks it’s smart money; overfit models are “confident but wrong.” Calibration—ensuring predicted probabilities match actual frequencies—helps prevent this. Techniques like temperature scaling, Platt scaling, and isotonic regression (which enforces a monotonic relationship between predicted scores and probabilities) adjust model confidence to better reflect true uncertainty.\nAvoid feedback loops: Be cautious when models influence the data they’ll later train on. This occurs in recommender systems, where showing users content based on past behavior creates training data from that very behavior. Financial trading algorithms face similar challenges when their price predictions actually affect market prices. Search engines encounter this when their ranking algorithms influence user clicks, which then become training data for future ranking decisions. Content moderation systems create feedback loops when automated decisions generate the training data for future automation, potentially amplifying initial biases or errors.\nProvide unambiguous feedback: Unlike markets where feedback is delayed and noisy, AI systems should enable rapid, clear feedback about prediction quality. This accelerates learning and prevents prolonged periods of confident incorrectness.\n\n\nThe Bias-Variance Tradeoff\nGalton’s experiment beautifully illustrates the bias-variance tradeoff. Individual estimates had high variance (probable error of 3.1%) but low bias (median off by only 0.8%). The median reduced variance dramatically while preserving the low bias—a core principle in statistical learning.\nEnsemble methods exploit the same principle. If individual models have low bias but high variance, averaging reduces variance without increasing bias. This explains why bagging (bootstrap aggregating) works so well: by training multiple models on random data subsets, we create high-variance predictors whose errors largely cancel when averaged.\nHowever, if models have systematic bias, averaging won’t help—it may even hurt. If all models in an ensemble underestimate values for a particular subgroup (perhaps due to underrepresentation in training data), taking their average still underestimates. This is the “dumb money” problem: when errors are correlated and biased in the same direction, aggregation entrenches rather than eliminates the problem.\n\n\nInformation Markets and Prediction Platforms\nModern prediction markets attempt to harness Galton’s wisdom while avoiding Heaton and Polson’s madness. Platforms like Metaculus, Good Judgment Open, and Manifold Markets aggregate forecasts from diverse participants to predict future events—from election outcomes to technological breakthroughs.\nThese platforms implement several design principles to promote wisdom over madness:\nProper scoring rules: Participants are rewarded for accuracy, not just correct predictions. The Brier score, for instance, penalizes both overconfidence and underconfidence, incentivizing honest reporting of beliefs rather than strategic betting.\nReputation systems: Track forecaster accuracy over time, weighting predictions by historical performance. This effectively filters out “dumb money” by giving less influence to consistently poor predictors.\nExtremization: Research by Tetlock and others shows that aggregated predictions often benefit from “extremizing”—adjusting the consensus forecast away from 50% toward the extremes. If the average forecast is 70%, adjusting to 75% often improves accuracy. This suggests crowds are sometimes too cautious, insufficiently updating on shared information.\nTransparency: Display the distribution of forecasts, not just the median. This reveals when consensus is strong versus when the crowd is divided, providing information about uncertainty in the aggregate.\nIncentive alignment: Some platforms use real money (prediction markets), while others use reputation points or tournament prizes. The key is creating genuine incentive to be accurate rather than to follow the crowd or game the system.\nEarly evidence suggests these platforms can achieve impressive accuracy. Before the 2020 U.S. presidential election, prediction markets aggregated thousands of bets to estimate Biden’s probability of victory at around 60-70%, roughly matching sophisticated polling models. During the COVID-19 pandemic, forecasting platforms predicted vaccine development timelines more accurately than many expert committees. The success of these platforms validates Galton’s core insight: properly aggregated diverse judgments can rival or exceed expert predictions.\nThe arc from Mackay through Galton to Heaton and Polson traces the evolution of our understanding of collective intelligence. Mackay warned that crowds “go mad in herds,” documenting the catastrophic consequences of synchronized delusion. Galton demonstrated that crowds can extract wisdom from noise through proper aggregation of independent judgments. Heaton and Polson revealed the subtle conditions under which madness persists—when systematic bias, strategic interaction, and barriers to learning prevent self-correction.\nFor AI and machine learning, these lessons are foundational. Every ensemble method, every data aggregation scheme, every model averaging technique implicitly bets on the wisdom of aggregation. But wisdom isn’t automatic—it emerges from independence, diversity, appropriate aggregation methods, and absence of systematic bias. When these conditions fail, we get not intelligence but amplified error: overfit models that are confident but wrong, biased algorithms that entrench inequality, feedback loops that amplify rather than correct mistakes.\nThe unreasonable effectiveness of data depends not just on having more data, but on aggregating it wisely. As we build increasingly sophisticated AI systems that combine multiple models, integrate diverse data sources, and make consequential decisions, the distinction between wisdom and madness—between Galton’s ox and Mackay’s tulips—becomes ever more critical. The crowds we build into our algorithms must be wise ones, designed with intention to harness collective intelligence while guarding against collective delusion.\n\n\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian: Roger Boscovich and the First Modern Map of the Papal States.” In History of Cartography: International Symposium of the ICA, 2012, 71–89. Springer.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75: 450–51.\n\n\nHeaton, J. B., and N. G. Polson. 2012. “Smart Money, Dumb Money: Learning Type from Price.” Working Paper.\n\n\nMackay, Charles. 1841. Extraordinary Popular Delusions and the Madness of Crowds. London: Richard Bentley.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least Squares.” The Annals of Statistics, 465–74.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Unreasonable Effectiveness of Data</span>"
    ]
  },
  {
    "objectID": "11-pattern.html",
    "href": "11-pattern.html",
    "title": "11  Pattern Matching",
    "section": "",
    "text": "11.1 Why Pattern Matching?\n“Prediction is very difficult, especially about the future.” Niels Bohr, Danish physicist and Nobel laureate\nThe history of data analysis is closely intertwined with the development of pattern matching techniques. The ability to identify and understand patterns in data has been crucial for scientific discoveries, technological advancements, and decision-making. From the early days of astronomy to modern machine learning, pattern matching has played a pivotal role in advancing our understanding of the world around us. This chapter explores the key concepts of pattern matching, its historical development, and its impact on data analysis.\nData science involves two major steps: collection and cleaning of data and building a model or applying an algorithm. In this chapter we present the process of building predictive models. To illustrate the process, think of your data as being generated by a black box in which a set of input variables \\(x\\) go through the box and generate an output variable \\(y\\).\nFor Gauss, Laplace, and many other scientists, the central challenge was estimating parameters when the functional form of the relationship was already known—often linear (as in the Earth-shape example) or multiplicative (e.g., Newton’s \\(F=ma\\) or Einstein’s \\(E=mc^2\\)). In many modern problems, however, the relationship itself is unknown and defies simple mathematical description; human behaviour and natural language are prominent examples (Halevy, Norvig, and Pereira (2009)).\nIn such cases a pattern-matching approach can uncover the hidden relationships directly from data. Pattern matching means identifying recurring sequences, relationships, or structures within a dataset—much like finding a puzzle piece that completes a picture. Recognising these patterns yields insights, reveals trends, supports prediction, and ultimately improves decisions. Initial pattern-matching studies have often sparked major scientific advances, as the early history of mammography illustrates.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#why-pattern-matching",
    "href": "11-pattern.html#why-pattern-matching",
    "title": "11  Pattern Matching",
    "section": "",
    "text": "Example 11.1 (Mammography and Early Pattern Matching) Early mammography relied on visual pattern matching to detect cancer signs like masses and microcalcifications. Radiologists used their expertise to distinguish these patterns from normal tissue, though the process was subjective and error-prone. Despite these challenges, this visual pattern matching laid the foundation for modern screening.\nGerman surgeon Albert Solomon pioneered this field with his 1913 monograph (Nicosia et al. (2023)). By comparing X-rays of surgically removed tissue with the actual specimens, he identified characteristic features of tumors and was among the first to link microcalcifications to breast cancer—a correlation that remains a key biomarker today, even as the underlying molecular mechanisms are still being studied (Bonfiglio et al. (2021)).\n\n\nRichard Feynman on Pattern Matching and Chess\nRichard Feynman, the renowned physicist, argued that many scientific discoveries begin with pattern matching—a skill experts develop to identify structures and regularities in their domain. He frequently engaged in discussions about artificial intelligence, often using chess as an analogy to illustrate the difference between human intuition and machine calculation.\nFeynman observed that while novices calculate moves, masters recognize patterns. They understand the “laws” of the board much like physicists understand the laws of nature. In an interview, he described learning the rules of chess simply by observing games: first noting that bishops maintain their color, then realizing they move diagonally. This process of uncovering rules from observations is the essence of scientific discovery.\nRegarding machine intelligence, Feynman was pragmatic. He noted that machines need not “think” like humans to achieve similar or superior results. Just as airplanes fly without flapping wings like birds, computers can play chess (or solve other problems) using different underlying mechanisms—such as massive calculation or statistical optimization—yet achieve superhuman performance.\n\n“If we would like to make something that runs rapidly over the ground… we could try to make a machine that runs like a cheetah. But, it’s easier to make a machine with wheels… later machines are not going to think like people think.” — Richard Feynman\n\nThis distinction is crucial in modern AI. Today’s systems, like AlphaZero, combine pattern matching (via neural networks) with search (Monte Carlo simulation), effectively “learning” chess principles from scratch. They don’t rely on human heuristics (like “control the center”) but discover their own statistical patterns that maximize the probability of winning.\nDiscussions with professional pianist Beatrice Rana revealed an interesting parallel. She compared modern AI’s ability to produce incredible results to a pianist capable of remembering and reproducing complex pieces of music—calling it “intelligenza artigiana”, or the intelligence of hands: a mastery of execution and pattern without necessarily possessing human-like consciousness.\nHow do we translate this conceptual “pattern matching” into concrete algorithms? In data science, we formalize this process by defining a mathematical structure (a model) and using data to adjust its parameters. Whether we are predicting election outcomes or classifying images, the core task remains the same: finding a function that maps inputs to outputs in a way that generalizes to new, unseen data.\nWe will now move from the intuitive understanding of pattern matching to its formalization in predictive modeling.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#supervised-learning",
    "href": "11-pattern.html#supervised-learning",
    "title": "11  Pattern Matching",
    "section": "11.2 Supervised Learning",
    "text": "11.2 Supervised Learning\nPrediction and forecasting are central challenges in data analysis, predominantly solved using pattern matching approaches. In business and engineering, the main motivation is to make better decisions; in science, to test and validate theories. By uncovering trends and patterns in historical data, analysts can make informed decisions about the future, identify potential risks and opportunities, and develop proactive strategies to capitalize on them. While unsupervised learning (clustering, dimensionality reduction) finds patterns without labeled data, this chapter focuses on supervised learning, where we have a target variable we wish to predict.\nThe problem of supervised learning is to learn patterns from observed data to make predictions on new, unseen data. The key idea is that we have input-output pairs \\((x_i, y_i)\\) where we know the correct output \\(y_i\\) for each input \\(x_i\\), and we use these examples to learn a function that maps inputs to outputs.\nSupervised learning powers a surprising range of modern systems. Manufacturing plants use sensor data to predict equipment failures before they happen. Autonomous vehicles rely on it for object detection, lane recognition, and real-time decision-making from camera and LiDAR feeds. Medical imaging systems detect diseases from X-rays and MRIs with accuracy rivaling radiologists. Banks use it for fraud detection and credit scoring; retailers for demand forecasting and inventory optimization; marketing teams for churn prediction and lead scoring.\nThe common thread: you have historical data with known outcomes, and you want to predict outcomes for new cases. The rest of this chapter develops the mathematical framework for doing this well.\nA typical prediction problem involves building a rule that maps observed inputs \\(x\\) into the output \\(y\\). The inputs \\(x\\) are often called predictors, features, or independent variables, while the output \\(y\\) is often called the response or dependent variable. The goal is to find a predictive rule \\[\ny = f(x).\n\\]\nThe map \\(f\\) can be viewed as a black box which describes how to find the output \\(y\\) from the input \\(x\\). One of the key requirements of \\(f\\) is that we should be able to efficiently find this function using an algorithm. In the simple case \\(y\\) and \\(x\\) are both univariate (scalars) and we can view the map as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Black box model\n\n\n\nThe goal of machine learning is to reconstruct this map from observed data. In a multivariate setting \\(x = (x_1,\\ldots,x_p)\\) is a list of \\(p\\) variables. This leads to a model of the form \\(y = f(x_1,\\ldots,x_p)\\). There are a number of possible goals of analysis, such as estimation, inference or prediction. The main one being prediction.\nThe prediction task is to calculate a response that corresponds to a new feature input variable. An example of inference is the task of establishing causation, with the goal of extracting information about the nature of the black box association of the response variable to the input variables.\nIn either case, the goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in its nature. To uncover the pattern we use a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\]\nwhere \\(x_i\\) is a set of \\(p\\) predictors and \\(y_i\\) is response variable. Prediction problem is to use a training dataset \\(D\\) to design a rule that can be used for predicting output values \\(y\\) for new observations \\(x\\).\nLet \\(f(x)\\) be predictor of \\(y\\), we will use notation \\[\n\\hat{y} = f(x).\n\\]\nTo summarize, we will use the following notation.\n\n\n\n\\(y\\)\noutput variable (response/outcome)\n\n\n\\(x\\)\ninput variable (predictor/covariate/feature)\n\n\n\\(f(x)\\)\npredictive rule\n\n\n\\(\\hat y\\)\npredicted output value\n\n\n\nWe distinguish several types of input or output variables. First, binary variables that can only have two possible values, e.g. yes/no, left/right, 0/1, up/down, etc. A generalization of binary variable is a categorical variable that can take a fixed number of possible values, for example, marriage status. Additionally, some of the categorical variable can have a natural order to them, for example education level or salary range. Those variables are called ordinal. Lastly, the most common type of a variable is quantitative which is described by a real number.\nDepending on the type of the output variable, there are three types of prediction problems.\n\nTypes of output variables and corresponding prediction problems.\n\n\n\n\n\n\n\nOutput Variable Type\nDescription\nPrediction Problem\n\n\n\n\nBinary\n\\(y\\in \\{0,1\\}\\)\nClassification\n\n\nCategorical\n\\(y\\in \\{0,\\ldots,K\\}\\) for \\(K\\) possible categories\nClassification\n\n\nQuantitative\n\\(y \\in \\mathbb{R}\\) (any real number)\nRegression\n\n\nOrdinal\n\\(y\\) has natural ordering\nRanking\n\n\n\nHere are some examples of prediction problems:\nBinary Classification: Predicting whether an email is spam or not spam involves input variables such as email content, sender information, presence of certain keywords, and email length. The output variable is \\(y \\in \\{0,1\\}\\) where 0 = not spam, 1 = spam. The goal is to classify new emails as spam or legitimate.\nCategorical Classification: Predicting the type of social media content based on text and image features uses input variables including text content, image features, user engagement metrics, posting time, and hashtags. The output variable is \\(y \\in \\{0,1,2,3,4\\}\\) where 0 = news, 1 = entertainment, 2 = educational, 3 = promotional, 4 = personal. The goal is to automatically categorize social media posts for content moderation and recommendation systems.\nRegression (Quantitative): Predicting house prices based on features uses input variables such as square footage, number of bedrooms, location, age of house, and lot size. The output variable is \\(y \\in \\mathbb{R}\\) (house price in dollars). The goal is to predict the selling price of a new house.\nRanking (Ordinal): Predicting customer satisfaction ratings involves input variables including product quality, customer service experience, delivery time, and price. The output variable is \\(y \\in \\{1,2,3,4,5\\}\\) where 1 = very dissatisfied, 5 = very satisfied. The goal is to predict customer satisfaction level for new customers.\nThere are several simple predictive rules we can use to predict the output variable \\(y\\). For example, in the case of regression problem, the simplest rule is to predict the average value of the output variable. This rule is called the mean rule and is defined as \\[\n\\hat f(x) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nNotice, this model does not depend on the input variable \\(x\\) and will predict the same value for all observations. This rule is simple and easy to implement, but it is not very accurate. In case of binary \\(y\\), we can apply thresholding to the mean rule to obtain a binary classifier. \\[\nf(x) = \\begin{cases}\n1 & \\text{if } \\bar{y} &gt; 0.5, \\\\\n0 & \\text{if } \\bar{y} \\leq 0.5.\n\\end{cases}\n\\]\nA more sophisticated rule is the nearest neighbor rule. This rule predicts the output value \\(y\\) for a new observation \\(x\\) by finding the closest observation in the training dataset and using its output value. The nearest neighbor rule is defined as \\[\nf(x) = y_{i^*},\n\\] where \\[i^* = \\arg\\min_{i=1,\\ldots,n} \\|x_i - x\\|\\] is the index of the closest observation in the training dataset. These two models represent two extreme cases of predictive rules: the mean rule is “stubborn” (it always predicts the same value) and the nearest neighbor rule is “flexible” (can be very sensitive to small changes in the inputs). Using the language of statistics the mean rule is of high bias and low variance, while the nearest neighbor rule is of low bias and high variance. Although those two rules are simple, they sometimes lead to useful models that can be used in practice. Further, those two models represent a trade-off between accuracy and complexity (the bias-variance trade-off). We will discuss this trade-off in more detail in the later section.\nThe mean model and nearest neighbor model belong to a class of so-called non-parametric models. The non-parametric models do not make explicit assumption about the form of the function \\(f(x)\\). In contrast, parametric models assume that the predictive rule \\(f(x)\\) is a specific function defined by vector of parameters, which we will denote as \\(\\theta\\). A typical notation is then \\[\nf_{\\theta}(x).\n\\]\nTraditional modeling culture employs statistical models characterized by single-layer transformations (transforming inputs directly into outputs without intermediate hidden layers), where the relationship between input variables and output is modeled through direct, interpretable mathematical formulations. These approaches typically involve linear combinations, additive structures, or simple nonlinear transformations that maintain analytical tractability and statistical interpretability. The list of widely used models includes:\n\n\n\nTable 11.1: Traditional statistical models.\n\n\n\n\n\n\n\n\n\n\nModel\nFormula\nParameters / Hyperparameters\n\n\n\n\nLinear Regression\n\\(y = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p\\)\n\\(\\theta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\)\n\n\nGeneralized Linear Model (GLM)\n\\(y = f^{-1}(\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p)\\)\n\\(\\theta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)\\)\n\n\nGeneralized Additive Model (GAM)\n\\(y = \\beta_0 + f_1(x_1) + \\ldots + f_k(x_k)\\)\n\\(\\theta = (\\beta_0, f_1, \\ldots, f_k)\\)\n\n\nPrincipal Component Regression (PCR)\n\\(y = \\beta^T (W x),\\quad W \\in \\mathbb{R}^{k \\times p},\\ k &lt; p\\)\n\\(\\theta = (\\beta, W)\\)\n\n\nk-Nearest Neighbors (KNN)\n\\(y = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\\)\n\\(k\\) (neighbors count)\n\n\n\n\n\n\nWe wish to find map \\(f\\) such that \\[\\begin{align*}\ny &= f ( x ) \\\\\ny &=  f ( x_1 , \\ldots , x _p )\n\\end{align*}\\]\nEssentially, the goal is to perform the pattern matching, also known as nonparametric regression. It involves finding complex relationships in data without assuming a specific functional form.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#complex-functions",
    "href": "11-pattern.html#complex-functions",
    "title": "11  Pattern Matching",
    "section": "11.3 Complex Functions",
    "text": "11.3 Complex Functions\nIn contrast to single-layer approaches, Deep Learning employs sophisticated high-dimensional multi-layer neural network architectures that can capture complex, non-linear relationships in data through hierarchical feature learning. In deep learning, we use composite functions rather than additive functions. We write the superposition of univariate functions as \\[\nf = f_1 \\circ \\ldots \\circ f_L   \\; \\; \\text{versus}  \\; \\; f_1 +  \\ldots + f_L\n\\] where composition \\(f = f_L(f_{L-1}(\\ldots f_1(x)))\\) creates a “deep” hierarchical structure, as opposed to the “flat” additive structure of models like GAMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Composition vs Addition of Functions\n\n\n\nEach function \\(f_i\\) in the composition is typically a combination of a linear transformation and a non-linear activation function \\[\nf_i(x) = \\sigma(W_i x + b_i),\n\\] The set of parameters that we need to find is \\(\\theta = (W_1, b_1, \\ldots, W_L, b_L)\\). The depth and complexity of these architectures allow deep learning models to automatically discover intricate patterns—such as edges in images or grammar in text—from raw input data.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#model-estimation",
    "href": "11-pattern.html#model-estimation",
    "title": "11  Pattern Matching",
    "section": "11.4 Model Estimation",
    "text": "11.4 Model Estimation\nThere are two main approaches to finding the set of parameters \\(\\theta\\). The first is optimization approach that minimizes a loss function. Loss function measures how well predictive rule \\(f\\) captures the relationship between input and output variables. The most common loss function is the mean squared error (MSE). The second approach is to use full Bayesian inference and to calculate the distribution over parameter \\(\\theta\\) given the observed data.\nFrom the perspective of representation, feature engineering can be viewed as the search for low-dimensional summaries of \\(x\\) that retain the information needed for prediction: an informal echo of the role of sufficient statistics in classical inference (Chapter 3).\nBoth approaches start with formulating likelihood function. Likelihood is a function that tells us how probable the observed data is, given a particular value of the parameter in a statistical model. It is not the same as probability; instead, it’s a function of the parameter, with the data fixed. Suppose you flip a biased coin 10 times and get 7 heads. You want to estimate the probability of getting heads on a single toss. You try different values of \\(\\theta\\) and ask: “How likely is it to get exactly 7 heads out of 10 flips if the true probability is \\(\\theta\\)?” This leads to the likelihood function. Formally, given \\(y_i \\sim f(y_i\\mid x_i,  \\theta)\\) as exchangeable (often simplified to i.i.d.) samples from a distribution with parameter \\(\\theta\\), the likelihood function is defined as \\[\nL(\\theta) = \\prod_{i=1}^n p(y_i\\mid x_i,  \\theta).\n\\] It treats the data \\(D = (y_i,x_i)_{i=1}^n\\) as fixed and varies \\(\\theta\\).\nLikelihood connects our model to the data generating process by quantifying how likely it is to observe the actual data we have under different parameter values. For example, if we assume our data follows a normal distribution \\(y \\sim N(f_\\theta(x), \\sigma^2)\\) with mean \\(f_\\theta(x)\\) and variance \\(\\sigma^2\\), the likelihood function would be:\n\\[\nL(\\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - f_\\theta(x_i))^2}{2\\sigma^2}\\right).\n\\tag{11.1}\\]\nFor the case of classification problem, we assume that \\(y_i\\) follows a Bernoulli distribution \\(y_i \\sim \\text{Bernoulli}(p_i)\\). The likelihood function is defined as \\[\nL(\\theta) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}.\n\\] Here \\(p_i\\) is the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate \\(p_i\\) is to use logistic function \\(\\sigma(\\cdot)\\) \\[\\begin{align*}\nf_{\\beta}(x_i) = & \\beta^Tx_i\\\\\np_i  = & \\sigma(f_{\\beta}(x_i)) =  \\frac{e^{f_{\\beta}(x_i)}}{1+e^{f_{\\beta}(x_i)}},\n\\end{align*}\\] Notice, that logistic function \\(\\sigma(\\cdot)\\) is restricted to output values in \\((0,1)\\).\nThe optimization-based approach is to find the set of parameters \\(\\theta\\) that maximizes the likelihood function. \\[\n\\theta^* = \\arg\\max_{\\theta} L(\\theta).\n\\]\nAlthough most often, it is easier to optimize the log-likelihood function. We define the log-likelihood by \\[\n\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^n \\log p(y_i\\mid x_i,  \\theta).\n\\] Notice that the log-likelihood is a sum of per-observation contributions, which is convenient for both analysis and computation. In many estimation problems we will instead minimize the negative log-likelihood (which plays the role of a loss), \\[\nl(\\theta) = -\\ell(\\theta),\n\\] which is called the cross-entropy loss function.\nWhy does the solution not change? Since the logarithm is a monotonically increasing function, if \\(L(\\theta_1) &gt; L(\\theta_2)\\), then \\(\\log L(\\theta_1) &gt; \\log L(\\theta_2)\\). This means that the parameter value that maximizes the likelihood function will also maximize the log-likelihood function. The maximum point stays the same, just the function values are transformed.\nThe value of parameters \\(\\theta\\) that maximizes the log-likelihood is called the maximum likelihood estimate (MLE).\nNow, rather than maximizing the log-likelihood function, we minimize the negative log-likelihood function \\[\n\\theta^* = \\arg\\min_{\\theta} l(\\theta).\n\\] This problem is called the least squares problem.\nThen the negative log-likelihood function is called the loss function. Thus the problem of finding maximum likelihood estimate is equivalent to minimizing the loss function.\nLet’s calculate the loss function that corresponds to the normal likelihood function given by Equation 11.1. Using the fact that the logarithm of a product is a sum of logarithms, we can write the negative log-likelihood as \\[\nl(\\theta) = -\\sum_{i=1}^n \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - f_\\theta(x_i))^2}{2\\sigma^2}\\right).\n\\] Inside the sum, we have a product of two terms. The first term is a constant with respect to \\(\\theta\\) and the second term is a function of \\(\\theta\\). We can rewrite the likelihood function as \\[\nl(\\theta) = -\\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\log \\exp\\left(-\\frac{(y_i - f_\\theta(x_i))^2}{2\\sigma^2}\\right)\\right].\n\\]\nThe first term \\(\\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) is a constant with respect to \\(\\theta\\), so we can drop it from the optimization problem. The second term can be simplified using the fact that \\(\\log \\exp(x) = x\\):\n\\[\nl(\\theta) = \\sum_{i=1}^n \\left[\\frac{(y_i - f_\\theta(x_i))^2}{2\\sigma^2}\\right] + C,\n\\]\nwhere \\(C\\) is a constant that does not depend on \\(\\theta\\). Since we are minimizing \\(l(\\theta)\\), we can drop constant terms that do not depend on \\(\\theta\\):\n\\[\nl(\\theta) = \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - f_\\theta(x_i))^2.\n\\]\nThis is the mean squared error (MSE) loss function, which is the most commonly used loss function for regression problems. The factor \\(\\frac{1}{2\\sigma^2}\\) is often absorbed into the learning rate or regularization parameter in optimization algorithms. Thus, another name of the estimator is the least squares estimator. It is the same as the maximum likelihood estimator, assuming that the \\(f_\\theta(x_i)\\) is normally distributed.\n\nPenalized Likelihood\nWhile maximum likelihood estimation provides a principled approach to parameter estimation, we can often find better estimators using what is called a penalized likelihood. In fact, there are certain cases, when penalized estimator leads to universally better estimators. In statistics, we would say that MLE is inadmissible in dimensions of 3 or higher, meaning there exists another estimator (like the James-Stein estimator) that is strictly better in terms of expected squared error (risk) by “shrinking” estimates towards a central value. Later in Chapter 17 we will discuss the theory of penalized estimators in more detail.\nPenalized likelihood addresses overfitting by adding a regularization term to the likelihood function. Instead of maximizing just the likelihood, we maximize:\n\\[\nL_{\\text{penalized}}(\\theta) = L(\\theta) \\cdot \\exp(\\lambda \\phi(\\theta))\n\\]\nOr equivalently, we minimize the negative log-likelihood plus a penalty: \\[\nl(\\theta) =\\sum_{i=1}^n l(y_i, f_{\\theta} (x_i)) +\\lambda \\sum_{j=1}^p \\phi(\\theta_j),\n\\] where \\(\\lambda &gt; 0\\) is the regularization parameter that controls the strength of regularization, and \\(\\phi(\\theta)\\) is the penalty function that measures model complexity. In machine learning the technique of adding the penalty term to the loss function is called regularization.\nRegularization can be viewed as constraint on the model space. The techniques were originally applied to solve ill-posed problems where a slight change in the initial data could significantly alter the solution. Regularization techniques were then proposed for parameter reconstruction in a physical system modeled by a linear operator implied by a set of observations. It had long been believed that ill-conditioned problems offered little practical value, until Tikhonov published his seminal paper Andrey Nikolayevich Tikhonov et al. (1943) on regularization. Andrei N. Tikhonov (1963) proposed methods for solving regularized problems. In our notation, this corresponds to finding parameters \\(\\theta\\) that minimize \\[\n\\min_\\theta  ||y- X\\theta||^2_2   + \\lambda||(\\theta - \\theta^{(0)})||^q_q.\n\\] Here \\(\\lambda\\) is the weight on the regularization penalty and the \\(\\ell_q\\)-norm is defined by \\(||\\theta||_q = (\\sum_i |\\theta_i|^q)^{1/q}\\). This optimization problem is a Lagrangian form of the constrained problem given by \\[\n\\mbox{minimize}_{\\theta}\\quad||y- X\\theta||^2_2\\qquad\\mbox{subject to }\\sum_{i=1}^{p}\\phi(\\theta_i) \\le s.\n\\] with \\(\\phi(\\theta_i) = |\\theta_i - \\theta_i^{(0)}|^q\\).\nLater, sparsity became a primary driving force behind new regularization methods. The idea is that the vector of parameters \\(\\theta\\) is sparse, meaning that most of its elements are zero. This is a natural assumption for many models, such as the linear regression model. We will discuss the sparsity in more detail later in the book.\nThere are multiple optimization algorithms that can be used to find the solution to the penalized likelihood problem. Later in the book we will discuss the Stochastic Gradient Descent (SGD) algorithm, which is a popular tool for training deep learning models.\n\n\nBayesian Approach\nSimilar to the likelihood maximization approach, the Bayesian approach to model estimation starts with the likelihood function. The difference is that we assume that the parameters \\(\\theta\\) are random variables and follow some prior distribution. Then we use the Bayes rule to find the posterior distribution of the parameters, given the data \\(D = (y_i,x_i)_{i=1}^n\\): \\[\np(\\theta \\mid D,\\lambda) \\propto L(\\theta) p(\\theta\\mid\\lambda),\n\\] where \\(p(\\theta\\mid\\lambda)\\) is the prior distribution with hyperparameter \\(\\lambda\\) and \\(L(\\theta)\\) is the likelihood function. It is a distribution over the parameters, not a single value.\nPenalized likelihood has a natural Bayesian interpretation. The penalty term corresponds to a prior distribution on the parameters: \\[\np(\\theta) = \\dfrac{1}{Z(\\lambda)} \\exp(-\\lambda \\phi(\\theta))\n\\] Then the penalized likelihood is proportional to the posterior distribution: \\[\np(\\theta \\mid y) \\propto p(y | \\theta) \\cdot p(\\theta) = L(\\theta) ~ \\dfrac{1}{Z(\\lambda)} \\exp(-\\lambda \\phi(\\theta))\n\\]\nThis means maximizing the penalized likelihood is equivalent to finding the maximum a posteriori (MAP) estimate, which is the mode of the posterior distribution.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "11-pattern.html#prediction-accuracy",
    "href": "11-pattern.html#prediction-accuracy",
    "title": "11  Pattern Matching",
    "section": "11.5 Prediction Accuracy",
    "text": "11.5 Prediction Accuracy\nAfter we fit our model and find the optimal value of the parameter \\(\\theta\\), denoted by \\(\\hat \\theta\\), we need to evaluate the accuracy of the predictive model. Once \\(\\hat{\\theta}\\) is obtained, it involves comparing the model’s predictions to actual outcomes. We can simply use the value of the loss function from the training step to evaluate model’s predictive power. However, this only tells us how well the model fits the training data. It doesn’t tell us how well the model will perform on unseen data. To evaluate the model’s performance on unseen data, we need to use a different approach.\nThe most common approach is to split the data into training and test sets. The training set is used to train the model, while the test set is used to evaluate its performance. This approach is known as the train-test split. It is a simple and effective way to evaluate how well model predicts for unseen inputs.\nAnother approach is to use cross-validation. It involves splitting the data into smaller subsets and using them to train and test the model multiple times. When our sample size is small, this allows for a more robust estimate of the model’s performance than simply splitting the data into a single training and test set. For small data sets, simple train-test split approach will be sensitive to choice of test samples, thus the estimated predicted performance will be unstable (high variance). Cross-validation helps to reduce this variance by averaging the performance across multiple folds. This makes the performance estimate more robust and less sensitive to the choice of test samples.\nCross-validation involves several steps. The data is randomly divided into \\(k\\) equal-sized chunks (folds). For each fold, the model is trained on \\(k-1\\) folds and tested on the remaining fold. This process is repeated \\(k\\) times, ensuring each fold is used for testing once. The performance of the model is evaluated on each fold using a chosen metric, such as accuracy, precision, recall, or F1 score. The average of the performance metrics across all k folds is reported as the final estimate of the model’s performance.\nA common choice for \\(k\\) is 5 or 10. When \\(k=n\\) (where \\(n\\) is the sample size), this is known as leave-one-out cross-validation. This method can be computationally expensive but is less likely to overfit the data. Stratified cross-validation ensures that each fold contains approximately the same proportion of each class as in the entire dataset. This is important for imbalanced datasets where one class is significantly larger than the others.\nNotice, that cross-validation requires re-training the model multiple times, which can be computationally expensive. Thus, for large datasets, we typically prefer simple train-test split. However, for small datasets, cross-validation can provide a more robust estimate of the model’s performance.\nEither method is limited to evaluating the model’s performance on data that is available to the modeler. What if we start using our model on data that is different from the training and test sets? Unlike physics, where models often represent universal laws, data science deals with data generated by processes that may vary across contexts. For example, if we are building a model to predict the price of a house, we can train and test the model on data from a specific city. However, if we start using the model to predict the price of a house in a different city, the model might not perform as well. This is because the data from the new city might be different from the data used to train and test the model. This is known as the problem of generalization. It refers to the ability of a model to perform well on data that is different from the training and test sets.\n\nEvaluation Metrics for Regression\nThere are several metrics that can be used to evaluate the performance of regression models. We can simply use the same function as we use for fitting the model, e.g. least squares \\[\n\\text{MSE} = \\dfrac{1}{m}\\sum_{i=1}^n (y_i -\\hat y_i)^2,\n\\] here \\(\\hat y_i\\) is the predicted value of the i-th data point by the model \\(\\hat y_i = f(x_i,\\hat\\theta)\\) and \\(m\\) is the total number of data points used for the evaluation. This metric is called the Mean Squared Error (MSE). It is the average squared difference between the actual and predicted values. Lower MSE indicates better model performance, as it means the model’s predictions are closer to the actual values.\nA slight variation of this metric is Root Mean Squared Error (RMSE). This is the square root of MSE and is also commonly used due to its units being the same as the target variable. \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}.\n\\] However, MSE is sensitive to outliers, as it squares the errors, giving more weight to large errors. This can lead to misleading results when the data contains outliers.\nMean Absolute Error (MAE) solves the sensitivity to the outliers problem. It is the mean of the absolute errors, providing a more robust measure than MSE for skewed error distributions \\[\n\\text{MAE} = \\dfrac{1}{m}\\sum_{i=1}^n |y_i -\\hat y_i|.\n\\] A variation of it is the Mean Absolute Percentage Error (MAPE), which is the mean of the absolute percentage errors \\[\n\\text{MAPE} = \\dfrac{1}{m}\\sum_{i=1}^n \\left | \\dfrac{y_i -\\hat y_i}{y_i} \\right |.\n\\]\nAlternative way to measure the predictive quality is to use the coefficient of determination, also known as the R-squared value, which measures the proportion of variance in the target variable that is explained by the model. Higher R-squared indicates better fit. However, R-squared can be misleading when comparing models with different numbers of features. R-squared is defined as follows \\[\nR^2 = 1 - \\dfrac{\\sum_{i=1}^n (y_i -\\hat y_i)^2}{\\sum_{i=1}^n (y_i -\\bar y_i)^2},\n\\] where \\(\\bar y_i\\) is the mean of the target variable. R-squared is a relative measure of fit, so it can be used to compare different models. However, it is not an absolute measure of fit, so it cannot be used to determine whether a model is good or bad. It is also sensitive to the number of features in the model, so it cannot be used to compare models with different numbers of features.\nFinally, we can use graphics to evaluate the model’s performance. For example, we can create a scatterplot of the actual and predicted values of the target variable to visually compare them. We can also plot the histogram or a boxplot of the residuals (errors) to see if they are normally distributed.\n\n\nEvaluation Metrics for Classification\nAccuracy is the most fundamental metric used to evaluate models. It is defined as the ratio of the number of correct predictions to the total number of predictions. The formula is given by \\[\\text{Accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}},\\] where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, respectively. However, it can be misleading for imbalanced datasets where one class is significantly larger than others. For example, if 95% of the data belongs to one class, a model that always predicts this class will be 95% accurate, even though it’s not very useful.\nA more comprehensive understanding of model performance can be achieved by calculating the sensitivity (precision) and specificity (recall) as well as confusion matrix discussed in Section 2.6. The confusion matrix is\n\n\n\nActual/Predicted\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\nPrecision measures the proportion of positive predictions that are actually positive. It is useful for evaluating how good the model is at identifying true positives. Recall measures the proportion of actual positives that are correctly identified by the model. It is useful for evaluating how good the model is at not missing true positives.\nThen we can use those to calculate F1 Score which is a harmonic mean of precision and recall, providing a balanced view of both metrics. The formula is given by \\[\n\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}.\n\\] Higher F1 score indicates better overall performance. If misclassifying certain instances is more costly than others, weighted metrics account for these different costs. For imbalanced datasets, metrics like F1 score or balanced accuracy are important to avoid misleading interpretations.\nSometimes, we use multiple metrics to get a comprehensive assessment of the model’s performance. Additionally, consider comparing the model’s performance to a baseline model or other existing models for the same task. Sometimes, it is hard to beat a “coin flip” classification model, when the model predicts the class randomly with equal probability. In regression, a simple baseline model is \\(f(x_i) = \\bar y\\), which is the mean of the target variable.\n\n\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca, and Elena Bonanno. 2021. “Molecular Aspects and Prognostic Significance of Microcalcifications in Human Pathology: A Narrative Review.” International Journal of Molecular Sciences 22 (120).\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The Unreasonable Effectiveness of Data.” IEEE Intelligent Systems 24 (2): 8–12.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini, Federico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023. “History of Mammography: Analysis of Breast Imaging Diagnostic Achievements over the Last Century.” Healthcare 11 (1596).\n\n\nTikhonov, Andrei N. 1963. “Solution of Incorrectly Formulated Problems and the Regularization Method.” Sov Dok 4: 1035–38.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of Inverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Pattern Matching</span>"
    ]
  },
  {
    "objectID": "12-regression.html",
    "href": "12-regression.html",
    "title": "12  Linear Regression",
    "section": "",
    "text": "12.1 Statistical Properties of Linear Models\nThe simplest form of a parametric model is a linear model that assumes a linear relationship between the input variables and the output variable. There are several ways to specify such a family of functions, for example as linear combinations of inputs \\[\nf_{\\beta}(x)=\\beta_0+\\beta_1x_1 + \\ldots + \\beta_p x_p.\n\\] Technically, this is not a linear function. In mathematics, it is called an affine function, which is a linear function with an additional constant term (\\(\\beta_0\\)). However, if we add a dummy variable \\(x_0 = 1\\) to the input vector \\(x\\), we can rewrite the function as a linear function of the parameters \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) using vector notation \\[\nf_{\\beta}(x) = \\beta^Tx, ~ x = (1,x_1,\\ldots,x_p).\n\\]\nNote that instead of using a generic notation \\(\\theta\\) for the parameters, we use \\(\\beta\\) to emphasize that we are talking about the parameters of the linear model. The vector \\(\\beta\\) is called the regression coefficients.\nRegression analysis is the most widely used statistical tool for understanding relationships among variables. It provides a conceptually simple method for investigating functional relationships between one or more factors and an outcome of interest. This relationship is expressed in the form of an equation, which we call the model, connecting the response or dependent variable and one or more explanatory or predictor variables.\nA more general form of a linear model is a linear combination of basis functions \\[\nf_{\\beta}(x)= \\beta_0 + \\beta_1 \\psi_1(x) + \\ldots + \\beta_M \\psi_M(x) = \\beta^T \\psi(x),\n\\] where \\(\\psi_1,\\ldots,  \\psi_M\\) are the basis functions and \\(\\psi(x) = (1, \\psi_1(x),\\ldots,\\psi_M(x))\\).\nNotice in the latter case, the function \\(f\\) is linear in the parameters \\(\\beta = (\\beta_0,\\beta_1,\\ldots, \\beta_M)\\) but not in the input variables \\(x\\). The goal of the modeler is to choose an appropriate set of predictors and basis functions that will lead to a good reconstruction of the input-output relations. After we’ve specified what the function \\(f\\) is, we need to find the best possible values of parameters \\(\\beta\\).\nFinding a predictive rule \\(f_{\\beta}(x)\\) starts by defining the criterion of what is a good prediction. We assume that the function \\(f_{\\beta}(x)\\) is parameterized by a vector of parameters \\(\\beta\\) and we want to find the best value of \\(\\beta\\) that will give us the best prediction.\nWe will use a loss function that quantifies the difference between the predicted and actual values of the output variable. It is closely related to the loss function used in decision theory (thus the name). In decision theory, a loss function is a mathematical representation of the “cost” associated with making a particular decision in a given state of the world. It quantifies the negative consequences of choosing a specific action and helps guide decision-makers towards optimal choices. You can think of the loss function in predictive problems as a “cost” associated with making an inaccurate prediction given the values of the input variables \\(x\\).\nThe least squares loss function, discussed in Chapter 11, is the sum of squared differences between the predicted and actual values. Given observed data set \\(\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}\\), the least squares estimator is the value of \\(\\beta\\) that minimizes the sum of squared errors In most regression settings we treat the observations as exchangeable (often simplified to i.i.d.), meaning the order of the data is not informative (Chapter 3). \\[\n\\min_{\\beta}\\sum_{i=1}^n (y_i - \\hat y_i)^2, ~ \\hat y_i =  \\beta^T \\psi(x_i).\n\\]\nIn the unconditional case, when we do not observe any inputs \\(x\\), the least squares estimator is the sample mean. We can solve this minimization problem by taking the derivative of the loss function and setting it to zero \\[\n\\frac{d}{d\\beta}\\sum_{i=1}^n (y_i - \\beta_0)^2 = -2\\sum_{i=1}^n (y_i - \\beta_0) = 0\n\\] which gives us the solution \\(\\hat{\\beta}_0 = 1/n\\sum_{i=1}^n y_i\\), which is the sample mean.\nWe used lm function to fit the linear model for the housing data. This function uses least squares loss function to estimate the parameters of the line. One of the nice properties of the least squares estimator is that it has a closed-form solution. This means that we can find the values of the parameters that minimize the loss function by solving a system of linear equations rather than using an optimization algorithm. The linear system is obtained by taking the gradient (multivariate derivative) of the loss function with respect to the parameters and setting it to zero. The loss function \\[\n\\mathcal{L}(\\beta) = \\sum_{i=1}^n (y_i - f_{\\beta}(x_i))^2\n\\] is a quadratic function of the parameters, so the solution is unique and can be found analytically. The gradient of the loss function with respect to the parameters is \\[\n\\nabla \\mathcal{L}(\\beta) = -2\\sum_{i=1}^n (y_i - f_{\\beta}(x_i))\\nabla f_{\\beta}(x_i).\n\\] Given that \\(f_{\\beta}(x_i) = \\beta^T \\psi(x_i)\\), the gradient of the loss function with respect to the parameters is \\[\n\\nabla \\mathcal{L}(\\beta)  = -2\\sum_{i=1}^n (y_i - \\beta^T \\psi(x_i))\\psi(x_i)^T.\n\\] Setting the gradient to zero gives us the normal equations \\[\n\\sum_{i=1}^n y_i \\psi(x_i)^T = \\sum_{i=1}^n \\beta^T \\psi(x_i) \\psi(x_i)^T.\n\\] In matrix form, the normal equations are \\[\n\\Psi^Ty = \\Psi^T\\Psi\\beta\n\\tag{12.1}\\] where \\(\\Psi\\) is the design matrix with rows \\(\\psi(x_i)^T\\) and \\(y\\) is the vector of output values. The solution to the normal equations is \\[\n\\hat{\\beta} = (\\Psi^T\\Psi)^{-1}\\Psi^Ty.\n\\]\nThe function solve solves the Equation 12.1, and indeed finds the same values as the lm function. Essentially this is what the lm function does under the hood. The solve function uses the elimination method to solve the system of linear equations. The method we all learned when we were introduced to linear equations. The technique is known in linear algebra as LU decomposition.\nIn our housing example we used a linear model to predict the price of a house based on its square footage. The model is simple and easy to interpret, making it suitable for both prediction and interpretation. The model provides insights into the relationship between house size and price, allowing us to understand how changes in house size affect the price. The model can also be used to make accurate predictions of house prices based on square footage. This demonstrates the versatility of linear models for both prediction and interpretation tasks.\nTo further demonstrate the versatility of linear models, let’s consider an example that allows us to understand the relationship between the performance of a stock portfolio managed by John Maynard Keynes and overall market performance.\nPreviously we demonstrated how the Central Limit Theorem enables us to derive key statistical properties of the sample mean, including its asymptotic normality, unbiasedness, and the relationship between sample size and estimation precision. These same principles extend naturally to linear models, where we can derive analogous properties for the least squares estimators of regression coefficients.\nFirst, we introduce a regression model using the language of probability. A regression model assumes that the mean of the output variable \\(y\\) depends linearly on predictors \\(x_1,\\ldots,x_p\\) \\[\ny = \\beta_0 +  \\beta_1 x + \\ldots + \\beta_p x_p + \\epsilon,~ \\text{where}~\\epsilon \\sim N(0, \\sigma^2).\n\\] Often, we use simpler dot-product notation \\[\ny = \\beta^Tx + \\epsilon,\n\\] where \\(\\beta = (\\beta_0,\\beta_1,\\ldots,\\beta_p)\\) is the vector of regression coefficients and \\(x = (1,x_1,\\ldots,x_p)\\) is the vector of predictors, with 1 appended to the beginning.\nA more convenient form of the model is as follows \\[\ny \\mid \\beta,x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2).\n\\]\nThe additional term \\(\\epsilon\\) is a random variable that captures the uncertainty in the relationship between \\(y\\) and \\(x\\); it is called the error term or the residual. The error term is assumed to be normally distributed with mean zero and variance \\(\\sigma^2\\). Thus, the linear regression model has a new parameter \\(\\sigma^2\\) that models dispersion of \\(y_i\\) around the mean \\(\\beta^Tx\\), let’s see an example.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#statistical-properties-of-linear-models",
    "href": "12-regression.html#statistical-properties-of-linear-models",
    "title": "12  Linear Regression",
    "section": "",
    "text": "Estimates and Intervals\nIn our housing example, we estimated the parameter \\(\\beta_1\\) to be equal to 113.12 and made a conclusion that the price of the house goes up by that amount when the living area goes up by one unit. However, the estimated value is based on a sample. The sample is a result of well-designed data collection procedure and is representative of the population, and we should expect the estimated value to be close to the true value. However, the estimated value is not the true value of the parameter, but an estimate of it. The true value of the parameter is unknown and the estimated value is subject to sampling error.\nThe sampling error is modeled by the normal distribution. The standard error of the estimate is a measure of the uncertainty in the estimated value of the parameter. The standard error is calculated from the sample data and is used to calculate confidence intervals and p-values for the estimated parameter.\nWe used the lm function to estimate the parameters of the linear model. The estimated values of the parameters are given in the Estimate column of the output. The estimated value of the intercept is \\(\\hat \\beta_0 = 13.439394\\) and the estimated value of the slope is \\(\\hat \\beta_1 = 113.1225418\\). These values are calculated using the least squares loss function, which minimizes the sum of squared differences between the predicted and actual values of the output variable. The estimated values of the parameters are subject to sampling error, which is modeled by the normal distribution. The standard error of the estimates is given in the Std. Error column of the output. The standard error is a measure of the uncertainty in the estimated values of the parameters. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error is the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nExample 12.3 (House Prices) Let’s go back to the Saratoga Houses dataset\n\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nl = lm(price ~ livingArea, data=d)\nl %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n13\n5.0\n2.7\n0.01\n\n\nlivingArea\n113\n2.7\n42.2\n0.00\n\n\n\n\n\nThe output of the lm function has several components. Besides calculating the estimated values of the coefficients, given in the Estimate column, the method also calculates standard error (Std. Error) and t-statistic (t value) for each coefficient. The t-statistic is the ratio of the estimated coefficient to its standard error. The p-value (Pr(&gt;|t|)) is the probability of observing a value at least as extreme as the one observed, assuming the null hypothesis is true. The null hypothesis is that the coefficient is equal to zero. If the p-value is less than 0.05, we typically reject the null hypothesis and conclude that the coefficient is statistically significant. In this case, the p-value for the livingArea coefficient is less than 0.05, so we conclude that the coefficient is statistically significant. This means that the size of the house is a statistically significant predictor of the price. The Residual standard error is the standard deviation of the residuals \\(\\hat y_i - y_i,~i=1,\\ldots,n\\).\n\nThe estimated values of the parameters were calculated using least squares loss function discussed above. The residual standard error is also relatively easy to calculate from the model residuals \\[\ns_e = \\sqrt{ \\frac{1}{n-2} \\sum_{i=1}^n ( \\hat y_i - y_i )^2 }.\n\\] Now the question is, how was the p-value for the estimates calculated? And why did we assume that \\(\\epsilon\\) is normally distributed in the first place? The normality of \\(\\epsilon\\) and as a consequence, the conditional normality of \\(y \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2)\\) is easy to explain; it is simply due to mathematical convenience. Plus, this assumption happens to describe the reality well in a wide range of applications. One of those conveniences is our ability to calculate the mean and variance of the distribution of \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\).\nTo understand how to calculate the p-values, we first notice that there is uncertainty about the values of the parameters \\(\\beta_i\\)s. To get a feeling for the amount of variability in our experiments, imagine that we have two sample data sets. For example, we have housing data from two different realtor firms. Do you think that the estimated value of price per square foot will be the same for both of those? The answer is no. Let’s demonstrate with an example; we simulate 20 data sets from the same distribution and estimate 20 different linear models.\n\nset.seed(92) #Kuzy\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\nplot(d$livingArea, d$price, col=\"lightblue\", xlab=\"Living Area\",\nylab=\"Price\", bg=\"lightblue\", pch=21, cex=0.6)\nfor (i in 1:10) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew, subset = sample(1:nrow(d),100))\n  abline(l, col=\"green\", lwd=3)\n}\n\n\n\n\n\n\n\nFigure 12.2: Twenty different linear models estimated using randomly selected subsample of the data.\n\n\n\n\n\nFigure 12.2 shows the results of this simulation. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are different for each of the 20 samples. This is due to the sampling error. The sampling error is the difference between the estimated value of a parameter and its true value. The value of \\(\\beta_1\\) will differ from sample to sample. In other words, it will be a random variable. The sampling distribution of \\(\\beta_1\\) describes how it varies over different samples with the \\(x\\) values fixed. Statistical view of linear regression allows us to calculate confidence and prediction intervals for estimated parameters. It turns out that when least squares principle is used, the estimated \\(\\hat\\beta_1\\) is normally distributed: \\(\\hat\\beta_1\\mid \\beta_1 \\sim N( \\beta_1 , s_1^2 )\\). Let’s see how we can derive this result.\nThe extension of the central limit theorem, sometimes called the Lindeberg CLT, states that a linear combination of independent random variables that satisfy some mild condition are approximately normally distributed. We can show that estimates of \\((\\beta_0\\ldots,\\beta_p)\\) are linear combinations of the observed values of \\(y\\) and are therefore normally distributed. Indeed, if we write the linear regression model in matrix form \\[\ny = X \\beta + \\epsilon,\n\\] where \\(Y\\) is the vector of observed values of the dependent variable, \\(X\\) is the matrix of observed values of the independent variables, \\(\\beta\\) is the vector of unknown parameters, and \\(\\epsilon\\) is the vector of errors. Then, if we take the derivative of the loss function for linear regression and set it to zero, we get the following expression for the estimated parameters \\[\n\\hat \\beta =  Ay, \\hat y = X \\hat \\beta = H y.\n\\] where \\(A = (X^TX)^{-1}X^T\\), \\(H = XA = X(X^TX)^{-1}X^T\\) is the hat matrix. Due to Lindeberg central limit theorem, \\(\\hat \\beta\\) is normally distributed. This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nNow, we need to compute the mean and variance of \\(\\hat \\beta\\). The mean is easy to compute, since the expectation of the sum is the sum of expectations, we have \\(\\hat{\\beta} = A(X\\beta + \\epsilon)\\), hence \\[\n\\hat{\\beta} = \\beta + A\\epsilon\n\\]\nThe expectation and variance of \\(\\hat{\\beta}\\) are then: \\[\\begin{align*}\n\\E{\\hat{\\beta}} = & \\E{\\beta + A\\epsilon} = \\E{\\beta} + \\E{A\\epsilon} = \\beta\\\\\n\\Var{\\hat{\\beta}} = & \\Var{A\\epsilon} = A\\Var{\\epsilon}A^T = \\sigma^2 A(X^TX)^{-1}A^T = \\sigma^2 (X^TX)^{-1}\n\\end{align*}\\] Putting together the expectation and variance, we get the following distribution for \\(\\hat{\\beta}\\): \\[\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X^TX)^{-1}).\n\\]\nWe can think of the estimated coefficients \\(\\hat \\beta_i\\) as an average amount of change in \\(y\\), when \\(x_i\\) goes up by one unit. Since this average was calculated using a sample data, it is subject to sampling error and the sampling error is modeled by the normal distribution. Assuming that residuals \\(\\epsilon\\) are independently normally distributed with a variance that does not depend on \\(x\\) (homoscedasticity), we can calculate the mean and variance of the distribution of \\(\\hat \\beta_i\\). This is a useful property that allows us to calculate confidence intervals and p-values for the estimated parameters.\nIn summary, the statistical view of the linear regression model is useful for understanding the uncertainty associated with the estimated parameters. It also allows us to calculate confidence intervals and prediction intervals for the output variable.\n\nAverage value of output \\(y\\) is a linear function of input \\(x\\) and lie on the straight line of regression \\(\\hat y_i = \\beta^Tx_i\\).\nThe values of \\(y\\) are statistically independent.\nThe true value of \\(y = \\hat y_i + \\epsilon_i\\) is a random variable, and it is normally distributed around the mean with variance \\(\\sigma^2\\). This variance is the same for all values of \\(y\\).\nThe estimated values of the parameters \\(\\hat \\beta_i\\) are calculated from observed data and are subject to the sampling error and we are not certain about them. This uncertainty is modeled by the normally distributed around the true values \\(\\beta\\). Given that errors \\(\\epsilon_i\\) are homoscedastic and independent, we have \\(Var(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\\).\n\nAgain, consider a house example. Say in our data we have 10 houses with the same squire footage, say 2000. Now the third point states, that the prices of those houses should follow a normal distribution and if we are to compare prices of 2000 sqft houses and 2500 sqft houses, they will have the same standard deviation. The second point means that price of one house does not depend on the price of another house.\nAll of the assumptions in the regression model can be written using probabilist notations: \\[\ny \\mid x \\overset{iid}{\\sim}  N(\\beta^Tx, \\sigma^2).\n\\]\nIn the case when we have only one predictor the variance of the estimated slope \\(\\hat \\beta_1\\) is given by \\[\n\\Var{\\hat \\beta_1} = \\frac{\\sigma^2}{\\sum_{i=1}^n ( x_i - \\bar{x} )^2 } = \\frac{ \\sigma^2 }{ (n-1) s_x^2 },\n\\] where \\(s_x^2\\) is the sample variance of \\(x\\). Thus, there are three factors that impact the size of standard error for \\(\\beta_1\\): sample size (\\(n\\)), error variance (\\(\\sigma^2\\)), and \\(x\\)-spread, \\(s_x\\).\nWe can empirically demonstrate the sampling error by simulating several samples from the same distribution and estimating several linear models. We can see that the estimated coefficients \\(\\hat \\beta_i\\) are normally distributed around the true values \\(\\beta_i\\). If we plot coefficients for 1000 different models, we can see that the empirical distribution resembles a normal distribution.\n\nset.seed(92) #Kuzy\n# Read housing data\nd = read.csv(\"../data/SaratogaHouses.csv\")\nd$price = d$price/1000; d$livingArea = d$livingArea/1000\n# Simulate 1000 samples\nn = 1000\n# Create a matrix to store the results\nbeta = matrix(0, nrow=n, ncol=2)\n# Simulate 1000 samples\nfor (i in 1:n) {\n  # Sample with replacement\n  dnew = d[sample(1:nrow(d),replace=T),]\n  # Fit a linear model\n  l = lm(price ~ livingArea, data=dnew)\n  # Store the coefficients\n  beta[i,] = coef(l)\n}\nind = 2\n# Plot the results\nplot(beta[,1], beta[,2], col=\"blue\", xlab=\"Intercept\", ylab=\"Slope\", bg=\"lightblue\", cex=0.8)\nabline(h=coef(l)[2], lwd=3, col=\"red\")\nabline(v=coef(l)[1], lwd=3, col=\"red\")\n\n\n\n\n\n\n\nX = cbind(1, d$livingArea)\n# Calculate the variance of the coefficients\nvar = sigma(l)^2 * solve(t(X) %*% X)\nvarb = var[ind,ind]/0.63\nhist(beta[,ind], col=\"lightblue\", xlab=\"Intercept\", main=\"\",freq=F)\n\n\n\n\n\n\n\nbt = seq(80,140,0.1)\n\nAccounting for uncertainty in \\(\\hat \\beta\\)s we can calculate confidence intervals for the predicted average \\(\\hat y\\). When we additionally account for the uncertainty in the predicted value \\(\\hat y\\), we can calculate prediction intervals.\nAnother advantage of adopting a statistical view of the linear regression model is ability to quantify information about potential outliers. Outliers are points that are extreme relative to our model predictions. Recall, that the residual is \\(e_i  = y_i- \\hat y_i\\). Since our predicted value \\(\\hat y_i\\) follows a normal distribution, the residual also follows a normal distribution, since it is a difference of normal random variable \\(\\hat y_i\\) and a constant \\(y_i\\). It easy to see that \\[e_i \\sim N(0, s_e^2),\\] where \\(s_e^2\\) is an empirical estimate of the error’s variance.\nConsider the relation between the fitted values \\(\\hat y_i\\) and residuals \\(e_i\\). Our predictions are given by the line. The residual \\(e_i\\) and predicted value \\(\\hat y_i\\) for the \\(i\\)th observation are related via \\[\ny_i = \\hat{y}_i + ( y_i - \\hat{y}_i ) = \\hat{y}_i + e_i.\n\\]\nResiduals allow us to define outliers. They simply have large residuals. We re-scale the residuals by their standard errors. This lets us define \\[r_i = \\frac{ e_i }{ s_{ e} } =\\frac{y_i - \\hat{y}_i   }{ s_{ e } }\\] Since residuals follow normal distribution \\(e \\sim N(0,\\sigma^2)\\), in 95% of the time we expect the standardized residuals to satisfy \\(- 2 &lt; r_i &lt; 2\\). Any observation with \\(|r_i| &gt; 3\\) is an extreme outlier; it is three sigmas away from the mean.\nAnother type of observations we are interested in are the influential points. These are observations that affect the magnitude of our estimates \\(\\hat{\\beta}\\)s. They are important to find as they typically have economic consequences. We will use Cook’s distance to assess the significance of an influential point. Cook’s distance associated with sample \\(i\\) measures the change in estimated model parameters \\(\\hat \\beta\\) when sample \\(i\\) is removed from the training data set.\nIntuitively, we model regression-back-to-the-mean effect. This is one of the most interesting statistical effects you’ll see in daily life. In statistics, regression does not mean “going backwards”, but rather the tendency for a variable that is extremely high or low to move closer to the average upon subsequent measurement. For example, Francis Galton, who was a cousin of Charles Darwin, in his study on regression to the mean height showed that if your parents are taller than the average, you’ll regress back to the average. While people might expect the children of tall parents to be even taller and the children of short parents to be even shorter, Galton found that this wasn’t the case. Instead, he observed that the heights of the children tended to be closer to the average height for the population. Galton termed this phenomenon “regression towards mediocrity” (now more commonly known as “regression to the mean”). It meant that extreme characteristics (in this case, height) in parents were likely to be less extreme (closer to the average) in their children. It is a classic example that helped introduce and explain this statistical concept. Galton’s finding was one of the first insights into what is now a well-known statistical phenomenon. It doesn’t imply that all individual cases will follow this pattern; rather, it’s a trend observed across a population. It’s important to understand that regression to the mean doesn’t suggest that extreme traits diminish over generations but rather that an extreme measurement is partly due to random variation and is likely to be less extreme upon subsequent measurement.\nAnother example was documented by Daniel Kahneman and Amos Tversky in their book Thinking, Fast and Slow. They found that when a person performs a task, their performance is partly due to skill and partly due to luck. They observed that when a person performs a task and achieves an extreme result, their subsequent performance is likely to be less extreme. Particularly they studied effect of criticism and praise used by Israeli Air Force fighter pilots trainers. After criticism, the low-scoring pilots were retested. Often, their scores improve. At first glance, this seems like a clear effect of feedback from the trainer. However, some of this improvement is likely a statistical artifact and demonstrates the regression to the mean effect.\nWhy? Those pilots who initially scored poorly were, statistically speaking, somewhat unlucky. Their low scores may have been due to a bad day, stress, or other factors. When retested, their scores are likely to be closer to their true skill level, which is closer to the average. This natural movement towards the average can give the illusion that the intervention (praise or criticism) was more effective than it actually was. Conversely, if the top performers were praised and retested, we might find their scores decrease slightly, not necessarily due to the inefficacy of the praise but due to their initial high scores being partly due to good luck or an exceptionally good day. In conclusion, in pilot training and other fields, it’s important to consider regression to the mean when evaluating the effectiveness of interventions. Without this consideration, one might draw incorrect conclusions about the impact of training or other changes.\n\nExample 12.4 (Google vs S&P 500) We will demonstrate how we can use statistical properties of a linear regression model to understand the relationship between returns of a google stock and the S&P 500 index. We will use Capital Asset Pricing Model (CAPM) regression model to estimate the expected return of an investment into Google stock and to price the risk. The CAPM model is\n\\[\n\\mathrm{GOOG} = \\alpha + \\beta \\mathrm{SP500} + \\epsilon\n\\] On the left hand side, we have the return that investors expect to earn from investing into Google stock. In the CAPM model, this return is typically modeled as a dependent variable.\nThe input variable SP500 represents the average return of the entire US market. Beta measures the volatility or systematic risk of a security or a portfolio in comparison to the market as a whole. A beta greater than 1 indicates that the security is more volatile than the market, while a beta less than 1 indicates it is less volatile. Alpha is the intercept of the regression line, it measures the excess return of the security over the market. The error term \\(\\epsilon\\) captures the uncertainty in the relationship between the returns of Google stock and the market.\nIn a CAPM regression analysis, the goal is to find out how well the model explains the returns of a security based on its beta. This involves regressing the security’s excess returns (returns over the risk-free rate) against the excess returns of the market. The slope of the regression line represents the beta, and the intercept should ideally be close to the risk-free rate, although in practice it often deviates. This model helps in understanding the relationship between the expected return and the systematic risk of an investment.\nBased on the uncertainty associated with the estimates for alpha and beta, we can formulate several hypothesis tests, for example:\n\n\n\n\n\n\n\nHypothesis\nQuestion\n\n\n\n\n\\(H_0: \\beta = 0\\)\nIs Google related to the market?\n\n\n\\(H_0: \\alpha = 0\\)\nDoes Google outperform the market in a consistent fashion?\n\n\n\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2017-01-03',to='2023-12-29')\n## \"GOOG\" \"SPY\"\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0003\n0.0003\n1.1\n0.27\n\n\nspyret\n1.1706\n0.0240\n48.8\n0.00\n\n\n\nGoogle vs S&P 500 returns between 2017-2023\n\n\n\nplot(gret, spyret, bg=\"lightblue\", xlab=\"Google Return\", ylab=\"SPY Return\")\nabline(l, lwd=3, col=\"red\")\n\n\n\n\nGoogle vs S&P 500 returns between 2017-2023\n\n\n\n\nHere’s what we get after we fit the model using function\nOur best estimates are \\[\n\\hat \\alpha = 0.0004 \\; , \\; \\hat{\\beta} = 1.01\n\\]\nNow we can provide the results for the hypotheses we set at the beginning. Given that the p-value for \\(H_0: \\beta = 0\\) is &lt;2e-16 we can reject the null hypothesis and conclude that Google is related to the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.06, which is greater than 0.05, so we cannot reject the null hypothesis and conclude that Google does not outperform the market in a consistent fashion in the 2017-2023 period.\nFurther, we can answer some of the other important questions, such as how much will Google move if the market goes up \\(10\\)%?\n\nalpha = coef(l)[1]\nbeta = coef(l)[2]\n# Calculate the expected return\nalpha + beta*0.1\n## (Intercept) \n##        0.12\n\nHowever, if we look at the earlier period between 2005-2016 (the earlier days of Google) the results will be different.\n\ngetSymbols(Symbols = c(\"GOOG\",\"SPY\"),from='2005-01-03',to='2016-12-29');\n## \"GOOG\" \"SPY\"\ngret = as.numeric(dailyReturn(GOOG))\nspyret = as.numeric(dailyReturn(SPY))\nl = lm(gret ~ spyret)\ntidy(l) %&gt;% knitr::kable(digits=4)\n\n\n\nTable 12.1: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0006\n0.0003\n2.2\n0.03\n\n\nspyret\n0.9231\n0.0230\n40.1\n0.00\n\n\n\n\n\n\n\n\n\nplot(spyret, gret, col=\"blue\", ylab=\"Google Return\", xlab=\"SPY Return\", bg = \"lightblue\", cex=0.8)\nabline(l, lwd=3, col=\"red\")\n\n\n\n\n\n\n\nFigure 12.3: Google vs S&P 500 returns between 2005-2016\n\n\n\n\n\nIn this period Google did consistently outperform the market. The p-value for \\(H_0: \\alpha = 0\\) is 0.03.\n\n\nExample 12.5 (CAPM Model for Yahoo! Stock) Rather than estimate \\(\\mu\\) directly, the CAPM estimates the difference between \\(\\mu\\) and the risk-free rate \\(r_f\\). This quantity \\(\\mu-r_f\\) is known as the expected excess return (excess relative to a risk-free investment). The CAPM relates the expected excess return of a stock to that of an underlying benchmark, typically a broad-based market index. Let \\(\\mu_M\\) and \\(\\sigma_M\\) denote the return and volatility on the market index. The implication of CAPM is that there is a linear relationship between the expected excess return of a stock, \\(\\mu-r_f\\), and the excess return of the market, \\(\\mu_M-r_f\\).\n\\[\n\\text{Excess \\; Return}_{\\text{Stock}} =  \\beta \\;  \\text{Excess \\;\nReturn}_{\\text{Market}}\n\\] \\[\n\\mu-r_f = \\beta(\\mu_M - r_f )\n\\] Put simply, the expected excess return of a stock is \\(\\beta\\) times the excess expected return of the market. Beta (\\(\\beta\\)) is a measure of a stock’s risk in relation to the market. A beta of 1.3 implies that the excess return on the stock is expected to move up or down 30% more than the market. A beta bigger than one implies the stock is riskier than the market and goes up (and down) faster than the market goes up (and down). A beta less than one implies the stock is less risky than the market.\nUsing the CAPM, the expected return of the stock can now be defined as the risk free interest rate plus beta times the expected excess return of the market, \\[\n\\mu = \\text{Expected \\; Return}_{\\text{Stock}} = r_f+\\beta (\\mu_M-r_f)\n\\] Beta is typically estimated from a regression of the individual stock’s returns on those of the market. The other parameters are typically measured as the historical average return on the market \\(\\mu_M\\) and the yield on Treasury Bills \\(r_f\\). Together these form an estimate of \\(\\mu\\). The volatility parameter \\(\\sigma\\) is estimated by the standard deviation of historical returns.\nOur qualitative discussion implicitly took the year as the unit of time. For our example, we make one minor change and consider daily returns so that \\(\\mu\\) and \\(\\sigma\\) are interpreted as a daily rate of return and daily volatility (or standard deviation). We use an annual risk-free rate of 5%; this makes a daily risk-free rate of .019%, \\(r_f = 0.00019\\), assuming there are 252 trading days in a year. A simple historical average is used to estimate the market return (\\(\\mu_M\\)) for the Nasdaq 100. The average annual return is about 23%, with corresponding daily mean \\(\\mu_M = 0.00083\\). A regression using daily returns from 1996-2000 leads to an estimate of \\(\\beta = 1.38\\). Combining these (pieces) leads to an estimated expected return of Yahoo!, \\(\\mu_{Yahoo} = 0.00019+1.38(0.00083-0.00019) = 0.00107\\) on a daily basis. Note that the CAPM model estimates a future return that is much lower than the observed rate over the last three-plus years of .42% per day or 289% per year.\nTo measure the riskiness of Yahoo! notice that the daily historical volatility is 5%, i.e. \\(\\sigma = 0.05\\). On an annual basis this implies a volatility of \\(\\sigma \\sqrt{T} = 0.05 \\sqrt{252} = 0.79\\), that is 79%. For comparison, the benchmark Nasdaq 100 has historical daily volatility 1.9% and an annual historical volatility of 30%. The estimates of all the parameters are recorded in Table 12.2.\n\n\n\nTable 12.2: Key Parameter Estimates Based on Daily Returns 1996–2000\n\n\n\n\n\n\n\n\n\n\n\nAsset\nExpected return\nVolatility\nRegression coefft (s.e.)\n\n\n\n\nYahoo!\n\\(\\mu = 0.00107\\)\n\\(\\sigma = 0.050\\)\n\\(\\beta = 1.38 (.07)\\)\n\n\nNasdaq 100\n\\(\\mu_M = 0.00083\\)\n\\(\\sigma_M = 0.019\\)\n1\n\n\nTreasury Bills\n\\(r_f = 0.00019\\)\n–\n–",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#factor-regression-and-feature-engineering",
    "href": "12-regression.html#factor-regression-and-feature-engineering",
    "title": "12  Linear Regression",
    "section": "12.2 Factor Regression and Feature Engineering",
    "text": "12.2 Factor Regression and Feature Engineering\nA linear model assumes that output variable is proportional to the input variable plus an offset. However, this is not always the case. Often, we need to transform input variables by combining multiple inputs into a single predictor, for example by taking a ratio or putting inputs on a different scale, e.g. log-scale. In machine learning, this process is called feature engineering.\nOne of the classic examples of feature engineering is Fama-French three-factor model which is used in asset pricing and portfolio management. The model states that asset returns depend on (1) market risk, (2) the outperforming of small versus big companies, and (3) the outperformance of high book/market versus small book/market companies, mathematically \\[r = R_f + \\beta(R_m - R_f) + b_s\\cdot SMB + b_v\\cdot HML + \\alpha\\] Here \\(R_f\\) is risk-free return, \\(R_m\\) is the return of market, \\(SMB\\) stands for \"Small market capitalization Minus Big\" and \\(HML\\) for \"High book-to-market ratio Minus Low\"; they measure the historic excess returns of small caps over big caps and of value stocks over growth stocks. These factors are calculated with combinations of portfolios composed by ranked stocks (BtM ranking, Cap ranking) and available historical market data.\n\nLogarithmic and Power Transformations\nConsider, the growth of the Apple stock between 2000 and 2024. With the exception of the 2008 financial crisis period and 2020 COVID 19 related declines, the stock price has been growing exponentially. Figure 12.4 shows the price of the Apple stock between 2000 and 2024. The price is closely related to the company’s growth.\n\ngetSymbols(Symbols = \"AAPL\",from='2000-01-01',to='2023-12-31');\n## \"AAPL\"\nplot(AAPL$AAPL.Adjusted, type='l', col=\"blue\", xlab=\"Date\", ylab=\"Price\", grid.col=NA)\n\n\n\n\n\n\n\nFigure 12.4: Apple stock price growth in the 2000-2024 period\n\n\n\n\n\nThe 2008 and 2020 declines are more related to extraneous factors, rather than the growth of the company. Thus, we can conclude that the overall growth of the company is exponential. The growth of a successful company typically follows the rule of compounding. Compounding is a fundamental principle that describes how some quantity grows over time when this quantity increases by a fixed percentage periodically. This is a very common phenomenon in nature and business. For example, if two parents have 2.2 children on average, then the population increases by 10% every generation. Another example is growth of investment in a savings account.\nA more intuitive example is probably an investment in a savings account. If you invest \\(1000\\) in a savings account with \\(10\\%\\) annual interest rate and you get paid once a year, then your account value will be \\(1100\\) by the end of the year. However, if you get paid \\(n\\) times a year, and initially invest \\(w_0\\), the final value \\(w_t\\) of the account after \\(t\\) years will be \\[\nw_t = w_0 \\times (1 + r/n)^{nt}\n\\] where \\(r\\) is the annual interest rate. When you get paid every month (\\(n=12\\)), a traditional payout schedule used by banks, then \\[\nw_t = 1000 \\times (1 + 0.1/12)^{12} = 1105.\n\\] A value slightly higher than the annual payout of 1100.\nThe effect of compounding is minimal in the short term. However, the effect of compounding is more pronounced when the growth rate is higher and time periods are longer. For example at \\(r=2\\), \\(n=365\\) and 4-year period \\(t=4\\), you get \\[\nw_t = 1000 \\times (1 + 2/365)^{3\\times 365} = 2,916,565.\n\\] Your account is close to 3 million dollars! Compared to \\(n=1\\) scenario \\[\nw_t = 1000 \\times (1 + 2)^{4} = 81,000,\n\\] when you will end up with merely 81 thousand. This is why compounding is often referred to as the “eighth wonder of the world” in investing contexts, emphasizing its power in growing wealth over time.\nIn general, as \\(n\\) goes up, the growth rate of the quantity approaches the constant\n\nT = 1:100\nr = 1\nplot(T, (1+r/T)^T, type='l', col=\"blue\", ylab=\"Future Value\")\nabline(h=exp(r), col=\"red\", lwd=3)\n\n\n\n\n\n\n\nFigure 12.5: Growth of an investment in a savings account when n increases and return rate is 100% per year\n\n\n\n\n\nFigure 12.5 shows the growth of an investment in a savings account when \\(n\\) increases and return rate is \\(100\\%\\) per year. We can see that the growth rate approaches the constant \\(e \\approx 2.72\\) as \\(n\\) increases. \\[\n(1+r/n)^n \\rightarrow e^r,~\\text{as}~n \\rightarrow \\infty.\n\\] This limit was first delivered by Leonhard Euler and the number \\(e\\) is known as Euler’s number.\nComing back to the growth of the Apple company, we can think of it growing at a small constant rate every day. The relation between the time and size of Apple is multiplicative. Meaning when time increases by one day, the size of the company increases by a small constant percentage. This is a multiplicative relation. In contrast, linear relation is additive, meaning that when time increases by one day, the size of the company increases by a constant amount. The exponential growth model is given by the formula \\[\ny = y_0 \\times e^{\\beta^Tx}.\n\\] There are many business and natural science examples where multiplicative relation holds. If we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log y_0 + \\beta^Tx.\n\\] This is a linear relation between \\(\\log y\\) and \\(x\\). Thus, we can use linear regression to estimate the parameters of the exponential growth model by putting the output variable \\(y\\) on the log-scale.\nAnother example of nonlinear relation that can be analyzed using linear regression is when variables are related via a power law. This concept helps us model proportional relationships or ratios. In a multiplicative relationship, when one variable changes on a percent scale, the other changes in a directly proportional manner, as long as the multiplying factor remains constant. For example, the relation between the size of a city and the number of cars registered in the city is given by a power law. When the size of the city doubles, the number of cars registered in the city is also expected to double. The power law model is given by the formula \\[\ny = \\beta_0 x^{\\beta_1}.\n\\] If we apply the \\(\\log\\) function to both sides of the equation, we get \\[\n\\log y = \\log \\beta_0 + \\beta_1 \\log x.\n\\] This is a linear relation between \\(\\log y\\) and \\(\\log x\\). Thus, we can use linear regression to estimate the parameters of the power law model by putting the output variable \\(y\\) and input variable \\(x\\) on the log-scale.\nHowever, there are several caveats when putting variables on the log-scale. We need to make sure that the variable is positive. This means that we cannot apply log transformations to variables that contain zeros, such as dummy variables or count variables with zero values.\n\nExample 12.6 (World’s Smartest Mammal) We will demonstrate the power relation using data on brain (measured in grams) and body (measured in kilograms) weights for 62 mammal species. The data was collected by Harry J. Jerison in 1973. The dataset contains the following variables:\n\nmammals = read.csv(\"../data/mammals.csv\")\nknitr::kable(head(mammals))\n\n\n\n\nMammal\nBrain\nBody\n\n\n\n\nAfrican_elephant\n5712.0\n6654.00\n\n\nAfrican_giant_pouched_rat\n6.6\n1.00\n\n\nArctic_Fox\n44.5\n3.38\n\n\nArctic_ground_squirrel\n5.7\n0.92\n\n\nAsian_elephant\n4603.0\n2547.00\n\n\nBaboon\n179.5\n10.55\n\n\n\n\n\nLet’s build a linear model.\n\nattach(mammals)\nmodel = lm(Brain~Body)\nplot(Body,Brain, pch=21, bg=\"lightblue\")\nabline(model, col=\"red\", lwd=3)\n\n\n\n\nBrain vs Body weight for 62 mammal species\n\n\n\n\nWe see a few outliers with large residuals. This suggests that normality assumption is violated. We can check the residuals by plotting residuals against fitted values and plotting fitted vs true values.\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\", xlab=\"Fitted y\", ylab=\"Residuals\")\nabline(h=0, col=\"red\", lwd=3)\nplot(Brain, model$fitted.values, pch=21, bg=\"lightblue\", xlab=\"True y\", ylab=\"Fitted y\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 12.6: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.7: Fitted y vs True y\n\n\n\n\n\n\nRemember, that residuals should roughly follow a normal distribution with mean zero and constant variance. We can see that the residuals are not normally distributed and the variance increases with the fitted values. This is a clear indication that we need to transform the data. We can try a log-log transformation.\n\nmodel = lm(log(mammals$Brain)~log(mammals$Body))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.18\n0.11\n20\n0\n\n\nlog(mammals$Body)\n0.74\n0.03\n23\n0\n\n\n\n\n\nplot(model$fitted.values, model$residuals, pch=21, bg=\"lightblue\")\nabline(h=0, col=\"red\", lwd=3)\nplot(log(Brain), model$fitted.values, pch=21, bg=\"lightblue\")\nabline(a=0,b=1, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\n\nFigure 12.8: Fitted y vs Residuals\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.9: Fitted y vs True y\n\n\n\n\n\n\nThat is much better! The residuals variance is constant and the plot of fitted vs true values shows a linear relationship. The log-log model is given by the formula \\[\n\\log \\mathrm{Brain} = 2.18 + 0.74 \\log \\mathrm{Body}.\n\\] seem to achieve two important goals, namely linearity and constant variance. The coefficients are highly significant.\nAlthough the log-log model fits the data rather well, there are a couple of outliers there. Let us print the observations with the largest residuals.\n\n# res = rstudent(model)\nres = model$residuals/sd(model$residuals)\noutliers = order(res,decreasing = T)[1:10]\ncbind(mammals[outliers,],\n      Std.Res = res[outliers], Residual=model$residuals[outliers],\n      Fit = exp(model$fitted.values[outliers])) %&gt;% knitr::kable(digits=2)\n\n\n\n\n\nMammal\nBrain\nBody\nStd.Res\nResidual\nFit\n\n\n\n\n11\nChinchilla\n64\n0.42\n3.41\n2.61\n4.7\n\n\n34\nMan\n1320\n62.00\n2.53\n1.93\n190.7\n\n\n50\nRhesus_monkey\n179\n6.80\n2.06\n1.58\n36.9\n\n\n6\nBaboon\n180\n10.55\n1.64\n1.26\n51.1\n\n\n42\nOwl_monkey\n16\n0.48\n1.44\n1.10\n5.1\n\n\n10\nChimpanzee\n440\n52.16\n1.26\n0.96\n167.7\n\n\n27\nGround_squirrel\n4\n0.10\n1.18\n0.91\n1.6\n\n\n43\nPatas_monkey\n115\n10.00\n1.11\n0.85\n49.1\n\n\n60\nVervet\n58\n4.19\n1.06\n0.81\n25.7\n\n\n3\nArctic_Fox\n44\n3.38\n0.92\n0.71\n22.0\n\n\n\n\n\nThere are two outliers, the Chinchilla and the Human, both have disproportionately large brains!\nIn fact, the Chinchilla has the largest standardized residual of 3.41. Meaning that the predicted value of 4.7 g is 3.41 standard deviations away from the recorded value of 64 g. This suggests that the Chinchilla is an intellectually superior species! However, after checking more carefully we realized that there was a recording error and the actual weight of an average Chinchilla’s brain is 6.4. We mistyped the decimal separator! Thus the actual residual is 0.4.\n\nabs(model$fitted.values[11] - log(6.4))/sd(model$residuals)\n##  11 \n## 0.4\n\nIn reality Chinchilla’s brain is not far from an average mammal of this size!\n\n\nExample 12.7 (Newfood) A six month market test has been performed on the Newfood product, which is a breakfast cereal. The goal is to build a multiple regression model that provides accurate sales forecasts. This dataset represents the outcome of a controlled experiment in which the values of the independent variables that affect sales were carefully chosen by the analyst.\nThe analysis aims to identify the factors that contribute to sales of a new breakfast cereal and to quantify the effects of business decisions such as the choice of advertising level, location in store, and pricing strategies.\n\n\n\nvariable\ndescription\n\n\n\n\nsales\nnew cereal sales\n\n\nprice\nprice\n\n\nadv\nlow or high advertising (\\(0\\) or \\(1\\))\n\n\nlocat\nbread or breakfast section (\\(0\\) or \\(1\\))\n\n\ninc\nneighborhood income\n\n\nsvol\nsize of store\n\n\n\nFirst, we need to understand which variables need to be transformed. We start by running the “kitchen-sink” regression with all variables. Then we perform diagnostic checks to assess model assumptions and identify potential issues. Based on these diagnostics, we decide which variables should be transformed. After running the new model with transformations, we perform additional diagnostics and variable selection to refine the model. Using the final model after transformations and eliminating variables, we examine what the largest Cook’s distance is to identify influential observations. Finally, we provide a summary of coefficients and their statistical significance.\nFirst, let’s examine the correlation matrix to understand the relationships between all variables in the dataset. This will help us identify potential multicollinearity issues and understand the strength and direction of associations between variables before building our regression model.\n\nnewfood = read.csv(\"../data/newfood.csv\")\nattach(newfood)\nnames(newfood)\n## \"sales\"  \"price\"  \"adv\"    \"locat\"  \"income\" \"svol\"   \"city\"   \"indx\"\n# knitr::kable()\nhead(newfood)\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\ncity\nindx\n\n\n\n\n225\n24\n0\n0\n7.3\n34\n3\n1\n\n\n190\n24\n0\n0\n7.3\n34\n3\n2\n\n\n205\n24\n0\n0\n7.3\n34\n3\n3\n\n\n323\n24\n0\n0\n8.3\n41\n4\n1\n\n\n210\n24\n0\n0\n8.3\n41\n4\n2\n\n\n241\n24\n0\n0\n8.3\n41\n4\n3\n\n\n\n\n\n# correlation matrix\ncm = cor(cbind(sales,price,adv,locat,income,svol))\ncm[upper.tri(cm, diag = TRUE)] = NA\n# knitr::kable(as.table(round(cm, 3)))\nas.table(round(cm, 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\nlocat\nincome\nsvol\n\n\n\n\nsales\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nprice\n-0.66\nNA\nNA\nNA\nNA\nNA\n\n\nadv\n0.00\n0.00\nNA\nNA\nNA\nNA\n\n\nlocat\n0.00\n0.00\n0.00\nNA\nNA\nNA\n\n\nincome\n0.16\n-0.13\n-0.75\n0.00\nNA\nNA\n\n\nsvol\n0.38\n-0.18\n-0.74\n-0.04\n0.81\nNA\n\n\n\n\n\nRemember, correlations between variables are not the same as regression coefficients (\\(\\beta\\)’s)! Looking at the correlation matrix, we can see that total sales volume (svol) is negatively correlated with advertising (adv), and income (income) is also negatively correlated with advertising (adv). The question is how might these negative correlations impact our ability to estimate the true advertising effects in our regression model?\n\nas.table(round(cm[2:4, 1:3], 3)) %&gt;% knitr::kable()\n\n\n\n\n\nsales\nprice\nadv\n\n\n\n\nprice\n-0.66\nNA\nNA\n\n\nadv\n0.00\n0\nNA\n\n\nlocat\n0.00\n0\n0\n\n\n\n\n\nThere’s no correlation in the \\(X\\)’s by design! Let’s start by only including price, adv, locat\n\nmodel = lm(sales~price+adv+locat)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n562.31\n53.1\n10.58\n0.00\n\n\nprice\n-12.81\n1.8\n-7.20\n0.00\n\n\nadv\n0.22\n14.5\n0.02\n0.99\n\n\nlocat\n-0.22\n14.5\n-0.02\n0.99\n\n\n\n\n\nWhy is the marketer likely to be upset by this regression? Why is the economist happy? The marketer might be upset because the advertising coefficient looks small or insignificant due to omitted variable bias (e.g. not controlling for store volume or income properly). The economist is happy because the price coefficient is negative, consistent with the law of demand. Let’s add income and svol to the regression and use log-log model.\n\nmodel = lm(log(sales) ~ log(price) + adv + locat + \n           log(income) + log(svol))\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.39\n6.06\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-7.90\n0.00\n\n\nadv\n0.15\n0.10\n1.49\n0.14\n\n\nlocat\n0.00\n0.06\n0.02\n0.99\n\n\nlog(income)\n-0.52\n0.50\n-1.06\n0.29\n\n\nlog(svol)\n1.03\n0.26\n4.04\n0.00\n\n\n\n\n\nWhy no logs for adv and locat variables? The log(svol) coefficient is close to one!\nThe reason we don’t apply logarithms to adv and locat variables is because they are binary categorical variables (taking values 0 or 1). Taking the logarithm of 0 is undefined, and taking the logarithm of 1 equals 0, which would not provide any meaningful transformation. For binary variables, the exponential transformation in the final model interpretation directly gives us the multiplicative effect on sales when the variable changes from 0 to 1.\nRegarding the log(svol) coefficient being close to one (1.03), this suggests that sales scale approximately proportionally with store volume. A coefficient of 1.0 would indicate perfect proportional scaling, meaning a 1% increase in store volume would lead to a 1% increase in sales. Our coefficient of 1.03 indicates slightly more than proportional scaling—a 1% increase in store volume leads to a 1.03% increase in sales, suggesting some economies of scale or network effects in larger stores.\nOn the transformed scale (log-log model), \\[\n\\log sales=8.41 - 1.74 \\log price + 0.150 {\\text{adv}} + 0.001 {\\text{locat}} - 0.524 \\log inc  + 1.03 \\log svol\n\\] On the un-transformed scale, \\[\n\\text{sales} = e^{8.41} ( \\text{price} )^{-1.74} e^{ 0.15 \\text{adv} } e^{ 0.001 \\text{locat}} ( \\text{inc} )^{-0.524}  ( \\text{svol} )^{1.03}\n\\] In the log-log regression model, the relationship between sales and the continuous variables (price, income, and store volume) follows a power function relationship. This means that a 1% change in these variables leads to a proportional change in sales according to their respective coefficients. Specifically, a 1% increase in price leads to a 1.74% decrease in sales, a 1% increase in income leads to a 0.524% decrease in sales, and a 1% increase in store volume leads to a 1.03% increase in sales.\nIn contrast, the binary variables (advertising and location) follow an exponential relationship with sales. When advertising is present (adv=1), sales increase by a factor of e^0.15 = 1.16, representing a 16% improvement. Similarly, when a store is in a good location (locat=1), sales increase by a factor of e^0.001 = 1.001, representing a 0.1% improvement. This exponential relationship arises because these variables are binary (0 or 1) and cannot be log-transformed, so their effects are multiplicative on the original sales scale.\nThe log-log regression model reveals several important relationships between the independent variables and sales performance.\n\nPrice elasticity is \\(\\hat{\\beta}_{\\text{price}} = - 1.74\\). A \\(1\\)% increase in price will drop sales \\(1.74\\)%\n\\(\\mathrm{adv}=1\\) increases sales by a factor of \\(e^{0.15} = 1.16\\). That’s a \\(16\\)% improvement\n\nWe should delete the locat variable from our regression model because it is statistically insignificant. The coefficient for locat has a very small magnitude (0.001) and a high p-value, indicating that there is insufficient evidence to reject the null hypothesis that this variable has no effect on sales. Including statistically insignificant variables in a model can lead to overfitting and reduce the model’s predictive accuracy on new data. By removing locat, we create a more parsimonious model that focuses only on the variables that have meaningful relationships with the outcome variable.\nNow, we are ready to use our model for prediction. predict.lm provides a \\(\\hat{Y}\\)-prediction given a new \\(X_f\\)\n\nmodelnew = lm(log(sales)~log(price)+adv+log(income)+log(svol))\nmodelnew  %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.41\n1.37\n6.1\n0.00\n\n\nlog(price)\n-1.74\n0.22\n-8.0\n0.00\n\n\nadv\n0.15\n0.10\n1.5\n0.14\n\n\nlog(income)\n-0.52\n0.49\n-1.1\n0.29\n\n\nlog(svol)\n1.03\n0.25\n4.1\n0.00\n\n\n\n\nnewdata=data.frame(price=30,adv=1,income=8,svol=34)\npredict.lm(modelnew, newdata, interval=\"confidence\", level=0.99)\n##   fit lwr upr\n## 1 5.2 4.9 5.5\n\nExponentiate-back to find \\(\\text{sales} = e^{5.1739} = 176.60\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#interactions",
    "href": "12-regression.html#interactions",
    "title": "12  Linear Regression",
    "section": "12.3 Interactions",
    "text": "12.3 Interactions\nIn many situations, \\(x_1\\) and \\(x_2\\) interact when predicting \\(y\\). An interaction occurs when the effect of one independent variable on the dependent variable changes at different levels of another independent variable. For example, consider a study analyzing the effect of study hours \\(x_1\\) and a tutoring program \\(x_2\\), a binary variable where 0 = no tutoring, 1 = tutoring) on test scores \\(y\\). Without an interaction term, we assume the effect of study hours on test scores is the same regardless of tutoring. With an interaction term, we can explore whether the effect of study hours on test scores is different for those who receive tutoring compared to those who do not. Here are a few more examples when there is potential interaction.\nExamples of potential interactions include whether gender changes the effect of education on wages, whether patients recover faster when taking drug A, and how advertisement affects price sensitivity. Interactions are particularly useful with dummy variables. We can build a kitchen-sink model with all possible dummies (day of the week, gender, etc.).\nIf we think that the effect of \\(x_1\\) on \\(y\\) depends on the value of \\(x_2\\), we model it using a linear relation \\[\n\\beta_1 = \\beta_{10} + \\beta_{11} x_2\n\\] and the model without interaction \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) becomes \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon.\n\\] The interaction term captures the effect of \\(x_1\\) on \\(y\\) when \\(x_2=1\\). The coefficient \\(\\beta_3\\) is the difference in the effect of \\(x_1\\) on \\(y\\) when \\(x_2=1\\) and \\(x_2=0\\). If \\(\\beta_3\\) is significant, then there is an interaction effect. If \\(\\beta_3\\) is not significant, then there is no interaction effect. The coefficients \\(\\beta_1\\) and \\(\\beta_2\\) are called marginal effects.\nIn R:\n\nmodel = lm(y = x1 * x2)\n\nestimates \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\\), and\n\nmodel = lm(y = x1:x2)\n\nestimates only \\(y = \\beta_3 x_1 x_2 + \\epsilon\\)\nIf \\(\\beta_3\\) is significant there’s an interaction effect and we must leave \\(\\beta_1\\) and \\(\\beta_2\\) in the model whether they are significant or not.\nWhen \\(x_2 = D\\) is a dummy variable with values of zero or one, we typically run a regression of the form \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1 \\star D + \\epsilon\\] The coefficient \\(\\beta_1 + \\beta_2\\) is the effect of \\(x_1\\) when \\(D=1\\). The coefficient \\(\\beta_1\\) is the effect when \\(D=0\\).\n\nExample 12.8 (Orange Juice) Understanding how advertising affects consumer price sensitivity is a fundamental question in marketing analytics. Do promotional campaigns make consumers more or less sensitive to price changes? To explore this question, we analyze a dataset from the orange juice category that includes sales data from 83 Chicagoland stores. This dataset provides rich information including price, sales volume (measured as log units moved), brand identity, and whether the product was featured in store displays or advertising circulars.\nLet’s begin by examining the structure of the data and the basic relationships between variables.\n\noj = read.csv(\"./../data/oj.csv\")\nknitr::kable(oj[1:5,1:10], digits=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstore\nbrand\nweek\nlogmove\nfeat\nprice\nAGE60\nEDUC\nETHNIC\nINCOME\n\n\n\n\n2\ntropicana\n40\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n46\n8.7\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n47\n8.2\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n48\n9.0\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n2\ntropicana\n50\n9.1\n0\n3.9\n0.23\n0.25\n0.11\n11\n\n\n\n\nmodel = lm(logmove ~ log(price)*feat, data=oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\nmodel = lm(log(price)~ brand-1, data = oj)\nmodel %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nbranddominicks\n0.53\n0\n254\n0\n\n\nbrandminute.maid\n0.79\n0\n382\n0\n\n\nbrandtropicana\n1.03\n0\n500\n0\n\n\n\n\n\nThe dataset includes three major brands: Dominick’s (the store brand), Minute Maid, and Tropicana. We can visualize the price distributions and the relationship between price and sales for these brands.\nbrandcol &lt;- c(\"green\",\"red\",\"gold\")\noj$brand = factor(oj$brand)\nboxplot(log(price) ~ brand, data=oj, col=brandcol)\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot reveals that Tropicana commands premium pricing, while Dominick’s store brand is positioned as the value option. The scatterplot shows a clear negative relationship between price and sales, as expected from basic economic theory: higher prices lead to lower sales volume.\nA critical modeling decision is whether to use logarithmic transformations for our variables. Let’s examine the relationship between price and sales on both the original and log scales to determine which specification better captures the underlying relationship.\nFirst, we plot sales against price on the original scale:\n\n\n\n\n\n\n\n\n\nThe pattern suggests a nonlinear relationship that might be better captured with a transformation. Next, we examine the relationship between price and log(sales):\n\nl1 &lt;- loess(logmove ~ price, data=oj, span=2)\nsmoothed1 &lt;- predict(l1) \nind = order(oj$price)\nplot(logmove ~ price, data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nlines(smoothed1[ind], x=oj$price[ind], col=\"blue\", lwd=2)\n\n\n\n\n\n\n\n\nWhile this shows some improvement, the relationship still exhibits curvature. Finally, we examine the log-log specification:\n\npar(mar = c(4, 4, 0.1, 0.1), bty=\"n\")\nplot(logmove ~ log(price), data=oj, col=brandcol[oj$brand], pch=16, cex=0.5, ylab=\"log(move)\")\nfor(i in 1:3) {\n  brand_data &lt;- oj[oj$brand == levels(oj$brand)[i], ]\n  l2 &lt;- lm(logmove ~ log(price), data=brand_data)\n  smoothed2 &lt;- predict(l2)\n  ind &lt;- order(brand_data$price)\n  lines(smoothed2[ind], x=log(brand_data$price[ind]), col='blue', lwd=2)\n}\n\n\n\n\n\n\n\n\nThe log-log specification produces a much more linear relationship. This makes economic sense: the relationship between price and sales is multiplicative rather than additive. A 10% increase in price leads to a proportional percentage decrease in sales, regardless of the starting price level. This constant elasticity model is the standard approach in demand analysis.\nOur central research question is whether featuring a product in advertisements affects consumers’ price sensitivity. We start with a baseline model that includes both log price and the advertising indicator: \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\]\nHowever, this model assumes that advertising only shifts demand up or down, without changing how consumers respond to price. To test whether advertising actually changes price sensitivity, we need to allow the price coefficient \\(\\beta_1\\) to vary with advertising. We can model this by assuming that the price sensitivity depends linearly on the advertising indicator: \\[\n\\beta_1 = \\beta_3 +  \\beta_4\\mathrm{feat}.\n\\]\nSubstituting this into our demand equation gives us an interaction model: \\[\n\\log(\\mathrm{sales}) = \\beta_0 + (\\beta_3 +  \\beta_4\\mathrm{feat})\\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\]\nExpanding this expression yields: \\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_3\\log(\\mathrm{price}) +  \\beta_4\\mathrm{feat} \\times \\log(\\mathrm{price}) + \\beta_2 \\mathrm{feat}.\n\\]\nThe coefficient \\(\\beta_4\\) on the interaction term tells us how much price sensitivity changes when the product is featured in advertising. Let’s estimate this model and compare it to simpler specifications.\nFirst, we fit the basic model with only log price:\n\nlm(logmove ~ log(price), data=oj) %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.4\n0.02\n679\n0\n\n\nlog(price)\n-1.6\n0.02\n-87\n0\n\n\n\n\n\nThis simple model suggests a price elasticity of approximately -3.1, meaning a 1% increase in price leads to roughly a 3.1% decrease in sales.\nNext, we add brand effects and the advertising indicator without interactions:\n\nlm(logmove ~ log(price) + feat + brand, data=oj) %&gt;% \n  tidy() %&gt;% \n  knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.28\n0.01\n708\n0\n\n\nlog(price)\n-2.53\n0.02\n-116\n0\n\n\nfeat\n0.89\n0.01\n85\n0\n\n\nbrandminute.maid\n0.68\n0.01\n58\n0\n\n\nbrandtropicana\n1.30\n0.01\n88\n0\n\n\n\n\n\nNow we estimate the full model with the interaction between price and advertising:\n\nojreg &lt;- lm(logmove ~ log(price)*feat, data=oj)\nojreg %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.66\n0.02\n588\n0\n\n\nlog(price)\n-0.96\n0.02\n-51\n0\n\n\nfeat\n1.71\n0.03\n56\n0\n\n\nlog(price):feat\n-0.98\n0.04\n-23\n0\n\n\n\n\n\nThe results reveal a striking finding: the interaction coefficient is -0.98, which is highly statistically significant. When products are not featured (feat = 0), the price elasticity is -0.96. However, when products are featured in advertising (feat = 1), the price elasticity becomes \\(-0.96 + (-0.98) = -1.94\\), essentially doubling in magnitude. This means that advertising makes consumers nearly twice as sensitive to price changes.\nThis result seems counterintuitive at first. We might expect that advertising would make consumers less price-sensitive by building brand loyalty or highlighting non-price attributes. Why would advertising increase price sensitivity?\nThe explanation lies in the promotional strategy employed by retailers. Let’s examine the pricing behavior during advertising campaigns:\ndoj = oj %&gt;% filter(brand==\"dominicks\")\nboxplot(price ~  feat, data = oj[oj$brand==\"dominicks\",], col=c(2,3), \n        main=\"Dominick's\", ylab=\"Price ($)\")\nboxplot(price ~  feat, data = oj[oj$brand==\"minute.maid\",], col=c(2,3), \n        main=\"Minute Maid\", ylab=\"Price ($)\")\nboxplot(price ~  feat, data = oj[oj$brand==\"tropicana\",], col=c(2,3), \n        main=\"Tropicana\", ylab=\"Price ($)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplots reveal the key insight: prices are systematically lower when products are featured (feat = 1) compared to when they are not (feat = 0). This is a classic promotional pricing strategy where advertising and price discounts are bundled together. Retailers use advertising to draw attention to temporary price reductions.\nThe increased price sensitivity we observe during advertising periods is not caused by the advertising itself making consumers more price-conscious. Rather, it reflects the fact that advertising campaigns coincide with periods of price variation, and consumers who respond to advertisements are precisely those who are most price-sensitive. The advertising serves as a signal that draws price-sensitive consumers into the market, creating a selected sample with higher elasticity.\nThis example demonstrates the importance of carefully interpreting interaction effects and understanding the underlying business practices that generate the data. What appears to be an advertising effect on price sensitivity is actually a selection effect driven by the correlation between advertising and pricing strategies.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#categorical-variables-and-dummy-encoding",
    "href": "12-regression.html#categorical-variables-and-dummy-encoding",
    "title": "12  Linear Regression",
    "section": "12.4 Categorical Variables and Dummy Encoding",
    "text": "12.4 Categorical Variables and Dummy Encoding\nIn our orange juice analysis, we want to understand how brand affects sales. Intuitively, we might want to include brand in our regression model:\n\\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\xcancel{\\beta_2\\mathrm{brand}}\n\\]\nHowever, we immediately encounter a problem: brand is a categorical variable, not a numerical one. We have three brands in our dataset - Dominick’s, Minute Maid, and Tropicana - but these are labels, not numbers. How can we incorporate such categorical information into a mathematical equation that requires numerical coefficients?\nThe solution lies in creating dummy variables (also called indicator variables). For a categorical variable with \\(k\\) categories, we create \\(k-1\\) binary variables, each taking the value 1 if the observation belongs to that category and 0 otherwise. One category is designated as the reference category and is represented when all dummy variables equal zero.\nFor our three-brand scenario, we create two dummy variables as shown in the table below:\n\n\n\nBrand\nIntercept\nbrandminute.maid\nbrandtropicana\n\n\n\n\nminute.maid\n1\n1\n0\n\n\ntropicana\n1\n0\n1\n\n\ndominicks\n1\n0\n0\n\n\n\nThis encoding transforms our regression equation into:\n\\[\n\\log(\\mathrm{sales}) = \\beta_0 + \\beta_1\\log(\\mathrm{price}) + \\beta_{21}\\mathrm{brandminute.maid} + \\beta_{22}\\mathrm{brandtropicana}.\n\\]\nFortunately, statistical software like R handles this transformation automatically when you include a categorical variable in your model specification:\n\nlm(logmove ~ log(price)+brand, data=oj) %&gt;% \n  tidy() %&gt;% \n  knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.83\n0.01\n745\n0\n\n\nlog(price)\n-3.14\n0.02\n-137\n0\n\n\nbrandminute.maid\n0.87\n0.01\n67\n0\n\n\nbrandtropicana\n1.53\n0.02\n94\n0\n\n\n\n\n\n\nExample 12.9 (Golf Performance Data) Dave Pelz has written two best-selling books for golfers, Dave Pelz’s Short Game Bible, and Dave Pelz’s Putting Bible. These books have become essential reading for serious golfers looking to improve their performance through data-driven analysis and scientific methodology.\nDave Pelz was formerly a “rocket scientist” (literally) at NASA, where he worked on the Apollo space program. His background in physics and engineering provided him with the analytical skills to revolutionize golf instruction through data analytics. His systematic approach to analyzing golf performance helped him refine his teaching methods and develop evidence-based strategies for improving players’ games. Through his research, Pelz discovered that it’s the short-game that matters most for overall scoring performance.\nOne of Pelz’s most famous findings concerns the optimal speed for a putt. Through extensive data collection and analysis, he determined that the best chance to make a putt is one that will leave the ball \\(17\\) inches past the hole, if it misses. This counterintuitive result challenges the common belief that golfers should aim to leave putts just short of the hole. Pelz’s research showed that putts hit with this specific speed have the highest probability of going in, as they account for the natural variations in green speed, slope, and other factors that affect putt trajectory.\nNow, we demonstrate how to use data to improve your golf game. We analyze the dataset that contains comprehensive year-end performance statistics for 195 professional golfers from the 2000 PGA Tour season. This rich dataset captures technicical abilities of the players as well as financial success (measured by the amount of prize money they made). Each observation represents season’s averages of the players’ performance and total prize money. List below shows the variables in the dataset.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nnevents\nThe number of official PGA events included in the statistics\n\n\nmoney\nThe official dollar winnings of the player\n\n\ndrivedist\nThe average number of yards driven on par 4 and par 5 holes\n\n\ngir\nGreens in regulation, measured as the percentage of time that the first (tee) shot on a par 3 hole ends up on the green, or the second shot on a par 4 hole ends up on the green, or the third shot on a par 5 hole ends up on the green\n\n\navgputts\nThe average number of putts per round\n\n\n\nWe will analyze these data to determine which of the variables nevents, drivedist, gir, and avgputts is most important for winning money on the PGA Tour. We begin by performing a regression of Money on all explanatory variables:\n\nd00 = read_csv(\"../data/pga-2000.csv\")\nd18 = read_csv(\"../data/pga-2018.csv\")\n\n\nmodel18 = lm(money ~ nevents + drivedist + gir + avgputts, data=d18)\nmodel00 = lm(money ~ nevents + drivedist + gir + avgputts, data=d00)\nmodel00 %&gt;%  tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n14856638\n4206466\n3.5\n0.00\n\n\nnevents\n-30067\n11183\n-2.7\n0.01\n\n\ndrivedist\n21310\n6913\n3.1\n0.00\n\n\ngir\n120855\n17429\n6.9\n0.00\n\n\navgputts\n-15203045\n2000905\n-7.6\n0.00\n\n\n\n\n\nLet’s look at the residuals:\n\nhist(rstandard(model00), breaks=20, col=\"lightblue\", \nxlab = \"Standardized Residual\", main=\"\")\n\n\n\n\n\n\n\n\nIt seems like we need to measure money on a log scale. Let’s transform with log(Money) as it has much better residual diagnostic plots.\n\nm = lm(formula = log(money) ~ nevents + drivedist + gir + avgputts, data = d00)\nm %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\n\n\nmodel00log = lm(log(money) ~ nevents + drivedist + gir + avgputts, data=d00)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.15\n3.58\n10.10\n0.00\n\n\nnevents\n-0.01\n0.01\n-0.94\n0.35\n\n\ndrivedist\n0.01\n0.01\n2.40\n0.02\n\n\ngir\n0.17\n0.01\n11.18\n0.00\n\n\navgputts\n-21.13\n1.70\n-12.42\n0.00\n\n\n\n\nhist(rstandard(model00log), breaks=20, col=\"lightblue\", \nxlab = \"Standardized Residual\", main=\"\")\narrows(x0 = 3,y0 = 20,x1 = 3.2,y1 = 2,length = 0.1)\ntext(x = 2.6,y = 22,labels = \"Tiger Woods\", cex=1)\n\n\n\n\n\n\n\n\nUsing log scale for money gives us a better model. We will keep it for now. How about selectng variables. Notice, that \\(t\\)-stats for nevents is \\(&lt;1.5\\). Thus, we can remove it.\n\nm1 = lm(formula = log(money) ~ drivedist + gir + avgputts, data = d00)\nm1 %&gt;% tidy() %&gt;% knitr::kable(digits=2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n36.17\n3.58\n10.1\n0.00\n\n\ndrivedist\n0.01\n0.01\n2.5\n0.01\n\n\ngir\n0.17\n0.01\n11.2\n0.00\n\n\navgputts\n-21.37\n1.68\n-12.7\n0.00\n\n\n\n\n\nIt is obvious that fewer putts indicate a better golfer. However, decreasing the average number of putts per round by one is extremely difficult to achieve.\nEvaluating the Coefficients\n\nGreens in Regulation (GIR) has a \\(\\hat{\\beta} = 0.17\\). If I can increase my GIR by one, I’ll earn \\(e^{0.17} = 1.18\\)% An extra \\(18\\)%.\nDriveDis has a \\(\\hat{\\beta} = 0.014\\). A \\(10\\) yard improvement, I’ll earn \\(e^{0.014 \\times 10} =e^{0.14}  = 1.15\\)% An extra \\(15\\)%.\n\nCaveat: Everyone has gotten better since 2000!\nTiger Woods was nine standard deviations better than what the model predicted, while taking the natural logarithm of money earnings significantly improves the residual diagnostics and an exponential model appears to fit the data well as evidenced by the good residual diagnostic plots; furthermore, the t-ratios for the number of events variable are consistently under 1.5, indicating it may not be a significant predictor.\nThe outliers represent the biggest over and under-performers in terms of money winnings when compared with their performance statistics, and Tiger Woods, Phil Mickelson, and Ernie Els won major championships by performing exceptionally well during tournaments with substantial prize money available.\nWe can see the over-performers and under-performers in the data.\n\n\n\nOver-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nTiger Woods\n9188321\n3584241\n5604080\n\n\n2\nPhil Mickelson\n4746457\n2302171\n2444286\n\n\n3\nErnie Els\n3469405\n1633468\n1835937\n\n\n4\nHal Sutton\n3061444\n1445904\n1615540\n\n\n20\nNotah Begay III\n1819323\n426061\n1393262\n\n\n182\nSteve Hart\n107949\n-1186685\n1294634\n\n\n\n\n\nNow, let’s extract the list of underperformers, which are given by large negative residuals. According to our model, Glasson and Stankowski should win more money based on their performance statistics, but they are not achieving the expected earnings. This could be due to several factors: they might be performing well in practice rounds but struggling under tournament pressure, they could be playing in fewer high-payout events, or their performance metrics might not capture other important aspects of tournament success like clutch putting or mental toughness during critical moments.\n\n\n\nUnder-Performers\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n47\nFred Couples\n990215\n1978477\n-988262\n\n\n52\nKenny Perry\n889381\n1965740\n-1076359\n\n\n70\nPaul Stankowski\n669709\n1808690\n-1138981\n\n\n85\nBill Glasson\n552795\n1711530\n-1158735\n\n\n142\nJim McGovern\n266647\n1397818\n-1131171\n\n\n\n\n\nLets look at 2018 data, the highest earners are\n\n\n\nHighest earners 2018\n\n\nname\nnevents\nmoney\ndrivedist\ngir\navgputts\n\n\n\n\nJustin Thomas\n23\n8694821\n312\n69\n1.7\n\n\nDustin Johnson\n20\n8457352\n314\n71\n1.7\n\n\nJustin Rose\n18\n8130678\n304\n70\n1.7\n\n\nBryson DeChambeau\n26\n8094489\n306\n70\n1.8\n\n\nBrooks Koepka\n17\n7094047\n313\n68\n1.8\n\n\nBubba Watson\n24\n5793748\n313\n68\n1.8\n\n\n\n\n\nOverperformers\n\n\n\nOverperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n1\nJustin Thomas\n8694821\n5026220\n3668601\n\n\n2\nDustin Johnson\n8457352\n6126775\n2330577\n\n\n3\nJustin Rose\n8130678\n4392812\n3737866\n\n\n4\nBryson DeChambeau\n8094489\n3250898\n4843591\n\n\n5\nBrooks Koepka\n7094047\n4219781\n2874266\n\n\n6\nBubba Watson\n5793748\n3018004\n2775744\n\n\n9\nWebb Simpson\n5376417\n2766988\n2609429\n\n\n11\nFrancesco Molinari\n5065842\n2634466\n2431376\n\n\n12\nPatrick Reed\n5006267\n2038455\n2967812\n\n\n84\nSatoshi Kodaira\n1471462\n-1141085\n2612547\n\n\n\n\n\nUnderperformers\n\n\n\nUnderperformers 2018\n\n\n\nname\nmoney\nPredicted\nError\n\n\n\n\n102\nTrey Mullinax\n1184245\n3250089\n-2065844\n\n\n120\nJ.T. Poston\n940661\n3241369\n-2300708\n\n\n135\nTom Lovelady\n700783\n2755854\n-2055071\n\n\n148\nMichael Thompson\n563972\n2512330\n-1948358\n\n\n150\nMatt Jones\n538681\n2487139\n-1948458\n\n\n158\nHunter Mahan\n457337\n2855898\n-2398561\n\n\n168\nCameron Percy\n387612\n3021278\n-2633666\n\n\n173\nRicky Barnes\n340591\n3053262\n-2712671\n\n\n176\nBrett Stegmaier\n305607\n2432494\n-2126887\n\n\n\n\n\nOur analysis reveals three particularly interesting insights from the golf performance data. First, Tiger Woods demonstrates exceptional performance, appearing as an outlier eight standard deviations above the model’s predictions. This indicates his extraordinary success relative to his statistical metrics. Second, the model shows that increasing driving distance by ten yards corresponds to a fifteen percent increase in earnings, suggesting that power off the tee provides a significant competitive advantage. Finally, improving greens in regulation (GIR) by one percentage point leads to an eighteen percent increase in earnings. This highlights the importance of approach shot accuracy in determining financial success on the PGA Tour. The model successfully identifies both under-performers and over-performers, providing valuable insights into which players may be exceeding or falling short of expectations based on their measurable skills.\n\nSo far, we have viewed regression coefficients \\(\\beta\\) as fixed, unknown constants that we estimate from data. However, in many applications, we may have prior knowledge about these parameters or wish to handle uncertainty more formally. This motivates the Bayesian approach to regression.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#quantile-regression",
    "href": "12-regression.html#quantile-regression",
    "title": "12  Linear Regression",
    "section": "12.5 Quantile Regression",
    "text": "12.5 Quantile Regression\nWhile least squares regression provides an estimate of the conditional mean, it can be sensitive to outliers. In contrast, quantile regression offers a robust alternative by estimating conditional quantiles, such as the median.\nThe least absolute deviations (Quantile) loss function is the sum of absolute differences between the predicted and actual values. It is used for regression problems with continuous variables. The goal is to minimize the sum of absolute errors (SAE) to improve the predictive performance of the model. Given observed data set the least absolute deviations estimator is the value of \\(\\beta\\) that minimizes the sum of absolute errors \\[\n\\mini_{\\beta}\\sum_{i=1}^n |y_i - f_{\\beta}(x_i)|\n\\] The least absolute deviations estimator is also known as the quantile estimator, where the quantile is set to 0.5 (the median). This is because the least absolute deviations estimator is equivalent to the median of the data (the 0.5 quantile).\nAgain, in the unconditional case, when we do not observe any inputs \\(x\\), the least absolute deviations estimator is the sample median. To solve this minimization problem, we need to consider the concept of a subgradient, since the absolute value function is not differentiable at zero.\nA subgradient generalizes the idea of a derivative for functions that are not differentiable everywhere. For a convex function \\(f(x)\\), a subgradient at a point \\(x_0\\) is any value \\(g\\) such that for all \\(x\\), \\[\nf(x) \\geq f(x_0) + g(x - x_0).\n\\] The set of all such \\(g\\) is called the subdifferential at \\(x_0\\).\nFor the absolute value function, the subgradient is given by: \\[\n\\frac{\\partial}{\\partial x} |x| =\n\\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n-1 & \\text{if } x &lt; 0 \\\\\n[-1, 1] & \\text{if } x = 0\n\\end{cases}\n\\] That is, at \\(x = 0\\), any value between \\(-1\\) and \\(1\\) is a valid subgradient.\nApplying this to our minimization problem, the subgradient of the sum of absolute deviations with respect to \\(\\beta\\) is: \\[\n\\sum_{i=1}^n \\operatorname{sign}(y_i - \\beta).\n\\] This equals to zero only when the number of positive items equals the number of negative which happens when \\(\\beta\\) is the median.\n\nA more rigorous and non-calculus proof is due to Schwertman, Gilks, and Cameron (1990). Let \\(y_1,\\ldots,y_n\\) be the observed data and \\(\\hat{\\beta}\\) be the least absolute deviations estimator. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - \\beta|\n\\] for any \\(\\beta\\). Let \\(y_{(1)},\\ldots,y_{(n)}\\) be the ordered data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(i)}|\n\\] Let \\(y_{(n/2)}\\) be the median of the data. Then we have \\[\n\\sum_{i=1}^n |y_i - \\hat{\\beta}| \\leq \\sum_{i=1}^n |y_i - y_{(n/2)}|\n\\] which implies that \\(\\hat{\\beta}\\) is the median of the data.\nThe generalization of the median estimator to the case of estimating value of quantile \\(\\tau\\) is as follows \\[\n\\mini_{\\beta}\\sum_{i=1}^n \\rho_{\\tau}(y_i - \\beta)\n\\] where \\(\\rho_{\\tau}(x) = x(\\tau - \\mathbb{I}(x &lt; 0))\\) is the quantile loss function. If we set \\(\\tau = 0.5\\), the loss function becomes the absolute value function and we get the median estimator. The expected loss is \\[\nE \\rho_{\\tau}(y - \\beta) = (\\tau-1)\\int_{-\\infty}^{\\beta} (y-\\beta)dF(y) + \\tau\\int_{\\beta}^{\\infty} (y-\\beta)dF(y)\n\\] Differentiating the expected loss function with respect to \\(\\beta\\) and setting it to zero gives the quantile estimator \\[\n\\hat{\\beta}_{\\tau} = F^{-1}(\\tau)\n\\] where \\(F^{-1}\\) is the quantile function of the distribution of \\(y\\). Thus, the problem of finding a quantile is solved via optimization.\nA key difference between the least squares and least absolute deviations estimators is their sensitivity to outliers. The least squares estimator is sensitive to outliers because it squares the errors, giving more weight to large errors. In contrast, the least absolute deviations estimator is less sensitive to outliers because it takes the absolute value of the errors, giving equal weight to all errors. This makes the least absolute deviations estimator more robust to outliers than the least squares estimator.\nAnother difference is the computational complexity. Least squares estimator can be found by solving a linear system of equations. There are fast and efficient algorithms for it, making the least squares estimator computationally efficient. In contrast, the least absolute deviations estimator requires more computationally expensive numerical optimization algorithms.\nThere is also a hybrid loss function, called Huber loss, which combines the advantages of squared errors and absolute deviations. It uses SE for small errors and AE for large errors, making it less sensitive to outliers.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "12-regression.html#bayes-regression",
    "href": "12-regression.html#bayes-regression",
    "title": "12  Linear Regression",
    "section": "12.6 Bayes Regression",
    "text": "12.6 Bayes Regression\nThe Bayesian approach to linear regression treats the regression coefficients as random variables rather than fixed unknowns. This perspective offers several advantages: it provides a natural way to incorporate prior knowledge, quantifies uncertainty about parameter estimates through probability distributions, and allows for principled inference even with limited data.\nConsider a linear regression model \\[\nf_{\\beta}(x) = x^T\\beta + \\epsilon,~~~\\epsilon \\sim N(0,\\sigma_e^2),\n\\] where \\(y = f_{\\beta}(x)\\) represents the observed response, \\(x\\) is a vector of predictors, and \\(\\epsilon\\) captures random noise with variance \\(\\sigma_e^2\\). In the Bayesian framework, we specify a prior distribution over the coefficients \\[\n\\beta \\sim N(0,\\Sigma).\n\\] The choice of prior covariance \\(\\Sigma\\) encodes our beliefs about the likely values of \\(\\beta\\) before seeing data. A diagonal \\(\\Sigma = \\sigma_\\beta^2 I\\) assumes independent, identically distributed coefficients with variance \\(\\sigma_\\beta^2\\). Smaller values of \\(\\sigma_\\beta^2\\) express stronger beliefs that coefficients are close to zero, effectively imposing shrinkage similar to ridge regression.\nThe goal of Bayesian inference is to calculate the posterior distribution over model parameters given the observed data \\((X, y)\\): \\[\np(\\beta\\mid X,y) = \\dfrac{p(y\\mid \\beta,X)p(\\beta)}{p(y\\mid X)}.\n\\] The numerator contains the likelihood \\(p(y\\mid \\beta, X)\\), which measures how well each \\(\\beta\\) explains the data, and the prior \\(p(\\beta)\\), which encodes our initial beliefs. The denominator \\(p(y\\mid X)\\) is the marginal likelihood or evidence, which serves as a normalizing constant ensuring the posterior integrates to one.\nA key property of the normal distribution is conjugacy: the product of two Gaussian density functions yields another Gaussian. This allows us to derive the posterior analytically: \\[\n\\begin{aligned}\np(\\beta\\mid X,y)  & \\propto \\exp\\left(-\\dfrac{1}{2\\sigma_e^2}(y-X^T\\beta)^T(y-X^T\\beta)\\right)\\exp\\left(-\\dfrac{1}{2}\\beta^T\\Sigma^{-1}\\beta\\right)\\\\\n& \\propto \\exp\\left(-\\dfrac{1}{2}(\\beta - \\bar\\beta)^TA(\\beta-\\bar\\beta)\\right)\n\\end{aligned}\n\\] where we have completed the square in the exponent. This reveals that the posterior is Gaussian: \\[\n\\beta\\mid X,y \\sim N(\\bar\\beta,A^{-1}),\n\\] where the precision matrix is \\[A = \\sigma_e^{-2}XX^T + \\Sigma^{-1},\\] and the posterior mean is \\[\\bar\\beta = \\sigma_e^{-2}A^{-1}Xy = A^{-1}\\left(\\sigma_e^{-2}Xy\\right).\\]\nThe posterior mean \\(\\bar\\beta\\) represents a compromise between the data and the prior. When we have abundant data (large \\(n\\)), the term \\(\\sigma_e^{-2}XX^T\\) dominates \\(\\Sigma^{-1}\\), and \\(\\bar\\beta\\) approaches the ordinary least squares estimate. Conversely, with limited data, the prior exerts stronger influence, pulling \\(\\bar\\beta\\) toward zero. The posterior covariance \\(A^{-1}\\) quantifies remaining uncertainty: diagonal entries give the variance of each coefficient, while off-diagonal entries capture correlations between parameters.\n\nExample 12.10 (Posterior Distribution for Simple Linear Regression) To build intuition, consider a simple linear model with a single predictor (\\(p = 1\\)): \\[\ny = \\beta_0 + \\beta_1 x + \\epsilon, ~~ \\beta_i \\sim N(0,1),~~~\\sigma_e = 1.\n\\] The prior \\(\\beta_i \\sim N(0, 1)\\) reflects moderate uncertainty: we expect coefficients to typically fall within \\([-2, 2]\\) (covering approximately 95% of probability mass), but we remain open to larger values if the data demand them.\nBefore observing any data, this prior induces a distribution over possible linear functions \\(y = \\beta_0 + \\beta_1 x\\). Each draw \\((\\beta_0, \\beta_1)\\) from the prior defines a different line. Sampling many such pairs generates an ensemble of lines representing our prior beliefs about plausible relationships between \\(x\\) and \\(y\\). Let’s visualize this:\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample from prior distribution over possible linear models\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample from posterior distribution over possible linear models\n\n\n\n\n\n\n\nFigure 12.10: Bayesian linear regression: prior and posterior distributions over linear models\n\n\n\nThese visualizations illustrate the fundamental mechanism of Bayesian learning in linear regression. The left panel displays a sample of twenty linear functions drawn from the prior distribution \\(\\beta_i \\sim N(0,1)\\), representing our initial beliefs before observing any data. Notice the wide dispersion of lines with varying slopes and intercepts, reflecting substantial uncertainty about the true relationship. The right panel shows the same twenty functions after conditioning on observed data points (shown as black dots). The posterior distribution dramatically concentrates around the data, with all sampled lines now passing close to the observations. This concentration demonstrates how Bayesian inference combines prior beliefs with empirical evidence to reduce uncertainty. The remaining spread in the posterior reflects both the inherent noise in the data (\\(\\sigma_e = 1\\)) and residual parameter uncertainty. This shrinkage from a diffuse prior to a concentrated posterior exemplifies the core principle of Bayesian learning: data updates our beliefs in a principled, probabilistic manner.\n\n\nPenalized Regression and Priors\nIn the standard linear regression setting, we minimize the sum of squared errors. However, when the number of predictors is large, or when we want to prevent overfitting, we often add a penalty term to the loss function. This is known as penalized regression or regularization.\nThe two most common forms of penalized regression are Ridge Regression and Lasso Regression.\n\nRidge Regression adds a penalty proportional to the sum of squared coefficients (\\(\\lambda \\sum \\beta_j^2\\)). This shrinks coefficients toward zero but rarely sets them exactly to zero.\nLasso Regression adds a penalty proportional to the sum of absolute values of coefficients (\\(\\lambda \\sum |\\beta_j|\\)). This has the unique property of setting some coefficients exactly to zero, effectively performing variable selection.\n\nInterestingly, these frequentist regularization methods have direct Bayesian interpretations: * Ridge regression corresponds to placing a Gaussian prior on the coefficients: \\(\\beta_j \\sim N(0, \\tau^2)\\). * Lasso regression corresponds to placing a Laplace prior (Double Exponential) on the coefficients: \\(\\beta_j \\sim \\text{Laplace}(0, b)\\).\nWhile Lasso is powerful for variable selection, it has a drawback: it shrinks all coefficients, including the large signals that we want to keep. This can introduce bias in our estimates of the important predictors. Ideally, we want a method that aggressively shrinks small coefficients (noise) to zero but leaves large coefficients (signals) relatively untouched.\n\n\nGlobal-Local Priors and the Horseshoe\nTo address the limitations of Lasso, modern Bayesian statistics uses Global-Local shrinkage priors, which have two variance components for each coefficient: a global shrinkage parameter (\\(\\tau\\)) that pulls all coefficients toward zero (estimating the overall sparsity level), and a local shrinkage parameter (\\(\\lambda_j\\)) for each coefficient that allows specific signals to escape the global shrinkage. The Horseshoe Prior is a state-of-the-art global-local prior that uses half-Cauchy distributions for both parameters, encouraging coefficients to be either very close to zero (strong shrinkage) or very large (no shrinkage), making it nearly ideal for sparse signal recovery.\nFor a detailed theoretical treatment of the horseshoe prior, including its mathematical formulation, shrinkage properties, computational implementation, and comprehensive examples, see Section 17.10.\n\n\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple Noncalculus Proof That the Median Minimizes the Sum of the Absolute Deviations.” The American Statistician 44 (1): 38–39.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "13-logistic.html",
    "href": "13-logistic.html",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "",
    "text": "13.1 Model Fitting\nClassification predicts categories rather than numbers. Does this patient have the disease or not? Is this email spam? Is that object in the camera feed a pedestrian, a vehicle, or a traffic sign? The output is discrete—often just 0 or 1 (binary classification), sometimes multiple classes. Self-driving cars classify objects in real-time from camera feeds; medical systems flag high-risk patients; fraud detection systems sort transactions into suspicious and legitimate.\nGiven observed data \\((x_i,y_i)_{i=1}^n\\), where each \\(y_i\\) is either 0 or 1, we start by assuming a binomial likelihood function for the response variable, defined as follows: \\[\nP(y_i = 1\\mid p_i) = p_i^{y_i} (1-p_i)^{1-y_i},\n\\] where \\(p_i\\) is the function of the inputs \\(x_i\\) and coefficients \\(\\beta\\) that gives us the probability of the response variable taking on a value of 1, given the input variables. A typical approach to calculate \\(p_i\\) is to use the logistic function \\[\\begin{align*}\nf_{\\beta}(x_i) = & \\beta^Tx_i\\\\\np_i  = & \\sigma(f_{\\beta}(x_i)) =  \\frac{e^{f_{\\beta}(x_i)}}{1+e^{f_{\\beta}(x_i)}},\n\\end{align*}\\]\nwhere \\(\\beta\\) is a vector of parameters. The logistic function \\(\\sigma(\\cdot)\\) maps any real number to the interval \\((0, 1)\\), interpreting the output as a probability.\nThen we fit the model using binomial log-likelihood minimization. It leads us to the maximum likelihood estimator for parameters \\(\\beta\\) (a.k.a cross-entropy estimator), defined as \\[\n\\hat \\beta = \\arg\\min_{\\beta}\\mathcal{L}(\\beta),\n\\] where \\[\n\\mathcal{L}(\\beta) =  -\\sum_{i=1}^n \\left[ y_i \\log p_i  + (1-y_i) \\log \\left ( 1-p_i \\right ) \\right].\n\\] Similar to the least squares estimator, the cross-entropy estimator optimization problem is convex, so it has a unique solution.\nIn the unconditional case (an intercept-only model with no inputs \\(x\\)), the cross-entropy estimator simplifies to the sample mean. If we take the derivative of the above expression with respect to \\(\\beta_0\\) and set it to zero, we get \\[\n- \\frac{d}{d\\beta_0}\\sum_{i=1}^n \\left[ y_i \\log \\left ( \\beta_0 \\right ) + (1-y_i) \\log \\left ( 1-\\beta_0 \\right ) \\right] = -\\sum_{i=1}^n \\left[ \\frac{y_i}{\\beta_0} - \\frac{1-y_i}{1-\\beta_0} \\right] = 0\n\\] which gives us the solution \\[\n\\hat{\\beta}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i.\n\\] which is the sample mean.\nUnlike the least squares estimator or the unconditional case, the system of equations \\[\n\\nabla \\mathcal{L}(\\beta) = 0\n\\] is not linear and cannot be solved by inverting a matrix. However, there are efficient iterative numerical optimization algorithms that can be used to find the optimal solution. The most common one is the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm. It is a quasi-Newton method that’s particularly well-suited for optimizing the cross-entropy loss function in logistic regression.\nWhen we have more than two classes \\(y \\in \\{1,\\ldots,K\\}\\), we build \\(K-1\\) models \\(f_{\\beta_1}(x),\\ldots, f_{\\beta_{K-1}}(x)\\), one for each of the first \\(K-1\\) classes, while treating the \\(K\\)-th class as the reference class with \\(f_{\\beta_K}(x) = 0\\). We then use the softmax function to convert the outputs into probabilities:\nFor classes \\(j = 1, \\ldots, K-1\\): \\[\nP(y = j \\mid x) = \\frac{\\exp(f_{\\beta_j}(x))}{1 + \\sum_{i=1}^{K-1} \\exp(f_{\\beta_i}(x))}\n\\]\nFor the reference class \\(K\\): \\[\nP(y = K \\mid x) = \\frac{1}{1 + \\sum_{i=1}^{K-1} \\exp(f_{\\beta_i}(x))}\n\\]\nSome implementations of the logistic regression use \\(K\\) models, one for each class, and then use the softmax function to convert the outputs into probabilities. This is equivalent to the above approach, but it is more computationally expensive.\nThe vector of non-scaled outputs \\((f_{\\beta_1}(x),\\ldots, f_{\\beta_{K-1}}(x))\\) is called the logits.\nThe softmax function is a generalization of the logistic function to the case of more than two classes. It is often used as the activation function in the output layer of neural networks for multi-class classification problems. It converts the output of each model into a probability distribution over the classes, making it suitable for multi-class classification with probabilistic outputs.\nThe logistic function has a nice statistical interpretation. It is the CDF of the logistic distribution, which is a symmetric distribution with mean 0 and variance \\(\\pi^2/3\\), thus \\(p_i\\) is simply a value of this CDF, evaluated at \\(\\beta^Tx_i\\).\nFurther, Logistic regression models the log-odds (logit) of the probability as a linear function of the predictors, which aligns with the maximum likelihood estimation framework and provides desirable statistical properties. Specifically, if we invert the logistic function, \\[\np_i  = \\sigma(\\beta^Tx_i) =  \\frac{e^{\\beta^Tx_i}}{1+e^{\\beta^Tx_i}},\n\\] we get the log-odds \\[\n\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\beta^Tx_i.\n\\] Meaning that \\(\\beta^Tx_i\\) measures how probability of \\(y_i = 1\\) changes with respect to the change in \\(x_i\\) on the log-odds scale. It allows us to interpret the model coefficients as the log-odds ratios of the response variable.\nIn some disciplines, such as econometrics, psychology and natural sciences, a normal CDF is used instead of the logistic CDF. This is often done for historical reasons or because the normal CDF implies slightly different theoretical assumptions that may be more appropriate for specific datasets.\nIn the case of the normal CDF, the model is called probit, it stands for probability unit, and the link function is called probit link. The probit model is defined as \\[\n\\Phi^{-1}(p_i) =  \\beta^Tx_i.\n\\] where \\(\\Phi(\\cdot)\\) is the normal CDF.\nThe term probit was coined in the 1930’s by biologists studying the dosage-cure rate link. We can fit a probit model using glm function in R.\nset.seed(92) # Kuzy\nx = seq(-3,3,length.out = 100)\ny = pnorm(x+rnorm(100))&gt;0.5\nprobitModel = glm(y~x, family=binomial(link=\"probit\"))\nmc = as.double(coef(probitModel))\n# we want to predict outcome for x = -1\nxnew = -1\n(yt = mc[1] + mc[2]*xnew)\n## -0.86\n(pnorm(yt))\n## 0.19\n(pred = predict(probitModel, list(x = c(xnew)), type=\"response\"))\n##    1 \n## 0.19\nnd = dnorm(mc[1] + mc[2]*x)\nplot(x,nd, type='l', col=\"red\", xlab=\"x\", ylab = \"P(y=1)\")\npolygon(c(-3,x[x&lt; -1],-1),c(0,nd[x&lt; -1],0), col=\"blue\")\nOur prediction is the blue area which is equal to 0.195.\nOutside fields like behavioral economics, the logistic function is generally preferred over the probit model due to the interpretability of log-odds and its natural extension to multi-class problems. The PDF of the logistic distribution is very similar to the normal PDF.\nlogitModel  = glm(y~x, family=binomial(link=\"logit\"))\npred_logit = predict(logitModel, list(x = x), type=\"response\")\npred_probit = predict(probitModel, list(x=x), type=\"response\")\nplot(x,pred_probit, pch=20, col=\"red\", cex=0.9, ylab=\"y\")\nlines(x,pred_logit, type='p', pch=20, cex=0.5, col=\"blue\")\nlines(x,y, type='p', pch=21, cex=0.5, bg=\"lightblue\")\nlegend(\"bottomright\",pch=20, legend=c(\"Logit\", \"Probit\"), col=c(\"blue\",\"red\"),y.intersp = 2,bty=\"n\")\nNotice that the predict function returns a numeric value between 0 and 1. However, if we want to make a decision (to bet or not to bet), we need to have a binary outcome. A simple method to move between the predicted probability and binary value is to use thresholding. \\[\n\\hat y_i = \\begin{cases}\n1 & \\text{if } \\hat p_i &gt; \\alpha \\\\\n0 & \\text{if } \\hat p_i \\leq \\alpha\n\\end{cases}\n\\] where \\(\\alpha\\) is a threshold value. A typical choice is \\(\\alpha = 0.5\\).\nNow let’s calculate the number of correct predictions using threshold \\(\\alpha = 0.5\\). R has a convenient table function that can summarize the counts of the predicted and actual values in a table.\ntable(NBA$favwin, as.integer(predict(nbareg, type=\"response\")&gt;0.5), dnn=c(\"Actual\", \"Predicted\"))\n##       Predicted\n## Actual   1\n##      0 131\n##      1 422\nOur model gets 0.7631103 of the predictions correctly. This number is called accuracy of the model.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#model-fitting",
    "href": "13-logistic.html#model-fitting",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "",
    "text": "Example 13.1 (Example: NBA point spread) We will use the NBA point spread data to illustrate the logistic regression. The data is available in the NBAspread.csv file. The data contains the point spread for each game in the NBA from 2013 to 2014 season. The data also contains the outcome of the game, whether the favorite won or not. The point spread is the number of points by which the favorite is expected to win the game and is predicted by the bookmakers. We simply want to see how well the point spread predicts the outcome of the game.\nWe start by loading the data and visualizing it.\n\nNBA = read.csv(\"../data/NBAspread.csv\")\nn = nrow(NBA)\nhead(NBA)\n\n\n\n\n\nfavwin\nfavscr\nundscr\nspread\nfavhome\nfregion\nuregion\n\n\n\n\n1\n72\n61\n7.0\n0\n3\n4\n\n\n1\n82\n74\n7.0\n1\n3\n1\n\n\n1\n87\n57\n17.0\n1\n3\n3\n\n\n0\n69\n70\n9.0\n1\n3\n3\n\n\n0\n77\n79\n2.5\n0\n2\n3\n\n\n1\n91\n65\n9.0\n0\n3\n4\n\n\n\n\n\n\nhist(NBA$spread[NBA$favwin==1], col=5, main=\"\", xlab=\"spread\")\nhist(NBA$spread[NBA$favwin==0], add=TRUE, col=6)\nlegend(\"topright\", legend=c(\"favwin=1\", \"favwin=0\"), fill=c(5,6), bty=\"n\")\nboxplot(NBA$spread ~ NBA$favwin, col=c(6,5), horizontal=TRUE, ylab=\"favwin\", xlab=\"spread\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the Vegas point spread predict whether the favorite wins or not? The histogram shows the distribution of point spreads for games where the favorite won (turquoise) versus games where the favorite lost (purple). The boxplot provides another view of this relationship. Let’s fit a logistic regression model to quantify this relationship:\n\nnbareg = glm(favwin~spread-1, family=binomial, data=NBA)\nnbareg %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nspread\n0.16\n0.01\n11\n0\n\n\n\n\ns = seq(0,30,length=100)\nfit = exp(s*nbareg$coef[1])/(1+exp(s*nbareg$coef[1]))\nplot(s, fit, typ=\"l\", col=4, lwd=2, ylim=c(0.5,1), xlab=\"spread\", ylab=\"P(favwin)\")\n\n\n\n\n\n\n\n\nThe \\(\\beta\\) measures how our log-odds change. For this model, we have \\(\\beta = 0.156\\), meaning that for every one point increase in the point spread, the log-odds of the favorite winning increases by 0.156.\nNow, we can use the model to predict the probability of the favorite winning for a new game with a point spread of 8 or 4.\n\npredict(nbareg, newdata = data.frame(spread = c(8,4)), type = \"response\")\n##    1    2 \n## 0.78 0.65\n\nThe code above simply “Plugs-in” the values for the new game into our logistic regression \\[\n{ P \\left ( \\mathrm{ favwin}  \\mid  \\mathrm{ spread} \\right ) = \\frac{ e^{ \\beta x } }{ 1 + e^{\\beta x} } }\n\\] We can calculate it manually as well.\n\nexp(0.156*8)/(1+exp(0.156*8))\n## 0.78\nexp(0.156*4)/(1+exp(0.156*4))\n## 0.65\n\nCheck that when \\(\\beta =0\\) we have \\(p= \\frac{1}{2}\\).\nGiven our new values spread\\(=8\\) or spread\\(=4\\), the win probabilities are \\(78\\)% and \\(65\\)%, respectively. Clearly, the bigger spread means a higher chance of winning.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#confusion-matrix",
    "href": "13-logistic.html#confusion-matrix",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.2 Confusion Matrix",
    "text": "13.2 Confusion Matrix\nWe will analyse the tennis data set to show what is the decision boundary for the logistic regression model. The decision boundary is the line that separates the two classes. It is defined as the line where the probability of the favorite winning is 0.5. Then we will use the confusion matrix to evaluate the performance of the model.\n\nExample 13.2 (Logistic Regression for Tennis Classification) Data science plays a major role in tennis, you can learn about recent AI tools developed by IBM from this Yahoo! Finance Article.\nWe will analyze the Tennis Major Tournament Match Statistics Data Set from the UCI ML repository. The data set has one per each game from four major Tennis tournaments in 2013 (Australia Open, French Open, US Open, and Wimbledon).\nLet’s look at a few columns of the randomly selected five rows of the data\n\nd = read.csv(\"./../data/tennis.csv\")\nd[sample(1:943,size = 5),c(\"Player1\",\"Player2\",\"Round\",\"Result\",\"gender\",\"surf\")]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer1\nPlayer2\nRound\nResult\ngender\nsurf\n\n\n\n\n532\nFlorian Mayer\nJuan Monaco\n1\n1\nM\nHard\n\n\n816\nL.Kubot\nJ.Janowicz\n5\n0\nM\nGrass\n\n\n431\nSvetlana Kuznetsova\nEkaterina Makarova\n1\n1\nW\nClay\n\n\n568\nMarcos Baghdatis\nGo Soeda\n1\n1\nM\nHard\n\n\n216\nMandy Minella\nAnastasia Pavlyuchenkova\n2\n0\nW\nHard\n\n\n\n\n\n\nWe have data for 943 matches and for each match we have 44 columns, including names of the players, their gender, surface type and match statistics. Let’s look at the number of break points won by each player. We will plot BPW (break points won) by each player on the scatter plot and will colorize each dot according to the outcome\n\nn = dim(d)[1]\nplot(d$BPW.1+rnorm(n),d$BPW.2+rnorm(n), pch=21, col=d$Result+2, cex=0.6, bg=\"yellow\", lwd=0.8,\n     xlab=\"BPW by Player 1\", ylab=\"BPW by Player 2\")\nlegend(\"bottomright\", c(\"P1 won\", \"P2 won\"), col=c(3,2), pch=21, bg=\"yellow\", bty='n')\n\n\n\n\n\n\n\n\nWe can clearly see that the number of break points won is a clear predictor of the match outcome. This is obvious and follows from the rules; to win a match, a player must win break points. Now, we want to understand the impact of winning a break point on the overall match outcome. We do it by building a logistic regression model\n\nwhich(is.na(d$BPW.1)) # there is one row with NA value for the BPW.1 value and we remove it\n## 171\nd = d[-171,]; n = dim(d)[1]\nm = glm(Result ~ BPW.1 + BPW.2-1, data=d, family = \"binomial\" )\nm %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nBPW.1\n0.40\n0.03\n15\n0\n\n\nBPW.2\n-0.42\n0.03\n-15\n0\n\n\n\n\n\nThe predicted values are stored in the fitted.values field of the model object. Those are the probabilities of player 1 winning the match. We need to convert them to binary predictions using \\(0.5\\) as a threshold for our classification.\n\ntable(d$Result, as.integer(m$fitted.values&gt;0.5), dnn=c(\"Actual\", \"Predicted\"))\n##       Predicted\n## Actual   0   1\n##      0 416  61\n##      1  65 400\n\nThis table shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. The first row shows the number of matches where player 1 won and the model predicted that player 1 won. The second row shows the number of matches where player 1 lost and the model predicted that player 1 lost. Thus, our model got (400+416)/942 = 86.6242038% of the predictions correctly! The accuracy is the ratio of the number of correct predictions to the total number of predictions.\nThis table is called confusion matrix. It is a table that shows the number of correct and incorrect predictions for each class. The rows are the actual outcomes and the columns are the predicted outcomes. Formally, it is defined as\n\nConfusion Matrix. TPR - True Positive Rate, FPR - False Positive Rate, TNR - True Negative Rate, FNR - False Negative Rate.\n\n\n\nPredicted: YES\nPredicted: NO\n\n\n\n\nActual: YES\nTPR\nFNR\n\n\nActual: NO\nFPR\nTNR\n\n\n\nFundamentally, logistic regression attempts to construct a linear boundary that separates the two classes. In our case, we have two predictors \\(x_1\\) = BPW.1 and \\(x_2\\) = BPW.2 and our model is \\[\n\\log\\left(\\dfrac{p}{1-p}\\right) = \\beta_1x_1 + \\beta_2 x_2,\n\\] where \\(p\\) is the probability of player 1 winning the match. We want to find the line along which the probability is 1/2, meaning that \\(p/(1-p) = 1\\) and log-odds \\(\\log(p/(1-p)) = 0\\), thus the equation for the line is \\(\\beta_1x_1 + \\beta_2 x_2 = 0\\) or \\[\nx_2 = \\dfrac{-\\beta_1}{\\beta_2}x_1\n\\]\nLet’s see the line found by the glm function\n\nplot(d$BPW.1+rnorm(n),d$BPW.2+rnorm(n), pch=21, col=d$Result+2, cex=0.6, bg=\"yellow\", lwd=0.8,\n     xlab=\"BPW by Player 1\", ylab=\"BPW by Player 2\")\n\nx = seq(0,30,length.out = 200)\ny  =  -m$coefficients[1]*x/m$coefficients[2]\nlines(x,y, lwd=2, col=\"red\") \n\n\n\n\n\n\n\n\nThere are a couple of observations. First, the effect of a break point on the game outcome is significant and symmetric; the effect of losing a break point is the same as the effect of winning one. We also can interpret the effect of winning a break point in the following way. We will keep BPW.2 = 0 and will calculate what happens to the probability of winning when BPW.1 changes from 0 to 1. The odds ratio for player 1 winning when BPW.1 = 0 is exp(0) which is 1, meaning that the probability that P1 wins is 1/2. Now when BPW.1 = 1, the odds ratio is\n\nexp(0.4019)\n## 1.5\n\nWe can calculate probability of winning from the regression equation \\[\n\\dfrac{p}{1-p} = 1.5,~~~p = 1.5(1-p),~~~2.5p = 1.5,~~~p = 0.6\n\\] Thus probability of winning goes from 50% to 60%, we can use predict function to get this result\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n##   1 \n## 0.5\npredict.glm(m,newdata = data.frame(BPW.1 = c(1), BPW.2 = c(0)), type=\"response\")\n##   1 \n## 0.6\n\nWhat happens to the chances of winning when P1 wins three more break points compared to the opponent\n\npredict.glm(m,newdata = data.frame(BPW.1 = c(0), BPW.2 = c(0)), type=\"response\")\n##   1 \n## 0.5\npredict.glm(m,newdata = data.frame(BPW.1 = c(3), BPW.2 = c(0)), type=\"response\")\n##    1 \n## 0.77\n\nChances go up by 27%.\nTennis is arguably the sport in which men and women are treated equally. Both men’s and women’s matches are shown during prime-time on TV, and they both have the same prize money. However, one of the comments you hear often is that women’s matches are “less predictable”, meaning that an upset (when the favorite loses) is more likely to happen in a women’s match compared to men’s matches. We can test this statement by looking at the residuals. The larger the residual the less accurate our prediction was.\n\noutlind = which(d$res&lt;2)\nboxplot(d$res[outlind] ~ d$gender[outlind], col=c(2,3), xlab=\"Gender\",ylab=\"Residual\")\n\n\n\n\n\n\n\n\nLet’s do a formal t-test on the residuals for men’s and women’s matches\n\nmen = d %&gt;% filter(res&lt;2, gender==\"M\") %&gt;% pull(res)\nwomen = d %&gt;% filter(res&lt;2, gender==\"W\") %&gt;% pull(res)\nt.test(men, women, alternative = \"two.sided\")\n## \n##  Welch Two Sample t-test\n## \n## data:  men and women\n## t = -5, df = 811, p-value = 0.000003\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.105 -0.043\n## sample estimates:\n## mean of x mean of y \n##       1.2       1.3\n\nThe difference of \\(0.07\\) between men and women and the statistic value of \\(-4.7\\) means that the crowd wisdom that women’s matches are less predictable is correct. The difference is statistically significant!",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#roc-curve-and-confounding-variables",
    "href": "13-logistic.html#roc-curve-and-confounding-variables",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.3 ROC Curve and Confounding Variables",
    "text": "13.3 ROC Curve and Confounding Variables\nUsing default data set, we will illustrate the concept of ROC curve and confounding variables.\n\nExample 13.3 (Horse Race Betting) Horse race betting provides a rich application of logistic regression, where predicting whether the favorite wins illustrates key classification concepts. This example uses data from the Hong Kong Jockey Club, one of the world’s largest horse racing operations (see the Benter case study for comprehensive analysis).\nThe betting public’s implied probabilities, derived from pari-mutuel odds, exhibit the well-documented favorite-longshot bias: favorites tend to win more often than the odds suggest, while longshots win less often. We can build a logistic regression model to predict whether the favorite wins based on race characteristics.\nFirst, we load and prepare the data. We identify the favorite in each race as the horse with the highest implied probability from the betting odds.\n\nlibrary(tidyverse)\n\n# Load HKJC race data\ndf &lt;- read_csv(\"case_studies/betting/data/hkjc/df_hkjc_with_factors.csv\", \n               show_col_types = FALSE)\n\n# Identify favorite in each race and create race-level dataset\nrace_df &lt;- df |&gt;\n  group_by(race_id) |&gt;\n  mutate(is_favorite = rank(-public_prob, ties.method = \"first\") == 1) |&gt;\n  filter(is_favorite) |&gt;\n  ungroup() |&gt;\n  dplyr::select(race_id, venue, field_size, date, public_prob, win, \n                jockey_win_rate, trainer_win_rate, draw_normalized, rating_diff)\n\ncat(sprintf(\"Dataset: %d races from %s to %s\\n\", \n            nrow(race_df), min(race_df$date), max(race_df$date)))\n## Dataset: 5885 races from 1997-11-30 to 2005-08-28\ncat(sprintf(\"Favorite win rate: %.1f%%\\n\", mean(race_df$win) * 100))\n## Favorite win rate: 29.9%\n\nOur outcome variable is whether the favorite won (win = 1) or not (win = 0). The favorite’s implied probability from public odds should be a strong predictor; if the market is perfectly calibrated, this probability would equal the true win rate.\n\nglm.fit &lt;- glm(win ~ public_prob, data = race_df, family = binomial)\nglm.fit %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.4\n0.11\n-22\n0\n\n\npublic_prob\n6.1\n0.41\n15\n0\n\n\n\n\n\nThe positive coefficient on public_prob confirms that higher implied probability predicts higher actual win probability. We can visualize this relationship:\n\nx &lt;- seq(0.15, 0.45, length.out = 100)\ny &lt;- predict(glm.fit, newdata = data.frame(public_prob = x), type = \"response\")\nplot(x, y, type = 'l', col = \"red\", lwd = 2,\n     xlab = \"Public Implied Probability\", ylab = \"P(Favorite Wins)\")\nabline(a = 0, b = 1, lty = 2, col = \"gray50\")  # Perfect calibration line\nlegend(\"topleft\", legend = c(\"Model prediction\", \"Perfect calibration\"), \n       col = c(\"red\", \"gray50\"), lty = c(1, 2), bty = \"n\")\n\n\n\n\n\n\n\n\nThe model prediction lies above the 45-degree line, confirming the favorite-longshot bias: favorites win more often than their odds imply.\nNow we build the confusion matrix. In betting, the threshold choice has financial implications: being too aggressive (low threshold) means betting on too many “favorites” that lose, while being too conservative means missing profitable opportunities.\n\npred &lt;- predict(glm.fit, type = \"response\")\nconfusion_matrix &lt;- table(race_df$win, as.integer(pred &gt; 0.3), \n                          dnn = c(\"Actual\", \"Predicted\"))\n# Convert to rates\ncm_rates &lt;- confusion_matrix\ncm_rates[1,] &lt;- cm_rates[1,] / sum(cm_rates[1,])\ncm_rates[2,] &lt;- cm_rates[2,] / sum(cm_rates[2,])\ncm_rates %&gt;% kable(digits = 3)\n\n\n\n\n\n0\n1\n\n\n\n\n0\n0.66\n0.34\n\n\n1\n0.50\n0.50\n\n\n\n\n\nWe use a threshold of 0.3 because the baseline win rate for favorites is around 30%. The ROC curve helps us understand the trade-off between correctly identifying winners (sensitivity) and avoiding false predictions on losers (specificity).\n\nroc &lt;- function(p, y, ...) {\n  y &lt;- factor(y)\n  n &lt;- length(p)\n  p &lt;- as.vector(p)\n  alpha &lt;- seq(0, 1, length.out = 100)\n  Q &lt;- p &gt; matrix(rep(alpha, n), ncol = 100, byrow = TRUE)\n  specificity &lt;- colMeans(!Q[y == levels(y)[1], ])\n  sensitivity &lt;- colMeans(Q[y == levels(y)[2], ])\n  return(list(specificity = specificity, sensitivity = sensitivity, alpha = alpha))\n}\n\n\nres &lt;- roc(p = pred, y = race_df$win)\nplot(1 - res$specificity, res$sensitivity, type = \"l\", bty = \"n\", \n     main = \"ROC: Predicting Favorite Wins\", lwd = 3)\nabline(a = 0, b = 1, lty = 2, col = 8)\n\n# Mark specific thresholds\npoints(x = 1 - mean((pred &lt; 0.3)[race_df$win == 0]), \n       y = mean((pred &gt; 0.3)[race_df$win == 1]), \n       cex = 1.5, pch = 20, col = 'red')\npoints(x = 1 - mean((pred &lt; 0.5)[race_df$win == 0]), \n       y = mean((pred &gt; 0.5)[race_df$win == 1]), \n       cex = 1.5, pch = 20, col = 'blue')\nlegend(\"bottomright\", fill = c(\"red\", \"blue\"),\n       legend = c(\"alpha=0.3\", \"alpha=0.5\"), bty = \"n\", title = \"cutoff\")\n\n\n\n\n\n\n\n\nThe ROC curve shows reasonable predictive power above the diagonal. The choice of threshold depends on the betting strategy: a lower threshold captures more true winners but also more false positives.\nNow let’s examine multiple predictors. Hong Kong has two racetracks: Happy Valley (a tight urban track) and Sha Tin (a modern facility with longer straights). We might expect venue to affect favorite performance.\n\nrace_df$venue_ST &lt;- as.integer(race_df$venue == \"ST\")\n\nglm.fit2 &lt;- glm(win ~ public_prob + field_size + venue_ST, \n                data = race_df, family = binomial)\nglm.fit2 %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.95\n0.32\n-9.37\n0.00\n\n\npublic_prob\n6.32\n0.43\n14.55\n0.00\n\n\nfield_size\n0.04\n0.02\n1.83\n0.07\n\n\nvenue_ST\n-0.07\n0.07\n-0.96\n0.34\n\n\n\n\n\nThe coefficient for field_size is negative, which seems intuitive; in larger fields, there’s more competition, so favorites may have harder time winning. But the coefficient for venue_ST (Sha Tin) is also negative. Does this mean favorites perform worse at Sha Tin?\nLet’s check for confounding by examining field sizes at each venue:\n\nboxplot(field_size ~ venue, data = race_df, \n        ylab = \"Field Size\", col = c(2, 3),\n        names = c(\"Happy Valley\", \"Sha Tin\"))\n\n\n\n\n\n\n\n\nSha Tin races have systematically larger fields. This creates confounding: the apparent venue effect may actually be a field size effect. When we control for field size in the model, the venue effect represents the residual impact of venue beyond what’s explained by field size.\nTo see this more clearly, let’s compare predictions across venues for races with the same field size:\n\n# Predictions for 10-horse races at each venue\nx_hv &lt;- data.frame(public_prob = seq(0.2, 0.4, length.out = 50), \n                   field_size = 10, venue_ST = 0)\nx_st &lt;- data.frame(public_prob = seq(0.2, 0.4, length.out = 50), \n                   field_size = 10, venue_ST = 1)\ny_hv &lt;- predict(glm.fit2, newdata = x_hv, type = \"response\")\ny_st &lt;- predict(glm.fit2, newdata = x_st, type = \"response\")\n\nplot(x_hv$public_prob, y_hv, type = 'l', col = \"red\", lwd = 2,\n     xlab = \"Public Implied Probability\", ylab = \"P(Favorite Wins)\",\n     ylim = c(0.2, 0.5))\nlines(x_st$public_prob, y_st, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"Happy Valley\", \"Sha Tin\"), \n       col = c(\"red\", \"blue\"), lwd = 2, bty = \"n\")\n\n\n\n\n\n\n\n\nAfter controlling for field size, the venue difference is smaller. The remaining gap may reflect track characteristics: Happy Valley’s tighter turns could favor front-running favorites who avoid traffic problems.\nTo summarize: ROC curves help visualize the trade-off between catching winners and avoiding false positives across different probability thresholds. In betting applications, this trade-off has direct financial consequences, making threshold selection a critical business decision. Confounding is also evident: Sha Tin races appear to disadvantage favorites, but this partly reflects larger field sizes rather than track characteristics. Including confounders in the model isolates the true venue effect. These principles from horse racing apply broadly to any classification problem where features are correlated.\n\nNow, a natural question is how to choose the cut-off value \\(\\alpha\\)? In betting, we place a bet when \\(p(\\text{win}) &gt; \\alpha\\). Here \\(\\alpha\\) is our confidence threshold. If we choose \\(\\alpha = 0\\) and bet on everything, we’ll lose money on bad bets. If we choose \\(\\alpha = 1\\), we never bet and make no money. In order to choose an appropriate \\(\\alpha\\), we need to know the payoffs.\nFor betting on favorites at typical odds (say, 2.5:1 decimal odds), the pay-off matrix looks like:\n\nPay-off matrix for betting\n\n\n\nWin\nLose\n\n\n\n\nBet\n1.5\n-1\n\n\nDon’t bet\n0\n0\n\n\n\nIf we bet $1 and win at 2.5:1 odds, we gain $1.50 profit. If we lose, we lose our $1 stake. If we don’t bet, we neither gain nor lose.\nGiven this pay-off matrix, we can calculate the expected profit across different thresholds using our horse racing data.\n\n# Recalculate predictions using our betting model from the earlier example\npred_bet &lt;- predict(glm.fit2, type = \"response\")\nres_bet &lt;- roc(p = pred_bet, y = race_df$win)\nalpha &lt;- res_bet$alpha\nspecificity &lt;- res_bet$specificity  # correctly identify non-winners\nsensitivity &lt;- res_bet$sensitivity  # correctly identify winners\n\n# Payoffs: win gives +1.5, lose gives -1, no bet gives 0\n# sensitivity = P(predict win | actual win), (1-specificity) = P(predict win | actual lose)\n# Expected profit = P(win) * sensitivity * 1.5 - P(lose) * (1-specificity) * 1\nbase_rate &lt;- mean(race_df$win)  # ~30%\nprofit &lt;- base_rate * sensitivity * 1.5 - (1 - base_rate) * (1 - specificity) * 1\nplot(alpha, profit, type = 'l', xlab = \"Threshold (alpha)\", ylab = \"Expected Profit per $1 Bet\", \n     lwd = 3, col = \"red\", cex.lab = 0.8)\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\nTo identify the most effective strategy, we evaluate the expected profit across all possible thresholds \\(\\alpha\\). The expected profit per dollar wagered is calculated as a weighted average of the gains from true positives and the losses from false positives: \\[E[\\text{Profit} \\mid \\alpha] = P(\\text{win}) \\cdot \\text{Sensitivity}(\\alpha) \\cdot 1.5 - P(\\text{lose}) \\cdot (1 - \\text{Specificity}(\\alpha)) \\cdot 1.0\\] where \\(P(\\text{win})\\) is the base rate of winners in the training data. This calculation allows us to identify the profitable threshold range (where expected profit is positive), the optimal threshold (which maximizes profit), and the resulting maximum expected profit.\n\nif(any(profit &gt; 0)) {\n  cat(sprintf(\"Profitable threshold range: %.2f to %.2f\\n\", \n              min(alpha[profit &gt; 0]), max(alpha[profit &gt; 0])))\n  cat(sprintf(\"Optimal threshold: %.2f\\n\", alpha[which.max(profit)]))\n  cat(sprintf(\"Maximum expected profit: $%.3f per $1 wagered\\n\", max(profit)))\n} else {\n  cat(\"No threshold produces positive expected profit at these odds\\n\")\n}\n## Profitable threshold range: 0.32 to 0.80\n## Optimal threshold: 0.40\n## Maximum expected profit: $0.028 per $1 wagered\n\nThis analysis shows that the choice of threshold has direct financial consequences. Unlike academic classification problems where we might default to \\(\\alpha = 0.5\\), real-world applications require understanding the asymmetric costs of different types of errors.\n\nExample 13.4 (LinkedIn Study) How to Become an Executive(Irwin 2016; Gan and Fritzler 2016)?\nLogistic regression was used to analyze the career paths of about \\(459,000\\) LinkedIn members who worked at a top 10 consultancy between 1990 and 2010 and became a VP, CXO, or partner at a company with at least 200 employees. About \\(64,000\\) members reached this milestone, \\(\\hat{p} = 0.1394\\), conditional on making it into the database. The goals of the analysis were the following\n\nLook at their profiles – educational background, gender, work experience, and career transitions.\nBuild a predictive model of the probability of becoming an executive\nProvide a tool for analysis of “what if” scenarios. For example, if you are to get a master’s degree, how your jobs perspectives change because of that.\n\nLet’s build a logistic regression model with \\(8\\) key features (a.k.a. covariates): \\[\n\\log\\left ( \\frac{p}{1-p} \\right ) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_8x_8\n\\]\nHere \\(p\\) is the probability of “success” – meaning the person reaches VP/CXO/Partner seniority at a company with at least 200 employees. The features to predict the “success” probability are \\(x_i (i=1,2,\\ldots,8)\\)\n\n\\(x_1\\): Metro region: whether a member has worked in one of the top 10 largest cities in the U.S. or globally.\n\\(x_2\\): Gender: Inferred from member names: ‘male’, or ‘female’\n\\(x_3\\): Graduate education type: whether a member has an MBA from a top U.S. program / a non-top program / a top non-U.S. program / another advanced degree\n\\(x_4\\): Undergraduate education type: whether a member has attended a school from the U.S. News national university rankings / a top 10 liberal arts college /a top 10 non-U.S. school\n\n\\(x_5\\): Company count: # different companies in which a member has worked\n\n\\(x_6\\): Function count: # different job functions in which a member has worked\n\\(x_7\\): Industry sector count: # different industries in which a member has worked\n\n\\(x_8\\): Years of experience: # years of work experience, including years in consulting, for a member.\n\nThe following estimated \\(\\hat\\beta\\)s of features were obtained. With a sample size of 456,000 they are measured rather accurately. Recall that in logistic regression, the coefficients represent the change in log-odds for a unit change in the feature.\n\n\n\nCategory\nFeature\nCoefficient (\\(\\hat{\\beta}\\))\n\n\n\n\nLocation\nMetro region\n0.28\n\n\nPersonal\nGender (Male)\n0.31\n\n\nEducation\nGraduate education type\n1.16\n\n\nEducation\nUndergraduate education type\n0.22\n\n\nWork Experience\nCompany count\n0.14\n\n\nWork Experience\nFunction count\n0.26\n\n\nWork Experience\nIndustry sector count\n-0.22\n\n\nWork Experience\nYears of experience\n0.09\n\n\n\nHere are three main findings\n\nWorking across job functions, like marketing or finance, is good. Each additional job function provides a boost that, on average, is equal to three years of work experience. Switching industries has a slight negative impact. Learning curve? lost relationships?\nMBAs are worth the investment. But pedigree matters. Top five program equivalent to \\(13\\) years of work experience!!!\nLocation matters. For example, NYC helps.\n\nWe can also personalize the prediction for predict future possible future executives. For example, Person A (p=6%): Male in Tulsa, Oklahoma, Undergraduate degree, 1 job function for 3 companies in 3 industries, 15-year experience.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\nPerson C (p=63%): Female in New York City, Top undergraduate program, Top MBA program, 4 different job functions for 4 companies in 1 industry, 15-year experience.\nLet’s re-design Person B.\nPerson B (p=15%): Male in London, Undergraduate degree from top international school, Non-MBA Master, 2 different job functions for 2 companies in 2 industries, 15-year experience.\n\nWork in one industry rather than two. Increase \\(3\\)%\nUndergrad from top \\(10\\) US program rather than top international school. \\(3\\)%\nWorked for \\(4\\) companies rather than \\(2\\). Another \\(4\\)%\nMove from London to NYC. \\(4\\)%\nFour job functions rather than two. \\(8\\)%. A \\(1.5\\)x effect.\nWorked for \\(10\\) more years. \\(15\\)%. A \\(2\\)X effect.\n\nChoices and Impact (Person B) are shown below\nChoices and Impact (Person B) are shown below in Figure 13.2. The chart illustrates the marginal impact of each individual strategic choice (e.g., getting an MBA, moving to NYC) on the probability of becoming an executive, compared to Person B’s baseline of 15%. When all these positive choices are combined, the probability skyrockets to 81%.\n\n\n\n\n\n\nFigure 13.2: Choices and Impact (Person B)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#imbalanced-data",
    "href": "13-logistic.html#imbalanced-data",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.4 Imbalanced Data",
    "text": "13.4 Imbalanced Data\nOften, you have much more observations with a specific label, such a sample is called imbalanced. This is a common problem in real-world classification tasks where one class significantly outnumbers the other(s). For example, in fraud detection, legitimate transactions vastly outnumber fraudulent ones; in medical diagnosis, healthy patients often outnumber those with rare diseases; and in manufacturing, defective products are typically much rarer than non-defective ones.\nWhen dealing with imbalanced data, you should avoid using accuracy as a metric to choose a model. Consider a binary classification problem with 95% of samples labeled as class 1. A naive classifier that simply assigns label 1 to every input will achieve 95% accuracy, making it appear deceptively good while being completely useless for practical purposes.\nInstead, more appropriate evaluation metrics should be used. The Receiver Operating Characteristic (ROC) curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various classification thresholds. The Area Under the Curve (AUC) provides a single scalar value that measures the model’s ability to distinguish between classes, regardless of the chosen threshold. An AUC of 0.5 indicates random guessing, while 1.0 represents perfect classification.\nThe F1 score combines precision and recall into a single score, providing a balanced measure that penalizes models that are either too conservative or too aggressive: \\[\nF1 = 2\\dfrac{\\mathrm{precision} \\times \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}\n\\] where precision measures the proportion of true positives among predicted positives, and recall measures the proportion of true positives that were correctly identified.\nThe precision-recall curve is particularly useful for imbalanced datasets, as it plots precision against recall at various thresholds, focusing on the performance of the positive class. Cohen’s Kappa measures agreement between predicted and actual classifications while accounting for agreement by chance, making it more robust to class imbalance than accuracy.\nTo address imbalanced data, several strategies can be employed. Data-level approaches include oversampling, where you synthetically generate more samples of the minority class using techniques like bootstrap sampling with replacement, SMOTE (Synthetic Minority Over-sampling Technique) which creates synthetic examples by interpolating between existing minority class samples, or generative models like GANs or variational autoencoders to create realistic synthetic data. Undersampling reduces the majority class samples, which is particularly effective when the dataset is large enough. Hybrid approaches combine both oversampling and undersampling techniques.\nAlgorithm-level approaches include cost-sensitive learning, where you assign different misclassification costs to different classes, ensemble methods using techniques like bagging or boosting that can naturally handle imbalanced data, and threshold adjustment to modify the classification threshold to optimize for specific metrics like F1-score.\nThe choice of approach depends on the specific problem, available data, and computational resources. It is often beneficial to experiment with multiple techniques and evaluate their performance using appropriate metrics rather than relying solely on accuracy.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#kernel-trick",
    "href": "13-logistic.html#kernel-trick",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.5 Kernel Trick",
    "text": "13.5 Kernel Trick\nWhile logistic regression is a powerful linear classifier, there are cases where the classes are not linearly separable in the original feature space. For example, one class might be encircled by another. In such situations, we need to extend our linear methods to handle non-linear decision boundaries.\nThe kernel trick is a method of using a linear classifier to solve a non-linear problem. The idea is to map the data into a higher dimensional space, where it becomes linearly separable. The kernel trick is to use a kernel function \\(K(x_i,x_j)\\) to calculate the inner product of two vectors in the higher dimensional space without explicitly calculating the mapping \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\). The kernel function is defined as \\(K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\\). The most popular kernel functions are polynomial kernel \\(K(x_i,x_j) = (x_i^Tx_j)^d\\) and Gaussian kernel \\(K(x_i,x_j) = \\exp(-\\gamma||x_i-x_j||^2)\\). The kernel trick is used in Support Vector Machines (SVM) and Gaussian Processes (GP).\n\n\nCode\ngencircledata = function(numSamples,radius,noise) {\n    d = matrix(0,ncol = 3, nrow = numSamples); # matrix to store our generated data\n    # Generate positive points inside the circle.\n    for (i in 1:(numSamples/2) ) {\n    r = runif(1,0, radius * 0.4);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(0,x,y)\n    }\n\n    # Generate negative points outside the circle.\n    for (i in (numSamples/2+1):numSamples ) {\n    r = runif(1,radius * 0.8, radius);\n    angle = runif(1,0, 2 * pi);\n    x = r * sin(angle);\n    y = r * cos(angle);\n    noiseX = runif(1,-radius, radius) * noise;\n    noiseY = runif(1,-radius, radius) * noise;\n    d[i,] = c(1,x,y)\n    }\n    colnames(d) = c(\"label\", \"x1\", \"x2\")\n    return(d)\n}\n\n\n\nd = gencircledata(numSamples=200, radius=1, noise=0.001)\nplot(d[,2],d[,3], col=d[,1]+1, pch=19, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\nFigure 13.3\n\n\n\n\n\nThe data in Figure 13.3 is not linearly separable in two dimensions; however, projecting it into a three-dimensional space via the following transformation allows for a linear separation: \\[\n\\begin{aligned}\n\\phi: R^{2} & \\longrightarrow R^{3} \\\\\n\\left(x_{1}, x_{2}\\right) & \\longmapsto\\left(z_{1}, z_{2}, z_{3}\\right)=\\left(x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}\\right),\n\\end{aligned}\n\\] and attempt to linearly separate the transformed data, the decision boundaries become hyperplanes in \\(R^{3}\\), expressed as \\(\\omega^{T} z + b = 0\\). In terms of the original variables \\(x\\), these boundaries take the form: \\[\n\\omega_{1} x_{1}^{2} + \\omega_{2} \\sqrt{2} x_{1} x_{2} + \\omega_{3} x_{2}^{2} = 0,\n\\] which corresponds to the equation of an ellipse. This demonstrates that we can apply a linear algorithm to transformed data to achieve a non-linear decision boundary with minimal effort.\nNow, consider what the algorithm is actually doing. It relies solely on the Gram matrix \\(K\\) of the data. Once \\(K\\) is computed, the original data can be discarded: \\[\n\\begin{aligned}\nK & = \\left[\\begin{array}{ccc}\nx_{1}^{T} x_{1} & x_{1}^{T} x_{2} & \\cdots \\\\\nx_{2}^{T} x_{1} & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right]_{n \\times n} = X X^{T}, \\\\\n\\text{where} \\quad X & = \\left[\\begin{array}{c}\nx_{1}^{T} \\\\\n\\vdots \\\\\nx_{n}^{T}\n\\end{array}\\right]_{n \\times d}.\n\\end{aligned}\n\\] Here, \\(X\\), which contains all the data, is referred to as the design matrix.\nWhen we map the data using \\(\\phi\\), the Gram matrix becomes: \\[\nK = \\left[\\begin{array}{ccc}\n\\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{1}\\right) & \\phi\\left(x_{1}\\right)^{T} \\phi\\left(x_{2}\\right) & \\cdots \\\\\n\\phi\\left(x_{2}\\right)^{T} \\phi\\left(x_{1}\\right) & \\ddots & \\\\\n\\vdots & &\n\\end{array}\\right].\n\\]\nLet us compute these inner products explicitly. For vectors \\(r\\) and \\(s\\) in \\(R^{3}\\) corresponding to \\(a\\) and \\(b\\), respectively: \\[\n\\begin{aligned}\n\\langle r, s \\rangle & = r_{1} s_{1} + r_{2} s_{2} + r_{3} s_{3} \\\\\n& = a_{1}^{2} b_{1}^{2} + 2 a_{1} a_{2} b_{1} b_{2} + a_{2}^{2} b_{2}^{2} \\\\\n& = \\langle a, b \\rangle^{2}.\n\\end{aligned}\n\\]\nThus, instead of explicitly mapping the data via \\(\\phi\\) and then computing the inner product, we can compute it directly in one step, leaving the mapping \\(\\phi\\) implicit. In fact, we do not even need to know \\(\\phi\\) explicitly; all we require is the ability to compute the modified inner product. This modified inner product is called a kernel, denoted \\(K(x, y)\\). The matrix \\(K\\), which contains the kernel values for all pairs of data points, is also referred to as the kernel matrix.\nSince the kernel itself is the primary object of interest, rather than the mapping \\(\\phi\\), we aim to characterize kernels without explicitly relying on \\(\\phi\\). Mercer’s Theorem provides the theoretical guarantee for this: it states that for any symmetric, positive-definite function \\(K(x, y)\\), there exists a mapping \\(\\phi\\) such that \\(K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle\\). This ensures we can implicitly work in a high-dimensional space just by defining a valid kernel function.\nLet’s implement it\n\nlibrary(\"scatterplot3d\")\nphi &lt;- function(x1, x2) {\n    z1 &lt;- x1^2\n    z2 &lt;- sqrt(2) * x1 * x2\n    z3 &lt;- x2^2\n    return(cbind(z1, z2, z3))\n}\n\n# Generate sample 2D data (you can replace this with your actual data)\n\n\n# Apply the transformation\ntransformed_data &lt;- phi(d[,2], d[,3])\nscatterplot3d(transformed_data, color = ifelse(d[,1] == 0, \"red\", \"blue\"), pch = 19, cex.symbols = 0.5,\n                xlab = \"z1 (x1^2)\", ylab = \"z2 (sqrt(2) * x1 * x2)\", zlab = \"z3 (x2^2)\",\n                main = \"3D Scatter Plot of Transformed Data\", angle=222, grid=FALSE, box=FALSE)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#sec-glm",
    "href": "13-logistic.html#sec-glm",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.6 Generalized linear models",
    "text": "13.6 Generalized linear models\nLogistic regression is one member of a larger family of models called generalized linear models (GLMs). A GLM has three components:\n\na distributional assumption for \\(Y\\) from the exponential family (e.g., Normal, Bernoulli, Poisson, Gamma),\na linear predictor \\(\\eta_i = x_i^\\top \\beta\\),\na link function \\(g(\\cdot)\\) connecting the mean response to the linear predictor, \\(g(\\E{Y_i\\mid x_i})=\\eta_i\\).\n\nThis framework makes it easy to move between problem types (continuous, binary, counts, positive-valued responses) while keeping a common estimation story based on likelihood and its negative log (Chapter 11).\nThe link function encodes how predictors move the mean response. Common choices include:\n\n\n\n\n\n\n\n\nLink Function\nDistribution\nUse Case\n\n\n\n\nlogit\nBernoulli/binomial\nlogistic regression for binary outcomes\n\n\nprobit\nBernoulli/binomial\nNormal-CDF alternative for binary data\n\n\nlog\nPoisson, Gamma\ncount data and positive-valued responses\n\n\ninverse\nGamma\npositive continuous data (alternative parameterization)\n\n\n\nFor count outcomes \\(Y_i \\in \\{0,1,2,\\ldots\\}\\), a standard GLM is Poisson regression: \\[\nY_i \\mid x_i \\sim \\mathrm{Poisson}(\\lambda_i), \\qquad \\log \\lambda_i = x_i^\\top \\beta.\n\\] The log link ensures \\(\\lambda_i&gt;0\\). Coefficients have a multiplicative interpretation: increasing a covariate by one unit changes the mean count by a factor of \\(\\exp(\\beta_j)\\), holding other predictors fixed.\nFor continuous responses that are strictly positive and right-skewed (e.g., waiting times, costs), a common GLM choice is Gamma regression. One parameterization is \\[\nY_i \\mid x_i \\sim \\mathrm{Gamma}(\\text{shape},\\text{rate}), \\qquad \\E{Y_i\\mid x_i}=\\mu_i, \\qquad \\log \\mu_i = x_i^\\top \\beta,\n\\] where the log link again enforces positivity of the mean. Alternatives include an inverse link, depending on the modeling context.\n\nDeviance and model comparison\nIn GLMs, goodness-of-fit is often summarized by the deviance, a likelihood-based measure that quantifies how well a fitted model explains the data relative to a benchmark. Understanding deviance requires first understanding what we mean by a saturated model.\nA saturated model is one that fits the data perfectly by using as many parameters as there are observations. For binary logistic regression with \\(n\\) distinct covariate patterns, a saturated model assigns a separate probability parameter \\(\\hat{p}_i\\) to each observation, setting \\(\\hat{p}_i = y_i\\). This achieves the maximum possible log-likelihood for the given data structure, though at the cost of using \\(n\\) parameters and having no degrees of freedom left for testing or prediction. The saturated model serves as an upper bound on how well any model can fit the observed data.\nFormally, the deviance of a fitted GLM is defined as \\[\nD = 2\\left[\\ell(\\text{saturated}) - \\ell(\\hat{\\beta})\\right],\n\\] where \\(\\ell(\\text{saturated})\\) is the log-likelihood of the saturated model and \\(\\ell(\\hat{\\beta})\\) is the log-likelihood of the fitted model with parameter estimates \\(\\hat{\\beta}\\). The factor of 2 appears for historical reasons related to asymptotic chi-squared distributions and makes the deviance directly comparable to likelihood ratio test statistics.\nFor binary logistic regression, this becomes \\[\nD = 2\\sum_{i=1}^n \\left[y_i \\log\\left(\\frac{y_i}{\\hat{p}_i}\\right) + (1-y_i)\\log\\left(\\frac{1-y_i}{1-\\hat{p}_i}\\right)\\right],\n\\] where \\(\\hat{p}_i = \\sigma(x_i^\\top\\hat{\\beta})\\) are the fitted probabilities. When \\(y_i \\in \\{0,1\\}\\), terms with \\(y_i\\log(y_i)\\) or \\((1-y_i)\\log(1-y_i)\\) equal zero by convention (\\(0\\log 0 = 0\\)), so each observation contributes either \\(-2\\log(\\hat{p}_i)\\) when \\(y_i=1\\) or \\(-2\\log(1-\\hat{p}_i)\\) when \\(y_i=0\\).\nLower deviance indicates better fit: a deviance of zero means the model fits as well as the saturated model. In practice, deviance is most useful for comparing nested models rather than as an absolute measure. Suppose we have two nested models with \\(M_1 \\subset M_2\\), where model \\(M_1\\) has \\(p_1\\) parameters and model \\(M_2\\) has \\(p_2 &gt; p_1\\) parameters. The difference in deviances \\[\n\\Delta D = D_1 - D_2 = 2\\left[\\ell(\\hat{\\beta}_2) - \\ell(\\hat{\\beta}_1)\\right]\n\\] follows approximately a \\(\\chi^2_{p_2-p_1}\\) distribution under the null hypothesis that the simpler model \\(M_1\\) is adequate. This provides a formal likelihood ratio test: if \\(\\Delta D\\) exceeds the critical value from the chi-squared distribution, we reject \\(M_1\\) in favor of the more complex \\(M_2\\).\nThis framework generalizes beyond logistic regression to all GLMs. For Poisson regression with counts \\(y_i\\) and fitted means \\(\\hat{\\mu}_i\\), the deviance takes the form \\[\nD = 2\\sum_{i=1}^n \\left[y_i\\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - (y_i - \\hat{\\mu}_i)\\right].\n\\] The specific formula changes with the exponential family distribution, but the interpretation remains consistent: deviance measures twice the log-likelihood gap between the fitted and saturated models, enabling principled comparison of nested specifications. This connects naturally to the model selection criteria in Chapter 16, where penalized versions of deviance like AIC and BIC trade off fit against model complexity.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#sec-bayes-logistic",
    "href": "13-logistic.html#sec-bayes-logistic",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.7 Bayesian Logistic Regression with The Polya-Gamma Distribution",
    "text": "13.7 Bayesian Logistic Regression with The Polya-Gamma Distribution\nTo perform Bayesian inference for logistic regression, we often need to sample from the posterior distribution of the coefficients \\(\\beta\\). Conceptually this follows the same updating pattern as in Chapter 3: posterior \\(\\propto\\) likelihood \\(\\times\\) prior. It also connects to the likelihood-to-loss framing in Chapter 11: the same likelihood that defines the posterior also defines an optimization objective through its negative log. However, the logistic likelihood does not have a conjugate prior, making direct sampling difficult. A powerful modern approach uses data augmentation with Polya-Gamma variables to make the sampling exact and efficient.\nThe Polya-Gamma distribution, denoted as PG(b,c), is carefully constructed as a subset of infinite convolutions of gamma distributions (essentially, a weighted sum of an infinite number of independent Gamma variables)(Polson, Scott, and Windle 2013). A random variable X follows a Polya-Gamma distribution with parameters \\(b &gt; 0\\) and \\(c \\in \\mathbb{R}\\) if:\n\\[\nX \\stackrel{d}{=} \\frac{1}{2\\pi^2} \\sum_{k=1}^{\\infty} \\frac{g_k}{(k-1/2)^2 + c^2/(4\\pi^2)},\n\\] where \\(g_k \\sim \\text{Ga}(b,1)\\) are independent gamma random variables, and \\(\\stackrel{d}{=}\\) indicates equality in distribution.\nThe Polya-Gamma family exhibits several remarkable properties that make it ideal for data augmentation:\n\nLaplace Transform: For \\(\\omega \\sim \\text{PG}(b,0)\\), the Laplace transform is \\(\\E{\\exp(-\\omega t)} = \\cosh^{-b}(\\sqrt{t}/2)\\).\nExponential Tilting: The general PG(b,c) distribution arises through exponential tilting of the PG(b,0) density:\n\n\\[\np(x|b,c) = \\frac{\\exp(-c^2x/2)p(x|b,0)}{\\E{\\exp(-c^2\\omega/2)}},\n\\]\nwhere the expectation is taken with respect to PG(b,0)\n\nConvolution Property: The family is closed under convolution for random variates with the same tilting parameter\nKnown Moments: All finite moments are available in closed form, with the expectation given by:\n\n\\[\\E{\\omega} = \\frac{b}{2c}\\tanh(c/2) = \\frac{b}{2c}\\frac{e^c-1}{1+e^c}.\\]\n\n\n\n\n\n\nTipComputational Advantage\n\n\n\nThe known moments and convolution properties make the Polya-Gamma distribution computationally tractable and theoretically well-behaved.\n\n\n\nThe Data-Augmentation Strategy\nThe core of the Polya-Gamma methodology rests on a fundamental integral identity that represents binomial likelihoods as mixtures of Gaussians(Polson, Scott, and Windle 2013). The key theorem states:\n\nTheorem 1: For \\(b &gt; 0\\) and \\(a \\in \\mathbb{R}\\), the following integral identity holds:\n\\[\\frac{(e^\\psi)^a}{(1+e^\\psi)^b} = 2^{-b}e^{\\kappa\\psi} \\int_0^{\\infty} e^{-\\omega\\psi^2/2} p(\\omega) d\\omega\\]\nwhere \\(\\kappa = a - b/2\\), and \\(p(\\omega)\\) is the density of \\(\\omega \\sim \\text{PG}(b,0)\\)(Polson, Scott, and Windle 2013).\nMoreover, the conditional distribution \\(p(\\omega|\\psi)\\) is also in the Polya-Gamma class: \\((\\omega|\\psi) \\sim \\text{PG}(b,\\psi)\\)(Polson, Scott, and Windle 2013).\n\nThis integral identity leads directly to a simple two-step Gibbs sampler for Bayesian logistic regression(Polson, Scott, and Windle 2013). For a dataset with observations \\(y_i \\mid \\psi_i \\sim \\text{Binom}(n_i, 1/(1+e^{-\\psi_i}))\\) where \\(\\psi_i = x_i^T\\beta\\), and a Gaussian prior \\(\\beta \\sim N(b,B)\\), the algorithm iterates:\n\nSample auxiliary variables: \\((\\omega_i|\\beta) \\sim \\text{PG}(n_i, x_i^T\\beta)\\) for each observation\nSample parameters: \\((\\beta|y,\\omega) \\sim N(m_\\omega, V_\\omega)\\) where:\n\n\\(V_\\omega = (X^T\\Omega X + B^{-1})^{-1}\\)\n\\(m_\\omega = V_\\omega(X^T\\kappa + B^{-1}b)\\)\n\\(\\kappa = (y_1-n_1/2, \\ldots, y_n-n_n/2)\\)\n\\(\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_n)\\)\n\n\nThis approach requires only Gaussian draws for the main parameters and Polya-Gamma draws for a single layer of latent variables, making it significantly simpler than previous methods(Polson, Scott, and Windle 2013).\nWe can visualize the hierarchical structure of this data augmentation strategy as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHierarchical structure of Polya-Gamma Data Augmentation\n\n\nThe practical success of the Polya-Gamma method depends on efficient simulation of Polya-Gamma random variables(Polson, Scott, and Windle 2013). The authors developed a sophisticated accept-reject sampler based on the alternating-series method of Devroye (Devroye 1986). For the fundamental PG(1,c) case, the sampler uses exponential and inverse-Gaussian draws as proposals, achieving an acceptance probability uniformly bounded below at 0.99919. This high acceptance rate requires no tuning for optimal performance. The acceptance criterion is evaluated using iterative partial sums, making the algorithm both efficient and robust.\nFor integer values of b, PG(b,z) random variables are generated by summing b independent PG(1,z) draws, exploiting the convolution property. This approach maintains efficiency for moderate values of b, though computational cost scales linearly with the total number of counts in negative binomial applications.\nThe BayesLogit package provides efficient tools for sampling from the Polya-Gamma distribution. The current version (2.1) focuses on core functionality: sampling from the Polya-Gamma distribution through the rpg() function and its variants.\nWe demonstrate the BayesLogit using a simulated data example. We generate \\(n = 100\\) observations with two predictors (plus an intercept term). The predictor matrix \\(X\\) consists of an intercept column and two columns of independent standard normal random variables. The true regression coefficients are set to \\(\\beta = (-0.5,1.2,-0.8)\\). The binary outcomes \\(y_i\\) are generated from a binomial distribution where the success probability for each observation follows the logistic transformation \\(p_i = 1/(1 + \\exp(-x_i^T\\beta))\\). This setup allows us to verify that the Bayesian inference procedure can recover the true parameter values from the simulated data.\n\n\nBayesian Logistic Regression with Polya-Gamma Data Augmentation\nlibrary(\"BayesLogit\")\n\n# Bayesian Logistic Regression with Polya-Gamma Data Augmentation\nbayesian_logit_pg &lt;- function(y, X, n_iter=5000, burn_in=1000) {\n  n &lt;- length(y)\n  p &lt;- ncol(X)\n  # Prior specification (weakly informative)\n  beta_prior_mean &lt;- rep(0, p)\n  beta_prior_prec &lt;- diag(0.01, p)  # Precision matrix\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter, p)\n  omega_samples &lt;- matrix(0, n_iter, n)\n  \n  # Initialize\n  beta &lt;- rep(0, p)\n  \n  for(iter in 1:n_iter) {\n    # Step 1: Sample omega (auxiliary variables)\n    psi &lt;- X %*% beta\n    omega &lt;- rpg(n, h=1, z=psi)\n    \n    # Step 2: Sample beta (regression coefficients)\n    # Posterior precision and mean\n    V_omega &lt;- solve(t(X) %*% diag(omega) %*% X + beta_prior_prec)\n    kappa &lt;- y - 0.5\n    m_omega &lt;- V_omega %*% (t(X) %*% kappa + beta_prior_prec %*% beta_prior_mean)\n    \n    # Sample from multivariate normal\n    beta &lt;- mvrnorm(1, m_omega, V_omega)\n    \n    # Store samples\n    beta_samples[iter, ] &lt;- beta\n    omega_samples[iter, ] &lt;- omega\n  }\n  \n  # Return samples after burn-in\n  list(\n    beta = beta_samples[(burn_in+1):n_iter, ],\n    omega = omega_samples[(burn_in+1):n_iter, ],\n    n_samples = n_iter - burn_in\n  )\n}\n\n# Example usage with simulated data\nset.seed(123)\nn &lt;- 100\nX &lt;- cbind(1, matrix(rnorm(n*2), n, 2))  # Intercept + 2 predictors\nbeta_true &lt;- c(-0.5, 1.2, -0.8)\nlogits &lt;- X %*% beta_true\nprobs &lt;- 1/(1 + exp(-logits))\ny &lt;- rbinom(n, 1, probs)\n\n# Fit model\nresults &lt;- bayesian_logit_pg(y, X, n_iter=3000, burn_in=500)\n\n# Posterior summaries\nposterior_means &lt;- colMeans(results$beta)\nposterior_sds &lt;- apply(results$beta, 2, sd)\n# Visualize posterior distributions\npar(mfrow=c(1,3))\nfor(j in 1:3) {\n  hist(results$beta[,j], \n       breaks=30, main=\"\",xlab=paste0(\"beta\", j-1),\n       col=\"lightblue\",border=\"white\")\n  abline(v=beta_true[j], col=\"red\", lwd=2, lty=2)\n  abline(v=posterior_means[j], col=\"blue\", lwd=2)\n}\n\n\n\n\n\nPosterior distributions for Bayesian Logistic Regression with Polya-Gamma Data Augmentation. Red line shows true value, blue line shows posterior mean.\n\n\n\n\nThe package offers several sampling methods for \\(\\omega\\).\n\nrpg(): Main function that automatically selects the best method\nrpg.devroye(): Devroye-like method for integer \\(h\\) values\nrpg.gamma(): Sum of gammas method (slower but works for all parameters)\nrpg.sp(): Saddlepoint approximation method\n\nIn the example above we use the automatic function rpg.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "13-logistic.html#bayesian-analysis-of-horse-race-betting",
    "href": "13-logistic.html#bayesian-analysis-of-horse-race-betting",
    "title": "13  Logistic Regression and Generalized Linear Models",
    "section": "13.8 Bayesian Analysis of Horse Race Betting",
    "text": "13.8 Bayesian Analysis of Horse Race Betting\nWe now apply the Polya-Gamma methodology to the horse racing data from Example 13.3, where we predict whether the race favorite wins. The Bayesian approach provides posterior distributions for the coefficients, quantifying uncertainty in our predictions, which is essential when making betting decisions.\nThe implementation uses Polya-Gamma augmentation to efficiently sample from the posterior distribution. The full R code for this analysis is available in case_studies/betting/benter.R, which implements the Gibbs sampler and generates all results shown below.\n\n\n\n\n\n\nFigure 13.4: Posterior distributions for horse racing logistic regression coefficients. Red dashed lines show MLE estimates; the Bayesian posteriors capture uncertainty around these point estimates.\n\n\n\nThe posterior distributions (Figure 13.4) reveal important insights for betting strategy. Table 13.1 shows the posterior summaries compared to maximum likelihood estimates:\n\n\n\nTable 13.1: Bayesian posterior summaries for horse racing logistic regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior SD\n95% CI Lower\n95% CI Upper\nMLE\n\n\n\n\nIntercept\n-0.885\n0.030\n-0.945\n-0.828\n-0.885\n\n\nPublic Prob (scaled)\n0.429\n0.029\n0.372\n0.486\n0.430\n\n\nField Size (scaled)\n0.046\n0.029\n-0.011\n0.103\n0.045\n\n\n\n\n\n\nThe Bayesian analysis confirms our findings: the public probability is a strong positive predictor of favorite success (posterior mean = 0.429, 95% CI: [0.372, 0.486]). The field size effect is weakly positive but uncertain, with the 95% credible interval spanning from -0.011 to 0.103, and only a 5.4% posterior probability that this effect is negative. This uncertainty is valuable for betting applications: we can propagate the posterior distribution through the Kelly criterion to obtain a distribution of optimal bet sizes rather than a single point estimate, leading to more robust position sizing.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Logistic Regression and Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html",
    "href": "14-tree.html",
    "title": "14  Tree Models",
    "section": "",
    "text": "14.1 Building a Tree via Recursive Binary Splitting\nImagine you’re a jewelry appraiser tasked with determining a diamond’s value. You might follow a series of questions: Is the carat weight above 1.0? If yes, is the clarity VS1 or better? Each question leads to another, creating a decision path that eventually arrives at a price estimate. This is precisely how decision trees work—they mirror our natural decision-making process by creating a flowchart of if-then rules.\nLogistic regression is a deliberately structured model: it trades flexibility for interpretability and stable estimation. Tree-based methods reverse that trade-off by letting the data determine interactions and nonlinearities. The cost is that algorithmic choices (splitting, pruning, ensembling) become part of the statistical model. This chapter introduces trees as a first major step into nonparametric prediction.\nWe’ve used decision trees before to describe the decision-making process as a sequence of actions and conditions. In this section, we’ll use decision trees to make predictions. You can think of a prediction as a decision task, where you need to decide which value of \\(y\\) to use for a given \\(x\\). Similar to a decision tree, a predictive tree model is a nested sequence of if-else statements that map any input data point \\(x\\) to a predicted output \\(y\\). Each if-else statement checks a feature of \\(x\\) and sends the data left or right along the tree branch. At the end of the branch, a single value of \\(y\\) is predicted.\nFigure 14.1 shows a decision tree for predicting a chess piece given a four-dimensional input vector that describes the types of moves available to the piece. The tree is a sequence of nested if-else statements that check the values of the input vector. The tree has six leaves, one for each of the chess pieces and has a depth of four. The tree is a predictive model that maps a four-dimensional input vector to a single output categorical value with six possible values.\nThe prediction mechanism is straightforward: traverse the tree from the root to a leaf node following the conditional logic. The process of building a tree, given a set of training data, is more complicated and has three main components:\nThe crux of the tree-building process lies in splitting: determining the optimal feature and threshold that best separates the data. At each step the splitting process needs to decide on the feature index \\(j\\) to be used for splitting and the location of the split. For a binary variable there is only one possible split location, but for continuous variables there are many possible split locations. The goal is to find the split that creates the most homogeneous subsets. In the case of regression trees, the best split is the one that minimizes the sum of squared errors. In the case of classification trees, the best split is the one that minimizes the Gini impurity. The Gini impurity is a measure of how homogeneous the subsets are.\nBelow we’ll explore tree-based models using the classic diamonds dataset, which contains prices and attributes for 53,940 diamonds. We’ll start with simple decision trees, progress to ensemble methods like random forests and gradient boosting, and develop deep insights into how these algorithms work, when to use them, and how to avoid common pitfalls.\nLet’s look at the data, which has 10 variables:\nLet’s plot price vs carat. Notice the strong non-linear relationship between carat and price. This suggests that log-transformations might help make the relationship linear.\nUnlike linear regression, tree models are naturally indifferent to non-linear relationships between predictors and the response. In general, we do not need to transform the variables.\nAlthough carat is the most important factor in determining the price of a diamond, it is not the only factor. We can see that there is a lot of variability in the price of diamonds with the same carat.\nLet’s start with a simple decision tree using just two predictors to visualize how trees partition the feature space:\nThe decision tree plot shows how the algorithm partitions the feature space based on carat and clarity to predict diamond prices. The tree structure reveals several interesting patterns:\nThis simple two-predictor tree demonstrates the key advantages of decision trees: they can handle non-linear relationships, provide interpretable rules, and naturally capture feature interactions without requiring explicit specification of interaction terms.\nLet’s plot the data.\nWe can see that for small and large diamonds, the price is consistently low and does not depend much on the clarity. However, at around 1 carat, we see some overlap in the price for different clarity levels. Clarity becomes important at this level.\nNow let’s plot the data with the tree regions.\nThe plot above shows the decision tree’s prediction regions as colored tiles, where each tile represents a specific combination of carat and clarity values. The color gradient from blue to red indicates the predicted price, with darker red representing higher predicted prices.\nLooking at this visualization, we can see several key patterns. The strongest predictor is clearly carat, as evidenced by the vertical bands of similar colors. As carat increases (moving right on the x-axis), the predicted prices generally increase (colors shift from blue to red). The tree captures non-linear patterns that a simple linear model would miss. For example, the rate of price increase with carat is not uniform across all clarity levels. Unlike smooth regression surfaces, the tree creates distinct rectangular regions with sharp boundaries, reflecting the binary splitting nature of decision trees.\nThe prediction using a tree is straightforward. The tree divides the predictor space-that is, the set of possible values for \\(x_1, x_2, \\ldots, x_p\\) - into \\(J\\) distinct and non-overlapping boxes, \\(R_1,R_2,...,R_J\\). For every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\). \\[\nf(x) = \\bar y_j, \\text{ for } x \\in R_j, \\text{ where } \\bar y_j = \\text{Average}(y_i \\mid x_i \\in R_j)\n\\]\nThe overall goal of building a tree is to find regions that lead to minima of the total Residual Sum of Squares (RSS) \\[\n\\mathrm{RSS} = \\sum_{j=1}^J\\sum_{i \\in R_j}(y_i - \\bar{y}_j)^2 \\rightarrow \\mathrm{minimize}\n\\]\nUnfortunately, it is computationally infeasible (NP-hard problem) to consider every possible partition of the feature space into \\(J\\) boxes. We can find a good approximate solution, using top-down approach (the CART algorithm).\nIt begins with the entire dataset at the “root” node and repeatedly splits the data into two “child” nodes. This process continues recursively on each new node, with the goal of making the resulting groups (nodes) as homogeneous as possible with respect to the target variable, price. At each iteration we decide on which variable \\(j\\) to split and the split point \\(s\\). \\[\nR_1(j, s) = \\{x\\mid x_j &lt; s\\} \\mbox{ and } R_2(j, s) = \\{x\\mid x_j \\ge s\\},\n\\] thus, we seek to minimize (in case of regression tree) \\[\n\\min_{j,s}\\left[ \\sum_{i:x_i\\in R_1}(y_i - \\bar{y}_1)^2 + \\sum_{i:x_i \\in R_2}(y_i - \\bar{y}_2)^2\\right]\n\\] As a result, every observed input point belongs to a single region.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#pruning-taming-an-overfit-tree",
    "href": "14-tree.html#pruning-taming-an-overfit-tree",
    "title": "14  Tree Models",
    "section": "14.2 Pruning: Taming an Overfit Tree",
    "text": "14.2 Pruning: Taming an Overfit Tree\nNow let’s discuss how many regions we should have. At one extreme end, we can have \\(n\\) regions, one for each observation. Then the tree model will work similar to the one-nearest neighbor model. At the other end, we can have one big region for the entire input space and then every prediction will be the same (average across observed \\(y\\)’s). Both models can be used but usually the best one is in the middle. The number of regions (branches) controls the complexity of the model. We need to find a good size on the variance-bias scale. A smaller tree with fewer splits (that is, fewer regions \\(R_1,...,R_J\\)) might lead to lower variance and better interpretation at the cost of a little bias. Deep trees often suffer from high variance, where slight perturbations in the training data produce vastly different structures, rendering the model unstable.\nHow do we construct a tree with a “manageable” number of branches? This is accomplished through the steps of forward tree construction and backward pruning. The forward step is a greedy algorithm that begins with a single region and divides it into two. This procedure is repeated until a certain stopping criterion is met. A practical method is to continue building the tree until the Residual Sum of Squares (RSS) plateaus. However, this method can be myopic as an initially unproductive split might be followed by a highly beneficial one, leading to a significant decrease in RSS in subsequent iterations. A more effective strategy is to grow an extensive tree \\(T_0\\), and then trim it down to obtain a subtree. The size of the subtree can be determined using cross-validation. However, be aware that the number of subtrees can be exponential!\nInstead of considering all possible sub-trees, we will do cost complexity pruning - also known as weakest link pruning. We consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\) there corresponds a subtree \\(T \\subset T_0\\) such that minimizes \\[\n\\sum_{m=1}^{|T|}\\sum_{i:x_i\\in R_m}(y_i - \\bar{y}_m)^2 + \\alpha |T|\n\\] The parameter \\(\\alpha\\) balances the complexity of the subtree and its adherence to the training data. When we increment \\(\\alpha\\) starting from zero, branches are predictably and sequentially pruned from the tree, making it straightforward to acquire the entire series of subtrees as a function of \\(\\alpha\\). We determine the optimal value \\(\\hat \\alpha\\) through cross-validation. Afterward, we refer back to the complete data set and extract the subtree that corresponds to \\(\\hat \\alpha\\).\n\nExample: Boston Housing Data\nTo demonstrate pruning and decision boundaries on a standard benchmark, we switch to the Boston Housing dataset. This dataset contains information about housing values in suburbs of Boston. We used it in previous chapters, but here it allows us to easily visualize pruning on a well-known problem.\n\nlibrary(MASS)\ndata(Boston)\nhead(Boston)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n0.01\n18\n2.3\n0\n0.54\n6.6\n65\n4.1\n1\n296\n15\n397\n5.0\n24\n\n\n0.03\n0\n7.1\n0\n0.47\n6.4\n79\n5.0\n2\n242\n18\n397\n9.1\n22\n\n\n0.03\n0\n7.1\n0\n0.47\n7.2\n61\n5.0\n2\n242\n18\n393\n4.0\n35\n\n\n0.03\n0\n2.2\n0\n0.46\n7.0\n46\n6.1\n3\n222\n19\n395\n2.9\n33\n\n\n0.07\n0\n2.2\n0\n0.46\n7.2\n54\n6.1\n3\n222\n19\n397\n5.3\n36\n\n\n0.03\n0\n2.2\n0\n0.46\n6.4\n59\n6.1\n3\n222\n19\n394\n5.2\n29\n\n\n\n\n\n\nWe will focus on predicting medv (median value of owner-occupied homes in $1000s) using lstat (lower status of the population percent) and other variables.\nFirst we build a big tree:\n\nlibrary(tree)\n# mindev param controls stopping: smaller value = bigger tree\ntemp = tree(medv~lstat, data=Boston, mindev=.0001)\nlength(unique(temp$where)) # first big tree size\n## 73\n\nThen prune it down to one with 7 leaves:\n\nboston.tree=prune.tree(temp,best=7)\nlength(unique(boston.tree$where)) # pruned tree size\n## 7\n\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\nboston.fit = predict(boston.tree) #get training fitted values\nplot(Boston$lstat,Boston$medv,cex=.5,pch=16, xlab=\"lstat\", ylab=\"medv\") #plot data\noo=order(Boston$lstat)\nlines(Boston$lstat[oo],boston.fit[oo],col='red',lwd=3) #step function fit\ncvals=c(9.725,4.65,3.325,5.495,16.085,19.9) #cutpoints from tree\nfor(i in 1:length(cvals)) abline(v=cvals[i],col='magenta',lty=2) #cutpoints\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s use more variables. We pick dis (weighted mean of distances to five Boston employment centres), lstat, and medv:\n\n# Select specific columns by name\ndf2 = Boston[, c(\"dis\", \"lstat\", \"medv\")]\nprint(names(df2))\n## \"dis\"   \"lstat\" \"medv\"\n\nBuild the big tree:\n\ntemp = tree(medv~., df2, mindev=.0001)\nlength(unique(temp$where)) #\n## 74\n\nThen prune it down to one with 7 leaves:\n\nboston.tree=prune.tree(temp,best=7)\n\nplot(boston.tree,type=\"u\")# plot tree and partition in x.\ntext(boston.tree,col=\"blue\",label=c(\"yval\"),cex=.8)\npartition.tree(boston.tree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet predictions on 2d grid and make perspective plot:\n\npv=seq(from=.01,to=.99,by=.05)\nx1q = quantile(df2$lstat,probs=pv)\nx2q = quantile(df2$dis,probs=pv)\nxx = expand.grid(x1q,x2q) #matrix with two columns using all combinations of x1q and x2q\ndfpred = data.frame(dis=xx[,2],lstat=xx[,1])\nlmedpred = predict(boston.tree,dfpred)\n\n# Create a perspective plot (3D surface)\n# theta sets the viewing angle, zlim ensures the vertical axis covers the data range\npersp(x1q, x2q, matrix(lmedpred, ncol=length(x2q), byrow=T),\n      theta=150, xlab='dis', ylab='lstat', zlab='medv',\n      zlim=c(min(df2$medv), 1.1*max(df2$medv)))\n\n\n\n\n\n\n\n\nAdvantages of Decision Trees: Decision trees are incredibly intuitive and simple to explain, often even more straightforward to understand than linear regression models. Some theorists argue that decision trees mimic human decision-making processes more accurately than other regression and classification methods. Decision trees can be visually represented, making them easily interpretable, even for those without a deep understanding of the underlying mechanics. Additionally, decision trees can effortlessly manage qualitative predictors, eliminating the need to create dummy variables.\nDisadvantages of Decision Trees: The main drawback is instability. Deep trees often suffer from high variance, where slight perturbations in the training data produce vastly different structures.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#classification-trees",
    "href": "14-tree.html#classification-trees",
    "title": "14  Tree Models",
    "section": "14.3 Classification Trees",
    "text": "14.3 Classification Trees\nA classification tree operates much like a regression tree. The prediction is made based on the “majority vote”, which means selecting the class that appears most frequently within the region. The process of developing a classification tree is largely the same as that of a regression tree, involving recursive binary splitting. However, instead of using the Residual Sum of Squares (RSS), we use criteria better suited for categorical data.\nWe start by introducing some notations. Let \\[\np_{mk} = \\dfrac{1}{N_m}\\sum_{x_i \\in R_m} I(y_i=k),\n\\] be the proportion of observations of class \\(k\\) in region \\(m\\).\nThe classification is then done by: \\[\np_m = \\max_k p_{mk},~~~ E_m = 1-p_m\n\\] where \\(k(m) = \\arg\\max_k p_{mk}\\) is the most frequent class in region \\(m\\). The error rate \\(E_m\\) is simplcy the proportion of observations in the region that do NOT belong to the majority class.\nThen classification prediction is: \\[\nP(y=k) = \\sum_{j=1}^J p_j I(x \\in R_j)\n\\]\nAn alternative method to evaluate the quality of a split in a classification tree is through the use of the Gini Index or Cross-Entropy. Let’s consider a scenario where we have an equal number of observations in each class, say 400 in each.\nNow, suppose we create a tree that results in two regions: one with a distribution of (300,100) and the other with (100,300). This means that in the first region, 300 observations belong to one class and 100 to the other, and vice versa in the second region. Consider another scenario where we have a different tree that results in two regions with distributions of (200,400) and (200,0).\nIn both cases, the misclassification rate is 0.25, meaning that 25% of the observations are incorrectly classified. However, the second tree is more desirable. Why is that? The second tree has a region with no misclassifications at all (200,0), which means it is perfectly classifying all observations in that region. This is an ideal situation in classification problems.\nThis illustrates that while the misclassification rate is a useful metric, it does not always capture the full picture. The Gini Index or Cross-Entropy are preferred for growing trees because they are more sensitive to node purity.\nThe Gini index: \\[\nG_m = \\sum_{k=1}^K p_{mk}(1-p_{mk})\n\\] It measures a variance across the \\(K\\) classes. It takes on a small value if all of the \\(p_{mk}\\)’s are close to zero or one (pure nodes).\nAn alternative to the Gini index is cross-entropy (a.k.a deviance), given by \\[\nD_m = -\\sum_{k=1}^Kp_{mk}\\log p_{mk}\n\\] It is near zero if the \\(p_{mk}\\)’s are all near zero or near one. Gini index and the cross-entropy led to similar results.\n\nExample: Classifying Diamond Quality\nLet’s use a classification tree to predict the cut of a diamond based on its price and carat. We will try to distinguish “Ideal” cuts from others. To make it a clear binary problem for visualization, let’s create a binary variable is_ideal.\n\ndiamonds_class &lt;- diamonds %&gt;%\n  mutate(is_ideal = as.factor(ifelse(cut == \"Ideal\", \"Ideal\", \"Non-Ideal\")))\n\n# Fit classification tree\nclass_tree &lt;- rpart(is_ideal ~ carat + price + clarity, data = diamonds_class, method=\"class\")\nrpart.plot(class_tree, main=\"Classification Tree for Ideal Cut\")\n\n\n\n\n\n\n\n\nThe tree shows how clairty and carat effectively separate Ideal cut diamonds from the rest. The nodes display the predicted class and the probability of that class, illustrating how the model estimates class probabilities (\\(p_{mk}\\)) in each region.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#ensemble-methods",
    "href": "14-tree.html#ensemble-methods",
    "title": "14  Tree Models",
    "section": "14.4 Ensemble Methods",
    "text": "14.4 Ensemble Methods\nThere are several techniques used to address the issue of overfitting in decision trees. We considered the pruning technique which reduces the complexity of the final classifier. Two other methods are random forests and boosting. Random Forests is an ensemble method where multiple decision trees are created and their predictions are averaged (for regression) or majority voting is done (for classification). Boosting is another ensemble technique where trees are built sequentially such that each subsequent tree aims to reduce the bias of the combined classifier.\nThe simple idea behind every ensemble method is that the variance of the average is lower than the variance of individual models (see Section 10.1.3.2 for a historical perspective on how Galton’s ox-weighing experiment illustrates this principle). Say we have \\(B\\) models \\(f_1(x),\\ldots,f_B(x)\\) then we combine those \\[\nf_{avg}(x) = \\dfrac{1}{B}\\sum_{b=1}^Bf_b(x)\n\\] Combining models helps fight overfitting. On the negative side, it is harder to interpret these ensembles compared to a single decision tree.\n\nBagging\nIn the bagging approach, we treat the sample as if it were the population and then take iid draws. That is, you sample with replacement so that you can get the same original sample value more than once in a bootstrap sample.\nTo bootstrap aggregate (Bag) we:\n\nTake \\(B\\) bootstrap samples from the training data, each of the same size as the training data.\nFit a large tree to each bootstrap sample (we know how to do this fast!). This will give us \\(B\\) trees.\nCombine the results from each of the \\(B\\) trees to get an overall prediction.\n\nWhen the target variable \\(y\\) is numeric, the bagging process is straightforward: the final prediction is simply the average. When \\(y\\) is categorical, we use a voting system or average the predicted probabilities.\nLet’s experiment with the number of trees in the model using the Boston dataset again. We calculate the mean squared error for each number of trees.\n\nlibrary(randomForest)\nn = nrow(Boston)\nntreev = c(10,500,5000)\nfmat = matrix(0,n,3)\nfor(i in 1:3) {\n rffit = randomForest(medv~lstat,data=Boston,ntree=ntreev[i],maxnodes=15)\n fmat[,i] = predict(rffit)\n mse &lt;- mean((fmat[,i] - Boston$medv)^2, na.rm = TRUE)\n cat(\"Mean Squared Error with\", ntreev[i], \"trees:\", round(mse, 3), \"\\n\")\n}\n## Mean Squared Error with 10 trees: 31 \n## Mean Squared Error with 500 trees: 29 \n## Mean Squared Error with 5000 trees: 29\n\nLet’s plot the results\noo = order(Boston$lstat)\nfor(i in 1:3) {\n plot(Boston$lstat,Boston$medv,xlab='lstat',ylab='medv',pch=16)\n lines(Boston$lstat[oo],fmat[oo,i],col=i+1,lwd=3)\n title(main=paste('bagging ntrees = ',ntreev[i]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith 10 trees our fit is too jumbly (high variance).\nWith 1,000 and 5,000 trees the fit is smooth and very similar.\nNote that although our method is based on multiple trees (average over) so we no longer have a simple step function!\n\n\n\nRandom Forest\nIn the bagging technique, models can become correlated, which prevents the achievement of a \\(1/B\\) reduction in variance. This happens because most, if not all, of the trees will use the most influential predictor in the top split. As a result, bagged trees tend to look very similar to each other.\nRandom Forests, on the other hand, introduce an element of randomness that helps to decorrelate the trees. Instead of considering all \\(p\\) predictors for a split, a random sample of \\(m\\) predictors is chosen as split candidates. This subset of predictors is different for each split.\nThe number of predictors considered at each split, \\(m\\), is typically chosen to be the square root of the total number of predictors, \\(p\\) for classification, or \\(p/3\\) for regression.\nOne of the “interpretation” tools that comes with ensemble models is importance ranking: the total amount that the deviance (loss) is decreased due to splits over a given predictor, averaged over all trees.\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=4,importance=TRUE,ntree=50)\nvarImpPlot(rf.boston,pch=21,bg=\"lightblue\",main=\"\")\n\n\n\n\n\n\n\n\n\nrf.boston = randomForest(medv~.,data=Boston,mtry=6,ntree=50, maxnodes=50)\nyhat.rf = predict(rf.boston,newdata=Boston)\noo=order(Boston$lstat)\nplot(Boston$lstat[oo],Boston$medv[oo],pch=21,bg=\"grey\", xlab=\"lstat\", ylab=\"medv\") #plot data\nlines(Boston$lstat[oo],yhat.rf[oo],col='red',lwd=3) #step function fit\n\n\n\n\nRandom Forest Fit\n\n\n\n\n\n\nBoosting\nBoosting, like Random Forests, is a method that combines multiple trees to create a more powerful predictive model. However, the approach it takes is quite distinct.\nHere’s how Boosting works:\n\nInitially, a single decision tree is fitted to the data. This initial tree is intentionally made weak.\nWe then examine the residuals, which represent the portion of the target variable \\(y\\) not explained by the weak tree.\nA new tree is then fitted to these residuals, essentially trying to predict the error of the first tree.\nThis new tree is also “weakened” or “shrunk” (multiplied by a learning rate \\(\\lambda\\)). The prediction from this tree is then added to the prediction of the previous trees.\nThis process is repeated iteratively. The final model is the sum of all these “shrunk” trees.\n\nThe key idea behind Boosting is to iteratively improve the model by focusing on the parts of the data that the current model is not explaining well (the residuals).\nMathematically, we want to minimize a loss function \\(\\mathcal{L}\\). For regression, we might take \\(\\mathcal{L}(y_i , \\theta_i ) = (y_i - \\theta_i )^2\\). We solve: \\[\\mathrm{minimize}_{\\beta \\in R^B} \\sum_{i=1}^n \\mathcal{L} \\left(y_i, \\sum_{b=1}^B \\beta_j \\cdot T_b(x_i)\\right)\\]\nGradient boosting acts like gradient descent in function space. Start with initial model, e.g., fit a single tree \\(\\theta^{(0)} = T_0\\). Repeat: - Evaluate gradient \\(g\\) at latest prediction \\(\\theta^{(k-1)}\\), \\[g_i = \\left.\\left[\\frac{\\partial \\mathcal{L}(y_i, \\theta_i)}{\\partial \\theta_i}\\right]\\right|_{\\theta_i = \\theta_i^{(k-1)}},\\ i=1,\\ldots,n\\] - Find a tree \\(T_k\\) that is close to \\(-g\\), i.e., \\(T_k\\) solves \\[\\mathrm{minimize}_{\\text{trees }T} \\sum_{i=1}^n (-g_i - T(x_i))^2\\] - Update our prediction: \\[\\theta^{(k)} = \\theta^{(k-1)} + \\lambda \\cdot T_k\\]\nAlgorithm: 1. Set \\(\\hat{f}(x)=0\\) and \\(r_i=y_i\\) for all \\(i\\) in training set. 2. For \\(b=1,2,\\ldots,B\\), repeat: (a) Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits (interactions) to the training data \\((X, r)\\). (b) Update \\(\\hat{f}\\) by adding a shrunken version of the new tree: \\[\\hat{f}(x) \\leftarrow \\hat{f}(x) +\\lambda \\hat{f}^b(x)\\] (c) Update the residuals: \\[r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)\\] 3. Output the boosted model: \\[\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)\\]\nThe parameter \\(\\lambda\\) is the learning rate or shrinkage parameter. By multiplying the new tree’s contribution by a small number (e.g., 0.01 or 0.1), we prevent the model from adapting too quickly to outliers or noise.\nHere are some boosting fits where we vary the number of trees, but fix the depth at 2 (suitable with 1 x) and shrinkage = \\(\\lambda\\) at .2.\n\nlibrary(gbm)\nboost.boston=gbm(medv~.,data=Boston,distribution=\"gaussian\",\nn.trees=5000,interaction.depth=4)\nyhat.boost=predict(boost.boston,newdata=Boston,n.trees=5000)\nmean((yhat.boost-Boston$medv)^2)\n## 0.0004\n\n\nsummary(boost.boston, plotit=FALSE)\n\n\n\n\n\n\nvar\nrel.inf\n\n\n\n\nlstat\nlstat\n36.32\n\n\nrm\nrm\n30.98\n\n\ndis\ndis\n7.63\n\n\ncrim\ncrim\n5.09\n\n\nnox\nnox\n4.63\n\n\nage\nage\n4.50\n\n\nblack\nblack\n3.45\n\n\nptratio\nptratio\n3.11\n\n\ntax\ntax\n1.74\n\n\nrad\nrad\n1.17\n\n\nindus\nindus\n0.87\n\n\nchas\nchas\n0.39\n\n\nzn\nzn\n0.13\n\n\n\n\n\n\nplot(boost.boston,i=\"rm\")\nplot(boost.boston,i=\"lstat\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting vs Random Forests: * Boosting often provides better accuracy by correcting specific errors (residuals), but is more prone to overfitting and noise. It requires careful tuning of \\(\\lambda\\) and \\(B\\). * Random Forests are more robust “out of the box”, easier to parallelize (trees are independent), and less prone to overfitting, but might not reach the same peak accuracy as a well-tuned boosting model.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#sec-bart-causal",
    "href": "14-tree.html#sec-bart-causal",
    "title": "14  Tree Models",
    "section": "14.5 BART for causal inference",
    "text": "14.5 BART for causal inference\nEstimating the causal effect of an intervention, such as a new drug, a marketing campaign, or a public policy, is a central goal across science and industry. While the gold standard for causal inference is the Randomized Controlled Trial (RCT), it is often infeasible, unethical, or too expensive to conduct. Researchers must therefore turn to observational data, where the assignment of treatment is not controlled by the investigator. This introduces a fundamental challenge: individuals who receive the treatment may be systematically different from those who do not, a problem known as confounding. Separating the true effect of the treatment from these pre-existing differences is the primary task of causal inference from observational data.\nTo formalize causal questions, we rely on the Rubin Causal Model (RCM), also known as the potential outcomes framework. For a binary treatment \\(Z\\) (where \\(Z_i=1\\) if individual \\(i\\) receives the treatment and \\(Z_i=0\\) otherwise), we posit that each individual \\(i\\) has two potential outcomes: * \\(Y_i(1)\\): The outcome that would be observed if individual \\(i\\) were exposed to the treatment. * \\(Y_i(0)\\): The outcome that would be observed if individual \\(i\\) were exposed to the control (no treatment).\nThis framework leads directly to what Holland (1986) termed the “fundamental problem of causal inference”: for any given individual, we can only ever observe one of these two potential outcomes. The outcome we do not see is the counterfactual. Causal inference can thus be viewed as a missing data problem, where the goal is to estimate the values of the unobserved potential outcomes.\nFrom this foundation, we can define several key causal quantities, or estimands:\n\nIndividual Treatment Effect (ITE): The effect for a single individual, defined as \\[\\tau_i = Y_i(1) - Y_i(0).\\] This is typically unobservable.\nAverage Treatment Effect (ATE): The average effect across the entire population, \\[\\tau_{ATE} = \\E{Y(1) - Y(0)}.\\] This is often the primary estimand of interest for broad policy questions.\nAverage Treatment Effect on the Treated (ATT): The average effect for those who actually received the treatment, \\[\\tau_{ATT} = \\E{Y(1) - Y(0) \\mid Z=1}.\\]\nConditional Average Treatment Effect (CATE): The average effect for a subpopulation defined by a set of covariates \\(X=x\\), \\[\\tau(x) = \\E{Y(1) - Y(0) \\mid X=x}.\\] Understanding the CATE allows for the exploration of treatment effect heterogeneity.\n\nTo estimate these causal estimands from observational data, we must rely on a set of critical, untestable assumptions that connect the observed data to the unobserved potential outcomes. These are known as identification assumptions.\n\nStable Unit Treatment Value Assumption (SUTVA): This assumption has two parts. First, it assumes there is no interference between units, meaning one individual’s treatment status does not affect another’s outcome. Second, it assumes there are no hidden variations of the treatment; the treatment assigned to one individual is the same as the treatment assigned to any other.\nIgnorability (or Unconfoundedness): This is the most crucial assumption. It states that, conditional on a set of observed pre-treatment covariates \\(X\\), treatment assignment \\(Z\\) is independent of the potential outcomes: \\[(Y(0), Y(1)) \\perp Z \\mid X\\]. In essence, it assumes that we have measured all the common causes of both treatment selection and the outcome. If this holds, then within any stratum defined by the covariates \\(X\\), the treatment assignment is “as-if” random.\nPositivity (or Overlap/Common Support): This assumption requires that for any set of covariate values \\(x\\) present in the population, there is a non-zero probability of being in either the treatment or the control group: \\(0 &lt; P(Z=1 \\mid X=x) &lt; 1\\). This ensures that we can find both treated and control individuals with similar characteristics, making comparison meaningful and avoiding extrapolation to regions with no data.\n\nTo demonstrate the application of Bayesian methods to this challenge, we use the famous Lalonde dataset, a canonical benchmark in the causal inference literature. The dataset addresses a real-world policy question: evaluating the effectiveness of the National Supported Work (NSW) Demonstration, a federally funded job training program implemented in the US from 1975-1979. The program was designed to help individuals facing significant social and economic barriers (e.g., former drug addicts, ex-convicts, high school dropouts) improve their labor market prospects. The treatment (\\(treat\\)) is participation in this program, and the primary outcome (\\(re78\\)) is the individual’s real earnings in 1978, after the program.\nThe historical importance of this dataset stems from Robert Lalonde’s 1986 paper, which delivered a powerful critique of the non-experimental methods used at the time. Lalonde started with data from an actual RCT, which provided an unbiased estimate of the program’s effect. He then took the treated group from the experiment but replaced the experimental control group with a non-experimental comparison group drawn from large public surveys—the Panel Study of Income Dynamics (PSID) and the Current Population Survey (CPS). He showed that the standard econometric models of the era failed to replicate the experimental benchmark when applied to this new, confounded dataset, casting serious doubt on their reliability for policy evaluation. Our task is to see if a modern, flexible Bayesian method—Bayesian Additive Regression Trees (BART)—can succeed where these earlier methods failed.\nThe challenge posed by the Lalonde dataset becomes immediately apparent when we examine the pre-treatment characteristics of the treated group versus the non-experimental control group. A naive comparison of their 1978 earnings would be deeply misleading because the groups were profoundly different before the program even began. Table 14.1 illustrates this imbalance for key covariates, including age, education, race, marital status, and earnings in the years prior to the intervention (1974 and 1975).\nThe Standardized Mean Difference (SMD) provides a scale-free measure of the difference between the group means. A common rule of thumb suggests that an absolute SMD greater than 0.1 indicates a potentially meaningful imbalance. As the table shows, the groups differ substantially on nearly every measured characteristic. The treated individuals were younger, less educated, more likely to be from minority groups, and had drastically lower earnings in the years before the program. This severe selection bias is precisely what makes the Lalonde dataset such a difficult and important test case for causal inference methods. Any credible method must be able to adjust for these vast pre-existing differences to isolate the true causal effect of the job training program.\n\n\n\nTable 14.1: Covariate Balance in the Lalonde Non-Experimental Dataset. Note: Data corresponds to the widely used Dehejia and Wahba (1999) sample of the Lalonde dataset. Standardized Mean Difference is calculated as the difference in means divided by the pooled standard deviation.\n\n\n\n\n\nCovariate\nTreated Mean\nControl Mean\nStd. Mean Diff.\n\n\n\n\nAge (years)\n25.82\n28.04\n-0.31\n\n\nEducation (years)\n10.35\n10.23\n0.06\n\n\nBlack (indicator)\n0.84\n0.20\n1.84\n\n\nHispanic (indicator)\n0.06\n0.14\n-0.32\n\n\nMarried (indicator)\n0.19\n0.51\n-0.81\n\n\nNo Degree (indicator)\n0.71\n0.60\n0.25\n\n\nEarnings 1974\n2095.57\n5630.71\n-0.63\n\n\nEarnings 1975\n1532.06\n5205.52\n-0.65\n\n\n\n\n\n\nTo address the challenge of confounding, we need a method that can flexibly model the relationship between the outcome, the treatment, and the many covariates shown to be imbalanced. Bayesian Additive Regression Trees (BART) is a powerful non-parametric machine learning algorithm that is exceptionally well-suited for this task. It combines the predictive power of ensemble methods with a rigorous Bayesian framework for regularization and uncertainty quantification.\nAt its core, BART models the expected value of an outcome \\(Y\\) as a sum of many individual regression trees. For a set of predictors \\(x\\), the model is:\n\\[Y = \\sum_{j=1}^{m} g(x; T_j, M_j) + \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim N(0, \\sigma^2)\\]\nHere, \\(m\\) is the number of trees in the ensemble (typically around 200), and each function \\(g(x; T_j, M_j)\\) represents a single regression tree. The structure of the tree is denoted by \\(T_j\\), and \\(M_j\\) is the set of parameter values in its terminal nodes (or leaves).\nCrucially, each individual tree is designed to be a “weak learner”. It is kept shallow and simple, meaning it explains only a small fraction of the variation in the outcome. The final, powerful prediction comes from summing up the contributions of all these simple components. This sum-of-trees structure allows BART to automatically capture very complex relationships, including high-order interactions and non-linearities, without the user needing to specify them in advance. For example, an interaction between age and education is implicitly modeled if a tree splits on education within a branch that has already been split on age. This flexibility is a major advantage in observational studies where the true functional form of the relationship between the outcome and the confounders is unknown.\nIn most machine learning algorithms, overfitting is controlled through techniques like cross-validation or complexity penalties. BART, being a fully Bayesian method, achieves this through a carefully specified set of regularization priors. These priors are designed to keep each tree simple and prevent any single tree from dominating the overall fit.\nThe key priors are:\n\nPrior on Tree Structure: This prior strongly encourages shallow trees. It is defined by a rule governing the probability that a node at a certain depth \\(d\\) will be split further. This probability is typically modeled as \\[p(T_j) = \\alpha(1+d)^{-\\beta},\\] where \\(\\alpha \\in (0,1)\\) and \\(\\beta \\ge 0\\) are hyperparameters. Setting \\(\\beta\\) to a value like 2 ensures that the probability of splitting decreases rapidly with depth, keeping the trees small.\nPrior on Terminal Node Parameters: After the response variable \\(Y\\) is centered and scaled, the values \\(\\mu_{jk}\\) in the terminal nodes of each tree are given a Normal prior, \\[\n\\mu_{jk} \\sim N(0, \\sigma_{\\mu}^2).\n\\] This prior shrinks the predictions within each leaf towards zero. Because the final prediction is a sum over \\(m\\) trees, this shrinkage ensures that the contribution of each individual tree is small.\nPrior on Error Variance: The residual variance \\(\\sigma^2\\) is typically given a conjugate Inverse-Gamma prior. This prior is usually chosen to be weakly informative, allowing the data to dominate the posterior estimate of the noise level, but it still constrains the variance to be reasonable.\n\nTogether, these priors act as a sophisticated regularization mechanism that allows BART to fit complex functions while being highly resistant to overfitting.\nBART models are fit using a Markov chain Monte Carlo (MCMC) algorithm, specifically a form of Gibbs sampler known as Bayesian backfitting. The algorithm does not find a single “best” model. Instead, it generates thousands of samples from the joint posterior distribution of all model parameters: \\(p(T_1,\\ldots,T_m, M_1,\\ldots,M_m, \\sigma \\mid Y, X)\\).\nThe fitting process works iteratively:\n\nInitialize all \\(m\\) trees and \\(\\sigma\\).\nFor each tree \\(j\\) from 1 to \\(m\\):\n\nCalculate the “partial residual” by subtracting the predictions of all other trees from the outcome: \\[R_j = Y - \\sum_{k \\neq j} g(x; T_k, M_k)\\].\nDraw a new tree structure \\(T_j\\) and its leaf parameters \\(M_j\\) from their posterior distribution conditional on this partial residual, \\[p(T_j, M_j \\mid R_j, \\sigma).\\]\n\nAfter iterating through all trees, draw a new value for \\(\\sigma\\) from its posterior conditional on the current residuals.\nRepeat steps 2 and 3 for thousands of iterations.\n\nThe output of this process is not one set of trees, but a collection of (e.g., 5000) sets of trees, where each set represents a plausible regression function drawn from the posterior distribution. This collection of draws is the key to quantifying uncertainty in a Bayesian way.\nThe power of BART for causal inference lies in how it leverages the full posterior distribution to estimate counterfactuals. The strategy aligns perfectly with the Bayesian view of causal inference as a missing data problem, as articulated by Rubin (1978).\nThe standard approach for causal inference with BART is to model the outcome \\(Y\\) as a function of both the covariates \\(X\\) and the treatment indicator \\(Z\\). The model learns a single, flexible response surface:\n\\[\\E{Y \\mid X, Z} = f(X, Z)\\]\nHere, the treatment \\(Z\\) is included as if it were “just another covariate” in the set of predictors fed to the BART algorithm. The model is free to discover how the effect of \\(Z\\) varies with \\(X\\) through the tree-splitting process. The Conditional Average Treatment Effect (CATE) is then simply the difference in the predictions from this learned function:\n\\[\\tau(x) = f(x, Z=1) - f(x, Z=0)\\]\nThe core of the estimation process is a predictive step that is repeated for each draw from the MCMC sampler. Suppose the MCMC algorithm has produced \\(S\\) posterior draws of the function \\(f\\). For each draw \\(s = 1,\\ldots, S\\):\n\nWe take the full dataset of \\(n\\) individuals with their observed covariates \\(X\\).\nWe create two hypothetical, or counterfactual, datasets:\n\nTreated World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=1\\) for everyone.\nControl World: The observed covariates \\(X\\) for all \\(n\\) individuals, but with the treatment indicator set to \\(Z=0\\) for everyone.\n\nUsing the fitted BART model corresponding to posterior draw \\(s\\) (i.e., \\(f^{(s)}\\)), we predict the outcome for every individual under both scenarios. This gives us a full set of posterior predictive draws for the potential outcomes: \\(\\tilde{Y}_i(1)^{(s)}\\) and \\(\\tilde{Y}_i(0)^{(s)}\\) for each individual \\(i\\).\n\nThis process is a direct implementation of the missing data analogy. For an individual \\(i\\) who was actually treated (\\(Z_i=1\\)), their observed outcome \\(Y_i\\) is their potential outcome \\(Y_i(1)\\). The BART model provides a posterior predictive draw for their missing counterfactual outcome, \\(\\tilde{Y}_i(0)^{(s)}\\). Conversely, for a control subject, we use the model to predict their missing \\(\\tilde{Y}_i(1)^{(s)}\\).\nOnce we have the posterior draws of the potential outcomes for every individual at each MCMC iteration, we can compute a posterior draw for any causal estimand of interest. For example, at each iteration \\(s\\):\n\nITE draw: \\[\\tau_i^{(s)} = \\tilde{Y}_i(1)^{(s)} - \\tilde{Y}_i(0)^{(s)}\\]\nATE draw: \\[\\tau_{ATE}^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\tau_i^{(s)}\\]\n\nBy collecting these values across all \\(S\\) MCMC iterations, we obtain \\[\\{\\tau_{ATE}^{(1)}, \\tau_{ATE}^{(2)},\\ldots, \\tau_{ATE}^{(S)}\\}.\\] This set is a Monte Carlo approximation of the entire posterior distribution of the Average Treatment Effect.\nThis is a profoundly powerful result. Instead of a single point estimate and a standard error, the Bayesian approach yields a full probability distribution for the unknown causal effect. From this posterior distribution, we can easily calculate a posterior mean (our best point estimate) and a 95% credible interval. Unlike a frequentist confidence interval, the Bayesian credible interval has a direct and intuitive probabilistic interpretation: given our data and model, there is a 95% probability that the true value of the ATE lies within this range. This propagation of uncertainty from the model parameters all the way to the final causal estimate is a hallmark of the Bayesian approach.\nWe now apply this framework to the Lalonde dataset to estimate the causal effect of the NSW job training program on 1978 earnings.\nThe analysis is streamlined by using the bartCause package in R, which is specifically designed for causal inference with BART. The package provides a wrapper around the core dbarts implementation, simplifying the process of fitting the model and generating counterfactuals. A typical function call would look like this:\n\n# Load the package and data\nlibrary(bartCause)\ndata(lalonde)\n\n# Define confounders\nconfounders &lt;- c('age', 'educ', 'black', 'hisp', 'married', 'nodegr', 're74', 're75')\n\n# Fit the BART model\nfit &lt;- bartc(\n  response = lalonde$re78,\n  treatment = lalonde$treat,\n  confounders = lalonde[, confounders],\n  estimand = \"ate\",\n  commonSup.rule = \"sd\" # Rule to handle poor overlap\n)\n\nIn this call, we specify the outcome (re78), the binary treatment (treat), and the matrix of pre-treatment confounders. We set estimand = ate to target the Average Treatment Effect.\nBefore interpreting the causal estimates, it is essential to perform MCMC diagnostics to ensure the algorithm has converged to a stable posterior distribution. The bartCause package provides plotting functions for this purpose. Trace plots for key parameters, such as the posterior draws of the ATE and the residual standard deviation (\\(\\sigma\\)), should be examined. These plots should show the chains mixing well and exploring a consistent region of the parameter space, without long-term drifts or stuck periods, indicating that the sampler has converged.\nThe primary result can be obtained by calling summary(fit). This provides the posterior mean of the ATE, which serves as our point estimate, along with a 95% credible interval. For a richer view, we can plot the entire posterior distribution of the ATE, which visualizes our uncertainty about the treatment effect.\nThe true power of this result is seen when placed in the context of other estimates, as shown in Table 14.2. The naive difference in means between the treated and control groups in the non-experimental data is large and negative, a direct consequence of the severe confounding. The experimental benchmark from the original RCT for this subset of treated individuals is an earnings gain of approximately $886. The BART estimate, after adjusting for the observed confounders, is remarkably close to this benchmark. This result demonstrates that a flexible, non-parametric Bayesian model like BART can successfully overcome the severe selection bias that plagued earlier econometric methods.\n\n\n\nTable 14.2: Comparison of ATE Estimates for the NSW Program. Note: Estimates are for the non-experimental Lalonde sample (treated units from NSW, control units from PSID). The experimental benchmark is the difference-in-means estimate from the randomized trial for the same treated units. Uncertainty for BART is the posterior standard deviation\n\n\n\n\n\n\n\n\n\n\nMethod\nATE Estimate\nUncertainty (Std. Dev. / Interval)\n\n\n\n\nExperimental Benchmark\n886.3\n-277.37\n\n\nNaive Difference-in-Means\n-8492.24\n-633.91\n\n\nPropensity Score Matching\n1079.13\n-158.59\n\n\nDouble Machine Learning\n370.94\n-394.68\n\n\nCausal BART\n818.79\n-184.46\n\n\n\n\n\n\nWhile the ATE provides a useful summary, it can mask important variations in how the treatment affects different people. A policy might be beneficial on average but ineffective or even harmful for certain subgroups. A key advantage of BART is its ability to move beyond the average and explore this Heterogeneous Treatment Effect (HTE), which is critical for developing more targeted and effective policies.\nEstimating HTE allows us to answer questions like: “For whom does this program work best?” or “Are there individuals for whom the program is detrimental?” In settings with limited resources, this information is vital for allocating the intervention to those most likely to benefit. The flexibility of BART, which does not assume a constant treatment effect, makes it an ideal tool for this task.\nBecause BART provides a posterior predictive distribution of potential outcomes for every individual in the dataset, we can estimate an Individual Conditional Average Treatment Effect (ICATE) for each person. By plotting a histogram of the posterior means of these ICATEs, we can visualize the distribution of effects across the sample. This reveals whether the effect is consistent for everyone or if there is substantial variation, with some individuals benefiting much more than others.\nTo understand what drives this heterogeneity, we can examine how the estimated CATE varies as a function of key pre-treatment covariates. These relationships are often visualized using partial dependence plots. For the Lalonde data, such analyses have revealed that the effect of the job training program is not constant but varies non-linearly with characteristics like age and pre-treatment income (re74). For instance, the program’s benefit might increase with age up to a certain point and then decline, or it might be most effective for individuals with low-to-moderate prior earnings but less so for those with very low or higher earnings. These are nuanced, data-driven insights that would be completely missed by a standard linear regression model that only estimates a single average effect.\nA subtle but important issue can arise when using flexible regularized models like BART for causal inference in the presence of strong confounding, as is the case here. The regularization priors, which are designed to prevent overfitting, can shrink the estimated effects of the confounders towards zero. Because the treatment Z is highly correlated with these confounders, the model may mistakenly attribute some of the effect of the confounders to the treatment, leading to a bias known as Regularization-Induced Confounding (RIC).\nA powerful solution, proposed by Hahn, Murray, and Carvalho (2020), is to first estimate the propensity score, \\(\\pi(x) = P(Z=1 \\mid X)\\), which is the probability of receiving treatment given the covariates X. This score serves as a one-dimensional summary of all confounding information. This estimated propensity score is then included as an additional predictor in the BART outcome model. By providing this confounding summary directly to the model, we help the BART algorithm differentiate between the prognostic effects of the covariates (captured by \\(\\pi(x)\\)) and the causal effect of the treatment Z, thereby mitigating RIC. This “ps-BART” approach is considered state-of-the-art and is easily implemented in the bartCause package by setting the argument p.scoreAsCovariate = TRUE.\n\nBART versus Propensity Score Matching (PSM)\nBART is one of several methods for causal inference from observational data. It is instructive to compare its philosophy with that of another widely used technique: Propensity Score Matching (PSM). BART and PSM represent two different philosophies for tackling confounding. Propensity Score Matching (PSM): This approach focuses on the design of the study. The goal is to use the observed data to construct a new sample in which the treatment and control groups are balanced on their observed covariates, thereby mimicking the properties of an RCT. The propensity score is the central tool used to achieve this balance. The analysis of the outcome is then performed on this newly created, “balanced” dataset.\nBART focuses on the analysis stage. The goal is to build a highly flexible and accurate predictive model for the outcome that explicitly includes the treatment and confounders, \\(\\E{Y \\mid X,Z}\\). It uses the full dataset and relies on the model’s ability to correctly adjust for the confounding variables to isolate the causal effect.\nEach approach has its own set of advantages and disadvantages. PSM is often praised for its transparency; one can assess the quality of the covariate balance achieved by the matching procedure before ever looking at the outcome variable, reducing the risk of “p-hacking” or specification searching. However, PSM can be inefficient, as it often requires discarding a significant portion of the control group that does not have good matches in the treated group (i.e., poor overlap). It can also suffer from residual confounding if the matches are not sufficiently close. BART, on the other hand, is highly efficient as it uses all available data. Its main strengths are its flexibility in capturing unknown functional forms and interactions, its ability to easily estimate heterogeneous effects, and its principled framework for uncertainty quantification. Its primary weakness is that it can be perceived as a “black box” if not diagnosed carefully. Its validity, like all modeling approaches, depends on the untestable ignorability assumption, and as discussed, it can be susceptible to regularization-induced confounding if not applied with care.\nIn modern practice, the line between these two philosophies is blurring. It is now common to see them used in conjunction. For example, many practitioners use flexible machine learning models, including BART itself, to estimate the propensity scores used for matching or weighting, which can improve the quality of the covariate balance over simpler logistic regression models. Conversely, the state-of-the-art application of BART for causal inference (ps-BART) incorporates the propensity score directly into the outcome model. This convergence reflects a mature understanding that both balancing the data structure and flexibly modeling the outcome are complementary and powerful tools for robust causal inference.\n\n\n\n\n\n\n\n\nFeature\nPropensity Score Matching (PSM)\nBayesian Additive Regression Trees (BART)\n\n\n\n\nPrimary Goal\nCreate balanced treatment/control groups (Design)\nFlexibly model the outcome-covariate relationship (Analysis)\n\n\nUse of Data\nOften discards unmatched units, reducing sample size\nUses all available data\n\n\nConfounding Control\nAchieved by balancing covariates via matching/weighting\nAchieved by conditioning on covariates in a flexible model\n\n\nKey Assumption\nCorrect specification of the propensity score model\nCorrect specification of the outcome model (though BART is very flexible)\n\n\nTreatment Effect\nPrimarily estimates ATT; ATE can be harder to estimate\nEasily estimates ATE, ATT, and CATE/HTE\n\n\nUncertainty\nOften requires bootstrapping for standard errors\nProvides full posterior distributions and credible intervals naturally\n\n\nFlexibility\nLimited by the PS model; main effect is assumed constant after matching\nHighly flexible; automatically models non-linearities and interactions\n\n\n\n:: Conceptual Comparison of BART and Propensity Score Matching\nThis example shows that BART, a flexible non-parametric method, can successfully adjust for severe confounding and recover a causal estimate that is remarkably close to the experimental benchmark, a feat that eluded many of the methods available when Lalonde first published his critique. It is crucial to remember that BART is not a panacea. Its validity, like that of any non-experimental method, rests on the untestable assumption of ignorability—that we have measured and adjusted for all relevant confounding variables.\n\n\nConclusion\nRandomization remains the cleanest route to causal identification, but it is not always feasible. When randomization is absent, modern Bayesian methods like BART offer a principled path forward: they combine flexible non-parametric modeling with uncertainty quantification and can, under the right assumptions, recover causal effects that approach the quality of experimental benchmarks. The broader narrative on experiments, randomization, and when we must rely on observational data is developed in Chapter 5.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#why-ensembles-work-a-geometric-perspective",
    "href": "14-tree.html#why-ensembles-work-a-geometric-perspective",
    "title": "14  Tree Models",
    "section": "15.1 Why Ensembles Work: A Geometric Perspective",
    "text": "15.1 Why Ensembles Work: A Geometric Perspective\nThe ensemble methods we’ve discussed—bagging, random forests, and boosting—all share a common goal: improving predictive performance by combining multiple weak learners. But why does this work so well? The answer lies in the bias-variance tradeoff and the geometry of high-dimensional spaces.\n\nSmoothing and Variance Reduction\nIn essence, individual decision trees are low-bias but high-variance estimators. They can capture complex patterns (low bias) but are sensitive to noise (high variance). When we average many such trees, as in Bagging or Random Forests, we are effectively performing smoothing.\nQuantitatively, if we have \\(N\\) uncorrelated predictors \\(f_1, \\dots, f_N\\), the variance of their average is: \\[\n\\text{Var}\\left(\\frac{1}{N}\\sum_{i=1}^N f_i\\right) = \\frac{1}{N} \\text{Var}(f_i)\n\\] Averaging \\(N\\) uncorrelated models reduces the variance by a factor of \\(N\\). This is the \\(1/N\\) rule. In practice, the trees in a forest are correlated, so the reduction isn’t quite \\(1/N\\), but the principle holds: the more diverse (uncorrelated) the trees, the more variance reduction we get. This is why Random Forests Randomly select features at each split—to force the trees to be different.\n\n\nThe Problem of High Dimensions\nOne might ask: “Why not just use K-Nearest Neighbors (KNN)? It also averages local points.” The problem is the curse of dimensionality. In high-dimensional feature spaces, data points become incredibly sparse.\nConsider a 50-dimensional sphere. As shown in Figure 15.1, if we sample points uniformly, almost all of them will reside near the “crust” or surface of the sphere, not in the center.\n\n\n\n\n\n\nFigure 15.1\n\n\n\nThis phenomenon means that in high dimensions, “local neighbors” are not actually close to you—they are far away on the other side of the space. A standard KNN algorithm fails because it averages points that aren’t truly similar.\n\n\nTrees as “Adaptive” Nearest Neighbors\nDecision trees solve this by defining “neighbors” differently. Instead of using a fixed distance metric (like Euclidean distance), trees define a neighborhood as a rectangular box (or cylindrical region) learned from the data (Figure 15.2).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: Cylindrical kernels for trees (left) and random forests (right).\n\n\n\nConstructing the regions is fundamental to reducing the curse of dimensionality. It is useful to imagine a very large dataset, e.g., 100k images, and think about how a new image’s input coordinates, \\(X\\), are “neighbors” to data points in the training set. Our predictor will then be a smart conditional average of the observed outputs, \\(Y\\), for our neighbors. When \\(p\\) is large, spheres (\\(L^2\\) balls or Gaussian kernels) are terrible: either no points or all points are “neighbors” of the new input variable. Trees are good as not too many “neighbors”.\nTo illustrate the problem further, Figure 15.1 below shows the 2D image of 1000 uniform samples from a 50-dimensional ball \\(B_{50}\\). The image is calculated as \\(w^T Y\\), where \\(w = (1,1,0,\\ldots,0)\\) and \\(Y \\sim U(B_{50})\\). Samples are centered around the equators and none of the samples fall close to the boundary of the set.\nAs dimensionality of the space grows, the variance of the marginal distribution goes to zero. We can empirically see it from Figure 15.3, which shows histogram of 1D image of uniform sample from balls of different dimensionality, i.e. \\(e_1^T Y\\), where \\(e_1 = (1,0,\\ldots,0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.3: Histogram of marginal distribution of \\(Y\\sim U(B_p)\\) for different dimensions \\(p\\) (x-axis).\n\n\n\nSimilar central limit results were known to Maxwell who showed that random variable \\(w^TY\\) is close to standard normal, when \\(Y \\sim U(B_p)\\), \\(p\\) is large, and \\(w\\) is a unit vector (lies on the boundary of the ball). For the history of this fact, see Diaconis and Freedman (1987). More general results in this direction were obtained in Klartag (2007). Further, Milman and Schechtman (2009) presents many analytical and geometrical results for finite dimensional normed spaces, as the dimension grows to infinity.\nDeep learning improves on this by performing a sequence of GLM-like transformations; effectively, DL learns a distributed partition of the input space. Specifically, suppose that we have \\(K\\) partitions. Then the DL predictor takes the form of a weighted average, or in the case of classification, a soft-max of the weighted average of observations in this partition. Given a new high-dimensional input \\(X_{\\mathrm{new}}\\), many deep learners are an average of learners obtained by our hyperplane decomposition. Generically, we have\n\\[\n\\hat{Y}(X) = \\sum_{k \\in K} w_k(X)\\hat{Y}_k(X),\n\\] where \\(w_k\\) are the weights learned in region \\(k\\), and \\(w_k(X)\\) is an indicator of the region with appropriate weighting given the training data. The weight \\(w_k\\) also indicates which partition the new \\(X_{\\mathrm{new}}\\) lies in.\nThe use of pooling (a.k.a. averaging) of multiple predictors is commonplace in machine learning. Ensemble methods (a.k.a. some form of clever conditional averaging) are prevalent in high dimensions. One reason for these procedures is that it is relatively easy to find unbiased predictors, with the caveat that they have large variances due to dimensionality. The following result on exchangeability (Kingman, 1975) shows that we can simply use the \\(1/N\\)-rule and average to reduce risk. Specifically, suppose that we have \\(K\\) exchangeable, \\(\\mathbb{E} ( \\hat{Y}_i ) = \\mathbb{E} ( \\hat{Y}_{\\pi(i)} )\\), predictors\n\\[\n\\hat{Y} = ( \\hat{Y}_1 , \\ldots , \\hat{Y}_K )\n\\]\nFind \\(w\\) to attain \\(\\operatorname{argmin}_W E l( Y , w^T \\hat{Y} )\\) where \\(l\\) convex in the second argument;\n\\[\nE l( Y , w^T \\hat{Y} ) = \\frac{1}{K!} \\sum_\\pi E l( Y , w^T \\hat{Y} ) \\geq E l \\left ( Y , \\frac{1}{K!} \\sum_\\pi w_\\pi^T \\hat{Y} )\\right )\n\\] \\[\n= E l \\left ( Y , (1/K) \\iota^T \\hat{Y} \\right )\n\\]\nwhere \\(\\iota = ( 1 , \\ldots ,1 )\\). Hence, the randomized multiple predictor with weights \\(w = (1/K)\\iota\\) provides close to optimal Bayes predictive performance. We now turn to algorithmic issues.\nAn alternative approach is to perform Bayesian model selection. Here we calculate the optimal Bayes weight for each predictor in accordance with Bayes Rule. We formalize the gains in Classification Risk with the following discussion.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "14-tree.html#classification-variance-decomposition",
    "href": "14-tree.html#classification-variance-decomposition",
    "title": "14  Tree Models",
    "section": "15.2 Classification variance decomposition",
    "text": "15.2 Classification variance decomposition\nAmit, Blanchard, and Wilder (2000) provide a rigorous connection between the strength of individual classifiers and their correlation. To formalize this, we need to establish some notation. Consider a \\(K\\)-class classification problem where:\n\n\\(X\\) denotes the input feature vector\n\\(Y \\in \\{1, 2, \\ldots, K\\}\\) denotes the true class label\n\\(c \\in \\{1, 2, \\ldots, K\\}\\) represents a specific true class\n\\(d \\in \\{1, 2, \\ldots, K\\}\\) represents a candidate class label (the class we’re voting for)\n\nIn an ensemble setting, let \\(h(X, d)\\) denote an individual classifier (e.g., a single decision tree) that outputs a vote for class \\(d\\) given input \\(X\\). Typically, \\(h(X, d) \\in \\{0, 1\\}\\) for hard voting (1 if the classifier predicts class \\(d\\), 0 otherwise) or \\(h(X, d) \\in [0, 1]\\) for soft voting (the probability that the classifier assigns to class \\(d\\)).\nThe ensemble aggregates votes from multiple classifiers. Let \\(\\mathbf{Q}\\) denote the distribution over classifiers—this is the randomization scheme (e.g., bootstrap sampling, random feature selection) that generates the trees in the ensemble. The aggregate classifier \\(H_{\\mathbf{Q}}(X, d)\\) is the average vote for class \\(d\\) over all classifiers sampled from \\(\\mathbf{Q}\\):\n\\[\nH_{\\mathbf{Q}}(X, d) = E_{\\mathbf{Q}}[h(X, d)] = \\frac{1}{B}\\sum_{b=1}^B h_b(X, d)\n\\]\nwhere \\(h_b\\) are individual classifiers sampled from \\(\\mathbf{Q}\\) and \\(B\\) is the number of classifiers in the ensemble. In practice, this is simply the proportion of trees that vote for class \\(d\\) (or the average probability assigned to class \\(d\\) across all trees).\nThe final classification decision \\(C_{\\mathbf{Q}}(X)\\) is the class that receives the most votes (or highest average probability):\n\\[\nC_{\\mathbf{Q}}(X) = \\arg\\max_{d \\in \\{1, \\ldots, K\\}} H_{\\mathbf{Q}}(X, d)\n\\]\nNow, let \\(P_{c}\\) denote the population conditional probability distribution of a point \\(X\\) given \\(Y=c\\), and let \\(E_{c}\\) and \\(\\operatorname{Var}_{c}\\) denote the associated conditional expectation and variance operators (i.e., \\(E_c[\\cdot] = E[\\cdot \\mid Y=c]\\) and \\(\\operatorname{Var}_c[\\cdot] = \\operatorname{Var}[\\cdot \\mid Y=c]\\)). Define the vectors of average aggregates conditional on class \\(c\\) as \\[\nM_{c}(d)=E_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right]=E\\left[H_{\\mathbf{Q}}(X, d) \\mid Y=c\\right]\n\\]\nfor \\(d=1, \\ldots, K\\). Intuitively, \\(M_c(d)\\) represents the expected vote share for class \\(d\\) when the true class is \\(c\\). Ideally, \\(M_c(c)\\) (votes for the correct class) should be much larger than \\(M_c(d)\\) (votes for incorrect classes).\nThe average conditional margin (ACM) for class \\(c\\) is defined as \\[\n\\theta_{c}=\\min _{d \\neq c}\\left(M_{c}(c)-M_{c}(d)\\right)\n\\] This \\(\\theta_c\\) measures the “safety gap” between the correct class and the closest competing class. A larger margin means the classifier is more robust.\nWe assume that \\(\\theta_{c}&gt;0\\). This assumption is very weak since it involves only the average over the population of class \\(c\\). It is quite natural since one would not expect good classification results when it is violated. Indeed as shown below it is satisfied in all cases.\nGiven that \\(\\theta_{c}&gt;0\\), the error rate for class \\(c\\) depends on the extent to which the aggregate classifier \\(H_{\\mathbf{Q}}(X, d)\\) is concentrated around \\(M_{c}(d)\\) for each \\(d=1, \\ldots, K\\). The simplest measure of concentration is the variance of \\(H_{\\mathbf{Q}}(X, d)\\) with respect to the distribution \\(P_{c}\\). Using Chebyshev’s inequality we write a coarse bound on the misclassification probability with respect to \\(P_{c}\\) as follows.\n\\[\\begin{align*}\nP_{c}\\left(C_{\\mathbf{Q}}(X) \\neq c\\right) \\leq & P_{c}\\left(H_{\\mathbf{Q}}(X, c)&lt;M_{c}(c)-\\theta_{c} / 2\\right) \\\\\n& +\\sum_{d \\neq c} P_{c}\\left(H_{\\mathbf{Q}}(X, d)&gt;M_{c}(d)+\\theta_{c} / 2\\right) \\\\\n\\leq & \\sum_{d=1}^{K} P_{c}\\left(\\left|H_{\\mathbf{Q}}(X, d)-M_{c}(d)\\right|&gt;\\theta_{c} / 2\\right) \\\\\n\\leq & \\frac{4}{\\theta_{c}^{2}} \\sum_{d=1}^{K} \\operatorname{Var}_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right] . \\tag{10}\n\\end{align*}\\]\nOf course Chebyshev’s inequality is coarse and will not give very sharp results in itself, but we state it here as a landmark pointing to the relative importance of margin and variance, and to the tradeoff between the two quantities. To reduce error, we must either increase the margin \\(\\theta_c\\) (build stronger classifiers) or decrease the variance (add more diverse trees).\nWe rewrite each of the variance terms of the last equation as\n\\[\\begin{align*}\n\\operatorname{Var}_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right] & =E_{c}\\left[H_{\\mathbf{Q}}(X, d)\\right]^{2}-\\left[E_{c} H_{\\mathbf{Q}}(X, d)\\right]^{2} \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} E_{c}\\left[h_{1}(X, d) h_{2}(X, d)\\right]-E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\left[E_{c}\\left[h_{1}(X, d)\\right] E_{c}\\left[h_{2}(X, d)\\right]\\right] \\\\\n& =E_{\\mathbf{Q} \\otimes \\mathbf{Q}} \\operatorname{Cov}_{c}\\left[h_{1}(X, d), h_{2}(X, d)\\right] \\doteq \\gamma_{c, d} \\tag{11}\n\\end{align*}\\]\nHere, \\(E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\) denotes the expectation over the product measure of two independent draws from \\(\\mathbf{Q}\\). That is, \\(h_1\\) and \\(h_2\\) are two classifiers sampled independently from the distribution \\(\\mathbf{Q}\\), and \\(E_{\\mathbf{Q} \\otimes \\mathbf{Q}}\\) averages over all such pairs. This allows us to express the variance of the ensemble in terms of the covariance between pairs of individual classifiers.\nThis equation formalizes the intuition: to minimize error, we must minimize the covariance \\(\\gamma_{c, d}\\) between trees. This is precisely what Random Forests do by randomizing splits (Amit, Blanchard, and Wilder (2000)).\n\nThe Nearest Neighbor Insight\nWhy do trees work so well on complex data? Cover and Hart (1967) provided a fundamental result for nearest-neighbor classifiers: as sample size grows, the error rate of a simple 1-Nearest Neighbor classifier is bounded by twice the Bayes Risk (the irreducible error). \\[\nR^* \\le R_{NN} \\le 2R^*(1-R^*)\n\\] This means that a simple “look at your neighbor” strategy captures at least half the available signal. Decision trees can be viewed as adaptive nearest-neighbor models. Instead of using a fixed distance metric (which fails in high dimensions due to sparsity), trees learn a custom metric—dividing space into “blocky” regions based only on relevant features.\n\n\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple Randomized Classifiers: MRCL.”\n\n\nCover, T., and P. Hart. 1967. “Nearest Neighbor Pattern Classification.” IEEE Transactions on Information Theory 13 (1): 21–27.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a Theory.” In Annales de l’IHP Probabilités Et Statistiques, 23:397–423.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020. “Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with Discussion).” Bayesian Analysis 15 (3): 965–1056.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex Sets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of Finite Dimensional Normed Spaces: Isoperimetric Inequalities in Riemannian Manifolds. Vol. 1200. Springer.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Tree Models</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html",
    "href": "15-forecasting.html",
    "title": "15  Forecasting",
    "section": "",
    "text": "Epidemic Forecasting: A Motivating Example\nIn 1766, Daniel Bernoulli developed the first mathematical model of disease transmission for smallpox, writing: “I wish simply that, in matters which so closely concern the well being of the human race, no decision shall be made without all knowledge which a little analysis and calculation can provide.” This pioneering work laid the foundation for epidemic modeling that continues to inform public health decisions today.\nOne of the most famous early applications involved the London Plague of 1665-1666, particularly in the village of Eyam near Sheffield. The data, shown below, tracked susceptible individuals (\\(S\\)) and infected individuals (\\(I\\)) over time:\nThe initial population was \\(N=261\\), and by the end of the outbreak, 83 individuals remained susceptible (never infected). This simple dataset illustrates a fundamental challenge in time series forecasting: understanding the mechanism driving the dynamics.\nThe minister of the local church in Eyam, John Ashton, kept detailed records of the outbreak. The missing record on Oct 3 is due to the fact that John Ashton himself was infected and died on that day. Crucially, Ashton recognized the necessity of isolation; he strictly enforced a quarantine on his own household until his death. He also meticulously recorded daily infection counts, pioneering data collection practices nearly two centuries before Florence Nightingale.\nTime series data appear across business, science, healthcare, and engineering. Unlike regression datasets, time series observations are not exchangeable—their order carries information. The modeling replacement for i.i.d. is typically conditional independence or state-space structure, but Bayesian updating logic remains the same as in Chapter 3.\nThis chapter describes the bsts software package, which fits sophisticated time series models with just a few lines of R code. The BSTS (Bayesian Structural Time Series) section is adapted from Steven Scott’s blog post: “Fitting Bayesian structural time series with the bsts R package”.\nThe Eyam example highlights several themes central to this chapter:\nModern epidemic forecasting combinines SEIR-type models with Bayesian methods to quantify uncertainty. While epidemic models differ from the business and economic time series we’ll focus on in this chapter, they share the fundamental state-space structure that makes Bayesian structural time series models so powerful.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#structural-time-series-models",
    "href": "15-forecasting.html#structural-time-series-models",
    "title": "15  Forecasting",
    "section": "15.1 Structural time series models",
    "text": "15.1 Structural time series models\nA structural time series model is defined by two equations. The observation equation relates the observed data \\(y_t\\) to a vector of latent variables \\(\\alpha_t\\) known as the “state.” \\[\ny_t = Z_t^T\\alpha_t + \\epsilon_t.\n\\]\nThe transition equation describes how the latent state evolves through time. \\[\n\\alpha_{t+1} = T_t \\alpha_t + R_t \\eta_t.\n\\]\nThe error terms \\(\\epsilon_t\\) and \\(\\eta_t\\) are Gaussian and independent of everything else. The arrays \\(Z_t\\), \\(T_t\\) and \\(R_t\\) are structural parameters. They may contain parameters in the statistical sense, but often they simply contain strategically placed 0’s and 1’s indicating which bits of \\(\\alpha_t\\) are relevant for a particular computation. An example will hopefully make things clearer.\nThe simplest useful model is the “local level model,” in which the vector \\(\\alpha_t\\) is just a scalar \\(\\mu_t\\). The local level model is a random walk observed in noise. \\[\\begin{align*}\ny_t = &\\mu_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\eta_t.\n\\end{align*}\\] Here \\(\\alpha_t=\\mu_t\\), and \\(Z_t\\), \\(T_t\\), and \\(R_t\\) all collapse to the scalar value 1. Similar to Bayesian hierarchical models for nested data, the local level model is a compromise between two extremes. The compromise is determined by variances of \\(\\epsilon_t \\sim N(0,\\sigma^2)\\) and \\(\\eta_t \\sim N(0,\\tau^2)\\). If \\(\\tau^2=0\\) then \\(\\mu_t\\) is a constant, so the data are IID Gaussian noise. In that case the best estimator of \\(y_{t+1}\\) is the mean of \\(y_1,\\ldots,y_t\\). Conversely, if \\(\\sigma^2=0\\) then the data follow a random walk, in which case the best estimator of \\(y_{t+1}\\) is \\(y_t\\). Notice that in one case the estimator depends on all past data (weighted equally) while in the other it depends only on the most recent data point, giving past data zero weight. If both variances are positive then the optimal estimator of \\(y_{t+1}\\) winds up being “exponential smoothing,” where past data are forgotten at an exponential rate determined by the ratio of the two variances. Also notice that while the state in this model is Markov (i.e. it only depends on the previous state), the dependence among the observed data extends to the beginning of the series.\n\n\n\n\n\n\n\n\n\nFigure 15.1: Apple Adjusted Closing Price\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: Apple Adjusted Closing Price\n\n\n\n\n\n\nIn the example above, one of the plots shows the price of Apple stock from 2021-01-01 to 2022-12-31. The other plot is a sequence generated from a random walk model fitted to the Apple price data. Can you spot which one is which?\nStructural time series models are useful because they are flexible and modular. The analyst chooses the structure of \\(\\alpha_t\\) based on things like whether short- or long-term predictions are more important, whether the data contains seasonal effects, and whether and how regressors are to be included. Many of these models are standard, and can be fit using a variety of tools, such as the StructTS function distributed with base R or one of several R packages for fitting these models (with the dlm package (Petris (2010), Campagnoli, Petrone, and Petris (2009)) deserving special mention). The bsts package handles all the standard cases, but it also includes several useful extensions, described in the next few sections through a series of examples. Each example includes a mathematical description of the model and example bsts code showing how to work with the model using the bsts software. To keep things short, details about prior assumptions are largely avoided.\n\nExample 15.1 (Nowcasting) S. Scott and Varian (2014) and Steven L. Scott and Varian (2015) used structural time series models to show how Google search data can be used to improve short-term forecasts (“nowcasts”) of economic time series. The figure below shows the motivating data set from S. Scott and Varian (2014), which is also included with the bsts package. The data consist of the weekly initial claims for unemployment insurance in the US, as reported by the US Federal Reserve. Like many official statistics, they are released with delay and subject to revision. At the end of the week, the economic activity determining these numbers has taken place, but the official numbers are not published until several days later. For economic decisions based on these and similar numbers, it would help to have an early forecast of the current week’s number as of the close of the week. Thus the output of this analysis is truly a “nowcast” of data that has already happened rather than a “forecast” of data that will happen in the future.\n\nlibrary(bsts)     # load the bsts package\ndata(iclaims)     # bring the initial.claims data into scope\nplot(initial.claims$iclaimsNSA, lwd=2, ylab=\"Unemployment claims (thousand)\", col=\"blue\")\n\n\n\n\n\n\n\nFigure 15.3: Weekly initial claims for unemployment in the US.\n\n\n\n\n\nThere are two sources of information about the current value \\(y_t\\) in the initial claims series: past values \\(y_{t-\\tau}\\) describing the time series behavior of the series, and contemporaneous predictors \\(x_t\\) from a data source which is correlated with \\(y_t\\), but which is available without the delay exhibited by \\(y_t\\). The time series structure shows an obvious trend (in which the financial and housing crises in 2008 - 2009 are apparent) as well as a strong annual seasonal pattern. The external data source explored by Scott and Varian was search data from Google trends with search queries such as “how to file for unemployment” having obvious relevance.\nScott and Varian modeled the data using a structural time series with three state components:\n\ntrend \\(\\mu_t\\)\nseasonal pattern \\(\\tau_t\\)\n\nregression component \\(\\beta^Tx_t\\).\n\nThe model is \\[\\begin{align*}\ny_t = & \\mu_t + \\tau_t + \\beta^T x_t + \\epsilon_t\\\\\n\\mu_{t+1} = &\\mu_t + \\delta_t + \\eta_{0t}\\\\\n\\delta_{t+1} = &\\delta_t + \\eta_{1t}\\\\\n\\tau_{t+1} = &-\\sum_{s = 1}^{S-1}\\tau_{t} + \\eta_{2t}.\n\\end{align*}\\]\nThe trend component looks similar to the local level model above, but it has an extra term \\(\\delta_t\\). Notice that \\(\\delta_t\\) is the amount of extra \\(\\mu\\) you can expect as \\(t\\rightarrow t+1\\), so it can be interpreted as the slope of the local linear trend. Slopes normally multiply some \\(x\\) variable, but in this case \\(x=\\Delta t\\), which is omitted from the equation because it is always 1. The slope evolves according to a random walk, which makes the trend an integrated random walk with an extra drift term. The local linear trend is a better model than the local level model if you think the time series is trending in a particular direction and you want future forecasts to reflect a continued increase (or decrease) seen in recent observations. Whereas the local level model bases forecasts around the average value of recent observations, the local linear trend model adds in recent upward or downward slopes as well. As with most statistical models, the extra flexibility comes at the price of extra volatility.\nThe best way to understand the seasonal component \\(\\tau_t\\) is in terms of a regression with seasonal dummy variables. Suppose you had quarterly data, so that \\(S=4\\). You might include the annual seasonal cycle using 3 dummy variables, with one left out as a baseline. Alternatively, you could include all four dummy variables but constrain their coefficients to sum to zero. The seasonal state model takes the latter approach, but the constraint is that the \\(S\\) most recent seasonal effects must sum to zero in expectation. This allows the seasonal pattern to slowly evolve. Scott and Varian described the annual cycle in the weekly initial claims data using a seasonal state component with \\(S=52\\). Of course weeks don’t neatly divide years, but given the small number of years for which Google data are available the occasional one-period seasonal discontinuity was deemed unimportant.\nLet’s ignore the regression component for now and fit a bsts model with just the trend and seasonal components.\n\nss &lt;- AddLocalLinearTrend(list(), initial.claims$iclaimsNSA)\nss &lt;- AddSeasonal(ss, initial.claims$iclaimsNSA, nseasons = 52)\nmodel1 &lt;- bsts(initial.claims$iclaimsNSA,state.specification = ss,niter = 1000)\n\nThe first thing to do when fitting a bsts model is to specify the contents of the latent state vector \\(\\alpha_t\\). The bsts package offers a library of state models, which are included by adding them to a state specification (which is just a list with a particular format). The call to AddLocalLinearTrend above adds a local linear trend state component to an empty state specification (the list() in its first argument). The call to AddSeasonal adds a seasonal state component with 52 seasons to the state specification created on the previous line. The state vector \\(\\alpha_t\\) is formed by concatenating the state from each state model. Similarly, the vector \\(Z_t\\) is formed by concatenating the \\(Z\\) vectors from the two state models, while the matrices \\(T_t\\) and \\(R_t\\) are combined in block-diagonal fashion.\nThe state specification is passed as an argument to bsts, along with the data and the desired number of MCMC iterations. The model is fit using an MCMC algorithm, which in this example takes about 20 seconds to produce 1000 MCMC iterations. The returned object is a list (with class attribute bsts). You can see its contents by typing\n\nnames(model1)\n## \"sigma.obs\"                  \"sigma.trend.level\"         \n## \"sigma.trend.slope\"          \"sigma.seasonal.52\"         \n## \"final.state\"                \"state.contributions\"       \n## \"one.step.prediction.errors\" \"log.likelihood\"            \n## \"has.regression\"             \"state.specification\"       \n## \"prior\"                      \"timestamp.info\"            \n## \"model.options\"              \"family\"                    \n## \"niter\"                      \"original.series\"\n\nThe first few elements contain the MCMC draws of the model parameters. Most of the other elements are data structures needed by various S3 methods (plot, print, predict, etc.) that can be used with the returned object. MCMC output is stored in vectors (for scalar parameters) or arrays (for vector or matrix parameters) where the first index in the array corresponds to MCMC iteration number, and the remaining indices correspond to dimension of the deviate being drawn.\nMost users won’t need to look inside the returned bsts object because standard tasks like plotting and prediction are available through familiar S3 methods. For example, there are several plot methods available.\n\npar(mar=c(4,4,2,0))\nplot(model1)\nplot(model1, \"components\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Components\n\n\n\n\n\n\n\nFigure 15.4: Structural time series model for unemployment claims\n\n\n\n\nThe Figure 15.4 (a) above shows the posterior distribution of model state. Blue circles are actual data points. The Figure 15.4 (b) shows the individual state components. The plot looks fuzzy because it is showing the marginal posterior distribution at each time point.\nThe default plot method plots the posterior distribution of the conditional mean \\(Z_t^T\\alpha_t\\) given the full data \\(y=y_1,\\ldots,y_T\\). Other plot methods can be accessed by passing a string to the plot function. For example, to see the contributions of the individual state components, pass the string “components” as a second argument, as shown above. The figure below shows the output of these two plotting functions. You can get a list of all available plots by passing the string help as the second argument.\nTo predict future values there is a predict method. For example, to predict the next 12 time points you would use the following commands.\n\npar(mar=c(4,4,0,0))\npred1 &lt;- predict(model1, horizon = 12)\nplot(pred1, plot.original = 156)\n\n\n\n\n\n\n\n\nThe output of predict is an object of class bsts.prediction, which has its own plot method. The plot.original = 156 argument says to plot the prediction along with the last 156 time points (3 years) of the original series.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#regression-with-spike-and-slab-priors",
    "href": "15-forecasting.html#regression-with-spike-and-slab-priors",
    "title": "15  Forecasting",
    "section": "15.2 Regression with spike and slab priors",
    "text": "15.2 Regression with spike and slab priors\nNow let’s add a regression component to the model described above, so that we can use Google search data to improve the forecast. The bsts package only includes 10 search terms with the initial claims data set, to keep the package size small, but S. Scott and Varian (2014) considered examples with several hundred predictor variables. When faced with large numbers of potential predictors it is important to have a prior distribution that induces sparsity. A spike and slab prior is a natural way to express a prior belief that most of the regression coefficients are exactly zero.\nA spike and slab prior is a prior on a set of regression coefficients that assigns each coefficient a positive probability of being zero. Upon observing data, Bayes’ theorem updates the inclusion probability of each coefficient. When sampling from the posterior distribution of a regression model under a spike and slab prior, many of the simulated regression coefficients will be exactly zero. This is unlike the “lasso” prior (the Laplace, or double-exponential distribution), which yields MAP estimates at zero but where posterior simulations will be all nonzero. You can read about the mathematical details of spike and slab priors in S. Scott and Varian (2014).\nWhen fitting bsts models that contain a regression component, extra arguments captured by ... are passed to the SpikeSlabPrior function from the BoomSpikeSlab package. This allows the analyst to adjust the default prior settings for the regression component from the bsts function call. To include a regression component in a bsts model, simply pass a model formula as the first argument.\n\n# Fit a `bsts` model with expected model size 1, the default.\nmodel2 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims)\n# Fit a `bsts` model with expected model size 5, to include more coefficients.\nmodel3 &lt;- bsts(iclaimsNSA ~ .,state.specification = ss,niter = 1000,data = initial.claims,expected.model.size = 5)  # Passed to SpikeSlabPrior.\n\nTo examine the output you can use the same plotting functions as before. For example, to see the contribution of each state component you can type\n\npar(mar=c(4,4,3,0))\nplot(model2, \"comp\")\n\n\n\n\n\n\n\n\nIt produces the contribution of each state component to the initial claims data, assuming a regression component with default prior. Compare to the previous model. The regression component is explaining a substantial amount of variation in the initial claims series.\nThere are also plotting functions that you can use to visualize the regression coefficients. The following commands plot posterior inclusion probabilities for predictors in the “initial claims” nowcasting example assuming an expected model size of 1 and 5.\npar(mar=c(4,0,0,0))\nplot(model2, \"coef\")\nplot(model3, \"coef\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Full\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\nFigure 15.5: Variable Importance\n\n\n\nThe search term “unemployment office” shows up with high probability in both models. Increasing the expected model size from 1 (the default) to 5 allows other variables into the model, though “Idaho unemployment” is the only one that shows up with high probability.\nThose probabilities are calculated from the histogram of the samples of each \\(\\beta\\) calculated by the estimation algorithm (MCMC)\npar(mar=c(4,4,0,0))\n# unemployment.office\nhist(model3$coefficients[,10], breaks = 40, main=\"\",xlab=\"unemployment.office\", col=\"lightblue\")\n# pennsylvania.unemployment\nhist(model3$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\nhist(model2$coefficients[,3], breaks = 40, main = \"\", xlab=\"pennsylvania.unemployment\", col=\"lightblue\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(b) Sparse\n\n\n\n\n\n\n\n\n\n\n\n(c) Full\n\n\n\n\n\n\n\nFigure 15.6: Sample from the distribution over two beta parameters",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#model-diagnostics-did-the-google-data-help",
    "href": "15-forecasting.html#model-diagnostics-did-the-google-data-help",
    "title": "15  Forecasting",
    "section": "15.3 Model diagnostics: did the Google data help?",
    "text": "15.3 Model diagnostics: did the Google data help?\nAs part of the model fitting process, the algorithm generates the one-step-ahead prediction errors \\(y_t - E(y_t | Y_{t-1}, \\theta)\\), where \\(Y_{t-1}=y_1,\\ldots,y_{t-1}\\), and the vector of model parameters \\(\\theta\\) is fixed at its current value in the MCMC algorithm. The one-step-ahead prediction errors can be obtained from the bsts model by calling bsts.prediction.errors(model1).\nThe one step prediction errors are a useful diagnostic for comparing several bsts models that have been fit to the same data. They are used to implement the function CompareBstsModels, which is called as shown below.\n\nCompareBstsModels(list(\"Model 1\" = model1,\n               \"Model 2\" = model2,\n               \"Model 3\" = model3),\n          colors = c(\"black\", \"red\", \"blue\"))\n\n\n\n\nComparison of Errors for the three models.\n\n\n\n\nThe bottom panel shows the original series. The top panel shows the cumulative total of the mean absolute one step prediction errors for each model. The final time point in the top plot is proportional to the mean absolute prediction error for each model, but plotting the errors as a cumulative total lets you see particular spots where each model encountered trouble, rather than just giving a single number describing each model’s predictive accuracy. This figure shows that the Google data help explain the large spike near 2009, where model 1 accumulates errors at an accelerated rate, but models 2 and 3 continue accumulating errors at about the same rate they had been before. The fact that the lines for models 2 and 3 overlap in this figure means that the additional predictors allowed by the relaxed prior used to fit model 3 do not yield additional predictive accuracy.\n\nExample 15.2 (Long-term forecasting) A common question about bsts is “which trend model should I use?” To answer that question it helps to know a bit about the different models that the bsts software package provides, and what each model implies. In the local level model, the state evolves according to a random walk: \\[\n\\mu_{t+1}=\\mu_t+\\eta_t.\n\\] If you place your eye at time 0 and ask what happens at time \\(t\\), you find that \\(\\mu_t \\sim N(\\mu_0,t\\sigma^2_\\eta)\\). The variance continues to grow with \\(t\\), all the way to \\(t=\\infty\\). The local linear trend is even more volatile. When forecasting far into the future, the flexibility provided by these models becomes a double-edged sword, as local flexibility in the near term translates into extreme variance in the long term.\nAn alternative is to replace the random walk with a stationary AR process. For example \\[\n\\mu_{t+1}=\\rho\\mu_t+\\eta_t,\n\\]\nwith \\(\\eta_t \\sim N(0,\\sigma^2_{\\eta})\\) and \\(|\\rho|&lt;1\\). This model has stationary distribution \\[\n\\mu_{\\infty} \\sim N\\left(0,\\frac{\\sigma^2_{\\eta}}{1-\\rho^2}\\right),\n\\] which means that uncertainty grows to a finite asymptote, rather than infinity, in the distant future. The bsts package offers autoregressive state models through the functions AddAr, when you want to specify a certain number of lags, and AddAutoAr when you want the software to choose the important lags for you.\nA hybrid model modifies the local linear trend model by replacing the random walk on the slope with a stationary AR(1) process, while keeping the random walk for the level of the process. The bsts package refers to this is the “semilocal linear trend” model. \\[\\begin{align*}\n\\mu_{t+1}=&    \\mu_t+\\delta_t+\\eta_{0t}\\\\\n\\delta_{t+1}=& D+\\rho(\\delta_t-D)+\\eta_{1t}\n\\end{align*}\\] The \\(D\\) parameter is the long-run slope of the trend component, to which \\(\\delta_t\\) will eventually revert. However, \\(\\delta_t\\) can have short-term autoregressive deviations from the long-term trend, with memory determined by \\(\\rho\\). Values of \\(\\rho\\) close to 1 will lead to long deviations from \\(D\\). To see the impact this can have on long-term forecasts, consider the time series of daily closing values for the S&P 500 stock market index over the last 5 years, shown below.\n\nGSPC = read.csv(\"../data/GSPC.csv\")\nGSPC = xts(GSPC, order.by = as.Date(rownames(GSPC), \"%Y-%m-%d\"))\nknitr::kable(head(GSPC))\nplot(GSPC$GSPC.Adjusted, main=\"\")\n\n\n\n\nDaily closing values for the S&P 500 stock market index\n\n\n\n\nConsider two forecasts of the daily values of this series for the next 360 days. The first assumes the local linear trend model. The second assumes the semilocal linear trend.\n\nsp500 = GSPC$GSPC.Adjusted\nss1 &lt;- AddLocalLinearTrend(list(), sp500)\nmodel1 &lt;- bsts(sp500, state.specification = ss1, niter = 1000)\nss2 &lt;- AddSemilocalLinearTrend(list(), sp500)\nmodel2 &lt;- bsts(sp500, state.specification = ss2, niter = 1000)\n\nThe figure below shows long-term forecasts of the S&P 500 closing values under the (left) local linear trend and (right) semilocal linear trend state models.\nload(\"../data/timeseries/model12-sp500.RData\")\npar(mar=c(4,4,0,1))\npred1 &lt;- predict(model1, horizon = 360)\npred2 &lt;- predict(model2, horizon = 360)\nplot(pred2, plot.original = 360, ylim = range(pred1))\nplot(pred1, plot.original = 360, ylim = range(pred1))\n\n\n\n\n\n\n\n\n\n\n\n(a) Semi-local trend\n\n\n\n\n\n\n\n\n\n\n\n(b) Local trend\n\n\n\n\n\n\n\nFigure 15.7: S&P 500 Prediction\n\n\n\nNot only are the forecast expectations from the two models different, but the forecast errors from the local linear trend model are implausibly wide, including a small but nonzero probability that the S&P 500 index could close near zero in the next 360 days. The error bars from the semilocal linear trend model are far more plausible, and more closely match the uncertainty observed over the life of the series thus far.\n\n\nExample 15.3 (Recession modeling using non-Gaussian data) Although we have largely skipped details about how the bsts software fits models, the Gaussian error assumptions in the observation and transition equations are important for the model fitting process. Part of that process involves running data through the Kalman filter, which assumes Gaussian errors in both the state and transition equations. In many settings where Gaussian errors are obviously inappropriate, such as for binary or small count data, one can introduce latent variables that give the model a conditionally Gaussian representation. Well known “data augmentation” methods exist for probit regression (Albert (1993)) and models with student-T errors (Rubin (2015)). Somewhat more complex methods exist for logistic regression (Frühwirth-Schnatter and Frühwirth (2007), Held and Holmes (2006), Gramacy and Polson (2012)) and Poisson regression (Frühwirth-Schnatter et al. (2008)). Additional methods exist for quantile regression (Benoit and Van den Poel (2012)), support vector machines (Polson and Scott (2011)), and multinomial logit regression (Frühwirth-Schnatter and Frühwirth (2010)). These are not currently provided by the bsts package, but they might be added in the future.\nTo see how non-Gaussian errors can be useful, consider the analysis done by Berge, Sinha, and Smolyansky (2016), who used Bayesian model averaging (BMA) to investigate which of several economic indicators would best predict the presence or absence of a recession. We will focus on their nowcasting example, which models the probability of a recession at the same time point as the predictor variables. Berge, Sinha, and Smolyansky (2016) also analyzed the data with the predictors at several lags.\nThe model used in Berge, Sinha, and Smolyansky (2016) was a probit regression, with Bayesian model averaging used to determine which predictors should be included. The response variable was the presence or absence of a recession (as determined by NBER).\n\ndat &lt;- read.csv(\"../data/timeseries/rec_data_20160613.csv\")\nrec = ts(dat$nber, start=c(1973, 1), end=c(2016, 5), frequency=12)\nplot(rec, type='l', col='blue', ylab=\"Recession\")\n\n\n\n\nRecession periods identified by NBER\n\n\n\n\nThe BMA done by Berge, Sinha, and Smolyansky (2016) is essentially the same as fitting a logistic regression under a spike-and-slab prior with the prior inclusion probability of each predictor set to 1/2. That analysis can be run using the BoomSpikeSlab R package (Steven L. Scott (2022)), which is similar to bsts, but with only a regression component and no time series.\nThe logistic regression model is highly predictive, but it ignores serial dependence in the data. To capture serial dependence, consider the following dynamic logistic regression model with a local level trend model. \\[\\begin{align*}\n\\mathrm{logit}(p_t)= &  \\mu_t+\\beta^Tx_t\\\\\n\\mu_{t+1}= &            \\mu_t+\\eta_t\n\\end{align*}\\] Here \\(p_t\\) is the probability of a recession at time \\(t\\) ,and \\(x_t\\) is the set of economic indicators used by Berge, Sinha, and Smolyansky (2016) in their analysis. The variables are listed in the table below\n\n\n\n\n\n\n\n\nVariable\nDefinition/notes\nTransformation\n\n\n\n\nFinancial Variables\n\n\n\n\nSlope of yield curve\n10-year Treasury less 3-month yield\n\n\n\nCurvature of yield curve\n2 x 2-year minus 3-month and 10-year\n\n\n\nGZ index\nGilchrist and Zakrajsek (AER, 2012)\n\n\n\nTED spread\n3-month ED less 3-month Treasury yield\n\n\n\nBBB corporate spread\nBBB less 10-year Treasury yield\n\n\n\nS 500, 1-month return\n\n1-month log diff.\n\n\nS 500, 3-month return\n\n3-month log diff.\n\n\nTrade-weighted dollar\n\n3-month log diff.\n\n\nVIX\nCBOE and extended following Bloom\n\n\n\nMacroeconomic Indicators\n\n\n\n\nReal personal consumption expend.\n\n3-month log diff.\n\n\nReal disposable personal income\n\n3-month log diff.\n\n\nIndustrial production\n\n3-month log diff.\n\n\nHousing permits\n\n3-month log diff.\n\n\nNonfarm payroll employment\n\n3-month log diff.\n\n\nInitial claims\n4-week moving average\n3-month log diff.\n\n\nWeekly hours, manufacturing\n\n3-month log diff.\n\n\nPurchasing managers index\n\n3-month log dif\n\n\n\nFirst, we prepare the data by shifting it by \\(h\\), which is the forecast horizon.\n\nh=0\n# predict h months ahead\ny.h &lt;- dat$nber[-(1:h)]\nhh &lt;- length(dat$nber) - h\ndat.h &lt;- dat[1:hh,-1]\n# h=0 is a special case\nif(h==0) y.h   &lt;- dat$nber\nif(h==0) dat.h &lt;- dat[,-1]\n\nTo fit this model, we can issue the commands shown below.\n\n# Because 'y' is 0/1 and the state is on the logit scale the default prior\n# assumed by AddLocalLevel won't work here, so we need to explicitly set the\n# priors for the variance of the state innovation errors and the initial value\n# of the state at time 0.  The 'SdPrior' and 'NormalPrior' functions used to\n# define these priors are part of the Boom package.  See R help for\n# documentation.  Note the truncated support for the standard deviation of the\n# random walk increments in the local level model.\n# A more complex model\nss &lt;- AddLocalLevel(list(),y.h,\n                    sigma.prior = SdPrior(sigma.guess = .1,\n                                          sample.size = 1,\n                                          upper.limit = 1),\n                    initial.state.prior = NormalPrior(0, 5))\n# Tell bsts that the observation equation should be a logistic regression by\n# passing the 'family = \"logit\"' argument.\nts.model &lt;- bsts(y.h ~ ., ss, data = dat.h, niter = 20000,family = \"logit\", expected.model.size = 10)\n\nNow let’s plot the results\npar(mar=c(4,4,0,0))\nplot(ts.model,\"coef\")\nplot(ts.model)\nlines(y.h, lwd=3,col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(ts.model,\"predictors\")\n\n\n\n\n\n\n\n\nNotice that the distribution of \\(p_t\\) is moving to very large values during a recession, and to very small values outside of a recession. This effect captures the strong serial dependence in the recession data. Recessions are rare, but once they occur they tend to persist. Assuming independent time points is therefore unrealistic, and it substantially overstates the amount of information available to identify logistic regression coefficients.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#final-remarks-on-structural-models",
    "href": "15-forecasting.html#final-remarks-on-structural-models",
    "title": "15  Forecasting",
    "section": "15.4 Final Remarks on Structural Models",
    "text": "15.4 Final Remarks on Structural Models\nThe preceding examples have shown that the bsts software package can handle several nonstandard, but useful, time series applications. These include the ability to handle large numbers of contemporaneous predictors with spike and slab priors, the presence of trend models suitable for long term forecasting, and the ability to handle non-Gaussian data. Further, bsts offers support for multiple seasonalities. For example, if you have several weeks of hourly data then you will have an hour-of-day effect as well as a day-of-week effect. You can model these using a single seasonal effect with 168 seasons (which would allow for different hourly effects on weekends and weekdays), or you can assume additive seasonal patterns using the season.duration argument to AddSeasonal.\nss &lt;- AddSeasonal(ss, y, nseasons = 24)\nss &lt;- AddSeasonal(ss, y, nseasons = 7, season.duration = 24)\nThe latter specifies that each daily effect should remain constant for 24 hours. For modeling physical phenomena, bsts also offers trigonometric seasonal effects, which are sine and cosine waves with time varying coefficients. You obtain these by calling AddTrig. Time varying effects are available for arbitrary regressions with small numbers of predictor variables through a call to AddDynamicRegression.\nIn addition to the trend models discussed so far, the function AddStudentLocalLinearTrend gives a version of the local linear trend model that assumes student-t errors instead of Gaussian errors. This is a useful state model for short term predictions when the mean of the time series exhibits occasional dramatic jumps. Student-t errors can be introduced into the observation equation by passing the family = \"student\" argument to the bsts function call. Allowing for heavy tailed errors in the observation equation makes the model robust against individual outliers, while heavy tails in the state model provides robustness against sudden persistent shifts in level or slope. This can lead to tighter prediction limits than Gaussian models when modeling data that have been polluted by outliers. The observation equation can also be set to a Poisson model for small count data if desired.\nFinally, the most recent update to bsts supports data with multiple observations at each time stamp. The Gaussian version of the model is \\[\\begin{align*}\ny_{it} = &\\beta^T x_{it} + Z_t^T\\alpha_t + \\epsilon_{it}\\\\\n\\alpha_{t+1} = & T_t \\alpha_t + R_t \\eta_t,\n\\end{align*}\\] which is best understood as a regression model with a time varying intercept.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#beyond-state-space-models-machine-learning-approaches",
    "href": "15-forecasting.html#beyond-state-space-models-machine-learning-approaches",
    "title": "15  Forecasting",
    "section": "15.5 Beyond State-Space Models: Machine Learning Approaches",
    "text": "15.5 Beyond State-Space Models: Machine Learning Approaches\nWhile structural time series models and bsts are powerful tools for interpretable probabilistic forecasting, the field of time series analysis has evolved rapidly, especially in the context of large-scale commercial applications. However, a fundamental trade-off exists: State-space models offer transparency and uncertainty quantification, while modern machine learning approaches prioritize pure predictive power.\n\nModern Era Forecasting\nA recent post by the Amazon Science group Amazon (2021) describes the evolution of the time series algorithms used for forecasting from 2007 to 2021. Figure below shows the entire evolution of the algorithms.\n\n\n\n\n\nThey went from standard textbook time series forecasting methods to make predictions to the quantile-based transformer models. The main problem of the traditional TS models is that they assume stationarity. A stationary time series is one whose properties do not depend on the time at which the series is observed. For example, a white noise series is stationary - it does not matter when you observe it, it should look much the same at any point in time.\n\nyt = rnorm(100)\nplot(yt,type='l')\n\n\n\n\n\n\n\n\nIn other words, all the coefficients of a time series model do not change over time. We know how to deal with trends and seasonality quite well. Thus, those types of non-stationary are not an issue. Below some of the example of time series data. Although most of those are not stationary, we can model them using traditional techniques (Hyndman and Athanasopoulos (2021)).\n\n\n\n\n\n\n\n\nFigure 15.8: Which of these series are stationary? (a) Dow Jones index on 292 consecutive days; (b) Daily change in the Dow Jones index on 292 consecutive days; (c) Annual number of strikes in the US; (d) Monthly sales of new one-family houses sold in the US; (e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs slaughtered in Victoria, Australia; (g) Annual total of lynx trapped in the McKenzie River district of north-west Canada; (h) Monthly Australian beer production; (i) Monthly Australian electricity production.\n\n\n\n\n\nHowever, when you try to forecast for a time series with no prior history or non-recurrent “jumps”, like recessions, traditional models are unlikely to work well.\nAmazon used a sequence of “patches” to hack the model and to make it produce useful results. All of those required manual feature engineering and led to less transparent and fragile models. One solution is to use random forests.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#quantile-regression-forests.",
    "href": "15-forecasting.html#quantile-regression-forests.",
    "title": "15  Forecasting",
    "section": "15.6 Quantile Regression Forests.",
    "text": "15.6 Quantile Regression Forests.\nMost estimators during prediction return \\(E(Y|X)\\), which can be interpreted as the answer to the question, what is the expected value of your output given the input?\nQuantile methods, return \\(y\\) at \\(q\\) for which \\(F(Y=y|X)=q\\) where \\(q\\) is the percentile and \\(y\\) is the quantile. One quick use-case where this is useful is when there are a number of outliers which can influence the conditional mean. It is sometimes important to obtain estimates at different percentiles, (when grading on a curve is done for instance.)\nNote, Bayesian models return the entire distribution of \\(P(Y|X)\\).\nIt is fairly straightforward to extend a standard decision tree to provide predictions at percentiles. When a decision tree is fit, the trick is to store not only the sufficient statistics of the target at the leaf node such as the mean and variance but also all the target values in the leaf node. At prediction, these are used to compute empirical quantile estimates.\nThe same approach can be extended to Random Forests. To estimate \\(F(Y=y|x)=q\\) each target value in training \\(y\\)s is given a weight. Formally, the weight given to \\(y_j\\) while estimating the quantile is \\[\n\\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}(y_j \\in L(x))}{\\sum_{i=1}^N \\mathbb{1}(y_i \\in L(x))},\n\\] where \\(L(x)\\) denotes the leaf that \\(x\\) falls into.\nInformally, what it means that for a new unknown sample, we first find the leaf that it falls into at each tree. Then for each \\((X, y)\\) in the training data, a weight is given to \\(y\\) at each tree in the following manner.\n\nIf it is in the same leaf as the new sample, then the weight is the fraction of samples in the same leaf.\nIf not, then the weight is zero.\n\nThese weights for each y are summed up across all trees and averaged. Now since we have an array of target values and an array of weights corresponding to these target values, we can use this to measure empirical quantile estimates.\nMotivated by the success of gradient boosting models for predicting Walmart sales (kaggle (2020)), Januschowski et al. (2022) tries to explain why tree-based methods were so widely used for forecasting.\n\n\n\nJanuschowski et al. (2022)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#deep-learning-for-time-series-temporal-fusion-transformers",
    "href": "15-forecasting.html#deep-learning-for-time-series-temporal-fusion-transformers",
    "title": "15  Forecasting",
    "section": "15.7 Deep Learning for Time Series: Temporal Fusion Transformers",
    "text": "15.7 Deep Learning for Time Series: Temporal Fusion Transformers\nWhile Random Forests and Gradient Boosting Machines are powerful for tabular regression, deep learning architectures designed specifically for time series have gained significant traction. One state-of-the-art architecture is the Temporal Fusion Transformer (TFT) Lim et al. (2021).\nTFT combines the strengths of several model components: 1. Gating mechanisms (GRN) to skip over unused components, allowing the model to adapt its depth. 2. Variable selection networks to select relevant input variables at each time step. 3. Static covariate encoders to integrate context (like location or store ID) that doesn’t change over time. 4. Temporal self-attention (multi-head attention) to learn long-term dependencies across time steps. 5. Quantile output to generate probabilistic forecasts (intervals) rather than just point estimates.\nThis architecture addresses a key critique of black-box models by offering interpretability: the attention weights can be visualized to show which past time points were most influential in making the current prediction.\n\nBenchmark: BSTS vs. TFT on Initial Claims\nTo illustrate the trade-off between traditional probabilistic models and modern deep learning, we compare a bsts model against a TFT model on the iclaims dataset.\nBSTS Implementation: We fit a standard local linear trend plus seasonal model using bsts.\n# R: Fit BSTS\nss &lt;- AddLocalLinearTrend(list(), y_train)\nss &lt;- AddSeasonal(ss, y_train, nseasons = 52)\nmodel1 &lt;- bsts(y_train, state.specification = ss, niter = 1000)\npred &lt;- predict(model1, horizon = length(y_test))\nTFT Implementation and The Reality of Tuning: We use the pytorch-forecasting library to train a Temporal Fusion Transformer. While Deep Learning models are often praised for their “auto-regressive” capabilities, achieving a sensible forecast on this dataset required a significant, iterative tuning process that highlights the practical challenges of these methods compared to bsts.\n\nIterative Tuning: Initially, the model produced flat lines or massive biases. Correcting this required:\n\nFeature Engineering: Explicitly adding a “week of year” feature to help the model latch onto seasonality.\nNormalization Battles: We had to revert from trend-aware scalers to standard scaling (GroupNormalizer) to ensure the model didn’t learn artificial offsets.\nTranslation Invariance: The most critical step was removing the global time index (time_idx) from the inputs. Without this, the model memorized the global trend of the training set and extrapolated it erroneously into the future.\n\nEnsembling: To reduce the inherent variance of the neural network weights, we had to train an ensemble of 3 independent models and average their predictions.\n\nDespite this “tedious” process, the TFT demonstrated remarkable flexibility. Unlike bsts, where we explicitly defined “Local Linear Trend” and “Seasonal” components, the TFT eventually learned these patterns entirely from the data structure, without us specifying the mathematical form of the trend.\n# Python: Fit TFT (Ensemble of 3 models)\n# Note: Global time index is excluded to enforce translation invariance\ntft = TemporalFusionTransformer.from_dataset(\n    training_dataset,\n    learning_rate=0.015,\n    hidden_size=64,\n    attention_head_size=4,\n    time_varying_known_reals=[], # No global time index\n    loss=QuantileLoss([0.05, 0.5, 0.95])\n)\ntrainer.fit(tft, train_dataloaders=train_dataloader)\nComparison Code: The full code for this case study, including the R script for BSTS and the Python script for TFT, along with the data files, is available in the case_studies/tft/ directory of the book’s repository.\nResults Comparison: The figure below compares the out-of-sample forecasts.\n\n\n\n\n\n\nFigure 15.9: Benchmark: BSTS (Blue) vs. TFT (Red) Forecasts. Note the difference in Uncertainty Quantification.\n\n\n\nCritical Assessment: Comparing the two forecasts reveals a fundamental difference in “honesty”:\n\nBSTS (The Sensible Benchmark): The BSTS forecast (Blue) is highly sensible. The predictive mean tracks the actual data closely throughout the horizon. More importantly, the 95% Credible Interval naturally expands as the forecast moves further into the future. This is a desirable, “honest” feature of the structural model—it admits that uncertainty accumulates over time.\nTFT (The Overconfident Learner): The TFT forecast (Red), while capturing the rhythm of the seasonality, exhibits undesirable overconfidence. Its prediction intervals are surprisingly narrow and do not expand over time, implying it is equally confident about next week as it is about next year. Furthermore, for many periods, the actual observed data falls outside the TFT’s 90% interval, suggesting the model is underestimating the true noise in the system.\n\nThis comparison serves as a reminder: while Deep Learning offers immense flexibility and power for complex patterns, structural Bayesian models often provide more reliable, well-calibrated, and “sane” defaults for decision-making in standard business forecasting contexts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#conclusion",
    "href": "15-forecasting.html#conclusion",
    "title": "15  Forecasting",
    "section": "15.8 Conclusion",
    "text": "15.8 Conclusion\nForecasting is a vast field where no single method rules supreme. In this chapter, we have focused on Structural Time Series models via the bsts package because they offer a sweet spot for many scientific and business applications:\n\nInterpretability: Unlike “black box” machine learning models, BSTS models decompose a time series into understandable components—trends, seasonality, and regressor effects. This allows analysts to explain why a forecast is what it is.\nUncertainty Quantification: By using a full Bayesian framework, we naturally get credible intervals for our predictions. We don’t just produce a single number; we quantify what we don’t know.\nFlexibility: The ability to incorporate priors (like spike-and-slab for variable selection) makes them robust even when data is noisy or predictors are numerous.\n\nHowever, as seen in the “Alternative Approaches” section, when the goal is pure predictive accuracy on massive datasets with complex, non-linear interactions, modern machine learning techniques like Gradient Boosted Trees and Deep Learning (Transformers) can outperform traditional state-space models. The choice of tool depends on the task: use bsts when you need to understand the mechanism and quantify risk; consider deep learning when you need to squeeze out the last percentage of accuracy from billions of data points.\nUltimately, the best forecaster is often a pragmatist—able to wield the precision of the Kalman Filter and the raw power of a Random Forest, depending on the problem at hand.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#advanced-theory-under-the-hood",
    "href": "15-forecasting.html#advanced-theory-under-the-hood",
    "title": "15  Forecasting",
    "section": "15.9 Advanced Theory: Under the Hood",
    "text": "15.9 Advanced Theory: Under the Hood\n\n[!TIP] Optional Section: This section delves into the mathematical machinery driving the models. Readers focused primarily on application can skip ahead to the Conclusion or the “Beyond State-Space Models” section.\n\n\n[!NOTE] A Note on Notation: In this section, we transition to the standard engineering notation for state-space models to align with classic literature (like the Kalman Filter).\n\nState: \\(x_t\\) (previously \\(\\alpha_t\\))\nTransition Matrix: \\(F_t\\) or \\(A\\) (previously \\(T_t\\))\nObservation Matrix: \\(G_t\\) or \\(H\\) (previously \\(Z_t^T\\))\n\nThis shift allows us to present the algorithms in their native mathematical form.\n\nThe models we have discussed in the bsts framework are all special cases of the linear Gaussian state space model (also known as the Dynamic Linear Model). While bsts provides a convenient high-level interface to fit these models using MCMC, understanding the underlying machinery provides deeper insight into how these forecasts are generated and updated as new data arrives.\nThe core algorithm for exact inference in linear Gaussian systems is the Kalman Filter. Originally developed for control engineering, it is the mathematical engine that computes the posterior distribution of the latent state \\(\\alpha_t\\) (or \\(x_t\\) in general notation) given the data up to time \\(t\\). The recursive nature of the Kalman Filter—updating the state estimate with each new observation—is what allows structural time series models to adapt dynamically to changing trends and seasonal patterns.\nThe classic filtering and prediction algorithms for linear and Gaussian systems are described in Rudolph Emil Kalman (1960) and R. E. Kalman and Bucy (1961). Early work on discrete recursions for hidden Markov models are in Baum et al. (1970) who use an EM-type algorithm, Viterbi (1967) who provides a modal state filter estimate and recursions developed in Lindgren (1978). While these can be used to evaluate the marginal likelihood for the parameters they are computationally too intensive to solve the filtering and learning, Lindgren (1978). Steven L. Scott (2002) provides a review of FFBS algorithms for discrete HMMs.\nMarkov chain Monte Carlo (MCMC) algorithms for parameter learning in nonlinear non-Gaussian state space models were developed by Carlin, Polson, and Stoffer (1992). For linear and Gaussian systems, Carter and Kohn (1994) introduced the filter forward and backwards sample (FFBS) algorithm, which efficiently draws the entire block of hidden states. For handling multinomial logit models, Steven L. Scott (2002) and Frühwirth-Schnatter et al. (2008) developed mixture of normals approximation methods. Additionally, West and Harrison (1997) proposed conditionally conjugate priors that allow parameters to be marginalized out of the updating equations, leading to more efficient inference procedures.\n\nKalman Filtering\nThe Normal/ Normal Bayesian learning model provides the basis for shrinkage estimation of multiple means and the basis of the Kalman filter for dynamically tracking a path of an object.\nThe Kalman filter is arguably the most common application of Bayesian inference. The Kalman filter assumes a linear and Gaussian state-space model: \\[\ny_{t}=x_{t}+\\sigma\\varepsilon_{t}^{y}\\text{ and }x_{t}=x_{t-1}+\\sigma\n_{x}\\varepsilon_{t}^{x},\n\\] where \\(\\varepsilon_{t}^{y}\\) and \\(\\varepsilon_{t}^{x}\\) are i.i.d. standard normal and \\(\\sigma\\) and \\(\\sigma_{x}\\) are known. The observation equation posits that the observed data, \\(y_{t}\\), consists of the random-walk latent state, \\(x_{t}\\), that is polluted by noise, \\(\\sigma\\varepsilon_{t}^{y}\\). Further, \\(\\sigma_{x}/\\sigma\\) is the “signal-to-noise” ratio, measures the information content of the signal. As \\(\\sigma\\) increases relatively to \\(\\sigma_{x}\\), the observations become noisier and less informative. The model is initialized via a prior distribution over \\(x_{0}\\), which simply for analytical tractability must be normally distributed, \\(x_{0}\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\).\nThe posterior distribution solves the filtering problem and is defined recursively via Bayes rule: \\[\np\\left(  x_{t+1} \\mid y^{t+1}\\right)  =\\frac{p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  }{p\\left(  y_{t+1} \\mid y^{t}\\right)  }\\propto\np\\left(  y_{t+1} \\mid x_{t+1}\\right)  p\\left(  x_{t+1} \\mid y^{t}\\right)  \\text{.}%\n\\] and the likelihood function, \\(p\\left( y_{t+1} \\mid x_{t+1}\\right)\\). The predictive distribution summarizes all of the information about \\(x_{t+1}\\) based on lagged observations. The likelihood function summarizes the new information in \\(y_{t+1}\\) about \\(x_{t+1}\\).\nThe Kalman filter relies on an inductive argument: assume that \\(p\\left( x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\) and then verify that \\(p\\left( x_{t+1} \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\) with analytical expressions for the hyperparameters. To verify, note that since \\(p\\left(x_{t} \\mid y^{t}\\right) \\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}\\right)\\), \\(x_{t}=\\mu_{t}+\\sigma_{t}\\eta_{t}\\) for some standard normal \\(\\eta_{t}\\). Substituting into the state evolution, the predictive is \\(x_{t+1} \\mid y^{t}\\sim\\mathcal{N}\\left( \\mu_{t},\\sigma_{t}^{2}+\\sigma_{x}^{2}\\right)\\). Since \\(p\\left( y_{t+1} \\mid x_{t+1}\\right) \\sim\\mathcal{N}\\left(x_{t+1},\\sigma^{2}\\right)\\), the posterior is \\[\\begin{align*}\np\\left(  x_{t+1} \\mid y^{t+1}\\right)   &  \\propto p\\left(  y_{t+1} \\mid x_{t+1}\\right)\np\\left(  x_{t+1} \\mid y^{t}\\right)  \\propto\\exp\\left[  -\\frac{1}{2}\\left( \\frac{\\left(  y_{t+1}-x_{t+1}\\right)  ^{2}}{\\sigma^{2}}+\\frac{\\left( x_{t+1}-\\mu_{t}\\right)  ^{2}}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\right)  \\right]\n\\\\\n&  \\propto\\exp\\left(  -\\frac{1}{2}\\frac{\\left(  x_{t+1}-\\mu_{t+1}\\right)\n^{2}}{\\sigma_{t+1}^{2}}\\right)\n\\end{align*}\\] where \\(\\mu_{t+1}\\) and \\(\\sigma_{t+1}^{2}\\) are computed by completing the square: \\[\\begin{equation}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}=\\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}%\n}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{ and }\\frac{1}{\\sigma_{t+1}^{2}}%\n=\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}+\\sigma_{x}^{2}}\\text{.}\\nonumber\n\\end{equation}\\] Here, inference on \\(x_{t}\\) is merely running the Kalman filter, that is, sequential computing \\(\\mu_{t}\\) and \\(\\sigma_{t}^{2}\\), which are state sufficient statistics.\nThe Kalman filter provides an excellent example of the mechanics of Bayesian inference: given a prior and likelihood, compute the posterior distribution. The Kalman filter offers the most natural Bayesian approach for this setting. The same approach applied to learning fixed static parameters. In this case, \\(y_{t}=\\mu+\\sigma\\varepsilon_{t},\\) where \\(\\mu\\sim\\mathcal{N}\\left( \\mu_{0},\\sigma_{0}^{2}\\right)\\) is the initial distribution. Using the same arguments as above, it is easy to show that \\(p\\left(\\mu \\mid y^{t+1}\\right) \\sim\\mathcal{N}\\left( \\mu_{t+1},\\sigma_{t+1}^{2}\\right)\\), where \\[\\begin{align*}\n\\frac{\\mu_{t+1}}{\\sigma_{t+1}^{2}}  &  =\\left(  \\frac{y_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{t}}{\\sigma_{t}^{2}}\\right)  =\\frac{\\left(  t+1\\right) \\overline{y}_{t+1}}{\\sigma^{2}}+\\frac{\\mu_{0}}{\\sigma_{0}^{2}}\\text{,}\\\\\n\\frac{1}{\\sigma_{t+1}^{2}}  &  =\\frac{1}{\\sigma^{2}}+\\frac{1}{\\sigma_{t}^{2}}=\\frac{\\left(  t+1\\right)  }{\\sigma^{2}}+\\frac{1}{\\sigma_{0}^{2}}\\text{,}\n\\end{align*}\\] and \\(\\overline{y}_{t}=t^{-1}\\sum_{t=1}^{t}y_{t}\\).\nThis intuition extends directly to the state variable learning problem, offering a coherent framework for sequential learning. In this case, researchers often have different feelings about assuming a prior distribution over the state variable and a parameter. In the state filtering problem, it is difficult to separate the prior distribution and the likelihood. In fact, one could view the initial distribution over \\(x_{0}\\), the linear evolution for the state variable, and the Gaussian errors as the “prior” distribution.\nNow consider,linear multivariate Gaussian state space model: \\[\\begin{align*}\ny_{t} &  =F_{t}x_{t}+\\varepsilon_{t} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}\\right) \\\\\nx_{t} &  =G_{t}x_{t-1}+\\varepsilon_{t}^{x} \\; \\; \\text{where} \\; \\; \\varepsilon_{t}^x\\sim\\mathcal{N}\\left( 0,\\Sigma_{t}^x\\right)\n\\end{align*}\\] where we allow for heteroscedascity in the error variance-covariance matrices. We complete the model specification with a normal prior on the initial starting condition \\(x_{0}\\sim\\mathcal{N}\\left(  \\mu_{0},\\Sigma_{0}\\right)\\). It is important to recognize that \\(\\varepsilon_{t}\\) and \\(\\varepsilon_{t}^{x}\\) need only be conditionally normal. There are a number of distributions of interest \\[\\begin{align*}\n\\text{Filtering}  &  :p\\left(  x_{t}|y^{t}\\right)  \\text{ }t=1,...,T\\\\\n\\text{Forecasting}  &  :p\\left(  x_{t+1}|y^{t}\\right)  \\text{ } t=1,...,T\\\\\n\\text{Smoothing}  &  :p\\left(  x_{t}|y^{t+1}\\right)  \\text{ }t=1,...,T \\\\\n\\text{Prediction}  &  :p\\left(  y_{t+1}|y^{t}\\right)  \\text{ }t=1,...,T\n\\end{align*}\\] For known parameters with linearity and Gaussianity we have the following Kalman filter recursions for calculation these distributions.\nThe fundamental filtering relationship is based on the fact that the filtering distribution is of the form \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right)\n\\;\\;\\mathrm{and}\\;\\;p(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu\n_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] where \\((\\mu_{t+1|t+1},\\Sigma_{t+1|t+1})\\) are related to \\((\\mu_{t|t}%\n,\\Sigma_{t|t})\\) via the Kalman filter recursions. In the following, it is sometimes useful to write this as \\[\np(x_{t}|y^{t})\\sim\\mathcal{N}\\left(  \\mu_{t|t},\\Sigma_{t|t}\\right) \\; \\Rightarrow \\; x_{t}=\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\widehat{\\varepsilon}_{t}%\n\\] where \\(\\widehat{\\varepsilon}_{t} \\sim \\mathcal{N}(0,1)\\). Before we derive the filtering recursions and characterize the state filtering distribution we first find the forecasting distribution. The predictive or forecast distribution is defined as follows.\nPredictive Distribution, \\(p( x_{t+1} | y^{t} )\\).\nThe key distributions in Bayes rule are the predictive and the conditional state posterior given by \\[\np( x_{t+1}|y^{t} )\\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t}\\right)\n\\] To compute the predictive or forecasting distribution note that:\n\\[\np\\left(  x_{t+1}|y^{t}\\right)  =p\\left(  G_{t+1}x_{t}+\\varepsilon_{t+1}%\n^{x}|y^{t}\\right)  \\sim\\mathcal{N}\\left(  \\mu_{t+1|t},\\Sigma_{t+1|t} \\right)\n\\] where the predictive moments are\n\\[\\begin{align*}\n\\mu_{t+1|t}  &  =G_{t+1}\\mu_{t}\\\\\n\\Sigma_{t+1|t}  &  =G_{t+1}\\Sigma_{t}G_{t+1}^T+\\Sigma_{t+1}^{x}.\n\\end{align*}\\] We now state and derive the main Kalman filtering recursions for linear Gaussian models with known parameters.\nFiltering Distribution The classic Kalman filter characterisation of the state filtering distributionp(x_{t+1}|y^{t+1})$ and moment recursions are given by \\[\np(x_{t+1}|y^{t+1})\\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}\\right)\n\\] The updated posterior means and variances are defined by \\[\\begin{align*}\n\\mu_{t+1|t+1} &  =\\mu_{t+1|t}+K_{t+1}e_{t+1}\\\\\n\\Sigma_{t+1|t+1} &  =(I-K_{t+1}F_{t+1})\\Sigma_{t+1|t}%\n\\end{align*}\\] where the Kalman gain \\(K_{t+1}\\) matrix and innovations vector \\(e_{t+1}\\) are \\[\\begin{align*}\nK_{t+1}  &  = \\Sigma_{t+1|t} F_{t+1}^T\\left(  F_{t+1} \\Sigma_{t+1|t}\nF_{t+1}^T+ \\Sigma_{t+1} \\right)  ^{-1}\\\\\ne_{t+1}  &  = y_{t+1} - F_{t+1} \\mu_{t+1|t}%\n\\end{align*}\\]\nTo prove this result we use the predictive distribution and an application of Bayes rule which implies that\n\\[\\begin{align*}\np\\left(  x_{t+1}|y^{t+1}\\right)   & = p\\left(  x_{t+1}|y_{t+1}%\n,y^{t}\\right) \\\\\n&  = \\frac{ p\\left(  y_{t+1}|x_{t+1}\\right)  p\\left(  x_{t+1}|y^{t}\\right)}{  p\\left(  y_{t+1}|y^t\\right) }\n\\text{.}%\n\\end{align*}\\] Under the normality assumption, the likelihood term is \\[\np( y_{t+1} | x_{t+1} ) = ( 2 \\pi )^{-\\frac{p}{2}} | \\Sigma_{t+1} |^{-\\frac{1}{2}}\n  \\exp \\left ( - \\frac{1}{2} ( y_{t+1} - F_{t+1} x_{t+1} )^T\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) \\right )\n\\] Combining with the exponent term from the state predicitive distribution, then gives an exponent for the filtering distribution of the form \\[\n( y_{t+1} - F_{t+1} x_{t+1} )^T\\Sigma_{t+1}^{-1} ( y_{t+1} - F_{t+1}\nx_{t+1} ) +( x_{t+1} - \\mu_{t+1|t} )^T\\Sigma_{t+1|t}^{-1} ( x_{t+1} -\n\\mu_{t+1|t} )\n\\] Now we define the de-meaned state and innovations vectors,\n\\[\n\\tilde{x}_{t+1} = x_{t+1} - \\mu_{t+1|t} \\; \\text{and} \\; e_{t+1} = y_{t+1} - F_{t+1} \\mu_{t+1|t}\n\\] Using the usual completing the square trick we can re-write the exponent as \\[\n( e_{t+1} - F_{t+1} \\tilde{x}_{t+1} )^T\\Sigma_{t+1}^{-1} ( e_{t+1} -\nF_{t+1} \\tilde{x}_{t+1} ) + \\tilde{x}_{t+1}^T\\Sigma_{t+1|t}^{-1}\n\\tilde{x}_{t+1}\n\\] The sums of squares can be decomposed further as \\[\n\\tilde{x}_{t+1}^T\\left(  F_{t+1}^T\\Sigma_{t+1}^T+\n\\Sigma_{t+1|t}^{-1} \\right)  \\tilde{x}_{t+1} + 2 \\tilde{x}_{t+1}^T\\left(  F_{t+1}^T\\Sigma_{t+1} e_{t+1} \\right)  + e_{t+1}^T\\Sigma_{t+1}^{-1} e_{t+1}\n\\] The exponent is then a quadratic form implying that the vector \\(\\tilde{x}_{t+1}^T\\) is normal distributed with the appropriate mean and variance-covariance matrix. The definitions are given by \\[\n\\Sigma_{t+1|t+1} F_{t+1}^T\\Sigma_{t+1} e_{t+1} \\; \\; \\mathrm{and} \\; \\;\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^T\\Sigma_{t+1}^{-1} F_{t+1} +\n\\Sigma_{t+1|t}^{-1} \\right)  ^{-1}\n\\] respectively. Hence, we obtain the identity \\[\n\\Sigma_{t+1|t+1} = \\left(  F_{t+1}^T\\Sigma_{t+1}^TF_{t+1} + \\Sigma_{t+1|t}^{-1} \\right)  ^{-1} = ( I - K_{t+1} F_{t+1} ) \\Sigma_{t+1|t}\n\\] where \\(K_{t+1} = \\Sigma_{t+1|t} F_{t+1}^T\\left(  F_{t+1} \\Sigma_{t+1|t}^{-1} F_{t+1}^T+ \\Sigma_{t+1} \\right)^{-1}\\) is the Kalman gain matrix.\nThe mean of the \\(\\tilde{x}_{t+1}^T = x_{t+1} - \\mu_{t+1|t}\\) distribution is then \\(K_{t+1}e_{t+1}\\). Un de-meaning the vector, we have \\(x_{t+1} = \\tilde{x}_{t+1} + \\mu_{t+1|t} =K_{t+1}e_{t+1}\\) leads to the following distributional result \\[\np( x_{t+1}|y^{t+1}) \\sim\\mathcal{N}\\left(  \\mu_{t+1|t+1},\\Sigma_{t+1|t+1}%\n\\right)  ,\n\\] The moments for the next filtering distribution are given by the classic recursions \\[\\begin{align*}\n\\mu_{t+1|t+1}  &  =\\mu_{t+1|t}+K_{t+1}\\left(  y_{t+1}-F_{t+1}\\mu_{t+1|t}\n\\right) \\\\\n\\Sigma_{t+1|t+1}  &  =\\left(  I-K_{t+1}F_{t+1}\\right)  \\Sigma_{t+1|t}\\text{.}%\n\\end{align*}\\] There are two other distributions to compute: the data predictive \\(p( y_{t+1} | y^t )\\) and the state smoothing distribution \\(p( x_t | y^{t+1} )\\). These are derived as follows.\nThe data predictive \\(p(y_{t+1}|y^t)\\) is determined from the observation equation and the state predictive distribution as follows \\[\\begin{align*}\ny_{t+1}  & =F_{t+1}x_{t+1}+\\varepsilon_{t+1} \\; \\text{with} \\;   \\varepsilon_{t+1}\\sim\\mathcal{N}\\left( 0,\\Sigma_{t+1}\\right) \\\\\np( x_{t+1} | y^t ) & \\sim \\mathcal{N} \\left ( \\mu_{t+1|t} , \\Sigma_{t+1|t} \\right )  \n\\end{align*}\\] Then substituting we have a predictive distribution for the next observation of the form \\[\np( y_{t+1}|y^{t}) \\sim\\mathcal{N}\\left(  F_{t+1}\\mu_{t+1|t},F_{t+1}%\n\\Sigma_{t+1|t} F_{t+1}+\\Sigma_{t+1} \\right)  \\text{.}%\n\\] The state smoothing distribution \\(p(x_t | y^{t+1} )\\) is determined from the joint distribution, \\(p( x_{t} , x_{t+1} | y^{t} )\\) as follows. First, factorise this joint distribution as \\[\np( x_{t+1} , x_{t} | y^{t} ) = p( x_{t+1} | x_{t} ) p( x_{t} | y^{t} )\n\\] Then calculate the conditional posterior distribution, \\(p( x_{t+1} | x_{t} , y^{t+1} )\\) by Bayes rule as \\[\np(x_{t+1}|x_{t},y^{t+1}) = \\frac{p\\left(  y_{t+1}|x_{t+1}\\right)\np(x_{t+1}|x_{t})p(x_{t}|y^{t})}{p(x_{t+1}|y^{t})}%\n\\] Now, we can view the system as having two observations on \\(x_{t+1}\\), namely \\[\\begin{align*}\nx_{t+1} &  =F_{t+1}x_{t}+\\Sigma_{t+1}^{x}\\epsilon_{t+1}^{x}\\\\\nx_{t} &  =\\mu_{t|t}+\\Sigma_{t|t}^{\\frac{1}{2}}\\hat{\\epsilon}_{t}%\n\\end{align*}\\] where the errors \\(\\epsilon_{t+1}^{x},\\hat{\\epsilon}_{t}\\) are independent.\nThis leads to a joint posterior with an exponent that is proportional to \\[\n(x_{t+1}-G_{t+1}x_{t})^T\\left(  \\Sigma\n_{t+1}^{x}\\right)  ^{-1}(x_{t+1}-G_{t+1}x_{t})- (x_{t}-\\mu\n_{t|t})^T\\Sigma_{t|t}^{-1}(x_{t}-\\mu_{t|t})\n\\] The first term comes from the state evolution and the second from the current filtering posterior. Completing the square gives \\[\n(x_{t+1}-G_{t+1}x_{t})^T\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}%\n(x_{t+1}-G_{t+1}x_{t})+(x_{t}-\\mu_{t|t})^T\\Sigma_{t|t}^{-1}(x_{t}%\n-\\mu_{t|t})\n\\] \\[\n=(x_{t+1}-\\mu_{t+1|t})^T\\Sigma_{t|t}^{-1}(x_{t+1}-\\mu_{t+1|t}%\n)+(x_{t}-\\mu_{t|t+1})^T\\Sigma_{t|t+1}^{-1}(x_{t}-\\mu_{t|t+1})\n\\] which leads to the smoothed state moments \\[\\begin{align*}\n\\mu_{t|t+1} &  =\\Sigma_{t|t+1}\\left(  \\Sigma_{t|t}\\mu_{t|t}+F_{t+1}^T\\left(  \\Sigma_{t+1}^{x}\\right)  ^{-1}x_{t+1}\\right)  \\\\\n\\Sigma_{t|t+1} &  =F_{t+1}^T\\left(  \\Sigma_{t+1}^{x}\\right)^{-1}F_{t+1}+\\Sigma_{t|t}^{-1}%\n\\end{align*}\\] The Kalman filter recursions then follow by induction.\n\nExample 15.4 (Kalman Filter for Robot Localization) The Kalman filter is a powerful tool for estimating the state of a system, given noisy observations. It is used in a wide range of applications, from tracking the position of a robot to estimating the state of a financial market. The Kalman filter is particularly useful when the state of the system is not directly observable, and must be inferred from noisy measurements.\nOften KF is used for localization problem: given noisy measurements about the position of a robot and the motion model of the robot, the Kalman filter can estimate the true position of the robot. The Kalman filter is a recursive algorithm that estimates the state of a system at each time step, based on the state estimate from the previous time step and a new observation. We will use the language of state-space models in this example and will use the notation \\(x_t\\) to denote the state of the system at time \\(t\\) (parameter we are trying to estimate), and \\(y_t\\) to denote the observation at time \\(t\\) (observed data). The state-space model is given by \\[\n\\begin{aligned}\nx_{t+1} & = A x_t + w,\\quad w \\sim N(0,Q)\\\\\ny_t &=G x_t + \\nu, \\quad \\nu \\sim N(0,R)\\\\\nx_0 & \\sim N(\\hat{x}_0, \\Sigma_0),\n\\end{aligned}\n\\] where \\(A\\) is the state transition matrix, \\(G\\) is the observation matrix, \\(w\\) is the process noise, and \\(\\nu\\) is the observation noise. The process noise and observation noise are assumed to be independent and normally distributed with zero mean and covariance matrices \\(Q\\) and \\(R\\), respectively. The initial state \\(x_0\\) is assumed to be normally distributed with mean \\(\\hat{x}_0\\) and covariance matrix \\(\\Sigma_0\\). The Kalman filter provides a recursive algorithm for estimating the state of the system at each time step, based on the state estimate from the previous time step and a new observation. The state estimate is normal with mean \\(\\hat{x}_t\\) and the covariance matrix \\(\\Sigma_t\\). The Kalman filter equations are given by \\[\n\\begin{aligned}\n\\hat{x}_{t+1} &= A \\hat{x}_t + K_{t} (y_t - G \\hat{x}_t) \\\\\nK_{t} &= A \\Sigma_t G^T (G \\Sigma_t G^T + R)^{-1}\\\\\n\\Sigma_{t+1} &= A \\Sigma_t A^T - K_{t} G \\Sigma_t A^T + Q\n\\end{aligned}\n\\] Kalman filter performs a multivariate normal-normal update using \\(N(A \\hat{x}_t,A \\Sigma_t A^T)\\) as prior and \\(N(y_t, G \\Sigma_t G^T + R)\\) as likelihood. The posterior distribution is \\(N(\\hat{x}_{t+1}, \\Sigma_{t+1})\\). Matrix \\(K_{t}\\) is called the Kalman gain and provides a weight on the residual between observed and prior \\(y_t - G \\hat{x}_t\\) in the update.\nAssume our robot starts at \\(\\hat x_0 = (0.2,-0.2)\\) (x-y Cartesian coordinates) and initial covariance is \\[\n\\Sigma_0 = \\begin{bmatrix} 0.4 & 0.3 \\\\ 0.3 & 0.45 \\end{bmatrix}.\n\\] The prior distribution of the robot’s position can be visualized in R with a contour plot.\n\nlibrary(mnormt)\nxhat &lt;- c(0.2, -0.2)\nSigma &lt;- matrix(c(0.4, 0.3, \n                  0.3, 0.45), ncol=2)\nx1 &lt;- seq(-2, 4,length=151)\nx2 &lt;- seq(-4, 2,length=151)\nf &lt;- function(x1,x2, mean=xhat, varcov=Sigma) \n  dmnorm(cbind(x1,x2), mean,varcov)\nz &lt;- outer(x1,x2, f)\nmycols &lt;- topo.colors(100,0.5)\nimage(x1,x2,z, col=mycols, main=\"Prior density\",\n      xlab=expression('x'[1]), ylab=expression('x'[2]))\ncontour(x1,x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow I get readings from GPS \\(y_0 = (2.3, -1.9)\\) and I know from the manufacturer that the GPS has a covariance matrix of \\(R = 0.5\\Sigma_0\\). We assume the measurement matrix \\(G\\) to be identity matrix, thus \\[\ny_t = Gx_t + \\nu_t = x_t + \\nu, \\quad \\nu \\sim N(0, R).\n\\]\n\nR &lt;- 0.5 * Sigma\nz2 &lt;- outer(x1,x2, f, mean=c(2.3, -1.9), varcov=R)\nimage(x1, x2, z2, col=mycols, main=\"Sensor density\")\ncontour(x1, x2, z2, add=TRUE)\npoints(2.3, -1.9, pch=19)\ntext(2.2, -1.9, labels = \"y\", adj = 1)\ncontour(x1, x2,z, add=TRUE)\npoints(0.2, -0.2, pch=19)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1)\n\n\n\n\n\n\n\n\nNow we combine our initial guess about the location \\(x_0\\) with the measure noisy location data \\(y_0\\) to obtain posterior distribution of the location of the robot \\(p(x\\mid \\hat x_0, \\Sigma,R) = N(x\\mid \\hat x_f, \\Sigma_f)\\) \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma^{-1} + R^{-1})^{-1} (\\Sigma^{-1} \\hat{x} + R^{-1} y) \\\\\n\\Sigma_f & = (\\Sigma^{-1} + R^{-1})^{-1}\n\\end{aligned}\n\\] Using the matrix inversion identity \\[\n\\begin{aligned}\n(A^{-1} + B^{-1})^{-1} & = A - A (A + B)^{-1}A = A (A + B)^{-1} B\n\\end{aligned}\n\\] I can write the above as: \\[\n\\begin{aligned}\n\\hat{x}_f & = (\\Sigma - \\Sigma (\\Sigma + R)^{-1}\\Sigma)(\\Sigma^{-1} \\hat{x} + R^{-1}y)\\\\\n& =\\hat{x} - \\Sigma (\\Sigma + R)^{-1} \\hat{x} + \\Sigma R^{-1} y -\\Sigma (\\Sigma + R)^{-1}\\Sigma R^{-1}y\\\\\n& = \\hat{x} + \\Sigma (\\Sigma + R)^{-1} (y - \\hat{x})\\\\\n& = (1.667, -1.333)\\\\\n\\Sigma_f &= \\Sigma - \\Sigma (\\Sigma + R)^{-1} \\Sigma\\\\\n&=\\left[\\begin{array}{lll}\n0.133 & 0.10\\\\\n0.100 & 0.15\n\\end{array}\n\\right]\n\\end{aligned}\n\\] In the more general case when \\(G\\) is not the identity matrix I have \\[\n\\begin{aligned}\n\\hat{x}_f & = \\hat{x} + \\Sigma G^T (G \\Sigma G^T + R)^{-1} (y - G \\hat{x})\\\\\n\\Sigma_f &= \\Sigma - \\Sigma G^T (G \\Sigma G^T + R)^{-1} G \\Sigma\n\\end{aligned}\n\\]\n\nG &lt;- diag(2)\ny &lt;- c(2.4, -1.9)\nxhatf &lt;- xhat + Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% (y - G %*% xhat)\nSigmaf &lt;- Sigma - Sigma %*% t(G) %*% solve(G %*% Sigma %*% t(G) + R) %*% G %*% Sigma\nz3 &lt;- outer(x1, x2, f, mean=c(xhatf), varcov=Sigmaf)\nimage(x1, x2, z3, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Filtered density\")\ncontour(x1, x2, z3, add=TRUE)\npoints(xhatf[1], xhatf[2], pch=19)\ntext(xhatf[1]-0.1, xhatf[2],\n     labels = expression(hat(x)[f]), adj = 1)\nlb &lt;- adjustcolor(\"black\", alpha=0.5)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\nNow I assume my robot moves according to the following model \\[\nx_t = A x_{t-1} + w_t, \\quad w_t \\sim N(0, Q)\n\\] with \\[\n\\begin{split}\nA = \\left( \\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array} \\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Then the next location is normally distributed with the parameters \\[\n\\begin{split}\nA= \\left(\\begin{array}{cc}\n1.2 & 0.0 \\\\\n0.0 & -0.2\n\\end{array}\\right),\n\\qquad\nQ = 0.3 \\Sigma\n\\end{split}\n\\] Here \\(K = A \\Sigma G^T (G \\Sigma G^T + R)^{-1}\\) is so-called Kalman gain matrix.\n\nA &lt;- matrix(c(1.2, 0,\n              0, -0.2), ncol=2)\nQ &lt;- 0.3 * Sigma\nK &lt;- A %*% Sigma %*% t(G) %*% solve(G%*% Sigma %*% t(G) + R)\nxhatnew &lt;- A %*% xhat + K %*% (y - G %*% xhat)\nSigmanew &lt;- A %*% Sigma %*% t(A) - K %*% G %*% Sigma %*% t(A) + Q\nz4 &lt;- outer(x1,x2, f, mean=c(xhatnew), varcov=Sigmanew)\nimage(x1, x2, z4, col=mycols,\n      xlab=expression('x'[1]), ylab=expression('x'[2]),\n      main=\"Predictive density\")\ncontour(x1, x2, z4, add=TRUE)\npoints(xhatnew[1], xhatnew[2], pch=19)\ntext(xhatnew[1]-0.1, xhatnew[2],\n     labels = expression(hat(x)[new]), adj = 1)\ncontour(x1, x2, z3, add=TRUE, col=lb)\npoints(xhatf[1], xhatf[2], pch=19, col=lb)\ntext(xhatf[1]-0.1, xhatf[2], col=lb, \n     labels = expression(hat(x)[f]), adj = 1)\ncontour(x1, x2, z, add=TRUE, col=lb)\npoints(0.2, -0.2, pch=19, col=lb)\ntext(0.1, -0.2, labels = expression(hat(x)), adj = 1, col=lb)\ncontour(x1, x2, z2, add=TRUE, col=lb)\npoints(2.3, -1.9, pch=19, col=lb)\ntext(2.2, -1.9,labels = \"y\", adj = 1, col=lb)\n\n\n\n\n\n\n\n\n\nForward filtering and Backwards Sampling\nThe Kalman filtering recursions lead to a fully recursive algorithm for characterizing \\(p( x| y)\\) known as FFBS (Forward filtering and Backwards Sampling). This provides the counterpart to the Baum-Welch algorithm developed earlier for HMMs. The details are as follows. The first step is to factorize the joint posterior distribution of the states via \\[\\begin{align*}\np\\left(  x|y^{T}\\right)   &  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}%\n^{T-1}p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  \\\\\n&  =p\\left(  x_{T}|y^{T}\\right)  \\prod_{t=1}^{T-1}p\\left(  x_{t}|x_{t+1}%\n,y^{t}\\right) \\label{ffbs-1}\n\\end{align*}\\] where we have used the fact that \\(p\\left(  x_{t}|x_{t+1},...,x_{T},y^{T}\\right)  =p\\left(  x_{t}|x_{t+1},y^{T}\\right)\\) by the Markov property (conditional on \\(x_{t+1}\\), \\(x_{t}\\) is independent of all \\(x_{t+2}\\), etc.).\nThis forms the FFBS algorithm: forward-filtering, backward sampling algorithm for generating a block sample from \\(p\\left(  x|y^{T}\\right)\\). Filter forward using the Kalman recursions and obtain a sample from \\(p( x_T|y^T)\\) and then backwards sample using \\(p\\left(x_{t}|x_{t+1},y^{t}\\right)\\) to generate a block draw of \\(x\\). In what follows, we will often write \\[\np\\left(  x|y^{T}\\right)  \\sim FFBS\n\\] to denote that the FFBS algorithm can be used to generate a block draw.\nBackwards Sampling.\nThe distribution of the final state given the data history \\(p( x_{T} | y^{T} )\\) is given by the Kalman filter \\[\np( x_{T}|y^{T}) \\sim\\mathcal{N}\\left(  \\mu_{T},\\Sigma_{T}\\right)\n\\] where \\(( \\mu_{T} , \\Sigma_{T} )\\) are computed via the Kalman filter recursions. The second distribution comes from the factorization of \\(p( x_{t}, x_{t+1} | y^{t} )\\) in the derivation of the Kalman filtering recursions. Hence, the conditional state filtering distribution given \\(( x_{t+1} , y^t )\\) is \\[\np( x_{t} | x_{t+1} , y^{t} ) \\sim\\mathcal{N}\\left(  \\mu_{t|t+1} ,\n\\Sigma_{t|t+1} \\right)\n\\] where the one-step back smoothed moments are \\[\\begin{align*}\n\\mu_{t|t+1}  &  = \\Sigma_{t|t+1} \\left(  \\Sigma_{t|t} \\mu_{t|t} +\nF_{t+1}^T\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1} x_{t+1} \\right) \\\\\n\\Sigma_{t|t+1}  &  = F_{t+1}^T\\left(  \\Sigma^{x}_{t+1} \\right)  ^{-1}\nF_{t+1} + \\Sigma_{t|t}^{-1}%\n\\end{align*}\\] as computed above. Then we can sequentially sample from this distribution.\n\n\nHMM: Hidden Markov Models\nThe algorithms described in this section were originally developed by Baum et al. (1970) and Viterbi (1967). Baum developed original trading algorithms for Renaissance Technology which later became a multi-billion dollar hedge fund. The algorithms are also known as the Baum-Welch and Viterbi algorithms, respectively. They are used to estimate the parameters of a Hidden Markov Model (HMM) from a sequence of observations. The HMM is a statistical model that describes a system that is assumed to be a Markov process with unobserved (hidden) states. These algorithms are now widely used in many applications, including speech recognition, bioinformatics, and finance.\nViterbi, on the other hand, was one of the founders of what is now known as Qualcomm, a multi-billion dollar semiconductor and telecommunications equipment company. Viterbi’s algorithm is used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations.\nBaum-Welch (1970) and Viterbi (1967) are the two famous discrete HMM algorithms.\nConsider a model with a Hidden Chain or regime-switching variable\n\\[\ny_{t} = \\mu\\left( x_{t}\\right) + \\sigma\\left( x_{t}\\right) \\varepsilon _{t}\\text{.}\n\\]\nSuppose that \\(x_{t}\\) is a finite state Markov chain with a time-homogeneous transition matrix P with entries \\(\\left\\{ p_{ij}\\right\\}\\) which are given by\n\\[\np_{ij} = P\\left( x_{t} = i | x_{t-1} = j, \\theta\\right) \\text{.}\n\\]\nWe define the marginal filtering and smoothing distributions\n\\[\np_{i}^{t,t} = P\\left( x_{t} = i | \\theta, y^{t}\\right) \\text{ and } p_{i}^{t,T} = P\\left( x_{t} = i | \\theta, y^{T}\\right)\n\\]\nand the corresponding joint filtering and smoothing matrices:\n\\[\np_{ij}^{t,t} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\text{ and } p_{ij}^{t,T} = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\text{.}\n\\]\nThe key to the algorithm is that we are just going to track the joint matrices, and then peel-off marginals from the rows and columns.\n\nForward-filtering\nTo derive the forward equations\n\\[\n\\begin{aligned}\np_{ij}^{t,t} & = P\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right) \\propto p\\left( y_{t}, x_{t-1} = i, x_{t} = j | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p\\left( x_{t} = j | x_{t-1} = i, \\theta\\right) p\\left( x_{t-1} = i | \\theta, y^{t-1}\\right) \\\\\n& \\propto p\\left( y_{t} | x_{t} = j, \\theta\\right) p_{ij}\\left( p_{i}^{t-1,t-1}\\right) \\text{.}\n\\end{aligned}\n\\]\nThis shows how to compute today’s filtering distribution given the likelihood. The advantage of this is that it only requires matrix multiplication.\n\n\nBackward-sampling\nThe result of the forward-filtering is the final observation \\(p_{ij}^{T,T}\\). Like in the previous section, we can then filter in reverse to compute \\(p_{ij}^{t,T}\\), which is required for the MCMC algorithm. We have that\n\\[\n\\begin{aligned}\np_{ij}^{t,T} & = p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{t}\\right) p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto \\frac{p\\left( x_{t-1} = i, x_{t} = j | \\theta, y^{t}\\right)}{p\\left( x_{t} = j | \\theta, y^{t}\\right)} p\\left( x_{t} = j | \\theta, y^{T}\\right) \\\\\n& \\propto p_{ij}^{t,t} \\frac{p_{j}^{t,T}}{p_{j}^{t,t}} \\text{.}\n\\end{aligned}\n\\]\nIn deriving this, we have used the fact that\n\\[\np\\left( x_{t-1} = i | x_{t} = j, \\theta, y^{T}\\right) \\propto p\\left( x_{t-1} = i | x_{t} = j, \\theta , y^{t}\\right)\n\\]\nbecause conditional time \\(t\\) information, the past transition is independent of the future. This is the discrete-state version of the FFBS algorithm.\n\n\nSmoothing: Forwards and Backwards\nLet \\(y^T = \\{y_1, \\dots, y_T\\}\\) be a sequence of random variables where the conditional distribution\n\\[\np(y^T | x^T) = p (y_1 | x_1 ) \\prod_{t=2}^T p(y_t | x_t, y_{t-1})\n\\]\nwhere we suppress the dependence of the mixture components on a parameter \\(\\theta\\). The full smoothing distribution can be written\n\\[\np( x | y ) = p( x_T | y^T ) \\prod_{t=1}^{T-1} p( x_t | x_{t+1} , \\theta , y_{t+1} )\n\\]\nSuppose \\(\\{x_t\\}\\) follows a finite state Markov chain with initial distribution \\(\\pi_0\\) and transition probabilities\n\\[\nQ_t(r,s) = Pr(x_t = s | x_{t-1} = r) \\; \\; \\mathrm{and} \\; \\; P_t ( t, r ,s) = Pr(x_{t-1} = r, x_t = s | y_1^t)\n\\]\nwhich we will compute sequentially. Let the current filtering distribution of the state be given by, for \\(t &gt; 0\\),\n\\[\np_t(s) = Pr(x_t = s | y^t) \\; \\; \\mathrm{and} \\; \\; A_t(x_{t-1}, x_t, y_t) = p(x_{t-1}, x_t, y_t | y^{t-1})\n\\]\nBy definition,\n\\[\nA_t(r,s,y_t) = \\frac{p_{t-1}(r) Q_t(r,s)}{p(y_t | y_{t-1})}.\n\\tag{15.1}\\]\nwhere the marginal likelihood is given by\n\\[\np(y_t | y_{1}^{t-1}) = \\sum_{r,s} A_t(r,s,y_t)\n\\]\nThe filtered transition distribution\n\\[\np_{trs} = A_t(r,s,y_t) / p(y_t | y^{t-1})\n\\]\nThe forward-backward recursions are used to efficiently compute the observed data likelihood\n\\[\np(y) = \\sum_{ x } p(y | x)p( x )\n\\]\nwhere \\(x = ( x_1 , \\ldots , x_T )\\). We also need the posterior distribution \\(p( x | y )\\) of the latent Markov chain given observed data. The recursions consist of a forward step that computes the distribution of the \\(t\\)’th transition given all the data up to time \\(t\\), and a backward recursion that updates each distribution to condition on all observed data.\nThe forward recursion operates on the set of transition distributions, represented by a sequence of matrices \\(P_t = ( p_{trs} )\\). Computing\n\\[\np_t(s) = \\sum_{r}p_{trs}\n\\]\nsets up the next step in the recursion. The recursion is initialized by replacing \\(p\\) with \\(p^0\\) in equation Equation 15.1.\nThe observed data log-likelihood can be computed as\n\\[\n\\log p(y) = \\log p(y_1) + \\sum_{t=2}^T \\log p(y_t | y^{t-1})\n\\]\nWith appropriate use of logarithms, \\(p\\) and \\(p(y_t | y^{t-1})\\) need only ever be evaluated on the log scale.\nThe stochastic version of the backward recursion simulates from\n\\[\np(x | y).\n\\]\nBegin with the factorization\n\\[\n\\label{eq:bkwd} p(x | y) = p(x_T | y^T) \\prod_{t=1}^{T-1} p(x_{t} | x_{t+1}^T, y).\n\\]\nThen notice that, given \\(x_{t+1}\\), \\(x_t\\) is conditionally independent of \\(y_{t+1}^T\\) and all later \\(x\\)’s. Thus\n\\[\np(x_t | x_{t+1}, y) = P(x_t = r | x_{t+1} = s, y^{t+1}) \\propto p_{t+1rs}\n\\]\nTherefore, if one samples \\((x_{t-1}, x_t)\\) from the discrete bivariate distribution given by \\(P_t\\) and then repeatedly samples \\(x_t\\) from a multinomial distribution proportional to column \\(x_{t+1}\\) of \\(P_{t+1}\\) then \\(x=(x_1, \\ldots , x_T)\\) is a draw from \\(p(x|y)\\).\n\n\n\nMixture Kalman filter\nWe can also introduce a \\(\\lambda_t\\) state variable and consider a system\n\\[\n\\begin{aligned}\ny_t & = F_{\\lambda_t} x_t + D_{\\lambda_t} \\epsilon_t  \\\\\nx_t & = G_{\\lambda_t} x_{t-1} + B_{\\lambda_t} v_t\n\\end{aligned}\n\\]\nThe Kalman filter gives moments of the state filtering distribution\n\\[\nx_t | \\lambda^t , y^t \\sim \\mathcal{N} \\left ( \\mu_{t|t} , \\Sigma_{t|t} \\right )\n\\]\nHere we assume that the iid auxiliary state variable shocks \\(\\lambda_t \\sim p( \\lambda_t )\\).\nFirst we marginalize over the state variable \\(x_t\\). Then we can track the sufficient statistics for the hidden \\(z_t\\) variable dynamically in time, namely \\(\\left ( z_{t|t} , S_t \\right )\\), just the Kalman filter moments, in the conditionally Gaussian and discrete cases Lindgren (1978). Then we re-sample \\(( z_{t|t} , S_t  , \\theta )^{(i)}\\) particles.\n\n\nRegime Switching Models\nThe general form of a continuous-time regime switching model is\n\\[\ndy_{t}=\\mu\\left(  \\theta,x_{t},y_{t}\\right)  dt+\\sigma\\left(  \\theta\n,x_{t},y_{t}\\right)  d B_{t}%\n\\]\nwhere \\(x_{t}\\) takes values in a discrete space with transition matrix \\(P_{ij}\\left(  t\\right)\\) with parameters \\(\\theta=\\left(  \\theta_{1},\\ldots,\\theta_{J}\\right)\\). Common specifications assume the drift and diffusion coefficients are parametric functions and the parameters switch over time. In this case, it is common to write the model as\n\\[\ndy_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t}\\right)  dt+\\sigma\\left(  \\theta_{x_{t}%\n},y_{t}\\right)  d B_{t}\\text{.}%\n\\]\nScott (2002) provides a fast MCMC algorithm for state filtering by adapting the FFBS algorithm. Time discretized the model is:\n\\[\ny_{t}=\\mu\\left(  \\theta_{x_{t}},y_{t-1}\\right)  +\\sigma\\left(  \\theta_{x_{t}%\n},y_{t-1}\\right)  \\varepsilon_{t}\\text{.}%\n\\]\nNote that we use the standard notation from discrete-time models where the time index on the Markov state is equal to the current observation. The discrete-time transition probabilities are\n\\[\np_{ij}=P\\left(  x_{t}=i|x_{t-1}=j\\right)\n\\]\nand we assume, apriori, that the transition functions are time and state invariant. The joint likelihood is given by\n\\[\np\\left(  y| x,\\theta\\right)  = \\prod_{t=1}^{T} p\\left(  y_{t}|y_{t-1}%\n,x_{t-1},\\theta\\right)\\]\nwhere \\(p\\left(  y_{t}|y_{t-1},x_{t-1},\\theta\\right)  =N\\left(  \\mu\\left(\n\\theta_{x_{t-1}},y_{t-1}\\right)  ,\\sigma^{2}\\left(  \\theta_{x_{t-1}}%\n,y_{t-1}\\right)  \\right)\\).\nClifford-Hammersley implies that the complete conditionals are given by \\(p\\left(  \\theta| x ,s , y \\right)\\), \\(p\\left(  s | x , \\theta, y \\right)\\), and \\(p\\left(  x | s ,\\theta, y \\right)\\). Conditional on the states and the transition probabilities, updating the parameters is straightforward. Conditional on the states, the transition matrix has a Dirchlet distribution, and updating this is also straightforward. To update the states use FFBS.\nAn important component of regime switching models is the prior distribution. Regime switching models (and most mixture models) are not formally identified. For example, in all regime switching models, there is a labeling problem: there is no unique way to identify the states. A common approach to overcome this identification issue is to order the parameters.\n\nChangepoint Problems\nSmith (1975) introduced the single changepoint problem from a Bayesian perspective. Consider a sequence of random variables \\(y_{1} , \\ldots, y_{T}\\) which has a change-point at time \\(\\tau\\) in the sense that\n\\[\ny_t | \\theta_{k} \\sim    \\left\\{\n\\begin{array}[c]{l}\np( y | \\theta_{1} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; 1 \\leq i \\leq\\tau \\\\\np( y | \\theta_{2} ) \\; \\; \\; \\; \\; \\; \\mathrm{for} \\; \\; \\tau+1 \\leq\ni \\leq T\n\\end{array}  \\right\\}\\]\nThis can be rewritten as a state space model\n\\[\ny_t = \\theta_{ x_t } + \\sigma_{ x_t } \\epsilon_t\n\\]\nwhere \\(x_t\\) has a Markov transition evolution.\nAn idea that appears to be under-exploited is that of “model reparametrisation”. The multiple change-point problem, which is computationally expensive if approached directly, has a natural model reparametrisation that makes the implementation of MCMC methods straightforward (see Chib (1998)). Specifically, suppose that the data generating process \\(y^{T} = \\{ y_{1} , \\ldots, y_{T} \\}\\) is given by a sequence of conditionals \\(f ( y_{t} | y^{t-1} , \\theta_{k} )\\) for parameters \\(\\theta_{k}\\) that change at unknown change-points \\(\\{ \\tau_{1} , \\ldots,\n\\tau_{k} \\}\\).\nThe model parameterization is based on using a hidden Markov state space model with a vector of latent variables \\(s_{t}\\) where \\(s_{t}\n= k\\) indicates that \\(y_{t}\\) is drawn from \\(p ( y_{t} | y^{t-1} ,\n\\theta_{k} )\\). Let the prior distribution on the \\(s_{t}\\)’s have transition matrix where \\(p_{ij} = P \\left(  s_{t} = j | s_{t-1} = i\n\\right)\\) is the probability of jumping regimes. With this model parameterization the \\(k\\)th change occurs at \\(\\tau_{k}\\) if \\(s_{\n\\tau_{k} } = k\\) and \\(s_{ \\tau_{k} + 1 } = k + 1\\). The reparameterisation automatically enforces the order constraints on the change-points and is it very easy to perform MCMC analysis on the posterior distribution. This provides a more efficient strategy for posterior computation. MCMC analysis of the \\(s_{t}\\)’s is straightforward and the posterior for the \\(\\tau_{k}\\)’s can be obtained by inverting the definition above. Hence\n\\[\np( \\tau = t | y ) = p( x_t =1 | y )\n\\]\nThe alternative is single state updating conditional on \\(\\tau\\) which is slow for finding the multiple-changeponts.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "15-forecasting.html#particle-learning-for-general-mixture-models",
    "href": "15-forecasting.html#particle-learning-for-general-mixture-models",
    "title": "15  Forecasting",
    "section": "15.10 Particle Learning for General Mixture Models",
    "text": "15.10 Particle Learning for General Mixture Models\nParticle learning (PL) offers a powerful and flexible approach for sequential inference in general mixture models. Unlike traditional MCMC methods, which require repeated passes over the entire dataset and can be computationally demanding, particle learning operates in an online fashion. This means it can efficiently update inference as new data arrives, making it particularly well-suited for large or high-dimensional datasets and real-time applications.\nThe particle learning framework is designed to efficiently and sequentially learn from a broad class of mixture models. At its core, the approach models data as arising from a mixture distribution:\n\\[\nf(z) = \\int k(z;\\theta) d G(\\theta)\n\\]\nwhere \\(G\\) is a discrete mixing measure and \\(k(z;\\theta)\\) is a kernel parameterized by \\(\\theta\\). The generality of this formulation allows PL to be applied to a wide variety of models, including finite mixture models, Dirichlet process mixtures, Indian buffet processes, and probit stick-breaking models. This flexibility is a significant advantage, as it enables practitioners to use PL across many settings without needing to redesign the inference algorithm for each new model structure.\nIn addition to its generality, particle learning provides an alternative to MCMC for tasks such as online model fitting, marginal likelihood estimation, and posterior cluster allocation. Its sequential nature makes it particularly attractive for streaming data and scenarios where computational resources are limited.\nA general mixture model can be described by three components: a likelihood, a transition equation for latent allocations, and a prior over parameters. Specifically,\n\nThe likelihood is given by \\(p(y_{t+1} | k_{t+1}, \\theta)\\), representing the probability of the next observation given the current allocation and parameters.\nThe transition equation \\(p(k_{t+1} | k^t, \\theta)\\) governs how the latent allocation variables evolve over time, where \\(k^t = \\{k_1,\\ldots, k_t\\}\\) denotes the history of allocations.\nThe parameter prior \\(p(\\theta)\\) encodes prior beliefs about the mixture component parameters.\n\nThis structure can be expressed in a state-space form:\n\\[\\begin{align}\ny_{t+1} &= f(k_{t+1}, \\theta) \\\\\nk_{t+1} &= g(k^t, \\theta)\n\\end{align}\\]\nwhere the first equation is the observation model and the second describes the evolution of the latent allocation states.\nThe mixture modeling framework described above is closely related to hidden Markov models (HMMs). In this context, the observed data \\(y_t\\) are assumed to be generated from a mixture, with allocation variables \\(k_t\\) determining which mixture component is responsible for each observation. The parameters \\(\\theta_{k_t}\\) for each component are drawn from the mixing measure \\(G\\). This structure allows for both standard mixture models, where each observation is assigned to a single component, and more general latent feature models, where multivariate allocation variables \\(k_t\\) allow an observation to be associated with multiple components simultaneously.\nA central concept in particle learning is the essential state vector \\(\\mathcal{Z}_t\\), which is tracked over time. This vector is constructed to be sufficient for sequential inference, meaning that it contains all the information needed to compute the posterior predictive distribution for new data, update the state as new observations arrive, and learn about the underlying parameters:\n\nPosterior predictive: \\(p(y_{t+1} | \\mathcal{Z}_t)\\)\nPosterior updating: \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t, y_{t+1})\\)\nParameter learning: \\(p(\\theta | \\mathcal{Z}_{t+1})\\)\n\n\nThe Particle Learning Algorithm\nParticle learning approximates the posterior distribution \\(p(\\mathcal{Z}_t | y^t)\\) with a set of equally weighted particles \\(\\{\\mathcal{Z}_t^{(i)}\\}_{i=1}^N\\). When a new observation \\(y_{t+1}\\) becomes available, the algorithm proceeds in two main steps:\n\nResample: The current set of particles is resampled with weights proportional to the predictive likelihood \\(p(y_{t+1} | \\mathcal{Z}_t^{(i)})\\). This step focuses computational effort on the most plausible states given the new data.\nPropagate: Each resampled particle is then propagated forward by sampling from the transition distribution \\(p(\\mathcal{Z}_{t+1} | \\mathcal{Z}_t^{(i)}, y_{t+1})\\), thus updating the state to incorporate the new observation.\n\nThis two-step process is grounded in Bayes’ theorem, where the resampling step corresponds to updating the posterior with the new data, and the propagation step advances the state according to the model dynamics. After these steps, the set of particles provides an updated approximation to the posterior \\(p(\\mathcal{Z}_{t+1} | y^{t+1})\\).\nOne important distinction between particle learning and standard particle filtering methods is that the essential state vector \\(\\mathcal{Z}_t\\) does not necessarily need to include the full history of allocation variables \\(k^t\\) to be sufficient for inference. This makes PL both more efficient and more flexible than many existing particle filtering approaches for mixture models. Furthermore, the order of resampling and propagation steps is reversed compared to standard filters, which helps mitigate particle degeneracy and improves performance in mixture modeling contexts.\nParticle learning also provides an efficient mechanism for sampling from the full posterior distribution of the allocation vector \\(p(k^t | y^t)\\). This is achieved using a backwards uncertainty update, which allows for the recovery of smoothed samples of the allocation history. For each particle, and for each time step in reverse order, the allocation variable \\(k_r\\) is sampled with probability proportional to the product of the likelihood and the prior for that allocation, given the state vector. This results in an algorithm with computational complexity linear in the number of particles, making it practical even for large datasets.\nThe particle learning framework is applicable to a wide range of density estimation problems involving mixtures of the form\n\\[\nf(y;G) = \\int k(y ; \\theta) dG(\\theta)\n\\]\nThere are many possible choices for the prior on the mixing measure \\(G\\). Common examples include finite mixture models, which use a finite number of components; Dirichlet process mixtures, which allow for an infinite number of components via a stick-breaking construction; beta two-parameter processes; and kernel stick-breaking processes. Each of these priors offers different modeling flexibility and computational properties, and the choice depends on the specific application and desired level of model complexity.\nIn some cases, it is useful to consider a collapsed state-space model, where the predictive distribution for a new observation is expressed as an expectation over the mixing measure \\(G\\) given the current state vector:\n\\[\n\\mathbb{E}[f(y_{t+1};G) | \\mathcal{Z}_t] = \\int k(y_{t+1};\\theta) d \\mathbb{E}[G(\\theta) | \\mathcal{Z}_t]\n\\]\nIf \\(t\\) observations have been allocated to \\(m_t\\) mixture components, the posterior expectation of \\(G\\) can be written as a weighted sum of the base measure and point masses at the component parameters. The predictive density then combines contributions from both new and existing components, weighted according to their posterior probabilities.\nParticle learning offers a versatile and efficient framework for sequential inference in general mixture models. By representing the posterior with a set of particles and updating these particles as new data arrives, PL enables real-time model fitting, efficient posterior allocation, and flexible density estimation across a wide range of mixture modeling scenarios. Its ability to handle both finite and infinite mixture models, as well as latent feature models, makes it a valuable tool for modern statistical analysis.\n\nExample 15.5 (Particle Learning for Poisson Mixture Models) We will implement Particle Learning (PL) for a finite mixture of Poisson distributions based on the example from Carvalho et al. (2010). This example follows Algorithm 1 for finite mixture models from Section 2.1 of the paper.\nWe generate data from a mixture of two Poisson distributions (\\(\\lambda_1=2\\) with weight 0.7, \\(\\lambda_2\\)=10 with weight 0.3).\n\nset.seed(8) # Ovi\n# Simulate data (500 observations)\nn_obs &lt;- 500\ntrue_z &lt;- sample(1:2, n_obs, replace=TRUE, prob=c(0.7, 0.3))\ny &lt;- ifelse(true_z == 1, rpois(n_obs, 2), rpois(n_obs, 10))\n\n\n\nCode\n# Plot two empirical density plots for each mixture component. Put them in one plot \nplot(density(y[true_z == 1]), xlab = \"y\", col = \"blue\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.4), main=\"\")\nlines(density(y[true_z == 2]), col = \"red\", lwd = 2)\nlines(density(y),xlab = \"y\", col = \"purple\", lwd = 2, xlim = c(0, 15), ylim = c(0, 0.3))\nlegend(\"topright\", legend=c(\"Component 1 (Lambda=2)\", \"Component 2 (Lambda=10)\", \"Mixture Density\"), col=c(\"blue\", \"red\", \"purple\"), lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\nThe code below implements the Particle Learning algorithm using the following steps: 1 Particle Initialization: - Each particle tracks sufficient statistics: - s: Sum of observations per component - n: Count of observations per component 2. PL Algorithm: - Resample: Particles are weighted by the posterior predictive probability of the next observation - Propagate: For each particle: - Compute component allocation probabilities - Sample component assignment - Update sufficient statistics - Learn: Store posterior estimates of \\(\\lambda\\) parameters and mixture weights\nThe key features of this implementation are the use of posterior predictive with Poisson-Gamma conjugacy and allocation of probabilities by combining prior weights and likelihood.\n\n\nCode\n# Model parameters\nm &lt;- 2  # Number of components\nalpha &lt;- c(1, 1)  # Gamma prior shape parameters\nbeta &lt;- c(1, 1)   # Gamma prior rate parameters\ngamma &lt;- c(1, 1)  # Dirichlet prior parameters\nn_particles &lt;- 1000  # Number of particles\n\n# Initialize particles\nparticles &lt;- lapply(1:n_particles, function(i) {\n  list(s = c(0, 0),    # Sufficient statistics (sum of y in each component)\n       n = c(0, 0))    # Counts per component\n})\n\n# Store posterior samples\nposterior_lambda &lt;- matrix(0, nrow = n_obs, ncol = m)\nposterior_weights &lt;- matrix(0, nrow = n_obs, ncol = m)\n\n# Particle Learning algorithm\nfor (t in 1:n_obs) {\n  y_t &lt;- y[t]\n  log_weights &lt;- numeric(n_particles)\n  \n  # 1. Compute weights using posterior predictive\n  for (i in 1:n_particles) {\n    total_obs &lt;- sum(particles[[i]]$n)\n    weight_components &lt;- (particles[[i]]$n + gamma) / (total_obs + sum(gamma))\n    \n    # Predictive for each component (Poisson-Gamma)\n    pred_prob &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      exp(dpois(y_t, shape_post/rate_post, log = TRUE) +\n            dgamma(shape_post/rate_post, shape_post, rate_post, log = TRUE))\n    })\n    \n    log_weights[i] &lt;- log(sum(weight_components * pred_prob))\n  }\n  \n  # Normalize weights\n  max_logw &lt;- max(log_weights)\n  weights &lt;- exp(log_weights - max_logw)\n  weights &lt;- weights / sum(weights)\n  \n  # 2. Resample particles\n  idx &lt;- sample(1:n_particles, size = n_particles, replace = TRUE, prob = weights)\n  particles &lt;- particles[idx]\n  \n  # 3. Propagate state\n  for (i in 1:n_particles) {\n    # Compute allocation probabilities\n    alloc_probs &lt;- sapply(1:m, function(j) {\n      shape_post &lt;- alpha[j] + particles[[i]]$s[j]\n      rate_post &lt;- beta[j] + particles[[i]]$n[j]\n      log_prior &lt;- log(particles[[i]]$n[j] + gamma[j]) - log(sum(particles[[i]]$n) + sum(gamma))\n      log_lik &lt;- dpois(y_t, shape_post/rate_post, log = TRUE)\n      exp(log_prior + log_lik)\n    })\n    alloc_probs &lt;- alloc_probs / sum(alloc_probs)\n    \n    # Sample component allocation\n    k &lt;- sample(1:m, size = 1, prob = alloc_probs)\n    \n    # Update sufficient statistics\n    particles[[i]]$s[k] &lt;- particles[[i]]$s[k] + y_t\n    particles[[i]]$n[k] &lt;- particles[[i]]$n[k] + 1\n  }\n  \n  # 4. Learn parameters (store posterior means)\n  lambda_samples &lt;- t(sapply(particles, function(p) {\n    (alpha + p$s) / (beta + p$n)\n  }))\n  weight_samples &lt;- t(sapply(particles, function(p) {\n    (gamma + p$n) / sum(gamma + p$n)\n  }))\n  \n  posterior_lambda[t,] &lt;- colMeans(lambda_samples)\n  posterior_weights[t,] &lt;- colMeans(weight_samples)\n}\n\n\nNow we are ready to plot the results\n\n\nCode\nlibrary(ggplot2)\n# Convert posterior estimates to data frames for ggplot\nposterior_df &lt;- data.frame(\n  Observation = 1:n_obs,\n  Lambda1 = posterior_lambda[, 1],\n  Lambda2 = posterior_lambda[, 2],\n  Weight1 = posterior_weights[, 1],\n  Weight2 = posterior_weights[, 2]\n)\n# Plot Lambda parameters\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Lambda1, color = \"Lambda1\"), size = 1) +\n  geom_line(aes(y = Lambda2, color = \"Lambda2\"), size = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 10, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Lambda Parameters\",\n       x = \"Observation\", y = \"Lambda\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n# Plot Weights\nggplot(posterior_df, aes(x = Observation)) +\n  geom_line(aes(y = Weight1, color = \"Weight1\"), size = 1) +\n  geom_line(aes(y = Weight2, color = \"Weight2\"), size = 1) +\n  geom_hline(yintercept = 0.7, linetype = \"dashed\", color = \"blue\") +\n  geom_hline(yintercept = 0.3, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Posterior Estimates: Mixture Weights\",\n       x = \"Observation\", y = \"Weight\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nPosterior Estimates: Lambda Parameters\n\n\n\n\n\n\n\nPosterior Estimates: Mixture Weights\n\n\n\n\n\nThe first plot shows the posterior estimates of \\(\\lambda\\) parameters converging to true values (2 and 10). The second plot shows mixture weights converging to true weights (0.7 and 0.3). Convergence typically occurs after 100-200 observations. Particle degeneracy is mitigated through systematic resampling\nAdvantages of PL for Mixtures:\n\nSequential Updating: Processes observations one-at-a-time\nEfficiency: Only tracks sufficient statistics, not full history\nFlexibility: Easily extends to other mixture types (DP, IBP, etc.)\nReal-time Inference: Posterior updates after each observation\n\nThis implementation demonstrates PL’s ability to handle finite mixtures, but the same framework extends to infinite mixtures (DP mixtures) and other general mixture models described in the paper by modifying the propagation and resampling steps.\n\n\n\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of Hitting Streaks in Baseball: Comment.” Journal of the American Statistical Association 88 (424): 1184–88.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting Algorithm.”\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970. “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains.” The Annals of Mathematical Statistics 41 (1): 164–71.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile Regression: A Bayesian Approach Based on the Asymmetric Laplace Distribution.” Journal of Applied Econometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which Market Indicators Best Forecast Recessions?” FEDS Notes, August.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009. Dynamic Linear Models with R. New York, NY: Springer.\n\n\nCarlin, Bradley P, Nicholas G Polson, and David S Stoffer. 1992. “A Monte Carlo Approach to Nonnormal and Nonlinear State-Space Modeling.” Journal of the American Statistical Association 87 (418): 493–500.\n\n\nCarter, Chris K, and Robert Kohn. 1994. “On Gibbs Sampling for State Space Models.” Biometrika 81 (3): 541–53.\n\n\nCarvalho, Carlos M, Hedibert F Lopes, Nicholas G Polson, and Matt A Taddy. 2010. “Particle Learning for General Mixtures.” Bayesian Analysis 5 (4): 709–40.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple Change-Point Models.” Journal of Econometrics 86 (2): 221–41.\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007. “Auxiliary Mixture Sampling with Applications to Logistic Models.” Computational Statistics & Data Analysis 51 (April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC for Binary and Multinomial Logit Models.” In Statistical Modelling and Regression Structures: Festschrift in Honour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard Rue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical Models of Non-Gaussian Data.” Statistics and Computing 19 (4): 479.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012. “Simulation-Based Regularized Logistic Regression.” arXiv. https://arxiv.org/abs/1005.3430.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary Variable Models for Binary and Multinomial Regression.” Bayesian Analysis 1 (1): 145–68.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. edition. Melbourne, Australia: Otexts.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf Hasson, and Jan Gasthaus. 2022. “Forecasting with Trees.” International Journal of Forecasting, Special Issue: M5 competition, 38 (4): 1473–81.\n\n\nkaggle. 2020. “M5 Forecasting - Accuracy.”\n\n\nKalman, R. E., and R. S. Bucy. 1961. “New Results in Linear Filtering and Prediction Theory.” Journal of Basic Engineering 83 (1): 95–108.\n\n\nKalman, Rudolph Emil. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Transactions of the ASME–Journal of Basic Engineering 82 (Series D): 35–45.\n\n\nLim, Bryan, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. “Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting.” International Journal of Forecasting 37 (4): 1748–64.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for Mixed Distributions and Switching Regressions.” Scandinavian Journal of Statistics 5 (2): 81–91.\n\n\nPetris, Giovanni. 2010. “An R Package for Dynamic Linear Models.” Journal of Statistical Software 36 (October): 1–16.\n\n\nPolson, Nicholas G., and Steven L. Scott. 2011. “Data Augmentation for Support Vector Machines.” Bayesian Analysis 6 (1): 1–23.\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data Analysis. 3rd ed. New York: Chapman and Hall/CRC.\n\n\nScott, Steven L. 2002. “Bayesian Methods for Hidden Markov Models.” Journal of the American Statistical Association 97 (457): 337–51.\n\n\nScott, Steven L. 2022. “BoomSpikeSlab: MCMC for Spike and Slab Regression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian Variable Selection for Nowcasting Economic Time Series.” In Economic Analysis of the Digital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the Present with Bayesian Structural Time Series.” Int. J. Of Mathematical Modelling and Numerical Optimisation 5 (January): 4–23.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to Inference about a Change-Point in a Sequence of Random Variables.” Biometrika 62 (2): 407–16.\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.” IEEE Transactions on Information Theory 13 (2): 260–69.\n\n\nWest, Mike, and Jeff Harrison. 1997. Bayesian Forecasting and Dynamic Models. Springer.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Forecasting</span>"
    ]
  },
  {
    "objectID": "16-select.html",
    "href": "16-select.html",
    "title": "16  Model Selection",
    "section": "",
    "text": "16.1 Fundamental Considerations in Model Selection\nIn the world of statistical modeling and machine learning, we face a fundamental challenge that echoes Sherlock Holmes’ famous words about eliminating the impossible to find the truth. When confronted with data, we must navigate through countless possible models, each representing a different hypothesis about the underlying relationships in our data. The art and science of model selection lies in systematically choosing the model that best captures the true signal while avoiding the trap of fitting noise.\nIn the next chapter (Chapter 17), we focus on regularization techniques, which can also be viewed as model selection mechanisms. Specifically, we discuss Ridge regression, LASSO, and their Bayesian interpretations. These methods offer elegant solutions to the overfitting problem by introducing penalties for model complexity, automatically balancing fit and parsimony. We saw how these frequentist approaches connect to Bayesian thinking through the lens of prior distributions and posterior inference.\nThis chapter explores the critical decisions involved in choosing the right model. We begin by examining the fundamental considerations that guide model selection: the bias-variance tradeoff, the challenges of overfitting and underfitting, and the practical constraints of computational resources and data quality. We’ll explore how the purpose of our analysis—whether prediction or interpretation—should influence our modeling choices, drawing on Leo Breiman’s influential distinction between the “two cultures” of statistical modeling.\nThe chapter then delves into practical methodologies for model evaluation and selection. We’ll cover exploratory data analysis techniques that help us understand our data before committing to a particular model form, followed by rigorous approaches to measuring out-of-sample performance through cross-validation and information criteria. These tools provide the foundation for making principled decisions about model complexity.\nBy the end of this chapter, you’ll have a comprehensive toolkit for approaching model selection problems, understanding when different techniques are appropriate, and implementing these methods in practice. Most importantly, you’ll develop the judgment to balance the competing demands of accuracy, interpretability, and computational efficiency that characterize real-world modeling challenges.\nWhen building predictive models, several important considerations guide our choices. Understanding these factors helps us make informed decisions about model complexity and selection.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#fundamental-considerations-in-model-selection",
    "href": "16-select.html#fundamental-considerations-in-model-selection",
    "title": "16  Model Selection",
    "section": "",
    "text": "Model Complexity and Generalization\nChoosing the right model for the relationship between \\(x\\) and \\(y\\) involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is excessively complex (e.g., high-degree polynomials or deep neural networks trained on insufficient data) risks overfitting by memorizing training examples rather than learning the underlying pattern. This results in excellent training performance but poor generalization to unseen examples. This challenge is exacerbated when dealing with non-linear relationships, high-dimensional data, or noisy signals, where the optimal complexity is not immediately obvious. Systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning is often required to strike the right balance between signal capture and noise rejection.\n\n\nOverfitting and Underfitting\nOverfitting arises when a statistical learning algorithm captures stochastic noise rather than the underlying signal. This phenomenon typically manifests when a model possesses excessive degrees of freedom relative to the training data size, allowing it to “memorize” specific examples. While such a model may achieve near-zero error on the training set, it fails to generalize to new data because it has learned idiosyncrasies specific to the training sample rather than the general population structure. Common indicators of overfitting include a diverging gap between training and validation error curves or performance degradation on held-out data during training.\nUnderfitting, conversely, occurs when a model lacks the sufficient complexity to capture the true underlying structure between inputs and outputs. This often results from specifying a model class that is too rigid (e.g., fitting a linear model to quadratic data) or from over-regularization. An underfit model exhibits high bias, performing poorly on both training and test datasets. Unlike overfitting, underfitting is characterized by consistently high error rates across all data partitions, indicating a fundamental inability to model the signal.\n\n\nData Quality and Quantity\nThe reliability of predictive models is intrinsically linked to the quality and richness of the available data. Noisy, incomplete, or biased data inevitably leads to suboptimal model performance. While sufficient data volume is crucial for learning complex relationships, data quality often plays a more decisive role. Issues such as missing values, inconsistent formatting, label noise, and sampling bias are pervasive in real-world applications.\nTo address these challenges, the industry has seen the rise of data-centric AI platforms. Services like Scale AI and Toloka offer human-in-the-loop solutions for high-quality data annotation and validation. These platforms leverage globally distributed workforces to perform tasks ranging from image segmentation to text classification, ensuring that the ground truth labels used for training are accurate. By implementing rigorous quality control mechanisms—such as consensus voting among multiple annotators and dynamic skill-based routing—these services mitigate the risks associated with poor data quality.\n\n\nModel Explainability\nIn many high-stakes domains—such as healthcare, finance, and criminal justice—predictive accuracy alone is insufficient. Stakeholders require interpretability: a clear understanding of how a model arrives at its decisions. This need drives the trade-off between using complex “black box” models (like deep neural networks) and simpler, transparent models (like logistic regression or decision trees).\nRegulatory frameworks, including the EU’s GDPR, increasingly mandate a “right to explanation,” compelling organizations to deploy systems that are not just accurate but also accountable. While we explore specific techniques for achieving this—such as LIME, SHAP, and attention mechanisms—in detail later in this chapter, it is vital to recognize at the outset that the choice of model often dictates the ceiling of explainability.\n\n\n\nComputational Cost\nTraining and serving predictive models can be computationally expensive, particularly for deep learning architectures operating on massive datasets. In resource-constrained environments, this necessitates a trade-off between model performance and computational efficiency.\nDevelopment of specialized hardware has played a pivotal role in addressing this. Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) allow for massive parallelization of matrix operations, reducing training times from weeks to hours. However, inference cost remains a challenge for deployment.\nEdge computing—processing data locally on devices rather than in the cloud—has emerged as a solution for low-latency applications like autonomous driving and IoT. To enable this, techniques such as quantization (reducing numerical precision from 32-bit floats to 8-bit integers) and model pruning (removing redundant connections) are frequently employed. These methods allow complex models to run efficiently on mobile and embedded hardware with minimal loss of accuracy.\n\n\nEthical Considerations\nPredictive models are not value-neutral; their deployment can have profound societal consequences. Ethical failures often manifest as algorithmic bias, where models perpetuate or amplify existing discrimination. For instance, facial recognition systems trained on imbalanced datasets have demonstrated significantly higher error rates for darker-skinned individuals. Similarly, hiring algorithms trained on historical data may learn to replicate past discriminatory hiring practices.\nFairness in machine learning is an active area of research, dealing with metrics like statistical parity and equalized odds. However, maximizing fairness often requires trade-offs with predictive accuracy, necessitating careful ethical judgment during model development.\nPrivacy is another key concern. Deep learning models can inadvertently memorize sensitive training data, making them vulnerable to inversion attacks. Differential privacy offers a rigorous mathematical framework to mitigate this risk by adding calibrated noise to computations, ensuring that the model’s output does not reveal whether any specific individual’s data was included in the training set.\nFinally, accountability is essential. “Algorithmic impact assessments” and “audits” are becoming standard practice to evaluate potential harms before deployment, ensuring that systems serve the public good while minimizing risk.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#prediction-vs-interpretation",
    "href": "16-select.html#prediction-vs-interpretation",
    "title": "16  Model Selection",
    "section": "16.2 Prediction vs Interpretation",
    "text": "16.2 Prediction vs Interpretation\nPredictive models can serve two distinct purposes: prediction and interpretation. While these goals are not mutually exclusive, they often conflict with each other. The goal of interpretation is to understand the relationship between the input and output variables, which typically requires simpler, more transparent models.\nA model that excels at prediction might not be suitable for interpretation. For example, a complex deep neural network might achieve high predictive accuracy but provide little insight into how the input variables influence the output. Conversely, a simple linear model might be highly interpretable but lack the flexibility to capture complex relationships in the data. A key advantage of linear models is their ability to serve both purposes effectively, unlike more complex models with many parameters that can be difficult to interpret.\nInterpretation problems typically require simpler models. We prioritize models that are easy to interpret and explain, even if they have slightly lower predictive accuracy. The evaluation metrics also differ: for interpretation, we typically use the coefficient of determination (R-squared) or p-values, which provide insights into the model’s fit and the statistical significance of the estimated relationships.\nThe choice between using a model for prediction or interpretation depends on the specific task and desired outcome. If the primary goal is accurate predictions, a complex model with high predictive accuracy might be preferred, even if it is less interpretable. However, if understanding the underlying relationships and causal mechanisms is crucial, a simpler and more interpretable model might be chosen, even if it has slightly lower predictive accuracy. Interpretive models are commonly used in scientific research, social sciences, and other fields where understanding the underlying causes and relationships is crucial.\nIn practice, it’s often beneficial to consider both prediction and interpretation when building and evaluating models. However, it is not unusual to build two different models, one for prediction and one for interpretation. This allows for a more nuanced analysis of the data and can lead to better insights than using a single model for both purposes.\n\nBreiman’s Two Cultures\nLet \\(x\\) be a high-dimensional input containing a large set of potentially relevant data, and let \\(y\\) represent an output (or response) to a task that we aim to solve based on the information in \\(x\\). Breiman [2000] summarizes the difference between statistical and machine learning philosophy as follows:\n\n“There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.”\n\n\n“The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.”\n\n\n“Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”\n\nThis distinction highlights the fundamental difference between the traditional statistical approach, which assumes a specific data-generating process, and the machine learning approach, which focuses on finding patterns in data without making strong assumptions about the underlying mechanism.\nStatistical prediction problems are of great practical and theoretical interest. Deep learning predictors offer several advantages over traditional predictors:\n\nInput data can include all data of possible relevance to the prediction problem at hand\nNonlinearities and complex interactions among input data are accounted for seamlessly\nOverfitting is more easily avoided than with traditional high-dimensional procedures\nFast, scalable computational frameworks (such as TensorFlow) are available\n\nTree-based models and deep learning models exemplify the “algorithmic culture” that Breiman describes. These models can capture complex, non-linear relationships in data without requiring explicit specification of the functional form. However, this flexibility comes at the cost of interpretability. While decision trees offer some interpretability through their hierarchical structure, deep neural networks are often considered “black boxes” due to their complex, multi-layered architecture.\nThe trade-off between interpretability and accuracy is a central theme in modern machine learning. Simple models like linear regression are highly interpretable but may lack the flexibility to capture complex patterns. Complex models like deep neural networks can achieve high accuracy but are difficult to interpret. This has led to the development of various techniques for making complex models more interpretable, including feature importance measures, attention mechanisms, and surrogate models that approximate the behavior of complex models with simpler, more interpretable ones.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#diagnostics-for-model-assumptions",
    "href": "16-select.html#diagnostics-for-model-assumptions",
    "title": "16  Model Selection",
    "section": "16.3 Diagnostics for Model Assumptions",
    "text": "16.3 Diagnostics for Model Assumptions\nWhat makes a good model? If the goal is prediction, then the model is as good as its predictions. The easiest way to visualize the quality of predictions is to plot \\(y\\) versus \\(\\hat{y}\\). Most of the time we use empirical assessment of model quality. However, sometimes theoretical bounds can be derived for a model that describe its accuracy. For example, in the case of the linear regression model, the prediction interval is defined by \\[\n\\hat{y} \\pm s\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}}\n\\] where \\(s\\) is the standard deviation of the residuals. The prediction interval is the confidence interval for the prediction. The prediction interval is wider than the confidence interval because it includes the uncertainty in the prediction.\nAssume we have a predictive model \\[\ny = f(x) + \\epsilon\n\\] and we have some modeling assumption regarding the distribution of \\(\\epsilon\\). For example, when we use linear regression or BART, we assume that \\(\\epsilon\\) follows a normal distribution. One simple approach to test if observed samples \\(\\epsilon_1,\\ldots,\\epsilon_n\\) follow a specific distribution is to use Exploratory Data Analysis (EDA).\nThe most common tools for exploratory data analysis are Q-Q plots, scatter plots, and bar plots/histograms.\nA Q-Q plot compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). A quantile is the fraction (or percent) of points below the given value. That is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\). On a Q-Q plot, if the two datasets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two datasets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\). If \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\).\n\nExample 16.1 (Normal Q-Q plot) Figure 16.1 shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\nbabyboom = read.csv(\"../data/babyboom.csv\")\nqqnorm(babyboom$wt, bg=\"lightblue\"); qqline(babyboom$wt, col=\"red\", lwd=3)\n\n\n\n\n\n\n\nFigure 16.1: Normal Q-Q plot of baby weights\n\n\n\n\n\nVisual inspection of the Q-Q plot strongly suggests that birth weights are not normally distributed. We can see that on the left side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.\nThe Q-Q plots look different if we split the data based on the gender\npar(mar=c(4,4,0.7,0.5), bty='n', cex.lab=1, cex.axis=0.5,cex.main=0.5,pch=21,cex=1.3)\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\") %&gt;% pull(wt) \nqqnorm(g, bg=\"lightblue\"); qqline(g, col=2)\nqqnorm(b, bg=\"lightblue\"); qqline(b, col=2)\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender\n\n\n\nHow about the times in hours between births of babies?\n\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))\n\n\n\n\nHistogram of births by hour\n\n\n\n\nWhat about the Q-Q plot?\n\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval, bg=\"lightblue\")\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb, col=2) # Overlay a line\n\n\n\n\n\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#out-of-sample-performance",
    "href": "16-select.html#out-of-sample-performance",
    "title": "16  Model Selection",
    "section": "16.4 Out-of-Sample Performance",
    "text": "16.4 Out-of-Sample Performance\nA parametric model that we choose to fit to data is selected from a family of functions. We then use optimization to find the best model from that family by either minimizing empirical loss or maximizing the likelihood. Finding an appropriate family of functions is a major problem called the model selection problem. For example, the choice of input variables to be included in the model is part of the model selection process. Model selection involves determining which predictors, interactions, or transformations should be included in the model to achieve the best balance between complexity and predictive accuracy. In practice, we often encounter several models for the same dataset that perform nearly identically, making the selection process challenging.\nCrucially, the optimal model is rarely the one that fits the training data perfectly, as this often indicates overfitting. Overfitting can occur when a model is overly complex, capturing noise rather than the underlying pattern. A good model strikes a balance between fitting the data well and maintaining simplicity to ensure generalizability to new, unseen data. For instance, including too many parameters can lead to a perfect fit when the number of observations equals the number of parameters, but such a model is unlikely to perform well on out-of-sample data.\nThe goal of model selection is not only to achieve a good fit but also to reduce complexity by excluding unnecessary parameters. This process typically involves selecting a model from a relevant class of functions while keeping in mind the trade-offs between bias, variance, and model complexity. Techniques such as cross-validation, information criteria (e.g., AIC, BIC), and regularization methods are commonly used to guide the model selection process.\nThe model selection task is sometimes one of the most time-consuming parts of data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem is as yet another optimization problem, with the goal of finding the best family of functions that describe the data. With a small number of predictors, we can use brute force (check all possible models). For example, with \\(p\\) predictors there are \\(2^p\\) possible models with no interactions. Thus, the number of potential function families is huge even for modest values of \\(p\\). One cannot consider all transformations and interactions.\nOur goal is to build a model that predicts well for out-of-sample data, i.e., data that was not used for training. Ultimately, we are interested in using our models for prediction, and thus out-of-sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when choosing a predictive model, as one of the winners of the Netflix prize put it: “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it.” Out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in nature. To uncover the pattern, we start with a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\] and to test the validity of our model, we use an out-of-sample testing dataset \\[\nD^* = (y_j^*, x_j^*)_{j=1}^m,\n\\] where \\(x_i\\) is a set of \\(p\\) predictors and \\(y_i\\) is the response variable.\nA good predictor will “generalize” well and provide low MSE out-of-sample. There are a number of methods/objective functions that we will use to find \\(\\hat{f}\\). In a parameter-based approach, we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map \\(f\\) that approximates the process that generated the data. For example, data could represent some physical observations, and our goal is to recover the “laws of nature” that led to those observations. One of the pitfalls is to find a map \\(f\\) that does not generalize. Generalization means that our model actually learned the “laws of nature” and not just identified patterns present in training. The lack of generalization of the model is called overfitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of \\(n\\) points can be approximated by a polynomial of degree \\(n\\), e.g., we can always draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to observations that were not in our training data. In other words, the empirical risk measure for \\(D^*\\) is likely to be very high. Let us illustrate that in-sample fit can be deceiving.\n\nExample 16.2 (Hard Function) Say we want to approximate the following function \\[\nf(x) = \\dfrac{1}{1+25x^2}.\n\\] This function is simply a ratio of two polynomial functions and we will try to build a linear model to reconstruct this function\n\nx = seq(-2,2,by=0.01)\ny = 1/(1+25*x^2)\n# Approximate with polynomial of degree 1 and 2\nm1 = lm(y~x)\nm2 = lm(y~poly(x,2))\n# Approximate with polynomial of degree 20 and 5\nm20 = lm(y~poly(x,20))\nm5 = lm(y~poly(x,5))\nx = seq(-3,3,by=0.01)\ny = 1/(1+25*x^2)\nplot(x,y,type='l',col='black',lwd=2)\nlines(x,predict(m1,list(x=x)),lwd=2, col=1)\nlines(x,predict(m2,poly(x,2)),lwd=2, col=2)\nlines(x,predict(m5,poly(x,5)),lwd=2, col=3)\nlines(x,predict(m20,poly(x,20)),lwd=2, col=4)\nlegend(\"topright\", legend=c(\"f(x)\",\"m1\",\"m2\",\"m5\",\"m20\"), col=c(\"black\",1:4), lty=1, cex=0.8, bty='n')\n\n\n\n\n\n\n\nFigure 16.2: Runge-Kutta function\n\n\n\n\n\nFigure 16.2 shows the function itself (black line) on the interval \\([-3,3]\\). We used observations of \\(x\\) from the interval \\([-2,2]\\) to train the data (solid line) and from \\([-3,-2) \\cup (2,3]\\) (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) is not a good model. However, as we increase the degree of the polynomial to 20, the resulting model \\(\\hat{y} = \\beta_0 + \\beta_1x + \\beta_2 x^2 +\\ldots+\\beta_{20}x^{20}\\) does fit the training dataset quite well, but does a very poor job on the test dataset. Thus, while in-sample performance is good, the out-of-sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice, in-sample loss or classification rates provide us with a metric for comparing different predictors. It is worth mentioning here that there should be a penalty for overly complex rules that fit extremely well in-sample but perform poorly on out-of-sample data. As Einstein famously said, “A model should be simple, but not simpler.”\n\nTo a Bayesian, the solutions to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.\nThese results have major implications for empirical work and practical applications, as they provide a guide for forecasting.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#bias-variance-trade-off",
    "href": "16-select.html#bias-variance-trade-off",
    "title": "16  Model Selection",
    "section": "16.5 Bias-Variance Trade-off",
    "text": "16.5 Bias-Variance Trade-off\nFor any predictive model, we seek to achieve the best possible results, i.e., the smallest MSE or misclassification rate. For a historical perspective on how aggregating independent estimates reduces variance while preserving low bias, see the discussion of Galton’s ox-weighing experiment in Section 10.1.3.2. However, model performance can vary depending on the specific training/validation split used. A model that performed well on one test set may not produce good results given additional data. Sometimes we observe situations where a small change in the data leads to a large change in the final estimated model, e.g., the parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias can reduce variance in the final results. Similarly, low bias can result in high variance, but can also produce an oversimplification of the final model. The bias/variance concept is depicted below.\n\n\n\n\n\n\nFigure 16.3: Bias-variance trade-off\n\n\n\n\nExample 16.3 (Bias-variance) We demonstrate bias-variance concept using Boston housing example. We fit a model \\(\\mathrm{medv} = f(\\mathrm{lstat})\\). We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree \\(1,\\ldots,12\\) ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial \\(f\\) we averaged MSE from each of the ten models.\nFigure 16.4 (a) shows bias and variance for our twelve different models. As expected, bias increases while variance decreases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!\n\n\n\n\n\n\n\n\n\n\n\n(a) Metrics for twelve polynomial functions fitted into Boston housing data set. As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off.\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal complexity model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots)\n\n\n\n\n\n\n\nFigure 16.4: Metrics for 12 models\n\n\n\nLet’s take another, more formal look at the bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error \\(\\E{(y-\\hat{y})^2}\\) as a function of bias \\(\\E{y-\\hat{y}}\\) and variance \\(\\Var{\\hat{y}}\\).\nHere \\(\\hat{y} = \\hat{f}_{\\beta}(x)\\) is the prediction from the model, and \\(y = f(x) + \\epsilon\\) is the true value, which is measured with noise \\(\\Var{\\epsilon} = \\sigma^2\\), where \\(f(x)\\) is the true unknown function. The expectation above measures the squared error of our model on a random sample \\(x\\). \\[\n\\begin{aligned}\n\\E{(y - \\hat{y})^2}\n& = \\E{y^2 + \\hat{y}^2 - 2 y\\hat{y}} \\\\\n& = \\E{y^2} + \\E{\\hat{y}^2} - \\E{2y\\hat{y}} \\\\\n& = \\Var{y} + \\E{y}^2 + \\Var{\\hat{y}} + \\E{\\hat{y}}^2 - 2f\\E{\\hat{y}} \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f^2 - 2f\\E{\\hat{y}} + \\E{\\hat{y}}^2) \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f - \\E{\\hat{y}})^2 \\\\\n& = \\sigma^2 + \\Var{\\hat{y}} + \\mathrm{Bias}(\\hat{y})^2\\end{aligned}\n\\] Here we used the following identity: \\(\\Var{X} = \\E{X^2} - \\E{X}^2\\) and the fact that \\(f\\) is deterministic and \\(\\E{\\epsilon} = 0\\), thus \\(\\E{y} = \\E{f(x)+\\epsilon} = f + \\E{\\epsilon} = f\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#cross-validation",
    "href": "16-select.html#cross-validation",
    "title": "16  Model Selection",
    "section": "16.6 Cross-Validation",
    "text": "16.6 Cross-Validation\nIf the dataset at hand is small and we cannot dedicate a large enough sample size for testing, simply measuring error on a test dataset can lead to wrong conclusions. When the size of the testing set \\(D^*\\) is small, the estimated out-of-sample performance has high variance, depending on precisely which observations are included in the test set. On the other hand, when the training set \\(D^*\\) is a large fraction of the entire sample available, the estimated out-of-sample performance will be underestimated. Why?\nA simple solution is to perform the training/testing split randomly several times and then use the average out-of-sample errors. This procedure has two parameters: the fraction of samples to be selected for testing \\(p\\) and the number of estimates to be performed \\(K\\). The resulting algorithm is as follows:\nfsz = as.integer(p*n)\nerror = rep(0,K)\nfor (k in 1:K)\n{\n    test_ind = sample(1:n,size = fsz)\n    training = d[-test_ind,]\n    testing = d[test_ind,]\n    m = lm(y~x, data=training)\n    yhat = predict(m,newdata = testing)\n    error[k] = mean((yhat-testing$y)^2)\n}\nres = mean(error)\nFigure 16.5 shows the process of splitting the dataset randomly five times.\nCross-validation modifies the random splitting approach to use a more “disciplined” way to split the dataset for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations, and thus each data point is used once for testing. Like the random approach, CV helps address the high variance issue of out-of-sample performance estimation when the available dataset is small. Figure 16.6 shows the process of splitting the dataset five times using the cross-validation approach.\n\n\n\n\n\n\n\n\n\nFigure 16.5: Bootstrap\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.6: Cross-validation\n\n\n\n\n\n\n\nTraining set (red) and testing set (green)\n\n\n\n\nExample 16.4 (Simulated) We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used \\(x=-2,-1.99,-1.98,\\ldots,2\\) and \\(y = 2+3x + \\epsilon, ~ \\epsilon \\sim N(0,\\sqrt{3})\\). We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. Figure 16.7 compares empirical distribution of errors estimated from 35 samples.\n\n\n\n\n\n\nFigure 16.7: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.\n\n\n\nAs we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#sec-model-eval",
    "href": "16-select.html#sec-model-eval",
    "title": "16  Model Selection",
    "section": "16.7 Model evaluation: calibration, discrimination, and scoring",
    "text": "16.7 Model evaluation: calibration, discrimination, and scoring\nVariable selection is often presented as choosing a subset of predictors. A complementary view is that we are searching for a low-dimensional summary of the inputs that retains predictive information: an informal analogue of the idea of a minimal sufficient statistic from classical inference (Chapter 3).\nWhen we evaluate models, it is useful to separate three questions. First, does the model rank or separate cases well (discrimination)? Second, do the model’s predicted probabilities match observed frequencies (calibration)? Third, when we must make decisions, how should we score probabilistic forecasts so that \"better\" means \"closer to the truth\" in a principled way?\nFor binary outcomes, discrimination is often summarized by the receiver operating characteristic (ROC) curve. For a thresholded score, define the true positive rate (TPR) and false positive rate (FPR) as the threshold varies. The ROC curve plots TPR against FPR across all thresholds, and the area under the curve (AUC) summarizes performance as a single number between 0 and 1, with 0.5 corresponding to random ranking; see Fawcett (2006).\n\nCalibration and calibration plots\nCalibration concerns the probabilistic interpretation of predicted probabilities. A well-calibrated model that outputs \\(\\hat p=0.8\\) on many cases should be correct about 80% of the time on those cases. A common diagnostic is a calibration plot (reliability diagram), which bins predictions and compares average predicted probability to the empirical frequency in each bin.\n\n\nProper scoring rules (and the decision-theory link)\nA proper scoring rule assigns a numerical score to a probabilistic forecast and is designed so that, in expectation, the best strategy is to report the true predictive distribution; see Gneiting and Raftery (2007). Two widely used examples are the log score (negative log predictive density) and the Brier score for binary outcomes Brier (1950). These connect directly to Chapter 4: choosing a scoring rule is equivalent to choosing a loss for probabilistic prediction, and the expected score becomes a decision-theoretic risk.\nFor Bayesian models, posterior predictive checks compare observed data to replicated data drawn from the posterior predictive distribution. The workflow is: sample parameters from the posterior, simulate replicated datasets, and compare summary statistics or discrepancies between observed and replicated data; see Gelman et al. (2013).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#bayesian-model-selection",
    "href": "16-select.html#bayesian-model-selection",
    "title": "16  Model Selection",
    "section": "16.8 Bayesian Model Selection",
    "text": "16.8 Bayesian Model Selection\nThe probabilistic models of interest are the joint probability distribution \\(p(D,\\theta)\\) (called a generative model) and \\(P(Y,\\theta \\mid X)\\) (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative models require modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying the topic of an article can be solved using a discriminative distribution. The problem of generating a new article requires a generative model.\nLet \\(D\\) denote data. Let \\(\\theta_M \\in \\Theta_M\\) denote a set of parameters under model \\(M \\in \\mathcal{M}\\). Let \\(\\theta_M = (\\theta_1, \\ldots, \\theta_M)\\) be the \\(p\\)-vector of parameters. The Bayesian approach is straightforward: implement the Bayesian paradigm by executing Bayes’ rule. This requires the laws of probability and not optimization techniques. The notion of model complexity is no different. Let \\(\\mathcal{M}\\) denote the space of models and \\(\\theta\\) be the parameter vector. The Bayesian paradigm simply places probabilities over parameters and models given the data, namely \\(p(\\theta_M, M \\mid y)\\), where \\(y = (y_1, \\ldots, y_n)\\).\nThis has a number of decompositions. Bayes’ theorem calculates the joint posterior over parameters and models given data \\(D\\), namely \\[\n    p(\\theta_M,M\\mid D) = p(\\theta_M \\mid M,D)P(M\\mid D).\n\\] Notice how this factors the posterior into two terms: the conditional posterior over parameters given the model and the posterior over models given data.\nThe key quantity is the weight of evidence (a.k.a. marginal distribution of the data \\(D\\) given the model \\(M\\)), defined by \\[\np(D \\mid M) = \\int_{\\Theta_M} p(D \\mid \\theta_M, M) p(\\theta_M \\mid M) d\\theta_M.\n\\] Here \\(p(D \\mid \\theta_M, M)\\) is the traditional likelihood function. The key conditional distribution, however, is the specification of the prior over parameters \\(p(\\theta_M \\mid M)\\). As this is used in the marginalization, it can affect the Bayes risk dramatically. Occam’s razor comes from the fact that this marginalization provides a weight of evidence that favors simpler models over more complex ones.\nThis leads to a posterior over models, which is calculated as: \\[\\begin{align*}\n    P(M\\mid D)  & = \\dfrac{p(D\\mid M)P(M)}{p(D)}, \\\\\n    p(D\\mid M ) & = \\int_{ \\Theta_M} p(D\\mid \\theta_M , M ) p( \\theta_M | M ) d \\theta_M.\n\\end{align*}\\] Notice that this requires a joint prior specification \\(p(\\theta_M, M) = p(\\theta_M | M)p(M)\\) over parameters and models. The quantity \\(p(M| D)\\) is the marginal posterior for model complexity given the data. There is an equivalent posterior \\(p(\\theta_M | D)\\) for the parameters. \\(p(D \\mid M)\\) is the evidence of the data \\(D\\) given the complexity (a.k.a. conditional likelihood). The full evidence is \\[\np( D ) = \\int p( D| M ) p(M) d M.\n\\] This has been used to select the amount of hyperparameter regularization; see, for example, MacKay (1992).\nWe will see that the prior \\(p(\\theta_M | M)\\) will lead to an Occam’s razor effect, namely that the marginal distribution will favor simpler models. Importantly, this Occam’s razor effect is not in conflict with the Bayesian double descent phenomenon, which emerges from the marginal posterior of models given data and the conditional prior specification \\(p(\\theta_M | M)\\).\n\nExample 16.5 (Dice Example) Let’s consider a simple example of throwing a dice. Say, there are three dice, one that has numbers 1 through 6 (regular dice), one with three sides with ones and three sides with 2s (dice 1-2), one with three sides with ones and three sides with 2s and three sides with 3s (dice 1-2-3).\n\n\n\n\n\n\nDice 1-2\n\n\n\n\n\n\n\nDice 1-2-3\n\n\n\n\n\n\n\nRegular Dice\n\n\n\n\n\nYou observe outcome of one dice throw and it is 3. Which dice is it?\nTwo out of three explanations are plausible (dice 1-2-3 and regular dice). Intuitively, the 1-2-3 dice is more likely to produce 3 than the regular dice. Thus, if we need to choose, we would choose the 1-2-3 dice. For the sake of completeness, we can use the Bayes rule to calculate the evidence for each model.\nUsing Bayes’ rule: \\[P(M_i | D) = \\frac{P(D | M_i) P(M_i)}{P(D)}\\]\nwhere \\(M_i\\) represents each dice model and \\(D\\) is the observed data (outcome = 3).\nWe set equal prior probabilities for each dice: \\[P(M_1) = P(M_2) = P(M_3) = \\frac{1}{3}.\\] Now, we calculate the likelihood for each model.\n\n\n\nDice Type\nProbability of Rolling a 3\n\n\n\n\nRegular dice (\\(M_1\\))\n\\(P(3 | M_1) = 1/6\\)\n\n\nDice 1-2 (\\(M_2\\))\n\\(P(3 | M_2) = 0\\)\n\n\nDice 1-2-3 (\\(M_3\\))\n\\(P(3 | M_3) = \\frac{1}{3}\\)\n\n\n\nThen, the marginal likelihood is: \\[\nP(D) = \\sum_{i=1}^{3} P(D | M_i) P(M_i) = \\frac{1}{6} \\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{1}{6}.\n\\]\nFinally, posterior probabilities are\n\n\n\n\n\n\n\nDice Type\nPosterior Probability\n\n\n\n\nRegular dice\n\\(P(M_1 | D) = \\frac{\\frac{1}{6} \\cdot \\frac{1}{3}}{\\frac{1}{6}}=1/3\\)\n\n\nDice 1-2\n\\(P(M_2 | D) = \\frac{0 \\cdot \\frac{1}{3}}{\\frac{1}{6}}=0\\)\n\n\nDice 1-2-3\n\\(P(M_3 | D) = \\frac{\\frac{1}{3} \\cdot \\frac{1}{3}}{\\frac{1}{6}}=2/3\\)\n\n\n\nGiven the observation of outcome 3, the dice 1-2-3 is twice as likely as the regular dice. The dice 1-2 is completely ruled out since it cannot produce a 3. This demonstrates how Bayesian model selection naturally eliminates impossible explanations and provides relative evidence for competing hypotheses.\n\nThis example demonstrates how the Bayesian paradigm provides a coherent framework to simultaneously infer parameters and model complexity. The fact that Bayesian approach selects the most probable model that explains the observed data, is called the automatic Occam’s razor. The Occam’s razor is a principle that states that the simplest explanation is the best explanation.\nWhile performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below\n\n\n\nStep\nGiven\nHidden\nWhat to find\n\n\n\n\nTraining\n\\(D = (X,Y) = \\{x_i,y_i\\}_{i=1}^n\\)\n\\(\\theta\\)\n\\(p(\\theta \\mid D)\\)\n\n\nPrediction\n\\(x_{\\text{new}}\\)\n\\(y_{\\text{new}}\\)\n\\(p(y_{\\text{new}}  \\mid  x_{\\text{new}}, D)\\)\n\n\n\nThe training can be performed via the Bayes rule \\[\np(\\theta \\mid D) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\] Now to perform the second step (prediction), we calculate \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta\n\\] Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with \\[\np(\\theta \\mid D) \\approx \\delta(\\theta_{\\text{MAP}}),~~\\theta_{\\text{MAP}} \\in \\arg\\max_{\\theta}p(\\theta \\mid D)\n\\] To calculate \\(\\theta_{\\text{MAP}}\\), we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta \\approx p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta_{\\text{MAP}}).\n\\]\nThe Figure 16.8 below illustrates the Bayesian model selection process. The figure shows the joint distribution over parameters and data for three models. You can think of each ellipse as the region where most of the probability mass is concentrated.\n\n\n\n\n\n\nFigure 16.8: Bayesian model Selection\n\n\n\nIf we project the ellipses onto the parameter space, we get the prior distributions for each model. We can see that the \\(M_2\\) is the most concentrated. If we project the ellipses onto the data space, we get the prior distributions over data for each model.\nAfter observing data \\(D\\) (horizontal line), each prior gets updated. The intersection of the observed data line with each ellipse shows how well each model can explain the data. Models with good overlap between prior and observed data will have higher posterior probability. \\(M_3\\) appears to have the best intersection with the observed data, it is the model with the highest marginal likelihood.\nThis illustrates how Bayesian model selection naturally favors models that achieve the best balance between explaining the observed data and maintaining appropriate complexity, automatically implementing Occam’s razor through the evidence calculation.\n\nExample 16.6 (Racial discrimination) Say we want to analyze racial discrimination by the US courts. We have three variables:\n\nMurderer: \\(m \\in {0,1}\\) (black/white)\nVictim: \\(v \\in \\{0,1\\}\\) (black/white)\nVerdict: \\(d \\in \\{0,1\\}\\) (prison/death penalty)\n\nSay we have the data\n\n\n\nm\nv\nd\nn\n\n\n\n\n0\n0\n0\n132\n\n\n0\n0\n1\n19\n\n\n0\n1\n0\n9\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n52\n\n\n1\n0\n1\n11\n\n\n1\n1\n0\n97\n\n\n1\n1\n1\n6\n\n\n\nWe would like to establish a causal relations between the race and verdict variables. For this, we consider several models\n\n\\(p(d \\mid m,v) = p(d) = \\theta\\)\n\\(p(d \\mid m,v) = p(d \\mid v)\\); \\(p(d \\mid v=0) = \\alpha, ~p(d \\mid v=1)=\\beta\\)\n\\(p(d \\mid v,m) = p(d \\mid m)\\); \\(p(d \\mid m=1) = \\gamma,~p(d \\mid m=1) = \\delta\\)\n\\(p(d|v,m)\\) cannot be reduced, and\n\n\n\n\n\\(p(d=1 \\mid m,v)\\)\n\\(m=0\\)\n\\(m=1\\)\n\n\n\n\n\\(v=0\\)\n\\(\\tau\\)\n\\(\\chi\\)\n\n\n\\(v=1\\)\n\\(\\nu\\)\n\\(\\zeta\\)\n\n\n\n\nWe calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model \\[\np(Y ,\\theta \\mid X) = p(Y \\mid X,\\theta)p(\\theta \\mid X)\n\\] Here \\(X\\) is the number of cases, and \\(Y\\) is the number of death penalties. We use uninformative prior \\(\\theta \\sim U[0,1]\\). To specify the likelihood, we use Binomial distribution \\[\nY \\mid X,\\theta \\sim \\text{Bin}(X,\\theta),~~\\text{Bin}(Y \\mid X,\\theta) = \\binom{X}{Y}\\theta^Y(1-\\theta)^{X-Y}\n\\] We assume \\(p(\\theta)\\sim Uniform\\). Now lets calculate the evidence \\[\np(Y, \\theta \\mid X) = \\int p(Y  \\mid  X,\\theta)p(\\theta)d\\theta\n\\] for each of the four models\n\n\\(p(Y \\mid X) = \\int \\text{Bin}(19 \\mid 151,\\theta)\\text{Bin}(0 \\mid 9,\\theta)\\text{Bin}(11 \\mid 63,\\theta)\\text{Bin}(6 \\mid 103,\\theta)d\\theta\\) \\(\\propto \\int_0^{1} \\theta^{36}(1-\\theta)^{290}d\\theta = \\text{Beta}(37,291) = 2.8\\times 10^{-51}\\)\n\\(p(Y \\mid X) = \\int\\int \\text{Bin}(19 \\mid 151,\\alpha)\\text{Bin}(0 \\mid 9,\\beta)\\text{Bin}(11 \\mid 63,\\alpha)\\text{Bin}(6 \\mid 103,\\beta)d\\alpha d\\beta \\propto 4.7\\times 10^{-51}\\)\n\\(p(d \\mid v,m) = p(d \\mid m)=\\int\\int \\text{Bin}(19 \\mid 151,\\gamma)\\text{Bin}(0 \\mid 9,\\gamma)\\text{Bin}(11 \\mid 63,\\delta)\\text{Bin}(6 \\mid 103,\\delta)d\\gamma d\\delta \\propto 0.27\\times10^{-51}\\)\n\\(p(d \\mid v,m) = \\int\\int\\int\\int \\text{Bin}(19 \\mid 151,\\tau)\\text{Bin}(0 \\mid 9,\\nu)\\text{Bin}(11 \\mid 63,\\chi)\\text{Bin}(6 \\mid 103,\\zeta)d\\tau d\\nu d\\chi d\\zeta \\propto 0.18\\times10^{-51}\\)\n\nThe last model (4) is overly complex. With enough parameters, it can perfectly fit any dataset, including the noise. Ideally, we want a model that fits the data well but is also parsimonious. The Bayesian evidence calculation naturally penalized the complex model for its large parameter space, resulting in the lowest evidence score. This illustrates how the Bayesian approach inherently protects against overfitting without requiring an explicit regularization term.\nThis dataset also illustrates Simpson’s paradox, a phenomenon where a trend appears in different groups of data but disappears or reverses when these groups are combined. This highlights the importance of considering confounding variables and the structure of the data. A related conceptual problem is Bertrand’s box paradox, which demonstrates how conditional probability can be counterintuitive.\n\n\nBayesian Information Criterion (BIC)\nThe Bayesian Information Criterion (BIC) is a model selection criterion that penalizes the complexity of the model. It is derived from a Bayesian approach. The BIC is defined as:\n\\[\n\\mathrm{BIC} = \\log p(D\\mid \\hat{\\theta}_k, M_k) - \\frac{k}{2} \\log n.\n\\]\nHere \\(\\hat{\\theta}_k\\) is the MAP estimate of the \\(k\\) parameters in model \\(M_k\\), and \\(n\\) is the sample size. As such, there is a penalty \\(-\\frac{k}{2} \\log n\\) for increasing the dimensionality \\(k\\) of the model under consideration.\nThe BIC uses the marginal likelihood of the data under model \\(M_k\\) (denoted \\(M\\) for simplicity here), which is approximated using Laplace’s method.\nThe idea of Laplace’s method is to approximate integrals of the form \\(\\int f(\\theta) e^{-g(\\theta)} d\\theta\\) where \\(g(\\theta)\\) has a sharp minimum at some point \\(\\hat{\\theta}\\). The method works by approximating \\(g(\\theta)\\) with its second-order Taylor expansion around the minimum \\(\\hat{\\theta}\\). Since \\(g'(\\hat{\\theta}) = 0\\) at the minimum, we have\n\\[\ng(\\theta) \\approx g(\\hat{\\theta}) + \\frac{1}{2}g''(\\hat{\\theta})(\\theta-\\hat{\\theta})^2.\n\\] So the integral transforms into a Gaussian form: \\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} \\int e^{-\\frac{1}{2}g''(\\hat{\\theta})(\\theta-\\hat{\\theta})^2} d\\theta.\n\\]\nThe remaining integral is a standard Gaussian integral that evaluates to \\(\\sqrt{\\frac{2\\pi}{g''(\\hat{\\theta})}}\\), giving us:\n\\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} \\sqrt{\\frac{2\\pi}{g''(\\hat{\\theta})}}.\n\\]\nIn the multivariate case, we have \\(\\theta \\in \\mathbb{R}^k\\) is a \\(k\\)-dimensional parameter vector. The second-order Taylor expansion around the minimum \\(\\hat{\\theta}\\) becomes: \\[\ng(\\theta) \\approx g(\\hat{\\theta}) + \\frac{1}{2}(\\theta-\\hat{\\theta})^T \\mathbf{H}(\\hat{\\theta}) (\\theta-\\hat{\\theta}),\n\\] where \\(\\mathbf{H}(\\hat{\\theta})\\) is the \\(k \\times k\\) Hessian matrix of second derivatives at \\(\\hat{\\theta}\\). The multivariate Gaussian integral then evaluates to:\n\\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} (2\\pi)^{k/2} |\\det(\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}\n\\]\nIn the context of Bayesian model selection, we apply this to approximate the marginal likelihood (evidence). We have:\n\\[\nP(D\\mid M) = \\int P(D\\mid \\theta,M)P(\\theta\\mid M)d\\theta\n\\]\nTaking the logarithm and identifying \\(g(\\theta) = -\\log P(D\\mid \\theta,M)P(\\theta\\mid M)\\), the maximum a posteriori (MAP) estimate \\(\\hat{\\theta}\\) corresponds to the minimum of \\(g(\\theta)\\). The second derivative (Hessian) \\(\\mathbf{H}(\\hat{\\theta})\\) at this point determines the curvature of the log-posterior.\n\\[\np(D\\mid M) = \\int p(D\\mid \\theta,M)p(\\theta\\mid M)d\\theta \\approx p(D\\mid \\hat{\\theta},M)p(\\hat{\\theta}\\mid M) (2 \\pi)^{k/2} |\\det(\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}.\n\\]\nHere \\(\\hat{\\theta}\\) is the posterior mode (MAP estimate), and \\(\\mathbf{H}(\\hat{\\theta})\\) is the negative Hessian of the log-posterior at the mode. Taking the logarithm, and assuming \\(P(\\hat{\\theta}|M)\\) and Hessian terms are \\(O_p(1)\\) or scale appropriately with \\(n\\) (this assumption is justified because as \\(n\\) increases, the likelihood dominates the prior, making the prior term negligible relative to the \\(O(\\log n)\\) likelihood term, while the Hessian determinant typically grows polynomially in \\(n\\), contributing at most \\(O(\\log n)\\) terms that are absorbed into the approximation), we get:\n\\[\n\\log p(D\\mid M) \\approx \\log p(D\\mid \\hat{\\theta},M) - \\dfrac{k}{2}\\log n,\n\\]\nwhich is proportional to the BIC. (Note: The exact definition and derivation of BIC can vary slightly, but this captures the essence). The BIC approximation shows how the Bayesian approach naturally penalizes model complexity through the dimensionality term \\(-\\frac{k}{2}\\log n\\).\nThe Bayesian approach averages over the posterior distribution of models given data. Suppose that we have a finite list of models \\(M \\in \\{M_1, \\ldots, M_J\\}\\). Then we can calculate the posterior over models as:\n\\[\np(M_j | y) = \\frac{p(y | M_j) p(M_j)}{\\sum_{i=1}^J p(y | M_i) p(M_i)}, \\quad \\text{where}\\; p(y | M_j) = \\int L_j(\\theta_j|y) p(\\theta_j | M_j) d\\theta_j.\n\\]\nLaplace’s approximation provides a simple (Lindley 1961) illustration of how dimensionality is weighted in the Bayesian paradigm. Hence, BIC is related to a log-posterior approximation. Hence, if prior model probabilities \\(P(M_j)\\) are uniform, then \\(P(M_j\\mid D) \\propto P(D \\mid M_j) \\approx \\exp(\\mathrm{BIC}_j)\\).\nIn a more general case, the evidence (a.k.a. marginal likelihood) for hypotheses (a.k.a. models) \\(M_i\\) is calculated as follows:\n\\[\nP(D\\mid M_i) = \\int P(D\\mid \\theta, M_i)P(\\theta\\mid M_i)d\\theta.\n\\]\nLaplace approximation, in the one-dimensional case (\\(k=1\\)), yields:\n\\[\nP(D\\mid M_i) \\approx P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i)\\sqrt{2\\pi}\\sigma_{\\text{post}}.\n\\]\nHere \\(\\hat{\\theta}\\) is the maximum (MAP) estimate of the parameter and \\(\\sigma_{\\text{post}} = (-H(\\hat{\\theta}))^{-1/2}\\) where \\(H(\\hat{\\theta})\\) is the second derivative of the log-posterior at \\(\\hat{\\theta}\\).\nGenerally, in the \\(k\\)-dimensional case, we have:\n\\[\nP(D\\mid M_i) \\approx P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i) (2\\pi)^{k/2} |\\det(-\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}.\n\\]\nHere \\(\\mathbf{H}(\\hat{\\theta}) = \\nabla^2\\log (P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i))\\) is the Hessian of the log-posterior function evaluated at the mode \\(\\hat{\\theta}\\). As the amount of data collected increases, this Gaussian approximation is expected to become increasingly accurate.\nMackay (MacKay 1992) proposes the NIC criterion for selection of neural networks.\n\n\nNeural Information Criterion (NIC)\nDavid MacKay’s Neural Information Criterion (NIC) and the associated evidence framework represent pioneering efforts to apply Bayesian model selection principles to neural networks. This framework addresses the fundamental challenge of selecting appropriate network architectures and hyperparameters by computing approximations to the marginal likelihood, or model evidence, for different neural network configurations.\nThe evidence framework builds upon the Laplace approximation to estimate the marginal likelihood of neural network models. Given a neural network with parameters \\(\\theta\\) and hyperparameters \\(\\alpha\\) (such as weight decay parameters), the evidence for a particular model configuration is:\n\\[\nP(D|M,\\alpha) = \\int P(D|\\theta,M) P(\\theta|M,\\alpha) d\\theta\n\\]\nwhere \\(D\\) represents the training data, \\(M\\) denotes the model architecture, and \\(P(\\theta|M,\\alpha)\\) is the prior distribution over network weights. The Laplace approximation evaluates this integral by expanding the log-posterior around its mode \\(\\hat \\theta\\), yielding:\n\\[\n\\log P(D|M,\\alpha) \\approx \\log P(D|\\hat \\theta,M) + \\log P(\\hat \\theta|M,\\alpha) - \\frac{1}{2}\\log|H|\n\\]\nwhere \\(H\\) is the Hessian of the negative log-posterior at the mode \\(\\hat \\theta\\). This approximation transforms the intractable integral into a computation involving the maximum a posteriori (MAP) estimate and the curvature of the posterior at that point.\nThe evidence framework provides a principled approach to several critical decisions in neural network design. For hyperparameter selection, the framework automatically determines optimal regularization strengths by maximizing the evidence with respect to hyperparameters such as weight decay coefficients. Rather than relying on cross-validation, which can be computationally expensive and may not capture the full uncertainty in hyperparameter selection, the evidence provides a direct measure of how well different hyperparameter values support the observed data.\nArchitecture comparison becomes feasible through direct evidence computation for different network structures. The framework can compare networks with different numbers of hidden units, layers, or connectivity patterns by evaluating their respective marginal likelihoods. This comparison naturally incorporates Occam’s razor, as more complex architectures are penalized through the integration over their larger parameter spaces, unless the additional complexity is justified by substantially improved fit to the data.\nThe Hessian computation required for the Laplace approximation presents significant computational challenges for modern deep networks with millions or billions of parameters. The full Hessian matrix would be prohibitively large to compute and store explicitly. MacKay’s original framework addressed this through various approximation strategies, including the use of automatic relevance determination (ARD) priors that allow the network to effectively prune irrelevant connections by driving their associated precision parameters to infinity.\n\n\nAutomatic Relevance Determination (ARD)\nThe key insight of Automatic Relevance Determination (ARD) is to introduce separate precision hyperparameters for different groups of parameters, allowing the model to automatically determine which features or components are relevant for the task.\nIn the context of neural networks, consider a network with weights \\(\\mathbf{w} = \\{w_{ij}\\}\\) connecting input features to hidden units. Instead of using a single precision parameter \\(\\alpha\\) for all weights, ARD introduces feature-specific precision parameters \\(\\{\\alpha_i\\}_{i=1}^{p}\\) where \\(p\\) is the number of input features. The prior distribution for weights becomes:\n\\[\np(\\mathbf{w}|\\boldsymbol{\\alpha}) = \\prod_{i=1}^{p} \\prod_{j=1}^{H} \\mathcal{N}(w_{ij}|0, \\alpha_i^{-1})\n\\]\nwhere \\(H\\) is the number of hidden units and \\(\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_p)\\) are the precision hyperparameters.\nThe hierarchical Bayesian model is completed by placing hyperpriors on the precision parameters:\n\\[\np(\\alpha_i) = \\text{Gamma}(\\alpha_i|a_i, b_i)\n\\]\nwhere \\(a_i\\) and \\(b_i\\) are shape and rate parameters, often set to small values (e.g., \\(a_i = b_i = 10^{-6}\\)) to create weakly informative priors.\nThe ARD mechanism works through the evidence framework by optimizing the marginal likelihood with respect to the hyperparameters. For a given precision \\(\\alpha_i\\), the effective contribution of feature \\(i\\) to the model evidence can be approximated as:\n\\[\n\\log p(D|\\alpha_i) \\approx -\\frac{1}{2}\\alpha_i \\|\\mathbf{w}_i\\|^2 + \\frac{H}{2}\\log\\alpha_i - \\frac{1}{2}\\log|\\mathbf{A}_i|\n\\]\nwhere \\(\\mathbf{w}_i\\) represents all weights associated with feature \\(i\\), and \\(\\mathbf{A}_i\\) is the corresponding block of the Hessian matrix.\nWhen a feature is irrelevant, the optimal precision \\(\\alpha_i^*\\) tends to infinity, effectively removing the feature from the model. This occurs because the evidence balances the model fit (first term) against the model complexity (second and third terms). For irrelevant features, the improvement in fit is insufficient to justify the complexity cost, driving \\(\\alpha_i\\) to large values.\nThe ARD update equations, derived by maximizing the marginal likelihood, are:\n\\[\n\\alpha_i^{\\text{new}} = \\frac{\\gamma_i}{\\|\\mathbf{w}_i\\|^2}\n\\]\nwhere \\(\\gamma_i\\) is the effective number of parameters associated with feature \\(i\\):\n\\[\n\\gamma_i = H - \\alpha_i \\text{Tr}(\\mathbf{A}_i^{-1})\n\\]\nHere, \\(\\text{Tr}(\\mathbf{A}_i^{-1})\\) represents the trace of the inverse of the Hessian block corresponding to feature \\(i\\).\nExample: Linear Regression with ARD\nConsider a linear regression model with ARD priors:\n\\[\ny = \\sum_{i=1}^{p} w_i x_i + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})\n\\]\nwith priors: \\[\nw_i \\sim \\mathcal{N}(0, \\alpha_i^{-1}), \\quad i = 1, \\ldots, p\n\\]\nThe posterior distribution for the weights is:\n\\[\np(\\mathbf{w}|D, \\boldsymbol{\\alpha}, \\beta) = \\mathcal{N}(\\mathbf{w}|\\boldsymbol{\\mu}, \\mathbf{\\Sigma})\n\\]\nwhere: \\[\n\\mathbf{\\Sigma} = (\\beta \\mathbf{X}^T\\mathbf{X} + \\text{diag}(\\boldsymbol{\\alpha}))^{-1}\n\\] \\[\n\\boldsymbol{\\mu} = \\beta \\mathbf{\\Sigma} \\mathbf{X}^T \\mathbf{y}\n\\]\nThe ARD updates become: \\[\n\\alpha_i^{\\text{new}} = \\frac{1 - \\alpha_i \\Sigma_{ii}}{\\mu_i^2}\n\\]\nWhen \\(\\alpha_i\\) becomes very large, the corresponding \\(\\mu_i \\approx 0\\) and \\(\\Sigma_{ii} \\approx 0\\), effectively removing feature \\(i\\) from the model.\nExample: Neural Network Feature Selection\nIn a neural network with \\(p\\) input features and \\(H\\) hidden units, ARD can automatically determine which input features are relevant. Suppose we have a dataset with features representing different types of measurements, some of which may be irrelevant for the prediction task.\nThe network architecture is: \\[\nh_j = \\tanh\\left(\\sum_{i=1}^{p} w_{ij} x_i + b_j\\right), \\quad j = 1, \\ldots, H\n\\] \\[\ny = \\sum_{j=1}^{H} v_j h_j + c\n\\]\nWith ARD priors on input weights: \\[\nw_{ij} \\sim \\mathcal{N}(0, \\alpha_i^{-1}), \\quad \\text{for all } j\n\\]\nAfter training with the evidence framework, features with large \\(\\alpha_i\\) values (typically \\(\\alpha_i &gt; 10^6\\)) are considered irrelevant and can be pruned. This automatic feature selection often reveals that only a subset of the original features are necessary for good predictive performance.\nThe ARD principle extends beyond feature selection to other forms of model selection, including:\n\nUnit pruning: Using separate precisions for different hidden units to determine optimal network architecture\nGroup selection: Applying ARD to groups of related parameters (e.g., all weights in a particular layer)\nSparse coding: In dictionary learning, ARD can automatically determine the effective dictionary size\n\nThe computational implementation of ARD involves iterating between updating the model parameters (weights) and the hyperparameters (precisions) until convergence. Modern implementations often use variational approximations or sampling methods to handle the computational challenges of the full Bayesian treatment, while maintaining the automatic model selection capabilities that make ARD so valuable in practice.\nModern adaptations of the evidence framework have developed sophisticated methods to handle the computational challenges of contemporary deep learning. For example, linearized Laplace approximations—such as those described by Ritter et al. (2018) (Ritter, Botev, and Barber 2018) and Immer et al. (2021) (Immer et al. 2021)—approximate the neural network through its first-order Taylor expansion around the MAP estimate, reducing the complexity of Hessian computations while maintaining reasonable approximation quality. These linearized approaches are particularly effective for networks that are sufficiently wide or when the posterior is approximately Gaussian.\nKronecker-structured approximations represent another significant advancement, exploiting the structure of neural network computations to factorize the Hessian matrix into more manageable components. By recognizing that gradients in neural networks can be expressed as Kronecker products of activations and error signals, these methods achieve substantial computational savings while preserving much of the information contained in the full Hessian matrix. Singh, Farrell-Maupin, and Faghihi (2024) revisit and advance the Laplace approximation for Bayesian deep learning, addressing its scalability and effectiveness for modern neural networks. The paper introduces new algorithmic and theoretical developments that make Laplace-based Bayesian inference practical for large-scale deep learning tasks. The authors propose efficient algorithms for computing the Laplace approximation in deep neural networks, leveraging block-diagonal and Kronecker-factored structures to approximate the Hessian of the loss function, which enables uncertainty quantification in models with millions of parameters.\nOn the theoretical side, the paper provides a new analysis of the Laplace approximation in high-dimensional and overparameterized regimes typical of deep learning. Specifically, the authors derive non-asymptotic error bounds for the Laplace approximation, showing how its accuracy depends on the curvature of the loss landscape and the concentration of the posterior. They analyze the impact of model width and data size on the quality of the Gaussian approximation, and clarify under what conditions the Laplace approximation remains reliable as the number of parameters grows. This theoretical work helps explain when and why Laplace-based uncertainty estimates are trustworthy in modern neural networks, and guides the design of scalable algorithms for practical Bayesian deep learning.\nThe evidence framework also naturally handles the multiple scales of uncertainty present in neural networks. Parameter uncertainty captures the uncertainty in individual weight values given the training data, while hyperparameter uncertainty reflects uncertainty about the appropriate level of regularization or architectural choices. Model uncertainty encompasses uncertainty about the fundamental model class or architecture family. The hierarchical Bayesian treatment allows simultaneous reasoning about all these sources of uncertainty within a unified framework.\nDespite its theoretical elegance, the evidence framework faces practical limitations in very large-scale applications. The computational requirements of Hessian approximation, even with modern efficient methods, can be substantial for networks with hundreds of millions of parameters. The Laplace approximation itself may be inadequate when the posterior is highly non-Gaussian, which can occur in networks with many local minima or complex loss landscapes.\nThe enduring value of MacKay’s evidence framework lies in its principled approach to the fundamental trade-offs in machine learning model design. By providing a theoretically grounded method for balancing model complexity against data fit, the framework offers insights that remain relevant even as the scale and sophistication of machine learning models continue to evolve. The automatic hyperparameter selection and architecture comparison capabilities of the evidence framework continue to influence contemporary approaches to neural architecture search and automated machine learning.\nYet another approach is the Widely applicable Bayesian Information Criterion (WBIC) proposed by Watanabe (2013). It is a generalization of the traditional BIC that addresses some of its fundamental limitations, particularly when dealing with singular statistical models and complex machine learning architectures. It addresses the problem when BIC’s assumptions that the true parameter lies in the interior of the parameter space and that the information matrix is positive definite are violated. These regularity conditions fail for many important models such as neural networks with hidden units, mixture models where the number of components is unknown, tree-based models with unknown structure, and models with parameter constraints or boundaries. Second, BIC requires knowing the effective number of parameters \\(k\\), which can be ambiguous for complex models. This becomes problematic when dealing with shared parameters across different parts of the model, regularization that effectively reduces the parameter dimension, or hierarchical structures where the effective dimensionality depends on the data. The challenge of defining the “true” number of parameters in modern machine learning models makes BIC’s penalty term difficult to specify correctly.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#model-selection-and-bayesian-relativity",
    "href": "16-select.html#model-selection-and-bayesian-relativity",
    "title": "16  Model Selection",
    "section": "16.9 Model Selection and Bayesian Relativity",
    "text": "16.9 Model Selection and Bayesian Relativity\nBayes’ rule provides only relative evidence between models. Classical approaches try to find absolute truth, but Bayesian model selection is fundamentally comparative. Consider the basic relationship: \\[\n\\frac{p(H_0 \\mid D)}{p(H_1 \\mid D)} = \\frac{p(D \\mid H_0)}{p(D \\mid H_1)} \\frac{p(H_0)}{p(H_1)}\n\\]\nWhen \\(D = \\{T(y) &gt; t_{obs}\\}\\), the numerator is the p-value. However, this needs to be assessed relative to the p-value under the alternative, which might be even more unlikely! As Sherlock Holmes noted: “When you have eliminated the impossible, whatever remains, however improbable, must be the truth.” Even though an event may be unlikely under \\(H_0\\), it could be the best available description given the alternatives.\nFisher recognized this issue: “In scientific inference, a hypothesis is never proved but merely shown to be more or less probable relative to the available alternatives.” The problem with p-values is that they attempt to be an objective measure of model “fit” without considering alternatives. Unlikely events do occur under a “true” model.\n\nExhaustive vs Non-Exhaustive Hypotheses\nA key point is that you can always calculate the relative evidence between two hypotheses. In cases where hypotheses are exhaustive, \\(p(H_0) + p(H_1) = 1\\), we can directly calculate \\(p(H_0 \\mid D)\\) and we obtain the true probability given the data. In general, we have some priot probability left over for a model that is not in the set of models under consideration. But, you can still use the Bayes rule for relative evidence to obtain just a relative ordering: \\[\n\\frac{p(H_0 \\mid D)}{p(H_1 \\mid D)} = \\frac{p(D \\mid H_0)}{p(D \\mid H_1)} \\frac{p(H_0)}{p(H_1)}\n\\]\nThis holds for \\(p(H_0) + p(H_1) &lt; 1\\). An important benefit of this rational approach is that if a new hypothesis \\(H_2\\) comes along, the relative calculation between \\(H_0\\) and \\(H_1\\) doesn’t change! This is a benefit of rational decision-making. However, posterior probabilities can change if we re-normalize to account for the new alternative.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#the-asymptotic-carrier",
    "href": "16-select.html#the-asymptotic-carrier",
    "title": "16  Model Selection",
    "section": "16.10 The Asymptotic Carrier",
    "text": "16.10 The Asymptotic Carrier\nWhat happens when the “true” model is not in the set of models under consideration? This is a critical question in modern machine learning, where model misspecification is the norm rather than the exception.\nThe asymptotic behavior of the posterior \\(p(\\theta \\mid y)\\) is characterized by the asymptotic carrier of the posterior. Let \\(F\\) denote the true data-generating process. The set \\(\\mathcal{C}\\) is defined by: \\[\n\\mathcal{C} = \\arg\\min_{\\theta \\in \\Theta} \\int f(y) \\log f_\\theta(y) dy = \\arg\\min_{\\theta \\in \\Theta} KL(f, f_\\theta)\n\\]\nThat is, the posterior over parameters \\(\\theta\\) in the model class \\(\\mathcal{M}\\) converges to the density that minimizes the Kullback-Leibler (KL) distance between the data-generating process \\(f\\) and the model class.\nThe posterior has the limiting property that for any \\(A \\subset \\mathcal{C}\\): \\[\n\\lim_{n \\to \\infty} P_{\\mathcal{M}}[A \\mid y_1, \\ldots, y_n] = 1 \\text{ almost surely under } F\n\\]\nSince Berk (1966), there has been extensive work on the limiting behavior of the posterior when the true model \\(f\\) lies outside the class \\(\\{f_\\theta\\}\\) indexed by the models under consideration. This theory provides important insights:\n\nConsistency under misspecification: Even when the true model is not in our class, the posterior will concentrate on the best approximation within that class.\nKL optimality: The limiting posterior focuses on parameters that minimize the KL divergence, which is often a reasonable criterion for model approximation.\nPractical implications: This suggests that Bayesian methods can be robust to model misspecification, concentrating probability mass on the best available approximation.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#model-explainability-1",
    "href": "16-select.html#model-explainability-1",
    "title": "16  Model Selection",
    "section": "16.11 Model Explainability",
    "text": "16.11 Model Explainability\nIn some applications, model explainability is an important criterion for model selection. For example, in finance or insurance, model explainability is important for the model to be accepted by the regulators. It is highly unlikely a black box model will ever be used for medical or criminal justice applications. While earlier in this chapter we introduced several explainability techniques in the context of fundamental considerations, here we provide a comprehensive treatment of model explainability methods, their applications, and a broader perspective on the role of explanation in scientific and practical domains.\nModern machine learning has developed a rich toolkit of methods to interpret and explain model predictions. These methods vary in their scope (global vs. local explanations), their model specificity (model-agnostic vs. model-specific), and their theoretical foundations.\nModel-Agnostic Methods provide explanations that work with any machine learning model by treating it as a black box. LIME (Local Interpretable Model-agnostic Explanations) Ribeiro, Singh, and Guestrin (2016) creates explanations for individual predictions by training simple, interpretable models locally around the prediction of interest. The algorithm perturbs the input and observes how the model’s predictions change, then fits a linear model weighted by proximity to the original instance. SHAP (SHapley Additive exPlanations) Lundberg and Lee (2017) provides a unified framework based on game theory, specifically Shapley values from cooperative game theory. SHAP assigns each feature an importance value for a particular prediction, satisfying desirable properties like local accuracy, missingness, and consistency. The method has become widely adopted due to its solid theoretical foundation and availability of efficient implementations for various model types.\nPartial Dependence Plots (PDP) Friedman (2001) and Individual Conditional Expectation (ICE) plots Goldstein et al. (2015) offer visual methods to understand how features influence predictions across the entire dataset. PDPs show the marginal effect of a feature on the predicted outcome by averaging over all other features, while ICE plots show the prediction dependence for individual instances. Accumulated Local Effects (ALE) Apley and Zhu (2020) improve upon PDPs by accounting for feature correlations, providing more reliable interpretations in high-dimensional spaces.\nGradient-Based Methods leverage the internal structure of differentiable models, particularly neural networks. Saliency maps Simonyan, Vedaldi, and Zisserman (2013) compute gradients of the output with respect to input features to identify which inputs most influence predictions. Integrated Gradients Sundararajan, Taly, and Yan (2017) improve upon simple gradients by integrating gradients along a path from a baseline to the actual input, satisfying important axioms like sensitivity and implementation invariance. Grad-CAM (Gradient-weighted Class Activation Mapping) Selvaraju et al. (2017) generates visual explanations for convolutional neural networks by computing gradients of the target class with respect to feature maps in the final convolutional layer, producing class-discriminative localization maps that highlight important regions in images.\nAttention Mechanisms Bahdanau, Cho, and Bengio (2014) provide built-in interpretability, particularly in natural language processing and computer vision. By learning to focus on relevant parts of the input, attention weights offer direct insight into which components the model considers important for its predictions. Transformer architectures Vaswani et al. (2017) have made attention mechanisms ubiquitous in modern deep learning, though interpreting multi-head attention in large language models remains an active research area Clark et al. (2019).\nModel-Specific Interpretability includes methods designed for particular model architectures. Tree-based models like random forests and gradient boosting machines provide natural feature importance measures based on how much each feature decreases impurity or loss across splits Breiman (2001) Friedman (2001). Linear models offer direct coefficient interpretation, though care must be taken with correlated features. Rule extraction methods Craven and Shavlik (1996) attempt to distill complex models into human-readable rule sets.\nCounterfactual Explanations Wachter, Mittelstadt, and Russell (2017) answer the question “what would need to change for the prediction to be different?” by finding the minimal modifications to input features that would alter the model’s decision. This approach is particularly valuable in applications like loan decisions, where explaining why an application was rejected is less actionable than explaining what changes would lead to approval.\nWe now provide detailed mathematical foundations for the most widely-used explainability methods, accompanied by practical demonstrations in R. These methods represent the core toolkit for model interpretation in modern machine learning practice.\n\nThe Imperative for Explainability\nThe demand for explainability stems from multiple sources, each with distinct requirements and motivations. In regulated industries, explainability is often a legal requirement. The European Union’s GDPR includes provisions for algorithmic transparency, requiring that individuals affected by automated decisions receive meaningful information about the logic involved. The Equal Credit Opportunity Act in the United States mandates that financial institutions provide specific reasons for adverse credit decisions. Healthcare applications face similar regulatory scrutiny, where the FDA increasingly requires evidence of interpretability for AI-assisted diagnostic tools.\nTrust and adoption present another critical dimension. Medical practitioners are unlikely to rely on diagnostic systems they don’t understand, regardless of their accuracy. A radiologist examining a potential tumor needs to see which image features led to the model’s assessment, allowing them to combine algorithmic insights with their clinical expertise. Financial advisors similarly require explanations to justify investment recommendations to clients and comply with fiduciary duties. In these contexts, explainability is not merely desirable but essential for practical deployment.\nDebugging and model improvement benefit significantly from interpretability tools. When a model fails on certain types of inputs, understanding its decision-making process helps identify the root cause. Is the model relying on spurious correlations? Has it learned dataset biases? Are certain features being misinterpreted? Explainability methods allow practitioners to diagnose these issues and refine their models accordingly. The famous case of the “wolf detector” that was actually detecting snow backgrounds Ribeiro, Singh, and Guestrin (2016) illustrates how explanations can reveal that models learn unexpected patterns.\nFairness and bias detection represent another crucial application of explainability. When models exhibit disparate performance across demographic groups, understanding which features drive predictions helps identify sources of bias. If a hiring model heavily weights features correlated with protected attributes, explanations can surface these problematic dependencies, enabling corrective action through feature engineering, reweighting, or architectural changes.\nScientific discovery increasingly relies on machine learning models that identify patterns humans might miss. In drug discovery, models that predict molecular properties need to be interpretable to generate scientific insights about structure-activity relationships Jiménez-Luna et al. (2020). Climate models, protein folding predictions Jumper et al. (2021), and materials science applications all benefit from understanding not just what the model predicts, but why it makes those predictions, as these explanations can suggest new hypotheses and research directions.\n\n\nThe Paradox of Understanding and Utility\nThe insistence on full explanatory understanding before deploying useful tools, however, represents a historically unusual stance. Many of our most valuable scientific and medical technologies were adopted and saved lives long before we fully understood their mechanisms.\nConsider mammography, one of the most important breast cancer screening tools developed in the 20th century. Mammograms began widespread use in the 1960s and 1970s, and large-scale trials in the 1970s and 1980s demonstrated clear mortality reduction in screened populations Shapiro (1988) Tabar et al. (1985). Yet our understanding of breast cancer biology, tumor heterogeneity, and the precise mechanisms by which early detection improves outcomes continued to evolve for decades afterward. We still grapple with questions about optimal screening intervals, the balance of benefits and harms, and which tumors are truly aggressive versus indolent. The initial adoption was based on empirical evidence of benefit, not complete mechanistic understanding. The lack of complete explanation did not prevent mammography from saving countless lives.\nSimilarly, many pharmaceutical interventions were discovered through empirical observation long before their mechanisms were understood. Aspirin was used for decades before we understood its inhibition of cyclooxygenase enzymes. Lithium became a treatment for bipolar disorder in the 1940s, but its precise mechanism of action remains incompletely understood. Anesthesia was used successfully for over 150 years before we developed adequate theories of how it works. In each case, careful empirical validation preceded mechanistic understanding, and the lack of explanation did not preclude tremendous benefit.\nEngineering provides equally striking examples. The Navier-Stokes equations, which describe fluid flow and form the foundation of aerodynamics, weather prediction, and countless other applications, were formulated in the 19th century. We use numerical solutions to these equations daily for critical applications: designing aircraft, predicting hurricanes, optimizing turbine efficiency, and modeling blood flow in cardiovascular research. Yet one of the Clay Mathematics Institute’s Millennium Prize Problems, offering a million-dollar prize, asks whether solutions to the Navier-Stokes equations even exist and are unique in three dimensions Fefferman (2006). We have built an entire technological civilization on equations whose mathematical foundations remain unproven. Engineers successfully use these equations not through complete mathematical understanding but through empirical validation, computational approximation, and careful attention to when the models work well and when they fail.\nThis historical pattern suggests a more nuanced view of explainability in machine learning. The demand that we fully understand and explain every prediction from a neural network before deploying it represents a higher standard than we have applied to many successful technologies. This is not to argue against explainability research—it is valuable and important. Rather, it suggests that we should focus on:\nEmpirical validation through rigorous testing, cross-validation, and real-world performance monitoring. A model that consistently makes accurate predictions on held-out test sets and in production may be deployable even if we cannot fully explain every decision.\nUnderstanding failure modes rather than complete mechanistic understanding. Knowing when and where a model is likely to fail, what types of inputs it handles poorly, and how confident it is in its predictions may be more actionable than detailed explanations of every prediction.\nComplementary human oversight in high-stakes decisions. Rather than requiring complete explainability, we might deploy powerful but less interpretable models in systems where humans remain in the loop, using model predictions as one input among many.\nAppropriate deployment contexts that match interpretability requirements to stakes and alternatives. A model that routes customer service calls can reasonably be less interpretable than one making parole decisions. The comparison should be to existing alternatives: is the model more or less fair, accurate, and interpretable than current practice?\nThe goal is not to abandon explainability but to maintain perspective. History suggests that empirically validated tools often precede complete understanding, and this gap need not prevent their careful deployment. As we develop more sophisticated explainability methods, we should simultaneously develop more sophisticated frameworks for when and why explanation is necessary, recognizing that the relationship between understanding and utility is subtle and context-dependent.\nThis balanced view—pursuing explainability while acknowledging the historical precedent for useful tools that outpace understanding—may lead to more productive deployment of machine learning in domains where it can provide real benefit, accompanied by the monitoring, validation, and human oversight appropriate to the stakes involved.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#model-elaboration-and-nested-model-testing",
    "href": "16-select.html#model-elaboration-and-nested-model-testing",
    "title": "16  Model Selection",
    "section": "16.12 Model Elaboration and Nested Model Testing",
    "text": "16.12 Model Elaboration and Nested Model Testing\nAn elaborated model in Bayesian statistics refers to a model that extends or generalizes a simpler, baseline (or “underlying”) model by introducing additional parameters or structure. The purpose of elaboration is to capture more complex features of the data, account for possible deviations from the assumptions of the simpler model, or to allow for greater flexibility in modeling.\nFormally, suppose we start with a baseline model \\(f(y \\mid \\theta)\\), where \\(\\theta\\) is a parameter of interest. An elaborated model introduces an additional parameter (or set of parameters) \\(\\lambda\\), resulting in a family of models \\(f(y \\mid \\theta, \\lambda)\\) indexed by \\(\\lambda \\in \\Lambda\\). The original model is recovered as a special case for some fixed value \\(\\lambda_0\\) (i.e., \\(f(y \\mid \\theta) = f(y \\mid \\theta, \\lambda_0)\\)). The set \\(\\Lambda\\) describes the ways in which the model can be elaborated.\nWhen we use elaborated models then we need to compare nested models (where the simpler model is a special case of the more complex one). In Bayesian analysis, inference in an elaborated model involves integrating over the additional parameters, reflecting uncertainty about both the original and the elaborating parameters.\nApplying the usual Bayesian paradigm (disciplined probability accounting) to the elaborated framework, we see that inference about \\(\\theta\\) is determined by \\[\np(\\theta \\mid y) = \\int_\\Lambda p(\\theta \\mid \\lambda, y) p(\\lambda \\mid y) d\\lambda\n\\] where \\[\n\\begin{aligned}\np(\\theta \\mid \\lambda, y) &\\propto p(y \\mid \\theta, \\lambda) p(\\theta \\mid \\lambda) \\\\\np(\\lambda \\mid y) &\\propto p(y \\mid \\lambda) p(\\lambda) \\\\\n\\text{where } p(y \\mid \\lambda) &= \\int p(y \\mid \\theta, \\lambda) p(\\theta \\mid \\lambda) d\\theta\n\\end{aligned}\n\\]\nFor consistency with the elaborated and underlying model, we take \\(p(\\theta \\mid \\lambda_0) = p(\\theta)\\). Since \\(\\lambda\\) labels the form of departure from the initial model \\(M_0: \\lambda = \\lambda_0\\), the form of \\(p(\\lambda)\\) should be chosen to reflect this departure.\nA classical example is the exponential power elaboration of the traditional normal family, allowing for robustness. Here \\(\\lambda \\in (0,3)\\) indexes the power and \\(\\lambda_0 = 2\\) is the Normal case.\nThe posterior mean is simply a weighted average with respect to \\(p(\\lambda \\mid y)\\): \\[\n\\E{\\theta \\mid y} = \\int \\E{\\theta \\mid \\lambda, y} p(\\lambda \\mid y) d\\lambda = \\E[\\lambda \\mid y]{\\E{\\theta \\mid \\lambda, y}}\n\\]\n\nThe Dickey-Savage Approach to Nested Models\nThe Dickey-Savage approach provides a principled Bayesian method for testing nested models—situations where a simpler model is a special case of a more complex one. This approach is particularly useful when we want to assess whether the data support the inclusion of additional parameters or structure in our model, or whether the simpler, baseline model suffices.\nIn the context of Bayesian hypothesis testing, the Dickey-Savage method allows us to compute the Bayes factor for comparing a nested (elaborated) model to its simpler counterpart using only the posterior and prior distributions of the parameter(s) that distinguish the two models. This not only streamlines the computation but also clarifies the relationship between the models and the evidence provided by the data.\n\n\n\n\n\n\nNoteThe Bayes Folklore\n\n\n\nIn essence, the Dickey-Savage result supports the Bayesian strategy of fitting the most comprehensive model possible—sometimes referred to as the ‘elephant’ model—and testing simpler sub-models as special cases. You fit a model \\(M\\) as complex as allowed by your computational budget and statistical skills. The calculations for any nested model, e.g., \\(M_0\\), can then be performed entirely within the framework of \\(M\\).\n\n\nSuppose you are conducting a hypothesis test comparing two models, \\(M_0\\) and \\(M_1\\). The null hypothesis \\(H_0\\) is that the simpler model \\(M_0\\) is true, and the alternative hypothesis \\(H_1\\) is that the more complex model \\(M_1\\) is true.\nLet’s explore how this approach works and why it is both elegant and practical for model comparison in Bayesian analysis.\nSuppose that \\(M_0 \\subset M\\). Let \\((\\theta, \\psi)\\) have matching priors such that \\[\np(\\psi \\mid \\theta = 0, M) = p(\\psi \\mid M_0)\n\\] where \\(\\theta = 0\\) corresponds to \\(M_0\\). That is, \\(p(y \\mid \\theta = 0, \\psi, M) = p(y \\mid \\psi, M_0)\\).\nThen, we can calculate solely under model \\(M\\) the Bayes factor as follows: \\[\nBF = \\frac{p(\\theta = 0 \\mid y, M)}{p(\\theta = 0 \\mid M)} = \\frac{p(y \\mid M_0)}{p(y \\mid M)}\n\\]\nThis is a ratio of posterior ordinates, valid as long as models are nested. By definition of marginals: \\[\n\\begin{aligned}\np(y \\mid \\theta = 0, M) &= \\int p(y \\mid \\theta = 0, \\psi, M) p(\\psi \\mid \\theta = 0, M) d\\psi \\\\\n&= \\int p(y \\mid \\psi, M_0) p(\\psi \\mid M_0) d\\psi \\\\\n&= p(y \\mid M_0)\n\\end{aligned}\n\\]\nThis elegant result shows that Bayes factors for nested models can be computed entirely within the larger model framework.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "16-select.html#conclusion",
    "href": "16-select.html#conclusion",
    "title": "16  Model Selection",
    "section": "16.13 Conclusion",
    "text": "16.13 Conclusion\nModel selection lies at the heart of the machine learning workflow. Throughout this chapter, we have explored the fundamental tensions that define this task: the bias-variance trade-off, the pursuit of out-of-sample generalization, and the balance between predictive power and interpretability.\nSeveral key principles emerge. First, in-sample performance is a poor guide to model quality; rigorous out-of-sample evaluation via cross-validation or held-out test sets is essential. Second, the Bayesian framework provides a principled, probabilistic approach to model comparison through the evidence or marginal likelihood, which naturally implements Occam’s razor by penalizing unnecessary complexity. Information criteria like BIC offer practical approximations to this ideal. Third, there is no single “best” model for all purposes—a model optimized for prediction may be entirely unsuitable for causal interpretation, and vice versa.\nThe chapter also engaged with the critical role of explainability. In high-stakes domains, understanding why a model makes its predictions is as important as the predictions themselves. We surveyed the modern toolkit of interpretability methods—SHAP, LIME, PDPs, and others—while cautioning against an unrealistic demand for complete mechanistic understanding before deployment. History teaches us that useful tools often precede full explanation.\nUltimately, model selection is an exercise in informed judgment!\n\n\n\n\nApley, Daniel W., and Jingyu Zhu. 2020. “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4): 1059–86.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. “What Does BERT Look at? An Analysis of BERT’s Attention.” In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 276–86. Association for Computational Linguistics.\n\n\nCraven, Mark, and Jude W. Shavlik. 1996. “Extracting Tree-Structured Representations of Trained Networks.” In Advances in Neural Information Processing Systems, 8:24–30. MIT Press.\n\n\nFawcett, Tom. 2006. “An Introduction to ROC Analysis.” Pattern Recognition Letters 27 (8): 861–74.\n\n\nFefferman, Charles L. 2006. “Existence and Smoothness of the Navier–Stokes Equation.” The Millennium Prize Problems, 57–67.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics, 1189–1232.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Boca Raton: Chapman and Hall/CRC.\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” Journal of the American Statistical Association 102 (477): 359–78.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” Journal of Computational and Graphical Statistics 24 (1): 44–65.\n\n\nImmer, Alexander, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and Khan Mohammad Emtiyaz. 2021. “Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning.” In International Conference on Machine Learning, 4563–73. PMLR.\n\n\nJiménez-Luna, José, Francesca Grisoni, Nils Weskamp, and Gisbert Schneider. 2020. “DrugEx V2: De Novo Design of Drug Molecule by Pareto-based Multi-Objective Reinforcement Learning in Polypharmacology.” Journal of Cheminformatics 12 (1): 1–12.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89.\n\n\nLindley, D. V. 1961. “The Use of Prior Probability Distributions in Statistical Inference and Decisions.” In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 4.1:453–69. University of California Press.\n\n\nLundberg, Scott M, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4765–74. Curran Associates, Inc.\n\n\nMacKay, David JC. 1992. “Bayesian Interpolation.” Neural Computation 4 (3): 415–47.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44. ACM.\n\n\nRitter, Hippolyt, Aleksandar Botev, and David Barber. 2018. “A Scalable Laplace Approximation For Neural Networks.”\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” In Proceedings of the IEEE International Conference on Computer Vision, 618–26. IEEE.\n\n\nShapiro, Sam. 1988. “Selection, Follow-up, and Analysis in the Health Insurance Plan Study: A Randomized Trial with Breast Cancer Screening.” Journal of the National Cancer Institute 80 (14): 1125–32.\n\n\nSimonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. 2013. “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.” arXiv Preprint arXiv:1312.6034. https://arxiv.org/abs/1312.6034.\n\n\nSingh, Pratyush Kumar, Kathryn A. Farrell-Maupin, and Danial Faghihi. 2024. “A Framework for Strategic Discovery of Credible Neural Network Surrogate Models Under Uncertainty.” arXiv. https://arxiv.org/abs/2403.08901.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic Attribution for Deep Networks.” In Proceedings of the 34th International Conference on Machine Learning, 3319–28. PMLR.\n\n\nTabar, Laszlo, CJ Gad Fagerberg, Anders Gad, Lennart Baldetorp, Lars H Holmberg, Ove Gröntoft, Ulf Ljungquist, et al. 1985. “Reduction in Mortality from Breast Cancer After Mass Screening with Mammography: Randomised Trial from the Breast Cancer Screening Working Group of the Swedish National Board of Health and Welfare.” The Lancet 325 (8433): 829–32.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30: 5998–6008.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” Harvard Journal of Law & Technology 31: 841–87.\n\n\nWatanabe, Sumio. 2013. “A Widely Applicable Bayesian Information Criterion.” The Journal of Machine Learning Research 14 (1): 867–97.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html",
    "href": "17-theoryai.html",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "",
    "text": "17.1 Normal Means Problem\nWhen AlphaGo defeated world champion Lee Sedol in 2016, its neural networks contained millions of parameters—far more than the number of training positions it had observed. How did it avoid simply memorizing the training data? The answer lies in regularization: the systematic imposition of constraints that prevent models from over-fitting to noise. This chapter reveals that regularization is not merely a computational trick but emerges naturally from Bayesian reasoning about uncertainty.\nThe development of learning algorithms has been driven by two fundamental paradigms: the classical frequentist approach centered around maximum likelihood estimation (MLE) and the Bayesian approach grounded in decision theory. This chapter explores how these seemingly distinct methodologies converge in modern AI theory, particularly through the lens of regularization and model selection.\nMaximum likelihood estimation represents the cornerstone of classical statistical inference. Given observed data \\(\\mathcal{D} = (x_i, y_i)_{i=1}^n\\) and a parametric model \\(f_{\\theta}(x)\\), the MLE principle seeks to find the parameter values that maximize the likelihood function: \\[\n\\hat{\\theta}_{\\mathrm{MLE}} = \\arg\\max_{\\theta} L(\\theta; \\mathcal{D}) = \\arg\\max_{\\theta} \\prod_{i=1}^n p(y_i \\mid x_i, \\theta)\n\\]\nThis approach has several appealing properties: it provides consistent estimators under mild conditions, achieves the Cramer-Rao lower bound asymptotically, and offers a principled framework for parameter estimation. These classical guarantees are typically stated under i.i.d. sampling assumptions; from a Bayesian perspective, exchangeability (Chapter 3) is a weaker condition that still supports learning from data in many settings. The Cramer-Rao bound is expressed in terms of Fisher information, and in exponential-family models Fisher information and sufficiency are closely related ways of formalizing how much information the data carry about a parameter.\nHowever, MLE has well-documented limitations, particularly in high-dimensional settings. MLE can lead to overfitting, poor generalization, and numerical instability. Furthermore, as shown by Stein’s paradox, MLE can be inadmissible, meaning there are other estimators that have lower risk than the MLE. We will start this chapter with the normal means problem and demonstrate how MLE can be inadmissible.\nConsider the vector of means case where \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). We have \\[\n\\bar y_i \\mid \\theta_i \\sim N(\\theta_i, \\sigma^2/n_i), \\quad i=1,\\ldots,p, \\quad p &gt; 2\n\\tag{17.1}\\] Here \\(\\bar y_i\\) is the mean of \\(n_i\\) observations, i.e., \\(\\bar y_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} y_{ij}\\).\nThe goal is to estimate the vector of means \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\), and we can achieve this by borrowing strength across the observations.\nThis is also a proxy for non-parametric regression, where \\(y_i = f(x_i) + \\varepsilon_i\\) and \\(\\theta_i = f(x_i)\\). Much has been written on the properties of the Bayes risk as a function of \\(n\\) and \\(p\\), and extensive work has been done on the asymptotic properties of the Bayes risk as \\(n\\) and \\(p\\) grow to infinity.\nThe classical inference is based on the CLT \\[\n\\hat \\theta_i \\mid \\theta_i \\sim N(\\theta_i, \\sigma^2/n_i),\n\\] and the MLE estimate is given by \\(\\hat \\theta_i = \\bar y_i\\). The MLE estimate is consistent and asymptotically normal, i.e., \\[\n\\hat \\theta_i \\to N(\\theta_i, \\sigma^2/n_i) \\quad \\text{as} \\quad n_i \\to \\infty.\n\\]\nOn the other hand, the Bayes estimator is based on \\(\\theta_i \\mid \\hat \\theta_i\\), where \\(\\hat \\theta_i = \\bar y_i\\). In other words, the classical approach is subject to the prosecutor’s fallacy—the logical error of confusing \\(P(\\text{data} \\mid \\text{hypothesis})\\) with \\(P(\\text{hypothesis} \\mid \\text{data})\\). Classical estimators are unbiased, whereas a Bayes estimator is biased.\nThe goal is to estimate the vector \\(\\theta\\) using squared loss: \\[\n\\mathcal{L}(\\theta, \\hat{\\theta}) = \\sum_{i=1}^p (\\theta_i - \\hat{\\theta}_i)^2,\n\\] where \\(\\hat{\\theta}\\) is the vector of estimates. We will compare the MLE estimate with the James-Stein estimate. A principled way to evaluate the performance of an estimator is to average its loss over the data; this metric is called the risk. The MLE estimate \\(\\hat{\\theta}_i = y_i\\) has a constant risk \\(p\\sigma^2\\): \\[\nR(\\theta,\\hat{\\theta}) = \\E[y]{\\mathcal{L}(\\theta, \\hat{\\theta})} = \\sum_{i=1}^p \\E[y_i]{(y_i - \\theta_i )^2}.\n\\]\nHere the expectation is over the data given by distribution Equation 17.1 and \\(y_i \\sim N(\\theta_i, \\sigma^2)\\), we have \\(\\E[y_i]{(\\theta_i - y_i)^2} = \\Var{y_i} = \\sigma^2\\) for each \\(i\\). Therefore: \\[\nR(\\theta,\\hat{\\theta}) = \\sum_{i=1}^p \\sigma^2 = p\\sigma^2.\n\\]\nThis shows that the MLE risk is constant and does not depend on the true parameter values \\(\\theta\\), only on the dimension \\(p\\) and the noise variance \\(\\sigma^2\\).\nGiven that MLE provides a natural baseline estimator with known risk properties, one might ask: can we do better? The Bayesian paradigm offers a fundamentally different perspective that often yields estimators with uniformly lower risk.\nBayesian inference offers a fundamentally different perspective by incorporating prior knowledge and quantifying uncertainty through probability distributions. The Bayesian approach begins with a prior distribution \\(p(\\theta)\\) over the parameter space and updates this belief using Bayes’ rule: \\[\np(\\theta | y) = \\frac{p(y | \\theta) p(\\theta)}{p(y)}\n\\]\nThe Bayes estimator is the value \\(\\hat{\\theta}^{B}\\) that minimizes the Bayes risk, the expected loss: \\[\n\\hat{\\theta}^{B} = \\arg\\min_{\\hat{\\theta}(y)} R(\\pi, \\hat{\\theta}(y))\n\\] Here \\(\\pi\\) is the prior distribution of \\(\\theta\\) and \\(R(\\pi, \\hat{\\theta}(y))\\) is the Bayes risk defined as: \\[\nR(\\pi, \\hat{\\theta}(y)) = \\mathbb{E}_{\\theta \\sim \\pi} \\left[ \\mathbb{E}_{y\\mid \\theta} \\left[ \\mathcal{L}(\\theta, \\hat{\\theta}(y)) \\right] \\right].\n\\tag{17.2}\\] For squared error loss, this yields the posterior mean \\(\\E{\\theta \\mid y}\\), while for absolute error loss, it gives the posterior median.\nFor the normal means problem with squared error loss, this becomes: \\[\nR(\\pi, \\hat{\\theta}(y)) = \\int_{\\theta \\in \\Theta} \\left( \\int_{y \\in \\mathcal{Y}} (\\theta - \\hat{\\theta}(y))^2 p(y|\\theta) dy \\right) \\pi(\\theta) d\\theta\n\\]\nThe Bayes risk quantifies the expected performance of an estimator, taking into account both the uncertainty in the data and the prior uncertainty about the parameter. It serves as a benchmark for comparing different estimators: an estimator with lower Bayes risk is preferred under the chosen prior and loss function. In particular, the Bayes estimator achieves the minimum possible Bayes risk for the given prior and loss.\nIn 1961, Charles Stein and Willard James proved a startling result: they constructed an estimator for the mean of a multivariate normal distribution that uniformly dominates the sample mean under squared error loss—a finding that challenged what seemed like an elementary law of statistical theory. This result, now known as Stein’s paradox, demonstrates that for dimensions \\(p \\geq 3\\), there always exists an estimator with strictly lower risk than the MLE for all parameter values.\nThe empirical evidence is striking: in applications ranging from baseball batting averages to toxoplasmosis prevalence rates to Pearson’s chi-square tests, the James-Stein estimator achieves mean squared errors less than half that of the sample mean. This result is paradoxical because it overturns the intuition that unbiased estimators should be optimal—despite introducing bias, the James-Stein estimator achieves strictly lower risk for all parameter values when \\(p \\geq 3\\). The philosophical implications extend beyond estimation theory: by demonstrating that individual estimates can be improved by considering them jointly, Stein’s paradox provides deep connections to Bayesian thinking and the empirical Bayes framework.\nStein’s phenomenon where \\(y_i \\mid \\theta_i \\sim N(\\theta_i, 1)\\) and \\(\\theta_i \\sim N(0, \\tau^2)\\) where \\(\\tau \\rightarrow \\infty\\) illustrates this point well. The MLE approach is equivalent to the use of the improper “non-informative” uniform prior and leads to an estimator with poor risk properties.\nLet \\(\\|y\\|^2 = \\sum_{i=1}^p y_i^2\\) denote the squared Euclidean norm of \\(y\\). Then, we can make the following probabilistic statements from the model: \\[\nP\\left( \\| y \\| &gt; \\| \\theta \\| \\right) &gt; \\frac{1}{2}\n\\] Now for the posterior, this inequality is reversed under a flat Lebesgue measure: \\[\nP\\left( \\| \\theta \\| &gt; \\| y \\| \\mid y \\right) &gt; \\frac{1}{2}\n\\] which is in conflict with the classical statement. This is a property of the prior which leads to a poor rule (the overall average) and risk.\nThe shrinkage rule (a.k.a. normal prior) where \\(\\tau^2\\) is “estimated” from the data avoids this conflict. More precisely, we have: \\[\n\\hat{\\theta}(y) = \\left( 1 - \\frac{k-2}{\\|y\\|^2} \\right) y \\quad \\text{and} \\quad \\E{\\| \\hat{\\theta} - \\theta \\|^2 } &lt; k, \\; \\forall \\theta.\n\\] Hence, when \\(\\|y\\|^2\\) is small, the shrinkage factor is more extreme. For example, if \\(k=10\\), \\(\\|y\\|^2=12\\), then \\(\\hat{\\theta} = (1/3) y\\). Now we have the more intuitive result that: \\[\nP\\left(\\|\\theta\\| &gt; \\|y\\| \\mid y\\right) &lt; \\frac{1}{2}.\n\\]\nThis shows that careful specification of default priors matters in high dimensions.\nThe resulting estimator is called the James-Stein estimator and is a shrinkage estimator that shrinks the MLE towards the prior mean. The prior mean is typically the sample mean of the data. The James-Stein estimator is given by: \\[\n\\hat{\\theta}_{i}^{JS} = (1 - \\lambda) \\hat{\\theta}_{i}^{MLE} + \\lambda \\bar{y},\n\\] where \\(\\lambda\\) is a shrinkage parameter and \\(\\bar{y}\\) is the sample mean of the data. The shrinkage parameter is typically chosen to minimize the risk of the estimator.\nThe key idea behind James-Stein shrinkage is that one can “borrow strength” across components. In this sense, the multivariate parameter estimation problem is easier than the univariate one.\nFollowing Efron and Morris (1975), we can view the James-Stein estimator through the lens of empirical Bayes methodology. Efron and Morris demonstrate that Stein’s seemingly paradoxical result has a natural interpretation when viewed as an empirical Bayes procedure that estimates the prior distribution from the data itself.\nConsider the hierarchical model: \\[\n\\begin{aligned}\ny_i \\mid \\theta_i &\\sim N(\\theta_i, \\sigma^2) \\\\\n\\theta_i \\mid \\mu, \\tau^2 &\\sim N(\\mu, \\tau^2)\n\\end{aligned}\n\\]\nThe marginal distribution of \\(y_i\\) is then \\(y_i \\sim N(\\mu, \\sigma^2 + \\tau^2)\\). In the empirical Bayes approach, we estimate the hyperparameters \\(\\mu\\) and \\(\\tau^2\\) from the marginal likelihood:\n\\[\nm(y \\mid \\mu, \\tau^2) = \\prod_{i=1}^p \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau^2)}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2(\\sigma^2 + \\tau^2)}\\right)\n\\]\nThe maximum marginal likelihood estimators are: \\[\n\\hat{\\mu} = \\bar{y} = \\frac{1}{p}\\sum_{i=1}^p y_i\n\\] \\[\n\\hat{\\tau}^2 = \\max\\left(0, \\frac{1}{p}\\sum_{i=1}^p (y_i - \\bar{y})^2 - \\sigma^2\\right)\n\\]\nThe empirical Bayes estimator then becomes: \\[\n\\hat{\\theta}_i^{EB} = \\frac{\\hat{\\tau}^2}{\\sigma^2 + \\hat{\\tau}^2} y_i + \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2} \\hat{\\mu}\n\\]\nThis can be rewritten as: \\[\n\\hat{\\theta}_i^{EB} = \\left(1 - \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2}\\right) y_i + \\frac{\\sigma^2}{\\sigma^2 + \\hat{\\tau}^2} \\bar{y}\n\\]\nWhen \\(\\mu = 0\\) and using the estimate \\(\\hat{\\tau}^2 = \\max(0, \\|y\\|^2/p - \\sigma^2)\\), this reduces to a form closely related to the James-Stein estimator: \\[\n\\hat{\\theta}_i^{JS} = \\left(1 - \\frac{(p-2)\\sigma^2}{\\|y\\|^2}\\right) y_i\n\\]\nEfron and Morris show that the empirical Bayes interpretation provides insight into why the James-Stein estimator dominates the MLE. The key insight is that the MLE implicitly assumes an improper flat prior \\(\\pi(\\theta) \\propto 1\\), which leads to poor risk properties in high dimensions.\nThe Bayes risk of the James-Stein estimator can be explicitly calculated due to the conjugacy of the normal prior and likelihood: \\[\nR(\\theta, \\hat{\\theta}^{JS}) = p\\sigma^2 - (p-2)\\sigma^2 \\mathbb{E}\\left[\\frac{1}{\\|\\theta + \\epsilon\\|^2/\\sigma^2}\\right]\n\\]\nwhere \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). Since the second term is always positive, we have: \\[\nR(\\theta, \\hat{\\theta}^{JS}) &lt; R(\\theta, \\hat{\\theta}^{MLE}) \\quad \\forall \\theta \\in \\mathbb{R}^p, \\quad p \\geq 3\n\\]\nThis uniform domination demonstrates the inadmissibility of the MLE under squared error loss for \\(p \\geq 3\\).\nIn an applied problem, the gap in risk between MLE and JS estimators can be large. For example, in the normal means problem with \\(p=100\\) and \\(n=100\\), the risk of the MLE is \\(R(\\theta,\\hat{\\theta}_{MLE}) = 100\\) while the risk of the JS estimator is \\(R(\\theta,\\hat{\\theta}_{JS}) = 1.5\\). The JS estimator is 67 times more efficient than the MLE. The JS estimator is also minimax optimal in the sense that it attains the minimax risk bound for the normal means problem. The minimax risk bound is the smallest risk that can be attained by any estimator.\nThe James-Stein estimator illustrates how incorporating prior information (via shrinkage) can lead to estimators with lower overall risk compared to the MLE, especially in high-dimensional settings. However, it is not the only shrinkage estimator that dominates the MLE. Other shrinkage estimators, such as the ridge regression estimator, also have lower risk than the MLE. The key insight is that shrinkage estimators can leverage prior information to improve estimation accuracy, especially in high-dimensional settings.\nNote, that we used the empirical Bayes version of the definition of risk. Full Bayes approach incorporates both the data and the prior distribution of the parameter as in Equation 17.2.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#normal-means-problem",
    "href": "17-theoryai.html#normal-means-problem",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "",
    "text": "Example 17.1 (Screening Corporate Performance) To illustrate the practical importance of the normal means problem, consider the challenge faced by investment analysts screening thousands of publicly traded companies for sustained superior performance (Polson and Scott 2012). Suppose we have return on assets (ROA) data for \\(p = 53,038\\) firms across 93 countries over 45 years. For each firm \\(i\\), we observe an average ROA performance \\(\\bar{y}_i\\) over some time period.\nThe fundamental question is: Which firms have genuinely superior performance versus those that appear successful due to luck? This is precisely a massive multiple testing problem in the normal means framework. Using MLE, we would estimate each firm’s performance as \\(\\hat{\\theta}_i = \\bar{y}_i\\) and declare any firm with \\(\\bar{y}_i &gt; 0\\) as superior. However, this approach ignores the massive multiplicity problem—with over 50,000 firms, many will appear successful purely by chance.\nThe Bayesian approach with appropriate shrinkage priors can distinguish between truly superior firms and those that are merely lucky. As we’ll see, this requires heavy-tailed priors that can accommodate the rare firms with genuine outperformance while shrinking the estimates of mediocre firms toward zero. This example demonstrates why the choice between MLE and Bayesian estimation has profound practical consequences in high-dimensional settings.\nIn practice, the normal means problem often requires careful preprocessing of the observed data. A common and crucial step is to normalize the observations by converting them to z-scores. This standardization serves multiple purposes and connects directly to the theoretical framework we’ve established.\nConsider our corporate performance example where we observe ROA values \\(\\bar{y}_i\\) for different firms. These firms may operate in different industries, countries, or time periods, making direct comparison problematic. Raw ROA values might range from -20% to +30%, with different scales and baseline expectations across sectors.\nThe z-score transformation standardizes each observation relative to a reference distribution: \\[\nz_i = \\frac{\\bar{y}_i - \\mu_i}{\\sigma_i}.\n\\] where \\(\\mu_i\\) and \\(\\sigma_i\\) represent the mean and standard deviation estimated and predicted by an autoregressive model.\nIn the Polson & Scott study (Polson and Scott 2012), standardization was essential for handling the massive heterogeneity across their dataset of 53,038 firms spanning 93 countries and 45 years. The authors faced three critical challenges that z-score normalization helped address:\n1. Cross-Country Comparisons: Raw ROA values varied dramatically across countries due to different accounting standards, economic conditions, and regulatory environments. A 5% ROA in Japan during the 1990s had very different implications than 5% ROA in Brazil during the same period.\n2. Temporal Adjustments: Economic cycles, inflation rates, and market conditions changed substantially over the 45-year study period. The authors needed to adjust for these time-varying factors to identify firms with genuinely superior performance rather than those that simply operated during favorable periods.\n3. Industry Heterogeneity: Different industries have fundamentally different ROA distributions. Technology firms typically show higher volatility and different baseline performance compared to utilities or manufacturing companies.\nThe authors implemented a sophisticated normalization procedure: \\[\nz_{i,t} = \\frac{\\text{ROA}_{i,t} - \\mu_{\\text{peer}(i),t}}{\\sigma_{\\text{peer}(i),t}}\n\\]\nwhere \\(\\text{peer}(i)\\) represents firm \\(i\\)’s reference group (defined by industry, country, and size), and the subscript \\(t\\) indicates time-varying adjustments. This created standardized performance measures where:\n\n\\(z_{i,t} = 0\\) indicates performance exactly at the peer group median\n\\(z_{i,t} = 1\\) indicates performance one standard deviation above peers\n\n\\(z_{i,t} = -1\\) indicates performance one standard deviation below peers\n\nAfter this rigorous standardization, the authors discovered a critical finding: sustained superior performance (\\(\\theta_i &gt; 0\\) consistently over time) was remarkably rare. Most firms showing high raw ROA were simply benefiting from favorable conditions rather than demonstrating genuine operational excellence. This finding emerged only after proper normalization—without standardization, hundreds of firms would have been incorrectly classified as superior performers.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#sec-unbiasedness-optimality",
    "href": "17-theoryai.html#sec-unbiasedness-optimality",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.2 Unbiasedness and the Optimality of Bayes Rules",
    "text": "17.2 Unbiasedness and the Optimality of Bayes Rules\nThe optimal Bayes rule \\(\\delta_\\pi(x) = \\E{\\theta \\mid y}\\) is the posterior mean under squared error loss. An interesting feature of the Bayes rule is that it is biased except in degenerate cases like improper priors, which can lose optimality properties. This can be seen using the following decomposition.\nIf the rule was unbiased, then the Bayes risk would be zero. This follows by contradiction: assume for the sake of argument that \\(\\E[x\\mid\\theta]{\\delta_{\\pi}(y)} = \\theta\\), then \\[\nr(\\pi, \\delta_{\\pi}(y)) = \\E[\\pi]{\\E[y\\mid\\theta]{(\\delta_{\\pi}(y) - \\theta)^2}} = \\E[\\pi]{\\theta^2} + \\E[y]{\\delta_{\\pi}(y)}^2 - 2\\E[\\pi]{\\theta \\E[y\\mid\\theta]{\\delta_{\\pi}(y)}} = 0\n\\] which is a contradiction since the Bayes risk cannot be zero in non-degenerate cases.\nThe key feature of Bayes rules is the bias-variance tradeoff inherent in their nature. You achieve a large reduction in variance for a small amount of bias. This is the underpinning of James-Stein estimation.\nAnother interesting feature is that the Bayes rule \\(\\E{\\theta \\mid y}\\) is always Bayes sufficient in the sense that \\[\n\\E[\\pi]{\\theta \\mid \\E[\\pi]{\\theta \\mid y}} = \\E[\\pi]{\\theta \\mid y}\n\\] So conditioning on \\(\\E[\\pi]{\\theta \\mid y}\\) is equivalent to conditioning on \\(y\\) when estimating \\(\\theta\\). This property is used in quantile neural network approaches to generative methods.\n\nExample 17.2 (Example: James-Stein for Baseball Batting Averages) We reproduce the baseball batting average example from Efron and Morris (1977). Data below has the number of hits for 18 baseball players after 45 at-bats in 1970 season.\n\n# Data source: https://www1.swarthmore.edu/NatSci/peverso1/Sports%20Data/JamesSteinData/Efron-Morris%20Baseball/EfronMorrisBB.txt\nbaseball = read.csv(\"../data/EfronMorrisBB.txt\", sep = \"\\t\", stringsAsFactors = FALSE) %&gt;% dplyr::select(LastName,AtBats,BattingAverage,SeasonAverage)\n\nNow, we can estimate overall mean and variance\n\nmu_hat &lt;- mean(baseball$BattingAverage)\nsigma2_hat &lt;- var(baseball$BattingAverage)\n\nAs well as the posterior mean for each player (James-Stein estimator)\n\nbaseball &lt;- baseball %&gt;%\n  mutate(\n    JS = (sigma2_hat / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * mu_hat +\n      ((BattingAverage * (1 - BattingAverage) / AtBats) / (sigma2_hat + (BattingAverage * (1 - BattingAverage) / AtBats))) * BattingAverage\n  )\nkable(baseball)\n\n\n\n\nLastName\nAtBats\nBattingAverage\nSeasonAverage\nJS\n\n\n\n\nClemente\n45\n0.40\n0.35\n0.34\n\n\nRobinson\n45\n0.38\n0.31\n0.32\n\n\nHoward\n45\n0.36\n0.28\n0.31\n\n\nJohnstone\n45\n0.33\n0.24\n0.30\n\n\nBerry\n45\n0.31\n0.28\n0.29\n\n\nSpencer\n45\n0.31\n0.27\n0.29\n\n\nKessinger\n45\n0.29\n0.27\n0.28\n\n\nAlvarado\n45\n0.27\n0.22\n0.27\n\n\nSanto\n45\n0.24\n0.27\n0.26\n\n\nSwaboda\n45\n0.24\n0.23\n0.26\n\n\nPetrocelli\n45\n0.22\n0.26\n0.25\n\n\nRodriguez\n45\n0.22\n0.22\n0.25\n\n\nScott\n45\n0.22\n0.30\n0.25\n\n\nUnser\n45\n0.22\n0.26\n0.25\n\n\nWilliams\n45\n0.22\n0.25\n0.25\n\n\nCampaneris\n45\n0.20\n0.28\n0.24\n\n\nMunson\n45\n0.18\n0.30\n0.23\n\n\nAlvis\n45\n0.16\n0.18\n0.22\n\n\n\n\n\nPlot below shows the observed averages vs. James-Stein estimate\n\n\nCode\nggplot(baseball, aes(x = BattingAverage, y = JS)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    x = \"Observed Batting Average\",\n    y = \"James-Stein Estimate\",\n    title = \"Empirical Bayes Shrinkage of Batting Averages\"\n  )\n\n\n\n\n\n\n\n\n\nCalculate mean squared error (MSE) for observed and James-Stein estimates\n\nmse_observed &lt;- mean((baseball$BattingAverage - mu_hat)^2)\nmse_js &lt;- mean((baseball$JS - mu_hat)^2)\n\ncat(sprintf(\"MSE (Observed): %.6f\\n\", mse_observed))\n## MSE (Observed): 0.004584\ncat(sprintf(\"MSE (James-Stein): %.6f\\n\", mse_js))\n## MSE (James-Stein): 0.001031\n\nWe can see that the James-Stein estimator has a lower MSE than the observed batting averages. This is a demonstration of Stein’s paradox, where the James-Stein estimator, which shrinks the estimates towards the overall mean, performs better than the naive sample mean estimator.\n\n\nCode\na = matrix(rep(1:3, nrow(baseball)), 3, nrow(baseball))\nb = matrix(c(baseball$BattingAverage, baseball$SeasonAverage, baseball$JS),    3, nrow(baseball), byrow=TRUE)\n\nmatplot(a, b, pch=\" \", ylab=\"predicted average\", xaxt=\"n\", xlim=c(0.5, 3.1), ylim=c(0.13, 0.42))\nmatlines(a, b)\ntext(rep(0.7, nrow(baseball)), baseball$BattingAverage, baseball$LastName, cex=0.6)\ntext(1, 0.14, \"First 45\\nat bats\", cex=0.5)\ntext(2, 0.14, \"Average\\nof remainder\", cex=0.5)\ntext(3, 0.14, \"J-S\\nestimator\", cex=0.5)\n\n\n\n\n\n\n\n\n\nNow if we look at the season dynamics for Clemente\n\n\nCode\n# Data source: https://www.baseball-almanac.com/players/hittinglogs.php?p=clemero01&y=1970\ncl = read.csv(\"../data/clemente.csv\")\nx = cumsum(cl$AB)\ny = cumsum(cl$H)/cumsum(cl$AB)\n# Plot x,y starting from index 2\nind = c(1,2)\nplot(x[-ind],y[-ind], type='o', ylab=\"Batting Average\", xlab=\"Number at Bats\", pch=21, bg=\"lightblue\", cex=0.8, lwd=2)\n# Add horizontal line for season average 145/412 and add text above line `Season Average`\ntext(200, 145/412 + 0.005, \"Season Average\", col = \"red\")\nabline(h = 145/412, col = \"red\", lty = 2)\n# Ted williams record is .406 in 1941, so you know the first data points are noise\ntext(200, baseball$JS[1] + 0.005, \"JS\", col = \"red\")\nabline(h = baseball$JS[1], col = \"red\", lty = 2)\ntext(200, baseball$BattingAverage[1] + 0.005, \"After 45 At Bats\", col = \"red\")\nabline(h = baseball$BattingAverage[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nFull Bayes Shrinkage\nThe alternative approach to the regularization is to use full Bayes, which places a prior distribution on the parameters and computes the full posterior distribution using the Bayes rule: \\[\np( \\theta \\mid  \\tau, y ) = \\frac{ f( y | \\theta ) p( \\theta \\mid \\tau ) }{ m(y \\mid \\tau) },\n\\] here \\[\nm(y \\mid \\tau) = \\int f( y\\mid  \\theta ) p( \\theta \\mid \\tau ) d \\theta,\n\\] Here \\(m(y \\mid \\tau)\\) is the marginal beliefs about the data.\nThe empirical Bayes approach is to estimate the prior distribution \\(p( \\theta \\mid \\tau )\\) from the data. This can be done by maximizing the marginal likelihood \\(m(y \\mid \\tau )\\) with respect to \\(\\tau\\). The resulting estimator is called the type II maximum likelihood estimator (MMLE). \\[\n\\hat{\\tau} = \\arg \\max_{\\tau} \\log m( y \\mid \\tau ).\n\\]\nFor example, in the normal-normal model, when \\(\\theta \\sim N(\\mu,\\tau^2)\\) with \\(\\mu=0\\), we can integrate out the high dimensional \\(\\theta\\) and find \\(m(y | \\tau)\\) in closed form as \\(y_i \\sim N(0, \\sigma^2 + \\tau^2)\\) \\[\nm( y | \\tau ) = ( 2 \\pi)^{-n/2} ( \\sigma^2 + \\tau^2 )^{- n/2}  \\exp \\left ( - \\frac{ \\sum y_i^2 }{ 2 ( \\sigma^2 + \\tau^2) } \\right )\n\\] The original JS estimator shrinks to zero and estimates prior variance using empirical Bayes (marginal MLE or Type II MLE). Efron and Morris and Lindley showed that you want to shrink to overall mean \\(\\bar y\\) and in this approach \\[\n\\theta \\sim N(\\mu,\\tau^2).\n\\] The original JS is \\(\\mu=0\\). To estimate the \\(\\mu\\) and \\(\\tau\\) you can do full Bayes or empirical Bayes that shrinks to overall grand mean \\(\\bar y\\), which serves as the estimate of the original prior mean \\(\\mu\\). It seems paradoxical that you estimate prior parameters from the data. However, this is not the case. You simply use mixture prior Diaconis and Ylvisaker (1983) with marginal MLE (MMLE). The MMLE is the product \\[\n\\int_{\\theta_i}\\prod_{i=1}^k p(\\bar y_i \\mid \\theta_i)p(\\theta_i \\mid \\mu, \\tau^2).\n\\]\nThe motivation for the shrinkage prior rather than a flat uniform prior are the following probabilistic arguments. They have an ability to balance signal detection and noise suppression in high-dimensional settings. Unlike flat uniform priors, shrinkage priors adaptively shrink small signals towards zero while preserving large signals. This behavior is crucial for sparse estimation problems, where most parameters are expected to be zero or near-zero. The James-Stein procedure is an example of global shrinkage, when the overall sparsity level across all parameters is controlled, ensuring that the majority of parameters are shrunk towards zero. Later in this section we will discuss local shrinkage priors, such as the horseshoe prior, which allow individual parameters to escape shrinkage if they represent significant signals.\nIn summary, flat uniform priors (MLE) fail to provide adequate regularization in high-dimensional settings, leading to poor risk properties and overfitting. By incorporating probabilistic arguments and hierarchical structures, shrinkage priors offer a principled approach to regularization that aligns with Bayesian decision theory and modern statistical practice.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#bias-variance-decomposition",
    "href": "17-theoryai.html#bias-variance-decomposition",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.3 Bias-Variance Decomposition",
    "text": "17.3 Bias-Variance Decomposition\nThe discussion of shrinkage priors and James-Stein estimation naturally leads us to a fundamental concept in statistical learning: the bias-variance decomposition. This decomposition provides the theoretical foundation for understanding why shrinkage methods like James-Stein can outperform maximum likelihood estimation, even when they introduce bias.\nThe key insight is that estimation error can be decomposed into two components: bias (systematic error) and variance (random error). While unbiased estimators like maximum likelihood have zero bias, they often suffer from high variance, especially in high-dimensional settings. Shrinkage methods intentionally introduce a small amount of bias to achieve substantial reductions in variance, leading to better overall performance.\nThis trade-off between bias and variance is not just a theoretical curiosity—it’s the driving force behind many successful machine learning algorithms, from ridge regression to neural networks with dropout. Understanding this decomposition helps us make principled decisions about model complexity and regularization.\nFor parameter estimation, we can decompose the mean squared error as follows: \\[\n\\begin{aligned}\n\\E{(\\hat{\\theta} - \\theta)^2} &= \\E{(\\hat{\\theta} - \\E{\\hat{\\theta}} + \\E{\\hat{\\theta}} - \\theta)^2} \\\\\n&= \\E{(\\hat{\\theta} - \\E{\\hat{\\theta}})^2} + \\E{(\\E{\\hat{\\theta}} - \\theta)^2} \\\\\n&= \\text{Var}(\\hat{\\theta}) + \\text{Bias}(\\hat{\\theta})^2\n\\end{aligned}\n\\] The cross term has expectation zero.\nFor prediction problems, we have \\(y = \\theta + \\epsilon\\) where \\(\\epsilon\\) is independent with \\(\\text{Var}(\\epsilon) = \\sigma^2\\). Hence \\[\n\\E{(y - \\hat{y})^2} = \\sigma^2 + \\E{(\\hat{\\theta} - \\theta)^2}\n\\]\nThis decomposition shows that prediction error consists of irreducible noise, estimation variance, and estimation bias. Bayesian methods typically trade a small increase in bias for a substantial reduction in variance, leading to better overall performance.\nThe bias-variance decomposition provides a framework for understanding estimation error, but it doesn’t tell us how to construct optimal estimators. To find the best decision rule—whether for estimation, hypothesis testing, or model selection—we need a more general framework that can handle different types of loss functions and prior information. This leads us to Bayesian decision theory and the concept of risk decomposition.\n\nRisk Decomposition\nHow does one find an optimal decision rule? It could be a test region, an estimation procedure or the selection of a model. Bayesian decision theory addresses this issue.\nThe a posteriori Bayes risk approach is as follows. Let \\(\\delta(x)\\) denote the decision rule. Given the prior \\(\\pi(\\theta)\\), we can simply calculate \\[\nR_n  ( \\pi , \\delta ) = \\int_x m( x )  \\left \\{ \\int_\\Theta \\mathcal{L}( \\theta , \\delta(x) )  p( \\theta | x ) d \\theta \\right \\} d x .\n\\] Then the optimal Bayes rule is to pointwise minimize the inner integral (a.k.a. the posterior Bayes risk), namely \\[\n\\delta^\\star ( x ) = \\arg \\max_\\delta \\int_\\Theta \\mathcal{L}( \\theta , \\delta(x) )  p( \\theta | x ) d \\theta .\n\\] The caveat is that this gives us no intuition into the characteristics of the prior which are important. Moreover, we do not need the marginal beliefs \\(m(x)\\).\nFor example, under squared error estimation loss, the optimal estimator is simply the posterior mean, \\(\\delta^\\star (x)  = E( \\theta | x )\\). The properties of this optimal rule—including its inherent bias and the bias-variance tradeoff it embodies—were established in Section 17.2.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#sparsity",
    "href": "17-theoryai.html#sparsity",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.4 Sparsity",
    "text": "17.4 Sparsity\nLet the true parameter be sparse with the form \\(\\theta_p = \\left( \\sqrt{d/p}, \\ldots, \\sqrt{d/p}, 0, \\ldots, 0 \\right)\\). The problem of recovering a vector with many zero entries is called sparse signal recovery. The “ultra-sparse” or “nearly black” vector case occurs when \\(p_n\\), denoting the number of non-zero parameter values, satisfies \\(\\theta \\in l_0[p_n]\\), which denotes the set \\(\\#(\\theta_i \\neq 0) \\leq p_n\\) where \\(p_n = o(n)\\) and \\(p_n \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nHigh-dimensional predictor selection and sparse signal recovery are routine statistical and machine learning tasks and present a challenge for classical statistical methods. Historically, James-Stein estimation (or \\(\\ell_2\\)-regularization) functions as a global shrinkage rule. Because it lacks local parameters to adapt to sparsity, it struggles to recover sparse signals effectively.\nJames-Stein is equivalent to the model: \\[\ny_i = \\theta_i + \\epsilon_i \\quad \\text{and} \\quad \\theta_i \\sim \\mathcal{N}(0, \\tau^2)\n\\] For the sparse \\(r\\)-spike problem, \\(\\hat{\\theta}_{JS}\\) performs poorly and we require a different rule. rather than using softmax For \\(\\theta_p\\) we have: \\[\n\\frac{p \\|\\theta\\|^2}{p + \\|\\theta\\|^2} \\leq R(\\hat{\\theta}^{JS}, \\theta_p) \\leq 2 + \\frac{p \\|\\theta\\|^2}{d + \\|\\theta\\|^2}.\n\\] This implies that \\(R(\\hat{\\theta}^{JS}, \\theta_p) \\geq (p/2)\\).\nIn the sparse case, a simple thresholding rule can beat MLE and JS when the signal is sparse. Assuming \\(\\sigma^2 = 1\\), the thresholding estimator is: \\[\n\\hat{\\theta}_{thr} = \\begin{cases}\n\\hat{\\theta}_i & \\text{if } \\hat{\\theta}_i &gt; \\sqrt{2 \\ln p} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] This simple example shows that the choice of penalty should not be taken for granted as different estimators will have different risk profiles.\nOne such estimator that achieves the optimal minimax rate is the horseshoe estimator Carvalho, Polson, and Scott (2010), which we discuss in detail in Section 17.10. It is a local shrinkage estimator that dominates the sample mean in MSE and has good posterior concentration properties for sparse signal problems.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#maximum-a-posteriori-estimation-map-and-regularization",
    "href": "17-theoryai.html#maximum-a-posteriori-estimation-map-and-regularization",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.5 Maximum A posteriori Estimation (MAP) and Regularization",
    "text": "17.5 Maximum A posteriori Estimation (MAP) and Regularization\nHaving established that Bayesian shrinkage reduces estimation risk, we now examine a computationally tractable alternative: maximum a posteriori (MAP) estimation. While Bayesian shrinkage provably reduces estimation risk, full posterior inference can be computationally demanding. Maximum a posteriori (MAP) estimation offers a tractable alternative that captures many regularization benefits without the cost of full integration. MAP captures the regularization benefits of Bayesian methods without requiring full posterior inference.\nGiven input-output pairs \\((x_i,y_i)\\), MAP learns the function \\(f\\) that maps inputs \\(x_i\\) to outputs \\(y_i\\) by minimizing \\[\n\\underset{f}{\\mathrm{minimize}} \\quad \\sum_{i=1}^N  \\mathcal{L}(y_i,f(x_i)) + \\lambda \\phi(f).\n\\] The first term is the loss function that measures the difference between the predicted output \\(f(x_i)\\) and the true output \\(y_i\\). The second term is a regularization term that penalizes complex functions \\(f\\) to prevent overfitting. The parameter \\(\\lambda\\) controls the trade-off between fitting the data well and keeping the function simple. In the case when \\(f\\) is a parametric model, then we simply replace \\(f\\) with the parameters \\(\\theta\\) of the model, and the regularization term becomes a penalty on the parameters.\nThe loss is simply a negative log-likelihood from a probabilistic model specified for the data generating process. For example, when \\(y\\) is numeric and \\(y_i \\mid x_i \\sim N(f(x_i),\\sigma^2)\\), we get the squared loss \\(\\mathcal{L}(y,f(x)) = (y-f(x))^2\\). When \\(y_i\\in \\{0,1\\}\\) is binary, we use the logistic loss \\(\\mathcal{L}(y,f(x)) = \\log(1+\\exp(-yf(x)))\\).\nThe penalty term \\(\\lambda \\phi(f)\\) discourages complex functions \\(f\\). Then, we can think of regularization as a technique to incorporate some prior knowledge about parameters of the model into the estimation process. Consider an example when regularization allows us to solve a hard problem of filtering noisy traffic data.\n\nExample 17.3 (Traffic) Consider traffic flow speed measured by an in-ground sensor installed on interstate I-55 near Chicago. Speed measurements are noisy and prone to have outliers. Figure 17.1 shows measured speed data, averaged over five minute intervals on one of the weekdays.\n\n\n\n\n\n\nFigure 17.1: Speed profile over 24 hour period on I-55, on October 22, 2013\n\n\n\nThe statistical model is \\[\ny_t = f_t + \\epsilon_t, ~ \\epsilon_t \\sim N(0,\\sigma^2), ~ t=1,\\ldots,n,\n\\] where \\(y_t\\) is the speed measurement at time \\(t\\), \\(f_t\\) is the true underlying speed at time \\(t\\), and \\(\\epsilon_t\\) is the measurement noise. There are two sources of noise. The first is the measurement noise, caused by the inherent nature of the sensor’s hardware. The second source is due to sampling error, vehicles observed on a specific lane where the sensor is installed might not represent well traffic in other lanes. A naive MLE approach would be to estimate the speed profile \\(f = (f_1, \\ldots, f_n)\\) by minimizing the squared loss \\[\n\\hat f = \\arg\\min_{f} \\sum_{t=1}^{n} (y_t - f_t)^2.\n\\] However, the minima of this loss function is 0 and corresponds to the case when \\(\\hat f_t = y_t\\) for all \\(t\\). We have learned nothing about the speed profile, and the estimate is simply the noisy observation \\(y_t\\). In this case, we have no way to distinguish between the true speed profile and the noise.\nHowever, we can use regularization and bring some prior knowledge about traffic speed profiles to improve the estimate of the speed profile and to remove the noise.\nSpecifically, we will use a trend filtering approach. Under this approach, we assume that the speed profile \\(f\\) is a piece-wise linear function of time, and we want to find a function that captures the underlying trend while ignoring the noise. The regularization term \\(\\phi(f)\\) is then the second difference of the speed profile, \\[\n\\lambda \\sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|\n\\] which penalizes the “kinks” in the speed profile. The value of this penalty is zero, when \\(f_{t-1}, f_t, f_{t+1}\\) lie on a straight line, and it increases when the speed profile has a kink. The parameter \\(\\lambda\\) is a regularization parameter that controls the strength of the penalty.\nTrend filtering penalized function is then \\[\n(1/2) \\sum_{t=1}^{n}(y_t - f_t)^2 + \\lambda \\sum_{t=1}^{n-1}|f_{t-1} - 2f_t + f_{t+1}|,\n\\] which is a variation of a well-known Hodrick-Prescott filter.\nThis approach requires us to choose the regularization parameter \\(\\lambda\\). A small value of \\(\\lambda\\) will lead to a function that fits the data well, but may not capture the underlying trend. A large value of \\(\\lambda\\) will lead to a function that captures the underlying trend, but may not fit the data well. The optimal value of \\(\\lambda\\) can be chosen using cross-validation or other model selection techniques. The left panel of Figure 17.2 shows the trend filtering for different values of \\(\\lambda \\in \\{5,50,500\\}\\). The right panel shows the optimal value of \\(\\lambda\\) chosen by cross-validation (by visual inspection).\n\n\n\n\n\n\n\n\nTrend filter for different penalty\n\n\n\n\n\n\n\nTrend filtering with optimal penalty\n\n\n\n\n\n\nFigure 17.2: Trend Filtering for Traffic Speed Data",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#the-duality-between-regularization-and-priors",
    "href": "17-theoryai.html#the-duality-between-regularization-and-priors",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.6 The Duality Between Regularization and Priors",
    "text": "17.6 The Duality Between Regularization and Priors\nA large number of statistical problems can be expressed in the canonical optimization form:\n\\[\n\\begin{aligned}\n& \\underset{x \\in \\mathbb{R}^d}{\\text{minimize}}\n& & l(x) + \\phi(x)\n\\end{aligned}\n\\tag{17.3}\\]\nPerhaps the most common example arises in estimating the regression coefficients \\(x\\) in a generalized linear model. Here \\(l(x)\\) is a negative log likelihood or some other measure of fit, and \\(\\phi(x)\\) is a penalty function that effects a favorable bias-variance tradeoff.\nFrom the Bayesian perspective, \\(l(x)\\) and \\(\\phi(x)\\) correspond to the negative logarithms of the sampling model and prior distribution in the hierarchical model:\n\\[\np(y \\mid x) \\propto \\exp\\{-l(x)\\}, \\quad p(x) \\propto \\exp\\{-\\phi(x)\\}\n\\tag{17.4}\\]\nand the solution to Equation 17.3 may be interpreted as a maximum a posteriori (MAP) estimate.\nAnother common case is where \\(x\\) is a variable in a decision problem where options are to be compared on the basis of expected loss, and where \\(l(x)\\) and \\(\\phi(x)\\) represent conceptually distinct contributions to the loss function. For example, \\(l(x)\\) may be tied to the data, and \\(\\phi(x)\\) to the intrinsic cost associated with the decision.\n\nMAP as a Poor Man’s Bayesian\nThere is a duality between using regularization term in optimization problem and assuming a prior distribution over the parameters of the model \\(f\\). Given the likelihood \\(L(y_i,f(x_i))\\), the posterior is given by Bayes’ rule: \\[\np(f \\mid y, x) = \\frac{\\prod_{i=1}^n L(y_i,f(x_i)) p(f)}{p(y \\mid x)}.\n\\] If we take the negative log of this posterior, we get: \\[\n-\\log p(f \\mid y, x) = - \\sum_{i=1}^n \\log L(y_i,f(x_i)) - \\log p(f) + \\log p(y \\mid x).\n\\] Since loss is the negative log-likelihood \\(\\mathcal{L}(y_i,f(x_i)) = -\\log L(y_i,f(x_i))\\), the posterior maximization is equivalent to minimizing the following regularized loss function: \\[\n\\sum_{i=1}^n \\mathcal{L}(y_i,f(x_i)) - \\log p(f).\n\\] The last term \\(\\log p(y_i \\mid x_i)\\) does not depend on \\(f\\) and can be ignored in the optimization problem. Thus, the equivalence is given by: \\[\n\\lambda \\phi(f) = -\\log p(f),\n\\] where \\(\\phi(f)\\) is the penalty term that corresponds to the prior distribution of \\(f\\). Below we will consider several choices for the prior distribution of \\(f\\) and the corresponding penalty term \\(\\phi(f)\\) commonly used in practice.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#ridge-regression-ell_2-norm",
    "href": "17-theoryai.html#ridge-regression-ell_2-norm",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.7 Ridge Regression (\\(\\ell_2\\) Norm)",
    "text": "17.7 Ridge Regression (\\(\\ell_2\\) Norm)\n\nTikhonov Regularization Framework\nThe Tikhonov regularization framework provides a general setting for regression that connects classical regularization theory with modern Bayesian approaches. Given observed data \\((x_i, y_i)_{i=1}^n\\) and a parametric model \\(f(x,w)\\) with parameter vector \\(w \\in \\mathbb{R}^k\\), we define a data misfit functional: \\[\nE_D(w) = \\sum_{i=1}^n (y_i - f(x_i,w))^2\n\\]\nThis yields a Gaussian likelihood: \\[\np(y \\mid w) = \\frac{1}{Z_D}\\exp\\left(-\\frac{1}{2\\sigma^2}E_D(w)\\right)\n\\] where \\(\\sigma^2\\) denotes the noise variance and \\(Z_D\\) is a normalization constant.\nA Gaussian prior on the weights can be specified as: \\[\np(w) = \\frac{1}{Z_W}\\exp\\left(-\\frac{1}{2\\sigma_w^2}E_W(w)\\right)\n\\] where \\(E_W(w)\\) denotes a quadratic penalty on \\(w\\) (e.g., \\(E_W(w) = w^T w\\)), \\(\\sigma_w^2\\) is the prior variance, and \\(Z_W\\) is its normalization constant.\nThe hyperparameters \\(\\sigma^2\\) and \\(\\sigma_w^2\\) control the strength of the noise and the prior. Following MacKay’s notation, we define precisions \\(\\tau_D^2 = 1/\\sigma^2\\) and \\(\\tau_w^2 = 1/\\sigma_w^2\\). Let \\(B\\) and \\(C\\) denote the Hessians of \\(E_D(w)\\) and \\(E_W(w)\\) at the maximum a posteriori (MAP) estimate \\(w^{\\text{MAP}}\\). The Hessian of the negative log-posterior is then \\(A = \\tau_w^2 C + \\tau_D^2 B\\).\nEvaluating the log-evidence under a Gaussian Laplace approximation yields: \\[\n\\log p(D \\mid \\tau_w^2,\\tau_D^2) = -\\tau_w^2 E_W^{\\text{MAP}} - \\tau_D^2 E_D^{\\text{MAP}} - \\frac{1}{2}\\log\\det A - \\log Z_W(\\tau_w^2) - \\log Z_D(\\tau_D^2) + \\frac{k}{2}\\log(2\\pi)\n\\]\nThe associated Occam factor, which penalizes excessively small prior variances, is: \\[\n-\\tau_w^2 E_W^{\\text{MAP}} - \\frac{1}{2}\\log\\det A - \\log Z_W(\\tau_w^2)\n\\]\nThis represents the reduction in effective volume of parameter space from prior to posterior.\n\n[!NOTE] Notation: In the previous sections on Normal Means, we used \\(\\theta\\) to denote the parameters. In the context of regression, we will follow standard convention and use \\(\\beta\\) to denote the vector of coefficients.\n\nThe ridge regression uses a Gaussian prior on the parameters of the model \\(f\\), which leads to a squared penalty term. Specifically, we assume that the parameters \\(\\beta\\) of the model \\(f(x) = x^T\\beta\\) are distributed as: \\[\n\\beta \\sim N(0, \\sigma_\\beta^2 I),\n\\] where \\(I\\) is the identity matrix and \\(\\sigma_\\beta^2\\) is the prior variance (distinct from the noise variance \\(\\sigma^2\\)). The prior distribution of \\(\\beta\\) is a multivariate normal distribution with mean 0 and covariance \\(\\sigma_\\beta^2 I\\). The negative log of this prior distribution is given by: \\[\n-\\log p(\\beta) = \\frac{1}{2\\sigma_\\beta^2} \\|\\beta\\|_2^2 + \\text{const},\n\\] where \\(\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2\\) is the squared 2-norm of the vector \\(\\beta\\). The regularization term \\(\\phi(f)\\) is then given by: \\[\n\\phi(f) = \\frac{1}{2\\sigma_\\beta^2} \\|\\beta\\|_2^2.  \n\\] This leads to the following optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2,\n\\] where \\(\\lambda = 1/\\sigma^2\\) is the regularization parameter that controls the strength of the prior. The solution to this optimization problem is given by: \\[\n\\hat{\\beta}_{\\text{ridge}} = ( X^T X + \\lambda I )^{-1} X^T y.\n\\] The regularization parameter \\(\\lambda\\) is related to the variance of the prior distribution. When \\(\\lambda=0\\), the function \\(f\\) is the maximum likelihood estimate of the parameters. When \\(\\lambda\\) is large, the function \\(f\\) is the prior mean of the parameters. When \\(\\lambda\\) is infinite, the function \\(f\\) is the prior mode of the parameters.\nNotice, that the OLS estimate (invented by Gauss) is a special case of ridge regression when \\(\\lambda = 0\\): \\[\n\\hat{\\beta}_{\\text{OLS}} = ( X^T X )^{-1} X^T y.\n\\]\nThe original motivation for ridge regularization was to address the problem of numerical instability in the OLS solution when the design matrix \\(X\\) is ill-conditioned, i.e. when \\(X^T X\\) is close to singular. In this case, the OLS solution can be very sensitive to small perturbations in the data, leading to large variations in the estimated coefficients \\(\\hat{\\beta}\\). This is particularly problematic when the number of features \\(p\\) is large, as the condition number of \\(X^T X\\) can grow rapidly with \\(p\\). The ridge regression solution stabilizes the OLS solution by adding a small positive constant \\(\\lambda\\) to the diagonal of the \\(X^T X\\) matrix, which improves the condition number and makes the solution more robust to noise in the data. The additional term \\(\\lambda I\\) simply shifts the eigenvalues of \\(X^T X\\) away from zero, thus improving the numerical stability of the inversion.\nAnother way to think and write the objective function of Ridge as the following constrained optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq t,\n\\] where \\(t\\) is a positive constant that controls the size of the coefficients \\(\\beta\\). This formulation emphasizes the idea that ridge regression is a form of regularization that constrains the size of the coefficients, preventing them from growing too large and leading to overfitting. The constraint \\(\\|\\beta\\|_2^2 \\leq t\\) can be interpreted as a budget on the size of the coefficients, where larger values of \\(t\\) allow for larger coefficients and more complex models.\nConstraint on the model parameters (and the original Ridge estimator) was proposed by Tikhonov et al. (1943) for solving inverse problems to “discover” physical laws from observations. The norm of the \\(\\beta\\) vector would usually represent amount of energy required. Many processes in nature are energy minimizing!\nAgain, the tuning parameter \\(\\lambda\\) controls trade-off between how well model fits the data and how small \\(\\beta\\)s are. Different values of \\(\\lambda\\) will lead to different models. We select \\(\\lambda\\) using cross validation.\n\nExample 17.4 (Shrinkage) Consider a simulated data with \\(n=50\\), \\(p=30\\), and \\(\\sigma^2=1\\). The true model is linear with \\(10\\) large coefficients between \\(0.5\\) and \\(1\\).\nOur approximators \\(\\hat f_{\\beta}\\) is a linear regression. We can empirically calculate the bias by calculating the empirical squared loss \\(1/n\\|y -\\hat y\\|_2^2\\) and variance can be empirically calculated as \\(1/n\\sum  (\\bar{\\hat{y}} - \\hat y_i)^2\\)\nBias squared \\(\\mathrm{Bias}(\\hat{y})^2=0.006\\) and variance \\(\\Var{\\hat{y}} =0.627\\). Thus, the prediction error = \\(1 + 0.006 + 0.627 = 1.633\\)\nWe’ll do better by shrinking the coefficients to reduce the variance. Let’s estimate how big a gain we can achieve with Ridge regression.\nThe figure below shows the distribution of true coefficient values used to generate the data. The histogram reveals a bimodal structure: approximately 20 coefficients are exactly zero (the tall bar at 0.0), while 10 coefficients are non-zero with values concentrated between 0.5 and 1.0. This sparse structure, where most features are irrelevant and only a subset truly matter for prediction, is common in many real-world problems such as genomics, text analysis, and high-dimensional sensor data.\n\n\n\nTrue model coefficients\n\n\nThe key question is: what value of the regularization parameter \\(\\lambda\\) should we choose? Too small, and we don’t shrink enough to reduce variance; too large, and we introduce excessive bias by shrinking the true non-zero coefficients toward zero. The optimal choice balances these competing concerns.\nFigure 17.3 (a) shows how prediction error varies with the amount of shrinkage (controlled by \\(\\lambda\\)). The horizontal dashed line represents the constant prediction error of 1.633 from ordinary linear regression (OLS). The red curve shows Ridge regression’s prediction error as we increase regularization. At low shrinkage (left side), Ridge behaves similarly to OLS with high variance. As we increase shrinkage, the prediction error decreases substantially, reaching a minimum around \\(\\lambda \\approx 5\\). Beyond this point, excessive shrinkage introduces too much bias, and prediction error begins to rise again. The U-shaped curve is characteristic of the bias-variance trade-off: we need enough regularization to stabilize predictions, but not so much that we distort the true signal.\nFigure 17.3 (b) decomposes Ridge regression’s prediction error into its constituent parts as a function of \\(\\lambda\\). The black dashed horizontal line at approximately 1.0 represents the irreducible error from noise in the data. The blue curve (Ridge Var) shows variance decreasing monotonically as \\(\\lambda\\) increases: stronger regularization makes the model more stable across different training sets. The red curve (Ridge Bias\\(^2\\)) shows squared bias increasing as \\(\\lambda\\) grows: we move further from the true model by shrinking coefficients. The black curve (Ridge MSE) is the sum of these components plus the irreducible error. The optimal \\(\\lambda\\) occurs where the rate of variance reduction exactly balances the rate of bias increase, minimizing total prediction error.\n\n\n\n\n\n\n\n\n\n\n\n(a) Prediction error as a function of \\(\\lambda\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Bias-variance decomposition\n\n\n\n\n\n\n\nFigure 17.3: Ridge regression performance metrics\n\n\n\nAt the optimal value of \\(\\lambda\\), Ridge regression achieves squared bias of 0.077 and variance of 0.402, yielding a total prediction error of \\(1 + 0.077 + 0.402 = 1.48\\). Compare this to the OLS solution with squared bias of 0.006 and variance of 0.627, giving prediction error of \\(1 + 0.006 + 0.627 = 1.633\\). By accepting a modest increase in bias (from 0.006 to 0.077), we achieve a substantial reduction in variance (from 0.627 to 0.402), resulting in an overall improvement of approximately 9% in prediction error.\nThis example illustrates a fundamental principle in statistical learning: the best predictor is not necessarily the one that fits the training data most closely. By deliberately introducing bias through regularization, we can build models that generalize better to new data. The Bayesian perspective makes this trade-off explicit: the prior distribution encodes our belief that coefficients should be small, and the posterior balances this belief against the evidence in the data.\n\n\n\nKernel View of Ridge Regression\nAnother interesting view stems from what is called the push-through matrix identity: \\[\n(aI + UV)^{-1}U = U(aI + VU)^{-1}\n\\] for \\(a\\), \\(U\\), \\(V\\) such that the products are well-defined and the inverses exist. We can obtain this from \\(U(aI + VU) = (aI + UV)U\\), followed by multiplication by \\((aI + UV)^{-1}\\) on the left and the right. Applying the identity above to the ridge regression solution with \\(a = \\lambda\\), \\(U = X^T\\), and \\(V = X\\), we obtain an alternative form for the ridge solution: \\[\n\\hat{\\beta} = X^T (XX^T + \\lambda I)^{-1} Y.\n\\] This is often referred to as the kernel form of the ridge estimator. From this, we can see that the ridge fit can be expressed as \\[\nX\\hat{\\beta} = XX^T (XX^T + \\lambda I)^{-1} Y.\n\\] What does this remind you of? This is precisely \\(K(K + \\lambda I)^{-1}Y\\) where \\(K = XX^T\\), which, recall, is the fit from RKHS regression with a linear kernel \\(k(x, z) = x^T z\\). Therefore, we can think of RKHS regression as generalizing ridge regression by replacing the standard linear inner product with a general kernel. (Indeed, RKHS regression is often called kernel ridge regression.)",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#scale-mixtures-representations",
    "href": "17-theoryai.html#scale-mixtures-representations",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.8 Scale Mixtures Representations",
    "text": "17.8 Scale Mixtures Representations\nWhy should we care about expressing distributions as mixtures? The answer lies in computational tractability and theoretical insight. Many important priors used in sparse estimation—including the Laplace (Lasso), horseshoe, and logistic—can be represented as Gaussian distributions with random variance. This representation is far more than a mathematical curiosity: it enables efficient Gibbs sampling algorithms where each conditional distribution has a closed form, and it reveals deep connections between seemingly different regularization approaches.\nScale mixtures of normals provide a powerful framework for constructing flexible priors and computational algorithms in Bayesian statistics. The key insight is that many useful distributions can be represented as Gaussian distributions with random variance, leading to tractable MCMC algorithms and analytical insights.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#lasso-regression-ell_1-norm",
    "href": "17-theoryai.html#lasso-regression-ell_1-norm",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.9 Lasso Regression (\\(\\ell_1\\) Norm)",
    "text": "17.9 Lasso Regression (\\(\\ell_1\\) Norm)\nThe Lasso (Least Absolute Shrinkage and Selection Operator) regression uses a Laplace prior on the parameters of the model \\(f\\), which leads to an \\(\\ell_1\\) penalty term. Specifically, we assume that the parameters \\(\\beta\\) of the model \\(f(x) = x^T\\beta\\) are distributed as: \\[\n\\beta_j \\sim \\text{Laplace}(0, b) \\quad \\text{independently for } j = 1, \\ldots, p,\n\\] where \\(b &gt; 0\\) is the scale parameter. The Laplace distribution has the probability density function: \\[\np(\\beta_j \\mid b) = \\frac{1}{2b}\\exp\\left(-\\frac{|\\beta_j|}{b}\\right)\n\\] and is shown in Figure 17.4.\n\n\nCode\n# PLot Laplace distribution\nlibrary(ggplot2)\nb &lt;- 1\nbeta &lt;- seq(-5, 5, length.out = 100)\nlaplace_pdf &lt;- function(beta, b) {\n  (1/(2*b)) * exp(-abs(beta)/b)\n}\nlaplace_data &lt;- data.frame(beta = beta, pdf = laplace_pdf(beta, b))\nggplot(laplace_data, aes(x = beta, y = pdf)) +\n  geom_line() +\n  labs(title = \"\", x = \"Beta\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 17.4: Laplace Distribution PDF\n\n\n\n\n\nThe negative log of this prior distribution is given by: \\[\n-\\log p(\\beta) = \\frac{1}{b} \\|\\beta\\|_1 + \\text{const},\n\\] where \\(\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|\\) is the \\(\\ell_1\\)-norm of the vector \\(\\beta\\). The regularization term \\(\\phi(f)\\) is then given by: \\[\n\\phi(f) = \\frac{1}{b} \\|\\beta\\|_1.  \n\\] This leads to the following optimization problem: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n\\] where \\(\\lambda = 2\\sigma^2/b\\) is the regularization parameter that controls the strength of the prior. Unlike ridge regression, the Lasso optimization problem does not have a closed-form solution due to the non-differentiable nature of the \\(\\ell_1\\) penalty. However, efficient algorithms such as coordinate descent and proximal gradient methods can be used to solve it.\nThe key distinguishing feature of Lasso is its ability to perform automatic variable selection. The \\(\\ell_1\\) penalty encourages sparsity in the coefficient vector \\(\\hat{\\beta}\\), meaning that many coefficients will be exactly zero. This property makes Lasso particularly useful for high-dimensional problems where feature selection is important.\nWhen \\(\\lambda=0\\), the Lasso reduces to the ordinary least squares (OLS) estimate. As \\(\\lambda\\) increases, more coefficients are driven to exactly zero, resulting in a sparser model. When \\(\\lambda\\) is very large, all coefficients become zero.\nThe geometric intuition behind Lasso’s sparsity-inducing property comes from the constraint formulation. We can write the Lasso problem as: \\[\n\\underset{\\beta}{\\mathrm{minimize}}\\quad \\|y- X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_1 \\leq t,\n\\] where \\(t\\) is a positive constant that controls the sparsity of the solution. The constraint region \\(\\|\\beta\\|_1 \\leq t\\) forms a diamond (in 2D) or rhombus-shaped region with sharp corners at the coordinate axes. The optimal solution often occurs at these corners, where some coefficients are exactly zero.\nFrom a Bayesian perspective, the Lasso estimator corresponds to the maximum a posteriori (MAP) estimate under independent Laplace priors on the coefficients. We use Bayes rule to calculate the posterior as a product of Normal likelihood and Laplace prior: \\[\n-\\log p(\\beta \\mid y, b) = \\|y-X\\beta\\|_2^2 + \\frac{2\\sigma^2}{b}\\|\\beta\\|_1.\n\\] For fixed \\(\\sigma^2\\) and \\(b&gt;0\\), the posterior mode is equivalent to the Lasso estimate with \\(\\lambda = 2\\sigma^2/b\\). Large variance \\(b\\) of the prior is equivalent to small penalty weight \\(\\lambda\\) in the Lasso objective function.\nOne of the most popular algorithms for solving the Lasso problem is coordinate descent. The algorithm iteratively updates each coefficient while holding all others fixed. For the \\(j\\)-th coefficient, the update rule is: \\[\n\\hat{\\beta}_j \\leftarrow \\text{soft}\\left(\\frac{1}{n}\\sum_{i=1}^n x_{ij}(y_i - \\sum_{k \\neq j} x_{ik}\\hat{\\beta}_k), \\frac{\\lambda}{n}\\right),\n\\] where the soft-thresholding operator is defined as: \\[\n\\text{soft}(z, \\gamma) = \\text{sign}(z)(|z| - \\gamma)_+ = \\begin{cases}\nz - \\gamma & \\text{if } z &gt; \\gamma \\\\\n0 & \\text{if } |z| \\leq \\gamma \\\\\nz + \\gamma & \\text{if } z &lt; -\\gamma\n\\end{cases}\n\\]\n\nExample 17.5 (Sparsity and Variable Selection) We will demonstrate the Lasso’s ability to perform variable selection and shrinkage using simulated data. The data will consist of a design matrix with correlated predictors and a sparse signal, where only a 5 out of 20 predictors have non-zero coefficients.\n\n\nCode\n# Generate simulated data\nset.seed(123)\nn &lt;- 100  # number of observations\np &lt;- 20   # number of predictors\nsigma &lt;- 1  # noise level\n\n# Create design matrix with some correlation structure\nX &lt;- matrix(rnorm(n * p), n, p)\n# Add some correlation between predictors\nfor(i in 2:p) {\n  X[, i] &lt;- 0.5 * X[, i-1] + sqrt(0.75) * X[, i]\n}\n\n\n\n# True coefficients - sparse signal\nbeta_true &lt;- c(3, -2, 1.5, 0, 0, 2, 0, 0, 0, -1, rep(0, 10))\nsparse_indices &lt;- which(beta_true != 0)\n\n# Generate response\ny &lt;- X %*% beta_true + sigma * rnorm(n)\n\nThen we use glmnet package to fit the Lasso model and visualize the coefficient paths. We will also perform cross-validation to select the optimal regularization parameter \\(\\lambda\\).\n\n# Fit LASSO path using glmnet\nlibrary(glmnet)\nlasso_fit &lt;- glmnet(X, y, alpha = 1)\n# Plot coefficient paths\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)\n\n\n\n\nLASSO Coefficient Paths\n\n\n\n\nThe coefficient paths plot shows how LASSO coefficients shrink toward zero as the regularization parameter lambda increases. The colored lines represent different predictors, demonstrating LASSO’s variable selection property. Note, that glmnet fitted the model for a sequence of \\(\\lambda\\) values. The algorithms starts with a large lambda value, where all coefficients are penalized to zero. Then, it gradually decreases lambda, using the coefficients from the previous, slightly more penalized model as a “warm start” for the current calculation. This pathwise approach is significantly more efficient than starting the optimization from scratch for every single \\(\\lambda\\). By default, glmnet computes the coefficients for a sequence of 100 lambda values spaced evenly on the logarithmic scale, starting from a data-driven maximum value (where all coefficients are zero) down to a small fraction of that maximum. The user can specify their own sequence of lambda values if specific granularity or range is desired\nFinally, we will perform cross-validation to select the optimal \\(\\lambda\\) value and compare the estimated coefficients with the true values.\n\n# Cross-validation to select optimal lambda\ncv_lasso &lt;- cv.glmnet(X, y, alpha = 1, nfolds = 10)\n# Plot cross-validation curve\nplot(cv_lasso)\n\n\n\n\nCross-validation for LASSO\n\n\n\n\nNow, we can extract the coefficients lambda.min and lambda.1se from the cross-validation results, which correspond to the minimum cross-validated error and the most regularized model within one standard error of the minimum, respectively.\n\n# Extract coefficients at optimal lambda\nlambda_min &lt;- cv_lasso$lambda.min\nlambda_1se &lt;- cv_lasso$lambda.1se\ncoef_min &lt;- coef(lasso_fit, s = lambda_min)\ncoef_1se &lt;- coef(lasso_fit, s = lambda_1se)\n# Print values of lambda\ncat(\"Optimal lambda (min):\", lambda_min, \"\\n\")\n## Optimal lambda (min): 0.016\ncat(\"Optimal lambda (1se):\", lambda_1se, \"\\n\")\n## Optimal lambda (1se): 0.1\n\n\n\nCode\n# Compare estimates with true values\ncomparison &lt;- data.frame(\n  True = c(0, beta_true),  # Include intercept\n  LASSO_min = as.vector(coef_min),\n  LASSO_1se = as.vector(coef_1se)\n)\nrownames(comparison) &lt;- c(\"Intercept\", paste0(\"X\", 1:p))\n# Visualization of coefficient estimates\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# Melt data for plotting\nplot_data &lt;- melt(comparison, id.vars = NULL)\nplot_data$Variable &lt;- rep(rownames(comparison), 3)\nplot_data$Variable &lt;- factor(plot_data$Variable, levels = rownames(comparison))\n\nggplot(plot_data, aes(x = Variable, y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(y = \"Coefficient Value\", fill = \"Method\") +\n  scale_fill_brewer(type = \"qual\", palette = \"Set2\")\n\n\n\n\n\nCoefficient Estimates Comparison\n\n\n\n\nIt seems like LASSO has successfully identified the non-zero coefficients and shrunk the noise variables to zero. The coefficient estimates at lambda.min and lambda.1se show that LASSO retains the true signals while effectively ignoring the noise. Let’s calculate the prediction errors and evaluate the variable selection performance of LASSO at both optimal \\(\\lambda\\) values.\n\n# Calculate prediction errors\npred_min &lt;- predict(lasso_fit, newx = X, s = lambda_min)\npred_1se &lt;- predict(lasso_fit, newx = X, s = lambda_1se)\n\nmse_min &lt;- mean((y - pred_min)^2)\nmse_1se &lt;- mean((y - pred_1se)^2)\n\ncat(\"Mean Squared Error (lambda.min):\", round(mse_min, 3), \"\\n\")\n## Mean Squared Error (lambda.min): 0.68\ncat(\"Mean Squared Error (lambda.1se):\", round(mse_1se, 3), \"\\n\")\n## Mean Squared Error (lambda.1se): 0.85\n\nIn summary, this example demonstrates how LASSO regression can be used for both variable selection and regularization in high-dimensional settings. By tuning the regularization parameter \\(\\lambda\\), LASSO is able to shrink irrelevant coefficients to zero, effectively identifying the true underlying predictors while controlling model complexity. The comparison of coefficient estimates and prediction errors at different \\(\\lambda\\) values highlights the trade-off between model sparsity and predictive accuracy. LASSO’s ability to produce interpretable, sparse models makes it a powerful tool in modern statistical learning, especially when dealing with datasets where the number of predictors may be large relative to the number of observations.\n\n\nLasso as a Scale Mixture\nThe Laplace distribution can be represented as a scale mixture of Normal distributions (Andrews and Mallows 1974): \\[\n\\begin{aligned}\n\\beta_j \\mid \\sigma^2,\\tau_j &\\sim N(0,\\tau_j^2\\sigma^2)\\\\\n\\tau_j^2 \\mid \\alpha &\\sim \\text{Exp}(\\alpha^2/2)\\\\\n\\sigma^2 &\\sim \\pi(\\sigma^2).\n\\end{aligned}\n\\] We can show equivalence by integrating out \\(\\tau_j\\): \\[\np(\\beta_j\\mid \\sigma^2,\\alpha) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi \\tau_j\\sigma^2}}\\exp\\left(-\\frac{\\beta_j^2}{2\\sigma^2\\tau_j^2}\\right)\\frac{\\alpha^2}{2}\\exp\\left(-\\frac{\\alpha^2\\tau_j^2}{2}\\right)d\\tau_j = \\frac{\\alpha}{2\\sigma}\\exp\\left(-\\frac{\\alpha|\\beta_j|}{\\sigma}\\right).\n\\] Thus it is a Laplace distribution with location 0 and scale \\(\\alpha/\\sigma\\). Representation of Laplace prior is a scale Normal mixture allows us to apply an efficient numerical algorithm for computing samples from the posterior distribution. This algorithms is called a Gibbs sample and it iteratively samples from \\(\\theta \\mid a,y\\) and \\(b\\mid \\theta,y\\) to estimate joint distribution over \\((\\hat \\theta, \\hat b)\\). Thus, we so not need to apply cross-validation to find optimal value of \\(b\\), the Bayesian algorithm does it “automatically”.\n\n\nMore Mixture Representations\nThe power of scale mixture representations extends beyond the Laplace distribution to many common loss functions used in machine learning and statistics. Polson and Scott (2011) provide a unified framework showing that various loss functions can be represented as variance-mean Gaussian mixtures, enabling efficient posterior sampling via data augmentation.\nConsider the regression or classification setting where \\(z_i = y_i - x_i^T\\beta\\) for regression, or \\(z_i = y_i x_i^T\\beta\\) for binary classification (with \\(y_i \\in \\{-1, +1\\}\\)). The key insight is that many loss functions \\(f(z_i \\mid \\beta, \\sigma)\\) can be expressed through the variance-mean mixture:\n\\[\np(z_i \\mid \\beta, \\sigma) = \\int_0^{\\infty} \\phi(z_i \\mid \\mu_z + \\kappa_z \\omega_i, \\sigma^2 \\omega_i) \\, dP(\\omega_i)\n\\]\nwhere \\(\\phi(\\cdot \\mid m, v)\\) denotes the normal density with mean \\(m\\) and variance \\(v\\), and \\(P(\\omega_i)\\) is an appropriate mixing distribution. The following table summarizes representations for several important loss functions:\n\n\n\n\n\n\n\n\n\n\nLoss Function\n\\(f(z_i \\mid \\beta, \\sigma)\\)\n\\(\\kappa_z\\)\n\\(\\mu_z\\)\nMixing Distribution \\(P(\\omega_i)\\)\n\n\n\n\nSquared-error\n\\(z_i^2/\\sigma^2\\)\n0\n0\n\\(\\omega_i = 1\\)\n\n\nAbsolute-error\n\\(\\|z_i/\\sigma\\|\\)\n0\n0\nExponential\n\n\nCheek loss\n\\(\\|z_i\\| + (2q-1)z_i\\)\n\\(1-2q\\)\n0\nGeneralized inverse Gaussian\n\n\nSupport vector machines\n\\(\\max(1-z_i, 0)\\)\n1\n1\nGeneralized inverse Gaussian\n\n\nLogistic\n\\(\\log(1+e^{z_i})\\)\n\\(1/2\\)\n0\nPólya-Gamma\n\n\n\nThis unified representation has profound computational implications. By introducing latent variables \\(\\{\\omega_i\\}\\) and \\(\\{\\lambda_j\\}\\) through appropriate equations, the exponential form \\(\\exp\\{-L(\\beta)\\}\\) reduces to a Gaussian linear model with heteroscedastic errors. This enables straightforward Gibbs sampling where each conditional distribution has closed form.\nThe working response \\(z_i\\) equals \\(y_i - x_i^T\\beta\\) for Gaussian regression, or \\(y_i x_i^T\\beta\\) for binary classification using logistic regression or support-vector machines. Both \\(\\sigma\\) and \\(\\tau\\) (from the prior) are hyperparameters typically estimated jointly with \\(\\beta\\), though they may also be specified by the user or chosen by cross-validation. Importantly, in logistic regression \\(\\sigma\\) does not appear in the model as a hyperparameter.\nThis framework characterizes the general case for many models. Previous studies (Polson and Scott 2011) have presented similar results for specific models, including support-vector machines and the powered-logit likelihood. The variance-mean mixture approach provides both theoretical insight into the connections between different loss functions and practical algorithms for efficient Bayesian inference.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#sec-horseshoe",
    "href": "17-theoryai.html#sec-horseshoe",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.10 Horseshoe",
    "text": "17.10 Horseshoe\nThe horseshoe prior represents a significant advancement in Bayesian sparse estimation, addressing fundamental limitations of the Lasso while maintaining computational tractability (Carvalho, Polson, and Scott 2010). The name derives from the shape of its shrinkage profile, which resembles an inverted horseshoe: it applies minimal shrinkage to large signals while aggressively shrinking noise coefficients toward zero. This behavior makes it particularly attractive for high-dimensional problems where strong sparsity is expected but we want to avoid over-shrinking the true signals.\nUnlike the Lasso, which applies constant shrinkage across all coefficient magnitudes, the horseshoe exhibits adaptive shrinkage. Small coefficients receive heavy regularization approaching zero, while large coefficients are left nearly unregularized. This is precisely the behavior we desire in sparse estimation: confidently shrink noise to zero while preserving signal fidelity. The horseshoe achieves this through a hierarchical Bayesian structure that places a half-Cauchy prior on local scale parameters.\n\nMathematical Formulation\nThe horseshoe prior is defined through a hierarchical scale mixture of normals. For the regression model \\(y = X\\beta + \\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2 I)\\), the horseshoe prior on coefficients \\(\\beta_j\\) is specified as:\n\\[\n\\begin{aligned}\n\\beta_j \\mid \\lambda_j, \\tau, \\sigma^2 &\\sim N(0, \\lambda_j^2 \\tau^2 \\sigma^2) \\\\\n\\lambda_j &\\sim \\text{Cauchy}^+(0, 1) \\\\\n\\tau &\\sim \\text{Cauchy}^+(0, 1)\n\\end{aligned}\n\\tag{17.5}\\]\nwhere \\(\\text{Cauchy}^+(0, 1)\\) denotes the half-Cauchy distribution (the positive half of a standard Cauchy). The parameter \\(\\lambda_j\\) is the local shrinkage parameter for coefficient \\(\\beta_j\\), while \\(\\tau\\) is the global shrinkage parameter that controls overall sparsity. The product \\(\\kappa_j = \\lambda_j \\tau\\) determines the effective scale of \\(\\beta_j\\).\nThe half-Cauchy distribution can be represented as a scale mixture itself:\n\\[\n\\lambda_j \\sim \\text{Cauchy}^+(0, 1) \\iff \\lambda_j^2 \\mid \\nu_j \\sim \\text{Inv-Gamma}(1/2, 1/\\nu_j), \\quad \\nu_j \\sim \\text{Inv-Gamma}(1/2, 1)\n\\]\nThis hierarchical representation enables efficient Gibbs sampling for posterior inference, as each conditional distribution has a closed form.\n\n\nShrinkage Properties\nThe horseshoe prior induces a shrinkage factor \\(\\kappa_j(\\beta_j) = E[\\kappa_j \\mid \\beta_j, y]\\) that adapts to the data. For large coefficient values \\(|\\beta_j|\\), the shrinkage factor approaches 1 (no shrinkage), while for small values it approaches 0 (complete shrinkage). This can be seen through the marginal prior:\n\\[\np(\\beta_j \\mid \\tau, \\sigma^2) = \\int_0^{\\infty} N(\\beta_j \\mid 0, \\lambda_j^2 \\tau^2 \\sigma^2) \\cdot \\text{Cauchy}^+(\\lambda_j \\mid 0, 1) d\\lambda_j\n\\]\nThe resulting marginal distribution has extremely heavy tails compared to the Laplace prior used in Lasso. Specifically, the horseshoe has infinite moments, while the Laplace has exponential tails. This heavy-tailed behavior means that large signals are barely regularized, avoiding the systematic bias inherent in \\(\\ell_1\\) penalties.\nThe posterior mean under the horseshoe prior can be approximated as:\n\\[\nE[\\beta_j \\mid y] \\approx (1 - \\kappa_j) \\cdot 0 + \\kappa_j \\cdot \\hat{\\beta}_j^{\\text{OLS}}\n\\]\nwhere \\(\\hat{\\beta}_j^{\\text{OLS}}\\) is the ordinary least squares estimate and \\(\\kappa_j \\in (0,1)\\) is the data-dependent shrinkage factor. Unlike Lasso, where the shrinkage is constant across coefficients, \\(\\kappa_j\\) adapts to the observed magnitude of \\(\\hat{\\beta}_j^{\\text{OLS}}\\).\n\n\nComparison with Other Priors\nThe horseshoe prior occupies a distinctive position in the landscape of sparse priors. Consider the shrinkage behavior as a function of the observed coefficient:\n\nRidge (\\(\\ell_2\\)): Shrinkage factor \\(\\kappa_j = 1/(1 + \\lambda)\\) is constant, providing uniform shrinkage regardless of signal strength\nLasso (\\(\\ell_1\\)): Shrinkage is constant for small coefficients and linear for large ones, \\(\\kappa_j \\approx 1 - \\lambda/|\\beta_j|\\) for large \\(|\\beta_j|\\)\nHorseshoe: Shrinkage factor varies from 0 for noise to nearly 1 for signals, \\(\\kappa_j \\approx (\\lambda_j^2 \\tau^2)/(\\lambda_j^2 \\tau^2 + \\sigma^2)\\)\n\nThe global-local structure separates two tasks: \\(\\tau\\) determines how many coefficients should be non-zero (controlling overall sparsity), while each \\(\\lambda_j\\) determines whether its specific coefficient belongs to the signal or noise group. This separation leads to superior performance in recovering sparse signals compared to the Lasso, particularly when true coefficients vary substantially in magnitude.\n\n\nComputational Implementation\nThe horseshoe prior admits efficient posterior sampling through Gibbs sampling, leveraging its hierarchical structure. The full conditional distributions are:\n\\[\n\\begin{aligned}\n(\\beta \\mid y, \\lambda, \\tau, \\sigma^2) &\\sim N\\left((X'X + D_\\lambda^{-1})^{-1}X'y, \\sigma^2(X'X + D_\\lambda^{-1})^{-1}\\right) \\\\\n(\\sigma^2 \\mid y, \\beta) &\\sim \\text{Inv-Gamma}\\left(\\frac{n}{2}, \\frac{1}{2}\\|y - X\\beta\\|^2\\right) \\\\\n(\\lambda_j^2 \\mid \\beta_j, \\tau, \\sigma^2, \\nu_j) &\\sim \\text{Inv-Gamma}\\left(1, \\frac{1}{\\nu_j} + \\frac{\\beta_j^2}{2\\tau^2\\sigma^2}\\right) \\\\\n(\\tau^2 \\mid \\beta, \\lambda, \\sigma^2, \\xi) &\\sim \\text{Inv-Gamma}\\left(\\frac{p+1}{2}, \\frac{1}{\\xi} + \\frac{1}{2\\sigma^2}\\sum_{j=1}^p \\frac{\\beta_j^2}{\\lambda_j^2}\\right)\n\\end{aligned}\n\\]\nwhere \\(D_\\lambda = \\text{diag}(\\lambda_1^2\\tau^2, \\ldots, \\lambda_p^2\\tau^2)\\) and \\(\\nu_j, \\xi\\) are auxiliary variables from the scale mixture representation of the half-Cauchy.\n\nExample 17.6 (Horseshoe Prior for Sparse Regression) We demonstrate the horseshoe prior’s ability to recover sparse signals in a high-dimensional regression setting. Consider a scenario with \\(n = 100\\) observations and \\(p = 50\\) predictors, where only 5 coefficients are truly non-zero with varying magnitudes.\n\n\nHorseshoe Prior for Sparse Regression\n#| warning: false\n#| message: false\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate sparse regression data\nn &lt;- 100  # number of observations\np &lt;- 50   # number of predictors\ns &lt;- 5    # number of non-zero coefficients\n\n# True sparse coefficient vector\nbeta_true &lt;- rep(0, p)\nbeta_true[1:s] &lt;- c(50, -30, 15, -4, 3.5)  # Varying magnitudes\n\n# Generate design matrix (standardized)\nX &lt;- matrix(rnorm(n * p), n, p)\nX &lt;- scale(X)\n\n# Generate response with noise\nsigma_true &lt;- 1\ny &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\ny &lt;- as.vector(y)\n\n# Horseshoe Gibbs sampler\nhorseshoe_gibbs &lt;- function(y, X, n_iter = 5000, burn_in = 1000) {\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Initialize parameters\n  beta &lt;- rep(0, p)\n  lambda_sq &lt;- rep(1, p)\n  tau_sq &lt;- 1\n  sigma_sq &lt;- 1\n  nu &lt;- rep(1, p)\n  xi &lt;- 1\n  \n  # Storage for samples\n  beta_samples &lt;- matrix(0, n_iter - burn_in, p)\n  tau_sq_samples &lt;- numeric(n_iter - burn_in)\n  \n  # Pre-compute X'X and X'y\n  XtX &lt;- crossprod(X)\n  Xty &lt;- crossprod(X, y)\n  \n  for (iter in 1:n_iter) {\n    # Sample beta from full conditional\n    D_lambda_inv &lt;- diag(1 / (lambda_sq * tau_sq))\n    V_beta &lt;- solve(XtX + D_lambda_inv)\n    mu_beta &lt;- V_beta %*% Xty\n    beta &lt;- mu_beta + t(chol(sigma_sq * V_beta)) %*% rnorm(p)\n    \n    # Sample sigma^2\n    resid &lt;- y - X %*% beta\n    sigma_sq &lt;- 1 / rgamma(1, n/2, sum(resid^2)/2)\n    \n    # Sample lambda_j^2 for each j\n    for (j in 1:p) {\n      lambda_sq[j] &lt;- 1 / rgamma(1, 1, 1/nu[j] + beta[j]^2/(2*tau_sq*sigma_sq))\n    }\n    \n    # Sample nu_j (auxiliary for half-Cauchy)\n    for (j in 1:p) {\n      nu[j] &lt;- 1 / rgamma(1, 1, 1 + 1/lambda_sq[j])\n    }\n    \n    # Sample tau^2 (global shrinkage)\n    tau_sq &lt;- 1 / rgamma(1, (p+1)/2, 1/xi + sum(beta^2/lambda_sq)/(2*sigma_sq))\n    \n    # Sample xi (auxiliary for tau)\n    xi &lt;- 1 / rgamma(1, 1, 1 + 1/tau_sq)\n    \n    # Store samples after burn-in\n    if (iter &gt; burn_in) {\n      beta_samples[iter - burn_in, ] &lt;- beta\n      tau_sq_samples[iter - burn_in] &lt;- tau_sq\n    }\n  }\n  \n  list(beta = beta_samples, tau_sq = tau_sq_samples)\n}\n\n\n\n\nFit Horseshoe model\nhs_fit &lt;- horseshoe_gibbs(y, X, n_iter = 5000, burn_in = 1000)\n# Compute posterior means\nbeta_hs &lt;- colMeans(hs_fit$beta)\ntau_hs &lt;- mean(hs_fit$tau_sq)\n\n# Compare with Lasso\nlibrary(glmnet)\nlasso_fit &lt;- cv.glmnet(X, y, alpha = 1)\nbeta_lasso &lt;- as.vector(coef(lasso_fit, s = \"lambda.min\"))[-1]\n\n\n\n\n\n\n\n\n\n\nFigure 17.5: Comparison of true, Horseshoe, and Lasso estimates\n\n\n\n\n\nFigure 17.5 reveals the distinctive shrinkage behaviors of the horseshoe prior compared to the Lasso penalty. The figure displays coefficient estimates (vertical axis) across all 50 predictors (horizontal axis), with black points indicating the true sparse signal where only the first five coefficients are non-zero with magnitudes ranging from 50 to -30. The horseshoe estimates (blue) exhibit a remarkable ability to preserve the large true coefficients nearly intact while aggressively shrinking the truly zero coefficients toward zero, demonstrating minimal bias for strong signals. In contrast, the Lasso estimates (red) systematically undershrink the large coefficients while leaving small spurious non-zero estimates scattered across many truly null predictors. This comparison illustrates the horseshoe prior’s theoretical advantage: its aggressive shrinkage of noise parameters combined with minimal penalization of signal parameters, resulting from the heavy tails of the half-Cauchy prior that allow large coefficients to escape shrinkage while the sharp peak near zero effectively eliminates noise.\nThe quantitative comparison in Table 17.1 confirms the theoretical advantages of the horseshoe prior observed in Figure 17.5. The horseshoe achieves substantially lower coefficient MSE, reflecting its superior recovery of the true sparse signal structure through minimal bias on large coefficients and aggressive shrinkage of noise, while both methods perform reasonably well for prediction MSE since the Lasso’s systematic bias in coefficient estimates can be partially compensated by its lower variance in zero coefficients. The horseshoe’s adaptive shrinkage through the heavy-tailed half-Cauchy prior provides better overall performance across both metrics, more accurately capturing the underlying signal while effectively suppressing noise. The global shrinkage parameter \\(\\tau\\) is automatically learned from the data, with posterior mean \\(\\tau^2 \\approx\\) 0.005, reflecting the appropriate level of sparsity and effectively performing automatic model selection without cross-validation.\n\n\n\n\nTable 17.1: Comparison of Coefficient and Prediction Mean Squared Errors\n\n\n\n\n\n\nMethod\nCoefficient.MSE\nPrediction.MSE\n\n\n\n\nHorseshoe\n0.00\n0.52\n\n\nLasso\n0.05\n3.17\n\n\n\n\n\n\n\n\nThe example above shows the horseshoe prior with manual Gibbs sampling implementation for pedagogical purposes. In practice, the horseshoe package provides efficient implementations of horseshoe regression that are easier to use. Below we demonstrate a practical application with \\(n = 50\\) observations and \\(p = 100\\) predictors, where only 10 coefficients have true effects.\n\nlibrary(horseshoe)\n# Create a 50 by 100 design matrix X\nX2 &lt;- matrix(rnorm(50*100), 50)\n# True betas: first 10 are 6 (signals), rest are 0 (noise)\nbeta2 &lt;- c(rep(6, 10), rep(0, 90))\n# Generate response\ny2 &lt;- X2 %*% beta2 + rnorm(50)\n# Fit Horseshoe model\ninvisible(capture.output(\n  hs.object &lt;- horseshoe(y2, X2, method.tau = \"truncatedCauchy\", method.sigma =\"Jeffreys\")\n));\n\n\n\nCode\n# Visualize Results\ndf &lt;- data.frame(index = 1:100,\n                 truth = beta2,\n                 post.mean = hs.object$BetaHat,\n                 lower.CI = hs.object$LeftCI,\n                 upper.CI = hs.object$RightCI\n                 )\n\nlibrary(ggplot2)\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2, aes(color = \"Truth\")) + \n  theme_classic() + ylab(\"Coefficient Value\") +\n  geom_point(aes(x = index, y = post.mean, color = \"Estimate\"), size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, color = \"Estimate\"), width = .1) +\n  scale_color_manual(values = c(\"Truth\" = \"black\", \"Estimate\" = \"red\")) +\n  ggtitle(\"Horseshoe Estimates: Retrieving Signals from Noise\")\n\n\n\n\n\n\n\n\nFigure 17.6: Horseshoe prior for sparse regression using horseshoe package\n\n\n\n\n\nFigure 17.6 demonstrates the Horseshoe prior’s remarkable ability to distinguish signal from noise in high-dimensional settings. The first 10 coefficients (the true signals with \\(\\beta_j = 6\\)) are accurately estimated with tight credible intervals around the true values, shown in red. Meanwhile, the remaining 90 noise coefficients (true \\(\\beta_j = 0\\)) are strongly shrunk toward zero, with their credible intervals tightly concentrated near the origin. This behavior exemplifies the Horseshoe’s “selective shrinkage” property: it applies minimal shrinkage to large coefficients (allowing signals to remain unbiased) while aggressively shrinking small coefficients toward zero (effectively removing noise). This makes the Horseshoe particularly well-suited for sparse recovery problems where the number of predictors greatly exceeds the sample size.\nWe can also perform variable selection by checking if the credible intervals exclude zero.\n\n\n\nComparison and Package Implementation\nTo illustrate the shrinkage behaviour of the horseshoe, let’s plot the posterior mean for \\(\\beta_i\\) as a function of \\(y_i\\) for three different values of \\(\\tau\\).\n\n\nCode\nlibrary(horseshoe)\nlibrary(ggplot2)\ntau.values &lt;- c(0.005, 0.05, 0.5)\ny.values &lt;- seq(-5, 5, length = 100)\ndf &lt;- data.frame(tau = rep(tau.values, each = length(y.values)),\ny = rep(y.values, 3),\npost.mean = c(HS.post.mean(y.values, tau = tau.values[1], Sigma2=1), \n              HS.post.mean(y.values, tau = tau.values[2], Sigma2=1), \n              HS.post.mean(y.values, tau = tau.values[3], Sigma2=1)) )\n\nggplot(data = df, aes(x = y, y = post.mean, group = tau, color = factor(tau))) + \n  geom_line(linewidth = 1.5) + \n  scale_color_brewer(palette=\"Dark2\") + \n  geom_abline(lty = 2) + geom_hline(yintercept = 0, colour = \"grey\") + \n  theme_classic() + ylab(\"\") + labs(color = \"Tau\") \n\n\n\n\n\n\n\n\nFigure 17.7: Horseshoe posterior mean for three values of tau\n\n\n\n\n\nSmaller values of \\(\\tau\\) lead to stronger shrinkage behaviour of the horseshoe. Observations that are in absolute value at most equal to \\(\\sqrt{2\\sigma^2\\log(1/\\tau)}\\) are shrunk to values close to zero (Van der Pas et al (2014)). For larger observed values, the horseshoe posterior mean will tend to the identity (that is, barely any shrinkage, the estimate will be very close to the observed value). The optimal value of \\(\\tau\\) is the proportion of true signals. This value is typically not known in practice but can be estimated.\n\nThe normal means problem\nThe normal means model is: \\[\ny_i = \\beta_i + \\varepsilon_i, \\quad i = 1, \\ldots, n,\n\\] with \\(\\varepsilon_i\\) i.i.d. \\(\\mathcal{N}(0, \\sigma^2/n_i)\\). first, we will be computing the posterior mean only, with known variance \\(\\sigma^2\\). The function HS.post.mean computes the posterior mean of \\((\\beta_1, \\ldots, \\beta_n)\\).\nAs an example, we generate 50 data points, the first 10 of which are coming from true signals. The first 10 \\(\\beta_i\\)’s are equal to five and the remaining \\(\\beta_i\\)’s are equal to zero. Then, we estimate \\(\\tau\\) using the MMLE and use it to find the posterior mean (red). Finally, we use the function HS.normal.means to compute posterior means and credible intervals using MCMC.\n\nCode\ndf &lt;- data.frame(index = 1:50, truth &lt;- c(rep(5, 10), rep(0, 40)),\n                 y &lt;- truth + rnorm(50)) # observations\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\")\n\ntau.est &lt;- HS.MMLE(df$y, Sigma2 = 1)\npost.mean &lt;- HS.post.mean(df$y, tau.est, 1)\ndf$post.mean &lt;- post.mean\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) + \n  geom_point(aes(x = index, y = y), size = 2, col = \"blue\") +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean), size = 2, col = \"red\")\n\ninvisible(capture.output(hs.object &lt;- HS.normal.means(df$y, method.tau = \"truncatedCauchy\", method.sigma = \"Jeffreys\")));\ndf$post.mean.full &lt;- hs.object$BetaHat\ndf$lower.CI &lt;- hs.object$LeftCI\ndf$upper.CI &lt;- hs.object$RightCI\ndf$selected.CI &lt;- HS.var.select(hs.object, df$y, method = \"intervals\")\n\nggplot(data = df, aes(x = index, y = truth)) + \n  geom_point(size = 2) +\n  theme_classic() + ylab(\"\") +\n  geom_point(aes(x = index, y = post.mean.full, col = factor(selected.CI)), size = 2) +\n  geom_errorbar(aes(ymin = lower.CI, ymax = upper.CI, col = factor(selected.CI)), width = .1) +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Truth vs observations\n\n\n\n\n\n\n\n\n\n\n\n(b) Horseshoe posterior mean estimates\n\n\n\n\n\n\n\n\n\n\n\n(c) Posterior mean with 95% credible intervals\n\n\n\n\n\n\n\nFigure 17.8: Normal means problem. Black = truth, Blue = observations, Red = estimates\n\n\n\nThe three panels in Figure 17.8 illustrate the horseshoe prior’s effectiveness in sparse estimation. Panel (a) shows the challenge: noisy observations (blue) obscure the true sparse signal structure (black), where only the first 10 parameters are non-zero. Panel (b) demonstrates the horseshoe posterior mean estimates (red), which successfully identify the signal locations while shrinking noise toward zero. The horseshoe’s adaptive shrinkage becomes most apparent in panel (c), where the full Bayesian analysis provides posterior means with credible intervals. Notice how the horseshoe correctly identifies the signal region (shown in teal) with narrow credible intervals that exclude zero, while the noise region (shown in red) has intervals that include zero. This automatic variable selection through credible intervals exemplifies the horseshoe’s oracle-like behavior: it simultaneously performs strong shrinkage on noise components while leaving true signals largely unshrunk, achieving near-optimal estimation without knowing the true sparsity level in advance.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#bridge-ell_alpha",
    "href": "17-theoryai.html#bridge-ell_alpha",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.11 Bridge (\\(\\ell_{\\alpha}\\))",
    "text": "17.11 Bridge (\\(\\ell_{\\alpha}\\))\nThe bridge estimator represents a powerful generalization that unifies many popular regularization approaches, bridging the gap between subset selection (\\(\\ell_0\\)) and Lasso (\\(\\ell_1\\)) penalties. For the regression model \\(y = X\\beta + \\epsilon\\) with unknown vector \\(\\beta = (\\beta_1, \\ldots, \\beta_p)'\\), the bridge estimator minimizes:\n\\[\nQ_y(\\beta) = \\frac{1}{2} \\|y - X\\beta\\|^2 + \\lambda \\sum_{j=1}^p |\\beta_j|^\\alpha\n\\tag{17.6}\\]\nwhere \\(\\alpha \\in (0,2]\\) is the bridge parameter and \\(\\lambda &gt; 0\\) controls the regularization strength. This penalty interpolates between different sparsity-inducing behaviors. As \\(\\alpha \\to 0\\), the penalty approaches best subset selection (\\(\\ell_0\\)); when \\(\\alpha = 1\\), it reduces to the Lasso penalty (\\(\\ell_1\\)); and when \\(\\alpha = 2\\), it becomes the Ridge penalty (\\(\\ell_2\\)). The bridge penalty is non-convex when \\(0 &lt; \\alpha &lt; 1\\), making optimization challenging but providing superior theoretical properties. Specifically, when \\(\\alpha &lt; 1\\), the penalty is concave over \\((0,\\infty)\\), leading to the oracle property under certain regularity conditions—the ability to identify the true sparse structure and estimate non-zero coefficients as efficiently as if the true model were known.\n\nBayesian Framework and Data Augmentation\nFrom a Bayesian perspective, the bridge estimator corresponds to the MAP estimate under an exponential-power prior. The Bayesian bridge model treats \\(p(\\beta \\mid y) \\propto \\exp\\{-Q_y(\\beta)\\}\\) as a posterior distribution, arising from assuming a Gaussian likelihood for \\(y\\) and independent exponential-power priors: \\[\np(\\beta_j \\mid \\alpha, \\tau) = \\frac{\\alpha}{2\\tau \\Gamma(1 + 1/\\alpha)} \\exp\\left(-\\left|\\frac{\\beta_j}{\\tau}\\right|^\\alpha\\right)\n\\tag{17.7}\\]\nwhere \\(\\tau = \\lambda^{-1/\\alpha}\\) is the scale parameter. The Bayesian framework offers compelling advantages over classical bridge estimation. Rather than providing only a point estimate, it yields the full posterior distribution, enabling uncertainty quantification and credible intervals. The regularization parameter \\(\\lambda\\) can be learned from the data through hyperpriors, avoiding cross-validation. Most importantly, the bridge posterior is often multimodal, especially with correlated predictors, and MCMC naturally explores all modes while optimization may get trapped in local optima.\nPosterior inference for the Bayesian bridge is facilitated by two key data augmentation representations. The first represents the exponential-power prior as a scale mixture of normals using Bernstein’s theorem: \\(\\exp(-|t|^{\\alpha}) = \\int_0^{\\infty} e^{-s t^2/2} g(s) \\, ds\\), where \\(g(s)\\) is the density of a positive \\(\\alpha/2\\)-stable random variable. However, the conditional posterior for the mixing variables becomes an exponentially tilted stable distribution, which lacks a closed form and requires specialized sampling algorithms.\nA novel alternative representation avoids stable distributions by expressing the exponential-power prior as a scale mixture of triangular (Bartlett-Fejer) kernels: \\[\n\\begin{aligned}\n(y \\mid \\beta, \\sigma^2) &\\sim N(X\\beta, \\sigma^2 I) \\\\\np(\\beta_j \\mid \\tau, \\omega_j, \\alpha) &= \\frac{1}{\\tau \\omega_j^{1/\\alpha}} \\left\\{ 1 - \\left| \\frac{\\beta_j}{\\tau \\omega_j^{1/\\alpha}} \\right| \\right\\}_+ \\\\\n(\\omega_j \\mid \\alpha) &\\sim \\frac{1+\\alpha}{2} \\cdot \\text{Gamma}(2+1/\\alpha,1) + \\frac{1-\\alpha}{2} \\cdot \\text{Gamma}(1+1/\\alpha,1)\n\\end{aligned}\n\\tag{17.8}\\]\nwhere \\(\\{a\\}_+ = \\max(a,0)\\). This mixture of gamma distributions is much simpler to sample from and naturally captures the bimodality of the bridge posterior through its two-component structure. The choice of representation depends on the design matrix structure: the Bartlett-Fejer representation is 2-3 times more efficient for orthogonal designs, while the scale mixture of normals performs better for collinear designs.\nThe bridge prior with \\(\\alpha &lt; 1\\) satisfies the oracle property under regularity conditions, correctly identifying the true sparsity pattern while avoiding over-shrinkage of large signals through its heavier-than-exponential tails and redescending score function. The Bartlett-Fejer representation enables an efficient Gibbs sampler that marginalizes over local scale parameters, achieving excellent mixing properties, though a distinctive feature is the posterior’s multimodality with correlated predictors, which reflects genuine model uncertainty that the Bayesian approach properly accounts for by averaging over all modes. Extensive simulations demonstrate dramatic improvements over classical methods: with \\(p = 100\\), \\(n = 101\\), and \\(\\alpha = 0.5\\), mean squared error was 2254 for least squares, 1611 for classical bridge, and only 99 for Bayesian bridge, with similar gains on benchmark datasets often exceeding 50% reduction in prediction error. Practical guidelines recommend \\(\\alpha \\in [0.5, 0.8]\\) or learning from data with a uniform prior, using Gamma(2,2) for the global scale \\(\\tau\\), selecting the Bartlett-Fejer representation for nearly orthogonal designs and scale mixture of normals for collinear predictors, and examining posterior multimodality to understand model uncertainty. The framework extends naturally to other likelihoods and is implemented in the R package BayesBridge, occupying a unique position in the regularization landscape by providing less bias than Lasso for large coefficients while maintaining sparsity, achieving variable selection unlike ridge regression, and offering computational advantages over spike-and-slab priors, making it an excellent choice for modern high-dimensional inference problems.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#full-bayes-for-sparsity-shrinkage",
    "href": "17-theoryai.html#full-bayes-for-sparsity-shrinkage",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.12 Full Bayes for Sparsity Shrinkage",
    "text": "17.12 Full Bayes for Sparsity Shrinkage\nThus far we have considered penalized optimization approaches that yield point estimates through maximum a posteriori (MAP) estimation. While these methods provide computationally efficient solutions for high-dimensional problems, they do not fully quantify uncertainty about which variables should be included in the model. In this section, we explore full Bayesian approaches to variable selection that compute the complete posterior distribution over both parameters and model structure, allowing us to assess uncertainty about sparsity patterns themselves.\n\nSpike-and-Slab Prior\nThe gold standard for Bayesian variable selection are spike-and-slab priors, also known as Bernoulli-Gaussian mixtures. These priors provide full model uncertainty quantification by explicitly modeling which variables should be included. Consider a linear regression problem \\[\ny = \\beta_1x_1+\\ldots+\\beta_px_p + e \\ , \\ \\  \\text{where } e \\sim N(0, \\sigma^2),~-\\infty \\le \\beta_i \\le \\infty \\ .\n\\] Our goal is to identify which input variables \\(x_i\\) should be included in the model, seeking a sparse solution where \\(\\|\\beta\\|_0 = k \\ll p\\), with \\(\\|\\beta\\|_0 \\defeq \\#\\{i : \\beta_i\\neq0\\}\\) denoting the cardinality of the support of \\(\\beta\\), also known as the \\(\\ell_0\\) (pseudo)norm. While continuous shrinkage priors like the Lasso and horseshoe adaptively shrink coefficients, spike-and-slab priors take a more explicit approach by placing positive probability mass at exactly zero.\nUnder spike-and-slab, each \\(\\beta_i\\) exchangeably follows a mixture prior consisting of \\(\\delta_0\\), a point mass at \\(0\\), and a Gaussian distribution centered at zero. Hence we write,\n\\[\n\\label{eqn:ss}\n\\beta_i | \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\] Here \\(\\theta\\in \\left(0, 1\\right)\\) controls the overall sparsity in \\(\\beta\\) and \\(\\sigma_\\beta^2\\) accommodates non-zero signals. This family is termed as the Bernoulli-Gaussian mixture model in the signal processing community.\nA useful re-parameterization, the parameters \\(\\beta\\) is given by two independent random variable vectors \\(\\gamma = \\left(\\gamma_1, \\ldots, \\gamma_p\\right)'\\) and \\(\\alpha = \\left(\\alpha_1, \\ldots, \\alpha_p\\right)'\\) such that \\(\\beta_i  =  \\gamma_i\\alpha_i\\), with probabilistic structure \\[\n\\label{eq:bg}\n\\begin{array}{rcl}\n\\gamma_i\\mid\\theta & \\sim & \\text{Bernoulli}(\\theta) \\ ;\n\\\\\n\\alpha_i \\mid \\sigma_\\beta^2 &\\sim & N\\left(0, \\sigma_\\beta^2\\right) \\ .\n\\\\\n\\end{array}\n\\] Since \\(\\gamma_i\\) and \\(\\alpha_i\\) are independent, the joint prior density becomes \\[\np\\left(\\gamma_i, \\alpha_i \\mid \\theta, \\sigma_\\beta^2\\right) =\n\\theta^{\\gamma_i}\\left(1-\\theta\\right)^{1-\\gamma_i}\\frac{1}{\\sqrt{2\\pi}\\sigma_\\beta}\\exp\\left\\{-\\frac{\\alpha_i^2}{2\\sigma_\\beta^2}\\right\\}\n\\ , \\ \\ \\ \\text{for } 1\\leq i\\leq p \\ .\n\\] The indicator \\(\\gamma_i\\in \\{0, 1\\}\\) can be viewed as a dummy variable to indicate whether \\(\\beta_i\\) is included in the model.\nLet \\(S = \\{i: \\gamma_i = 1\\} \\subseteq \\{1, \\ldots, p\\}\\) be the “active set” of \\(\\gamma\\), and \\(\\|\\gamma\\|_0 = \\sum\\limits_{i = 1}^p\\gamma_i\\) be its cardinality. The joint prior on the vector \\(\\{\\gamma, \\alpha\\}\\) then factorizes as \\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right) & = & \\prod\\limits_{i = 1}^p p\\left(\\alpha_i, \\gamma_i \\mid \\theta, \\sigma_\\beta^2\\right) \\\\\n& = &\n\\theta^{\\|\\gamma\\|_0}\n\\left(1-\\theta\\right)^{p - \\|\\gamma\\|_0}\n\\left(2\\pi\\sigma_\\beta^2\\right)^{-\\frac p2}\\exp\\left\\{-\\frac1{2\\sigma_\\beta^2}\\sum\\limits_{i = 1}^p\\alpha_i^2\\right\\} \\ .\n\\end{array}\n\\]\nLet \\(X_\\gamma \\defeq \\left[X_i\\right]_{i \\in S}\\) be the set of “active explanatory variables\" and \\(\\alpha_\\gamma \\defeq \\left(\\alpha_i\\right)'_{i \\in S}\\) be their corresponding coefficients. We can write \\(X\\beta = X_\\gamma \\alpha_\\gamma\\). The likelihood can be expressed in terms of \\(\\gamma\\), \\(\\alpha\\) as \\[\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\n=\n\\left(2\\pi\\sigma_e^2\\right)^{-\\frac n2}\n\\exp\\left\\{\n-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n\\right\\} \\ .\n\\]\nUnder this re-parameterization by \\(\\left\\{\\gamma, \\alpha\\right\\}\\), the posterior is given by\n\\[\n\\begin{array}{rcl}\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y\\right) & \\propto &\np\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2\\right)\np\\left(y \\mid \\gamma, \\alpha, \\theta, \\sigma_e^2\\right)\\\\\n& \\propto &\n\\exp\\left\\{-\\frac1{2\\sigma_e^2}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n-\\frac1{2\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n-\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0\n\\right\\} \\ .\n\\end{array}\n\\] Our goal then is to find the regularized maximum a posterior (MAP) estimator \\[\n\\arg\\max\\limits_{\\gamma, \\alpha}p\\left(\\gamma, \\alpha \\mid \\theta, \\sigma_\\beta^2, \\sigma_e^2, y \\right) \\ .\n\\] By construction, the \\(\\gamma\\) \\(\\in\\left\\{0, 1\\right\\}^p\\) will directly perform variable selection. Spike-and-slab priors, on the other hand, will sample the full posterior and calculate the posterior probability of variable inclusion. Finding the MAP estimator is equivalent to minimizing over \\(\\left\\{\\gamma, \\alpha\\right\\}\\) the regularized least squares objective function\n\\[\n\\min\\limits_{\\gamma, \\alpha}\\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2\n+ 2\\sigma_e^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\n\\left\\|\\gamma\\right\\|_0 \\ .\n\\tag{17.9}\\] This objective possesses several interesting properties:\n\nThe first term is essentially the least squares loss function.\nThe second term looks like a ridge regression penalty and has connection with the signal-to-noise ratio (SNR) \\(\\sigma_\\beta^2/\\sigma_e^2\\). Smaller SNR will be more likely to shrink the estimates towards \\(0\\). If \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the prior uncertainty on the size of non-zero coefficients is much larger than the noise level, that is, the SNR is sufficiently large, this term can be ignored. This is a common assumption in spike-and-slab framework in that people usually want \\(\\sigma_\\beta \\to \\infty\\) or to be “sufficiently large\" in order to avoid imposing harsh shrinkage to non-zero signals.\nIf we further assume that \\(\\theta &lt; \\frac12\\), meaning that the coefficients are known to be sparse a priori, then \\(\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0\\), and the third term can be seen as an \\(\\ell_0\\) regularization.\n\nTherefore, our Bayesian objective inference is connected to \\(\\ell_0\\)-regularized least squares, which we summarize in the following proposition.\n(Spike-and-slab MAP & \\(\\ell_0\\) regularization)\nFor some \\(\\lambda &gt; 0\\), assuming \\(\\theta &lt; \\frac12\\), \\(\\sigma_\\beta^2 \\gg \\sigma_e^2\\), the Bayesian MAP estimate defined by Equation 17.9 is equivalent to the \\(\\ell_0\\) regularized least squares objective, for some \\(\\lambda &gt; 0\\), \\[\n\\min_{\\beta}\n\\frac12\\left\\|y - X\\beta\\right\\|_2^2\n+ \\lambda\n\\left\\|\\beta\\right\\|_0 \\ .\n\\tag{17.10}\\]\nFirst, assuming that \\[\n\\theta &lt; \\frac12, \\ \\ \\  \\sigma_\\beta^2 \\gg \\sigma_e^2, \\ \\ \\  \\frac{\\sigma_e^2}{\\sigma_\\beta^2}\\left\\|\\alpha\\right\\|_2^2 \\to 0 \\ ,\n\\] gives us an objective function of the form \\[\n\\min_{\\gamma, \\alpha}\n\\frac12 \\left\\|y - X_\\gamma \\alpha_\\gamma\\right\\|_2^2\n+ \\lambda\n\\left\\|\\gamma\\right\\|_0,  \\ \\ \\ \\  \\text{where } \\lambda \\defeq \\sigma_e^2\\log\\left(\\left(1-\\theta\\right) / \\theta\\right) &gt; 0 \\ .\n\\tag{17.11}\\]\nThis can be seen as a variable selection version of equation Equation 17.10. To show this, we need only to check that the optimal solution corresponds to a feasible solution and vice versa. This is explained as follows.\nOn the one hand, assuming \\(\\hat\\beta\\) is an optimal solution, then we can correspondingly define \\(\\hat\\gamma_i \\defeq I\\left\\{\\hat\\beta_i \\neq 0\\right\\}\\), \\(\\hat\\alpha_i \\defeq \\hat\\beta_i\\), such that \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is feasible and gives the same objective value as \\(\\hat\\beta\\)..\nOn the other hand, assuming \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\) is optimal, implies that we must have all of the elements in \\(\\hat\\alpha_\\gamma\\) should be non-zero, otherwise a new \\(\\tilde\\gamma_i \\defeq I\\left\\{\\hat\\alpha_i \\neq 0\\right\\}\\) will give a lower objective value. As a result, if we define \\(\\hat\\beta_i \\defeq \\hat\\gamma_i\\hat\\alpha_i\\), \\(\\hat\\beta\\) will be feasible and gives the same objective value as \\(\\left\\{\\hat\\gamma, \\hat\\alpha\\right\\}\\).",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#subset-selection-ell_0-norm",
    "href": "17-theoryai.html#subset-selection-ell_0-norm",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.13 Subset Selection (\\(\\ell_0\\) Norm)",
    "text": "17.13 Subset Selection (\\(\\ell_0\\) Norm)\nThe \\(\\ell_0\\) norm directly counts the number of non-zero parameters, making it the most natural penalty for variable selection. However, \\(\\ell_0\\)-regularized optimization problems are NP-hard due to their combinatorial nature. The optimization problem is:\n\\[\n\\min_{\\beta} \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_0\n\\]\nwhere \\(\\|\\beta\\|_0 = \\#\\{j : \\beta_j \\neq 0\\}\\) is the number of non-zero coefficients. This directly penalizes model complexity by limiting the number of active predictors.\n\nConnection to Spike-and-Slab Priors\nA remarkable connection exists between Bayesian spike-and-slab priors and \\(\\ell_0\\) regularization. Consider the spike-and-slab prior where each coefficient follows:\n\\[\n\\beta_j \\mid \\theta, \\sigma_\\beta^2 \\sim (1-\\theta)\\delta_0 + \\theta N(0, \\sigma_\\beta^2)\n\\]\nHere \\(\\theta \\in (0,1)\\) controls the sparsity level and \\(\\sigma_\\beta^2\\) governs the size of non-zero coefficients. This can be reparametrized using indicator variables \\(\\gamma_j \\in \\{0,1\\}\\) and continuous coefficients \\(\\alpha_j\\):\n\\[\n\\begin{aligned}\n\\beta_j &= \\gamma_j \\alpha_j \\\\\n\\gamma_j \\mid \\theta &\\sim \\text{Bernoulli}(\\theta) \\\\\n\\alpha_j \\mid \\sigma_\\beta^2 &\\sim N(0, \\sigma_\\beta^2)\n\\end{aligned}\n\\]\nThe maximum a posteriori (MAP) estimator under this prior yields the objective:\n\\[\n\\min_{\\gamma, \\alpha} \\|y - X_\\gamma \\alpha_\\gamma\\|_2^2 + \\frac{\\sigma^2}{\\sigma_\\beta^2}\\|\\alpha\\|_2^2 + 2\\sigma^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\\|\\gamma\\|_0\n\\]\nwhere \\(X_\\gamma\\) contains only the columns corresponding to \\(\\gamma_j = 1\\). Under the assumptions \\(\\theta &lt; 1/2\\) (favoring sparsity) and \\(\\sigma_\\beta^2 \\gg \\sigma^2\\) (weak shrinkage on non-zero coefficients), this reduces to the \\(\\ell_0\\)-regularized least squares with \\(\\lambda = 2\\sigma^2\\log\\left(\\frac{1-\\theta}{\\theta}\\right)\\).\n\n\nSingle Best Replacement (SBR) Algorithm\nSince exact \\(\\ell_0\\) optimization is intractable, practical algorithms focus on finding good local optima. The Single Best Replacement (SBR) algorithm addresses the fundamental challenge in sparse regression: finding the optimal subset of predictors when the search space is exponentially large. For \\(p\\) predictors, there are \\(2^p\\) possible subsets to consider, making exhaustive search computationally prohibitive for even moderate \\(p\\).\nRather than searching over all possible coefficient vectors \\(\\beta\\), SBR reformulates the \\(\\ell_0\\)-regularized problem as a discrete optimization over active sets \\(S \\subseteq \\{1,2,\\ldots,p\\}\\):\n\\[\n\\min_{S} f(S) = \\frac{1}{2}\\|y - X_S \\hat{\\beta}_S\\|_2^2 + \\lambda |S|\n\\]\nwhere \\(\\hat{\\beta}_S = (X_S^T X_S)^{-1} X_S^T y\\) is the least squares solution on the active set \\(S\\). This reformulation creates a natural bias-variance tradeoff where larger models with bigger active sets reduce bias but increase the penalty, while smaller models reduce the penalty but may increase bias.\nThe SBR algorithm operates through a systematic iterative process. The initialization phase begins with an empty active set \\(S_0 = \\emptyset\\), computes the initial objective \\(f(S_0) = \\frac{1}{2}\\|y\\|_2^2\\) (corresponding to no predictors), and sets the iteration counter \\(k = 0\\).\nThe main iteration loop proceeds as follows for each iteration \\(k\\). During candidate generation, the algorithm considers each variable \\(j \\in \\{1,\\ldots,p\\}\\) and defines the single replacement operation: \\[S_k \\cdot j = \\begin{cases}\nS_k \\cup \\{j\\} & \\text{if } j \\notin S_k \\text{ (addition)} \\\\\nS_k \\setminus \\{j\\} & \\text{if } j \\in S_k \\text{ (removal)}\n\\end{cases}\\]\nFor objective evaluation, each candidate \\(S_k \\cdot j\\) is assessed by computing: \\[f(S_k \\cdot j) = \\frac{1}{2}\\|y - X_{S_k \\cdot j} \\hat{\\beta}_{S_k \\cdot j}\\|_2^2 + \\lambda |S_k \\cdot j|\\]\nThe best replacement selection identifies: \\[j^* = \\arg\\min_{j \\in \\{1,\\ldots,p\\}} f(S_k \\cdot j)\\]\nFinally, the improvement check determines whether to accept the move: if \\(f(S_k \\cdot j^*) &lt; f(S_k)\\), the algorithm accepts the move and sets \\(S_{k+1} = S_k \\cdot j^*\\); otherwise, it stops and returns \\(S_k\\) as the final solution.\nUnlike pure forward selection which only adds variables or backward elimination which only removes variables, SBR can both add and remove variables at each step. This bidirectionality provides substantial advantages. The algorithm can escape local optima by correcting early mistakes through the removal of previously selected variables. When variables are correlated, the algorithm can swap between equivalent predictors to find better solutions. Additionally, the adaptive model size capability allows the algorithm to both grow and shrink the model as needed during the optimization process.\nWhen compared with standard stepwise methods, the advantages become clear. Forward selection uses greedy addition only and can become trapped if early selections are poor. Backward elimination starts with the full model, making it computationally expensive for large \\(p\\). Traditional forward-backward approaches use separate forward and backward phases, while SBR provides a unified framework that considers both additions and removals at each step.\nThe key computational challenge lies in evaluating \\(f(S \\cdot j)\\) for all \\(p\\) variables at each iteration, as naive implementation would require \\(p\\) separate least squares computations per iteration. Efficient matrix updates provide the solution to this challenge.\nFor the addition case where \\(j \\notin S\\), adding variable \\(j\\) to active set \\(S\\) employs rank-one updates to the Cholesky decomposition. If \\(X_S^T X_S = L_S L_S^T\\), then updating for \\(X_{S \\cup \\{j\\}}^T X_{S \\cup \\{j\\}}\\) requires only \\(O(|S|^2)\\) operations instead of \\(O(|S|^3)\\). Similarly, for the removal case where \\(j \\in S\\), removing variable \\(j\\) from active set \\(S\\) uses rank-one downdates to the Cholesky decomposition with similar \\(O(|S|^2)\\) complexity.\nThe overall computational complexity analysis reveals that each iteration requires \\(O(p|S|^2)\\) operations where \\(|S|\\) is the current active set size, the total number of iterations is typically \\(O(|S_{final}|)\\) in practice, and the overall complexity becomes \\(O(p|S|^3)\\), which is much more efficient than exhaustive search requiring \\(O(2^p)\\) operations.\n\nTheoretical Properties\nThe convergence properties of SBR are well-established. The algorithm demonstrates finite convergence, provably converging in finite steps since there are only finitely many possible active sets. It exhibits monotonic improvement where the objective function decreases or stays the same at each iteration. Finally, the algorithm achieves local optimality, with the final solution satisfying local optimality conditions.\nSBR can be viewed as a coordinate-wise proximal gradient method, establishing a connection to proximal gradient methods. The proximal operator for the \\(\\ell_0\\) norm is: \\[\\text{prox}_{\\lambda\\|\\cdot\\|_0}(z) = z \\odot \\mathbf{1}_{|z|&gt;\\sqrt{2\\lambda}}\\]\nThis hard thresholding operation is exactly what SBR implements in a coordinate-wise manner.\nUnder certain regularity conditions, SBR can achieve statistical consistency across multiple dimensions. It demonstrates variable selection consistency by correctly identifying the true active set with high probability. The algorithm provides estimation consistency through consistent estimates of the non-zero coefficients. Additionally, it achieves prediction consistency by attaining optimal prediction error rates.\n\n\nPractical Implementation Considerations\nSeveral practical considerations affect SBR implementation. For regularization parameter selection, cross-validation provides the standard approach but is computationally expensive, while information criteria such as BIC and AIC can provide faster alternatives. Stability selection offers another approach by running SBR on bootstrap samples and selecting stable variables.\nInitialization strategies vary in their effectiveness. The empty start approach using \\(S_0 = \\emptyset\\) is most common, while forward start begins with forward selection for a few steps. Random start employs multiple random initializations for better global search capabilities.\nHandling numerical issues requires attention to several factors. Multicollinearity concerns necessitate checking condition numbers of \\(X_S^T X_S\\). Rank deficiency situations require handling cases where \\(X_S\\) is rank deficient. Numerical stability can be improved by using QR decomposition instead of normal equations when needed.\n\n\nStatistical Properties and Performance\nEmpirical studies demonstrate that SBR achieves statistical performance comparable to the gold-standard spike-and-slab priors while being orders of magnitude faster. The algorithm shows superior variable selection performance compared to Lasso and elastic net in high-correlation settings, achieves lower mean squared error than convex relaxation methods for estimation accuracy, and provides better recovery of the true sparse structure compared to \\(\\ell_1\\) penalties for sparsity detection.\nThe connection between spike-and-slab priors and \\(\\ell_0\\) regularization provides theoretical justification for why SBR performs well: it approximates the MAP estimator of a principled Bayesian model while remaining computationally tractable.\n\n\nAdvantages and Limitations\nSBR offers several computational and theoretical advantages. The algorithm provides computational efficiency, running much faster than full Bayesian methods. It maintains a theoretical foundation through its principled connection to spike-and-slab priors. The approach demonstrates flexibility in handling various problem sizes and correlation structures. Finally, it produces sparse, interpretable models that enhance model interpretability.\nHowever, SBR also has certain limitations. The algorithm provides no guarantee of global optimality, potentially stopping at local optima. Its greedy nature may lead to suboptimal early decisions that affect the final solution. Performance sensitivity depends heavily on \\(\\lambda\\) selection, requiring careful parameter tuning. Additionally, the algorithm may struggle with highly correlated predictors in certain scenarios.\n\n\nExtensions and Variations\nSeveral extensions expand SBR’s applicability. Grouped SBR extends to group selection by replacing single variables with groups: \\[S \\cdot G = \\begin{cases}\nS \\cup G & \\text{if } G \\cap S = \\emptyset \\\\\nS \\setminus G & \\text{if } G \\subseteq S\n\\end{cases}\\]\nRegularization path computation involves computing solutions for a sequence of \\(\\lambda\\) values while using warm starts from previous solutions. Multiple best replacements consider the \\(k\\) best replacements at each step for more thorough search rather than restricting to single best replacement.\n\n\nProximal Perspective\nThe SBR algorithm can be deeply understood through the lens of proximal operators. The proximal operator for the \\(\\ell_0\\) norm corresponds to hard thresholding: \\[\n[\\text{prox}_{\\gamma \\lambda \\|\\cdot\\|_0}(v)]_j = v_j \\odot \\mathbf{1}_{|v_j| &gt; \\sqrt{2\\gamma\\lambda}}\n\\]\nThis reveals that SBR essentially performs coordinate-wise hard thresholding, where the decision to include or exclude a variable depends on whether its magnitude exceeds a threshold derived from the regularization parameter. This contrasts with the Lasso’s soft thresholding (which shrinks all coefficients) and Ridge’s uniform shrinkage. This perspective unifies SBR with continuous optimization approaches, showing it implements the “hard” decision boundary required for exact sparsity.\n\n\nSpike-and-Slab Examples: Bernoulli-Gaussian and Bernoulli-Laplace\nTo illustrate the practical implications of different spike-and-slab priors in the proximal framework, we examine two important cases: Bernoulli-Gaussian and Bernoulli-Laplace priors. These examples demonstrate how different prior specifications lead to different shrinkage behaviors and associated penalty functions.\nFor the normal means problem where \\(y \\mid \\beta \\sim N(\\beta, \\sigma^2)\\) with Bernoulli-Gaussian prior \\(\\beta \\sim (1-\\theta)\\delta_0 + \\theta N(0, \\sigma_\\beta^2)\\), the marginal distribution of \\(y\\) is: \\[\ny \\mid \\theta \\sim (1-\\theta) N(0, \\sigma^2) + \\theta N(0, \\sigma^2 + \\sigma_\\beta^2)\n\\]\nThe posterior mean takes the form: \\[\n\\hat{\\beta}^{BG} = w(y) y\n\\] where the weight function is: \\[\nw(y) = \\frac{\\sigma_\\beta^2}{\\sigma^2 + \\sigma_\\beta^2} \\left(1 + \\frac{(1-\\theta)\\phi(y/\\sigma)}{\\theta\\phi(y/\\sqrt{\\sigma^2 + \\sigma_\\beta^2})\\sqrt{1 + \\sigma_\\beta^2/\\sigma^2}}\\right)^{-1}\n\\]\nand \\(\\phi(\\cdot)\\) denotes the standard normal density.\nFor the Bernoulli-Laplace prior \\(\\beta \\sim (1-\\theta)\\delta_0 + \\theta \\text{Laplace}(0, b)\\), the expressions become more complex, involving the cumulative distribution function of the standard normal distribution.\n\n\nPosterior mean functions for Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) priors\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Parameters\ny_vals &lt;- seq(-5, 5, length.out = 1000)\nsigma &lt;- 1\ntheta &lt;- 0.1\nsigma_beta_vals &lt;- c(0.5, 1, 2)\n\n# Bernoulli-Gaussian posterior mean\nbg_posterior_mean &lt;- function(y, sigma, theta, sigma_beta) {\n  var_spike &lt;- sigma^2\n  var_slab &lt;- sigma^2 + sigma_beta^2\n  \n  weight_factor &lt;- (1 - theta) * dnorm(y, 0, sqrt(var_spike)) / \n                   (theta * dnorm(y, 0, sqrt(var_slab)))\n  \n  weight &lt;- (sigma_beta^2 / var_slab) / (1 + weight_factor)\n  return(weight * y)\n}\n\n# Bernoulli-Laplace posterior mean (simplified approximation)\nbl_posterior_mean &lt;- function(y, sigma, theta, b) {\n  # Simplified version - in practice this involves complex integrals\n  weight_gaussian &lt;- (1 - theta) * dnorm(y, 0, sigma)\n  weight_laplace &lt;- theta * (1/(2*b)) * exp(-abs(y)/b) * exp(sigma^2/(2*b^2)) * \n                   (pnorm(-y/sigma - sigma/b) + pnorm(y/sigma - sigma/b))\n  \n  total_weight &lt;- weight_gaussian + weight_laplace\n  weight &lt;- weight_laplace / total_weight\n  \n  # Simplified shrinkage factor\n  shrinkage_factor &lt;- y - sign(y) * pmax(0, abs(y) - sqrt(2) * sigma/b)\n  return(weight * shrinkage_factor + (1 - weight) * 0)\n}\n\n# Create data for plotting\nplot_data_bg &lt;- expand.grid(y = y_vals, sigma_beta = sigma_beta_vals)\nplot_data_bg$posterior_mean &lt;- mapply(bg_posterior_mean, \n                                     plot_data_bg$y, sigma, theta, plot_data_bg$sigma_beta)\nplot_data_bg$sigma_beta_label &lt;- paste(\"sigma =\", plot_data_bg$sigma_beta)\n\nplot_data_bl &lt;- expand.grid(y = y_vals, b = sigma_beta_vals)\nplot_data_bl$posterior_mean &lt;- mapply(bl_posterior_mean, \n                                     plot_data_bl$y, sigma, theta, plot_data_bl$b)\nplot_data_bl$b_label &lt;- paste(\"b =\", plot_data_bl$b)\n\n# Bernoulli-Gaussian plot\np1 &lt;- ggplot(plot_data_bg, aes(x = y, y = posterior_mean, color = sigma_beta_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"y\", y = \"E[beta|y]\", title = \"Bernoulli-Gaussian Prior\",\n       color = \"Parameter\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  xlim(-5, 5) + ylim(-3, 3)\n\n# Bernoulli-Laplace plot  \np2 &lt;- ggplot(plot_data_bl, aes(x = y, y = posterior_mean, color = b_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"y\", y = \"E[beta|y]\", title = \"Bernoulli-Laplace Prior\",\n       color = \"Parameter\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  xlim(-5, 5) + ylim(-3, 3)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 17.9: Posterior mean functions for Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) priors\n\n\n\n\n\n\n\nPenalty functions \\(\\theta\\) associated with Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) posterior means\n# Penalty function computation (numerical approximation)\ncompute_penalty_bg &lt;- function(z, sigma, theta, sigma_beta) {\n  # Find y such that posterior mean equals z\n  objective &lt;- function(y) (bg_posterior_mean(y, sigma, theta, sigma_beta) - z)^2\n  \n  # Use optimization to find y\n  tryCatch({\n    y_opt &lt;- optimize(objective, interval = c(-10, 10))$minimum\n    penalty &lt;- -0.5 * (y_opt - z)^2 / sigma^2 - log(\n      (1 - theta) * dnorm(y_opt, 0, sigma) + \n      theta * dnorm(y_opt, 0, sqrt(sigma^2 + sigma_beta^2))\n    )\n    return(penalty)\n  }, error = function(e) return(NA))\n}\n\ncompute_penalty_bl &lt;- function(z, sigma, theta, b) {\n  # Penalty function for Bernoulli-Laplace: phi(beta) = -log p(beta)\n  # where p(beta) = (1-theta)*delta_0(beta) + theta * (1/(2b)) * exp(-|beta|/b)\n  \n  if (abs(z) &lt; 1e-10) {\n    # At z = 0: phi(0) = -log[(1-theta) + theta/(2b)]\n    return(-log((1-theta) + theta/(2*b)))\n  } else {\n    # For z != 0: phi(z) = -log[theta/(2b)] + |z|/b\n    return(-log(theta/(2*b)) + abs(z)/b)\n  }\n}\n\n# Create penalty data\nz_vals &lt;- seq(-3, 3, length.out = 200)\npenalty_data_bg &lt;- expand.grid(z = z_vals, sigma_beta = sigma_beta_vals)\npenalty_data_bg$penalty &lt;- mapply(function(z, sb) {\n  if (abs(z) &lt; 0.01) return(0)\n  compute_penalty_bg(z, sigma, theta, sb)\n}, penalty_data_bg$z, penalty_data_bg$sigma_beta)\npenalty_data_bg$sigma_beta_label &lt;- paste(\"sigma =\", penalty_data_bg$sigma_beta)\n\npenalty_data_bl &lt;- expand.grid(z = z_vals, b = sigma_beta_vals)\npenalty_data_bl$penalty &lt;- mapply(compute_penalty_bl, \n                                 penalty_data_bl$z, sigma, theta, penalty_data_bl$b)\npenalty_data_bl$b_label &lt;- paste(\"b =\", penalty_data_bl$b)\n\n# Remove invalid values and normalize to have minimum at 0\npenalty_data_bg &lt;- penalty_data_bg[!is.na(penalty_data_bg$penalty), ]\npenalty_data_bl &lt;- penalty_data_bl[!is.na(penalty_data_bl$penalty), ]\n\n# Normalize each curve to have minimum at 0\nfor (sb in sigma_beta_vals) {\n  idx_bg &lt;- penalty_data_bg$sigma_beta == sb\n  if (any(idx_bg)) {\n    min_val &lt;- min(penalty_data_bg$penalty[idx_bg], na.rm = TRUE)\n    penalty_data_bg$penalty[idx_bg] &lt;- penalty_data_bg$penalty[idx_bg] - min_val\n  }\n  \n  idx_bl &lt;- penalty_data_bl$b == sb\n  if (any(idx_bl)) {\n    min_val &lt;- min(penalty_data_bl$penalty[idx_bl], na.rm = TRUE)\n    penalty_data_bl$penalty[idx_bl] &lt;- penalty_data_bl$penalty[idx_bl] - min_val\n  }\n}\n\n# Bernoulli-Gaussian penalty plot\np3 &lt;- ggplot(penalty_data_bg, aes(x = z, y = penalty, color = sigma_beta_label)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"beta\", y = \"phi(beta)\", title = \"Bernoulli-Gaussian Penalty\",\n       color = \"Parameter\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  xlim(-3, 3) + ylim(0, 5)\n\n# Bernoulli-Laplace penalty plot\np4 &lt;- ggplot(penalty_data_bl, aes(x = z, y = penalty, color = b_label)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = \"beta\", y = \"phi(beta)\", title = \"Bernoulli-Laplace Penalty\",\n       color = \"Parameter\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  xlim(-3, 3) + ylim(0, 5)\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 17.10: Penalty functions associated with Bernoulli-Gaussian (left) and Bernoulli-Laplace (right) posterior means\n\n\n\n\n\nThese figures illustrate several key properties of spike-and-slab priors in the proximal framework. Both priors shrink small observations towards zero, but their behavior differs significantly for larger observations. When the slab variance \\(\\sigma_\\beta^2\\) (or scale parameter \\(b\\)) is small, Bernoulli-Gaussian priors behave similarly to ridge regression, applying excessive shrinkage to large observations. In contrast, Bernoulli-Laplace priors exhibit behavior more similar to Lasso, with softer shrinkage characteristics.\nAs the slab parameters increase, both priors approach hard thresholding behavior, and their associated penalty functions \\(\\phi\\) become increasingly similar to non-convex penalties such as SCAD. The penalty functions reveal the “spiky” nature around zero that induces sparsity for small observations, while the different tail behaviors reflect the distinct shrinkage properties of Gaussian versus Laplace slab components.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#advanced-topics-in-regularization",
    "href": "17-theoryai.html#advanced-topics-in-regularization",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.14 Advanced Topics in Regularization",
    "text": "17.14 Advanced Topics in Regularization\n\nThe Vertical Likelihood Duality\nFor computational efficiency in model evaluation, consider the problem of estimating \\(\\int_{\\mathcal{X}} L(x) P(dx)\\) where \\(L: \\mathcal{X} \\to \\mathbb{R}\\). By letting \\(Y = L(x)\\), we can transform this to a one-dimensional integral \\(\\int_0^1 F_Y^{-1}(s) ds\\).\nWe therefore have a duality: \\[\n\\int_{\\mathcal{X}} L(x) P(dx) = \\int_0^1 F_Y^{-1}(s) ds\n\\] where \\(Y\\) is a random variable \\(Y = L(X)\\) with \\(X \\sim P(dx)\\).\nThis approach offers several advantages:\n\nWe can use sophisticated grids (Riemann) to approximate one-dimensional integrals\nA grid on inverse CDF space is equivalent to importance weighting in the original \\(\\mathcal{X}\\) space\nIf \\(F_Y^{-1}(s)\\) is known and bounded, we could use deterministic grids on \\([0,1]\\) with \\(O(N^{-4})\\) convergence properties\n\nThe main caveats are:\n\n\\(F_Y^{-1}(s)\\) is typically unknown\nIt becomes infinite if \\(L\\) is unbounded\nWe often resort to stochastic Riemann sums\n\nThe duality implies that finding a good importance function on \\([0,1]\\) corresponds to finding good “weighting” in \\(\\mathcal{X}\\) space. As a diagnostic for any importance sampling scheme, you should plot the equivalent grid on \\([0,1]\\) and estimated values of \\(F_Y^{-1}\\) to assess performance.\n\n\nFundamental Integral Identities\nThe two key integral identities for hyperbolic/GIG (Generalized Inverse Gaussian) and Pólya mixtures are: \\[\n\\begin{aligned}\n\\frac{\\alpha^2 - \\kappa^2}{2\\alpha} e^{-\\alpha|\\theta - \\mu| + \\kappa(\\theta - \\mu)} &= \\int_0^\\infty \\phi\\left(\\theta \\mid \\mu + \\kappa\\omega, \\omega\\right) p_{\\text{gig}}\\left(\\omega \\mid 1, 0, \\sqrt{\\alpha^2 - \\kappa^2}\\right) d\\omega \\\\\n\\frac{1}{B(\\alpha,\\kappa)} \\frac{e^{\\alpha(\\theta - \\mu)}}{(1 + e^{\\theta - \\mu})^{2(\\alpha - \\kappa)}} &= \\int_0^\\infty \\phi\\left(\\theta \\mid \\mu + \\kappa\\omega, \\omega\\right) p_{\\text{pol}}(\\omega \\mid \\alpha, \\alpha - 2\\kappa) d\\omega\n\\end{aligned}\n\\] where \\(p_{\\text{pol}}\\) denotes the Pólya density and \\(p_{\\text{gig}}\\) the GIG density.\n\n\nQuantile Regression Example\nTo illustrate these integral identities in practice, consider quantile regression. Start by choosing \\(q \\in (0,1)\\) and define the quantile loss function \\(l(x) = |x| + (2q-1)x\\). This is also known as the check loss or hinge loss function, and is used in quantile regression for the \\(q\\)th quantile Koenker (2005). Johannes, Polson, and Yae (2009) represent this as a pseudo-likelihood involving the asymmetric Laplace distribution.\nWe can derive the corresponding envelope representation as a variance-mean Gaussian mixture. Let \\(\\kappa = 2q-1\\). Then \\(g(x) = l(x) + \\kappa x = |x|\\) is symmetric in \\(x\\) and concave in \\(x^2\\). Using the general envelope representation framework, we obtain: \\[\nl(x) = \\inf_{\\lambda \\geq 0} \\left\\{ \\frac{\\lambda}{2}\\left( x - \\frac{2q-1}{\\lambda} \\right)^2 - \\psi(\\lambda) \\right\\} \\, .\n\\] where \\(\\psi(\\lambda) = \\frac{\\kappa^2}{2\\lambda} - \\frac{1}{2 \\lambda^2}\\).\nThis representation shows how the asymmetric quantile loss can be expressed as an envelope of quadratic functions, connecting it to the Gaussian mixture framework and enabling efficient computational algorithms.\n\n\nImproper Limit Cases\nThese expressions lead to three important identities concerning improper limits of GIG and Pólya mixing measures for variance-mean Gaussian mixtures: \\[\n\\begin{aligned}\na^{-1} \\exp\\left\\{-2c^{-1} \\max(a\\theta, 0)\\right\\} &= \\int_0^\\infty \\phi(\\theta \\mid -av, cv) dv \\\\\nc^{-1} \\exp\\left\\{-2c^{-1} \\rho_q(\\theta)\\right\\} &= \\int_0^\\infty \\phi(\\theta \\mid -(2\\tau - 1)v, cv) e^{-2\\tau(1-\\tau)v} dv \\\\\n(1 + \\exp\\{\\theta - \\mu\\})^{-1} &= \\int_0^\\infty \\phi(\\theta \\mid \\mu - \\frac{1}{2}v, v) p_{\\text{pol}}(v \\mid 0, 1) dv\n\\end{aligned}\n\\] where \\(\\rho_q(\\theta) = \\frac{1}{2}|\\theta| + (q - \\frac{1}{2})\\theta\\) is the check-loss function.\nThese representations connect to several important regression methods. The first identity relates to Support Vector Machines through the hinge loss in the max function. The second identity connects to Quantile and Lasso Regression via check-loss and \\(\\ell_1\\) penalties. The third identity provides the Logistic Regression representation.\nWith GIG and Pólya mixing distributions alone, one can generate the following objective functions: \\[\n\\theta^2, \\; |\\theta|, \\; \\max(\\theta,0), \\; \\frac{1}{2}|\\theta| + (\\tau - \\frac{1}{2})\\theta, \\; \\frac{1}{1+e^{-\\theta}}, \\; \\frac{1}{(1+e^{-\\theta})^r}\n\\]\nThese correspond to ridge, lasso, support vector machine, check loss/quantile regression, logit, and multinomial models, respectively. More general families can generate other penalty functions—for example, the bridge penalty \\(|u|^\\alpha\\) from a stable mixing distribution.\n\n\nComputational Advantages of Scale Mixtures\nThe scale mixture representation provides several computational benefits. Gibbs Sampling becomes efficient because the conditional distributions in the augmented parameter space are often conjugate, enabling tractable MCMC algorithms. EM Algorithms benefit from the E-step involving expectations with respect to the mixing distribution.\nThis framework has been instrumental in developing efficient algorithms for sparse regression, robust statistics, and non-conjugate Bayesian models. The ability to represent complex penalties as hierarchical Gaussian models bridges the gap between computational tractability and statistical flexibility.\n\n\nGeneralized Ridge Regression in the Canonical Basis\nWe revisit ridge regression from the perspective of the canonical coordinate system, using the singular value decomposition \\(X = UDW^T\\) and rotated coefficients \\(\\alpha = W^T\\beta\\). In this basis: \\[\nY = X\\beta + \\epsilon = UD\\alpha + \\epsilon\n\\]\nProjecting onto \\(U\\): \\[\n\\hat{\\alpha}_i = \\alpha_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2/d_i^2)\n\\]\nWith independent priors \\(\\alpha_i \\sim N(0, v_i)\\), the posterior mean yields a shrinkage estimator: \\[\n\\alpha_i^* = \\kappa_i \\hat{\\alpha}_i, \\quad \\text{with} \\quad \\kappa_i = \\frac{d_i^2}{d_i^2 + k_i}\n\\] where \\(k_i = \\sigma^2/v_i\\) is the generalized ridge penalty.\nThis framework offers componentwise optimality and anticipates global-local shrinkage priors where individual parameters receive adaptive regularization.\n\n\nTrend Filtering and Composite Penalties\nTrend filtering provides an example of regularized least squares with composite penalty, useful when a 1D signal has jumps at few positions: \\[\n\\hat{\\beta} = \\arg\\min_{\\beta} \\left\\{\\frac{1}{2}\\|y - \\beta\\|_2^2 + \\lambda\\phi(D\\beta)\\right\\}\n\\] where \\(D\\) is the difference matrix and \\(\\phi\\) is a penalty function that encourages sparsity in the differences, leading to piecewise-constant or piecewise-polynomial solutions.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "17-theoryai.html#final-thoughts",
    "href": "17-theoryai.html#final-thoughts",
    "title": "17  Statistical Learning Theory and Regularization",
    "section": "17.15 Final Thoughts",
    "text": "17.15 Final Thoughts\nOur exploration began with a sobering revelation: the maximum likelihood estimator, long considered the gold standard of classical statistics, is inadmissible in high-dimensional settings. Stein’s paradox demonstrates that for \\(p \\geq 3\\) dimensions, there always exist estimators with uniformly lower risk than the MLE. This is not merely a theoretical curiosity—in the normal means problem with \\(p=100\\), the James-Stein estimator can achieve 67 times lower risk than the MLE. This dramatic improvement illustrates why classical approaches often fail in modern high-dimensional problems and why shrinkage methods have become essential tools in contemporary data science.\nThe James-Stein estimator’s success stems from its ability to “borrow strength” across components through global shrinkage, demonstrating that multivariate estimation problems are fundamentally easier than their univariate counterparts when approached with appropriate regularization. However, global shrinkage alone is insufficient for sparse signals, motivating the development of more sophisticated approaches that can adapt to local signal structure.\nA central theme throughout this chapter is the profound duality between Bayesian priors and regularization penalties. Every regularization term \\(\\lambda \\phi(f)\\) corresponds to a prior distribution through the relationship \\(\\phi(f) = -\\log p(f)\\), making maximum a posteriori (MAP) estimation equivalent to penalized optimization. This duality provides both theoretical justification for regularization methods and practical guidance for prior specification:\n\nPenalty types and their corresponding prior distributions.\n\n\n\n\n\n\n\n\nMethod\nPenalty\nPrior\nKey Property/Behavior\n\n\n\n\nRidge\n\\(\\beta^2\\)\nGaussian\nGrows Quadratically. Shrinks slowly. Encourages smoothness and numerical stability\n\n\nLasso\n\\(|\\beta|\\)\nLaplace\nGrows Linearly. Shrinks strongly. Induces sparsity but shrinks strong signals\n\n\nBridge\n\\(|\\beta|^\\alpha\\)\nExponential\nInterpolates between Ridge and Lasso.\n\n\nCauchy\n\\(\\log(1+\\beta^2)\\)\nCauchy\nHeavy-tailed. Grows logarithmically. Strong shrinkage near zero, minimal shrinkage for large signals\n\n\nHorseshoe\n\\(-\\log\\log(1 + 2\\tau^2/\\beta^2)\\)\nHorseshoe\nGrows almost flat for large signals. Shrinks slowly. Optimal sparsity with strong signals preserved. \\(\\tau\\) is a scale parameter.\n\n\nSubset selection\n\\(I(\\beta \\neq 0)\\)\nSpike-and-slab\nShrinks strongly. Exact sparsity through hard thresholding. Computationally hard.\n\n\n\nThis mapping reveals why certain penalties work well for particular problem types and guides the selection of appropriate regularization strategies based on problem structure and prior beliefs about the solution. Figure 17.11 shows the unit balls defined by the norms induced by different priors.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Bridge\n\n\n\n\n\n\n\n\n\n\n\n(b) Horseshoe\n\n\n\n\n\n\n\n\n\n\n\n(c) Cauchy\n\n\n\n\n\n\n\nFigure 17.11: Comparison of unit balls for norms induced by different priors\n\n\n\n\nThe Figure 17.11 (a) illustrates the unit balls for different penalty norms, each revealing distinct geometric properties. The \\(\\ell_2\\) (Ridge) penalty creates a circular unit ball that encourages smoothness across all parameters. The \\(\\ell_1\\) (Lasso) penalty forms a diamond shape that induces sparsity through soft thresholding. Bridge penalties with \\(\\alpha = 0.5\\) create more pointed shapes that provide stronger sparsity than Lasso, while \\(\\alpha = 1.5\\) produces shapes between the diamond and circle. The \\(\\ell_0\\) penalty reduces to discrete points on the coordinate axes, representing exact subset selection.\nFigure 17.11 (b) shows the unit balls for penalty norms induced by the horseshoe prior. Visually, it looks similar to the unit ball induced by the bridge prior with \\(\\alpha = 0.5\\). It demonstrates several remarkable properties that make it particularly effective for sparse estimation problems. Its penalty function \\(\\phi(\\theta_i) = -\\log(\\log(1 + 2\\tau^2/\\theta_i^2))\\) exhibits a unique double-logarithmic structure that creates an ideal balance between sparsity induction and signal preservation.\nThe horseshoe’s most distinctive characteristic is its asymmetric shrinkage behavior. Near the origin, when parameters approach zero, the penalty function grows to infinity, creating extremely strong shrinkage that encourages exact zeros and thus induces sparsity. However, for large parameter values, the penalty grows slowly, allowing large signals to escape penalization with minimal distortion. This behavior is fundamentally different from methods like Lasso, which applies uniform shrinkage regardless of signal magnitude.\nThe global shrinkage parameter \\(\\tau\\) provides crucial control over the prior’s behavior. Smaller values of \\(\\tau\\) impose more aggressive shrinkage across all parameters, while larger values become more permissive, allowing signals to emerge more easily. This parameter effectively controls the trade-off between sparsity and signal detection.\nFrom a theoretical perspective, the horseshoe prior achieves optimal minimax convergence rates of \\(O(s \\log(p/s))\\) for \\(s\\)-sparse vectors, making it particularly well-suited for “needle-in-a-haystack” problems where researchers seek to identify a few large signals among many noise variables. Under appropriate regularity conditions, the horseshoe also possesses oracle properties, meaning it can correctly identify the true signal structure.\nThe geometric visualization reveals why this prior is so effective. The characteristic “horseshoe” shape displays concave contours near the coordinate axes, reflecting strong shrinkage toward zero, while showing convex contours away from the origin, indicating minimal shrinkage for large coefficients. Unlike the Lasso’s diamond shape or Ridge’s circular contours, the horseshoe’s heavy tails allow large coefficients to escape penalization entirely.\nThis comprehensive visualization demonstrates why the horseshoe prior has become increasingly popular for modern high-dimensional problems where the true signal structure is sparse but individual signals may be large in magnitude. The prior’s ability to provide aggressive shrinkage for noise while preserving signal integrity makes it an ideal choice for contemporary machine learning applications.\nFigure 17.11 (c) shows the unit ball for the Cauchy prior. It demonstrates several remarkable properties that make it particularly effective for sparse estimation problems. Its penalty function \\(\\phi(\\theta_i) = \\log(1 + \\theta_i^2)\\) exhibits a unique logarithmic structure that creates an ideal balance between sparsity induction and signal preservation.\nThe Cauchy prior’s most distinctive characteristic is its heavy-tailed behavior. Near the origin, when parameters approach zero, the penalty function grows logarithmically, creating extremely strong shrinkage that encourages exact zeros and thus induces sparsity. However, for large parameter values, the penalty grows slowly, allowing large signals to escape penalization with minimal distortion. This behavior is fundamentally different from methods like Lasso, which applies uniform shrinkage regardless of signal magnitude.\nThe global shrinkage parameter \\(\\tau\\) provides crucial control over the prior’s behavior. Smaller values of \\(\\tau\\) impose more aggressive shrinkage across all parameters, while larger values become more permissive, allowing signals to emerge more easily. This parameter effectively controls the trade-off between sparsity and signal detection.\nOur analysis reveals a rich spectrum of approaches to sparsity, each with distinct theoretical properties and practical advantages.\nGlobal shrinkage methods such as James-Stein estimation and Ridge regression apply uniform regularization across all parameters. While computationally simple and numerically stable, they cannot adapt to local signal structure and may over-shrink large signals in sparse settings.\nAdaptive shrinkage methods like Lasso and horseshoe priors provide differential shrinkage based on signal strength. Lasso employs soft thresholding, while the horseshoe applies aggressive shrinkage to small signals while preserving large ones, achieving the optimal minimax rate \\(p_n \\log(n/p_n)\\). The horseshoe is particularly effective for needle-in-a-haystack problems.\nVariable selection methods including spike-and-slab priors and \\(\\ell_0\\) regularization make discrete inclusion or exclusion decisions. These produce the most interpretable sparse solutions and can achieve oracle properties under appropriate conditions, though they are combinatorially challenging to implement.\nBuilding on the theoretical foundations and geometric intuition developed above, we can distill several actionable guidelines for practitioners and researchers working with regularized models and sparse estimation:\n\n\n\nTable 17.2: Practical guidelines for regularized models and sparse estimation.\n\n\n\n\n\n\n\n\n\n\nScenario\nRecommended Methods\nRationale\n\n\n\n\nLow-dimensional (\\(p &lt; n\\))\nRidge regression\nNumerical stability; sufficient for moderate \\(p\\)\n\n\nHigh-dimensional sparse\nLasso, Bridge (\\(\\alpha \\approx 0.7\\)), Horseshoe\nLasso balances efficiency and sparsity; Bridge and Horseshoe offer better properties\n\n\nUltra-sparse\nSpike-and-slab, \\(\\ell_0\\) methods\nOptimal performance and exact sparsity\n\n\nCorrelated predictors\nBridge, full Bayesian\nHandle correlation better than Lasso\n\n\nUncertainty quantification\nFull Bayesian methods\nProvide credible intervals and posterior probabilities\n\n\n\n\n\n\nWhile our focus has been on linear models, the principles developed here extend broadly to modern AI systems. Deep learning architectures routinely employ regularization techniques (dropout, weight decay, batch normalization) that can be understood through the Bayesian lens developed in this chapter. The success of techniques like variational autoencoders and Bayesian neural networks demonstrates the continued relevance of probabilistic thinking in contemporary machine learning.\nMoreover, the sparse estimation techniques discussed here are fundamental to interpretable AI, compressed sensing, and efficient neural architecture design. The theoretical insights about when different regularization approaches succeed provide guidance for designing and analyzing complex learning systems.\nThe evolution from maximum likelihood estimation to sophisticated Bayesian regularization represents more than technical progress—it reflects a fundamental shift in how we approach learning from data. Rather than seeking single “best” estimates, modern methods embrace uncertainty, incorporate prior knowledge, and adaptively balance model complexity with empirical fit.\nThe remarkable fact that Bayesian approaches often dominate classical frequentist methods, even from a frequentist perspective (as demonstrated by Stein’s paradox), suggests that probabilistic thinking provides not just philosophical appeal but concrete practical advantages. In an era of ever-growing data complexity and dimensionality, these theoretical insights become increasingly valuable for developing robust, interpretable, and effective learning algorithms.\nThe unified framework presented in this chapter—connecting classical statistics, Bayesian inference, optimization theory, and modern machine learning—provides both historical perspective and forward-looking guidance for the continued development of artificial intelligence systems. As we confront increasingly complex learning challenges, from personalized medicine to autonomous systems, the principled approach to regularization and uncertainty quantification developed here will remain fundamental to building trustworthy and effective AI systems.\n\n\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale Mixtures of Normal Distributions.” Journal of the Royal Statistical Society. Series B (Methodological) 36 (1): 99–102.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010. “The Horseshoe Estimator for Sparse Signals.” Biometrika, asq017.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior Opinion.”\n\n\nEfron, Bradley, and Carl Morris. 1975. “Data Analysis Using Stein’s Estimator and Its Generalizations.” Journal of the American Statistical Association 70 (350): 311–19.\n\n\n———. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (5): 119–27.\n\n\nJohannes, Michael S., Nick Polson, and Seung M. Yae. 2009. “Quantile Filtering and Learning.” SSRN Electronic Journal.\n\n\nKoenker, Roger. 2005. Quantile Regression. Econometric Society Monographs. Cambridge: Cambridge University Press.\n\n\nPolson, Nicholas G., and James G. Scott. 2012. “Good, Great, or Lucky? Screening for Firms with Sustained Superior Performance Using Heavy-Tailed Priors.” The Annals of Applied Statistics 6 (1): 161–85.\n\n\nPolson, Nicholas G., and Steven L. Scott. 2011. “Data Augmentation for Support Vector Machines.” Bayesian Analysis 6 (1): 1–23.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of Inverse Problems.” In Dokl. Akad. Nauk Sssr, 39:195–98.",
    "crumbs": [
      "AI",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Learning Theory and Regularization</span>"
    ]
  },
  {
    "objectID": "18-nn.html",
    "href": "18-nn.html",
    "title": "18  Neural Networks",
    "section": "",
    "text": "18.1 Introduction\nIn 2012, a neural network called AlexNet stunned the computer vision community by winning the ImageNet competition with an error rate nearly half that of the previous year’s winner. This breakthrough marked the beginning of deep learning’s modern era—a period in which neural networks have transformed fields from medical diagnosis to language translation, from game playing to autonomous driving.\nThe goal of this chapter is to provide a comprehensive overview of deep learning (DL) methods. From a statistical perspective, deep learning extends the framework of generalized linear models by introducing multiple layers of nonlinear transformations. While traditional statistical models are well-studied and interpretable, they often lack the flexibility to capture complex input-output relationships. Black-box predictive methods, such as decision trees and neural networks, offer greater flexibility at the cost of interpretability. Deep learning excels precisely where high-dimensional problems make traditional model selection challenging.\nTo understand this transition, consider the architectural trade-off between traditional regression and deep learning. Classical statistical methods follow a wide and shallow paradigm: we engineer many features (making the model wide) but apply only a single transformation layer—typically a linear combination followed by a link function, as in GLMs. For instance, polynomial regression with interactions creates a wide feature space (\\(x_1, x_2, x_1^2, x_2^2, x_1x_2, \\ldots\\)) but remains shallow (one layer). In contrast, deep learning adopts a deep and narrow architecture: it uses many successive transformations (making the model deep) but potentially fewer units per layer (keeping layers narrow). Rather than manually constructing a vast feature space, deep networks learn hierarchical representations—each layer extracts increasingly abstract features from the layer below. This architectural shift explains why neural networks can model complex relationships without explicit feature engineering: depth provides representational power that compensates for narrower layers, while the hierarchy of transformations discovers features automatically.\nDeep learning thus offers a powerful alternative in domains traditionally served by statistical methods, opening rich avenues for future research—including uncertainty quantification, principled architecture selection, and Bayesian deep learning. Although DL models have been most prominently applied to image analysis and natural language processing, they have also demonstrated superior performance in traditional engineering and scientific domains such as spatio-temporal modeling and financial forecasting (N. Polson, Sokolov, and Xu 2021; Nicholas G. Polson, Sokolov, et al. 2017; Dixon, Polson, and Sokolov 2019; Sokolov 2017; Bhadra et al. 2021; Behnia, Karbowski, and Sokolov 2023; Nareklishvili, Polson, and Sokolov 2022, 2023b, 2023a; Nicholas G. Polson and Sokolov 2023; N. Polson and Sokolov 2020).\nIn this chapter, we focus on feed-forward neural networks and their applications to regression and classification. We begin with the mathematical foundations, work through illustrative examples, and discuss practical implementation in R and Python.\nPart II largely relied on feature engineering: transforming raw inputs into predictors that make linear or tree-based models effective. Part III shifts that burden onto the model by learning representations jointly with prediction. This transition is powered by regularization ideas (Chapter 17) and by optimization methods for large parameter spaces (Chapter 20), which make it feasible to fit networks with millions or billions of parameters.\nMachine learning and statistical analysis share common foundations, but they emphasize different priorities: ML focuses on scalable algorithms and predictive accuracy, while statistics prioritizes interpretability and inferential rigor. Deep learning sits at the intersection, providing powerful pattern-matching capabilities suitable for AI applications such as image recognition and text analysis.\nFrom a computational perspective, images and text can be represented as high-dimensional matrices and vectors, respectively. The challenge of recognizing objects in images or understanding natural language requires learning complex decision boundaries in these high-dimensional input spaces. Deep learning addresses this challenge by using hierarchical layers of transformations that progressively extract meaningful features from raw data. This approach differs from traditional statistical models in that the features are learned from data rather than specified a priori.\nThere are several deep learning architectures - each has its own uses and purposes. Convolutional Neural Networks (CNN) deal with 2-dimensional input objects, i.e., images, and have been shown to outperform other techniques. Recurrent Neural Networks (RNN) have shown the best performance on speech and text analysis tasks.\nIn general, a neural network can be described as follows. Let \\(f_1 , \\ldots , f_L\\) be given univariate activation functions for each of the \\(L\\) layers. Activation functions are nonlinear transformations of weighted data. A nonlinear activation of an affine transformation is then defined by \\[\nf_l^{W,b} = f_l \\left ( \\sum_{j=1}^{N_l} W_{lj} X_j + b_l \\right ) = f_l ( W_l X_l + b_l )\\,\n\\] which implicitly needs the specification of the number of hidden units \\(N_l\\). Our deep predictor, given the number of layers \\(L\\), then becomes the composite map\n\\[\n\\hat{Y}(X) = F(X) = \\left ( f_l^{W_1,b_1} \\circ \\ldots \\circ f_L^{W_L,b_L} \\right ) ( X)\\,.\n\\]\nDeep learning’s capacity to serve as a universal function approximator has deep mathematical roots—a principle traceable to the functional analysis of the nineteenth century. From a practical perspective, given a large enough dataset, we can empirically learn an optimal predictor. Similar to a classic basis decomposition, the deep approach uses univariate activation functions to decompose a high-dimensional input \\(X\\).\nLet \\(Z^{(l)}\\) denote the \\(l\\)th layer, and so \\(X = Z^{(0)}\\). The final output \\(Y\\) can be numeric or categorical. The explicit structure of a deep prediction rule is then \\[\n\\begin{aligned}\n\\hat{Y} (X) & = W^{(L)} Z^{(L)} + b^{(L)} \\\\\nZ^{(1)} & = f^{(1)} \\left ( W^{(0)} X + b^{(0)} \\right ) \\\\\nZ^{(2)} & = f^{(2)} \\left ( W^{(1)} Z^{(1)} + b^{(1)} \\right ) \\\\\n\\ldots  & \\\\\nZ^{(L)} & = f^{(L)} \\left ( W^{(L-1)} Z^{(L-1)} + b^{(L-1)} \\right )\\,.\n\\end{aligned}\n\\] Here \\(W^{(l)}\\) is a weight matrix and \\(b^{(l)}\\) are bias terms that shift the activation functions. Designing a good predictor depends crucially on the choice of univariate activation functions \\(f^{(l)}\\). The \\(Z^{(l)}\\) are hidden features (or latent representations) that the algorithm extracts from the data.\nPut differently, the deep approach employs hierarchical predictors comprising a series of \\(L\\) nonlinear transformations applied to \\(X\\). Each of the \\(L\\) transformations is referred to as a layer, where the original input is \\(X\\), the output of the first transformation is the first layer, and so on, with the output \\(\\hat Y\\) as the final layer. The layers \\(1\\) to \\(L\\) are called hidden layers. The number of layers \\(L\\) represents the depth of our network.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#mathematical-foundations",
    "href": "18-nn.html#mathematical-foundations",
    "title": "18  Neural Networks",
    "section": "18.2 Mathematical Foundations",
    "text": "18.2 Mathematical Foundations\nBefore exploring practical examples, it is instructive to see how neural networks can represent common mathematical operations. Interaction terms such as \\(x_1 x_2\\) and \\((x_1 x_2)^2\\), as well as max functions like \\(\\max(x_1, x_2)\\), can be expressed as nonlinear functions of semi-affine combinations. Specifically:\n\\[\nx_1x_2 = \\frac{1}{4} ( x_1+x_2 )^2 - \\frac{1}{4} (x_1-x_2)^2\n\\]\n\\[\n\\max(x_1,x_2) = \\frac{1}{2} | x_1+x_2 | + \\frac{1}{2} | x_1-x_2 |\n\\]\n\\[\n(x_1x_2)^2 = \\frac{1}{4} ( x_1+x_2 )^4 + \\frac{7}{4 \\cdot 3^3} (x_1-x_2)^4 - \\frac{1}{2 \\cdot 3^3} ( x_1+ 2 x_2)^4 - \\frac{2^3}{3^3} ( x_1 + \\frac{1}{2} x_2 )^4\n\\]\nThese identities reveal that products and maxima—operations central to modeling interactions and thresholds—can be constructed from combinations of univariate functions applied to linear projections of the input. This is precisely what neural networks do.\nDiaconis and Shahshahani (1981) provides further discussion in the context of Projection Pursuit Regression, where the model uses a layered structure \\(\\sum_{i=1}^N f(w_i^\\top X)\\). Their work on composite iterated functions foreshadowed the modern use of multiple layers to model complex multivariate systems.\n\nReLU Networks as Max-Sum Networks\nDeep ReLU architectures can be viewed as max-sum networks via a simple identity. Define \\(x^+ = \\max(x,0)\\). Let \\(f_x(b) = (x + b)^+\\) where \\(b\\) is an offset. Then \\((x + y^+)^+ = \\max(0, x, x+y)\\). This is generalized in Feller (1971) (p.272), who shows by induction that\n\\[\n( f_{x_1} \\circ \\ldots \\circ f_{x_k} ) (0) = ( x_1 + ( x_2 + \\ldots + ( x_{k-1} + x_k^+ )^+ )^+ = \\max_{1 \\leq j \\leq k} ( x_1 + \\ldots + x_j )^+\n\\]\nA composition of max-layers thus reduces to a single-layer max-sum network. This mathematical insight helps explain why deep ReLU networks are so effective: they can efficiently represent a rich class of piecewise linear functions.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#classification-with-neural-networks",
    "href": "18-nn.html#classification-with-neural-networks",
    "title": "18  Neural Networks",
    "section": "18.3 Classification with Neural Networks",
    "text": "18.3 Classification with Neural Networks\nHaving established the mathematical framework of layers and activation functions, let us now ground these concepts in a concrete classification problem. We will start with a simple 2D visual example to build intuition before moving to more complex architectures.\nTo illustrate the concepts introduced above, we apply a feed-forward neural network with one hidden layer to a binary classification problem. Consider the simulated dataset shown below, where points are generated from two distributions: the inner cluster (green) represents one class, while the outer ring (red) represents the other. The goal is to learn a decision boundary that separates these two classes.\nAs a baseline, we first attempt to classify the data using logistic regression:\n\n# Fit a logistic regression model\nfit = glm(label~x1+x2, data=as.data.frame(d), family=binomial(link='logit'))\n# Plot the training dataset\nplot(d[,2],d[,3], col=d[,1]+2, pch=16, xlab=\"x1\", ylab=\"x2\")\nth = fit$coefficients\n# Plot the decision boundary\nabline(-th[1]/th[3], -th[2]/th[3], col=2)\n\n\n\n\n\n\n\n\nLogistic regression fails to distinguish the classes because the data is not linearly separable. However, we can use multiple lines to separate the data.\n\nplot(x1~x2, data=d,col=d[,1]+2, pch=16)\n# Plot lines that separate once class (red) from another (green)\nlines(x1, -x1 - 6); text(-4,-3,1)\nlines(x1, -x1 + 6); text(4,3,2)\nlines(x1,  x1 - 6); text(4,-3,3)\nlines(x1,  x1 + 6); text(-3,4,4)\n\n\n\n\n\n\n\n\nNow, we do the same thing as in simple logistic regression and apply logistic function to each of those lines\n\n# Define sigmoid function\nsigmoid  = function(z) exp(z)/(1+exp(z))\n\n# Define hidden layer of our neural network\nfeatures = function(x1,x2) {\n  z1 =  6 + x1 + x2; a1 = sigmoid(z1) # The bias 6 shifts the decision boundaries\n  z2 =  6 - x1 - x2; a2 = sigmoid(z2)\n  z3 =  6 - x1 + x2; a3 = sigmoid(z3)\n  z4 =  6 + x1 - x2; a4 = sigmoid(z4)\n  return(c(a1,a2,a3,a4))\n}\n\nUsing the matrix notation, we have \\[\nz = \\sigma(Wx + b), ~ W = \\begin{bmatrix} 1 & 1 \\\\ -1 & -1 \\\\ -1 & 1 \\\\ 1 & -1 \\end{bmatrix}, ~ b = \\begin{bmatrix} 6 \\\\ 6 \\\\ 6 \\\\ 6 \\end{bmatrix}, ~ \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\]\nThe model shown above is the first layer of our neural network. It takes a two-dimensional input \\(x\\) and produces a four-dimensional output \\(z\\) which is called a feature vector. The feature vector is then passed to the output layer, which applies simple logistic regression to the feature vector. \\[\n\\hat{y} = \\sigma(w^Tz + b), ~ w = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, ~ b = -3.1, ~ \\sigma(z) = \\frac{1}{1+e^{-z}}\n\\]\nThe output of the output layer is the probability of the positive class.\n\n# Calculate prediction (classification) using our neural network\npredict_prob = function(x){\n  x1 = x[1]; x2 = x[2]\n  z = features(x1,x2)\n  # print(z)\n  mu = sum(z) - 3.1\n  # print(mu)\n  sigmoid(mu)\n}\n\nWe can use our model to do the predictions now\n\n# Predict the probability of the positive class for a given point\npredict_prob(c(0,0))\n## 0.71\npredict_prob(c(0,10))\n## 0.26\n\nThe model generates sensible predictions, let’s plot the decision boundary to see how well it separates the data.\n\nx1 = seq(-11,11,length.out = 100)\nx2 = seq(-11,11,length.out = 100)\ngr = as.matrix(expand.grid(x1,x2));\n## 10000     2\nyhat = apply(gr,1,predict_prob)\n## 10000\nimage(x1,x2,matrix(yhat,ncol = 100), col = heat.colors(20,0.7))\n\n\n\n\n\n\n\n\nHow about a regression model? We will use a one-layer neural network to fit a quadratic function. We simulate noisy data from the following model \\[\ny = 0.5 + 0.3x^2 + \\epsilon, ~ \\epsilon \\sim N(0,0.02^2)\n\\] And use 3 hidden units in the first hidden layer and two units in the second hidden layer. The output layer is a single unit. We will use the hyperbolic tangent (tanh) activation function for all layers. The model is defined as follows\n\nrelu = function(x) max(0,x)\nnn = function(W,f=relu) {\n    b0 = W[1]; w0=W[2:4];b1 = W[5]; w1 = W[6:8]\n    z0 = apply(b0 + outer(x,w0,'*'),1:2,f)\n    yhat = b1 + z0 %*% w1\n    return(list(yhat = yhat[,1],z0=z0))\n}\n\nThe hidden layer has three outputs (neurons) and uses the ReLU activation function. The output linear layer has a single output. Thus, the prediction yhat is generated as a linear model of the feature vector z0. The model has 8 parameters. Let’s generate training data and fit the model. We will use the BFGS optimization algorithm to minimize the loss function (negative log-likelihood) of the model.\n\nset.seed(99) #gretzky\nnl  = c(3,2)\nparams = c(0,rnorm(3),0,rnorm(3))\nx = seq(-1,1,0.02)\ny = 0.5 + 0.3*x^2 + rnorm(length(x),0,0.02)\nloss = function(W) sum((nn(W)$yhat - y)^2)\nres = optim(params, loss, method='BFGS') # BFGS is a quasi-Newton optimization method\nres$par\n## -0.24  1.39 -0.84  0.46  0.50  0.18  0.45  0.37\n\nFigure 18.1 shows the quadratic function and the neural network model. The solid black line is the neural network model, and the dashed lines are the basis functions. The model fits the data well.\n\no = nn(res$par)\nplot(x,y); lines(x,o$yhat, lwd=2)\nlines(x,0.5+o$z0[,1],col=2, lwd=2, lty=2); lines(x,0.5+o$z0[,2],col=3, lwd=2, lty=2); lines(x,0.5+o$z0[,3],col=4, lwd=2, lty=2)\n\n\n\n\n\n\n\nFigure 18.1: Noisy quadratic function approximated by a neural network with ReLU activation function.\n\n\n\n\n\nLet’s try the \\(\\tanh\\) function\n\nset.seed(8) #gretzky\nparams = c(0,rnorm(3),0,rnorm(3))\nloss = function(W) mean((nn(W,f=tanh)$yhat - y)^2)\nres = optim(params, loss, method='BFGS')\no = nn(res$par, f=tanh)\nplot(x,y, ylim=c(0.4,0.95)); lines(x,o$yhat, lwd=2);\nlines(x,0.5*o$z0[,1]+0.9, lwd=2, lty=2, col=2); lines(x,0.5*o$z0[,2]+0.9, lwd=2, lty=2, col=3); lines(x,0.5*o$z0[,3]+0.9, lwd=2, lty=2, col=4)\n\n\n\n\nNoisy quadratic function approximated by a neural network with tanh activation function.\n\n\n\n\nNotice that we did not need to explicitly specify a quadratic term; the neural network discovered this structure from the data. The model automatically learned an appropriate basis function representation.\nThe same approach extends naturally to interaction terms. Consider the following data-generating process: \\[\ny = 0.5 + 0.1x_1 + 0.2x_2  + 0.5x_1x_2+ \\epsilon, ~ \\epsilon \\sim N(0,0.02^2)\n\\] We can use the same model as above, but with two input variables. The model will learn the interaction term from the data.\n\nset.seed(99) #ovi\nx1 = seq(-1,1,0.01)\nx2 = x1\ny = 0.5 + 0.1*x1 + 0.2*x2 + 0.5*x1*x2 + rnorm(length(x1),0,0.02)\nlibrary(\"scatterplot3d\")\ns3d = scatterplot3d(x1,x2,y, pch=16)\nx = cbind(x1,x2)\nnn = function(W,f=relu) {\n    b0 = W[1]; w0 = W[2:5]; b1 = W[6]; w1 = W[7:8]\n    w0 = matrix(w0,nrow=2)\n    z0 = apply(b0 + x%*%w0,1:2,f)\n    yhat = b1 + z0 %*% w1\n    return(list(yhat = yhat[,1],z0=z0))\n}\nW = c(0,rnorm(4),0,rnorm(2))\nloss = function(W) sum((nn(W, f=tanh)$yhat - y)^2)\nres = optim(W, fn=loss, method='BFGS')\no = nn(res$par, f=tanh)\ns3d$points3d(x1,x2,o$yhat, col=2, type='l', lwd=5)\ns3d$points3d(x1,x2,o$z0[,1], col=3, type='l', lwd=5)\ns3d$points3d(x1,x2,o$z0[,2], col=4, type='l', lwd=5)\n\n\n\n\n\n\n\n\nEffectively, neural networks serve as flexible function approximators, analogous to nonparametric regression methods but with the crucial advantage of learning the basis functions directly from data.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#activation-functions",
    "href": "18-nn.html#activation-functions",
    "title": "18  Neural Networks",
    "section": "18.4 Activation Functions",
    "text": "18.4 Activation Functions\nThe last output layer of a neural network has sigmoid activation function for binary output variable (classification) and no activation function for continuous output variable regression. The hidden layers can have different activation functions. The most common activation functions are the hyperbolic tangent function and the rectified linear unit (ReLU) function.\nA typical approach is to use the same activation function for all hidden layers. The hyperbolic tangent function is defined as \\[\n\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n\\] Notice that the hyperbolic tangent function is a scaled version of the sigmoid function, with \\(\\tanh(0) = 0\\). It is a smooth function which is differentiable everywhere.\n\n\n\n\n\n\ntanh\n\n\n\n\n\n\n\nHard tanh\n\n\n\n\n\n\n\nsoftplus\n\n\n\n\n\n\n\n\n\nReLU\n\n\n\n\n\n\n\nLeaky ReLU\n\n\n\n\n\n\n\nsigmoid\n\n\n\n\n\nA significant limitation of sigmoid and tanh functions is their tendency to saturate at extreme input values, causing gradients to vanish. When we try to learn the weights of the network, the optimization algorithms make small steps in the space of the parameters and when the weights are large the small changes won’t affect the values of the layers’ outputs and optimization will “stagnate.”\nConsequently, the gradients become negligible, causing the optimization process to stagnate (the “vanishing gradient” problem). The ReLU function is defined as \\[\n\\text{ReLU}(z) = \\max(0,z)\n\\] The ReLU function is a piecewise linear function which is computationally efficient and easy to optimize. The ReLU function is the most commonly used activation function in deep learning. The ReLU function is not differentiable at \\(z=0\\), but it is differentiable everywhere else. The derivative of the ReLU function is \\[\n\\text{ReLU}'(z) = \\begin{cases} 0 & \\text{if } z &lt; 0 \\\\ 1 & \\text{if } z &gt; 0 \\end{cases}\n\\]",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#unsupervised-learning-auto-encoders",
    "href": "18-nn.html#unsupervised-learning-auto-encoders",
    "title": "18  Neural Networks",
    "section": "18.5 Unsupervised Learning: Auto-Encoders",
    "text": "18.5 Unsupervised Learning: Auto-Encoders\nAuto-encoding is an important dimensionality reduction technique. An auto-encoder is a neural network architecture designed to replicate its input \\(X\\), i.e., \\(Y = X\\), via a bottleneck structure. The model \\(F^{W,b}(X)\\) learns to compress the information required to recreate \\(X\\) into a lower-dimensional representation. Heaton, Polson, and Witte (2016) provide an application to smart indexing in finance.\nSuppose that we have \\(N\\) input vectors \\(X = \\{ x_1 , \\ldots , x_N \\} \\in \\mathbb{R}^{M\\times N}\\) where \\(N\\) is the sample size and \\(M\\) is the feature dimension, and we wish to reconstruct them. Setting biases to zero, for the purpose of illustration, and using only one hidden layer (\\(L=2\\)) with \\(K &lt; N\\) factors, gives for \\(j=1, \\ldots, N\\):\n\\[\nY_j(x) = F^m_{W} ( X )_j = \\sum_{k=1}^K W^{jk}_2 f \\left ( \\sum_{i=1}^N W^{ki}_1 x_i \\right ) =  \\sum_{k=1}^K W^{jk}_2 Z_j \\quad \\text{for } Z_j =  f \\left ( \\sum_{i=1}^N W^{ki}_1 x_i \\right )\n\\]\nSince, in an auto-encoder, we fit the model \\(X = F_{W}( X)\\), and train the weights \\(W = (W_1, W_2)\\) with regularization penalty in a\n\\[\n\\mathcal{L} ( W )  =  \\operatorname{argmin}_W \\Vert X - F_W (X) \\Vert^2  + \\lambda \\phi(W)\n\\]\nwith\n\\[\n\\phi(W) = \\sum_{i,j,k} | W^{jk}_1 |^2 +  | W^{ki}_2 |^2.\n\\]\nWriting the deep learning objective as an augmented Lagrangian using the Alternating Direction Method of Multipliers (ADMM) with a hidden factor \\(Z\\), leads to a two step algorithm: an encoding step (a penalty for \\(Z\\)), and a decoding step for reconstructing the output signal via\n\\[\n\\operatorname{argmin}_{W,Z} \\Vert X - W_2 Z \\Vert^2 + \\lambda \\phi(Z) + \\Vert Z -  f( W_1, X ) \\Vert^2,\n\\]\nwhere the regularization on \\(W_1\\) induces a penalty on \\(Z\\). The last term is the encoder, the first two the decoder.\nIf \\(W_2\\) is estimated from the structure of the training data matrix, then we have a traditional factor model, and the \\(W_1\\) matrix provides the factor loadings. Principal Component Analysis (PCA), Partial Least Squares (PLS), and Sliced Inverse Regression (SIR) fall into this category (see Cook 2007 for further discussion). If \\(W_2\\) is trained based on the pair \\(\\hat{X}=\\{Y,X\\}\\) then we have a sliced inverse regression model. If \\(W_1\\) and \\(W_2\\) are simultaneously estimated based on the training data \\(X\\), then we have a two layer deep learning model.\nAuto-encoding demonstrates that deep learning does not directly model variance-covariance matrix explicitly as the architecture is already in predictive form. Given a hierarchical non-linear combination of deep learners, an implicit variance-covariance matrix exists, but that is not the driver of the algorithm.\n\nDynamic Auto-Encoders\nAnother interesting area for future research involves applying these concepts to time-series, such as using Long Short Term Memory models (LSTMs). For example, a dynamic one layer auto-encoder for a financial time series \\((Y_t)\\) is a coupled system of the form\n\\[\nY_t = W_x X _t + W_y Y_{t-1} \\quad \\text{and} \\quad \\begin{pmatrix} X_t \\\\ Y_{t-1} \\end{pmatrix} = W Y_t\n\\]\nHere, the state equation encodes and the matrix \\(W\\) decodes the \\(Y_t\\) vector into its history \\(Y_{t-1}\\) and the current state \\(X_t\\).",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#modern-deep-learning-architectures",
    "href": "18-nn.html#modern-deep-learning-architectures",
    "title": "18  Neural Networks",
    "section": "18.6 Modern Deep Learning Architectures",
    "text": "18.6 Modern Deep Learning Architectures\nThe evolution of deep learning has led to increasingly sophisticated architectural patterns that go beyond simple stacked layers. Modern systems often combine multiple neural networks, reuse pre-trained components, and employ ensemble methods to achieve state-of-the-art performance. In this section, we explore three key architectural paradigms that have become central to contemporary deep learning practice: transfer learning with pre-trained blocks, mixture of experts models, and ensemble methods.\n\nTransfer Learning and Pre-trained Blocks\nOne of the most significant developments in deep learning is the recognition that neural networks learn hierarchical representations, where early layers capture general features and deeper layers encode task-specific patterns. This observation has given rise to transfer learning, where models trained on large-scale datasets can be adapted to new tasks with limited data.\nConsider the analogy of learning to drive different vehicles. Once you learn to drive a car, adapting to a truck requires learning the differences rather than starting from scratch. Similarly, a neural network trained to recognize objects in millions of images has learned fundamental visual features—edges, textures, shapes—that are useful for many computer vision tasks.\nThe typical transfer learning workflow involves taking a pre-trained model, removing its final layers, and replacing them with new layers appropriate for the target task. For example, a convolutional neural network (CNN) trained on ImageNet we discuss CNNs in detail in 22 can be adapted for medical image classification by freezing the early convolutional layers and retraining only the final classification layers. The mathematical formulation is straightforward. Let \\(F_{\\text{pre}}(X; \\theta_{\\text{frozen}})\\) represent the pre-trained feature extractor with frozen parameters, and \\(G(Z; \\theta_{\\text{new}})\\) the new task-specific layers. The complete model becomes:\n\\[\n\\hat{Y} = G(F_{\\text{pre}}(X; \\theta_{\\text{frozen}}); \\theta_{\\text{new}})\n\\]\nDuring training, we only optimize \\(\\theta_{\\text{new}}\\) while keeping \\(\\theta_{\\text{frozen}}\\) fixed. This approach has several advantages. First, it dramatically reduces the amount of labeled data required for the new task. Second, it decreases training time since we only update a subset of parameters. Third, it often leads to better generalization, especially when target task data is limited.\nModern frameworks make transfer learning remarkably accessible. In natural language processing, transformer-based models like BERT (Devlin et al. 2019) and GPT (Radford et al. 2018; Brown et al. 2020) are routinely fine-tuned for downstream tasks such as sentiment analysis, named entity recognition, and question answering. These models, pre-trained on billions of tokens, capture rich linguistic representations that transfer across diverse language tasks [we explore transformers and language models in Chapters 24 and 25].\nThe practice of using pre-trained blocks extends beyond simple transfer learning. Modern architectures often incorporate multiple pre-trained components as building blocks. For instance, a system for visual question answering might combine a pre-trained CNN for image encoding with a pre-trained transformer for language understanding, with only a small fusion network trained from scratch.\n\n\nMixture of Experts\nAs models grow in scale and complexity, a natural question arises: must every parameter be active for every input? The mixture of experts (MoE) architecture provides an elegant answer by using different sub-networks (experts) for different inputs, with a gating mechanism that decides which experts to activate.\nThe core idea traces back to work by Jacobs et al. (1991), but has seen renewed interest in modern large-scale systems. Think of a medical diagnosis system where different specialists (cardiologist, neurologist, oncologist) examine patients based on their symptoms. A triage system routes each patient to the appropriate specialist. Similarly, in an MoE model, a gating network routes inputs to specialized expert networks.\nFormally, let \\(f_1, \\ldots, f_K\\) denote \\(K\\) expert models (networks), each implementing a function \\(f_k(X; \\theta_k)\\). A gating network \\(g(X; \\phi)\\) produces a probability distribution over experts, for example: \\[\ng(X; \\phi) = \\text{softmax}(W_g X + b_g) \\in \\mathbb{R}^K\n\\]\nThe final output is a weighted combination of expert predictions:\n\\[\n\\hat{Y}(X) = \\sum_{k=1}^K g_k(X; \\phi) \\cdot f_k(X; \\theta_k).\n\\]\nIn practice, to improve computational efficiency, only the top-\\(N\\) experts with highest gating scores are activated for each input, setting others to zero. This sparse activation pattern allows MoE models to have enormous capacity while maintaining reasonable computational costs.\nFrom a statistical perspective, Jiang and Tanner (1999a) provided important theoretical foundations for mixture of experts models. They demonstrated that hierarchical mixtures of experts (HME) can approximate arbitrary mean functions at a rate of \\(O(m^{-2/s})\\) in \\(L_p\\) norm, where \\(m\\) is the number of experts and \\(s\\) is the input dimensionality. This result establishes that MoE models are universal approximators with quantifiable convergence rates, connecting them to the broader statistical literature on nonparametric regression.\nThe work by Stroud, Müller, and Polson (2003) demonstrates an elegant application of mixture-of-experts principles within Bayesian inference for complex time series models. Their approach addresses the fundamental challenge of estimating hidden states in nonlinear state-space models where exact inference is mathematically intractable. The core innovation lies in using an auxiliary mixture model to approximate the intractable nonlinear components. This auxiliary model functions as a mixture of experts, where each expert is a simple linear regression model of the form \\(p^{a}(y_{t}|x_{t},z_{t}=k) = \\mathcal{N}(\\alpha_{k}+\\beta_{k}x_{t},\\tau_{k}^{2})\\). These linear experts provide local approximations of the true nonlinear function around specific “knots” in the state space. The gating mechanism employs state-dependent weights \\(\\pi_{k}(x_{t})\\) determined by Gaussian kernels centered at different locations. The current hidden state value \\(x_t\\) serves as input to this gating network, which determines the relevance of each linear expert for that particular region of the state space.\nA critical theoretical challenge with MoE models is identifiability: can the model parameters be uniquely determined from the data? Jiang and Tanner (1999b) showed that without constraints, MoE models suffer from inherent non-identifiability due to invariant transformations. For example, permuting expert labels or translating gating parameters can yield identical predictions. They established that by imposing order restrictions on expert parameters (such as requiring experts to be ordered by their intercepts) and proper initialization of gating parameters, MoE systems become identifiable. This work provides the statistical rigor necessary for reliable inference and interpretation of mixture of experts models, complementing the computational advances in modern implementations.\nRecent work has demonstrated the power of this approach at scale. Fedus, Zoph, and Shazeer (2022) trained a mixture of experts language model with 1.6 trillion parameters, where only a small fraction are active for any given input. The model achieves strong performance while requiring less computation per token than dense models of comparable quality. Similarly, Riquelme et al. (2021) showed that vision transformers with mixture of experts layers can achieve better accuracy-computation trade-offs than their dense counterparts.\nThe gating mechanism introduces an interesting learning problem: the network must simultaneously learn to specialize experts and to route inputs appropriately. Various training strategies have been proposed, including auxiliary losses that encourage load balancing across experts and techniques to promote expert specialization (Shazeer et al. 2017).\n\n\nEnsembles\nWhile mixture of experts learns a single integrated model with internal routing, ensemble methods combine predictions from multiple independently trained models. The rationale is straightforward: different models trained on the same data often make different errors, and averaging their predictions can reduce variance and improve overall performance.\nEnsemble methods have a long history in statistics and machine learning. The classical result from Dietterich (2000) shows that an ensemble is effective when individual models are accurate and diverse. Consider \\(M\\) models \\(f_1, \\ldots, f_M\\). For regression, a simple ensemble averages predictions:\n\\[\n\\hat{Y}_{\\text{ensemble}}(X) = \\frac{1}{M} \\sum_{m=1}^M f_m(X)\n\\]\nFor classification, we can use majority voting or average the predicted probabilities. The bias-variance decomposition provides insight into why ensembles work. The expected squared error of a single model can be written as:\n\\[\n\\E{(Y - f(X))^2} = \\text{Bias}^2 + \\text{Variance}\n\\]\nIf we average \\(M\\) models with the same bias but independent errors, the variance term reduces by a factor of \\(M\\), assuming the models are uncorrelated. In practice, models are not perfectly independent, but ensembles still typically achieve substantial variance reduction.\nIn deep learning, several ensemble strategies are commonly employed:\n\nModel averaging: Train multiple neural networks with different random initializations or architectures, then average their predictions at test time.\nSnapshot ensembles (Huang et al. 2017): Save model checkpoints at various points during training (particularly at local minima) and ensemble them.\nMulti-architecture ensembles: Combine models with different architectures (e.g., CNNs, transformers, and recurrent networks) to capture complementary patterns.\n\nA particularly effective technique is dropout as ensemble (Srivastava et al. 2014). During training, dropout randomly deactivates neurons with probability \\(p\\). At test time, all neurons are active but their outputs are scaled by \\((1-p)\\). Gal and Ghahramani (2016) showed that this can be interpreted as approximate Bayesian inference, where dropout at test time (called Monte Carlo dropout) produces an ensemble of sub-networks, providing both predictions and uncertainty estimates.\nThe computational cost of ensembles is their main drawback—inference requires running multiple models. However, the reliability gains are often worth the expense, particularly in high-stakes applications like medical diagnosis, autonomous driving, and financial forecasting. Modern competitions and benchmarks frequently use ensembles: winning solutions in Kaggle competitions typically combine dozens of models (Chen and Guestrin 2016).\n\n\nArchitectural Innovations\nThese three paradigms—pre-trained blocks, mixture of experts, and ensembles—represent different philosophies for building capable systems. Transfer learning emphasizes knowledge reuse, recognizing that representations learned on large datasets are broadly useful. Mixture of experts emphasizes conditional computation, activating only relevant capacity for each input. Ensembles emphasize diversity and robustness, combining multiple perspectives to improve reliability.\nModern systems often combine these approaches. For instance, a production system might ensemble multiple models, each of which uses pre-trained components and employs mixture of experts layers for efficiency. The field continues to develop new architectural patterns, but these fundamental ideas—reusing learned knowledge, routing computation dynamically, and aggregating diverse predictions—are likely to remain central to deep learning practice.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "18-nn.html#summary",
    "href": "18-nn.html#summary",
    "title": "18  Neural Networks",
    "section": "18.7 Summary",
    "text": "18.7 Summary\nIn this chapter, we introduced the mathematical foundations of neural networks and deep learning. We began by establishing that neural networks are hierarchical compositions of nonlinear transformations—a framework that extends generalized linear models while providing the flexibility to learn complex patterns from data. Key concepts covered include:\n\nFeedforward architecture: A neural network applies successive layers of weighted linear combinations followed by nonlinear activation functions, extracting increasingly abstract features at each layer.\nActivation functions: Functions like ReLU, \\(\\tanh\\), and sigmoid introduce nonlinearity, enabling networks to approximate arbitrary functions. ReLU has become the default choice for hidden layers due to its computational efficiency and favorable gradient properties.\nAuto-encoders: These networks learn compressed representations by training to reconstruct their inputs through a bottleneck layer, providing a powerful approach to unsupervised dimensionality reduction.\nTransfer learning and pre-trained blocks: Knowledge learned on large datasets can be transferred to new tasks, dramatically reducing data and computation requirements.\nMixture of experts: Conditional computation allows models to activate only relevant sub-networks for each input, enabling massive scale with manageable computational costs.\nEnsemble methods: Combining diverse models improves robustness and reduces variance, with techniques ranging from simple averaging to dropout-based approximations.\n\nThe implementations presented in this chapter—from simple classification problems to regression with learned basis functions—demonstrate that neural networks can be understood through the lens of traditional statistical methods while offering capabilities that extend far beyond them.\nLooking ahead, Chapter 22 covers convolutional neural networks (CNNs), which exploit spatial structure in images through weight sharing and local connectivity. Chapter 23 and Chapter 24 introduce transformers and large language models, architectures that have revolutionized natural language processing through attention mechanisms and massive scale. These architectures build directly on the principles established in this chapter: layered representations, learned features, and the power of deep hierarchical computation.\n\n\n\n\nBehnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2023. “Deep Generative Models for Vehicle Speed Trajectories.” Applied Stochastic Models in Business and Industry 39 (5): 701–19.\n\n\nBhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and Jianeng Xu. 2021. “Merging Two Cultures: Deep and Statistical Learning.” arXiv Preprint arXiv:2110.11561. https://arxiv.org/abs/2110.11561.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–94. New York, NY, USA: ACM.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1981. “Generating a Random Permutation with Random Transpositions.” Probability Theory and Related Fields 57 (2): 159–79.\n\n\nDietterich, Thomas G. 2000. “Ensemble Methods in Machine Learning.” In Multiple Classifier Systems, 1–15. Berlin, Heidelberg: Springer.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. “Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.” Applied Stochastic Models in Business and Industry 35 (3): 788–807.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” Journal of Machine Learning Research 23 (120): 1–39.\n\n\nFeller, William. 1971. An Introduction to Probability Theory and Its Applications. Wiley.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–59.\n\n\nHeaton, J. B., N. G. Polson, and Jan Hendrik Witte. 2016. “Deep Learning for Finance: Deep Portfolios.” Applied Stochastic Models in Business and Industry.\n\n\nHuang, Gao, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. 2017. “Snapshot Ensembles: Train 1, Get M for Free.” In International Conference on Learning Representations.\n\n\nJacobs, Robert A., Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. “Adaptive Mixtures of Local Experts.” Neural Computation 3 (1): 79–87.\n\n\nJiang, Wenxin, and Martin A. Tanner. 1999a. “Hierarchical Mixtures-of-Experts for Generalized Linear Models: Some Results on Denseness and Consistency.” In Proceedings of the Sixteenth International Conference on Machine Learning, 214–22. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\n———. 1999b. “On the Identifiability of Mixtures-of-Experts.” Neural Networks 12 (9): 1253–58.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022. “Deep Partial Least Squares for Iv Regression.” arXiv Preprint arXiv:2207.02612. https://arxiv.org/abs/2207.02612.\n\n\n———. 2023a. “Generative Causal Inference,” June. https://arxiv.org/abs/2306.16096.\n\n\n———. 2023b. “Feature Selection for Personalized Policy Analysis,” July. https://arxiv.org/abs/2301.00251.\n\n\nPolson, Nicholas G., and Vadim Sokolov. 2023. “Generative AI for Bayesian Computation.” https://arxiv.org/abs/2305.14972.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, and Vadim Sokolov. 2020. “Deep Learning: Computational Aspects.” Wiley Interdisciplinary Reviews: Computational Statistics 12 (5): e1500.\n\n\nPolson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. “Deep Learning Partial Least Squares.” arXiv Preprint arXiv:2106.14085. https://arxiv.org/abs/2106.14085.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.” OpenAI.\n\n\nRiquelme, Carlos, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. “Scaling Vision with Sparse Mixture of Experts.” In Advances in Neural Information Processing Systems, 34:8583–95.\n\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” In International Conference on Learning Representations.\n\n\nSokolov, Vadim. 2017. “Discussion of ‘Deep Learning for Finance: Deep Portfolios’.” Applied Stochastic Models in Business and Industry 33 (1): 16–18.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” Journal of Machine Learning Research 15 (1): 1929–58.\n\n\nStroud, Jonathan R., Peter Müller, and Nicholas G. Polson. 2003. “Nonlinear State-Space Models with State-Dependent Variances.” Journal of the American Statistical Association 98 (462): 377–86.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html",
    "href": "19-theorydl.html",
    "title": "19  Theory of Deep Learning",
    "section": "",
    "text": "19.1 Ridge and Projection Pursuit Regression\nThis chapter explores the theoretical foundations of deep learning through the lens of multivariate function approximation. We begin with ridge functions as fundamental building blocks—functions of the form \\(f(x) = g(w^Tx)\\) that represent one of the simplest forms of nonlinear multivariate functions by combining a single linear projection with a univariate nonlinear transformation. Their key geometric property—remaining constant along directions orthogonal to the projection vector \\(w\\)—makes them particularly useful for high-dimensional approximation.\nBuilding on ridge functions, we introduce projection pursuit regression, which approximates complex input-output relationships using linear combinations of ridge functions. This technique, developed in the 1980s, provides the mathematical foundation for understanding how neural networks decompose high-dimensional problems.\nThe chapter culminates with the Kolmogorov Superposition Theorem (KST), a profound result showing that any real-valued continuous function can be represented as a sum of compositions of single-variable functions. This theorem provides a theoretical framework for understanding how multivariate functions can be decomposed into simpler, more manageable components—a principle that underlies the architecture of modern neural networks.\nThroughout, we examine a central question: can we achieve superior performance through mathematically elegant representations of multivariate functions rather than raw computational power? This tension between theoretical efficiency and practical computation motivates much of the research in deep learning theory.\nTo understand the significance of this trade-off, we consider ridge functions, which represent a fundamental building block in multivariate analysis. Since our ultimate goal is to model arbitrary multivariate functions \\(f\\), we need a way to reduce dimensionality while preserving the ability to capture nonlinear relationships. Ridge functions accomplish this by representing one of the simplest forms of nonlinear multivariate functions, requiring only a single linear projection and a univariate nonlinear transformation. Formally, a ridge function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) takes the form \\(f(x) = g(w^Tx)\\), where \\(g\\) is a univariate function and \\(x,w \\in \\mathbb{R}^n\\). The non-zero vector \\(w\\) is called the direction. The term “ridge” reflects a key geometric property: the function remains constant along any direction orthogonal to \\(w\\). Specifically, for any direction \\(u\\) such that \\(w^Tu = 0\\), we have\n\\[\nf(x+u) = g(w^T(x+u)) = g(w^Tx) = f(x)\n\\]\nThis structural simplicity makes ridge functions particularly useful as building blocks for high-dimensional approximation.\nRidge functions play a central role in high-dimensional statistical analysis. For example, projection pursuit regression approximates input-output relations using a linear combination of ridge functions (Friedman and Stuetzle 1981; Huber 1985): \\[\n\\phi(x) = \\sum_{i=1}^{p}g_i(w_i^Tx),\n\\] where both the directions \\(w_i\\) and functions \\(g_i\\) are learnable parameters, and \\(w_i^Tx\\) represents the one-dimensional projection of the input vector \\(x\\) onto the direction defined by \\(w_i\\). Each \\(g_i(w_i^Tx)\\) can be interpreted as a learned feature extracted from the data. Diaconis and Shahshahani (1984) extended this approach using nonlinear functions of linear combinations, establishing the mathematical framework that would later inform the design of multi-layer neural networks.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#ridge-and-projection-pursuit-regression",
    "href": "19-theorydl.html#ridge-and-projection-pursuit-regression",
    "title": "19  Theory of Deep Learning",
    "section": "",
    "text": "From Approximation to Representation\nWhile ridge functions and projection pursuit provide powerful approximation tools, they rely on sums of increasing numbers of components to achieve precision. A fundamental question in deep learning theory is whether we can achieve exact representation of multivariate functions using a finite number of components, rather than just approximation.\nThis question addresses a core tension in machine learning: the trade-off between the mathematical efficiency of a representation and its computational feasibility. Modern deep learning succeeds not just because of hardware acceleration (GPUs), but because deep architectures may implicitly leverage efficient representations of high-dimensional functions.\nTo understand this, we turn to a foundational result that seemingly solves the representation problem entirely: the Kolmogorov Superposition Theorem.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#kolmogorov-superposition-theorem-kst",
    "href": "19-theorydl.html#kolmogorov-superposition-theorem-kst",
    "title": "19  Theory of Deep Learning",
    "section": "19.2 Kolmogorov Superposition Theorem (KST)",
    "text": "19.2 Kolmogorov Superposition Theorem (KST)\nKolmogorov demonstrated that any real-valued continuous function \\(f(\\mathbf{x})\\) on \\(E^n\\) can be represented as a composition of single-variable functions:\n\\[\nf(x_1,\\ldots,x_n) = \\sum_{q=1}^{2n+1} g_q\\left(\\phi_q(x_1,\\ldots,x_n)\\right)\n\\]\nwhere \\(g_q\\) are continuous single-variable functions defined on \\(\\phi_q(E^n)\\). Kolmogorov further showed that the \\(\\phi_q\\) functions can be decomposed into sums of single-variable functions:\n\\[\n\\phi_q(x_1,\\ldots,x_n) = \\sum_{i=1}^n \\psi_{q,i}(x_i)\n\\]\nThe Kolmogorov representation theorem Kolmogorov (1956) takes the form:\n\\[\nf(x_1,\\ldots,x_n) = \\sum_{q=1}^{2n+1} g_q\\left(\\sum_{i=1}^n \\psi_{q,i}(x_i)\\right)\n\\]\nThe theorem has been refined over time, with inner functions becoming Hölder continuous and Lipschitz continuous, requiring modifications to both outer and inner functions.\nThe inner functions \\(\\psi_{q,i}\\) partition the input space into distinct regions, and the outer function \\(g\\) must be constructed to provide the correct output values across these regions. For each input configuration, the inner functions generate a unique encoding, and \\(g\\) must map this encoding to the appropriate value of \\(f(x)\\). This creates a dictionary-like structure that associates each region with its corresponding output value.\nThe constructive proof of KST has been refined through several contributions. Sprecher (1965) provided the first explicit construction of the inner functions, though his proof contained technical gaps. Köppen (2000) corrected these errors and provided a complete algorithmic construction. More recently, Actor (2018) and Dembo (2021) proposed computational improvements to the algorithm. Braun and Riedmiller (2009) further enhanced the understanding by providing precise definitions of the shift parameters \\(\\delta_k\\) and characterizing the topological structure induced by the inner functions.\nA fundamental trade-off in KST exists between function smoothness and dimensionality. The inner functions \\(\\psi_{p,q}\\) can be chosen from two different function spaces, each offering distinct advantages. The first option is to use functions from \\(C^1([0,1])\\)—the space of continuously differentiable functions—but this limits the network’s ability to handle higher dimensions effectively. The second option is to relax the smoothness requirement to Hölder continuous functions (\\(\\psi_{p,q} \\in \\text{Hölder}_\\alpha([0,1])\\)), which satisfy the inequality \\(\\|\\psi(\\mathbf{x}) - \\psi(\\mathbf{y})\\| &lt; \\|\\mathbf{x}-\\mathbf{y}\\|^\\alpha\\) for vector-valued inputs. These functions are less smooth, but this “roughness” enables better approximation in higher dimensions.\n\nKolmogorov-Arnold Networks\nA significant development has been the emergence of Kolmogorov-Arnold Networks (KANs). The key innovation of KANs is their use of learnable functions rather than weights on the network edges. This replaces traditional linear weights with univariate functions, typically parametrized by splines, enhancing both representational capacity and interpretability.\nThere is a practical connection between KST and neural networks by showing that any KAN can be constructed as a 3-layer MLP. Consider a KST in the form of sums of functions, a two layer model:\n\\[\nf( x_1 , \\ldots , x_d ) = f( x) = ( g \\circ \\psi ) (x )\n\\]\nThen KAN not only a superposition of functions but also a particular case of a tree of discrete Urysohn operators:\n\\[\nU(x_1 , \\ldots , x_d ) = \\sum_{j=1}^d g_j (x_j )\n\\]\nThis structure enables fast, scalable algorithms that avoid backpropagation for any GAM model through projection descent with Kaczmarz schemes—iterative methods originally developed for solving systems of linear equations that can be adapted for training these networks (Kaczmarz 1937).",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#kolmogorov-generalized-additive-models-k-gam",
    "href": "19-theorydl.html#kolmogorov-generalized-additive-models-k-gam",
    "title": "19  Theory of Deep Learning",
    "section": "19.3 Kolmogorov Generalized Additive Models (K-GAM)",
    "text": "19.3 Kolmogorov Generalized Additive Models (K-GAM)\nRather than using learnable functions as network node activations, Polson and Sokolov directly use KST representation. This 2-layer network with non-differentiable inner function has architecture:\n\\[\nf(x_1,\\ldots,x_d) = \\sum_{q=0}^{2d} g_q(z_q)\n\\]\nwhere the inner layer embeds \\([0,1]^d\\) to \\(\\mathbb{R}^{2d+1}\\) via:\n\\[\nz_q = \\eta_q ( x_1 , \\ldots , x_d ) = \\sum_{p=1}^ d \\lambda_p \\psi  ( x_p + q a )\n\\]\nHere, \\(\\lambda_p = \\sum_{r=1}^\\infty \\gamma^{-(p-1)\\beta(r)}\\) represents \\(p\\)-adic expansion with \\(\\beta(r) = (n^r-1)/(n-1)\\) and \\(\\gamma \\geq d+2\\), \\(a = (\\gamma(\\gamma-1))^{-1}\\).\nThe Koppen function \\(\\psi\\) is defined through a recursive limit:\n\\[\n\\psi(x) = \\lim_{k \\rightarrow \\infty} \\psi_k\\left(\\sum_{l=1}^{k}i_l\\gamma^{-l}\\right)\n\\]\nwhere each \\(x \\in [0,1]\\) has the representation:\n\\[\nx = \\sum_{l=1}^{\\infty}i_l\\gamma^{-l} = \\lim_{k \\rightarrow \\infty} \\left(\\sum_{l=1}^{k}i_l\\gamma^{-l}\\right)\n\\]\nand \\(\\psi_k\\) is defined recursively as:\n\\[\n\\psi_k =\n\\begin{cases}\n    d, & d \\in D_1\\\\\n    \\psi_{k-1}(d-i_k\\gamma^{-k}) + i_k\\gamma^{-\\beta_n(k)}, & d \\in D_k,k&gt;1,i_k&lt;\\gamma-1\\\\\n    \\frac{1}{2}\\left(\\psi_k(d-\\gamma^{-k}) + \\psi_{k-1}(d+\\gamma^{-k})\\right), & d \\in D_k, k&gt;1, i_k = \\gamma - 1\n\\end{cases}\n\\]\nThe most striking aspect of KST is that it leads to a Generalized Additive Model (GAM) with fixed features that are independent of the target function \\(f\\). These features, determined by the Koppen function, provide universal topological information about the input space, effectively implementing a k-nearest neighbors structure that is inherent to the representation.\nThis leads to the following architecture. Any deep learner can be represented as a GAM with feature engineering (topological information) given by features \\(z_k\\) in the hidden layer:\n\\[\\begin{align*}\ny_i &= \\sum_{k=1}^{2n+1} g(z_k)\\\\\nz_k &= \\sum_{j=1}^n \\lambda^k\\psi(x_j + \\epsilon k) + k\n\\end{align*}\\]\nwhere \\(\\psi\\) is a single activation function common to all nodes, and \\(g\\) is a single outer function.\nOne approach replaces each \\(\\phi_j\\) with a single ReLU network \\(g\\):\n\\[\ng(x) = \\sum_{k=1}^K \\beta_k\\text{ReLU}(w_kx + b_k)\n\\]\nwhere \\(K\\) is the number of neurons.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#space-partitioning",
    "href": "19-theorydl.html#space-partitioning",
    "title": "19  Theory of Deep Learning",
    "section": "19.4 Space Partitioning",
    "text": "19.4 Space Partitioning\nThe partitioning of the input space by a deep learner is similar to that performed by decision trees and partition-based models such as CART, MARS, and RandomForests. However, trees are more local in the regions that they use to construct their estimators. Each neuron in a deep learning model corresponds to a manifold that divides the input space. In the case of the ReLU activation function \\(f(x) = \\max(0,x)\\), the manifold is simply a hyperplane. The neuron activates when the new observation is on the “right” side of this hyperplane, with the activation magnitude equal to the distance from the boundary. For example, in two dimensions, three neurons with ReLU activation functions will divide the space into seven regions, as shown in Figure 19.1.\n\n\n\n\n\n\nFigure 19.1: Space partition by three ReLU neurons in 2D.\n\n\n\nThe key difference between tree-based architectures and neural network-based models lies in how hyperplanes are combined. Figure 19.2 compares space decomposition by hyperplanes in tree-based versus neural network architectures. We compare a two-layer neural network (bottom row) with a tree model trained via the CART algorithm (top row). The network architecture used is:\n\\[\n\\begin{aligned}\nY &=  \\mathrm{softmax}(W^{(0)}Z^{(2)} + b^{(0)})\\\\\nZ^{(2)} &=  \\tanh(W^{(2)}Z^{(1)} + b^{(2)})\\\\\nZ^{(1)} &=  \\tanh(W^{(1)}X + b^{(1)})\n\\end{aligned}\n\\]\nThe weight matrices for simple data are \\(W^{(1)}, W^{(2)} \\in \\mathbb{R}^{2 \\times 2}\\); for circle data \\(W^{(1)} \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^{(2)} \\in \\mathbb{R}^{3 \\times 2}\\); and for spiral data we have \\(W^{(1)} \\in \\mathbb{R}^{2 \\times 2}\\) and \\(W^{(2)} \\in \\mathbb{R}^{4 \\times 2}\\). An advantage of deep architectures is that the number of hyperplanes grows exponentially with the number of layers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19.2: Space partition by tree architectures (top row) and deep learning architectures (bottom row) for three different data sets.\n\n\n\n\nConnection to Bayesian Model Averaging\nThis space partitioning perspective connects naturally to Bayesian ensemble methods. A Bayesian probabilistic approach can optimally weight predictors via model averaging:\n\\[\n\\hat{Y}(X) = \\sum_{r=1}^R w_r \\hat{Y}_r(X)\n\\]\nwhere \\(\\hat{Y}_r(X) = \\E{Y \\mid X, \\text{model } r}\\) and weights \\(w_r\\) are posterior model probabilities. Amit, Blanchard, and Wilder (2000) demonstrated the success of multiple randomized classifiers (e.g., an ensemble of 100 trees), which reduces error rates significantly compared to single trees by exploiting the weak correlation between diverse classifiers.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#kernel-smoothing-and-interpolation",
    "href": "19-theorydl.html#kernel-smoothing-and-interpolation",
    "title": "19  Theory of Deep Learning",
    "section": "19.5 Kernel Smoothing and Interpolation",
    "text": "19.5 Kernel Smoothing and Interpolation\nThe theory of kernel methods was developed by Fredholm in the context of integral equations (Fredholm 1903). The idea is to represent a function as a linear combination of basis functions, which are called kernels.\n\\[\nf(x) = \\int_{a}^{b} K(x,x') \\, d\\mu(x') \\quad \\text{where} \\quad \\mathbf{x} = ( x_1 , \\ldots , x_d )\n\\]\nHere, the unknown function \\(f(x)\\) is represented as a linear combination of kernels \\(K(x,x')\\) with unknown coefficients \\(\\phi(x')\\). The kernels are known, and the coefficients are unknown. The coefficients are found by solving the integral equation. The first work in this area was done by Abel who considered equations of the form above.\nNowadays, we call those equations Volterra integral equations of the first kind. Integral equations typically arise in inverse problems. Their significance extends beyond their historical origins, as kernel methods have become instrumental in addressing one of the fundamental challenges in modern mathematics: the curse of dimensionality.\nNadaraya (1964) and Watson (1964) independently proposed using kernels to estimate the regression function. The idea is to estimate \\(f(x)\\) at a point \\(x\\) by computing a weighted average of the response values \\(y_i\\) at nearby points \\(x_i\\), with the kernel function defining the weights.\nThe regression function estimate:\n\\[\n\\hat{f}(x) = \\sum_{i=1}^n  y_i K(x,x_i)/ \\sum_{i=1}^n K(x,x_i) ,\n\\]\nwith normalized kernel weights.\nBoth Nadaraya and Watson considered the symmetric kernel \\(K(x,x') = K(\\|x'-x\\|_2)\\), where \\(||\\cdot||_2\\) is the Euclidean norm. The most popular kernel of that sort is the Gaussian kernel:\n\\[\nK(x,x') = \\exp\\left( -\\dfrac{\\|x-x'\\|_2^2}{2\\sigma^2}\\right).\n\\]\nAlternatively, replacing the 2-norm with inner products: \\(K(x,x') = \\exp(x^Tx'/2\\sigma^2)\\).\nKernel methods are supported by numerous generalization bounds which often take the form of inequalities that describe the performance limits of kernel-based estimators. A particularly important example is the Bayes risk for \\(k\\)-nearest neighbors (\\(k\\)-NN), which can be expressed in a kernel framework as:\n\\[\n\\hat{f} ( x) =  \\sum_{i=1}^N w_i y_i        \\; \\text{where}\\; w_i := K( x_i , x ) /  \\sum_{i=1}^N K( x_i ,x )   \n\\]\n\\(k\\)-NN classifiers converge to error rates bounded relative to Bayes error rate, with relationships depending on class number. For binary classification, asymptotic \\(k\\)-NN error rate is at most \\(2R^*(1-R^*)\\) where \\(R^*\\) is Bayes error rate. Cover and Hart proved interpolated \\(k\\)-NN schemes are consistent estimators with performance improving as sample size increases.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#transformers-as-kernel-smoothing",
    "href": "19-theorydl.html#transformers-as-kernel-smoothing",
    "title": "19  Theory of Deep Learning",
    "section": "19.6 Transformers as Kernel Smoothing",
    "text": "19.6 Transformers as Kernel Smoothing\nBahdanau, Cho, and Bengio (2014) proposed kernel smoothing for sequence-to-sequence learning, estimating next-word probability using context vectors—weighted averages of input sequence vectors \\(h_j\\):\n\\[\nc_i = \\sum_{j=1}^n \\alpha_{ij} h_j,\n\\]\nwhere weights \\(\\alpha_{ij}\\) are defined by the kernel function:\n\\[\n\\alpha_{ij} = \\dfrac{\\exp\\left( e_{ij}\\right)}{\\sum_{k=1}^n \\exp\\left( e_{ik}\\right)}.\n\\]\nInstead of using a traditional similarity measure like the 2-norm or inner product, the authors used a neural network to define the energy function \\(e_{ij} = a(s_{i-1},h_j)\\). This neural network measures the similarity between the last generated element of the output sequence \\(s_{i-1}\\) and \\(j\\)-th element of the input sequence \\(h_j\\). The resulting context vector is then used to predict the next word in the sequence.\n\nTransformer\nTransformers have become the dominant architecture for natural language processing, achieving state-of-the-art results across tasks from machine translation to language modeling and text generation. Originally designed to handle sequential data, the transformer architecture has since been extended to computer vision (Vision Transformers), protein structure prediction (AlphaFold), and speech recognition. Its success stems from a novel self-attention mechanism that efficiently captures long-range dependencies.\nThe idea to use kernel smoothing for sequence to sequence was called “attention”, or cross-attention, by Bahdanau, Cho, and Bengio (2014). When used for self-supervised learning, it is called self-attention. When a sequence is mapped to a matrix \\(M\\), it is called multi-head attention. The concept of self-attention and attention for natural language processing was further developed by Vaswani et al. (2023) who developed a smoothing method that they called the transformer.\nThe transformer architecture revolves around a series of mathematical concepts and operations:\n\nEmbeddings: The input text is converted into vectors using embeddings. Each word (or token) is represented by a unique vector in a high-dimensional space.\nPositional Encoding: Since transformers do not have a sense of sequence order (like RNNs do), positional encodings are added to the embeddings to provide information about the position of each word in the sequence.\nMulti-Head Attention: The core of the transformer model. It enables the model to focus on different parts of the input sequence simultaneously. The attention mechanism is defined as: \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] where \\(Q\\), \\(K\\), and \\(V\\) are query, key, and value matrices respectively.\nQuery (Q), Key (K), and Value (V) Vectors: These are derived from the input embeddings. They represent different aspects of the input.\nScaled Dot-Product Attention: The attention mechanism calculates the dot product of the Query with all Keys, scales these values, and then applies a softmax function to determine the weights of the Values.\nMultiple ‘Heads’: The model does this in parallel multiple times (multi-head), allowing it to capture different features from different representation subspaces.\nLayer Normalization and Residual Connections: After each sub-layer in the encoder and decoder (like multi-head attention or the feed-forward layers), the transformer applies layer normalization and adds the output of the sub-layer to its input (residual connection). This helps in stabilizing the training of deep networks.\nFeed-Forward Neural Networks: Each layer in the transformer contains a fully connected feed-forward network applied to each position separately and identically. It is defined as: \\[ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 \\] where \\(W_1\\), \\(W_2\\), \\(b_1\\), and \\(b_2\\) are learnable parameters.\nOutput Linear Layer and Softmax: The decoder’s final output passes through a linear layer followed by a softmax layer. This layer converts the decoder output into predicted next-token probabilities.\nTraining and Loss Function: Transformers are often trained using a variant of Cross-Entropy Loss to compare the predicted output with the actual output.\nMasking: In the decoder, to prevent future tokens from being used in the prediction, a technique called ‘masking’ is applied.\nBackpropagation and Optimization: The model’s parameters are adjusted through backpropagation and optimization algorithms like Adam.\n\nLater, Lin et al. (2017) proposed using similar idea for self-supervised learning, where a sequence of words (sentence) is mapped to a single matrix:\n\\[\nM = AH,\n\\]\nwhere \\(H\\) is the matrix representing an input sequence \\(H = (h_1,\\ldots,h_n)\\) and \\(A\\) is the matrix of weights:\n\\[\nA = \\mathrm{softmax}\\left(W_2\\tanh\\left(W_1H^T\\right)\\right).\n\\]\nThis allows to represent a sequence of words of any length \\(n\\) using a “fixed size” \\(r\\times u\\) matrix \\(M\\), where \\(u\\) is the dimension of a vector that represents an element of a sequence (word embedding) and \\(r\\) is the hyper-parameter that defines the size of the matrix \\(M\\).\nThe primary advantage of transformers over recurrent architectures is their parallelizability—all positions in a sequence can be processed simultaneously rather than sequentially. Modern large language models such as BERT (Devlin et al. 2019), GPT (Brown et al. 2020), and T5 (Raffel et al. 2020) are built on transformer architectures. The transformer’s ability to capture long-range dependencies through attention, combined with its scalability to massive datasets and model sizes, has made it the foundation for the current generation of AI systems. See Tsai et al. (2019) for a comprehensive survey of transformer applications beyond NLP.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#deep-learning-as-representation-learning",
    "href": "19-theorydl.html#deep-learning-as-representation-learning",
    "title": "19  Theory of Deep Learning",
    "section": "19.7 Deep Learning as Representation Learning",
    "text": "19.7 Deep Learning as Representation Learning\nA fundamental perspective on deep learning is that it automates the process of feature engineering. Formally, we seek to find a predictor \\(\\mathbf{Y} = f(\\mathbf{X})\\). We can decompose this function into two stages: a data transformation (representation learning) \\(\\phi(\\mathbf{X})\\) and a predictive model \\(g(\\cdot)\\).\n\\[\n\\mathbf{Y} \\sim p(\\mathbf{Y} \\mid \\mathbf{Z}), \\quad \\mathbf{Z} = \\phi(\\mathbf{X})\n\\]\nHere, \\(\\mathbf{Z}\\) represents latent features. The transformation \\(\\phi(\\cdot)\\) effectively performs dimensionality reduction (or expansion) to uncover a structure where the relationship between \\(\\mathbf{Z}\\) and \\(\\mathbf{Y}\\) is simpler (often linear).\n\nDimensionality Expansion vs. Reduction\nClassically, statisticians approached complex functions through dimensionality expansion (basis expansion). Methods like Kernel Regression or Splines map the input \\(\\mathbf{X}\\) into a higher-dimensional space of features \\(\\phi(\\mathbf{X}) = (\\phi_1(\\mathbf{X}), \\ldots, \\phi_M(\\mathbf{X}))\\) (e.g., interactions, polynomials) and then fit a linear model. The “Kernel Trick” allows this to be done efficiently without explicit computation of features.\nDeep learning, in contrast, often emphasizes dimensionality reduction (feature learning). It searches for a low-dimensional manifold embedded in the high-dimensional input space.\n\n\nLinear Baselines: PCA and PLS\nTo understand the contribution of deep learning, it is useful to compare it with linear methods for Representation Learning:\n\nPrincipal Component Analysis (PCA): An unsupervised method that finds orthogonal directions (eigenvectors of \\(\\mathbf{X}^T\\mathbf{X}\\)) maximizing variance. It creates features \\(\\mathbf{Z}_{PCA} = \\mathbf{X}\\mathbf{W}\\) without knowledge of the target \\(\\mathbf{Y}\\).\nPartial Least Squares (PLS): A supervised method that finds directions maximizing the covariance between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\). It effectively finds features that are both variable and predictive.\n\nDeep Neural Networks extrapolate this concept to the nonlinear regime. They can be viewed as performing Nonlinear PLS. The layers of a network apply successive semi-affine transformations:\n\\[\n\\mathbf{Z}^{(l)} = \\sigma(W^{(l)}\\mathbf{Z}^{(l-1)} + b^{(l)})\n\\]\nThis hierarchical stacking allows the network to learn progressively more abstract representations, discovering nonlinear factors that maximize predictive power, much like a nonlinear generalization of the supervised dimensionality reduction performed by PLS.\n\n\nUncertainty Quantification\nWe can extend this framework to quantify uncertainty. Our probabilistic model is \\(\\mathbf{Y} \\mid \\mathbf{Z} \\sim p(\\mathbf{Y} \\mid \\mathbf{Z})\\), where \\(\\mathbf{Z} = g(\\mathbf{X})\\) is the deep feature extraction. A key result by Brillinger (2012) suggests that for certain input distributions (e.g., Gaussian), the central subspace estimated by methods like PLS or Deep Learning can be consistent.\nBy treating the top layer as a generalized linear model on learned features \\(\\mathbf{Z}\\), we can leverage standard Bayesian techniques for the final prediction layer while relying on the deep network for feature discovery. This “Last Layer Bayesian” approach provides a practical path to uncertainty quantification in deep learning, enabling the calculation of predictive intervals: \\[\n\\mathbf{Y}_{\\star} \\sim \\int p(\\mathbf{Y} \\mid \\mathbf{Z}_{\\star}, \\theta) p(\\theta \\mid \\mathcal{D}) d\\theta\n\\] where \\(\\theta\\) represents the parameters of the output layer.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#sec-double-descent",
    "href": "19-theorydl.html#sec-double-descent",
    "title": "19  Theory of Deep Learning",
    "section": "19.8 Double Descent",
    "text": "19.8 Double Descent\nDouble descent is a phenomenon of over-parameterized statistical models. In this section, we present a view of double descent from a Bayesian perspective. Over-parameterized models such as deep neural networks have an interesting re-descending property in their risk characteristics. This is a recent phenomenon in machine learning and has been the subject of many studies. As the complexity of the model increases, there is a U-shaped region corresponding to the traditional bias-variance trade-off, but then as the number of parameters equals the number of observations and the model becomes one of interpolation, the risk can become infinite and then, in the over-parameterized region, it re-descends—the double descent effect. We show that this has a natural Bayesian interpretation. Moreover, we show that it is not in conflict with the traditional Occam’s razor that Bayesian models possess, in that they tend to prefer simpler models when possible.\nEmpirically, the double descent effect was initially observed for high-dimensional neural network regression models and the good performance of these models on such tasks as large language models, image processing, and generative AI methods(Nareklishvili, Polson, and Sokolov 2023). The double descent effect extends the classical bias-variance trade-off curve that shrinkage estimators possess. This phenomenon was first observed in the context of linear regression(Belkin et al. 2019). The authors showed that the test error of the estimator can decrease as the number of parameters increases. Bach (2024) extends these results to stochastic regression models.\nInterpolators—estimators that achieve zero training error—were then shown to have attractive properties due to the double descent effect(Hastie et al. 2022). Our goal is to show that Bayesian estimators can also possess a double descent phenomenon. Interpolators such as ReLU neural networks(Polson, Sokolov, et al. 2017) have increased in popularity with many applications such as traffic flow modeling(Polson, Sokolov, et al. 2017) and high-frequency trading(Dixon, Polson, and Sokolov 2019), among many others.\nOccam’s razor—the favoring of simpler models over complex ones—is a natural feature of Bayesian methods that are based on the weight of evidence (a.k.a. the marginal likelihood of the data). To do this, they penalize models with higher complexity via a correction term as in the Bayesian Information Criterion (BIC). This seems inconsistent with the double descent phenomenon. We show that this is not the case, as even though Bayesian methods shift the posterior towards lower-complexity models, highly parameterized Bayesian models can also have good risk properties due to the conditional prior of parameters given the model. We illustrate this with an application to neural network models.\nDouble descent has been studied from a frequentist point of view in Belkin et al. (2019), Bach (2024). The phenomenon of double descent is illustrated in Figure 19.3. The first part of the curve represents the classical U-shaped bias-variance trade-off. The second part demonstrates the double descent phenomenon, where the test error of the estimator can decrease as the model becomes over-parameterized beyond the interpolation threshold. This phenomenon was later observed in the context of deep learning(Nakkiran et al. 2021). The authors showed that the test error of the estimator can decrease as the number of parameters increases.\n\n\n\n\n\n\nFigure 19.3\n\n\n\n\nExample 19.1 (Double Descent Demonstration using Polynomial Regression) To illustrate the double descent phenomenon in a concrete setting, we present a detailed example using polynomial regression with Legendre basis functions. This example demonstrates how the test error can exhibit the characteristic U-shaped curve followed by a re-descent as model complexity increases far beyond the interpolation threshold.\nOur demonstration uses a one-dimensional regression problem where we attempt to learn a sinusoidal function \\(f(x) = \\sin(5x)\\) from a small dataset of only \\(n = 20\\) observations sampled from the interval \\([-1, 1]\\). We add Gaussian noise with standard deviation \\(\\sigma = 0.3\\) to simulate realistic measurement error. The choice of a small sample size is crucial for observing double descent, as it creates a regime where the number of model parameters can substantially exceed the number of observations.\nWe fit polynomial models of varying degrees \\(d = 1, 2, \\ldots, 50\\) using Legendre polynomial basis functions. Legendre polynomials provide a numerically stable orthogonal basis that helps avoid the numerical instabilities associated with standard monomial bases in high-degree polynomial fitting. For each degree \\(d\\), we estimate the coefficients using the Moore-Penrose pseudoinverse, which provides the minimum-norm solution when the system is overdetermined (i.e., when \\(d &gt; n\\)).\nFigure 19.4 illustrates how model behavior changes dramatically across different polynomial degrees. The four panels show representative cases that capture the key phases of the double descent phenomenon:\n\nDegree 1 (Underparameterized): The linear model is too simple to capture the oscillatory nature of the underlying sine function, resulting in high bias and poor fit to both training and test data.\nDegree 5 (Classical Optimum): This represents the sweet spot of the classical bias-variance tradeoff, where the model has sufficient complexity to capture the main features of the sine function without overfitting severely.\nDegree 20 (Interpolation Threshold): At this degree, the model has exactly as many parameters as training observations, enabling perfect interpolation of the training data. However, the resulting fit exhibits wild oscillations between data points, leading to poor generalization performance.\nDegree 50 (Over-parameterized): Surprisingly, despite having far more parameters than observations, this highly over-parameterized model achieves better test performance than the interpolating model, demonstrating the double descent effect.\n\n\n\nCode\nn = 20  # number of samples\nsigma = 0.3  # stdev of noise\na = -1\nb = 1  # range of x values\n# Use Legendre polynomial basis by default\nG = np.polynomial.legendre.legvander\n\ndef poly(pts, beta, d):\n    return G(pts, d).dot(beta)\n\n# Initialize data\nground_truth = sin\nx = np.linspace(a, b, n)\ny = ground_truth(x) + sigma * np.random.normal(size=n)\nxdense = np.linspace(a, b, 100)\nygt = ground_truth(xdense)\n\n# Core functions\ndef solve_reg(A, y, lamb):\n    return la.solve(A.T.dot(A) + lamb * np.identity(A.shape[1]), A.T.dot(y))\n\ndef fit(d):\n    betaHat = la.pinv(G(x, d)).dot(y)\n    mseos = np.sqrt(np.mean((G(xdense, d).dot(betaHat) - ygt)**2))\n    mseis = np.sqrt(np.mean((G(x, d).dot(betaHat) - y)**2))\n    return betaHat, mseos, mseis\n\ndef run(d, ax):\n    \"\"\"Compute the regressor for degree d, and plot the solution.\"\"\"\n    betaHat, mseos, mseois = fit(d)\n    ax.plot(xdense, ygt, label='ground-truth')\n    ax.scatter(x, y, c='r', label='samples')\n    ax.plot(xdense, poly(xdense, betaHat, d), label='model')\n    ax.set_ylim(-2, 2)\n    ax.axis('off')\n    ax.legend()\n    ax.set_title('d=%d, MSE: Test=%.2f, Train=%.2f' % (d, mseos, mseois))\n\n\n\n\nCode\n# Create 2x2 subplot grid (all functions and variables are now persistent!)\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Double Descent Phenomenon: Polynomial Regression', fontsize=16)\n\n# Plot each degree in its respective subplot\nrun(1, axes[0, 0])   # Top left\nrun(5, axes[0, 1])   # Top right  \nrun(20, axes[1, 0])  # Bottom left\nrun(50, axes[1, 1])  # Bottom right\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 19.4: Double Descent Phenomenon: Polynomial Regression with Different Degrees\n\n\n\n\n\nNow, let’s plot the MSE curve. We will plot the test error (blue line) and the training error (red line) for different polynomial degrees from 1 to 50.\n\n\nCode\n# Generate MSE data for different polynomial degrees (using persistent functions!)\nnd = 50\nmse1 = np.zeros(nd)  # Test MSE\nmse2 = np.zeros(nd)  # Train MSE\n\nfor d in range(1, nd):\n    betaHat, mseos, mseois = fit(d)\n    mse1[d] = mseos\n    mse2[d] = mseois\nfig, ax1 = plt.subplots()\nax1.set_xlabel('Number of Features')\nax1.set_ylabel('MSE', color='black')\nax1.plot(np.log10(mse1), color='blue', label='out-of-sample')\nax2 = ax1.twinx()\nax2.plot(np.log10(mse2), color='red', label='in-sample')\n# plt.plot(mse1, label='out-of-sample')\n# plt.plot(mse2, label='in-sample')\nfig.legend()\n\n\n\n\n\n\n\n\nFigure 19.5: Bias-Variance Trade-off: Training and Test MSE vs Model Complexity\n\n\n\n\n\nThe key insight from Figure 19.5 is the characteristic double descent shape in the test error (blue line). The curve exhibits three distinct phases:\n\nClassical Regime: For low degrees (\\(d &lt; 5\\)), increasing model complexity reduces both bias and test error, following the traditional understanding of the bias-variance tradeoff.\nInterpolation Crisis: Around the interpolation threshold (\\(d \\approx n = 20\\)), test error peaks dramatically as the model begins to perfectly fit the training data while generalizing poorly.\nOver-parameterized Regime: For very high degrees (\\(d &gt; 30\\)), test error decreases again, demonstrating that extreme over-parameterization can lead to improved generalization despite the model’s ability to memorize the training data.\n\nThis behavior challenges the conventional wisdom that more parameters necessarily lead to worse generalization. The double descent phenomenon arises from the implicit regularization effects of minimum-norm solutions in over-parameterized settings. When \\(d &gt; n\\), the pseudoinverse solution corresponds to the minimum \\(\\ell_2\\)-norm coefficients among all possible interpolating solutions. This implicit bias toward simpler functions can lead to surprisingly good generalization properties.\nWhile this example uses polynomial regression for clarity, the double descent phenomenon has been observed across a wide range of modern machine learning models, including deep neural networks, random forests, and kernel methods. The implications for practice are significant. Given that model selection is time consuming and computationally expensive, this example shows, that instead of spending time to do model selection to find the “sweet spot” model with 5-degree polynomial, we just over-parametrise and get a good model for free!\nThis example serves as a concrete illustration of how classical statistical intuitions about model complexity may not apply in contemporary machine learning settings, particularly when dealing with over-parameterized models that have become increasingly common in practice.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#application",
    "href": "19-theorydl.html#application",
    "title": "19  Theory of Deep Learning",
    "section": "19.9 Application",
    "text": "19.9 Application\n\nSimulated Data\nWe also apply the K-GAM architecture to a simulated dataset to evaluate its performance on data with known structure and relationships. The dataset contains 100 observations generated from the following function:\n\\[\\begin{align*}\n    &y = \\mu(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1)\\\\\n         &\\mu(x) = 10\\sin(\\pi x_1 x_2) + 20(x_3-0.5)^2 + 10x_4 + 5x_5.\n\\end{align*}\\]\nThe goal is to predict the function \\(y(x)\\) based on the input \\(x\\). The dataset is often used as a benchmark dataset for regression algorithms due to its diverse mix of relationships (linear, quadratic, nonlinear, Gaussian random noise) between the input features and the target function.\nWe use the Koppen function to transform the five-dimensional input into a set of 11 features (\\(2d+1\\)). We then learn the outer function \\(g\\) using a ReLU network. To thoroughly investigate the model’s capabilities, we implement two distinct approaches to learning the outer function. The first approach uses different \\(g\\) functions for each feature, following the original KST formulation. This allows each function to specialize in capturing specific patterns, but might be more difficult to train and has more parameters. The second approach uses a single \\(g\\) function for all features, as proposed by Lorentz (1976), providing a more unified and parameter-efficient representation.\nThe first model with multiple \\(g_i\\) functions has dimensions: \\(W^0_i \\in \\mathbb{R}^{16\\times 1}\\) and \\(W^j_i \\in \\mathbb{R}^{16\\times 16}\\) for \\(j=1,\\ldots,18\\).\nThe second architecture using single function \\(g\\) for all features maintains similar structure but increases inner layer width from 16 to 200. This increased capacity allows single functions to learn complex patterns, compensating for the constraint versus multiple specialized functions.\n\n\nTraining Rates\nConsider nonparametric regression \\(y_i= f (x_i) + \\epsilon_i\\) where \\(x_i = ( x_{1i} , \\ldots , x_{di} )\\). We estimate \\(f( x_1 , \\ldots , x_d )\\) for \\(x  = ( x_1 , \\ldots , x_d ) \\in [0,1]^d\\). From a classical risk perspective:\n\\[\nR ( f , \\hat{f}_N ) = E_{X,Y} \\left ( \\lVert  f - \\hat{f}_N \\rVert^2 \\right ),\n\\] where \\(\\lVert \\cdot \\rVert\\) denotes \\(L^2(P_X)\\)-norm.\nUnder standard assumptions, optimal minimax rate \\(\\inf_{\\hat{f}} \\sup_f R( f , \\hat{f}_N ) = O_p ( N^{- 2 \\beta /( 2 \\beta + d )} )\\) for \\(\\beta\\)-Holder smooth functions \\(f\\). This rate depends on dimension \\(d\\), problematic in high dimensions. Restricting function classes yields better rates independent of \\(d\\), avoiding the curse of dimensionality. Common approaches include linear superpositions (ridge functions) and projection pursuit models.\nAnother asymptotic result comes from posterior concentration. Here, \\(\\hat{f}_N\\) is a regularized MAP (maximum a posteriori) estimator solving:\n\\[\n\\hat{f}_N = \\arg \\min_{ \\hat{f}_N } \\frac{1}{N} \\sum_{i=1}^N ( y_i - \\hat{f}_N ( x_i ))^2 + \\phi ( \\hat{f}_N )\n\\]\nwhere \\(\\phi(\\hat{f})\\) is regularization. Under appropriate conditions, posterior distribution \\(\\Pi(f | x, y)\\) concentrates around true function at minimax rate (up to \\(\\log N\\) factor).\nA key result in the deep learning literature provides convergence rates for deep neural networks. Given a training dataset of input-output pairs \\(( x_i , y_i)_{i=1}^N\\) from the model \\(y = f(x) + \\epsilon\\) where \\(f\\) is a deep learner (i.e. superposition of functions)\n\\[\nf = g_L \\circ \\ldots g_1 \\circ g_0\n\\]\nwhere each \\(g_i\\) is a \\(\\beta_i\\)-smooth Holder function with \\(d_i\\) variables, that is \\(| g_i (x) -g_i (y) &lt; | x-y |^\\beta\\).\nThen, the estimator has optimal rate:\n\\[\nO \\left ( \\max_{1\\leq i \\leq L } N^{- 2 \\beta^* /( 2 \\beta^* + d_i ) } \\right )  \\; \\text{where}\\; \\beta_i^* = \\beta_i \\prod_{l = i+1}^L \\min ( \\beta_l , 1 )\n\\]\nThis result can be applied to various function classes, including generalized additive models of the form\n\\[\nf_0 ( x ) = h \\left ( \\sum_{p=1}^d f_{0,p} (x_p) \\right )\n\\]\nwhere \\(g_0(z) = h(z)\\), \\(g_1 ( x_1 , \\ldots , x_d ) = ( f_{01}(x_1) , \\ldots , f_{0d} (x_d) )\\) and \\(g_2 ( y_1 , \\ldots , y_d ) = \\sum_{i=1}^d y_i\\). In this case, \\(d_1 = d_2 = 1\\), and assuming \\(h\\) is Lipschitz, we get an optimal rate of \\(O(N^{-1/3})\\), which is independent of \\(d\\).\nSchmidt-Hieber (2021) show that deep ReLU networks also have optimal rate of \\(O( N^{-1/3} )\\) for certain function classes. For \\(3\\)-times differentiable (e.g. cubic B-splines ), Coppejans (2004) finds a rate of \\(O( N^{-3/7} ) = O( N^{-3/(2 \\times 3 + 1) } )\\). Igelnik and Parikh (2003) finds a rate \\(O( N^{-1} )\\) for Kolmogorov Spline Networks.\nFinally, it’s worth noting the relationship between expected risk and empirical risk. The expected risk, \\(R\\), is typically bounded by the empirical risk plus a term of order \\(1/\\sqrt{N}\\):\n\\[\nR(y, f^\\star) \\leq \\frac{1}{N} \\sum_{i=1}^N R(y_i, f^\\star(x_i)) + O\\left(\\frac{\\|f\\|}{\\sqrt{N}}\\right)\n\\]\nwhere \\(f^\\star\\) is the minimizer of the expected risk. However, in the case of interpolation, where the model perfectly fits the training data, the empirical risk term becomes zero, leaving only the \\(O(1/\\sqrt{N})\\) term.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "19-theorydl.html#conclusion",
    "href": "19-theorydl.html#conclusion",
    "title": "19  Theory of Deep Learning",
    "section": "19.10 Conclusion",
    "text": "19.10 Conclusion\nThis chapter has traced the theoretical foundations of deep learning through the lens of multivariate function approximation. Several key insights emerge from this analysis:\nMathematical Foundations: The Kolmogorov Superposition Theorem provides a rigorous theoretical basis for understanding why neural networks can approximate any continuous function. The theorem shows that any multivariate function can be decomposed into compositions of univariate functions—a principle that directly informs the layered architecture of modern neural networks.\nRepresentation vs. Computation: While KST guarantees the existence of such representations, the inner functions (like the Koppen function) are typically non-smooth and computationally challenging. This highlights a fundamental tension: mathematically elegant representations may not always translate to efficient computation.\nConnections Across Methods: We have seen deep connections between seemingly disparate approaches. Transformers can be understood as kernel smoothing methods. Deep networks perform sophisticated space partitioning similar to decision trees. The K-GAM framework reveals that deep learners are essentially generalized additive models with automatically learned features.\nDouble Descent and Over-parameterization: Perhaps most surprisingly, the double descent phenomenon suggests that classical intuitions about the bias-variance trade-off may not apply to modern over-parameterized models. Networks with more parameters than training examples can still generalize well, challenging conventional wisdom about model complexity.\nThese theoretical insights not only deepen our understanding of why deep learning works but also suggest new directions for developing more efficient and interpretable architectures. The ongoing research into Kolmogorov-Arnold Networks and related approaches hints at the possibility of achieving the representational power of deep learning with potentially simpler, more interpretable structures.\n\n\n\n\nActor, Jonas. 2018. “Computation for the Kolmogorov Superposition Theorem.” {{MS Thesis}}, Rice.\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple Randomized Classifiers: MRCL.”\n\n\nBach, Francis. 2024. “High-Dimensional Analysis of Double Descent for Linear Regression with Random Projections.” SIAM Journal on Mathematics of Data Science 6 (1): 26–50.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. “Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.” Proceedings of the National Academy of Sciences 116 (32): 15849–54.\n\n\nBraun, Heinrich, and Martin Riedmiller. 2009. “Constructive Neural Network Learning Algorithms for Pattern Classification.” IEEE Transactions on Neural Networks 20 (1): 84–97.\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model With ‘Gaussian’ Regressor Variables.” In Selected Works of David Brillinger, edited by Peter Guttorp and David Brillinger, 589–606. Selected Works in Probability and Statistics. New York, NY: Springer.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nCoppejans, Mark. 2004. “On Kolmogorov’s Representation of Functions of Several Variables by Functions of One Variable.” Journal of Econometrics 123 (1): 1–31.\n\n\nDembo, Amir. 2021. “A Note on the Universal Approximation Capability of Deep Neural Networks.” arXiv Preprint arXiv:2104.xxxxx.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1984. “On Nonlinear Functions of Linear Combinations.” SIAM Journal on Scientific and Statistical Computing 5 (1): 175–91.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. “Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.” Applied Stochastic Models in Business and Industry 35 (3): 788–807.\n\n\nFredholm, Ivar. 1903. “Sur Une Classe d’équations Fonctionnelles.” Acta Mathematica 27 (none): 365–90.\n\n\nFriedman, Jerome H., and Werner Stuetzle. 1981. “Projection Pursuit Regression.” Journal of the American Statistical Association 76 (376): 817–23.\n\n\nHastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. 2022. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” The Annals of Statistics 50 (2): 949–86.\n\n\nHuber, Peter J. 1985. “Projection Pursuit.” The Annals of Statistics 13 (2): 435–75.\n\n\nIgelnik, B., and N. Parikh. 2003. “Kolmogorov’s Spline Network.” IEEE Transactions on Neural Networks 14 (4): 725–33.\n\n\nKaczmarz, Stefan. 1937. “Angenäherte Auflösung von Systemen Linearer Gleichungen.” Bulletin International de l’Académie Polonaise Des Sciences Et Des Lettres 35: 355–57.\n\n\nKolmogorov, AN. 1956. “On the Representation of Continuous Functions of Several Variables as Superpositions of Functions of Smaller Number of Variables.” In Soviet. Math. Dokl, 108:179–82.\n\n\nKöppen, Mario. 2000. “The Curse of Dimensionality.” 5th Online World Conference on Soft Computing in Industrial Applications (WSC5) 1: 4–8.\n\n\nLin, Zhouhan, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. “A Structured Self-attentive Sentence Embedding.” arXiv. https://arxiv.org/abs/1703.03130.\n\n\nLorentz, George G. 1976. “The 13th Problem of Hilbert.” In Proceedings of Symposia in Pure Mathematics, 28:419–30. American Mathematical Society.\n\n\nNadaraya, E. A. 1964. “On Estimating Regression.” Theory of Probability & Its Applications 9 (1): 141–42.\n\n\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2021. “Deep Double Descent: Where Bigger Models and More Data Hurt*.” Journal of Statistical Mechanics: Theory and Experiment 2021 (12): 124003.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2023. “Generative Causal Inference,” June. https://arxiv.org/abs/2306.16096.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” Journal of Machine Learning Research 21 (140): 1–67.\n\n\nSchmidt-Hieber, Johannes. 2021. “The Kolmogorov–Arnold Representation Theorem Revisited.” Neural Networks 137 (May): 119–26.\n\n\nSprecher, David A. 1965. “On the Structure of Continuous Functions of Several Variables.” Transactions of the American Mathematical Society 115: 340–55.\n\n\nTsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. “Transformer Dissection: A Unified Understanding of Transformer’s Attention via the Lens of Kernel.” arXiv. https://arxiv.org/abs/1908.11775.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nWatson, Geoffrey S. 1964. “Smooth Regression Analysis.” Sankhyā: The Indian Journal of Statistics, Series A (1961-2002) 26 (4): 359–72.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Theory of Deep Learning</span>"
    ]
  },
  {
    "objectID": "20-sgd.html",
    "href": "20-sgd.html",
    "title": "20  Gradient Descent",
    "section": "",
    "text": "20.1 Deep Learning and Least Squares\nHow does a neural network with millions of parameters learn the right values? Classical statistical models rely on second-order optimization: least squares for linear regression and Newton-type methods like Broyden-Fletcher-Goldfarb-Shanno (BFGS) for generalized linear models. These algorithms use curvature information—the Hessian matrix—to take efficient steps toward the optimum.\nHowever, second-order optimization algorithms do not scale well to deep learning models. When a model has millions or billions of parameters, computing and storing the Hessian matrix becomes prohibitive from both computational and memory standpoints. Instead, first-order gradient descent methods—which require only gradient information—are the workhorses for training deep neural networks.\nThis chapter unifies a theme that already appeared earlier: in Chapter 11 we framed estimation as likelihood maximization and showed how the negative log-likelihood becomes a loss. In Chapter 8 we used the same idea computationally to fit Gaussian process hyperparameters by maximizing a marginal likelihood (via optim). Here we make the optimization mechanics explicit and reusable.\nGiven a parametric model \\(f_{\\theta}(x)\\), the problem of parameter estimation (when the likelihood belongs to the exponential family) is an optimization problem: \\[\n    \\min_{\\theta} l(\\theta) := -\\dfrac{1}{n} \\sum_{i=1}^n \\log p(y_i \\mid x_i, \\theta),\n\\] where \\(l\\) is the negative log-likelihood of a sample (equivalently, \\(l(\\theta)=-\\ell(\\theta)\\) up to scaling, with \\(\\ell(\\theta)=\\log L(\\theta)\\) as in Chapter 11), and \\(\\theta\\) is the vector of parameters. The gradient descent method is an iterative algorithm that starts with an initial guess \\(\\theta^{0}\\) and then updates the parameter vector \\(\\theta\\) at each iteration \\(t\\) as follows: \\[\n    \\theta^{t+1} = \\theta^t - \\alpha_t \\nabla l(\\theta^t).\n\\] Here \\[\n\\nabla l(\\theta^t) = \\left(\\frac{\\partial l}{\\partial \\theta_0}, \\ldots,\\frac{\\partial l}{\\partial \\theta_p}\\right),\n\\] is the gradient of the loss function with respect to the parameters at iteration \\(t\\).\nThis algorithm is based on the simple fact that a function value decreases in the direction of the negative gradient, at least locally. This fact follows from the definition of the gradient, which is the vector of partial derivatives of the function with respect to its parameters. The gradient points in the direction of the steepest ascent, and thus the negative gradient points in the direction of the steepest descent. It is easy to see in the figure below, where the function \\(l(x)\\) is shown in blue and the gradient at the point \\(x_0\\) is shown in red. The negative gradient points in the direction of the steepest descent, which is the direction we want to go to minimize the function. We can easily see it in a one-dimensional case. The derivative is defined as \\[\n    l'(\\theta) = \\lim_{h \\to 0} \\frac{l(\\theta+h) - l(\\theta)}{h}.\n\\] The derivative is the slope of the tangent line to the function at the point \\(\\theta\\). The derivative is positive, when \\(l(\\theta+h) &gt; l(\\theta)\\) for small \\(h &gt; 0\\), then the function is increasing at that point, and if it is negative, then the function is decreasing at that point. Thus, we can use the negative derivative to find the direction of the steepest descent.\nFor the derivative (or gradient in the multivariate case) to be defined, the function \\(l(\\theta)\\) must be continuous and smooth at the point \\(\\theta\\). In practice, this means that the function must not have any sharp corners or discontinuities. For example, \\(f(\\theta) = |\\theta|\\) has a sharp corner at zero, with a derivative of \\(+1\\) for \\(\\theta &gt; 0\\) and \\(-1\\) for \\(\\theta &lt; 0\\). At zero itself, the derivative is undefined—this is called a non-differentiable point. Another example is the sign function \\(f(\\theta) = \\text{sign}(\\theta)\\), which has a discontinuity at zero.\nThis matters for deep learning because the popular ReLU activation function \\(\\max(0, x)\\) has a non-differentiable point at zero. Gradient descent—despite operating on these non-smooth loss surfaces—remains the workhorse for training deep neural networks. In practice, this works because: (1) the probability of landing exactly at a non-differentiable point is zero for continuous distributions, and (2) subgradients provide valid descent directions even at kinks.\nThe second component of the gradient descent algorithm is the learning rate \\(\\alpha_t\\), which is a positive scalar that controls the step size of the update (a.k.a. learning rate). The learning rate is a hyperparameter that needs to be tuned for each problem. If the learning rate is too small, the algorithm will converge slowly, and if it is too large, the algorithm may diverge or oscillate around the minimum. The Figure 20.1 below shows the effect of different learning rates on the convergence of the gradient descent algorithm. The blue line is the function \\(l(\\theta)\\), the red arrows are the updates of the parameter from \\(\\theta_0 = 2\\). When \\(\\alpha = 0.5\\) (just right), the algorithm converges to the minimum in one step, when \\(\\alpha = 0.25\\), it takes two steps, and when \\(\\alpha = 1\\), it “overshoots” the minimum and goes to the other side of the function, which is not desirable.\nIn practice, the learning rate is often set to a small value, such as \\(0.01\\) or \\(0.001\\), and then adjusted during the training process. There are several techniques for adjusting the learning rate, such as learning rate decay, where the learning rate is reduced by a factor at each epoch, or adaptive learning rates, where the learning rate is adjusted based on the gradient of the loss function.\nThe deep learning model approximates the relation between inputs \\(x\\) and outputs \\(y\\) using a non-linear function \\(f_{\\theta}(x)\\), where \\(\\theta\\) is a vector of parameters. The goal is to find the optimal value of \\(\\theta\\) that minimizes the negative log-likelihood (loss function), given a training data set \\(D = \\{x_i,y_i\\}_{i=1}^n\\). The loss function is a measure of discrepancy between the true value of \\(y\\) and the predicted value \\(f_{\\theta}(x)\\) \\[\n    l(\\theta) = - \\sum_{i=1}^n \\log p(y_i | x_i, \\theta),\n\\] where \\(p(y_i | x_i, \\theta)\\) is the conditional likelihood of \\(y_i\\) given \\(x_i\\) and \\(\\theta\\). In the case of regression, we have \\[\n  y_i = f_{\\theta}(x_i) + \\epsilon, ~ \\epsilon \\sim N(0,\\sigma^2),\n\\] The loss function is then (dropping the constant term \\(-n\\log(\\sigma\\sqrt{2\\pi})\\)): \\[\n    l(\\theta) = - \\sum_{i=1}^n \\log p(y_i | x_i, \\theta) \\propto \\sum_{i=1}^n (y_i - f_{\\theta}(x_i))^2,\n\\]\nNow, let’s demonstrate the gradient descent algorithm on a simple example of linear regression using the iris dataset.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#deep-learning-and-least-squares",
    "href": "20-sgd.html#deep-learning-and-least-squares",
    "title": "20  Gradient Descent",
    "section": "",
    "text": "Example 20.1 (Linear Regression) Linear regression can be viewed as a degenerate neural network: a single layer with no hidden units and an identity activation function. The insight of deep learning is that using deep and narrow architectures—multiple layers with fewer units per layer—enables learning complex, hierarchical representations.\nLet’s fit a linear regression model to the iris dataset using gradient descent. We use \\(y =\\) Petal.Length as the response variable and \\(x =\\) Petal.Width as the predictor. The model is \\[\n    y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i,\n\\] or in matrix form: \\[\n    y = X \\theta + \\epsilon,\n\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), \\(X = [1 ~ x]\\) is the design matrix with first column being all ones.\nThe negative log-likelihood function for the linear regression model is: \\[\n    l(\\theta) = \\sum_{i=1}^n (y_i - \\theta_0 - \\theta_1 x_i)^2.\n\\] The gradient is then: \\[\n    \\nabla l(\\theta) = -2 \\sum_{i=1}^n (y_i - \\theta_0 - \\theta_1 x_i) \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix}.\n\\] In matrix form, we have: \\[\n    \\nabla l(\\theta) = -2 X^T (y - X \\theta).\n\\]\nFirst, let’s load the data and prepare the design matrix. We will use the Petal.Length as a response variable and Petal.Width as a predictor.\n\ndata(iris)\ny = iris$Petal.Length\n\nLet’s implement the gradient descent algorithm to estimate the parameters of the linear regression model. We will use a small learning rate and a large number of iterations to ensure convergence.\n\n# initialize theta\ntheta &lt;- matrix(c(0, 0), nrow = 2, ncol = 1)\n# learning rate\nalpha &lt;- 0.0001\n# number of iterations\nn_iter &lt;- 40\nloss = numeric(n_iter)\n# gradient descent\nfor (i in 1:n_iter) {\n    # compute gradient\n    grad &lt;- -2 * t(x) %*% (y - x %*% theta)\n    # update theta\n    theta &lt;- theta - alpha * grad\n    # compute loss\n    loss[i] &lt;- sum((y - x %*% theta)^2)\n}\n\nOur gradient descent minimization algorithm finds the following coefficients\n\n\n\n\n\nIntercept (\\(\\theta_0\\))\nPetal.Width (\\(\\theta_1\\))\n\n\n\n\n1.3\n2\n\n\n\n\n\nLet’s plot the data and model estimated using gradient descent\n\nplot(x[,2],y,pch=16, xlab=\"Petal.Width\")\nabline(theta[2],theta[1], lwd=3,col=\"red\")\n\n\n\n\n\n\n\n\nLet’s compare it to the standard estimation algorithm\n\nm = lm(Petal.Length~Petal.Width, data=iris)\n\n\n\n\n(Intercept)\nPetal.Width\n\n\n\n\n1.1\n2.2\n\n\n\n\n\nThe values found by gradient descent are very close to the ones found by the standard OLS algorithm.\nThe standard way to monitor the convergence of the gradient descent algorithm is to plot the value of the loss function at each iteration.\n\nplot(loss[1:40], type='l', xlab=\"Iteration\", ylab=\"Loss\", bty='n', col=\"blue\", lwd=2)\nabline(h=min(loss), lty=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe can see that the loss smoothly decays to the minimum value. This is because our loss function is convex (quadratic function of the parameters \\(\\theta\\)), meaning that it has a single global minimum and no local minima and our learning rate of \\(0.0001\\) is small enough to ensure that the algorithm does not overshoot the minimum. However, this is often not the case for more complex models, such as deep neural networks. In those cases, the loss function may oscillate between multiple local minima and saddle points, which can make the convergence of the gradient descent algorithm difficult. In practice, we often use a larger learning rate and a modified version of the gradient descent algorithm, such as Nesterov accelerated gradient descent (NAG) or Adam, to help with the convergence.\n\n\nExample 20.2 (Logistic Regression) Logistic regression is a generalized linear model (GLM) with a logit link function, defined as: \\[\n    \\log \\left(\\frac{p}{1-p}\\right) = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_p x_p,\n\\] where \\(p\\) is the probability of the positive class. The negative log-likelihood function for logistic regression is a cross-entropy loss \\[\n    l(\\theta) = - \\sum_{i=1}^n \\left[ y_i \\log p_i + (1-y_i) \\log (1-p_i) \\right],\n\\] where \\(p_i = 1/\\left(1 + \\exp(-\\theta_0 - \\theta_1 x_{i1} - \\ldots - \\theta_p x_{ip})\\right)\\). The derivative of the negative log-likelihood function is \\[\n    \\nabla l(\\theta) = - \\sum_{i=1}^n \\left[ y_i - p_i \\right] \\begin{pmatrix} 1 \\\\ x_{i1} \\\\ \\vdots \\\\ x_{ip} \\end{pmatrix}.\n\\] In matrix notation, we have \\[\n    \\nabla l(\\theta) = - X^T (y - p).\n\\] Let’s implement the gradient descent algorithm now.\n\ny = ifelse(iris$Species==\"setosa\",1,0)\nx = cbind(rep(1,150),iris$Sepal.Length)\nlrgd  = function(x,y, alpha, n_iter) {\n  theta &lt;- matrix(c(0, 0), nrow = 2, ncol = 1)\n  for (i in 1:n_iter) {\n    # compute gradient\n    p = 1/(1+exp(-x %*% theta))\n    grad &lt;- -t(x) %*% (y - p)\n    # update theta\n    theta &lt;- theta - alpha * grad\n  }\n  return(theta)\n}\ntheta = lrgd(x,y,0.005,20000)\n\nThe gradient descent parameters are\n\n\n\n\n\nIntercept (\\(\\theta_0\\))\nSepal.Length (\\(\\theta_1\\))\n\n\n\n\n28\n-5.2\n\n\n\n\n\nAnd the plot is\n\n\nplot(x[,2],y,pch=16, xlab=\"Sepal.Length\")\nlines(x[,2],p,type='p', pch=16,col=\"red\")\n\n\n\n\n\n\n\n\nLet’s compare it to the standard estimation algorithm\n\nglm(y~x-1, family=binomial(link=\"logit\")) %&gt;% tidy() %&gt;% kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nx1\n27.8\n4.83\n5.8\n0\n\n\nx2\n-5.2\n0.89\n-5.8\n0",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#stochastic-gradient-descent",
    "href": "20-sgd.html#stochastic-gradient-descent",
    "title": "20  Gradient Descent",
    "section": "20.2 Stochastic Gradient Descent",
    "text": "20.2 Stochastic Gradient Descent\nStochastic gradient descent (SGD) is a variant of the gradient descent algorithm. The main difference is that instead of computing the gradient over the entire dataset, SGD computes the gradient over a randomly selected subset of the data. This allows SGD to be applied to estimate models when the dataset is too large to fit into memory, which is often the case with deep learning models. The SGD algorithm replaces the gradient of the negative log-likelihood function with the gradient computed over a randomly selected subset of the data: \\[\n    \\nabla l(\\theta) \\approx \\dfrac{1}{|B|} \\sum_{i \\in B} \\nabla l(y_i, f(x_i, \\theta)),\n\\] where \\(B \\subseteq \\{1,2,\\ldots,n\\}\\) is a batch (random subset) sampled from the dataset, and \\(|B|\\) denotes the batch size (number of samples in the batch). This method can be interpreted as gradient descent using noisy gradients, which are typically called mini-batch gradients.\nSGD is based on the idea of stochastic approximation introduced by Robbins and Monro (1951). Stochastic approximation simply replaces the expected gradient with its Monte Carlo approximation.\nIn a small mini-batch regime, when \\(|B| \\ll n\\) and typically \\(|B| \\in \\{32,64,\\ldots,1024\\}\\), it was shown that SGD converges faster than standard gradient descent, does converge to minimizers of strongly convex functions (the negative log-likelihood from the exponential family is strongly convex) (Bottou, Curtis, and Nocedal 2018), and is more robust to noise in the data (Hardt, Recht, and Singer 2016). Further, SGD can avoid saddle points, which is often an issue with deep learning loss functions. In the case of multiple minima, SGD can find a good solution (Keskar et al. 2016; LeCun et al. 2002), meaning that out-of-sample performance is often worse when trained with large-batch methods compared to small-batch methods.\nNow, we implement SGD for logistic regression and compare performance for different batch sizes \n\nlrgd_minibatch  = function(x,y, alpha, n_iter, bs) {\n  theta &lt;- matrix(c(0, 0), nrow = 2, ncol = n_iter+1)\n  n = length(y)\n  for (i in 1:n_iter) {\n    s = ((i-1)*bs+1)%%n\n    e = min(s+bs-1,n)\n    xl = x[s:e,]; yl = y[s:e]\n    p = 1/(1+exp(-xl %*% theta[,i]))\n    grad &lt;- -t(xl) %*% (yl - p)\n    # update theta\n    theta[,i+1] &lt;- theta[,i] - alpha * grad\n  }\n  return(theta)\n}\n\nNow run our SGD algorithm with different batch sizes.\n\nset.seed(92) # kuzy\nind = sample(150)\ny = ifelse(iris$Species==\"setosa\",1,0)[ind] # shuffle data\nx = cbind(rep(1,150),iris$Sepal.Length)[ind,] # shuffle data\nnit=200000\nlr = 0.01\nth1 = lrgd_minibatch(x,y,lr,nit,5)\nth2 = lrgd_minibatch(x,y,lr,nit,15)\nth3 = lrgd_minibatch(x,y,lr,nit,30)\n\n\n\n\n\n\n\n\nWe run it with 200000 iterations and the learning rate of 0.01 and plot the values of \\(\\theta_1\\) every 1000 iteration. There are a couple of important points we need to highlight when using SGD. First, we shuffle the data before using it. The reason is that if the data is sorted in any way (e.g., by date or by value of one of the inputs), then data within batches can be highly correlated, which reduces the convergence speed. Shuffling helps avoid this issue, but it also implicitly treats the dataset as exchangeable; for time series and streaming data, shuffling is typically inappropriate (see Chapter 15). Second, the larger the batch size, the smaller the number of iterations required for convergence, which is something we would expect. However, in this specific example, from the computational point of view, the batch size does not change the number of calculations required overall. Let’s look at the same plot, but scale the x-axis according to the amount of computations\n\nplot(ind/1000,th1[1,ind], type='l', ylim=c(0,33), col=1, ylab=expression(theta[1]), xlab=\"Iteration\")\nabline(h=27.83, lty=2)\nlines(ind/1000*3,th2[1,ind], type='l', col=2)\nlines(ind/1000*6,th3[1,ind], type='l', col=3)\nlegend(\"bottomright\", legend=c(5,15,30),col=1:3, lty=1, bty='n',title = \"Batch Size\")\n\n\n\n\n\n\n\n\nThere are several important considerations about choosing the batch size for SGD.\n\nThe larger the batch size, the more memory is required to store the data.\nParallelization is more efficient with larger batch sizes. Modern hardware supports parallelization of matrix operations, which is the main operation in SGD. The larger the batch size, the more efficient the parallelization is. Usually there is a sweet spot \\(|B|\\) for the batch size, which is the largest batch size that can fit into memory or be parallelized. This means it takes the same amount of time to compute an SGD step for batch size \\(1\\) and \\(B\\).\nThird, the larger the batch size, the less noise in the gradient. This means that the larger the batch size, the more accurate the gradient is. However, it was empirically shown that in many applications we should prefer noisier gradients (small batches) to obtain high-quality solutions when the objective function (negative log-likelihood) is non-convex (Keskar et al. 2016).",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#automatic-differentiation-backpropagation",
    "href": "20-sgd.html#automatic-differentiation-backpropagation",
    "title": "20  Gradient Descent",
    "section": "20.3 Automatic Differentiation (Backpropagation)",
    "text": "20.3 Automatic Differentiation (Backpropagation)\nGradient descent requires computing gradients at each iteration. But how do we efficiently compute gradients for complex compositional functions like deep neural networks? The answer lies in automatic differentiation—a technique whose development is intertwined with the history of neural networks themselves.\nOriginal neural networks (Perceptrons) proposed by Rosenblatt (1958) were single-layer feedforward networks with a sigmoid activation function in the output layer. They were trained using the so-called delta rule.\nThe delta rule, introduced by Rosenblatt (1958), provides a simple learning algorithm for single-layer networks. For a single-layer perceptron with output \\(\\hat{y} = \\sigma(w^T x + b)\\), where \\(\\sigma\\) is an activation function, the delta rule updates weights as: \\[\nw^{t+1} = w^t - \\alpha (\\hat{y} - y) \\sigma'(w^T x + b) x\n\\] where \\(\\alpha\\) is the learning rate and \\(y\\) is the target output. This rule is intuitive: when the prediction \\(\\hat{y}\\) differs from the target \\(y\\), we adjust the weights proportionally to the input \\(x\\) and the derivative of the activation function.\nHowever, the delta rule encounters a fundamental limitation with multi-layer networks. Consider a network with hidden layers: \\(h = \\sigma_1(W_1 x + b_1)\\) and \\(\\hat{y} = \\sigma_2(W_2 h + b_2)\\). To update weights \\(W_1\\) in the first layer, we need to compute \\(\\frac{\\partial L}{\\partial W_1}\\), where \\(L\\) is the loss function. By the chain rule: \\[\n\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W_1}\n\\]\nThe delta rule could not efficiently compute this gradient through multiple layers, as it lacked a systematic method for propagating error signals backward through the network. This meant that while single-layer networks could be trained effectively, there was no practical way to train deeper architectures that could learn more complex representations.\nThe inability to train multi-layer networks led to what is now known as the first AI winter in the late 1960s and 1970s. Minsky and Papert (1969) demonstrated that single-layer perceptrons could not solve simple non-linearly separable problems like XOR, and without a method to train multi-layer networks, neural network research largely stagnated. Funding dried up, and many researchers abandoned neural networks in favor of symbolic AI approaches. This period lasted for over a decade, with neural networks considered a dead end by much of the AI community.\nUnknown to most of the neural network community at the time, the solution to training multi-layer networks through systematic application of the chain rule was being developed independently by several researchers. In the Soviet Union, Galushkin (1973) developed methods for training multi-layer networks using gradient-based approaches, though this work remained largely unknown in the West due to the Cold War.\nIn the Western literature, the first clear formulation of backpropagation came from P. Werbos (1974), who in his 1974 PhD thesis at Harvard University introduced the idea of using the chain rule to efficiently compute gradients in multi-layer networks. Werbos explicitly showed how to propagate derivatives backward through a network by decomposing complex functions into simpler components. He refined these ideas further in P. J. Werbos (1982), demonstrating how reverse-mode automatic differentiation could be applied to train neural networks efficiently.\nInterestingly, the same mathematical principle—systematic application of the chain rule for computing derivatives—was being applied in a different context around the same time. Nelder and Wedderburn (1972) introduced Generalized Linear Models (GLMs) and developed an iteratively reweighted least squares (IRLS) algorithm that explicitly used the chain rule to compute gradients for maximum likelihood estimation.\nTo understand their contribution, consider a GLM where the response variable \\(y_i\\) follows a distribution from the exponential family with mean \\(\\mu_i\\). The model relates the mean to the predictors through a link function \\(g(\\cdot)\\): \\[\ng(\\mu_i) = \\eta_i = x_i^T \\theta\n\\] where \\(x_i\\) is the vector of predictors for observation \\(i\\), \\(\\theta\\) is the parameter vector, and \\(\\eta_i\\) is the linear predictor. For example, in logistic regression (which we saw in Example 20.2), we have \\(g(\\mu) = \\log(\\mu/(1-\\mu))\\) (the logit link).\nThe key innovation of Nelder and Wedderburn was recognizing that computing the gradient of the log-likelihood with respect to \\(\\theta\\) requires systematic application of the chain rule. Specifically, for observation \\(i\\), the gradient decomposes as: \\[\n\\frac{\\partial \\log L}{\\partial \\theta_j} = \\frac{\\partial \\log L}{\\partial \\mu_i} \\cdot \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\cdot \\frac{\\partial \\eta_i}{\\partial \\theta_j}\n\\]\nThe first term, \\(\\partial \\log L/\\partial \\mu_i\\), depends on the specific distribution (e.g., Gaussian, Bernoulli, Poisson). For exponential family distributions, this takes the form \\((y_i - \\mu_i)/V(\\mu_i)\\), where \\(V(\\mu_i)\\) is the variance function. The second term, \\(\\partial \\mu_i/\\partial \\eta_i\\), is the derivative of the inverse link function. The third term is simply \\(x_{ij}\\), the \\(j\\)-th predictor value.\nCombining these using the chain rule, they derived the gradient: \\[\n\\frac{\\partial \\log L}{\\partial \\theta_j} = \\sum_{i=1}^n \\frac{(y_i - \\mu_i)}{V(\\mu_i)} \\cdot \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\cdot x_{ij}\n\\]\nFor the Newton-Raphson optimization method (also known as Fisher scoring when using expected second derivatives), they needed to compute the Hessian matrix. Applying the chain rule again to the gradient yields the Fisher information matrix with elements: \\[\n\\mathcal{I}_{jk} = \\sum_{i=1}^n w_i x_{ij} x_{ik}\n\\] where the weight \\(w_i = \\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2 / V(\\mu_i)\\) is itself computed by the chain rule. This leads to the iteratively reweighted least squares algorithm, where at each iteration \\(t\\) we solve: \\[\n\\theta^{t+1} = \\theta^t + \\mathcal{I}(\\theta^t)^{-1} \\nabla \\log L(\\theta^t)\n\\]\nThe beauty of this formulation is that it decomposes a complex gradient computation into three conceptually simple pieces—the error term, the link function derivative, and the design matrix—and composes them through the chain rule. This is precisely the same principle that underlies backpropagation in neural networks, though applied to a different class of models.\nWhat makes this parallel especially striking is the timing. Nelder and Wedderburn published their GLM framework in 1972, just two years before Werbos’s PhD thesis on backpropagation. Both independently discovered that systematic chain rule application could efficiently compute gradients in compositional models. However, the connection between these developments remained largely unrecognized for decades. The statistical community focused on GLMs for their interpretability and inference properties, while the neural network community emphasized the ability to learn complex representations through multiple layers.\nThe success of GLMs in statistics demonstrated that chain rule-based gradient computation was not only theoretically sound but practically valuable for fitting sophisticated models to real data. Yet this connection to neural networks was not widely recognized at the time. Despite these parallel discoveries, backpropagation did not enter mainstream neural network research until Rumelhart, Hinton, and Williams (1986) popularized the systematic use of the chain rule for neural networks and coined the term backpropagation that is now standard in machine learning.\nTo calculate the value of the gradient vector at each step of the optimization process, gradient descent algorithms require calculations of derivatives. In general, there are three different ways to calculate those derivatives. First, we can do it by hand. This is slow and error-prone. Second is numerical differentiation, when a gradient is approximated by a finite difference; for some small \\(h\\), calculate \\(f'(x) \\approx (f(x+h)-f(x))/h\\), which requires two function evaluations. This approach is not backward stable (Griewank, Kulshreshtha, and Walther 2012), meaning that for a small perturbation in input value \\(x\\), the calculated derivative is not the correct one. Lastly, we can use symbolic differentiation or automatic differentiation (AD) which has been used for decades in computer algebra systems such as Mathematica or Maple. Symbolic differentiation uses a tree form representation of a function and applies the chain rule to the tree to calculate the symbolic derivative of a given function.\nThe advantage of symbolic calculations is that the analytical representation of the derivative is available for further analysis, for example, when derivative calculation is an intermediate step of the analysis. A third way to calculate a derivative is to use automatic differentiation (AD). Similar to symbolic differentiation, AD recursively applies the chain rule and calculates the exact value of the derivative, thus avoiding the problem of numerical instability. The difference between AD and symbolic differentiation is that AD provides the value of the derivative evaluated at a specific point, rather than an analytical representation of the derivative.\nAD does not require analytical specification and can be applied to a function defined by a sequence of algebraic manipulations, logical and transcendent functions applied to input variables and specified in computer code. AD can differentiate complex functions which involve IF statements and loops, and AD can be implemented using either forward or backward mode. Consider an example of calculating a derivative of the logistic function \\[\nf(x) = \\frac{1}{1+e^{-(Wx+b)}}.\n\\]\nWhich is implemented as follows:\n\nsigmoid = function(x,b,w){\n  v1 = w*x;\n  v2 = v1 + b\n  v3 = 1/(1+exp(-v2))\n}\n\n\n\nGiven the function \\(f(x) = \\frac{1}{1+e^{-(Wx+b)}} = v_3\\), the derivative with respect to \\(w\\) is given by the chain rule. Recall that for the sigmoid function \\(\\sigma(z) = 1/(1+e^{-z})\\), we have \\(\\sigma'(z) = \\sigma(z)(1-\\sigma(z))\\). Therefore: \\[\n\\frac{\\partial f}{\\partial w} = \\frac{\\partial v_3}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial w} = v_3(1-v_3) \\cdot 1 \\cdot x\n\\]\n\\[\n\\frac{\\partial f}{\\partial b} = \\frac{\\partial v_3}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial b} = v_3(1-v_3) \\cdot 1\n\\]\nIn the forward mode, an auxiliary variable, called a dual number, will be added to each line of the code to track the value of the derivative associated with this line. In our example, if we set x=2, w=3, b=5, we get the calculations given in the table below.\n\n\n\n\n\n\n\nFunction calculations\nDerivative calculations\n\n\n\n\n1. v1 = w*x = 6\n1. dv1 = w = 3 (derivative of v1 with respect to x)\n\n\n2. v2 = v1 + b = 11\n2. dv2 = dv1 = 3 (derivative of v2 with respect to x)\n\n\n3. v3 = 1/(1+exp(-v2)) = 0.99\n3. dv3 = eps2*exp(-v2)/(1+exp(-v2))**2  = 5e-05\n\n\n\nVariables dv1,dv2,dv3 correspond to partial (local) derivatives of each intermediate variable v1,v2,v3 with respect to \\(x\\), and are called dual variables. Tracking for dual variables can either be implemented using source code modification tools that add new code for calculating the dual numbers or via operator overloading.\nThe reverse AD also applies the chain rule recursively but starts from the outer function, as shown in the table below.\n\n\n\n\n\n\n\nFunction calculations\nDerivative calculations\n\n\n\n\n1. v1 = w*x = 6\n4. dv1dx =w; dv1 = dv2*dv1dx = 3*1.3e-05=5e-05\n\n\n2. v2 = v1 + b = 11\n3. dv2dv1 =1; dv2 = dv3*dv2dv1 = 1.3e-05\n\n\n3. v3 = 1/(1+exp(-v2)) = 0.99\n2. dv3dv2 = exp(-v2)/(1+exp(-v2))**2;\n\n\n4. v4 = v3\n1. dv4=1\n\n\n\nThe key difference between forward and reverse mode is the direction of computation:\n\n\n\n\n\n\nFigure 20.2: Comparison of Forward and Reverse Mode Automatic Differentiation\n\n\n\n\nForward mode: Propagates derivatives with the computation, from inputs to outputs. Efficient when number of inputs &lt; number of outputs.\nReverse mode: Propagates derivatives against the computation, from outputs to inputs. Efficient when number of outputs &lt; number of inputs.\n\nFor neural networks with millions of parameters (inputs to the loss function) but a single scalar loss (one output), reverse mode is dramatically more efficient—this is why backpropagation uses reverse mode AD.\nFigure 20.3 shows a tree representation of the composition of affine and sigmoid functions (the first layer of our neural network). The numbers in the nodes are the values of the variables at a specific point, and the arrows show the flow of data. During the forward pass (solid arrows), the data flows from inputs (x, w, b) to the final output v3 and all intermediate variables are computed. During the backward pass (dashed arrows), the gradient values flow from the output back to the inputs by applying the chain rule to each variable.\n\n\n\n\n\n\ngraph LR\n    %% Input nodes\n    X[\"x = 2&lt;br/&gt;∇x = w × v3(1-v3)\"]\n    W[\"w = 3&lt;br/&gt;∇w = x × v3(1-v3)\"]\n    B[\"b = 5&lt;br/&gt;∇b = v3(1-v3)\"]\n    \n    %% Computation nodes\n    V1[\"v1 = w × x = 6&lt;br/&gt;∇v1 = v3(1-v3)\"]\n    V2[\"v2 = v1 + b = 11&lt;br/&gt;∇v2 = v3(1-v3)\"]\n    NEG[\"-v2 = -11\"]\n    EXP[\"exp(-v2) = 0.0000&lt;br/&gt;∇exp = -v3(1-v3)\"]\n    ONE[\"1\"]\n    ADD1[\"1 + exp(-v2) = 1.0000&lt;br/&gt;∇add = v3(1-v3)\"]\n    V3[\"v3 = 1/(1+exp(-v2)) = 0.99998&lt;br/&gt;∇v3 = 1\"]\n    \n    %% Forward pass connections (solid arrows)\n    X --&gt; V1\n    W --&gt; V1\n    V1 --&gt; V2\n    B --&gt; V2\n    V2 --&gt; NEG\n    NEG --&gt; EXP\n    ONE --&gt; ADD1\n    EXP --&gt; ADD1\n    ADD1 --&gt; V3\n    \n    %% Backward pass connections (dashed arrows - gradient flow)\n    V3 -.-&gt; ADD1\n    ADD1 -.-&gt; EXP\n    ADD1 -.-&gt; ONE\n    EXP -.-&gt; NEG\n    NEG -.-&gt; V2\n    V2 -.-&gt; V1\n    V2 -.-&gt; B\n    V1 -.-&gt; W\n    V1 -.-&gt; X\n    \n    %% Styling\n    classDef input fill:#e1f5fe,stroke:#0277bd,stroke-width:2px\n    classDef computation fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n    classDef constant fill:#e8f5e8,stroke:#388e3c,stroke-width:2px\n    classDef output fill:#fff3e0,stroke:#f57c00,stroke-width:2px\n    \n    class X,W,B input\n    class V1,V2,NEG,EXP,ADD1 computation\n    class ONE constant\n    class V3 output\n\n\n\n\nFigure 20.3: Computational Graph for Sigmoid Function\n\n\n\n\n\nFor DL, derivatives are calculated by applying the reverse AD algorithm to a model which is defined as a superposition of functions. A model is defined either using a general-purpose language as it is done in PyTorch or through a sequence of function calls defined by framework libraries (e.g., in TensorFlow). Forward AD algorithms calculate the derivative with respect to a single input variable, but reverse AD produces derivatives with respect to all intermediate variables. For models with many parameters, it is much more computationally feasible to perform the reverse AD.\nIn the context of neural networks, the reverse AD algorithm is called backpropagation and was popularized in AI by Rumelhart, Hinton, and Williams (1986). According to Schmidhuber (2015), the first version of what we call today backpropagation was published in 1970 in a master’s thesis Linnainmaa (1970) and was closely related to the work of Ostrovskii, Volin, and Borisov (1971). However, similar techniques rooted in Pontryagin’s maximum principle were discussed in the context of multi-stage control problems (Arthur E. Bryson 1961; Arthur E. Bryson and Ho 1969). Dreyfus (1962) applies backpropagation to calculate the first-order derivative of a return function to numerically solve a variational problem. Later Dreyfus (1973) used backpropagation to derive an efficient algorithm to solve a minimization problem. The first neural network-specific version of backpropagation was proposed in P. Werbos (1974) and an efficient backpropagation algorithm was discussed in P. J. Werbos (1982).\nModern deep learning frameworks fully automate the process of finding derivatives using AD algorithms. For example, PyTorch relies on the autograd library which automatically finds gradients using the back-propagation algorithm. Here is a small code example using the autograd library in jax.\nThe code below implements a simple neural network with one hidden layer and uses back-propagation to find the gradient of the negative log-likelihood function. The model is trained on a synthetic dataset generated from a circle. The code uses the jax library, which is a high-performance numerical computing library that supports automatic differentiation.\n\n\nCode\nimport jax.numpy as jnp\nimport pandas as pd\nfrom jax import random\nimport matplotlib.pyplot as plt\n\ndef abline(slope, intercept):\n    \"\"\"Plot a line from slope and intercept\"\"\"\n    axes = plt.gca()\n    x_vals = jnp.array(axes.get_xlim())\n    ylim = axes.get_xlim()\n    y_vals = intercept + slope * x_vals\n    plt.plot(x_vals, y_vals, '-'); plt.ylim(ylim)\n\nd = pd.read_csv('../data/circle.csv').values\nx = d[:, 1:3]; y = d[:, 0]\nk = random.PRNGKey(0)\nw1 = 0.1*random.normal(k,(2,4))\nb1 = 0.01*random.normal(k,(4,))\nw2 = 0.1*random.normal(k,(4,1))\nb2 = 0.01*random.normal(k,(1,))\n\n\n\nfrom jax import grad,jit\ndef sigmoid(x):\n    return 1 / (1 + jnp.exp(-x))\ndef predict(x, w1,b1,w2,b2):\n    z = sigmoid(jnp.dot(x, w1)+b1)\n    return sigmoid(jnp.dot(z, w2)+b2)[:,0]\ndef nll(x, y, w1,b1,w2,b2):\n    yhat = predict(x, w1,b1,w2,b2)\n    return -jnp.sum(y * jnp.log(yhat) + (1 - y) * jnp.log(1 - yhat))\n@jit\ndef gd_step(x, y, w1,b1,w2,b2, lr):\n    grads = grad(nll,argnums=[2,3,4,5])(x, y, w1,b1,w2,b2)\n    return w1 - lr * grads[0],b1 - lr * grads[1],w2 - lr * grads[2],b2 - lr * grads[3]\ndef accuracy(x, y, w1,b1,w2,b2):\n    y_pred = predict(x, w1,b1,w2,b2)\n    return jnp.mean((y_pred &gt; 0.5) == y)\n\n\nfor i in range(1500):\n    w1,b1,w2,b2 = gd_step(x,y,w1,b1,w2,b2,0.02)\nprint(accuracy(x,y,w1,b1,w2,b2))\n## 1.0\n\nNote that inside the gradient descent function gd_step we use the grad function to calculate the gradient of the negative log-likelihood function with respect to the weights and biases. The jit decorator is used to compile the function for faster execution. The gd_step function updates the weights and biases using the calculated gradients and a learning rate of 0.02.\nFinally, we can plot the decision boundary of the trained model\n\n\nCode\nfig, ax = plt.subplots();\nax.scatter(x[:,0], x[:,1], c=['r' if x==1 else 'g' for x in y],s=7); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.xlim(-10,10);\nax.spines['top'].set_visible(False);\n# plt.scatter((x[:,1]*w1[1,0] - b1[0])/w1[0,0], x[:,1])\nabline(w1[1,0]/w1[0,0],b1[0]/w1[0,0]);\nabline(w1[1,1]/w1[0,1],b1[1]/w1[0,1]);\nabline(w1[1,2]/w1[0,2],b1[2]/w1[0,2]);\nabline(w1[1,3]/w1[0,3],b1[3]/w1[0,3]);\nplt.show();\n\n\n\n\n\n\n\n\n\nWe can see that the model has learned to separate the two classes. Two of the lines are almost parallel, which is expected. We could have gotten away with using only three hidden units instead of four.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#advanced-optimization-methods",
    "href": "20-sgd.html#advanced-optimization-methods",
    "title": "20  Gradient Descent",
    "section": "20.4 Advanced Optimization Methods",
    "text": "20.4 Advanced Optimization Methods\nWhile the basic SGD algorithm introduced earlier works well for many problems, practitioners have developed numerous enhancements to improve convergence speed and stability. This section surveys the most important variants used in modern deep learning.\n\nMomentum Methods\nOne limitation of vanilla SGD is that the descent in the loss function is not guaranteed at every iteration, and progress can be slow when gradients are noisy. To address this, momentum-based methods add memory to the search process by combining new gradient information with previous search directions (Nesterov 1983). Empirically, momentum-based methods have been shown to have better convergence for deep learning networks (Sutskever et al. 2013). The gradient only influences changes in the velocity of the update, which then updates the variable:\n\\[\nv^{k+1} = \\mu v^k - t_k g((W,b)^k)\n\\] \\[\n(W,b)^{k+1} = (W,b)^k +v^k\n\\]\nThe hyperparameter \\(\\mu\\) controls the damping effect on the rate of update of the variables. The physical analogy is the reduction in kinetic energy that allows to “slow down” the movements at the minima. This parameter can also be chosen empirically using cross-validation.\nNesterov’s momentum method (a.k.a. Nesterov acceleration) calculates the gradient at the point predicted by the momentum. One can view this as a look-ahead strategy with the updating scheme\n\\[\nv^{k+1} = \\mu v^k - t_k g((W,b)^k +v^k)\n\\] \\[\n(W,b)^{k+1} = (W,b)^k +v^k\n\\]\n\n\nAdaptive Learning Rate Methods\nAnother popular modification is AdaGrad (Duchi, Hazan, and Singer 2011), which adaptively scales the learning rate for each parameter based on the history of gradients:\n\\[\nc^{k+1} = c^k + g(\\theta^k)^2\n\\] \\[\n\\theta^{k+1} = \\theta^k - \\frac{t_k}{\\sqrt{c^{k+1}} + \\epsilon} g(\\theta^k)\n\\]\nwhere \\(\\epsilon\\) is a small constant (e.g., \\(\\epsilon = 10^{-8}\\)) to prevent division by zero. RMSprop (Hinton, Srivastava, and Swersky 2012) extends AdaGrad by using an exponentially weighted moving average of squared gradients:\n\\[\nc^{k+1} = \\rho c^k + (1-\\rho) g(\\theta^k)^2\n\\] \\[\n\\theta^{k+1} = \\theta^k - \\frac{t_k}{\\sqrt{c^{k+1}} + \\epsilon} g(\\theta^k)\n\\]\nThe Adam method (Kingma and Ba 2014) combines RMSprop with momentum by maintaining exponentially weighted averages of both the gradient (first moment) and the squared gradient (second moment):\n\\[\nm^{k+1} = \\beta_1 m^k + (1-\\beta_1) g(\\theta^k)\n\\] \\[\nc^{k+1} = \\beta_2 c^k + (1-\\beta_2) g(\\theta^k)^2\n\\] \\[\n\\theta^{k+1} = \\theta^k - \\frac{t_k}{\\sqrt{\\hat{c}^{k+1}} + \\epsilon} \\hat{m}^{k+1}\n\\]\nwhere \\(\\hat{m}^{k+1}\\) and \\(\\hat{c}^{k+1}\\) are bias-corrected estimates. The typical hyperparameter values are \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), and \\(\\epsilon = 10^{-8}\\).\nThe following table summarizes the key optimization methods:\n\n\n\n\n\n\n\n\nMethod\nKey Idea\nUpdate Rule\n\n\n\n\nSGD\nNoisy gradient estimates\n\\(\\theta^{k+1} = \\theta^k - t_k g^k\\)\n\n\nMomentum\nAccumulate gradient direction\n\\(v^{k+1} = \\mu v^k + g^k\\); \\(\\theta^{k+1} = \\theta^k - t_k v^{k+1}\\)\n\n\nNesterov\nLook-ahead gradient\n\\(v^{k+1} = \\mu v^k + g(\\theta^k + \\mu v^k)\\)\n\n\nAdaGrad\nPer-parameter learning rates\nScale by \\(1/\\sqrt{\\sum g^2}\\)\n\n\nRMSprop\nExponential moving average\nScale by \\(1/\\sqrt{\\text{EMA}(g^2)}\\)\n\n\nAdam\nMomentum + adaptive rates\nCombines first and second moment estimates\n\n\n\n\n\nSecond-Order Methods\nSecond-order methods solve the optimization problem by solving a system of nonlinear equations \\(\\nabla f(\\theta) = 0\\) using Newton’s method:\n\\[\n\\theta^{+} = \\theta - \\{ \\nabla^2 f(\\theta) \\}^{-1} \\nabla f(\\theta)\n\\]\nWe can see that SGD effectively approximates the inverse Hessian \\(\\{\\nabla^2 f(\\theta)\\}^{-1}\\) by \\(t_k I\\), where \\(I\\) is the identity matrix. The advantages of second-order methods include much faster convergence rates and insensitivity to the conditioning of the problem. However, in practice, second-order methods are rarely used for deep learning applications (Dean et al. 2012). The major disadvantage is the inability to train models using mini-batches as SGD does. Since typical deep learning models rely on large-scale datasets, computing and storing the Hessian becomes memory and computationally prohibitive.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#why-robbins-monro",
    "href": "20-sgd.html#why-robbins-monro",
    "title": "20  Gradient Descent",
    "section": "20.5 Why Robbins-Monro?",
    "text": "20.5 Why Robbins-Monro?\nThe Robbins-Monro algorithm was introduced in their seminal 1951 paper “A Stochastic Approximation Method” Robbins and Monro (1951). The paper addressed the problem of finding the root of a function when only noisy observations are available.\nConsider a function \\(M(\\theta)\\) where we want to find \\(\\theta^*\\) such that \\(M(\\theta^*) = \\alpha\\) for some target value \\(\\alpha\\). In the original formulation, \\(M(\\theta)\\) represents the expected value of some random variable \\(Y(\\theta)\\):\n\\[M(\\theta) = \\mathbb{E}[Y(\\theta)] = \\alpha\\]\nThe key insight is that we can only observe noisy realizations \\(y(\\theta)\\) where:\n\\[y(\\theta) = M(\\theta) + \\epsilon(\\theta)\\]\nwhere \\(\\epsilon(\\theta)\\) is a zero-mean random error term.\nThe Robbins-Monro algorithm iteratively updates the estimate \\(\\theta_n\\) using:\n\\[\\theta_{n+1} = \\theta_n - a_n(y(\\theta_n) - \\alpha)\\]\nwhere \\(a_n\\) is a sequence of positive step sizes that must satisfy:\n\\[\\sum_{n=1}^{\\infty} a_n = \\infty \\quad \\text{and} \\quad \\sum_{n=1}^{\\infty} a_n^2 &lt; \\infty\\]\nThese conditions ensure that the algorithm can explore the entire space (first condition) while eventually converging (second condition).\nUnder appropriate conditions on \\(M(\\theta)\\) (monotonicity and boundedness), the algorithm converges almost surely to \\(\\theta^*\\):\n\\[\\lim_{n \\to \\infty} \\theta_n = \\theta^* \\quad \\text{almost surely}\\]\nThe convergence rate depends on the choice of step sizes. For \\(a_n = c/n\\) with \\(c &gt; 0\\), the algorithm achieves optimal convergence rates.\nThis foundational work established the theoretical basis for stochastic approximation methods that are now widely used in machine learning, particularly in stochastic gradient descent and related optimization algorithms.\n\nConnection to M-Estimation\nThe Robbins-Monro framework connects naturally to statistical estimation. Many estimands—including means, medians, quantiles, and regression coefficients—can be expressed as solutions to convex optimization problems:\n\\[\\theta^* = \\arg\\min_{\\theta \\in \\mathbb{R}^p} \\mathbb{E}[\\ell_\\theta(X_i, Y_i)],\\]\nwhere \\(\\ell_\\theta\\) is a convex loss function. Under mild conditions, the optimum \\(\\theta^*\\) satisfies the first-order condition: \\[\n\\mathbb{E}[g_\\theta(X_i, Y_i)] = 0,\n\\tag{20.1}\\]\nwhere \\(g_\\theta\\) is a subgradient of the loss. SGD can be viewed as applying the Robbins-Monro algorithm to find roots of this gradient condition using noisy samples.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#the-em-ecm-and-ecme-algorithms",
    "href": "20-sgd.html#the-em-ecm-and-ecme-algorithms",
    "title": "20  Gradient Descent",
    "section": "20.6 The EM, ECM, and ECME Algorithms",
    "text": "20.6 The EM, ECM, and ECME Algorithms\nGradient descent is not the only iterative optimization algorithm used in statistics. When dealing with latent variables or missing data, directly computing gradients of the log-likelihood can be intractable. The Expectation-Maximization (EM) algorithm provides an elegant alternative that iteratively maximizes a surrogate function, avoiding explicit gradient computation while guaranteeing monotonic improvement in the likelihood.\nFirst, define a function \\(Q(\\theta,\\phi)\\) such that \\(Q(\\theta) = Q(\\theta,\\theta)\\) and it satisfies a convexity constraint \\(Q(\\theta,\\phi) \\geq Q(\\theta,\\theta)\\). Then define\n\\[\\theta^{(g+1)} = \\arg\\max_{\\theta \\in \\Theta} Q(\\theta,\\theta^{(g)})\\]\nThis satisfies the convexity constraint \\(Q(\\theta,\\theta) \\geq Q(\\theta,\\phi)\\) for any \\(\\phi\\). In order to prove convergence, you get a sequence of inequalities\n\\[Q(\\theta^{(0)},\\theta^{(0)}) \\leq Q(\\theta^{(1)},\\theta^{(0)}) \\leq Q(\\theta^{(1)},\\theta^{(1)}) \\leq \\ldots \\leq Q\\]\nIn many models we have to deal with a latent variable and require estimation where integration is also involved. For example, suppose that we have a triple \\((y,z,\\theta)\\) with joint probability specification \\(p(y,z,\\theta) = p(y|z,\\theta)p(z,\\theta)\\). This can occur in missing data problems and estimation problems in mixture models.\nA standard application of the EM algorithm is to find\n\\[\\arg\\max_{\\theta \\in \\Theta} \\int_z p(y|z,\\theta)p(z|\\theta)dz\\]\nAs we are just finding an optimum, you do not need the prior specification \\(p(\\theta)\\). The EM algorithm finds a sequence of parameter values \\(\\theta^{(g)}\\) by alternating between an expectation and a maximisation step. This still requires the numerical (or analytical) computation of the criteria function \\(Q(\\theta,\\theta^{(g)})\\) described below.\nEM algorithms have been used extensively in mixture models and missing data problems. The EM algorithm uses the particular choice where\n\\[Q(\\theta) = \\log p(y|\\theta) = \\log \\int p(y,z|\\theta)dz\\]\nHere the likelihood has a mixture representation where \\(z\\) is the latent variable (missing data, state variable etc.). This is termed a Q-maximization algorithm with:\n\\[Q(\\theta,\\theta^{(g)}) = \\int \\log p(y|z,\\theta)p(z|\\theta^{(g)},y)dz = \\mathbb{E}_{z|\\theta^{(g)},y} [\\log p(y|z,\\theta)]\\]\nTo implement EM you need to be able to calculate \\(Q(\\theta,\\theta^{(g)})\\) and optimize at each iteration.\nThe EM algorithm and its extensions ECM and ECME are methods of computing maximum likelihood estimates or posterior modes in the presence of missing data. Let the objective function be \\(\\ell(\\theta) = \\log p(\\theta|y) + c(y)\\), where \\(c(y)\\) is a possibly unknown normalizing constant that does not depend on \\(\\beta\\) and \\(y\\) denotes observed data. We have a mixture representation,\n\\[p(\\theta|y) = \\int p(\\theta,z|y)dz = \\int p(\\theta|z,y)p(z|y)dz\\]\nwhere distribution of the latent variables is \\(p(z|\\theta,y) = p(y|\\theta,z)p(z|\\theta)/p(y|\\theta)\\).\nIn some cases the complete data log-posterior is simple enough for \\(\\arg\\max_{\\theta} \\log p(\\theta|z,y)\\) to be computed in closed form. The EM algorithm alternates between the Expectation and Maximization steps for which it is named. The E-step and M-step computes\n\\[Q(\\beta|\\beta^{(g)}) = \\mathbb{E}_{z|\\beta^{(g)},y} [\\log p(y,z|\\beta)] = \\int \\log p(y,z|\\beta)p(z|\\beta^{(g)},y)dz\\]\n\\[\\beta^{(g+1)} = \\arg\\max_{\\beta} Q(\\beta|\\beta^{(g)})\\]\nThis has an important monotonicity property that ensures \\(\\ell(\\beta^{(g)}) \\leq \\ell(\\beta^{(g+1)})\\) for all \\(g\\). In fact, the monotonicity proof given by Dempster et al. (1977) shows that any \\(\\beta\\) with \\(Q(\\beta,\\beta^{(g)}) \\geq Q(\\beta^{(g)},\\beta^{(g)})\\) also satisfies the log-likelihood inequality \\(\\ell(\\beta) \\geq \\ell(\\beta^{(g)})\\).\nIn problems with many parameters the M-step of EM may be difficult. In this case \\(\\theta\\) may be partitioned into components \\((\\theta_1,\\ldots,\\theta_k)\\) in such a way that maximizing \\(\\log p(\\theta_j|\\theta_{-j},z,y)\\) is easy. The ECM algorithm pairs the EM algorithm’s E-step with \\(k\\) conditional maximization (CM) steps, each maximizing \\(Q\\) over one component \\(\\theta_j\\) with each component of \\(\\theta_{-j}\\) fixed at the most recent value. Due to the fact that each CM step increases \\(Q\\), the ECM algorithm retains the monotonicity property. The ECME algorithm replaces some of ECM’s CM steps with maximizations over \\(\\ell\\) instead of \\(Q\\). Liu and Rubin (1994) show that doing so can greatly increase the rate of convergence.\nSeen through the optimization lens, ECM is a form of block coordinate ascent on a surrogate objective: it improves parameters one block at a time while holding the rest fixed. This contrasts with SGD, which takes small stochastic steps along noisy gradients of a single objective; both are iterative methods, but they trade off different sources of computational convenience (closed-form block updates versus cheap gradient estimates).\nIn many cases we will have a parameter vector \\(\\theta = (\\beta,\\nu)\\) partitioned into its components and a missing data vector \\(z = (\\lambda,\\omega)\\). Then we compute the \\(Q(\\beta,\\nu|\\beta^{(g)},\\nu^{(g)})\\) objective function and then compute E- and M steps from this to provide an iterative algorithm for updating parameters. To update the hyperparameter \\(\\nu\\) we can maximize the fully data posterior \\(p(\\beta,\\nu|y)\\) with \\(\\beta\\) fixed at \\(\\beta^{(g+1)}\\). The algorithm can be summarized as follows:\n\\[\\beta^{(g+1)} = \\arg\\max_{\\beta} Q(\\beta|\\beta^{(g)},\\nu^{(g)}) \\quad \\text{where} \\quad Q(\\beta|\\beta^{(g)},\\nu^{(g)}) = \\mathbb{E}_{z|\\beta^{(g)},\\nu^{(g)},y} \\log p(y,z|\\beta,\\nu^{(g)})\\]\n\\[\\nu^{(g+1)} = \\arg\\max_{\\nu} \\log p(\\beta^{(g+1)},\\nu|y)\\]\n\nECM and ECME Extensions\nIn problems with many parameters, the M-step of EM may be difficult. In this case \\(\\theta\\) may be partitioned into components \\((\\theta_1,\\ldots,\\theta_k)\\) such that maximizing \\(\\log p(\\theta_j|\\theta_{-j},z,y)\\) is manageable.\nThe ECM (Expectation/Conditional Maximization) algorithm pairs the EM algorithm’s E-step with \\(k\\) conditional maximization (CM) steps, each maximizing \\(Q\\) over one component \\(\\theta_j\\) with each component of \\(\\theta_{-j}\\) fixed at the most recent value. Due to the fact that each CM step increases \\(Q\\), the ECM algorithm retains the monotonicity property.\nThe ECME (Expectation/Conditional Maximization Either) algorithm replaces some of ECM’s CM steps with maximizations over \\(\\ell\\) instead of \\(Q\\). Liu and Rubin (1994) show that doing so can greatly increase the rate of convergence.\nFor a parameter vector \\(\\theta = (\\beta,\\nu)\\) partitioned into components and missing data vector \\(z = (\\lambda,\\omega)\\), the ECME algorithm proceeds as follows:\n\\[\n\\begin{aligned}\n\\beta^{(g+1)} &= \\arg\\max_{\\beta} Q(\\beta|\\beta^{(g)},\\nu^{(g)}) \\\\\n&\\text{where } Q(\\beta|\\beta^{(g)},\\nu^{(g)}) = \\mathbb{E}_{z|\\beta^{(g)},\\nu^{(g)},y}[\\log p(y,z|\\beta,\\nu^{(g)})] \\\\\n\\nu^{(g+1)} &= \\arg\\max_{\\nu} \\log p(\\beta^{(g+1)},\\nu|y)\n\\end{aligned}\n\\]\nThe key insight is that for updating hyperparameter \\(\\nu\\), we maximize the fully observed data posterior \\(p(\\beta,\\nu|y)\\) with \\(\\beta\\) fixed at \\(\\beta^{(g+1)}\\), rather than the Q-function. This often leads to simpler optimization problems and faster convergence.\n\nExample 20.3 Simulated Annealing (SA) is a simulation-based approach to finding\n\\[\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} H(\\theta)\\]\n\\[\\pi_J(\\theta) = \\frac{e^{-JH(\\theta)}}{\\int e^{JH(\\theta)}d\\mu(\\theta)}\\]\nwhere \\(J\\) is a temperature parameter. Instead of looking at derivatives and performing gradient-based optimization you can simulate from the sequence of densities. This forms a time-homogeneous Markov chain and under suitable regularity conditions on the relaxation schedule for the temperature we have \\(\\theta^{(g)} \\to \\hat{\\theta}\\). The main caveat is that we need to know the criterion function \\(H(\\theta)\\) to evaluate the Metropolis probability for sampling from the sequence of densities. This is not always available.\nAn interesting generalisation which is appropriate in latent variable mixture models is the following. Suppose that \\(H(\\theta) = \\mathbb{E}_{z|\\theta} \\{H(z,\\theta)\\}\\) is unavailable in closed-form where without loss of generality we assume that \\(H(z,\\theta) \\geq 0\\). In this case we can use latent variable simulated annealing (LVSA) methods. Define a joint probability distribution for \\(z_J = (z_1,\\ldots,z_J)\\) as\n\\[\\pi_J(z_J,\\theta) \\propto \\prod_{j=1}^J H(z_j,\\theta)p(z_j|\\theta)\\mu(\\theta)\\]\nfor some measure \\(\\mu\\) which ensures integrability of the joint. This distribution has the property that its marginal distribution on \\(\\theta\\) is given by\n\\[\\pi_J(\\theta) \\propto \\mathbb{E}_{z|\\theta} \\{H(z,\\theta)\\}^J \\mu(\\theta) = e^{J \\ln H(\\theta)}\\mu(\\theta)\\]\nBy the simulated annealing argument we see that this marginal collapses on the maximum of \\(\\ln H(\\theta)\\). The advantage of this approach is that it is typically straightforward to sample with MCMC from the conditionals:\n\\[\\pi_J(z_i|\\theta) \\sim H(z_j,\\theta)p(z_j|\\theta) \\quad \\text{and} \\quad \\pi_J(\\theta|z) \\sim \\prod_{j=1}^J H(z_j,\\theta)p(z_j|\\theta)\\mu(\\theta)\\]\nJacquier, Johannes and Polson (2007) apply this framework to finding MLE estimates for commonly encountered latent variable models. The LVSA approach is particularly useful when the criterion function involves complex integrals that make direct optimization challenging, but where sampling from the conditional distributions remains tractable.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "20-sgd.html#conclusion",
    "href": "20-sgd.html#conclusion",
    "title": "20  Gradient Descent",
    "section": "20.7 Conclusion",
    "text": "20.7 Conclusion\nThis chapter has covered the computational foundations of training deep learning models. Several key insights emerge:\nGradient Descent as the Foundation: While classical statistical methods rely on second-order optimization (Newton-Raphson, BFGS), deep learning’s scale demands first-order methods. Gradient descent—despite its simplicity—remains effective because it requires only gradient information, which scales linearly with the number of parameters.\nStochastic Approximation: The Robbins-Monro framework provides theoretical justification for SGD: noisy gradient estimates, when properly scheduled, converge to the true optimum. Mini-batch SGD exploits this insight, trading exact gradients for computational efficiency and—counterintuitively—often better generalization through implicit regularization.\nAutomatic Differentiation: The backpropagation algorithm is simply reverse-mode automatic differentiation applied to neural networks. Understanding this connection reveals why modern frameworks (PyTorch, JAX) can automatically compute gradients for arbitrary computational graphs. The key insight is that reverse mode is efficient when outputs are few and parameters are many—precisely the situation in supervised learning.\nAdvanced Optimizers: Building on vanilla SGD, momentum methods accelerate convergence by accumulating gradient information, while adaptive methods (AdaGrad, RMSprop, Adam) automatically tune per-parameter learning rates. These innovations have proven essential for training deep networks efficiently, though the optimal choice remains problem-dependent.\nBeyond Gradients: The EM algorithm provides an alternative optimization paradigm for latent variable models, avoiding explicit gradient computation through clever algebraic manipulation. Simulated annealing offers yet another approach, using stochastic sampling to escape local minima.\nThe practical success of SGD and its variants remains somewhat surprising from a theoretical perspective. Why does an algorithm designed for convex optimization work so well on highly non-convex loss landscapes? Ongoing research suggests that the geometry of over-parameterized networks may be more benign than previously thought—saddle points abound, but local minima tend to generalize well. Understanding these phenomena remains an active area of research at the intersection of optimization theory and deep learning.\n\n\n\n\nBottou, Léon, Frank E Curtis, and Jorge Nocedal. 2018. “Optimization Methods for Large-Scale Machine Learning.” SIAM Review 60 (2): 223–311.\n\n\nBryson, Arthur E. 1961. “A Gradient Method for Optimizing Multi-Stage Allocation Processes.” In Proc. Harvard Univ. Symposium on Digital Computers and Their Applications. Vol. 72.\n\n\nBryson, Arthur E., and Yu-Chi Ho. 1969. Applied Optimal Control: Optimization, Estimation, and Control. Waltham, MA: Blaisdell Publishing Company.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep Networks.” In Advances in Neural Information Processing Systems, 1223–31.\n\n\nDreyfus, Stuart. 1962. “The Numerical Solution of Variational Problems.” Journal of Mathematical Analysis and Applications 5 (1): 30–45.\n\n\n———. 1973. “The Computational Solution of Optimal Control Problems with Time Lag.” IEEE Transactions on Automatic Control 18 (4): 383–85.\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (61): 2121–59.\n\n\nGalushkin, A. I. 1973. “Synthesis of Multilayer Systems of Pattern Recognition.” Neurocomputers and Their Application.\n\n\nGriewank, Andreas, Kshitij Kulshreshtha, and Andrea Walther. 2012. “On the Numerical Stability of Algorithmic Differentiation.” Computing. Archives for Scientific Computing 94 (2-4): 125–49.\n\n\nHardt, Moritz, Ben Recht, and Yoram Singer. 2016. “Train Faster, Generalize Better: Stability of Stochastic Gradient Descent.” In International Conference on Machine Learning, 1225–34. PMLR.\n\n\nHinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. 2012. “Neural Networks for Machine Learning, Lecture 6a: Overview of Mini-Batch Gradient Descent.” Coursera Lecture.\n\n\nKeskar, Nitish Shirish, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2016. “On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.” arXiv Preprint arXiv:1609.04836. https://arxiv.org/abs/1609.04836.\n\n\nKingma, Diederik, and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv Preprint arXiv:1412.6980. https://arxiv.org/abs/1412.6980.\n\n\nLeCun, Yann, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. 2002. “Efficient Backprop.” In Neural Networks: Tricks of the Trade, 9–50. Springer.\n\n\nLinnainmaa, Seppo. 1970. “The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors.” Master’s Thesis (in Finnish), Univ. Helsinki, 6–7.\n\n\nMinsky, Marvin, and Seymour Papert. 1969. Perceptrons: An Introduction to Computational Geometry. Cambridge, MA: MIT Press.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Royal Statistical Society. Journal. Series A: General 135 (3): 370–84.\n\n\nNesterov, Yurii. 1983. “A Method of Solving a Convex Programming Problem with Convergence Rate O (1/K2).” In Soviet Mathematics Doklady, 27:372–76.\n\n\nOstrovskii, GM, Yu M Volin, and WW Borisov. 1971. “Uber Die Berechnung von Ableitungen.” Wissenschaftliche Zeitschrift Der Technischen Hochschule f Ur Chemie, Leuna-Merseburg 13 (4): 382–84.\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic Approximation Method.” The Annals of Mathematical Statistics 22 (3): 400–407.\n\n\nRosenblatt, F. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” Psychological Review 65 (6): 386–408.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533.\n\n\nSchmidhuber, Jürgen. 2015. “Deep Learning in Neural Networks: An Overview.” Neural Networks 61: 85–117.\n\n\nSutskever, Ilya, James Martens, George Dahl, and Geoffrey Hinton. 2013. “On the Importance of Initialization and Momentum in Deep Learning.” In International Conference on Machine Learning, 1139–47.\n\n\nWerbos, Paul. 1974. “Beyond Regression:\" New Tools for Prediction and Analysis in the Behavioral Sciences.” Ph. D. Dissertation, Harvard University.\n\n\nWerbos, Paul J. 1982. “Applications of Advances in Nonlinear Sensitivity Analysis.” In System Modeling and Optimization, 762–70. Springer.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Gradient Descent</span>"
    ]
  },
  {
    "objectID": "21-qnn.html",
    "href": "21-qnn.html",
    "title": "21  Quantile Neural Networks",
    "section": "",
    "text": "21.1 Quantile Regression\nIn Chapter 3, we explored how to learn posterior distributions \\(p(\\theta\\mid y)\\) using Bayesian learning and to use it for predictions, hypothesis testing, and other tasks. In Chapter 4, we explored how rational agents make decisions under uncertainty by maximizing expected utility. Traditional Bayesian approaches to such problems require computing posterior distributions \\(p(\\theta\\mid y)\\), which in turn requires specifying likelihood functions \\(p(y\\mid \\theta)\\) and often involves challenging density estimation. But what if we could bypass density estimation entirely and directly learn the quantities we need for decision-making and other tasks?\nThis chapter introduces quantile neural networks, a powerful approach that learns posterior distributions through their quantile functions rather than their densities. This shift from densities to quantiles has profound implications: it enables likelihood-free inference, provides natural connections to decision theory through the quantile-expectation identity, and scales to high-dimensional problems where density estimation becomes intractable.\nThe key insight is straightforward. Any expectation—including the expected utility central to decision theory—can be represented as an integral over quantiles: \\[\nE[f(\\theta)] = \\int_0^1 F^{-1}_{f(\\theta) | y}(\\tau) d\\tau\n\\] Rather than learning densities and then computing expectations via sampling, we can directly learn the quantile function \\(F^{-1}_{f(\\theta) | y}(\\tau)\\) using neural networks. This approach is not only more efficient but also naturally handles simulation-based models where likelihoods are unavailable or computationally expensive.\nThroughout this chapter, we develop quantile neural networks from theoretical foundations through practical applications. We begin by establishing the fundamentals of quantile regression (Section 21.1), deriving the check loss function and demonstrating its properties. We then show how to extend this framework to the generative approach (Section 21.2), using the noise outsourcing theorem to represent posterior distributions through quantile functions rather than densities. Before moving to neural networks, we examine trend filtering (Section 21.3) as an elegant middle ground that provides nonlinear function approximation while maintaining computational tractability. With these foundations in place, we establish Bayes rule for quantiles (Section 21.4), showing how Bayesian updating can be performed entirely in terms of quantile functions. We then turn to three major applications demonstrating the versatility of this framework: maximum expected utility problems (Section 21.5), where utility functions are incorporated directly into training; portfolio optimization under parameter uncertainty (Section 21.7), extending beyond cases with closed-form solutions; and supply chain forecasting (Section 21.8), where companies like Amazon use quantile methods to predict entire demand distributions for inventory optimization. After detailing the neural network implementation (Section 21.6), including cosine embeddings and Wasserstein distance connections, we connect to modern artificial intelligence through distributional reinforcement learning (Section 21.9), showing how agents learn entire distributions of returns for more robust decision-making.\nWe begin by developing the theoretical foundations of quantile regression, deriving the loss functions from first principles, and then show how neural networks provide a flexible architecture for learning complex quantile functions in high dimensions.\nBefore diving into neural network implementations, we present the foundational concepts of quantile regression. This section derives the quantile loss function from first principles and discusses applications across multiple fields, setting the stage.\nStandard calculus shows that for observed values \\(y_1, \\ldots, y_n\\), the median is the value that minimizes the expected absolute deviation: \\[\nm = \\arg\\min_q \\frac{1}{n} \\sum_{i=1}^n |y_i - q|   = \\arg\\min_q E[|Y - q|]\n\\] Intuitively, the sum of absolute deviations is minimized when the median is the value that is closest to half of the observations.\nThe median is the special case \\(\\tau = 0.5\\), but the same principle generalizes to any quantile \\(\\tau \\in (0,1)\\). We use a generalization of the absolute value function—the check loss or pinball loss—to find the minimizer: \\[\nq_\\tau = \\arg\\min_q \\frac{1}{n} \\sum_{i=1}^n \\rho_\\tau(y_i - q).\n\\] Here \\(\\rho_\\tau(u)\\) is the check loss or pinball loss and is defined as: \\[\n\\rho_\\tau(u) = u(\\tau - I(u &lt; 0)) = \\begin{cases}\n\\tau u & \\text{if } u \\geq 0 \\\\\n(\\tau - 1) u & \\text{if } u &lt; 0\n\\end{cases}\n\\] This can also be written in the more computationally convenient form: \\[\n\\rho_\\tau(u) = \\max(u\\tau, u(\\tau-1))\n\\]\nTraditional quantile regression assumes \\(f_\\tau(x, \\theta)\\) is linear in parameters. This limitation becomes severe in several important scenarios. First, many real-world relationships are inherently nonlinear—demand forecasting, for instance, involves complex interactions between time, seasonality, promotions, and product features that cannot be captured by linear models. Second, when working with high-dimensional inputs such as image or text data, we need feature learning capabilities that neural networks provide naturally. Third, when estimating multiple quantiles simultaneously, neural networks can learn shared representations across quantiles, substantially improving computational efficiency. Finally, as we demonstrate in Section 21.5, neural architectures enable us to incorporate utility functions directly into the learning process, seamlessly integrating prediction with decision-making.\nNeural quantile regression addresses these limitations by combining the robustness and interpretability of quantile methods with the flexibility and scalability of deep learning. This synthesis proves particularly valuable in applications where both distributional uncertainty and complex feature interactions matter. We now formalize this scalable approach using the generative framework.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-quantile-regression",
    "href": "21-qnn.html#sec-qnn-quantile-regression",
    "title": "21  Quantile Neural Networks",
    "section": "",
    "text": "Example 21.1 (Linear Quantile Regression) To illustrate quantile regression in practice, we’ll analyze the relationship between engine displacement and fuel efficiency using the classic mtcars dataset. Rather than only estimating the mean relationship (as ordinary least squares would), we’ll also estimate conditional quantiles to understand how this relationship varies across the distribution of fuel efficiency. This allows us to capture the heteroskedasticity in the data. Figure 21.1 shows the quantile regression results.\n\n\nQuantile regression on mtcars dataset\n# Load mtcars dataset\ndata(mtcars)\n\n# Define the check loss (pinball loss) function\ncheck_loss &lt;- function(u, tau) {\n    # rho_tau(u) = u * (tau - I(u &lt; 0))\n    u * (tau - (u &lt; 0))\n}\n\n# Objective function: sum of check losses for quantile regression\nquantile_objective &lt;- function(beta, X, y, tau) {\n    # Predicted values\n    y_pred &lt;- X %*% beta\n    # Residuals\n    residuals &lt;- y - y_pred\n    # Sum of check losses\n    sum(check_loss(residuals, tau))\n}\n\n# Prepare data\nX &lt;- cbind(1, log(mtcars$disp)) # Design matrix with intercept\ny &lt;- log(mtcars$mpg)\n\n# Fit quantile regression models using optim()\nquantiles &lt;- c(0.1, 0.5, 0.9)\nresults &lt;- list()\n\nfor (tau in quantiles) {\n    # Initial values (use OLS estimates as starting point)\n    ols_fit &lt;- lm(log(mpg) ~ log(disp), data = mtcars)\n    beta_init &lt;- coef(ols_fit)\n\n    # Optimize using BFGS (quasi-Newton method)\n    # We use BFGS because the check loss is non-differentiable at zero,\n    # but BFGS can handle this with numerical approximations\n    optim_result &lt;- optim(\n        par = beta_init,\n        fn = quantile_objective,\n        X = X,\n        y = y,\n        tau = tau,\n        method = \"BFGS\"\n    )\n\n    results[[as.character(tau)]] &lt;- list(\n        coefficients = optim_result$par,\n        value = optim_result$value,\n        convergence = optim_result$convergence\n    )\n}\n\n# Also fit OLS for comparison\nols_model &lt;- lm(log(mpg) ~ log(disp), data = mtcars)\n\n# Create visualization\nplot(log(mtcars$disp), log(mtcars$mpg),\n    pch = 16, col = \"gray30\",\n    xlab = \"Displacement (cu.in.)\",\n    ylab = \"Miles per Gallon\",cex.lab = 0.8,cex.axis = 0.8,cex.main = 0.8\n)\n\n# Add regression lines\ndisp_range &lt;- seq(min(log(mtcars$disp)), max(log(mtcars$disp)), length.out = 100)\n\n# OLS line\nabline(ols_model, col = \"black\", lwd = 2, lty = 2)\n\n# Quantile regression lines\ncolors &lt;- c(\"blue\", \"darkgreen\", \"red\")\nfor (i in seq_along(quantiles)) {\n    tau &lt;- quantiles[i]\n    beta &lt;- results[[as.character(tau)]]$coefficients\n    # Predicted values: y = beta_0 + beta_1 * x\n    pred &lt;- beta[1] + beta[2] * disp_range\n    lines(disp_range, pred, col = colors[i], lwd = 2)\n}\n\n# Add legend\nlegend(\"topright\",\n    legend = c(\n        \"OLS (Mean)\",\n        expression(tau == 0.1),\n        expression(tau == 0.5 ~ \"(Median)\"),\n        expression(tau == 0.9)\n    ),\n    col = c(\"black\", colors),lty = c(2, 1, 1, 1),\n    lwd = 2,bg = \"white\",bty = \"n\",cex = 0.6\n)\n\n\n\n\n\n\n\n\nFigure 21.1: Quantile regression on mtcars dataset. The relationship between engine displacement and fuel efficiency varies across quantiles, revealing heteroskedasticity in the data.\n\n\n\n\n\nTable Table 21.1 shows the estimated coefficients.\n\n\nQuantile regression results\n# Print model summaries\n# Create results table\nresults_table &lt;- data.frame(\n    Model = character(),\n    Intercept = numeric(),\n    Slope = numeric(),\n    stringsAsFactors = FALSE\n)\n\n# Add quantile regression results\nfor (tau in quantiles) {\n    results_table &lt;- rbind(results_table, data.frame(\n        Model = sprintf(\"Quantile tau = %.1f\", tau),\n        Intercept = results[[as.character(tau)]]$coefficients[1],\n        Slope = results[[as.character(tau)]]$coefficients[2]\n    ))\n}\n\n# Add OLS results\nresults_table &lt;- rbind(results_table, data.frame(\n    Model = \"OLS (Mean)\",\n    Intercept = coef(ols_model)[1],\n    Slope = coef(ols_model)[2]\n))\n\n# Display as kable table\nknitr::kable(results_table,\n    digits = 4,\n    caption = \"Quantile Regression Results: Fuel Efficiency vs. Engine Displacement\",\n    col.names = c(\"Model\", \"Intercept\", \"Slope\")\n)\n\n\n\n\nTable 21.1: Quantile regression results\n\n\n\n\nQuantile Regression Results: Fuel Efficiency vs. Engine Displacement\n\n\n\nModel\nIntercept\nSlope\n\n\n\n\n(Intercept)\nQuantile tau = 0.1\n5.6\n-0.53\n\n\n(Intercept)1\nQuantile tau = 0.5\n5.5\n-0.48\n\n\n(Intercept)2\nQuantile tau = 0.9\n5.1\n-0.37\n\n\n(Intercept)3\nOLS (Mean)\n5.4\n-0.46\n\n\n\n\n\n\n\n\nThe quantile regression results reveal several important patterns in the relationship between engine displacement and fuel efficiency. First, the slopes differ substantially across quantiles, indicating heteroskedasticity in the conditional distribution. The 10th percentile slope is -0.5340, while the 90th percentile slope is -0.3660. This difference suggests that the negative relationship between displacement and fuel efficiency is stronger for more fuel-efficient cars.\nSecond, the widening gap between the 10th and 90th percentiles as displacement increases reveals increasing uncertainty in fuel efficiency for larger engines. This pattern likely reflects varying driving conditions, maintenance practices, and vehicle technology across different cars with similar engine sizes.\nThird, the median regression (\\(\\tau\\) = 0.5) demonstrates robustness to outliers. Unlike OLS, which minimizes squared errors and thus heavily weights extreme values, quantile regression uses the asymmetric absolute loss that treats positive and negative residuals differently based on the quantile level. This asymmetry makes the estimator less sensitive to unusual observations.\nFinally, the check loss \\(\\rho_\\tau(u)\\) is piecewise linear, making it non-differentiable at zero. This property explains why we use BFGS rather than gradient-based methods that assume smoothness. The BFGS algorithm builds a quasi-Newton approximation that handles the kink effectively, converging reliably despite the non-smooth objective function.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-generative",
    "href": "21-qnn.html#sec-qnn-generative",
    "title": "21  Quantile Neural Networks",
    "section": "21.2 From Densities to Quantiles: A Generative Approach",
    "text": "21.2 From Densities to Quantiles: A Generative Approach\nLet \\((X,Y) \\sim P_{X,Y}\\) be input-output pairs and \\(P_{X,Y}\\) a joint measure from which we can simulate a training dataset \\((x_i, y_i)_{i=1}^N \\sim P_{X,Y}\\). Standard prediction techniques focus on the conditional posterior mean \\(\\hat{X}(Y) = E(X|Y) = f(Y)\\) of the input given the output. The standard approach formulates this as nonparametric regression \\(X = f(Y) + \\epsilon\\) and estimates the conditional mean using methods such as kernel smoothing or K-nearest neighbors. Recently, deep learning approaches have been proposed, with theoretical properties established by N. G. Polson and Sokolov (2023).\nGenerative methods take this approach one step further. Let \\(Z \\sim P_Z\\) be a base measure for a latent variable, \\(Z\\), typically a standard multivariate normal or vector of uniforms. The goal of generative methods is to characterize the posterior measure \\(P_{X|Y}\\) from the training data \\((x_i, y_i)_{i=1}^N \\sim P_{X,Y}\\) where \\(N\\) is chosen to be suitably large. A deep learner is used to estimate \\(\\hat{f}\\) via the non-parametric regression \\(X = f(Y, Z)\\). In the case where \\(Z\\) is uniform, this amounts to inverse CDF sampling, namely \\(X = F_{X|Y}^{-1}(Z)\\)—the quantile function that we develop formally in Section 21.1.\nIn general, we characterize the posterior map for any output \\(Y\\). We characterize the posterior by evaluating the network at any \\(Y\\) using the transport map \\[\nX = H(S(Y), \\psi(Z))\n\\] Here \\(\\psi\\) denotes the embedding function. The deep learners \\(H\\) and \\(S\\) are estimated from the triples \\((X_i, Y_i, \\psi(Z_i))_{i=1}^N \\sim P_{X,Y} \\times P_Z\\). The ensuing estimator \\(\\hat{H}\\) can be thought of as a transport map from the base distribution to the posterior as required.\nThe following diagram illustrates the transport map architecture:\n\n\n\nTransport map architecture for generative inference. Data \\(Y\\) is summarized by statistics \\(S(Y)\\), while latent variable \\(Z\\) is embedded via \\(\\psi\\). The transport function \\(H\\) maps these inputs to posterior samples.\n\n\nSpecifically, the idea of generative methods is straightforward. Let \\(y\\) denote data and \\(\\theta\\) a vector of parameters including any hidden states (a.k.a. latent variables) \\(z\\). First, we generate a “look-up” table of “fake” data \\(\\{y^{(i)}, \\theta^{(i)}\\}_{i=1}^N\\). By simulating a training dataset of outputs and parameters, we can use deep learning to solve for the inverse map via a supervised learning problem. Generative methods have the advantage of being likelihood-free. For example, our model might be specified by a forward map \\(y^{(i)} = f(\\theta^{(i)})\\) rather than a traditional random draw from a likelihood function \\(y^{(i)} \\sim p(y^{(i)}|\\theta^{(i)})\\). The base distribution \\(P_Z\\) is typically uniform (for univariate problems) or a very high-dimensional Gaussian vector (for multivariate problems).\nThe theoretical foundation for this approach is the noise outsourcing theorem, which guarantees that we can represent any posterior distribution through a deterministic function of the data and a base random variable.\nNoise Outsourcing Theorem (Kallenberg 1997): If \\((Y, \\Theta)\\) are random variables in a standard probability space \\((\\mathcal{Y}, \\Theta)\\), then there exists a random variable \\(\\tau \\sim U(0,1)\\) which is independent of \\(Y\\) and a function \\(H: [0,1] \\times \\mathcal{Y} \\rightarrow \\Theta\\) such that \\[\n(Y, \\Theta) \\stackrel{a.s.}{=} (Y, H(Y, \\tau))\n\\] Moreover, if there is a sufficient statistic \\(S(Y)\\) with \\(Y\\) independent of \\(\\Theta | S(Y)\\), then \\[\n\\Theta\\mid Y \\stackrel{a.s.}{=} H(S(Y), \\tau).\n\\]\nThis result tells us that posterior uncertainty can be characterized via an inverse non-parametric regression problem where we predict \\(\\theta^{(i)}\\) from \\(y^{(i)}\\) and \\(\\tau^{(i)}\\), where \\(\\tau^{(i)}\\) is drawn from a base distribution \\(p(\\tau)\\). The base distribution is typically uniform (for univariate problems) or a very high-dimensional Gaussian vector (for multivariate problems). We train a deep neural network \\(H\\) on \\[\n\\theta^{(i)} = H(S(y^{(i)}), \\tau^{(i)}).\n\\] Here \\(S(y)\\) is a statistic used to perform dimension reduction with respect to the signal distribution—analogous to sufficient statistics in traditional Bayesian inference. A result due to Brillinger (2012) shows that for single-index models, we can estimate the subspace spanned by \\(S\\) via ordinary least squares, effectively learning the sufficient summary statistics from data.\nSpecifying the architecture of \\(H\\) is key to the efficiency of the approach. N. Polson, Ruggeri, and Sokolov (2024) propose using quantile neural networks implemented with ReLU activation functions, which we detail in Section 21.6.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-trend-filtering",
    "href": "21-qnn.html#sec-qnn-trend-filtering",
    "title": "21  Quantile Neural Networks",
    "section": "21.3 Nonlinear Quantile Regression via Trend Filtering",
    "text": "21.3 Nonlinear Quantile Regression via Trend Filtering\nBefore exploring the full power of neural networks for quantile regression, we examine an elegant middle ground: trend filtering combined with quantile loss. This approach, developed by N. G. Polson and Scott (2016), provides nonlinear function approximation while maintaining computational tractability through a hierarchical representation. Trend filtering estimates smooth, nonlinear functions by penalizing differences in derivatives rather than the function values themselves, making it particularly suitable for data with local smoothness but global complexity.\nConsider the nonparametric regression problem where we observe pairs \\((x_i, y_i)\\) for \\(i = 1, \\ldots, n\\) with \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_n\\). Traditional smoothing methods like cubic splines require choosing knot locations, while kernel smoothing requires bandwidth selection. Trend filtering offers an alternative: estimate a function \\(f(x)\\) by solving\n\\[\n\\min_{f} \\sum_{i=1}^n \\rho_\\tau(y_i - f(x_i)) + \\lambda \\sum_{i=1}^{n-k} |\\Delta^k f_i|\n\\]\nwhere \\(\\Delta^k\\) denotes the \\(k\\)-th order discrete derivative operator and \\(\\rho_\\tau\\) is the check loss for quantile \\(\\tau\\). The penalty term controls smoothness: \\(k=1\\) penalizes changes in slope (linear trend filtering), \\(k=2\\) penalizes changes in curvature (quadratic trend filtering), and so on.\nFor \\(k=2\\), the penalty becomes \\(\\sum_{i=2}^{n-1} |f_{i+1} - 2f_i + f_{i-1}|\\), which approximates the integrated squared second derivative \\(\\int (f''(x))^2 dx\\) used in smoothing splines. However, the \\(\\ell_1\\) penalty produces locally adaptive estimates—sharp changes are preserved while smooth regions remain smooth.\nN. G. Polson and Scott (2016) show that trend filtering admits an elegant hierarchical representation through envelope duality. The key insight is that the \\(\\ell_1\\) penalty can be represented as an exponential prior in a hierarchical model. Specifically, for second-order trend filtering with quantile loss, we have the hierarchical model:\n\\[\n\\begin{aligned}\ny_i &\\sim \\text{AsymmetricLaplace}(f_i, \\tau, \\sigma) \\\\\n\\Delta^2 f_i &\\sim \\text{Laplace}(0, 1/\\lambda) \\quad \\text{for } i = 2, \\ldots, n-1\n\\end{aligned}\n\\]\nThe asymmetric Laplace distribution naturally arises from the check loss—it is the distribution whose maximum likelihood estimator at quantile \\(\\tau\\) minimizes \\(\\rho_\\tau\\). This connection between optimization (minimizing penalized quantile loss) and probability (maximum a posteriori estimation in a hierarchical model) provides both computational and conceptual advantages.\nThe hierarchical formulation enables efficient computation through data augmentation schemes. Rather than directly optimizing the non-smooth objective, we introduce auxiliary variables that yield closed-form conditional distributions, leading to straightforward EM or Gibbs sampling algorithms.\nWe now demonstrate trend filtering for nonlinear quantile regression using synthetic data with both smooth regions and sharp transitions. Figure Figure 21.2 shows the results.\n\n\nTrend filtering for nonlinear quantile regression\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate nonlinear synthetic data with heteroskedasticity\nn &lt;- 100\nx &lt;- sort(runif(n, 0, 10))\n\n# True function with different regimes\nf_true &lt;- ifelse(x &lt; 3, 1 + 0.5 * x,\n    ifelse(x &lt; 5, 2.5 + 2 * (x - 3),\n        6.5 + 0.2 * (x - 5)\n    )\n)\n\n# Heteroskedastic noise that increases with x\nsigma &lt;- 0.3 + 0.1 * x\ny &lt;- f_true + rnorm(n, 0, sigma)\n\n# Define second-order difference operator\nD2 &lt;- function(n) {\n    D &lt;- matrix(0, n - 2, n)\n    for (i in 1:(n - 2)) {\n        D[i, i:(i + 2)] &lt;- c(1, -2, 1)\n    }\n    return(D)\n}\n\n# Trend filtering objective function for quantile regression\ntrend_filter_obj &lt;- function(f, y, lambda, tau, D) {\n    # Check loss\n    residuals &lt;- y - f\n    quantile_loss &lt;- sum(residuals * (tau - (residuals &lt; 0)))\n\n    # L1 penalty on second differences\n    penalty &lt;- lambda * sum(abs(D %*% f))\n\n    return(quantile_loss + penalty)\n}\n\n# Fit trend filtering for multiple quantiles using optim\n# For better performance, we use BFGS with box constraints\nquantiles &lt;- c(0.1, 0.5, 0.9)\nlambda &lt;- 2.0 # Smoothing parameter\nD &lt;- D2(n)\n\nresults_tf &lt;- list()\n\nfor (tau in quantiles) {\n    # Initialize with linear quantile regression\n    X_init &lt;- cbind(1, x)\n    init_fit &lt;- lm(y ~ x)\n    f_init &lt;- predict(init_fit)\n\n    # Optimize using L-BFGS-B\n    opt_result &lt;- optim(\n        par = f_init,\n        fn = trend_filter_obj,\n        y = y,\n        lambda = lambda,\n        tau = tau,\n        D = D,\n        method = \"L-BFGS-B\"\n    )\n\n    results_tf[[as.character(tau)]] &lt;- opt_result$par\n}\n\n# Visualization\npar(mfrow = c(1, 2), mar = c(4, 4, 3, 1))\n\n# Left panel: Data and trend filtering estimates\nplot(x, y,\n    pch = 16, col = \"gray50\", cex = 0.8,\n    xlab = \"x\", ylab = \"y\",\n    main = \"Trend Filtering Quantile Regression\"\n)\n\n# True function\nlines(x, f_true, col = \"black\", lwd = 2, lty = 2)\n\n# Quantile estimates\ncolors &lt;- c(\"blue\", \"darkgreen\", \"red\")\nfor (i in seq_along(quantiles)) {\n    tau &lt;- quantiles[i]\n    lines(x, results_tf[[as.character(tau)]],\n        col = colors[i], lwd = 2.5\n    )\n}\n\nlegend(\"topleft\",\n    legend = c(\"True mean\", \"tau = 0.1\", \"tau = 0.5\", \"tau = 0.9\"),\n    col = c(\"black\", colors),\n    lty = c(2, 1, 1, 1),\n    lwd = c(2, 2.5, 2.5, 2.5),\n    bg = \"white\", \n    bty = \"n\"\n)\n\n# Right panel: Uncertainty quantification\nplot(x, y,\n    pch = 16, col = \"gray50\", cex = 0.8,\n    xlab = \"x\", ylab = \"y\",\n    main = \"Conditional Quantiles and Prediction Intervals\"\n)\n\n# Fill prediction intervals\npolygon(c(x, rev(x)),\n    c(results_tf[[\"0.1\"]], rev(results_tf[[\"0.9\"]])),\n    col = rgb(0.5, 0.5, 1, 0.3), border = NA\n)\n\n# Quantile curves\nlines(x, results_tf[[\"0.1\"]], col = \"blue\", lwd = 2, lty = 2)\nlines(x, results_tf[[\"0.5\"]], col = \"darkgreen\", lwd = 2.5)\nlines(x, results_tf[[\"0.9\"]], col = \"red\", lwd = 2, lty = 2)\n\nlegend(\"topleft\",\n    legend = c(\"P50 (Median)\", \"P10-P90 (80% interval)\"),\n    col = c(\"darkgreen\", rgb(0.5, 0.5, 1)),\n    lwd = c(2.5, 8),\n    bg = \"white\",\n    bty = \"n\"\n)\n\n# Calculate empirical coverage\n# coverage &lt;- mean(y &gt;= results_tf[[\"0.1\"]] & y &lt;= results_tf[[\"0.9\"]])\n# cat(sprintf(\"\\nEmpirical 80%% interval coverage: %.1f%%\\n\", coverage * 100))\n\n\n\n\n\n\n\n\nFigure 21.2: Nonlinear quantile regression via trend filtering. The method adapts to both smooth regions and sharp transitions while estimating conditional quantiles.\n\n\n\n\n\nFigure 21.2 illustrates trend filtering quantile regression on synthetic data with heteroskedastic noise. The left panel shows how different quantile levels adapt to the nonlinear underlying structure. The 10th percentile (\\(\\tau=0.1\\)) tracks the lower boundary of the data cloud, the median (\\(\\tau=0.5\\)) estimates the central tendency, and the 90th percentile (\\(\\tau=0.9\\)) follows the upper boundary. Each estimated quantile function captures both the smooth regions and the more abrupt transitions in the data without requiring parametric assumptions about the functional form.\nThe right panel demonstrates how these estimated quantiles provide prediction intervals that adapt to the changing variability across the input space. The shaded region between the 10th and 90th percentiles represents an 80% prediction interval. Notice how this interval widens in regions where the data exhibits greater dispersion and narrows where the data are more tightly clustered—precisely the behavior we desire for uncertainty quantification. The empirical coverage for this interval is 68.0%, which is somewhat below the nominal 80% level, reflecting the typical challenge that estimated quantiles tend to undercover when sample sizes are modest and the underlying function is complex. This underscores an important practical consideration: prediction intervals from quantile regression should be interpreted as approximate, with their reliability improving as sample size increases and as the ratio of smoothing penalty to noise level is appropriately calibrated.\nTrend filtering for quantile regression offers computational efficiency through sparse linear algebra and flexibility in capturing both smooth regions and abrupt transitions—properties that make it particularly attractive for applications with ordered, low-dimensional data.\nTrend filtering provides an important conceptual bridge to neural quantile networks. Both approaches learn nonlinear functions through composition: trend filtering composes piecewise polynomials, while neural networks compose nonlinear activation functions. The key difference lies in how they handle high-dimensional inputs. Trend filtering is most effective for univariate or low-dimensional problems with ordered inputs, while neural networks excel when inputs are high-dimensional or lack natural ordering.\nFor problems with structured, low-dimensional inputs—time series, spatial data along a transect, dose-response curves—trend filtering often provides better interpretability and requires less data than neural networks. For high-dimensional problems—images, text, complex multivariate relationships—neural networks become essential. Understanding both approaches allows practitioners to choose the right tool for their specific problem structure.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-bayes-quantiles",
    "href": "21-qnn.html#sec-qnn-bayes-quantiles",
    "title": "21  Quantile Neural Networks",
    "section": "21.4 Bayes Rule for Quantiles",
    "text": "21.4 Bayes Rule for Quantiles\nHaving established the fundamentals of quantile regression, we now develop the connection to Bayesian inference. Parzen (2004) showed that quantile methods provide direct alternatives to density-based Bayesian computations. This section establishes the theoretical foundation for using quantiles to perform Bayesian updating.\nGiven a cumulative distribution function \\(F_{\\theta|y}(u)\\) (non-decreasing, right-continuous), we define the quantile function as: \\[Q_{\\theta| y} (u) \\defeq  F^{-1}_{\\theta|y}  ( u ) = \\inf \\left \\{ \\theta : F_{\\theta|y} (\\theta) \\geq u \\right \\}\\]\nThe quantile function is non-decreasing and left-continuous. Parzen (2004) established the fundamental probabilistic property: \\[\n\\theta \\stackrel{P}{=} Q_\\theta ( F_\\theta (\\theta ) )\n\\]\nThis identity enables efficient implementation: we can increase computational efficiency by ordering the samples of \\(\\theta\\) and the baseline uniform draws \\(\\tau\\), exploiting the monotonicity of the inverse CDF map.\nA crucial property for understanding why quantiles naturally compose (and thus suit deep learning) is the following. Let \\(g(y)\\) be non-decreasing and left-continuous with \\(g^{-1}(z) = \\sup \\{ y : g(y) \\leq z \\}\\). Then the transformed quantile has a compositional nature: \\[Q_{g(Y)}(u) = g(Q(u))\\]\nThis composition property shows that quantiles act as superpositions—exactly the structure that deep neural networks learn through their layered architecture.\nThe connection to Bayesian learning is made explicit through the conditional quantile representation. For the Bayesian learning problem, we have the following result for updating prior to posterior quantiles: \\[Q_{\\theta | Y=y}(u) = Q_\\theta(s) \\quad \\text{where} \\quad s = Q_{F(\\theta) | Y=y}(u)\\]\nTo compute \\(s\\), note that by definition: \\[\nu = F_{F(\\theta) | Y=y}(s) = P(F(\\theta) \\leq s | Y=y) = P(\\theta \\leq Q_\\theta(s) | Y=y) = F_{\\theta | Y=y}(Q_\\theta(s))\n\\]\nThis result shows that Bayesian updating can be performed entirely in terms of quantile functions, without ever computing or manipulating density functions. The posterior quantile function is obtained by composing the prior quantile function with a learned transformation.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-meu",
    "href": "21-qnn.html#sec-qnn-meu",
    "title": "21  Quantile Neural Networks",
    "section": "21.5 Maximum Expected Utility via Quantile Neural Networks",
    "text": "21.5 Maximum Expected Utility via Quantile Neural Networks\nHaving established how to learn posterior distributions via quantile neural networks, we now show how to extend this framework to decision problems—the central application where quantile methods truly shine. Recall from Chapter 4 that optimal Bayesian decisions maximize expected utility: \\[\nd^\\star(y) = \\arg \\max_d E_{\\theta|y}[U(d, \\theta)] = \\arg \\max_d \\int U(d, \\theta) p(\\theta | y) d\\theta\n\\]\nThe naive approach would be to first learn the posterior \\(p(\\theta|y)\\), then use Monte Carlo to approximate the expected utility for each decision \\(d\\), and finally optimize over \\(d\\). However, this approach is inefficient for several reasons:\n\nComputational waste: Monte Carlo requires many samples in regions of high posterior probability, but utility functions often place high weight on tail events (risk scenarios) that have low posterior probability.\nDensity estimation: We must first estimate the potentially high-dimensional posterior density before we can compute expectations.\nOptimization difficulty: The expectation must be recomputed for each candidate decision during optimization.\n\nQuantile neural networks provide a more direct path. The key insight is that we can incorporate the utility function directly into the training process rather than as a post-processing step.\nThe foundation of our approach is a classical result relating expectations to quantiles. Given any random variable \\(U\\), its expectation can be computed as an integral over its quantile function: \\[\nE[U] = \\int_0^1 F^{-1}_{U}(\\tau) d\\tau\n\\] This is sometimes called the quantile representation of expectations or the Lorenz curve identity.\n\n\n\n\n\n\nNoteMathematical Foundation: Alternative Expectation Representations\n\n\n\nThe quantile-expectation identity above can be proven formally. Let \\(X\\) be a random variable with CDF \\(F\\) and PDF \\(f\\). The quantile function is defined as \\(F^{-1}(p) = \\inf\\{y \\in \\mathbb{R} \\mid F(y)\\ge p\\}\\). Then: \\[\n\\E{X} = \\int_{0}^1 F^{-1}(p) \\,\\mathrm{d}p = \\int_{0}^1 \\int_{-\\infty}^{F^{-1}(p)} f(x) \\,\\mathrm{d}x \\,\\mathrm{d}p = \\int_{-\\infty}^{\\infty} \\int_{0}^{F(x)} \\,\\mathrm{d}p f(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} x f(x) \\,\\mathrm{d}x.\n\\]\nFor non-negative random variables (where the variable has nonzero density/probability for only positive values), there is an alternative representation using the survival function: \\[\n\\E{X} = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x,\n\\] where \\(F(x)\\) is the CDF of \\(X\\). The proof uses Fubini’s theorem: \\[\n\\E{X} = \\int_0^\\infty \\left( 1 - F(x) \\right) \\,\\mathrm{d}x = \\int_0^\\infty \\int_x^\\infty f(y) \\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^\\infty \\int_0^y \\,\\mathrm{d}x f(y) \\,\\mathrm{d}y = \\int_0^\\infty y f(y) \\,\\mathrm{d}y.\n\\]\nNote that expectations may not exist for all distributions. For example, the Cauchy distribution has an undefined expectation because the integral diverges.\n\n\nFor decision problems, this means: \\[\nE_{\\theta|y}[U(d, \\theta)] = \\int_0^1 F^{-1}_{U|d,y}(\\tau) d\\tau\n\\]\nRather than learning \\(p(\\theta|y)\\) and then computing the expectation, we directly learn the quantile function \\(F^{-1}_{U|d,y}(\\tau)\\) of the utility distribution.\n\nImplementation Strategy\nTo extend our generative method to MEU problems, we assume that the utility function \\(U(d, \\theta)\\) is given (a standard assumption in decision theory). The training procedure is as follows:\n\nGenerate synthetic dataset: Simulate triples \\(\\{y^{(i)}, \\theta^{(i)}, \\tau^{(i)}\\}_{i=1}^N\\) where \\(y^{(i)} \\sim p(y|\\theta^{(i)})\\), \\(\\theta^{(i)} \\sim p(\\theta)\\), and \\(\\tau^{(i)} \\sim U(0,1)\\).\nCompute utilities: For each decision \\(d\\) of interest, compute \\(U^{(i)}_d \\defeq U(d,\\theta^{(i)})\\).\nAugment training data: Create the augmented dataset \\[\n\\{U_d^{(i)}, S(y^{(i)}), \\tau^{(i)}, d\\}_{i=1}^N.\n\\]\nTrain quantile network: Learn a neural network \\(H\\) that predicts utilities by minimizing the check loss: \\[\nU_d^{(i)} = H(S(y^{(i)}), \\tau^{(i)}, d)\n\\]\n\nOnce trained, the network \\(H\\) represents the quantile function \\(F^{-1}_{U|d,y}(\\tau)\\). For any observed data \\(y\\) and candidate decision \\(d\\), we can:\n\nCompute expected utility: Numerically integrate \\(\\int_0^1 H(S(y), \\tau, d) d\\tau\\)\nFind optimal decision: \\[\nd^\\star(y) = \\arg \\max_d \\int_0^1 H(S(y), \\tau, d) d\\tau\n\\]\n\nThis approach has several advantages over naive Monte Carlo:\n\nThe network learns to focus on regions of the \\((\\theta, \\tau)\\) space that matter for utility computation.\nWe avoid explicit density estimation of \\(p(\\theta|y)\\).\nThe same network handles all decisions \\(d\\) simultaneously if \\(d\\) is included as an input.\nThe approach naturally handles likelihood-free models where \\(p(y|\\theta)\\) is unavailable but we can simulate from the forward model.\n\nFor completeness, we provide the formal measure-theoretic framework. Let \\(\\mathcal{Y}\\) denote a locally compact metric space of signals \\(y\\) and \\(\\Theta\\) a space of parameters \\(\\theta\\) (including any latent variables). Let \\(P(dy|\\theta)\\) denote the conditional distribution of signals given parameters. Let \\(\\Pi(d\\theta|y)\\) denote the posterior distribution. In many cases, \\(\\Pi\\) is absolutely continuous with density \\(\\pi\\): \\[\n\\Pi(d\\theta|y) = \\pi(\\theta|y) \\mu(d\\theta).\n\\]\nThe framework handles both traditional likelihood-based models where \\(P(dy|\\theta) = p(y|\\theta) \\lambda(dy)\\) and likelihood-free models specified by forward simulators \\(y = f(\\theta)\\). This generality is crucial for modern applications in economics, epidemiology, and climate science where complex simulation models replace closed-form likelihoods.\nFor multivariate parameters \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\), we can use autoregressive structures to model the sequence of conditional quantiles: \\[\n(F^{-1}_{\\theta_1}(\\tau_1), F^{-1}_{\\theta_2|\\theta_1}(\\tau_2), \\ldots, F^{-1}_{\\theta_p|\\theta_{1:p-1}}(\\tau_p))\n\\] This factorization is analogous to autoregressive density models but operates directly on quantiles, avoiding normalization constraints.\nAn important architectural choice distinguishes our approach from standard posterior learning followed by Monte Carlo integration: we incorporate the utility function \\(U(d, \\theta)\\) directly into the first layer of the network. This allows the network to learn representations optimized for utility computation rather than pure posterior approximation. As utility functions often place high weight on tail events (representing rare but consequential outcomes), this direct incorporation significantly improves efficiency compared to the naive two-step approach.\n\nExample 21.2 (Normal-Normal Model and Wang Distortion) For illustration, we consider the normal-normal conjugate learning model—a case where the quantile updating rule can be derived analytically. This example connects quantile methods to Wang’s risk distortion measure from finance, showing that the distortion function is precisely the transformation that needs to be learned.\nConsider the model: \\[\ny_1, \\ldots, y_n \\mid \\theta \\sim N(\\theta, \\sigma^2)\n\\] \\[\n\\theta \\sim N(\\mu,\\alpha^2)\n\\]\nThe sufficient statistic for \\(\\mu\\) (assuming known \\(\\sigma^2\\)) is \\(S(y) = \\bar y = \\frac{1}{n} \\sum_{i=1}^n y_i\\). Define \\(t = \\sigma^2 + n\\alpha^2\\). The posterior is \\(\\theta \\mid y \\sim N(\\mu_*, \\sigma_*^2)\\) with \\[\n\\mu_* = \\frac{\\sigma^2 \\mu + n\\alpha^2\\bar{y}}{t}, \\quad \\sigma^2_* = \\frac{\\alpha^2 \\sigma^2}{t}\n\\]\nThe remarkable result is that the posterior and prior CDFs are related via a Wang distortion function: \\[\n1-\\Phi(\\theta; \\mu_*,\\sigma_*) = g(1 - \\Phi(\\theta; \\mu, \\alpha^2))\n\\] where \\(\\Phi(\\cdot; \\mu, \\sigma^2)\\) denotes the normal CDF. The Wang distortion is: \\[\ng(p) = \\Phi\\left(\\lambda_1 \\Phi^{-1}(p) + \\lambda\\right)\n\\] with distortion parameters: \\[\n\\lambda_1 = \\frac{\\alpha}{\\sigma_*}, \\quad \\lambda = \\frac{\\alpha\\lambda_1(n\\bar{y}-n\\mu)}{t}\n\\]\nThe detailed derivation, provided by Wang (1996), shows that the distortion parameters depend on the sample statistics and prior hyperparameters through the posterior updating formulas.\nThis analytical result has several implications:\n\nWang distortions are natural: The distortion functions used in risk management (Wang 1996) arise naturally from Bayesian updating in the normal case.\nLearning the distortion: In more complex models, a neural network can learn this distortion function \\(g\\) directly from data, without requiring conjugacy.\nComputational efficiency: When the distortion is smooth and well-behaved, neural networks with relatively few parameters can accurately represent it.\n\nNumerical Example: Consider prior \\(\\theta \\sim N(0,5)\\) and data from \\(y_i \\sim N(3,10)\\) with \\(n=100\\) observations. The posterior is \\(\\theta \\mid y \\sim N(3.28, 0.98)\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Model for simulated data\n\n\n\n\n\n\n\n\n\n\n\n(b) Distortion Function \\(g\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) Survival Functions 1 - \\(\\Phi\\)\n\n\n\n\n\n\n\nFigure 21.3: The Normal-Normal learning model. Left: Prior, likelihood, and posterior densities. Center: The Wang distortion function \\(g\\) that transforms the prior CDF to the posterior CDF. Right: Survival functions showing how \\(g\\) maps the prior tail probabilities to posterior tail probabilities.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-implementation",
    "href": "21-qnn.html#sec-qnn-implementation",
    "title": "21  Quantile Neural Networks",
    "section": "21.6 Neural Network Implementation",
    "text": "21.6 Neural Network Implementation\nHaving established the theory, we now detail how to implement quantile neural networks in practice. The key components are: (1) an appropriate loss function, (2) a neural architecture that handles the quantile input \\(\\tau\\), and (3) training strategies for learning multiple quantiles simultaneously.\nThe 1-Wasserstein distance (also known as earth mover’s distance) provides theoretical justification for quantile methods. For two distributions with quantile functions \\(F^{-1}_U\\) and \\(F^{-1}_V\\), the 1-Wasserstein distance is: \\[\nW_1(F^{-1}_U, F^{-1}_V) = \\int_0^1 |F^{-1}_U(\\tau) - F^{-1}_V(\\tau)| d\\tau\n\\]\nThis distance can be computed efficiently using order statistics (Levina and Bickel 2001). The success of Wasserstein GANs (Arjovsky, Chintala, and Bottou 2017) over vanilla GANs stems partly from this improved metric—Wasserstein distance provides meaningful gradients even when distributions have non-overlapping supports.\nThe key insight is that minimizing the quantile loss is equivalent to minimizing the 1-Wasserstein distance. For a target quantile \\(q_\\tau = F^{-1}_U(\\tau)\\), the check loss \\(\\rho_\\tau\\) we derived in Section 21.1 minimizes the expected prediction error: \\[\nq_\\tau = \\arg\\min_q E_U[\\rho_{\\tau}(U-q)]\n\\]\nFor training neural networks, we use a combination of quantile loss and mean-squared error (MSE). Given training data \\(\\{x_i, y_i\\}_{i=1}^N\\) and a quantile \\(\\tau\\), the loss is: \\[\nL_{\\tau}(\\phi) = \\sum_{i=1}^N \\rho_{\\tau}(y_i - f(\\tau, x_i, \\phi))\n\\]\nHere \\(\\phi\\) denotes the neural network weights, distinct from the model parameters \\(\\theta\\) discussed elsewhere. Empirically, adding an MSE term improves stability and predictive accuracy: \\[\nL(\\phi) = \\alpha L_{\\tau}(\\phi) + (1-\\alpha) \\cdot \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i, \\phi))^2\n\\]\nThe weighting parameter \\(\\alpha \\in [0,1]\\) balances quantile accuracy against overall fit. Typical values are \\(\\alpha \\in [0.7, 0.9]\\). The MSE term encourages the median prediction (\\(\\tau = 0.5\\)) to align with the conditional mean, which often improves generalization.\n\nLearning Multiple Quantiles Simultaneously\nRather than training separate networks for each quantile \\(\\tau_k\\), it is more efficient to learn all quantiles with a single network that takes \\(\\tau\\) as an input. Given quantiles \\(0 &lt; \\tau_1 &lt; \\tau_2 &lt; \\ldots &lt; \\tau_K &lt; 1\\), we minimize: \\[\nL(\\phi) = \\frac{1}{NK} \\sum_{i=1}^N \\sum_{k=1}^K \\rho_{\\tau_k}(y_i - f_{\\tau_k}(x_i, \\phi))\n\\]\nThis approach has several advantages:\n\nShared representations: The network learns features useful across all quantiles, improving sample efficiency.\nEnforcing monotonicity: A single network makes it easier to ensure quantiles don’t cross.\nSmooth quantile function: Interpolation between trained quantiles is more reliable.\n\n\n\nNon-Crossing Constraints\nA valid distribution function must satisfy \\(F^{-1}(\\tau_i) \\leq F^{-1}(\\tau_j)\\) for \\(\\tau_i &lt; \\tau_j\\). Without explicit constraints, neural networks may learn quantile functions that cross: \\[\nf_{\\tau_i}(x, \\theta) &gt; f_{\\tau_j}(x, \\theta) \\quad \\text{for some } x, \\text{ despite } \\tau_i &lt; \\tau_j\n\\]\nSeveral approaches address this (Chernozhukov, Fernández-Val, and Galichon 2010; Cannon 2018):\n\nSoft penalties: Add a term to the loss penalizing violations.\nMonotonic networks: Design architectures that guarantee monotonicity in \\(\\tau\\) (e.g., using monotonic activation functions or cumulative link structures).\nPost-processing: After training, rearrange predictions to enforce monotonicity.\n\nFor the implementations in this chapter, we use soft penalties during training combined with post-processing for final predictions.\n\n\nCosine Embedding for \\(\\tau\\)\nA key architectural choice is how to incorporate the quantile level \\(\\tau \\in (0,1)\\) as an input to the network. Simply concatenating \\(\\tau\\) as an additional feature works but is inefficient—the network must learn the entire relationship between \\(\\tau\\) and the output from scratch.\nA more effective approach uses a cosine embedding to represent \\(\\tau\\) in a higher-dimensional feature space. This leverages Fourier analysis: smooth functions can be well-approximated by cosine bases. The quantile function \\(F^{-1}(\\tau, x)\\) is typically smooth in \\(\\tau\\), making Fourier representations natural.\nWe represent the quantile network as: \\[\nF^{-1}(\\tau, x) = f_\\theta(\\tau, x) = g(\\psi(x) \\circ \\phi(\\tau))\n\\] where \\(\\circ\\) denotes element-wise multiplication (Hadamard product), \\(g\\) and \\(\\psi\\) are feed-forward networks, and \\(\\phi\\) is the cosine embedding: \\[\n\\phi_j(\\tau) = \\mathrm{ReLU}\\left(\\sum_{i=0}^{n-1} \\cos(\\pi i \\tau) w_{ij} + b_j\\right)\n\\]\nThe cosine embedding \\(\\phi(\\tau)\\) transforms the scalar \\(\\tau\\) into a vector of dimension \\(m\\), where \\(n\\) controls the frequency resolution. This embedding has several advantages:\n\nSmooth interpolation: The cosine basis ensures smooth quantile functions.\nUniversal approximation: Barron (1993) showed that cosine-embedded networks achieve approximation rates of \\(O(N^{-1/2})\\) for sufficiently smooth functions.\nParameter efficiency: The embedding significantly reduces the number of parameters needed compared to learning the \\(\\tau\\) dependence from scratch.\n\nThis architecture was successfully applied to distributional reinforcement learning by Dabney et al. (2018), where it enabled agents to learn entire distributions of returns rather than just expectations. We use the same principle here for Bayesian posterior quantiles.\n\n\nSynthetic Data Example\nTo validate our approach before applying it to real data, we first test on synthetic data where the true quantile function is known. This allows us to assess both accuracy and the quality of uncertainty quantification.\nConsider synthetic data generated from the model: \\[\nx \\sim U(-1, 1), \\quad y | x \\sim N\\left(\\frac{\\sin(\\pi x)}{\\pi x}, \\frac{\\exp(1-x)}{10}\\right)\n\\]\nThis model has heteroskedastic noise—the variance increases as \\(x\\) decreases. The conditional mean is the sinc function \\(\\text{sinc}(x) = \\sin(\\pi x)/(\\pi x)\\), which has interesting non-linear behavior near zero.\nThe true conditional \\(\\tau\\)-quantile function is: \\[\nf_{\\tau}(x) = \\frac{\\sin(\\pi x)}{\\pi x} + \\Phi^{-1}(\\tau) \\sqrt{\\frac{\\exp(1-x)}{10}}\n\\]\nWe train two types of quantile networks:\n\nImplicit network: Uses cosine embedding for \\(\\tau\\), trained on random \\((\\tau, x, y)\\) triples\nExplicit network: Trains separate outputs for fixed quantiles \\(\\tau \\in \\{0.05, 0.5, 0.95\\}\\)\n\n\n\n\nQuantile neural network predictions on synthetic data. Both implicit and explicit architectures recover the true quantile functions accurately. The shaded region shows the 90% prediction interval (\\(\\tau = 0.05\\) to \\(\\tau = 0.95\\)).\n\n\nThe figure shows both networks recover the true quantiles accurately. The implicit network provides smooth interpolation across all \\(\\tau\\) values, while the explicit network gives predictions only at the three trained quantiles. For applications requiring full distributional predictions, the implicit approach is preferable despite slightly higher computational cost during training.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-portfolio",
    "href": "21-qnn.html#sec-qnn-portfolio",
    "title": "21  Quantile Neural Networks",
    "section": "21.7 Portfolio Optimization with Quantile Neural Networks",
    "text": "21.7 Portfolio Optimization with Quantile Neural Networks\nWe now demonstrate how quantile neural networks can solve portfolio optimization problems under parameter uncertainty. This example, developed by N. Polson, Ruggeri, and Sokolov (2024), shows how to maximize expected utility when the optimal decision depends on unknown parameters about which we are learning.\nConsider power utility and log-normal returns without leverage. We assume that a portfolio value \\(X = e^W\\) follows a log-normal distribution \\[\nW(\\omega) = (1-\\omega)r_f + \\omega R, \\quad R \\sim \\mathcal{N}(\\mu,\\sigma^2)\n\\] Here \\(\\omega \\in (0,1)\\) is the portfolio weight, \\(r_f\\) is the risk-free rate, \\(\\mu\\) is the mean return and \\(\\sigma^2\\) is the variance of the return. The utility function is then given by \\[\nU(W) = -e^{-\\gamma W}.\n\\] Here, \\(U^{-1}\\) exists, and the expected utility is \\[\nU(\\omega) = E(-e^{\\gamma W}) = \\exp\\left\\{\\gamma \\mathrm{E}(W) + \\frac{1}{2}\\omega^2\\mathrm{Var}(W)\\right\\}.\n\\] In this case, we have a closed-form solution for the expected utility, as a function of the decision variable \\(\\omega\\) (portfolio weight). It is the moment-generating function of the log-normal. We can plug-in the mean and variance of \\(W\\) to get the expected utility \\[\nU(\\omega) = \\exp\\left\\{\\gamma \\left\\{(1-\\omega)r_f + \\omega\\mu\\right\\}\\right\\} \\exp \\left \\{ \\dfrac{1}{2}\\gamma^2\\omega^2\\sigma^2 \\right \\}.\n\\] The optimal Kelly-Brieman-Thorpe-Merton value of \\(\\omega\\) is given by \\[\n\\omega^* = (\\mu - r_f)/(\\sigma^2\\gamma).\n\\]\nWithin the GBC framework, it is easy to add learning or uncertainty on top of \\(\\sigma^2\\) and have a joint posterior distribution \\(p(\\mu, \\sigma^2 \\mid R)\\).\nNow we reorder the integral in terms of quantiles of the utility function. We assume utility is the random variable and re-order the sum as the expected value of \\(U\\) \\[\nE(U(W)) = \\int_{0}^{1}F_{U(W)}^{-1}(\\tau)d\\tau\n\\] Hence, if we can approximate the inverse of the CDF of \\(U(W)\\) with a quantile NN, we can approximate the expected utility and optimize over \\(\\omega\\).\nThe stochastic utility is modeled with a deep neural network, and we write \\[\nZ = U(W) \\approx F, \\quad W  = U^{-1}(F)\n\\] We can do optimization by doing the grid search for \\(\\omega\\).\nThe decision variable \\(\\omega\\) affects the distribution of the returns. The utility only depends on the returns \\(W\\). Our quantile neural network solution is given by the following algorithm:\n\nSimulate log-returns \\(W^{(i)}\\mid \\omega^{(i)} \\sim N((1-\\omega^{(i)})r_f + \\omega^{(i)}\\mu,\\sigma^2\\omega^{(i)}2)\\)\nCalculate corresponding utilities \\(Z^{(i)} = U(W^{(i)})\\)\nLearn \\(F_{Z_{\\omega}}^{-1}\\) with a quantile NN\nFind the optimal portfolio weight \\(\\omega^\\star\\) via \\[\nE(Z_{\\omega}) = \\sum_{i=1}^{N}F^{-1}_{Z_{\\omega}}(u_i) \\rightarrow \\underset{\\omega}{\\mathrm{maximize}}\n\\]\n\n\nEmpirical Example\nConsider \\(\\omega \\in (0,1)\\), \\(r_f = 0.05\\), \\(\\mu=0.1\\), \\(\\sigma=0.25\\), \\(\\gamma = 2\\). We have the closed-form fractional Kelly criterion solution \\[\n\\omega^* = \\frac{1}{\\gamma}   \\frac{ \\mu - r_f}{ \\sigma^2} = \\frac{1}{2} \\frac{ 0.1 - 0.05 }{ 0.25^2 } = 0.40\n\\] We can simulate the expected utility and compare with the closed-form solution.\n\n\n\n\n\n\n\n\n\n\n\n(a) Quantile function of utility \\(Z = -\\exp(-0.1W)\\) versus \\(\\tau\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) Expected utility integral as function of portfolio weight \\(\\omega\\)\n\n\n\n\n\n\n\nFigure 21.4: Portfolio optimization via quantile neural networks. Left panel shows plot of sorted values of \\(\\tau\\) versus sorted values of random draws from \\(-\\exp(-\\omega W)\\) for \\(\\omega=0.1\\). Right panel shows values of integral of \\(Z\\) with respect to \\(\\tau\\) versus corresponding values of \\(\\omega\\). The integral was calculated using trapezoid rule. The red vertical line corresponds to \\(\\omega = 0.4\\), which is the analytical optimum.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-forecasting",
    "href": "21-qnn.html#sec-qnn-forecasting",
    "title": "21  Quantile Neural Networks",
    "section": "21.8 Supply Chain Forecasting at Scale",
    "text": "21.8 Supply Chain Forecasting at Scale\nHaving developed the theory and core implementations, we now turn to a major industrial application: probabilistic forecasting for inventory management. This case study illustrates how quantile neural networks scale to millions of products while providing the uncertainty quantification essential for optimal decision-making.\nAmazon, Walmart, and other large retailers face a fundamental challenge: for millions of products across thousands of warehouses, how much inventory should be kept in stock? Too little inventory leads to stockouts and lost sales; too much ties up capital and risks obsolescence. The optimal inventory level depends critically on future demand, which is inherently uncertain.\nTraditional forecasting provides point estimates—a single expected demand value. But optimal inventory decisions require understanding the entire demand distribution:\n\nSafety stock (\\(P_{10}\\)): Conservative estimate—ensures 90% of demand scenarios are covered, preventing stockouts\nBase stock (\\(P_{50}\\)): Median demand—balances inventory holding costs against stockout risk\nCapacity planning (\\(P_{90}\\)): Optimistic scenario—ensures warehouse and logistics capacity for high-demand periods\n\nThe asymmetry matters enormously. For a highly profitable product with long lead times, the cost of stockouts (lost sales, customer dissatisfaction) far exceeds the cost of overstock. The optimal policy may target \\(P_{70}\\) or \\(P_{80}\\). For perishable goods or fast-moving consumer products with low margins, the reverse holds—target \\(P_{30}\\) or \\(P_{40}\\) to minimize waste.\nAmazon developed DeepAR (Salinas, Flunkert, and Gasthaus 2019), a deep learning approach for probabilistic time series forecasting. DeepAR uses recurrent neural networks (specifically LSTMs) to model temporal dependencies, but its key innovation is forecasting entire probability distributions rather than point estimates.\nThe model predicts parameters of a parametric distribution (e.g., mean and variance of a Gaussian or negative binomial). To obtain quantiles, one samples from this learned distribution. However, parametric assumptions can be restrictive. Amazon later extended the approach to directly forecast quantiles using the check loss functions we developed in Section 21.1.\nThe quantile approach has several advantages for demand forecasting:\n\nRobustness: Demand data often contains outliers (promotional events, viral products). Quantile regression is robust to these.\nIntermittent demand: Many products have sparse, intermittent demand (many zeros). Parametric distributions struggle; quantile methods handle this naturally.\nAsymmetric costs: Different quantiles inform different decisions. The \\(P_{10}\\) quantile is more relevant for safety stock than the mean.\nModel flexibility: Neural quantile regression makes no distributional assumptions beyond smoothness.\n\nWhile production systems at Amazon and similar retailers deploy deep architectures with LSTMs, attention mechanisms, and learned embeddings, the core principles of quantile forecasting remain the same regardless of model complexity. In the following example, we illustrate these principles using interpretable linear quantile regression before discussing how to scale to neural network implementations.\n\nDemand Forecasting Setup\nConsider forecasting demand \\(y_t\\) for a product given the features in Table 21.2:\n\n\n\nTable 21.2: Features for demand forecasting\n\n\n\n\n\n\n\n\n\n\nFeature Category\nExamples\nPurpose\n\n\n\n\nTemporal features\nDay of week, month, holiday indicators, days since launch\nCapture seasonal patterns and product lifecycle\n\n\nLagged demand\n\\(y_{t-1}, y_{t-7}, y_{t-28}\\) (yesterday, last week, last month)\nIncorporate recent demand history and autocorrelation\n\n\nProduct features\nCategory, price, promotional flags\nAccount for product-specific characteristics\n\n\nExternal covariates\nWeather, events, competitor prices\nInclude external factors affecting demand\n\n\n\n\n\n\nThe model predicts conditional quantiles: \\[\nq_\\tau(t) = F^{-1}_{Y_t | X_t}(\\tau)\n\\]\nwhere \\(X_t\\) represents all available features at time \\(t\\). Training data consists of historical demand \\((y_t, x_t)\\) for \\(t = 1, \\ldots, T\\) across many products.\nTraditional quantile regression assumes linear relationships between features and quantiles. For demand forecasting, this assumption is inadequate. Holiday effects interact with day-of-week patterns in complex, non-linear ways. Products exhibit substitution and complementarity effects that vary across categories and time periods. Promotional dynamics create non-linear price elasticity that depends on product category, timing, and competitive environment. Moreover, products with similar characteristics often exhibit similar demand patterns, suggesting opportunities for transfer learning across the product catalog.\nQuantile neural networks address these limitations through their flexible architecture. They can learn shared embeddings for product categories, allowing the model to discover latent similarities between products. Recurrent or attention layers capture complex temporal patterns that extend beyond simple autoregressive relationships. The network automatically models interactions between features without requiring manual specification of interaction terms. Perhaps most importantly, the shared representations enable transfer learning from data-rich products to new products with limited historical data, a critical capability for retailers constantly introducing new items.\n\n\nImplementation Strategy\nFor a retailer with millions of SKUs (stock-keeping units), computational efficiency is critical. The architecture typically involves:\n\nEmbedding layers: Map categorical variables (product ID, category, location) to dense vectors\nTemporal encoder: LSTM or Transformer to process time series history\nFeature fusion: Combine embeddings with numerical features\nQuantile head: Final layer produces \\(\\hat{q}_\\tau(t)\\) for input \\(\\tau\\), using cosine embedding\n\nTraining uses mini-batches sampling randomly across products and time periods, with the combined quantile + MSE loss we discussed. The model is trained to predict multiple quantiles simultaneously (\\(\\tau \\in \\{0.1, 0.2, \\ldots, 0.9\\}\\)).\nThe following diagram illustrates the neural network architecture for demand forecasting:\n\n\n\nNeural network architecture for quantile demand forecasting. Categorical features are embedded, temporal history is encoded, and all features are fused before the quantile head produces conditional quantile predictions.\n\n\nAt inference time, for a given product and time period, the model outputs all quantiles instantly, enabling inventory optimization algorithms to compute optimal stock levels.\nWe now demonstrate demand forecasting using quantile regression to predict the entire demand distribution. The simulation generates realistic demand data for three products over two years, incorporating multiple real-world patterns. Each product exhibits a positive trend reflecting business growth, weekly seasonality capturing day-of-week effects, monthly seasonality for longer-term patterns, and random promotional events occurring on approximately 10% of days that boost demand by 30-50 units. The demand also features heteroskedastic noise where variance increases proportionally with demand level, mimicking the greater uncertainty in forecasting high-demand periods.\nThe model uses quantile regression with features including product identifier, time trend, promotional indicator, day of week, day of month, and three lagged demand values (1-day, 7-day, and 30-day lags) to capture short-term momentum, weekly patterns, and monthly cycles. Five quantile models are trained simultaneously at \\(\\tau \\in \\{0.1, 0.3, 0.5, 0.7, 0.9\\}\\), each estimating a different point in the conditional demand distribution. The training set spans 640 days, with the final 90 days held out for evaluation.\n\n\nDemand forecasting with quantile neural networks\n# Load required libraries\nlibrary(quantreg) # For quantile regression\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate synthetic demand data for multiple products\ngenerate_demand_data &lt;- function(n_days = 365 * 2, n_products = 5) {\n    days &lt;- 1:n_days\n\n    # Create data frame\n    data_list &lt;- lapply(1:n_products, function(p) {\n        # Base parameters varying by product\n        base_demand &lt;- 50 + p * 20\n        trend &lt;- 0.05 * p\n\n        # Seasonal components\n        weekly_pattern &lt;- 10 * sin(2 * pi * days / 7 + p)\n        monthly_pattern &lt;- 15 * sin(2 * pi * days / 30.5 + p * 0.5)\n\n        # Promotional effect (random promotions)\n        promo &lt;- rbinom(n_days, 1, 0.1) # 10% of days have promotions\n        promo_effect &lt;- promo * (30 + 20 * p)\n\n        # Trend + seasonality + promotions + noise\n        mu &lt;- base_demand + trend * days + weekly_pattern + monthly_pattern + promo_effect\n\n        # Heteroskedastic noise (variance increases with demand)\n        sigma &lt;- 5 + 0.1 * mu\n        demand &lt;- pmax(0, rnorm(n_days, mu, sigma)) # Demand can't be negative\n\n        data.frame(\n            product = paste0(\"Product_\", p),\n            day = days,\n            demand = demand,\n            promo = promo,\n            day_of_week = (days %% 7) + 1,\n            day_of_month = ((days - 1) %% 30) + 1\n        )\n    })\n\n    do.call(rbind, data_list)\n}\n\n# Generate data\ndemand_data &lt;- generate_demand_data(n_days = 365 * 2, n_products = 3)\n\n# Create lagged features\ndemand_data &lt;- demand_data %&gt;%\n    group_by(product) %&gt;%\n    arrange(day) %&gt;%\n    mutate(\n        lag_1 = lag(demand, 1, default = NA),\n        lag_7 = lag(demand, 7, default = NA),\n        lag_30 = lag(demand, 30, default = NA)\n    ) %&gt;%\n    ungroup() %&gt;%\n    filter(!is.na(lag_30)) # Remove rows with NA lags\n\n# Train/test split\ntrain_days &lt;- max(demand_data$day) - 90\ntrain_data &lt;- demand_data %&gt;% filter(day &lt;= train_days)\ntest_data &lt;- demand_data %&gt;% filter(day &gt; train_days)\n\n# Quantile regression for multiple quantiles\nquantiles &lt;- c(0.1, 0.3, 0.5, 0.7, 0.9)\nmodels &lt;- list()\n\n# cat(\"Training quantile regression models...\\n\")\nfor (q in quantiles) {\n    # cat(sprintf(\"  Quantile %.2f...\\n\", q))\n    models[[as.character(q)]] &lt;- rq(\n        demand ~ product + day + promo + day_of_week + day_of_month +\n            lag_1 + lag_7 + lag_30,\n        tau = q,\n        data = train_data\n    )\n}\n\n# Generate predictions for test set\npredictions &lt;- test_data\nfor (q in quantiles) {\n    col_name &lt;- paste0(\"q_\", gsub(\"\\\\.\", \"\", sprintf(\"%.2f\", q)))\n    predictions[[col_name]] &lt;- predict(models[[as.character(q)]], newdata = test_data)\n}\n\n# Calculate prediction intervals and coverage\npredictions &lt;- predictions %&gt;%\n    mutate(\n        in_80_interval = (demand &gt;= q_010 & demand &lt;= q_090),\n        in_60_interval = (demand &gt;= q_030 & demand &lt;= q_070)\n    )\n\n# Calculate metrics\ncoverage_80 &lt;- mean(predictions$in_80_interval, na.rm = TRUE)\ncoverage_60 &lt;- mean(predictions$in_60_interval, na.rm = TRUE)\nmae_median &lt;- mean(abs(predictions$demand - predictions$q_050), na.rm = TRUE)\n\n# cat(sprintf(\"\\nForecast Performance:\\n\"))\n# cat(sprintf(\"  80%% interval coverage: %.1f%% (target: 80%%)\\n\", coverage_80 * 100))\n# cat(sprintf(\"  60%% interval coverage: %.1f%% (target: 60%%)\\n\", coverage_60 * 100))\n# cat(sprintf(\"  Median absolute error: %.2f units\\n\", mae_median))\n\n# Visualization\npar(mfrow = c(2, 1), mar = c(4, 4, 3, 1))\n\n# Plot 1: Training data for one product\nproduct_1_train &lt;- train_data %&gt;% filter(product == \"Product_1\")\nplot(product_1_train$day, product_1_train$demand,\n    type = \"l\", lwd = 1.5, col = \"darkblue\",\n    xlab = \"Day\", ylab = \"Demand\",\n    main = \"Historical Demand (Product 1)\"\n)\npoints(product_1_train$day[product_1_train$promo == 1],\n    product_1_train$demand[product_1_train$promo == 1],\n    col = \"red\", pch = 16, cex = 0.5\n)\nlegend(\"topleft\",\n    legend = c(\"Demand\", \"Promotion\"),\n    col = c(\"darkblue\", \"red\"), lwd = c(2, NA), pch = c(NA, 16)\n)\ngrid()\n\n# Plot 2: Forecasts with quantiles (fan chart)\nproduct_1_test &lt;- predictions %&gt;% filter(product == \"Product_1\")\nplot(product_1_test$day, product_1_test$demand,\n    type = \"l\", lwd = 2, col = \"black\",\n    xlab = \"Day\", ylab = \"Demand\",\n    main = \"Demand Forecasts with Quantiles (Product 1)\",\n    ylim = range(c(product_1_test$q_010, product_1_test$q_090, product_1_test$demand))\n)\n\n# Add prediction intervals\npolygon(c(product_1_test$day, rev(product_1_test$day)),\n    c(product_1_test$q_010, rev(product_1_test$q_090)),\n    col = rgb(0.7, 0.7, 1, 0.3), border = NA\n)\npolygon(c(product_1_test$day, rev(product_1_test$day)),\n    c(product_1_test$q_030, rev(product_1_test$q_070)),\n    col = rgb(0.5, 0.5, 1, 0.4), border = NA\n)\n\n# Add quantile lines\nlines(product_1_test$day, product_1_test$q_010, col = \"blue\", lty = 2, lwd = 1.5)\nlines(product_1_test$day, product_1_test$q_050, col = \"darkblue\", lwd = 2)\nlines(product_1_test$day, product_1_test$q_090, col = \"blue\", lty = 2, lwd = 1.5)\n\n# Actual demand\nlines(product_1_test$day, product_1_test$demand, col = \"black\", lwd = 2)\n\nlegend(\"topleft\",\n    legend = c(\"Actual\", \"P50 (Median)\", \"P10-P90 (80%)\", \"P30-P70 (60%)\"),\n    col = c(\"black\", \"darkblue\", rgb(0.7, 0.7, 1), rgb(0.5, 0.5, 1)),\n    lwd = c(2, 2, 8, 8), lty = c(1, 1, 1, 1)\n)\ngrid()\n\n\n\n\n\nDemand forecasting with quantile neural networks. Top: Historical demand with trend and seasonality. Bottom: Out-of-sample forecasts showing P10, P50, P90 quantiles (fan chart).\n\n\n\n\nThe quantile regression model demonstrates strong calibration on the held-out test period. The 80% prediction interval (P10 to P90) achieves 79.6% empirical coverage, meaning actual demand falls within this range about 80% of the time, closely matching the theoretical target. The 60% interval (P30 to P70) achieves 33.7% coverage, somewhat below the 60% target, suggesting the model may be overestimating uncertainty in the middle of the distribution. The median forecast (P50) produces a mean absolute error of 21.94 units, providing reasonably accurate central tendency estimates while the full quantile spectrum captures demand uncertainty.\nThe figure visualizes both historical patterns and probabilistic forecasts. The top panel displays historical demand for Product 1 from the training period, revealing the complex interplay of trend, seasonality, and promotional effects (marked in red). Demand oscillates around an upward trend with clear weekly and monthly cycles, punctuated by sharp spikes during promotional periods.\nThe bottom panel presents the out-of-sample forecasts as a fan chart, a standard visualization for probabilistic predictions. The black line shows actual realized demand, while the dark blue line represents the median forecast (P50). The shaded regions illustrate prediction intervals: the lighter blue band spans P10 to P90 (80% interval), while the darker blue band covers P30 to P70 (60% interval). This visualization immediately conveys forecast uncertainty, with wider intervals during high-demand periods reflecting heteroskedastic noise. The median forecast tracks actual demand reasonably well, while the intervals appropriately capture most realizations, demonstrating the model’s ability to quantify forecast risk rather than providing only point predictions.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-rl",
    "href": "21-qnn.html#sec-qnn-rl",
    "title": "21  Quantile Neural Networks",
    "section": "21.9 Distributional Reinforcement Learning",
    "text": "21.9 Distributional Reinforcement Learning\nQuantile neural networks have also revolutionized reinforcement learning (RL). Recall from Chapter 9 that RL agents learn policies \\(\\pi\\) that maximize expected cumulative reward. Traditional RL algorithms like Q-learning estimate the expected value function \\(Q^\\pi(s,a) = E[R | s, a, \\pi]\\)—the expected return from taking action \\(a\\) in state \\(s\\) and following policy \\(\\pi\\) thereafter.\nDistributional reinforcement learning (Bellemare, Dabney, and Munos 2017) extends this by learning the entire distribution of returns rather than just the expectation. Using quantile neural networks, agents learn \\(F^{-1}_{R|s,a}(\\tau)\\), the quantile function of returns.\nLearning return distributions provides several advantages:\n\nRisk-sensitive policies: Different quantiles inform different behaviors. Conservative agents might maximize \\(P_{10}\\) (avoid worst-case scenarios), while risk-seeking agents target \\(P_{90}\\) (optimize for best-case outcomes).\nBetter learning signals: The Bellman operator naturally contracts in Wasserstein distance when formulated for quantiles, leading to more stable learning (Dabney et al. 2018).\nUncertainty quantification: Understanding return variance helps agents explore intelligently—high uncertainty indicates potential for learning.\n\nDabney et al. (2018) use quantile neural networks for distributional Q-learning. The key insight is that expectations can be computed as integrals over quantiles (the Lorenz curve identity): \\[\nE[R] = \\int_{-\\infty}^{\\infty} r dF(r) = \\int_0^1 F^{-1}(u) du\n\\]\nThe distributional RL algorithm finds the optimal policy: \\[\n\\pi^\\star(s) = \\arg\\max_a \\int_0^1 F^{-1}_{R|s,a}(\\tau) d\\tau = \\arg\\max_a E_{Z \\sim z(s, a)}[Z]\n\\]\nThe network is trained using the quantile loss \\(\\rho_\\tau\\) we developed, and Q-learning updates can be applied since the quantile projection operator preserves the contraction property of the Bellman operator. This approach has achieved state-of-the-art performance on Atari games and robotic control tasks.\nConnections to dual utility theory (Yaari 1987) suggest that distributional RL naturally incorporates risk preferences—agents can be trained to maximize any utility functional, not just expectations.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "21-qnn.html#sec-qnn-discussion",
    "href": "21-qnn.html#sec-qnn-discussion",
    "title": "21  Quantile Neural Networks",
    "section": "21.10 Discussion and Summary",
    "text": "21.10 Discussion and Summary\nAs Keynes observed, it is better to be roughly right than precisely wrong. Quantile methods embrace this philosophy: they provide the distributional information needed for sound decisions without claiming to know the complete probability model. In an era of increasingly complex data and high-stakes applications, this combination of flexibility, robustness, and decision-focus makes quantile neural networks an essential tool for the modern data scientist.\nQuantile neural networks represent a convergence of classical statistical theory (quantile regression, robust statistics) with modern machine learning (deep learning, representation learning). By focusing on the quantities we actually need—quantiles for decision-making—rather than intermediate densities, these methods offer a pragmatic and powerful approach to uncertainty quantification.\nThe chapter’s central contributions rest on several foundational insights. The quantile-expectation identity \\(E[U] = \\int_0^1 F^{-1}_U(\\tau) d\\tau\\) provides the fundamental insight that enables direct learning of expected utilities without intermediate density estimation. This mathematical relationship connects quantile functions directly to the expectations needed for decision-making. The check loss \\(\\rho_\\tau(u) = u(\\tau - I(u &lt; 0))\\) emerges naturally from minimizing Wasserstein distance and handles asymmetric costs elegantly, providing a principled training objective. Neural architectures employing cosine embeddings for \\(\\tau\\) leverage Fourier approximation theory to provide efficient universal approximators with \\(O(N^{-1/2})\\) convergence rates, making quantile learning computationally tractable even in high dimensions. These methods find application across diverse domains: portfolio optimization under parameter uncertainty in finance (Section 21.7), demand forecasting for inventory management in supply chains (Section 21.8), posterior quantile learning via Wang distortions in Bayesian inference (Section 21.4), and distributional Q-learning for risk-sensitive policies in reinforcement learning (Section 21.9).\nQuantile neural networks are particularly well-suited for decision problems where expectations or quantiles are needed for optimization rather than full densities. They excel in likelihood-free settings with complex simulators where \\(p(y|\\theta)\\) is unavailable, making them valuable for models that can simulate but not evaluate likelihoods. Their robustness to outliers and heavy tails makes them attractive when data quality is uncertain or distributions exhibit non-Gaussian behavior. The framework naturally handles asymmetric costs where different quantiles drive different decisions, such as the asymmetric penalties for stockouts versus overstock in inventory management. In high-dimensional settings where density estimation becomes intractable, quantile regression remains computationally feasible, providing a practical alternative. Finally, when real-time requirements demand fast inference, neural networks provide instant quantile predictions without the computational overhead of sampling-based methods.\n\n\n\n\nArjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017. “Wasserstein Generative Adversarial Networks.” Proceedings of the 34th International Conference on Machine Learning, 214–23.\n\n\nBarron, Andrew R. 1993. “Universal Approximation Bounds for Superpositions of a Sigmoidal Function.” IEEE Transactions on Information Theory 39 (3): 930–45.\n\n\nBellemare, Marc G., Will Dabney, and Rémi Munos. 2017. “A Distributional Perspective on Reinforcement Learning.” Proceedings of the 34th International Conference on Machine Learning, 449–58.\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model With ‘Gaussian’ Regressor Variables.” In Selected Works of David Brillinger, edited by Peter Guttorp and David Brillinger, 589–606. Selected Works in Probability and Statistics. New York, NY: Springer.\n\n\nCannon, Alex J. 2018. “Non-Crossing Nonlinear Regression Quantiles by Monotone Composite Quantile Regression Neural Network, with Application to Rainfall Extremes.” Stochastic Environmental Research and Risk Assessment 32 (11): 3207–25.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010. “Quantile and Probability Curves Without Crossing.” Econometrica : Journal of the Econometric Society 78 (3): 1093–1125.\n\n\nDabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018. “Implicit Quantile Networks for Distributional Reinforcement Learning.” arXiv. https://arxiv.org/abs/1806.06923.\n\n\nKallenberg, Olav. 1997. Foundations of Modern Probability. 2nd ed. edition. Springer.\n\n\nLevina, Elizaveta, and Peter Bickel. 2001. “The Earth Mover’s Distance Is the Mallows Distance: Some Insights from Statistics.” In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, 2:251–56. IEEE.\n\n\nParzen, Emanuel. 2004. “Quantile Probability and Statistical Data Modeling.” Statistical Science 19 (4): 652–62.\n\n\nPolson, Nicholas G., and James G. Scott. 2016. “Mixtures, Envelopes and Hierarchical Duality.” Journal of the Royal Statistical Society Series B: Statistical Methodology 78 (4): 701–27.\n\n\nPolson, Nicholas G., and Vadim Sokolov. 2023. “Generative AI for Bayesian Computation.” https://arxiv.org/abs/2305.14972.\n\n\nPolson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024. “Generative Bayesian Computation for Maximum Expected Utility.” Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies 26 (12): 1076.\n\n\nSalinas, David, Valentin Flunkert, and Jan Gasthaus. 2019. “DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks.” arXiv:1704.04110 [Cs, Stat], February. https://arxiv.org/abs/1704.04110.\n\n\nWang, Shaun. 1996. “Premium Calculation by Transforming the Layer Premium Density.” ASTIN Bulletin 26 (1): 71–92.\n\n\nYaari, Menahem E. 1987. “The Dual Theory of Choice Under Risk.” Econometrica : Journal of the Econometric Society 55 (1): 95–115.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Quantile Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html",
    "href": "22-cnn.html",
    "title": "22  Convolutional Neural Networks",
    "section": "",
    "text": "22.1 The MNIST Dataset\nThe foundational architecture for convolutional neural networks emerged in 1980, when Kunihiko Fukushima introduced the Neocognitron (Fukushima 1980)—a hierarchical neural network inspired by Hubel and Wiesel’s discoveries about simple and complex cells in the mammalian visual cortex. The Neocognitron was the first architecture to use convolutional layers for feature extraction and pooling layers for translation invariance, successfully recognizing handwritten Japanese characters through unsupervised learning. In 1988, Wei Zhang and colleagues applied backpropagation to train a simplified Neocognitron for alphabet recognition (Zhang et al. 1988), demonstrating that supervised learning could be effectively combined with the convolutional architecture. This combination of Fukushima’s architectural insights with gradient-based supervised learning became the foundation for modern CNNs, which now power everything from facial recognition on smartphones to autonomous vehicle perception systems.\nThis chapter introduces CNNs by first demonstrating the limitations of fully connected networks on image data, then building intuition for how convolutions extract local features. We will work with the MNIST dataset—the same handwritten digit recognition.\nThe MNIST dataset contains grayscale images of handwritten digits, each 28×28 pixels. It serves as the “Hello World” of computer vision—a standard benchmark that is simple enough to train quickly but complex enough to demonstrate the mechanics of image classification. The original dataset includes 60,000 training images and 10,000 test images. For simplicity, we will use only the test dataset in our examples. Each image is labeled with the digit it represents (0-9), making this a 10-class classification problem.\nImage data is represented as a matrix of pixel values. A grayscale image of size 28×28 pixels is a matrix of shape (28, 28), where each entry contains an intensity value from 0 (black) to 255 (white). Color images add a third dimension for the red, green, and blue channels, resulting in shape (28, 28, 3). Figure 22.1 shows some sample MNIST digits.\nimport gzip\nimport struct\nimport array\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef parse_labels(filename):\n    with gzip.open(filename, \"rb\") as fh:\n        _ = struct.unpack(\"&gt;II\", fh.read(8))\n        return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\ndef parse_images(filename):\n    with gzip.open(filename, \"rb\") as fh:\n        _, num_data, rows, cols = struct.unpack(\"&gt;IIII\", fh.read(16))\n        return np.array(array.array(\"B\", fh.read()),dtype=np.uint8).reshape(num_data, rows, cols)\nx = parse_images(\"../data/t10k-images-idx3-ubyte.gz\")\ny = parse_labels(\"../data/t10k-labels-idx1-ubyte.gz\")\nplt.imshow(x[0,:,:], cmap='gray'); plt.title(f'Label: {y[0]}'); plt.show()\nplt.imshow(x[1,:,:], cmap='gray'); plt.title(f'Label: {y[1]}'); plt.show()\nplt.imshow(x[2,:,:], cmap='gray'); plt.title(f'Label: {y[2]}'); plt.show()\nplt.imshow(x[3,:,:], cmap='gray'); plt.title(f'Label: {y[3]}'); plt.show()",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html#the-mnist-dataset",
    "href": "22-cnn.html#the-mnist-dataset",
    "title": "22  Convolutional Neural Networks",
    "section": "",
    "text": "Figure 22.1: Sample MNIST digits",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html#fully-connected-approach",
    "href": "22-cnn.html#fully-connected-approach",
    "title": "22  Convolutional Neural Networks",
    "section": "22.2 Fully Connected Approach",
    "text": "22.2 Fully Connected Approach\nA natural starting point for image classification is a fully connected neural network, which treats the image as a flat vector of independent pixels. The input to the model is a 28×28 matrix, which is reshaped into a vector of size 784. The output is a vector of size 10, representing the probability of each class. We split the 10,000 images using 80% for training and 20% for validation, shuffling before splitting to avoid bias.\nWe use jax, a library for numerical computing similar to numpy but designed for accelerators like GPUs and TPUs. Training a neural network requires two core computational capabilities: automatic differentiation to compute gradients, and efficient linear algebra—particularly matrix multiplication—to propagate signals through layers. jax provides both, along with a jit function for compiling operations to run efficiently on accelerators.\n\nimport numpy.random as npr\ndef flatten(x):\n    \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n    return np.reshape(x, (x.shape[0], -1))\n\ndef one_hot(x, k, dtype=np.float32):\n    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n    return np.array(x[:, None] == np.arange(k), dtype)\nrng = npr.RandomState(0)\nn = x.shape[0]\nperm = rng.permutation(n)\ntraini = perm[0:8000]\ntesti = perm[8000:n]\ntrain_images = flatten(x[traini,:,:])/ np.float32(255.)\ntest_images = flatten(x[testi,:,:])/ np.float32(255.)\ntrain_labels  = one_hot(y[traini],10)\ntest_labels =  one_hot(y[testi],10)\n\nThe model is trained by minimizing the cross-entropy loss and we use accuracy to evaluate the model.\n\ndef loss(params, batch):\n  inputs, targets = batch\n  preds = predict(params, inputs)\n  return -jnp.mean(jnp.sum(preds * targets, axis=1))\n\ndef accuracy(params, batch):\n  inputs, targets = batch\n  target_class = jnp.argmax(targets, axis=1)\n  predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n  return jnp.mean(predicted_class == target_class)\n\nFinally, we implement the feed-forward network with \\(\\tanh\\) activation. The last layer is a linear layer with the \\(z_i - \\ln \\left(\\sum_j e^{z_j}\\right)\\) function applied component-wise. Thus, our predict function simply returns the logarithm of the probability of each class. Typically we would use the softmax function to compute the probability of each class. Given a vector of logits \\(z\\), the softmax function is defined as follows: \\[\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}\n\\] However, computation of the exponential of large numbers can lead to numerical instability (overflow). Instead, we work with the logarithm of the softmax function, which is more numerically stable: \\[\n\\ln\\sigma(z)_i = z_i - \\ln\\sum_{j=1}^{K}e^{z_j}\n\\] This is exactly what we need to compute the cross-entropy loss.\n\ndef predict(params, inputs):\n    activations = inputs\n    for w, b in params[:-1]:\n        outputs = jnp.dot(activations, w) + b\n        activations = jnp.tanh(outputs)\n\n    final_w, final_b = params[-1]\n    logits = jnp.dot(activations, final_w) + final_b\n    return logits - logsumexp(logits, axis=1, keepdims=True)\n\n\nfrom jax import jit, grad\nimport jax.numpy as jnp\nfrom jax.scipy.special import logsumexp\nlayer_sizes = [784, 1024, 1024, 10]\nparam_scale = 0.1\nstep_size = 0.001\nnum_epochs = 10\nbatch_size = 128\n\nnum_train = train_images.shape[0]\nnum_complete_batches, leftover = divmod(num_train, batch_size)\nnum_batches = num_complete_batches + bool(leftover)\n\ndef init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):\n  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]\n\ndef data_stream():\n    while True:\n        perm = rng.permutation(num_train)\n        for i in range(num_batches):\n            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n            yield train_images[batch_idx], train_labels[batch_idx]\nbatches = data_stream()\n\n@jit\ndef update(params, batch):\n    grads = grad(loss)(params, batch)\n    return [(w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip(params, grads)]\n\nparams = init_random_params(param_scale, layer_sizes)\nlog = np.empty((2,num_epochs))\nfor epoch in range(num_epochs):\n    for _ in range(num_batches):\n        params = update(params, next(batches))\n    train_acc = accuracy(params, (train_images, train_labels))\n    test_acc = accuracy(params, (test_images, test_labels))\n    log[:,epoch] = [train_acc,test_acc,]\nprint(f\"Training set accuracy {train_acc}\")\nprint(f\"Test set accuracy {test_acc}\")\n\nplt.plot(range(num_epochs),log[0,:]);\nplt.plot(range(num_epochs),log[1,:]);\n\nThe final architecture is shown in Figure 22.2 and achieves an accuracy of 77% on the test set and 80% on the training set.\n\n\n\n\n\n\ngraph LR\n    subgraph Input[\"Input Layer\"]\n        I[784 nodes&lt;br/&gt;28×28 flattened image]\n    end\n    \n    subgraph Hidden1[\"Hidden Layer 1\"]\n        H1[1024 nodes&lt;br/&gt;tanh activation]\n    end\n    \n    subgraph Hidden2[\"Hidden Layer 2\"]\n        H2[1024 nodes&lt;br/&gt;tanh activation]\n    end\n    \n    subgraph Output[\"Output Layer\"]\n        O[10 nodes&lt;br/&gt;log-softmax]\n    end\n    \n    I --&gt;|W1: 784×1024&lt;br/&gt;b1: 1024| H1\n    H1 --&gt;|W2: 1024×1024&lt;br/&gt;b2: 1024| H2\n    H2 --&gt;|W3: 1024×10&lt;br/&gt;b3: 10| O\n    \n    O --&gt; P[Predicted digit&lt;br/&gt;0-9]\n    \n    style Input fill:#e1f5ff\n    style Hidden1 fill:#fff4e1\n    style Hidden2 fill:#fff4e1\n    style Output fill:#e8f5e9\n    style P fill:#f3e5f5\n\n\n\n\nFigure 22.2: Fully Connected MNIST\n\n\n\n\n\nFigure 22.3 shows the accuracy of the fully connected network on the test and training sets from 0 to 10 epochs.\n\n\n\n\n\n\nFigure 22.3: Accuracy of the fully connected network on the test and training sets\n\n\n\nWe can see that even simple fully connected network achieves reasonable accuracy, but it has fundamental limitations. It treats each pixel independently, ignoring the spatial structure of the image—the fact that neighboring pixels are related. A fully connected layer connecting a 28$$28 image to 1024 hidden units requires over 800,000 parameters, most of which are redundant.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html#convolutions",
    "href": "22-cnn.html#convolutions",
    "title": "22  Convolutional Neural Networks",
    "section": "22.3 Convolutions",
    "text": "22.3 Convolutions\nA fully connected layer connecting a 28$$28 image to 1024 hidden units requires over 800,000 parameters, most of which are redundant. Convolutional neural networks address these limitations by exploiting two key properties of images:\n\nLocal connectivity: Useful features (edges, textures, shapes) are local—they depend on small neighborhoods of pixels, not the entire image.\nTranslation invariance: A feature detector useful in one part of an image is likely useful in other parts.\n\n\nKey Concepts\nA filter (also called a kernel) is a small matrix of learned weights—typically 3×3 or 5×5—that extracts local features from an image by computing a weighted sum of pixel values within a sliding window. When the filter’s pattern aligns with a pattern in the image, the dot product produces a high activation; when they don’t match, the activation is low.\n\nKernel size: The dimensions of the filter (e.g., 3×3). Odd sizes are preferred so the filter has a well-defined center.\nReceptive field: The region of the input image that influences a single output value.\nStride: The number of pixels by which the filter shifts at each step. A stride of 1 moves pixel-by-pixel; a stride of 2 skips every other position, reducing the output size.\nPadding: Adding zeros around the border of the input to control the output size. “Same” padding preserves the input dimensions; “valid” padding uses no padding, shrinking the output.\n\nWhile mathematically identical to kernel smoothing in statistics, convolutions in deep learning differ in one critical aspect: the kernel weights are learned from data rather than predefined.\nLet us look at a one-dimensional example. Suppose we have a one-dimensional input signal \\(x\\) and a one-dimensional filter \\(w\\). The convolution of \\(x\\) and \\(w\\) is defined as follows: \\[\n(x \\ast w)(t) = \\sum_{i=0}^{h}x(t+i)w(i),\n\\] where \\(h\\) is the size of the filter. The convolution operation is used to filter the input signal.\nTo illustrate this, consider a simple example where we apply a moving average filter to a noisy sinusoidal signal. We generate a sine wave over the interval \\([0, 10]\\) and add Gaussian noise to simulate realistic measurement errors. The filter uses a uniform window of size 10, where each weight equals \\(1/10\\), effectively computing the local average of the signal. When we convolve this filter with the noisy signal, each output point becomes the weighted average of its neighboring input points, which smooths out the random fluctuations while preserving the underlying periodic structure. Figure 22.4 shows the original noisy signal in light gray and the smoothed result in black, demonstrating how convolution effectively extracts the signal from noise by exploiting local spatial structure.\n\nimport matplotlib.pyplot as plt\n\nfrom jax import random\nimport jax.numpy as jnp\nimport numpy as np\n\nkey = random.PRNGKey(1701)\n\nx = jnp.linspace(0, 10, 500)\ny = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\n\nwindow = jnp.ones(10) / 10\ny_smooth = jnp.convolve(y, window, mode='same')\n\nplt.plot(x, y, 'lightgray')\nplt.plot(x, y_smooth, 'black');\n\n\n\n\n\n\n\nFigure 22.4: Smoothing a noisy signal using 1D convolution with a moving average filter. The light gray line shows the original noisy sinusoidal signal, while the black line displays the smoothed output after applying a uniform filter of size 10.\n\n\n\n\n\n\n\nA Toy Example\nTo build intuition, consider a simple example with 5×5 images and 3×3 filters. We will classify images into three classes: cross, diagonal, and right-diagonal.\nimg = np.genfromtxt(\"../data/img-x.tsv\", delimiter=\"\\t\")\nimgx = img[0:5,:]\nimgd = img[5:10,:]\nimgrd = img[10:15,:]\nplt.imshow(imgx, cmap='binary'); plt.title(f'Cross'); plt.show();\nplt.imshow(imgd, cmap='binary');  plt.title(f'Diagonal'); plt.show();\nplt.imshow(imgrd, cmap='binary'); plt.title(f'Right-Diagonal'); plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will use two filters.\nf = np.genfromtxt(\"../data/img-filter.tsv\", delimiter=\"\\t\")\nf1 = f[0:3,:]\nf2 = f[3:6,:]\nplt.imshow(f1, cmap='binary'); plt.title(f'Filter 1'); plt.show();\nplt.imshow(f2, cmap='binary'); plt.title(f'Filter 2'); plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first filter has 1s along its main diagonal and 0s elsewhere—when placed over a matching diagonal line in the image, the dot product is maximized. The second filter detects the opposite diagonal. This is why convolutions are effective feature detectors: they respond strongly when their pattern matches what’s in the image.\ndef conv(img, f):\n    out = np.zeros((3,3))\n    for i in range(3):\n        for j in range(3):\n            out[i,j] = np.sum(img[i:i+3,j:j+3]*f)\n    return out\ndef maxpool(img):\n    return np.max(img)\ndef fc(x, w, b):\n    return jnp.dot(w,x) + b\ndef softmax(x):\n    return jnp.exp(x) / jnp.sum(jnp.exp(x))\n\ndef cnn(img, f1, f2):\n    out1 = conv(img,f1)\n    out2 = conv(img,f2)\n    x = np.array([maxpool(out1), maxpool(out2)])\n    w = np.array([[1,0],[0,1],[0.6,0.6]])\n    b = 0\n    z = fc(x,w,b)\n    return softmax(z),out1,out2\n\nzx, outx1, outx2 = cnn(imgx,f1,f2)\nplt.imshow(outx1, cmap='binary'); plt.title(f'Output 1'); plt.show();\nplt.imshow(outx2, cmap='binary'); plt.title(f'Output 2'); plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that both outputs have pixels with high values. However, when we apply the same two filters to the diagonal image, we get different results.\nzd, outd1, outd2 = cnn(imgd,f1,f2)\nplt.imshow(outd1, vmin=0,vmax=3,cmap='binary'); plt.title(f'Output 1');  plt.show();\nplt.imshow(outd2, vmin=0,vmax=3, cmap='binary'); plt.title(f'Output 2'); plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe result of applying the second filter to the diagonal image is a matrix close to zero—the filter pattern doesn’t match the image content. Applying the second filter to the diagonal image yields near-zero activations, illustrating that the filter remains dormant when its specific search pattern is absent. This observation motivates the pooling layer: by retaining only the maximum activation within each region, the network preserves the presence of a detected feature while discarding its exact location. This provides a degree of translation invariance. Finally, we concatenate the pooled outputs and apply a fully connected layer for classification.\n\nzrd, outrd1, outrd2 = cnn(imgrd,f1,f2)\nprint(f'Cross: {zx}')\n## Cross: [0.26163495 0.26163495 0.47673005]\nprint(f'Diagonal: {zd}')\n## Diagonal: [0.5937724  0.08035836 0.32586923]\nprint(f'Right-Diagonal: {zrd}')\n## Right-Diagonal: [0.08035836 0.5937724  0.32586923]\n\nThe model correctly predicted all three classes. This toy example illustrates the core CNN pipeline: convolutions extract local features, pooling provides spatial invariance, and fully connected layers perform classification.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html#cnn-for-mnist",
    "href": "22-cnn.html#cnn-for-mnist",
    "title": "22  Convolutional Neural Networks",
    "section": "22.4 CNN for MNIST",
    "text": "22.4 CNN for MNIST\nNow we apply these concepts to MNIST using PyTorch. While we used JAX for the fully connected network to show the low-level mechanics of gradients and matrix operations, we switch to PyTorch here to demonstrate a high-level deep learning framework. PyTorch provides powerful abstractions like nn.Conv2d and nn.Module that make building and debugging complex architectures like CNNs more intuitive, and it effectively handles the boilerplate of autograd and parameter management.\nThe architecture follows the same pattern: convolutional layers extract hierarchical features, pooling reduces spatial dimensions, and fully connected layers produce class probabilities.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nx = parse_images(\"../data/t10k-images-idx3-ubyte.gz\")\ny = parse_labels(\"../data/t10k-labels-idx1-ubyte.gz\")\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 1 input channel (grayscale), 32 output channels, 3x3 kernel\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # 32 input channels, 64 output channels\n        # Note: PyTorch defaults to valid padding (no padding). \n        # After conv1 (28-&gt;26), conv2 (26-&gt;24), pool (24-&gt;12): 64 channels × 12 × 12 = 9216\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef data_stream():\n    perm = rng.permutation(num_train)\n    while True:\n        for i in range(num_batches):\n            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n            yield torch.from_numpy(x[batch_idx,:,:]/ np.float32(255.))[:, None, :, :], torch.from_numpy(y[batch_idx])\n\ndef train(model, optimizer):\n    model.train()\n    batches = data_stream()\n    for epoch in range(num_epochs):\n        for _ in range(num_batches):\n            optimizer.zero_grad()\n            data, target = next(batches)\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch {epoch}: Loss {loss.item()}\")\n\nmodel = Net()\noptimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\ntrain(model, optimizer)\n\nThe CNN uses the Adadelta optimizer, an adaptive learning rate method that adjusts step sizes based on gradient history. Unlike vanilla SGD (which we used for the fully connected network), Adadelta typically requires less hyperparameter tuning.\n\nmodel.eval()\nwith torch.no_grad():\n    test_data = torch.from_numpy(x[testi,:,:]/ np.float32(255.))[:, None, :, :]\n    test_targets = torch.from_numpy(y[testi])\n    output = model(test_data)\n    pred = output.argmax(dim=1)\n    correct = pred.eq(test_targets).sum().item()\n    accuracy = 100. * correct / len(testi)\n    print(f\"CNN Test Accuracy: {accuracy:.1f}%\")\n\nThe CNN architecture is shown in Figure 22.5.\n\n\n\n\n\n\nflowchart TB\n    subgraph ConvLayers[\"Convolutional Layers\"]\n        direction LR\n        I[\"Input&lt;br/&gt;1 × 28 × 28&lt;br/&gt;Grayscale MNIST\"]\n        C1[\"Conv2d: 1→8&lt;br/&gt;kernel: 3×3&lt;br/&gt;8 × 26 × 26\"]\n        R1[\"ReLU\"]\n        P1[\"MaxPool2d&lt;br/&gt;2×2&lt;br/&gt;8 × 13 × 13\"]\n        C2[\"Conv2d: 8→16&lt;br/&gt;kernel: 3×3&lt;br/&gt;16 × 11 × 11\"]\n        R2[\"ReLU\"]\n        P2[\"MaxPool2d&lt;br/&gt;2×2&lt;br/&gt;16 × 5 × 5\"]\n        F[\"Flatten&lt;br/&gt;400 features\"]\n        \n        I --&gt; C1 --&gt; R1 --&gt; P1 --&gt; C2 --&gt; R2 --&gt; P2 --&gt; F\n    end\n    \n    subgraph FCLayers[\"Fully Connected&lt;br/&gt;Classification\"]\n        direction LR\n        FC1L[\"Linear&lt;br/&gt;400 → 32\"]\n        R3[\"ReLU\"]\n        FC2L[\"Linear&lt;br/&gt;32 → 10\"]\n        LS[\"Log-Softmax\"]\n        O[\"Output&lt;br/&gt;10 classes&lt;br/&gt;Digits 0-9\"]\n        \n        FC1L --&gt; R3 --&gt; FC2L --&gt; LS --&gt; O\n    end\n    \n    ConvLayers --&gt; FCLayers\n\n    %% Styling\n    style I fill:#e1f5ff,stroke:#01579b,stroke-width:2px,color:#000\n    style C1 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style C2 fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:#000\n    style FC1L fill:#ffe0b2,stroke:#f57c00,stroke-width:2px,color:#000\n    style FC2L fill:#ffe0b2,stroke:#f57c00,stroke-width:2px,color:#000\n    style R1 fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    style R2 fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    style R3 fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    style P1 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style P2 fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style F fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    style LS fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#000\n    style O fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,color:#000\n\n    style ConvLayers fill:#fafafa,stroke:#ccc,stroke-width:2px,color:#000\n    style FCLayers fill:#fafafa,stroke:#ccc,stroke-width:2px,color:#000\n\n\n\n\nFigure 22.5: CNN Architecture\n\n\n\n\n\nThis model achieves an accuracy of 93% on the test set, compared to 77% for the fully connected network. More importantly, it uses far fewer parameters! The number of parameters in the model is 14k, compared to 800k for the fully connected network. The convolutional layers share weights across spatial positions rather than learning separate weights for each pixel location. This parameter efficiency, combined with the inductive bias toward local and translation-invariant features, explains why CNNs have become the dominant architecture for computer vision tasks.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "22-cnn.html#summary",
    "href": "22-cnn.html#summary",
    "title": "22  Convolutional Neural Networks",
    "section": "22.5 Summary",
    "text": "22.5 Summary\nConvolutional neural networks exploit the spatial structure of images through three key mechanisms:\n\nConvolutional layers apply learned filters that detect local patterns, sharing weights across the entire image.\nPooling layers reduce spatial dimensions while providing invariance to small translations.\nHierarchical feature learning: early layers detect simple features (edges, textures); deeper layers combine these into complex patterns (shapes, objects).\n\nThe CNN architecture we built achieves significantly higher accuracy than the fully connected network while using far fewer parameters—the convolutional layers share weights across spatial positions rather than learning separate weights for each pixel location. This parameter efficiency, combined with the inductive bias toward local and translation-invariant features, explains why CNNs have become the dominant architecture for computer vision tasks.\n\n\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” Biological Cybernetics 36 (4): 193–202.\n\n\nZhang, Wei, Katsuyuki Itoh, Jun Tanida, and Yoshiki Ichioka. 1988. “Shift-Invariant Pattern Recognition Neural Network and Its Optical Architecture.” Proceedings of Annual Conference of the Japan Society of Applied Physics.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "23-nlp.html",
    "href": "23-nlp.html",
    "title": "23  Natural Language Processing",
    "section": "",
    "text": "23.1 Converting Words to Numbers (Embeddings)\nThe ability to understand and generate human language has long been considered a hallmark of intelligence. When Alan Turing proposed his famous test in 1950, he chose natural conversation as the ultimate benchmark for machine intelligence. Yet for decades, this goal remained frustratingly elusive. Early attempts at machine translation in the 1950s, which simply replaced words using bilingual dictionaries, produced nonsensical results that highlighted the profound complexity of human language. The phrase “The spirit is willing, but the flesh is weak” allegedly translated to Russian and back as “The vodka is good, but the meat is rotten”—a cautionary tale about the subtleties of meaning that transcend mere word substitution.\nThis chapter traces the remarkable journey from those early failures to today’s language models that can engage in nuanced dialogue, translate between languages with near-human accuracy, and even generate creative text. At the heart of this transformation lies a fundamental shift in how we represent language computationally: from discrete symbols manipulated by hand-crafted rules to continuous vector spaces learned from vast corpora of text.\nThe chapter develops the mathematical foundations of natural language understanding. We begin with word embeddings—continuous vector representations that encode semantic relationships. We then introduce attention mechanisms that enable context-dependent representations, culminating in the Transformer architecture that powers modern language models like BERT and GPT.\nLanguage poses distinctive challenges for mathematical modeling: unlike images, which naturally exist as arrays of continuous pixel values, or audio signals, which are continuous waveforms, text consists of discrete symbols with no inherent geometric structure. The word “cat” is not inherently closer to “dog” than to “quantum”—at least not in any obvious mathematical sense. Yet humans effortlessly recognize that cats and dogs share semantic properties that neither shares with abstract physics concepts.\nA naive way to represent words is through one-hot encoding, where each word in a vocabulary is assigned a unique vector with a single non-zero entry. For example, in a vocabulary of size \\(|V|\\), the word “cat” might be represented as \\[\nv_{\\text{cat}} = \\overbrace{[0, 0, \\ldots, 1, \\ldots, 0]}^{|V|}\n\\] with the 1 in the \\(i\\)-th position corresponding to “cat”. However, this approach fails to capture any notion of semantic similarity: the cosine similarity between any two distinct one-hot vectors is zero, erasing all information about how words relate to each other.\nThis type of representation makes even the seemingly simple task of determining whether two sentences have similar meanings challenging. The sentences “The cat sat on the mat” and “A feline rested on the rug” express nearly identical ideas despite sharing no words except “the” and “on.” Conversely, “The bank is closed” could refer to a financial institution or a river’s edge—the same words encoding entirely different meanings. These examples illustrate why early symbolic approaches to natural language processing, based on logical rules and hand-crafted features, struggled to capture the fluid, contextual nature of meaning.\nThe breakthrough came from reconceptualizing the representation problem. Instead of treating words as atomic symbols, what if we could embed them in a continuous vector space where geometric relationships encode semantic relationships? This idea, simple in retrospect, revolutionized the field. In such a space, we might find that \\(v_{\\text{cat}} - v_{\\text{dog}}\\) has similar direction to \\(v_{\\text{car}} - v_{\\text{bicycle}}\\), capturing the analogical relationship “cat is to dog as car is to bicycle” through vector arithmetic.\nTo formalize this intuition, we seek a mapping \\(\\phi: \\mathcal{V} \\rightarrow \\mathbb{R}^d\\) from a vocabulary \\(\\mathcal{V}\\) of discrete tokens to \\(d\\)-dimensional vectors. The challenge lies in learning this mapping such that the resulting geometry reflects semantic relationships.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#converting-words-to-numbers-embeddings",
    "href": "23-nlp.html#converting-words-to-numbers-embeddings",
    "title": "23  Natural Language Processing",
    "section": "",
    "text": "The Math of Twenty Questions\nMy (Vadim’s) daughter and I play a game of Twenty Questions during road trips. The rules are simple: one person thinks of something, and the other person has to guess what it is by asking yes-or-no questions. The person who is guessing can ask up to twenty questions, and then they have to make a guess. If they guess correctly, they win; if not, the other person wins. The game is fun, but it’s also a great way to illustrate how AI systems can learn to represent words and phrases as numbers. The trick is to come up with an optimal set of yes-or-no questions that will allow you to distinguish between all the words or phrases you might want to represent. Surprisingly, most of the words can be identified with a small set of universal questions asked in the same order every time. Usually the person who has a better set of questions wins. For example, you might ask:\n\nIs it an animal? (Yes)\nIs this a domestic animal? (No)\nIs it larger than a human? (Yes)\nDoes it have a long tail? (No)\nIs it a predator? (Yes)\nCan move on two feet? (Yes)\nIs it a bear? (Yes)\n\nThus, if we use a 20-dimensional 0-1 vector to represent the word “bear,” the portion of this vector corresponding to these questions would look like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n1\n0\n1\n1\n\n\n\nThis is called a word vector. Specifically, it’s a “binary” or 0/1 vector: 1 means yes, 0 means no. Different words would produce different answers to the same questions, so they would have different word vectors. If we stack all these vectors in a matrix, where each row is a word and each column is a question, we get something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n1\n0\n1\n1\n\n\nDog\n1\n1\n0\n1\n0\n0\n\n\nCat\n1\n1\n0\n1\n1\n0\n\n\n\nThe binary nature of these vectors forces us to choose 1 or 0 for the “Larger than human” question for “Dog.” However, there are some dog breeds that are larger than humans, so this binary representation is not very useful in this case. We can do better by allowing the answers to be numbers between 0 and 1, rather than just 0 or 1. This way, we can represent the fact that some dogs are larger than humans, but most are not. For example, we might answer the question “Is it larger than a human?” with a 0.1 for a dog and a 0.8 for a bear. Some types of bears can be smaller than humans, for example, black bears that live in North America, but most bears are larger than humans.\nUsing this approach, the vectors now become\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal\nDomestic\nLarger than human\nLong tail\nPredator\nCan move on two feet\n\n\n\n\nBear\n1\n0\n0.8\n0\n1\n0.8\n\n\nDog\n1\n1\n0.1\n0.6\n0\n0\n\n\nCat\n1\n0.7\n0.01\n1\n0.6\n0\n\n\n\nAI systems also use non-binary scoring rules to judge a win or loss. For example, if the answer is “bear,” then the score might be 100 points for a correct guess, 90 points for a close guess like “wolf” or “lion,” and 50 points for a distant guess like “eagle.” This way of keeping score matches the real-world design requirements of most NLP systems. For example, if you translate JFK saying “Ich bin ein Berliner” as “I am a German,” you’re wrong, but a lot closer than if you translate it as “I am a butterfly.”\nThe process of converting words into numbers is called “embedding.” The resulting vectors are called “word embeddings.” The only part that is left is how to design an algorithm that can find a good set of questions to ask. Usually real-life word embeddings have hundreds of questions, not just twenty. The process of finding these questions is called “training” the model. The goal is to find a set of questions that will allow the model to distinguish between all the words in the vocabulary, and to do so in a way that captures their meanings. However, the algorithms do not have a notion of meaning in the same way that humans do. Instead, they learn by counting word co-location statistics—that is, which words tend to appear with which other words in real sentences written by humans.\nThese co-occurrence statistics serve as surprisingly effective proxies for meaning. For example, consider the question: “Among all sentences containing ‘fries’ and ‘ketchup,’ how frequently does the word ‘bun’ also appear?” This is the kind of query a machine can easily formulate and answer, since it relies on counting rather than true understanding.\nWhile such a specific question may be too limited if you can only ask a few hundred, the underlying idea—using word co-occurrence patterns—is powerful. Word embedding algorithms are built on this principle: they systematically explore which words tend to appear together, and through optimization, learn the most informative “questions” to ask. By repeatedly applying these learned probes, the algorithms construct a vector for each word or phrase, capturing its relationships based on co-occurrence statistics. These word vectors are then organized into a matrix for further use.\nThere are many ways to train word embeddings, but the most common one is to use a neural network. The neural network learns to ask questions that are most useful for distinguishing between different words. It does this by looking at a large number of examples of words and their meanings, and then adjusting the weights of the questions based on how well they perform. One of the first and most popular algorithms for this is called Word2Vec, which was introduced by Mikolov et al. (2013) at Google in 2013.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#word2vec-and-distributional-semantics",
    "href": "23-nlp.html#word2vec-and-distributional-semantics",
    "title": "23  Natural Language Processing",
    "section": "23.2 Word2Vec and Distributional Semantics",
    "text": "23.2 Word2Vec and Distributional Semantics\nThe theoretical foundation for learning meaningful word representations comes from the distributional hypothesis, articulated by linguist J.R. Firth in 1957: “You shall know a word by the company it keeps.” This principle suggests that words appearing in similar contexts tend to have similar meanings. If “coffee” and “tea” both frequently appear near words like “drink,” “hot,” “cup,” and “morning,” we can infer their semantic similarity.\nThe word2vec framework, introduced by Mikolov et al. (2013), operationalized this insight through a beautifully simple probabilistic model. The skip-gram variant posits that a word can be used to predict its surrounding context words. Given a corpus of text represented as a sequence of words \\(w_1, w_2, \\ldots, w_T\\), the model maximizes the likelihood:\n\\[\\mathcal{L} = \\sum_{t=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\\]\nwhere \\(m\\) is the context window size. The conditional probability is parameterized using two sets of embeddings: \\(\\mathbf{v}_w\\) for words as centers and \\(\\mathbf{u}_w\\) for words as context:\n\\[P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^T \\mathbf{v}_c)}{\\sum_{w \\in \\mathcal{V}} \\exp(\\mathbf{u}_w^T \\mathbf{v}_c)}\\]\nThis formulation reveals deep connections to the theoretical frameworks discussed in previous chapters. The dot product \\(\\mathbf{u}_o^T \\mathbf{v}_c\\) acts as a compatibility score between center and context words, while the softmax normalization ensures a valid probability distribution. From the perspective of ridge functions, we can view this as learning representations where the function \\(f(w_c, w_o) = \\mathbf{u}_o^T \\mathbf{v}_c\\) captures the log-odds of co-occurrence.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#word2vec-for-war-and-peace",
    "href": "23-nlp.html#word2vec-for-war-and-peace",
    "title": "23  Natural Language Processing",
    "section": "23.3 Word2Vec for War and Peace",
    "text": "23.3 Word2Vec for War and Peace\nThis analysis showcases the application of Word2Vec on Leo Tolstoy’s “War and Peace,” demonstrating how neural network-based techniques can learn vector representations of words by analyzing their contextual relationships within the text. The model employs the skip-gram algorithm with negative sampling (explained later), a widely used configuration for Word2Vec. We use vector dimension of 100, providing a balance between capturing semantic detail and computational efficiency. The context window size is set to 5, and the model is trained over several iterations to ensure convergence. Finally, we use PCA to reduce the dimensionality of the vectors to 2D for visualization.\n\n\nWord2Vec for War and Peace\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot\n# define training data, read from ../data/warpeace.txt\nwith open('../data/warpeace.txt', 'r') as file:\n    Text = file.read()\nsentences = [s.split() for s in Text.split('. ') ] # split the text into sentences\n# Remove spaces and punctuation\nsentences = [[word.strip('.,!?;:()[]') for word in sentence if word.strip('.,!?;:()[]')] for sentence in sentences]\n\n# Remove empty sentences\nsentences = [sentence for sentence in sentences if sentence]\nmodel = Word2Vec(sentences, min_count=1,seed=42,workers=1)\n# fit a 2d PCA model to the vectors, only use the words from ../data/embedding_words.txt\nwith open('../data/embedding_words.txt', 'r') as file:\n    words = file.read().splitlines()\nX = model.wv[words]  # get the vectors for the words\n# reduce the dimensionality of the vectors to 2D using PCA\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\n# words = model.wv.index_to_key\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()\n\n\n\n\n\nWord2Vec for War and Peace\n\n\n\n\nThe results reveal meaningful semantic relationships within the text and highlight several key insights into the thematic structure of Tolstoy’s War and Peace. First, the clustering of military-related terms (soldier, regiment, battle) underscores Word2Vec’s ability to group semantically related words based on their co-occurrence patterns. Additionally, the grouping of words such as “ballroom,” “court,” and “marriage” reflects the novel’s emphasis on aristocratic society and its social hierarchy. The use of PCA for dimensionality reduction effectively preserves meaningful relationships while reducing the original high-dimensional Word2Vec space (100 dimensions) to a two-dimensional representation. Overall, this visualization demonstrates how Word2Vec captures distinct thematic domains, including military, social, and government. Notice that a more abstract concept like peace is surrounded by words from both government (history, power, war) and social domains (family, friendship, marriage,court, ballroom, society), indicating its central role in the narrative but is distant from the military domain, which is more focused on the war aspect of the story.\nThese insights have practical applications in literary analysis, theme extraction, character relationship mapping, historical text understanding, and similarity-based text search and recommendation systems.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#computational-efficiency-through-negative-sampling",
    "href": "23-nlp.html#computational-efficiency-through-negative-sampling",
    "title": "23  Natural Language Processing",
    "section": "23.4 Computational Efficiency Through Negative Sampling",
    "text": "23.4 Computational Efficiency Through Negative Sampling\nThe elegance of word2vec’s formulation belies a serious computational challenge. Computing the normalization term in the softmax requires summing over the entire vocabulary—potentially millions of terms—for every gradient update. With large corpora containing billions of words, this quickly becomes intractable.\nNegative sampling transforms the problem from multi-class classification to binary classification. Instead of predicting which word from the entire vocabulary appears in the context, we ask a simpler question: given a word pair, is it a real center-context pair from the corpus or a randomly generated negative example? The objective becomes:\n\\[\\mathcal{L}_{\\text{NS}} = \\log \\sigma(\\mathbf{u}_o^T \\mathbf{v}_c) + \\sum_{k=1}^K \\mathbb{E}_{w_k \\sim P_n} \\left[\\log \\sigma(-\\mathbf{u}_{w_k}^T \\mathbf{v}_c)\\right]\\]\nwhere \\(\\sigma(x) = 1/(1 + e^{-x})\\) is the sigmoid function, and \\(P_n\\) is a noise distribution over words. The clever insight is that by carefully choosing the noise distribution—typically \\(P_n(w) \\propto f(w)^{3/4}\\) where \\(f(w)\\) is word frequency—we can approximate the original objective while reducing computation from \\(O(|\\mathcal{V}|)\\) to \\(O(K)\\), where \\(K \\ll |\\mathcal{V}|\\) is a small number of negative samples (typically 5-20).\nAn alternative solution is hierarchical softmax, which replaces the flat softmax over the vocabulary with a binary tree where each leaf represents a word. The probability of a word is then the product of binary decisions along the path from root to leaf:\n\\[P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left([\\![n(w_o, j+1) = \\text{leftChild}(n(w_o, j))]\\!] \\cdot \\mathbf{u}_{n(w_o,j)}^T \\mathbf{v}_c\\right)\\]\nwhere \\(L(w_o)\\) is the length of the path to word \\(w_o\\), \\(n(w_o, j)\\) is the \\(j\\)-th node on this path, and \\([\\![\\cdot]\\!]\\) is the Iverson bracket—a notation that returns \\(+1\\) if the condition inside is true (the next node is the left child) and \\(-1\\) otherwise (the next node is the right child). This sign flip determines whether we use the sigmoid or its complement at each binary decision. Hierarchical softmax reduces computational complexity from \\(O(|\\mathcal{V}|)\\) to \\(O(\\log |\\mathcal{V}|)\\), though negative sampling typically performs better in practice.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#global-vectors-and-matrix-factorization",
    "href": "23-nlp.html#global-vectors-and-matrix-factorization",
    "title": "23  Natural Language Processing",
    "section": "23.5 Global Vectors and Matrix Factorization",
    "text": "23.5 Global Vectors and Matrix Factorization\nWhile word2vec learns from local context windows, GloVe (Global Vectors) leverages global co-occurrence statistics. The key observation is that the ratio of co-occurrence probabilities can encode semantic relationships. Consider the words “ice” and “steam” in relation to “solid” and “gas”: \\(P(\\text{solid} \\mid \\text{ice}) / P(\\text{solid} \\mid \\text{steam})\\) is large (around 8.9), while \\(P(\\text{gas} \\mid \\text{ice}) / P(\\text{gas} \\mid \\text{steam})\\) is small (around 0.085), and \\(P(\\text{water} \\mid \\text{ice}) / P(\\text{water} \\mid \\text{steam})\\) is close to 1 (around 1.36).\nThese ratios capture the semantic relationships: ice is solid, steam is gas, and both relate to water. GloVe learns embeddings that preserve these ratios through the objective:\n\\[\\mathcal{L}_{\\text{GloVe}} = \\sum_{i,j} h(X_{ij}) \\left(\\mathbf{v}_i^T \\mathbf{u}_j + b_i + c_j - \\log X_{ij}\\right)^2\\]\nwhere \\(X_{ij}\\) counts co-occurrences, \\(h(\\cdot)\\) is a weighting function that prevents very common or very rare pairs from dominating, and \\(b_i, c_j\\) are bias terms. A typical choice is \\(h(x) = (x/x_{\\max})^{\\alpha}\\) if \\(x &lt; x_{\\max}\\), else 1, with \\(\\alpha = 0.75\\) and \\(x_{\\max} = 100\\).\nThis formulation reveals GloVe as a weighted matrix factorization problem. We seek low-rank factors \\(\\mathbf{V}\\) and \\(\\mathbf{U}\\) such that \\(\\mathbf{V}^T\\mathbf{U} \\approx \\log \\mathbf{X}\\), where the approximation is weighted by the function \\(h(\\cdot)\\). This connection to classical linear algebra provides theoretical insights: the optimal embeddings lie in the subspace spanned by the top singular vectors of an appropriately transformed co-occurrence matrix.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#beyond-words-subword-and-character-models-tokenization",
    "href": "23-nlp.html#beyond-words-subword-and-character-models-tokenization",
    "title": "23  Natural Language Processing",
    "section": "23.6 Beyond Words: Subword and Character Models (Tokenization)",
    "text": "23.6 Beyond Words: Subword and Character Models (Tokenization)\nA fundamental limitation of word-level embeddings is their inability to handle out-of-vocabulary words or capture morphological relationships. The word “unhappiness” shares obvious morphological connections with “happy,” “unhappy,” and “happiness,” but word2vec treats these as completely independent tokens.\nFastText addresses this limitation by representing words as bags of character n-grams. For the word “where” with n-grams of length 3 to 6, we extract: “&lt;wh”, “whe”, “her”, “ere”, “re&gt;”, and longer n-grams up to the full word. The word embedding is then the sum of its n-gram embeddings:\n\\[\\mathbf{v}_{\\text{where}} = \\sum_{g \\in \\mathcal{G}_{\\text{where}}} \\mathbf{z}_g\\]\nThis approach naturally handles out-of-vocabulary words by breaking them into known n-grams and provides better representations for rare words by sharing parameters across morphologically related words.\nAn even more flexible approach is Byte Pair Encoding (BPE), which learns a vocabulary of subword units directly from the data. Starting with individual characters, BPE iteratively merges the most frequent pair of adjacent units until reaching a desired vocabulary size. For example, given a corpus with word frequencies {“fast”: 4, “faster”: 3, “tall”: 5, “taller”: 4}, BPE might learn merges like “t” + “a” \\(\\rightarrow\\) “ta”, then “ta” + “l” \\(\\rightarrow\\) “tal”, and so on. This data-driven approach balances vocabulary size with representation power, enabling models to handle arbitrary text while maintaining reasonable computational requirements.\nThe choice of tokenization strategy has profound implications for model performance and behavior. Modern LLMs typically use vocabularies of 50,000 to 100,000 tokens, carefully balanced to represent different languages while maintaining computational efficiency. A vocabulary that’s too small forces the model to represent complex concepts with many tokens, making it harder to capture meaning efficiently.\nConsider how the sentence “The Verbasizer helped Bowie create unexpected word combinations” might be tokenized by a BPE-based system. Rather than treating each word as a single unit, the tokenizer might split it into subword pieces like [“The”, ” Ver”, “bas”, “izer”, ” helped”, ” Bow”, “ie”, ” create”, ” unexpected”, ” word”, ” combinations”]. This fragmentation allows the model to handle rare words and proper names by breaking them into more common subcomponents.\nThis tokenization process explains some behavioral patterns in modern language models. Names from fictional universes, specialized terminology, and code snippets can be tokenized in ways that fragment their semantic meaning, potentially affecting model performance on these inputs. Understanding these tokenization effects is crucial for interpreting model behavior and designing effective prompting strategies.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#attention-mechanisms-and-contextual-representations",
    "href": "23-nlp.html#attention-mechanisms-and-contextual-representations",
    "title": "23  Natural Language Processing",
    "section": "23.7 Attention Mechanisms and Contextual Representations",
    "text": "23.7 Attention Mechanisms and Contextual Representations\nStatic word embeddings suffer from a fundamental limitation: they assign a single vector to each word, ignoring context. The word “bank” receives the same representation whether it appears in “river bank” or “investment bank.” This conflation of multiple senses into a single vector creates an information bottleneck that limits performance on downstream tasks.\nThe evolution of word embeddings culminated in the development of contextual representations, which dynamically adjust based on surrounding context. The context problem was solved by the introduction of attention mechanisms, particularly in the transformer architecture, which allows models to capture complex dependencies and relationships between words. The idea is to mimic the human ability to selectively focus attention. When reading this sentence, your visual system processes every word simultaneously, yet your conscious attention flows sequentially from word to word. You can redirect this attention at will—perhaps noticing a particular phrase or jumping back to reread a complex clause. This dynamic allocation of cognitive resources allows us to extract meaning from information-rich environments without being overwhelmed by irrelevant details.\nMachine learning systems faced analogous challenges in processing sequential information. Early neural networks, particularly recurrent architectures, processed sequences step by step, maintaining a hidden state that theoretically encoded all previous information. However, this sequential bottleneck created fundamental limitations: information from early time steps could vanish as sequences grew longer, and the inherently sequential nature prevented parallel computation that could leverage modern hardware effectively.\nThe attention Vaswani et al. (2023) mechanism revolutionized this landscape by allowing models to directly access and combine information from any part of the input sequence. Rather than compressing an entire sequence into a fixed-size vector, attention mechanisms create dynamic representations that adaptively weight different parts of the input based on their relevance to the current computation. This breakthrough enabled the development of the Transformer architecture, which forms the foundation of modern language models from BERT to GPT.\nAt its core, attention implements an information retrieval system inspired by database operations. Consider how you might search through a library: you formulate a query (what you’re looking for), examine the keys (catalog entries or book titles), and retrieve the associated values (the actual books or their contents). Attention mechanisms formalize this intuition through three learned representations:\n\nQueries (\\(\\mathbf{Q}\\)): What information are we seeking?\nKeys (\\(\\mathbf{K}\\)): What information is available at each position?\nValues (\\(\\mathbf{V}\\)): What information should we retrieve?\n\nGiven a set of keys and values, attention computes a weighted sum of the values based on their relevance to the query. Mathematically, this can be expressed as: \\[\n\\textrm{Attention}(\\mathbf{q}) \\stackrel{\\textrm{def}}{=} \\sum_{i=1}^m \\alpha_i(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i,\n\\]\nwhere \\(\\alpha_i\\) are scalar attention weights computed from the queries and keys. The operation itself is typically referred to as attention pooling. The name “attention” derives from the fact that the operation pays particular attention to the terms for which the weight \\(\\alpha_i\\) is significant (i.e., large).\nIt is typical in AI applications to assume that the attention weights \\(\\alpha_i\\) are nonnegative and form a convex combination, i.e., \\(\\sum_{i=1}^n \\alpha_i = 1\\). A common strategy for ensuring that the weights sum up to 1 is to normalize the unnormalized scores \\(s_i\\) via the softmax function: \\[\n\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^n \\exp(s_j)}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nThis diagram shows the basic attention computation. A query vector \\(q\\) is compared to each key \\(k_i\\) to produce a score, which is then turned into an attention weight \\(\\alpha(q,k_i)\\) (often normalized so the weights sum to one). The output is a weighted sum of the value vectors, \\(\\sum_{i=1}^m \\alpha(q,k_i) v_i\\), so values associated with keys most relevant to the query contribute more to the final representation.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#kernel-smoothing-as-attention",
    "href": "23-nlp.html#kernel-smoothing-as-attention",
    "title": "23  Natural Language Processing",
    "section": "23.8 Kernel Smoothing as Attention",
    "text": "23.8 Kernel Smoothing as Attention\nThe concept of attention pooling can actually be traced back to classical kernel methods like Nadaraya-Watson regression, where similarity kernels determine how much weight to give to different data points. Given a sequence of observations \\(y_1, \\ldots, y_n\\), and each observation has a two-dimensional index \\(x^{(i)} \\in \\mathbb{R}^2\\), such as in a spatial process. Then the question is what would be the value of \\(y\\) at a previously unobserved location \\(x_{new} = (x_1,x_2)\\). Let’s simulate some data and see what we can do.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some data\nx1 = np.random.uniform(0, 1, 100)\nx2 = np.random.uniform(0, 1, 100)\ny = np.sin(2 * np.pi * x1) + x2*x2+ np.random.normal(0, 0.1, 100)\n\nplt.scatter(x1, x2, c=y, cmap='viridis')\nplt.colorbar();\nplt.show()\n\n\n\n\n\n\n\nFigure 23.1: Simulated data for kernel smoothing\n\n\n\n\n\nThe kernel smoothing method then uses \\(q = (x_1,x_2)\\) as the query and observed \\(x_i\\)s as the keys. The weights are then computed as \\(\\alpha_i = K(q, x_i)\\) where \\(K\\) is a kernel function, that measures, how close \\(q\\) is to \\(x_i\\). Then, our prediction would be \\[\n\\hat{y} = \\sum_{i=1}^n \\alpha_i y_i.\n\\]\nThe weights \\(\\alpha_i\\) assign importance to \\(y_i\\) based on how close \\(q = (x_1, x_2)\\) is to \\(x^{(i)}\\). Common kernel choices and their formulas are summarized below:\n\n\n\n\n\n\n\nKernel\nFormula for \\(K(q, x^{(i)})\\)\n\n\n\n\nGaussian\n\\(\\exp\\left(-\\frac{\\|q - x^{(i)}\\|^2}{2h^2}\\right)\\)\n\n\nBoxcar\n\\(\\mathbb{I}\\left(\\|q - x^{(i)}\\| &lt; h\\right)\\)\n\n\nConstant\n\\(1\\)\n\n\nEpanechikov\n\\(\\max\\left(1 - \\frac{\\|q - x^{(i)}\\|}{h},\\ 0\\right)\\)\n\n\n\nHere, \\(h\\) is a bandwidth parameter controlling the width of the kernel, and \\(\\mathbb{I}\\) is the indicator function. Code for the kernels is given below.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Define some kernels for attention pooling\ndef gaussian(x): return np.exp(-x**2 / 2)\ndef boxcar(x): return np.abs(x) &lt; 1.0\ndef constant(x): return 1.0 + 0 * x  # noqa: E741\ndef epanechikov(x): return np.maximum(1 - np.abs(x), np.zeros_like(x))\n\n\n\nPlotting kernels\nfig, axes = plt.subplots(1, 4, sharey=True, figsize=(12, 3))\nkernels = (gaussian, boxcar, constant, epanechikov)\nnames = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')\nx = np.arange(-2.5, 2.5, 0.1)\nfor kernel, name, ax in zip(kernels, names, axes):\n    ax.plot(x, kernel(x))\n    ax.set_xlabel(name)\nplt.show()\n\n\n\n\n\nPlotting kernels\n\n\n\n\nEach of these kernels represents a different way of weighting information based on similarity or distance. In neural networks, this translates to learning how to attend to different parts of the input sequence, but with the flexibility to learn much more complex patterns than these simple mathematical functions.\nNow, lets plot the true value of the function and the kernel smoothed value.\n\n\nKernel Smoothing\nimport numpy as np\nimport matplotlib.pyplot as pyplot\n\nprint(\"DEBUG: Imported pyplot\")\n## DEBUG: Imported pyplot\n# print(dir())\n\ndef gaussian_kernel_smoothing(X1, X2, x1_train, x2_train, y_train, bandwidth=0.1):\n    # Get the shape of the grid\n    shape = X1.shape\n    \n    # Flatten the grid points for easier computation\n    X1_flat = X1.flatten()\n    X2_flat = X2.flatten()\n    \n    # Initialize predictions array\n    y_pred_flat = np.zeros(len(X1_flat))\n    \n    # For each test point, compute weighted average using Gaussian kernel\n    for i in range(len(X1_flat)):\n        test_x1, test_x2 = X1_flat[i], X2_flat[i]\n        \n        # Compute distances from test point to all training points\n        distances_squared = (x1_train - test_x1)**2 + (x2_train - test_x2)**2\n        \n        # Compute Gaussian weights\n        weights = np.exp(-distances_squared / (2 * bandwidth**2))\n        \n        # Compute weighted average (kernel smoothing prediction)\n        if np.sum(weights) &gt; 0:\n            y_pred_flat[i] = np.sum(weights * y_train) / np.sum(weights)\n        else:\n            y_pred_flat[i] = 0  # fallback if all weights are zero\n    \n    # Reshape back to grid shape\n    y_pred = y_pred_flat.reshape(shape)\n    \n    return y_pred\n\ndef gaussian(x):\n    return np.exp(-x**2 / 2)\n# Generate grid data for x1 and x2 (test data)\nx1 = np.linspace(0, 1, 100)\nx2 = np.linspace(0, 1, 100)\n# Now compute the grid of x1 and x2 values\nX1, X2 = np.meshgrid(x1, x2)\n\n# Generate training data\nx1_train = np.random.uniform(0, 1, 100)\nx2_train = np.random.uniform(0, 1, 100)\ny_train = np.sin(2 * np.pi * x1_train) + x2_train*x2_train + np.random.normal(0, 0.1, 100)\n\n\n# Test the function with the existing data\ny_pred = gaussian_kernel_smoothing(X1, X2, x1_train, x2_train, y_train, bandwidth=0.1)\n\n# Plot comparison between true function and kernel smoothing prediction\nfig, axes = pyplot.subplots(1, 3, figsize=(15, 5))\n\n# True function (without noise for cleaner visualization)\ny_true = np.sin(2 * np.pi * X1) + X2*X2\n\n# Plot 1: True function\nim1 = axes[0].contourf(X1, X2, y_true, levels=20, cmap='viridis')\naxes[0].scatter(x1_train, x2_train, c='red', marker='x', s=30, alpha=0.7)\naxes[0].set_title('True Function')\naxes[0].set_xlabel('x1')\naxes[0].set_ylabel('x2')\npyplot.colorbar(im1, ax=axes[0])\n## &lt;matplotlib.colorbar.Colorbar object at 0x152d09b50&gt;\n\n# Plot 2: Kernel smoothing prediction\nim2 = axes[1].contourf(X1, X2, y_pred, levels=20, cmap='viridis')\naxes[1].scatter(x1_train, x2_train, c='red', marker='x', s=30, alpha=0.7)\naxes[1].set_title('Gaussian Kernel Smoothing')\naxes[1].set_xlabel('x1')\naxes[1].set_ylabel('x2')\npyplot.colorbar(im2, ax=axes[1])\n## &lt;matplotlib.colorbar.Colorbar object at 0x1527db890&gt;\n\n# Plot 3: Difference (error)\ndiff = y_true - y_pred\nim3 = axes[2].contourf(X1, X2, diff, levels=20, cmap='RdBu_r')\naxes[2].scatter(x1_train, x2_train, c='black', marker='x', s=30, alpha=0.7)\naxes[2].set_title('Error (True - Predicted)')\naxes[2].set_xlabel('x1')\naxes[2].set_ylabel('x2')\nplt.colorbar(im3, ax=axes[2])\n## &lt;matplotlib.colorbar.Colorbar object at 0x1527dcb30&gt;\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nKernel Smoothing using Gaussian Kernel\n\n\n\n\nWe can see that a relatively simple concept of calculating a prediction by averaging the values of the training data, weighted by the kernel function, can be used to estimate the value of the function at a new point rather well. Clearly, the model is not performing well in the regions where we do not have training data, but it is still able to capture the general shape of the function.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#cross-attention-for-translation",
    "href": "23-nlp.html#cross-attention-for-translation",
    "title": "23  Natural Language Processing",
    "section": "23.9 Cross-attention for Translation",
    "text": "23.9 Cross-attention for Translation\nIn the context of sequence-to-sequence models, one of the key challenges is effectively aligning words and phrases between the source and target languages. This alignment is crucial for ensuring that the translated output maintains the intended meaning and grammatical structure of the original text. Cross-attention mechanisms address this problem by allowing the model to focus on relevant parts of the source sequence when generating each word in the target sequence. This selective focus helps in capturing the nuances of translation, such as word order differences and syntactic variations, which are common in multilingual contexts.\nThe sequence-to-sequence is probably the most commonly used application of language models. Examples include machine translation, text summarization, and text generation from a prompt, question-answering, and chatbots.\nConsider a problem of translating ``The cat sleeps’’ into French. The encoder will produce a sequence of embeddings for the source sentence, and the decoder will produce a sequence of embeddings for the target sentence. The cross-attention mechanism will use English embeddings as keys and values, and French embeddings as queries. The attention weights will be calculated as shown in Figure 23.2.\n\n\n\n\n\n\nFigure 23.2: Cross-attention matrix for the translation of “The cat sleeps” into French\n\n\n\nThe rows are queries, and are represented by the French decoder tokens [“Le”, “chat”, “dort”]. Columns, which serve as Keys and Values, are represented by the English encoder tokens [“The”, “cat”, “sleeps”, “”]. The attention matrix is non-square, with dimensions of 3×4, corresponding to the decoder length and encoder length, respectively.\nKey Attention Patterns:\n\nWord-by-Word Alignment:\n\nThe French word “Le” aligns with the English word “The” with an attention weight of 0.85, indicating that determiners align across languages.\nThe French word “chat” aligns with the English word “cat” with an attention weight of 0.80, demonstrating a direct semantic translation.\nThe French word “dort” aligns with the English word “sleeps” with an attention weight of 0.75, showing that verb concepts align.\n\nCross-Linguistic Dependencies:\n\nEach French word primarily focuses on its English equivalent, with minimal attention given to unrelated source words. This pattern reflects the model’s ability to learn translation alignments.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#transformer-architecture",
    "href": "23-nlp.html#transformer-architecture",
    "title": "23  Natural Language Processing",
    "section": "23.10 Transformer Architecture",
    "text": "23.10 Transformer Architecture\nThe Transformer represents a fundamental shift in how we approach neural network architectures for sequential data. Introduced in the seminal paper “Attention is All You Need” by Vaswani et al. (2023) in 2017, the Transformer has since become the backbone of virtually every state-of-the-art language model, from OpenAI’s GPT series to Google’s Gemini and Meta’s Llama. Beyond text generation, Transformers have proven remarkably versatile, finding applications in audio generation, image recognition, protein structure prediction, and even game playing.\nAt its core, text-generative Transformer models operate on a deceptively simple principle: next-word prediction. Given a text prompt from the user, the model’s fundamental task is to determine the most probable next word that should follow the input. This seemingly straightforward objective, when scaled to billions of parameters and trained on vast text corpora, gives rise to emergent capabilities that often surprise even their creators.\nThe first step in the Transformers is the self-attention mechanism, which allows them to process entire sequences simultaneously rather than sequentially. This parallel processing capability not only enables more efficient training on modern hardware but also allows the model to capture long-range dependencies more effectively than previous architectures like recurrent neural networks.\nWhile modern language models like GPT-4 contain hundreds of billions of parameters, the fundamental architectural principles can be understood through smaller models like GPT-2. The GPT-2 small model, with its 124 million parameters, shares the same core components and principles found in today’s most powerful models, making it an ideal vehicle for understanding the underlying mechanics.\nThe figure below shows the schematic representation of a Transformer architecture.\n\n\n\nTransformer Architecture\n\n\nTransformer consists of four fundamental components that work in concert to transform raw text into meaningful predictions. The first step involves converting text to numbers (Tokenizer, Embedding Layer, Positional Encoding). The second step involves the Self-Attention mechanism. The third step involves the Multi-Layer Perceptron (MLP). The final step involves the Output Probabilities. We have covered the first two steps in the previous section. Now we will cover the third step, the Multi-Layer Perceptron. Following the attention mechanism, each token’s representation passes through a position-wise feedforward network. The MLP serves a different purpose than attention: while attention routes information between tokens, the MLP transforms each token’s representation independently.\nThe MLP consists of two linear transformations with a GELU activation function:\n\\[\\text{MLP}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2\\]\nThe first transformation expands the dimensionality, allowing the model to use a higher-dimensional space for computation before projecting back to the original size. This expansion provides the representational capacity needed for complex transformations while maintaining consistent dimensions across layers.\nAfter processing through all Transformer blocks, the final step converts the rich contextual representations back into predictions over the vocabulary. This involves two key operations. First, The final layer representations are projected into vocabulary space using a linear transformation:\n\\[\\text{logits} = \\text{final\\_representations} \\cdot W_{\\text{output}} + b_{\\text{output}}\\]\nThis produces a vector of \\(|V|\\) values (one for each token in the vocabulary) called logits, representing the model’s raw preferences for each possible next token.\nThe logits are converted into a probability distribution using the softmax function:\n\\[P(\\text{token}_i) = \\frac{\\exp(\\text{logit}_i)}{\\sum_{j=1}^{|\\mathcal{V}|} \\exp(\\text{logit}_j)}\\]\nThis creates a valid probability distribution where all values sum to 1, allowing us to sample the next token based on the model’s learned preferences.\nWhile the three main components form the core of the Transformer, several additional mechanisms enhance stability and performance:\n\nLayer Normalization\nApplied twice in each Transformer block (before attention and before the MLP), layer normalization stabilizes training by normalizing activations across the feature dimension:\n\\[\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\odot \\gamma + \\beta\\]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the mean and standard deviation computed across the feature dimension, and \\(\\gamma\\) and \\(\\beta\\) are learned scaling and shifting parameters.\n\n\nResidual Connections\nResidual connections create shortcuts that allow gradients to flow directly through the network during training:\n\\[\\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\\]\nThese connections are crucial for training deep networks, preventing the vanishing gradient problem that would otherwise make it difficult to update parameters in early layers.\n\n\nDropout\nDuring training, dropout randomly zeros out a fraction of neuron activations, preventing overfitting and encouraging the model to learn more robust representations. Dropout is typically disabled during inference.\n\n\nComputational Complexity and Scalability\nThe Transformer architecture’s quadratic complexity with respect to sequence length (\\(O(n^2)\\) for self-attention) has motivated numerous efficiency improvements. However, this complexity enables the model’s remarkable ability to capture long-range dependencies that simpler architectures cannot handle effectively.\nModern implementations leverage optimizations like: - Gradient checkpointing to trade computation for memory - Mixed precision training using 16-bit floating point - Efficient attention implementations that minimize memory usage\nThe Transformer’s success stems from its elegant balance of expressivity and computational efficiency. By enabling parallel processing while maintaining the ability to model complex dependencies, it has become the foundation for the current generation of large language models that are reshaping our interaction with artificial intelligence.\nThe complete Transformer architecture consists of two main components working in harmony. The encoder stack processes input sequences through multiple layers of multi-head self-attention and position-wise feedforward networks, enhanced with residual connections and layer normalization that help stabilize training. The decoder stack uses masked multi-head self-attention to prevent the model from “cheating” by looking at future tokens during training.\nThis architecture enabled the development of three distinct families of models, each optimized for different types of tasks. Encoder-only models like BERT excel at understanding tasks such as classification, question answering, and sentiment analysis. They can see the entire input at once, making them particularly good at tasks where understanding context from both directions matters.\nDecoder-only models like GPT are particularly good at generation tasks, producing coherent, contextually appropriate text. These models are trained to predict the next token given all the previous tokens in a sequence, which might seem simple but turns out to be incredibly powerful for natural text generation.\nEncoder-decoder models like T5 bridge both worlds, excelling at sequence-to-sequence tasks like translation and summarization. The text-to-text approach treats all tasks as text generation problems. Need to translate from English to French? The model learns to generate French text given English input. Want to answer a question? The model generates an answer given a question and context.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#pretraining-at-scale-bert-and-beyond",
    "href": "23-nlp.html#pretraining-at-scale-bert-and-beyond",
    "title": "23  Natural Language Processing",
    "section": "23.11 Pretraining at Scale: BERT and Beyond",
    "text": "23.11 Pretraining at Scale: BERT and Beyond\nThe availability of powerful architectures raised a crucial question: how can we best leverage unlabeled text to learn general-purpose representations? BERT (Bidirectional Encoder Representations from Transformers) introduced a pretraining framework that has become the foundation for modern NLP.\nBERT’s key innovation was masked language modeling (MLM), where 15% of tokens are selected for prediction. For each selected token, 80% are replaced with [MASK], 10% with random tokens, and 10% left unchanged. This prevents the model from simply learning to copy tokens when they’re not masked. The loss function only considers predictions for masked positions:\n\\[\\mathcal{L}_{\\text{MLM}} = -\\sum_{m \\in \\mathcal{M}} \\log P(x_m \\mid \\mathbf{x}_{\\backslash \\mathcal{M}})\\]\nBERT combines MLM with next sentence prediction (NSP), which trains the model to understand relationships between sentence pairs. Training examples contain 50% consecutive sentences and 50% randomly paired sentences. The input representation concatenates both sentences with special tokens and segment embeddings to distinguish between them.\nThe scale of BERT pretraining represents a quantum leap from earlier approaches. The original BERT models were trained on a combination of BookCorpus (800 million words from over 11,000 books) and English Wikipedia (2,500 million words). This massive dataset enables the model to see diverse writing styles, topics, and linguistic phenomena. The preprocessing pipeline removes duplicate paragraphs, filters very short or very long sentences, and maintains document boundaries to ensure coherent sentence pairs for NSP.\n\nBERT Architecture and Training Details\nTo understand BERT’s architectural significance, it’s helpful to compare it with its predecessors ELMo and GPT, which represent different approaches to contextual representation learning:\n\n\n\n\n\n\nFigure 23.3: Comparison of ELMo, GPT, and BERT architectures\n\n\n\nELMo uses bidirectional LSTMs with task-specific architectures, requiring custom model design for each application. GPT employs a unidirectional Transformer decoder that processes text left-to-right, making it task-agnostic but unable to see future context. BERT combines the best of both: bidirectional context understanding through Transformer encoders with minimal task-specific modifications (just an output layer). BERT comes in two main configurations that balance model capacity with computational requirements:\n\nBERT configurations and parameters\n\n\n\n\n\n\n\n\n\nModel\nTransformer Layers\nHidden Dimensions\nAttention Heads\nParameters\n\n\n\n\nBERT-Base\n12\n768\n12\n110M\n\n\nBERT-Large\n24\n1024\n16\n340M\n\n\n\nBoth models use a vocabulary of 30,000 WordPiece tokens, learned using a data-driven tokenization algorithm similar to BPE. The maximum sequence length is 512 tokens, though most pretraining uses sequences of 128 tokens to improve efficiency, with only the final 10% of training using full-length sequences.\nThe pretraining procedure involves several techniques to stabilize and accelerate training:\n\nWarm-up Learning Rate: The learning rate increases linearly for the first 10,000 steps to \\(10^{-4}\\), then decreases linearly. This warm-up prevents large gradients early in training when the model is randomly initialized.\nGradient Accumulation: To simulate larger batch sizes on limited hardware, gradients are accumulated over multiple forward passes before updating weights. BERT uses an effective batch size of 256 sequences.\nMixed Precision Training: Using 16-bit floating point for most computations while maintaining 32-bit master weights speeds up training significantly on modern GPUs.\n\n\n\nData Preparation for BERT Pretraining\nThe data preparation pipeline for BERT is surprisingly complex. Starting with raw text, the process involves:\n\n\n\n\n\ngraph LR\n    A[Raw Text] --&gt; B[Document Segmentation]\n    B --&gt; C[Sentence Segmentation]\n    C --&gt; D[WordPiece Tokenization]\n    D --&gt; E[Creating Training Examples]\n    E --&gt; F[Creating TFRecord Files]\n\n\n\n\n\n\nIn the document segmentation stage, text is split into documents, maintaining natural boundaries. For books, this means chapter boundaries; for Wikipedia, article boundaries. The next stage is sentence segmentation, where each document is split into sentences using heuristic rules. Typically, this involves identifying periods followed by whitespace and capital letters, while accounting for exceptions such as abbreviations.\nFollowing sentence segmentation, the text undergoes WordPiece tokenization. This process uses a learned WordPiece vocabulary to break text into subword units, ensuring that unknown words can be represented as sequences of known subwords. Special handling is applied to mark the beginning of words.\nOnce tokenized, the data is organized into training examples. For each example, a target sequence length is sampled from a geometric distribution to introduce variability. Sentences are packed together until the target length is reached. For the next sentence prediction (NSP) task, half of the examples use the actual next segment, while the other half use a randomly selected segment. Masked language modeling (MLM) is applied by masking 15% of tokens, following the 80/10/10 strategy: 80% of the time the token is replaced with [MASK], 10% with a random token, and 10% left unchanged. Special tokens such as [CLS] at the beginning and [SEP] between segments are added, and segment embeddings are created (0 for the first segment, 1 for the second). Sequences are padded to a fixed length using [PAD] tokens.\nFinally, the prepared examples are serialized into TFRecord files for efficient input/output during training. Examples are grouped by sequence length to minimize the amount of padding required.\nThe democratization of pretraining through libraries like Hugging Face Transformers has made it possible for smaller organizations to leverage these powerful techniques, either by fine-tuning existing models or pretraining specialized models for their domains.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#transfer-learning-and-downstream-applications",
    "href": "23-nlp.html#transfer-learning-and-downstream-applications",
    "title": "23  Natural Language Processing",
    "section": "23.12 Transfer Learning and Downstream Applications",
    "text": "23.12 Transfer Learning and Downstream Applications\nThe power of pretrained models lies in their transferability. The landscape of NLP applications can be broadly categorized into two fundamental types based on their input structure and prediction granularity. Sequence-level tasks operate on entire text sequences, producing a single output per input sequence or sequence pair. Token-level tasks make predictions for individual tokens within sequences, such as named entity recognition that identifies person names, locations, and organizations.\n\n\n\n\n\n%%{init: {'themeCSS': '.mermaid svg { background-color: #f8f9fa; }'}}%%\n%%| eval: false\ngraph TB\n    subgraph \"Pretrained Representations\" [\"Pretrained Text Representations\"]\n        P1[Word2Vec/GloVe] \n        P2[BERT/RoBERTa]\n        P3[GPT/T5]\n    end\n    \n    subgraph \"Deep Learning Architectures\" [\"Model Architectures\"]\n        A1[MLPs]\n        A2[CNNs] \n        A3[RNNs]\n        A4[Transformers]\n        A5[Attention Mechanisms]\n    end\n    \n    subgraph \"Sequence-Level Tasks\" [\"Sequence-Level Applications\"]\n        S1[Sentiment Analysis]\n        S2[Text Classification]\n        S3[Natural Language Inference]\n        S4[Semantic Similarity]\n    end\n    \n    subgraph \"Token-Level Tasks\" [\"Token-Level Applications\"]\n        T1[Named Entity Recognition]\n        T2[Part-of-Speech Tagging]\n        T3[Question Answering]\n        T4[Text Summarization]\n    end\n    \n    P1 --&gt; A1\n    P1 --&gt; A2\n    P1 --&gt; A3\n    P2 --&gt; A4\n    P2 --&gt; A5\n    P3 --&gt; A4\n    \n    A1 --&gt; S1\n    A1 --&gt; S3\n    A2 --&gt; S1\n    A2 --&gt; S2\n    A3 --&gt; S1\n    A3 --&gt; S3\n    A4 --&gt; S2\n    A4 --&gt; S3\n    A4 --&gt; T1\n    A4 --&gt; T3\n    A5 --&gt; S3\n    A5 --&gt; T3\n\n\n\n\n\n\nFor sentiment analysis, we add a linear layer on top of the [CLS] token representation and fine-tune on labeled data. Popular datasets include IMDb movie reviews (50K examples) and Stanford Sentiment Treebank (11,855 sentences). Modern models can capture nuanced semantic relationships that traditional lexicon-based approaches miss. For instance, the phrase “not bad” expresses mild approval despite containing the negative word “bad,” while “insanely good” uses typically negative intensity (“insanely”) to convey strong positive sentiment. Fine-tuning typically requires only 2-4 epochs, demonstrating the effectiveness of transfer learning.\nNatural language inference (NLI) determines logical relationships between premise and hypothesis sentences. The Stanford Natural Language Inference corpus contains 570,000 sentence pairs labeled as entailment, contradiction, or neutral. For BERT-based NLI, we concatenate premise and hypothesis with [SEP] tokens and classify using the [CLS] representation. This allows the model to leverage cross-attention to identify which words and phrases support or contradict the proposed logical relationship.\nToken-level tasks like named entity recognition classify each token independently. Common datasets include CoNLL-2003 (English and German entities) and OntoNotes 5.0 (18 entity types). The BIO tagging scheme marks entity boundaries: B-PER for beginning of person names, I-PER for inside, and O for outside any entity. For example, in “Apple Inc. was founded by Steve Jobs”, “Apple” would be tagged B-ORG, “Inc.” as I-ORG, “Steve” as B-PER, and “Jobs” as I-PER.\nQuestion answering presents unique challenges. The SQuAD dataset contains 100,000+ questions where answers are text spans from Wikipedia articles. BERT approaches this by predicting start and end positions independently, with the final answer span selected to maximize the product of start and end probabilities subject to length constraints. For a question like “Who founded Apple?”, the model identifies the span “Steve Jobs” in the context passage.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#model-compression-and-efficiency",
    "href": "23-nlp.html#model-compression-and-efficiency",
    "title": "23  Natural Language Processing",
    "section": "23.13 Model Compression and Efficiency",
    "text": "23.13 Model Compression and Efficiency\nWhile large pretrained models achieve impressive performance, their computational requirements limit deployment. Knowledge distillation trains a small “student” model to mimic a large “teacher” model through a combined loss:\n\\[\\mathcal{L}_{\\text{distill}} = \\alpha \\mathcal{L}_{\\text{task}} + (1-\\alpha) \\text{KL}(p_{\\text{teacher}} \\| p_{\\text{student}})\\]\nDistilBERT achieves 97% of BERT’s performance with 40% fewer parameters and 60% faster inference. Quantization reduces numerical precision from 32-bit to 8-bit or even lower, while pruning removes connections below a magnitude threshold. These techniques can reduce model size by an order of magnitude with minimal performance degradation.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#theoretical-perspectives-and-future-directions",
    "href": "23-nlp.html#theoretical-perspectives-and-future-directions",
    "title": "23  Natural Language Processing",
    "section": "23.14 Theoretical Perspectives and Future Directions",
    "text": "23.14 Theoretical Perspectives and Future Directions\nThe success of language models connects to several theoretical frameworks. Transformers are universal approximators for sequence-to-sequence functions—given sufficient capacity, they can approximate any continuous function to arbitrary precision. The self-attention mechanism provides an inductive bias well-suited to capturing long-range dependencies.\nDespite having hundreds of millions of parameters, these models generalize remarkably well. This connects to implicit regularization in overparameterized models, where gradient descent dynamics bias toward solutions with good generalization properties. Language models automatically learn hierarchical features: early layers capture syntax and morphology, middle layers semantic relationships, and later layers task-specific abstractions.\nYet significant challenges remain. Models struggle with compositional generalization—understanding “red car” and “blue house” doesn’t guarantee understanding “red house” if that combination is rare in training. The sample efficiency gap is stark: a child masters basic grammar from thousands of examples, while BERT requires billions—suggesting fundamental differences between neural network optimization and human language acquisition.\nInterpretability poses ongoing challenges. While attention visualizations provide some insights, we lack principled methods for understanding distributed representations across hundreds of layers and attention heads. Future directions include multimodal understanding (integrating text with vision and speech), more efficient architectures that maintain performance while reducing computational requirements, and developing theoretical frameworks to predict and understand model behavior.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "23-nlp.html#conclusion",
    "href": "23-nlp.html#conclusion",
    "title": "23  Natural Language Processing",
    "section": "23.15 Conclusion",
    "text": "23.15 Conclusion\nThe journey from symbolic manipulation to neural language understanding represents one of the great success stories of modern artificial intelligence. By reconceptualizing language as geometry in high-dimensional spaces, leveraging self-supervision at scale, and developing powerful architectural innovations like transformers, the field has achieved capabilities that seemed like science fiction just a decade ago.\nThe mathematical frameworks developed—from distributional semantics to attention mechanisms—provide not just engineering tools but lenses through which to examine fundamental questions about meaning and understanding. As these systems become more capable and widely deployed, understanding their theoretical foundations, practical limitations, and societal implications becomes ever more critical.\nThe techniques discussed in this chapter—word embeddings, contextual representations, pretraining, and fine-tuning—form the foundation of modern NLP systems. Yet despite impressive engineering achievements, we’ve only begun to scratch the surface of true language understanding. The rapid progress offers both tremendous opportunities and sobering responsibilities. The most exciting chapters in this story are yet to be written.\n\n\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.\n\n\nMnih, Volodymyr, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. “Recurrent Models of Visual Attention.” Advances in Neural Information Processing Systems 27: 2204–12.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. “Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "24-llm.html",
    "href": "24-llm.html",
    "title": "24  Large Language Models",
    "section": "",
    "text": "24.1 Autoregressive Generation: Adding One Word at a Time\nLarge Language Models (LLMs) have emerged as a defining technology in artificial intelligence. While their aptitude for writing code is well-known, they also translate languages, analyze legal documents, and converse with fluency that feels human. This chapter explores the mechanisms underlying these capabilities.\nLLMs depend on the Transformer architecture, which introduced the attention mechanism. Attention allows the model to dynamically weigh different words within a sequence, capturing long-range dependencies that previous architectures missed. While the mathematical foundations are detailed in Chapter 23, this chapter focuses on operational logic: from Transformer mechanics to emergent reasoning capabilities.\nThe most visible capability of LLMs is text generation—specifically, the ability to produce coherent, contextually relevant continuations from a given prompt. This phenomenon relies on a process known as autoregressive modeling. Fundamentally, an LLM is a probability distribution over sequences of text, trained to predict the next token given a preceding context. A token serves as the atomic unit of processing; depending on the tokenization schema, it may represent a full word, a subword unit (like “-ing”), or a single character. Given the prompt \\(Q\\) and a sequence of tokens \\(x_1, x_2, \\ldots, x_t\\), the model samples the next token \\(x_{t+1}\\) from the conditional distribution \\[\nx_{t+1} \\sim p(x_{t+1} | Q, x_1, x_2, \\ldots, x_t).\n\\] The conditional varaibels \\(Q, x_1, x_2, \\ldots, x_t\\) are called the context. In this case, the prompt might include the user’s question and documents “attached” to the question. To illustrate this mechanism in practice, we will use the SmolLM2 model. We begin by loading the model and its associated tokenizer—the component responsible for translating raw text into the discrete inputs the model understands.\nsys.path.append('./code')\n# Import our custom functions\nfrom llm_chapter import (ask_smol_lm, get_next_word_suggestions, generate_text_step_by_step)\n\nlocal_cache_dir = \"./models_cache\"  # or use absolute path like \"/Users/your_username/ai_models\"\n# Create directory if it doesn't exist\nPath(local_cache_dir).mkdir(parents=True, exist_ok=True)\n# Load model with custom cache directory\nmodel_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n# This will download once and store in your specified directory\ntokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=local_cache_dir)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,cache_dir=local_cache_dir,device_map=\"auto\",torch_dtype=torch.float16,low_cpu_mem_usage=True)\ndevice = model.device\n# print(\"Model loaded. Using device:\", device)\nConsider the text The best thing about AI is its ability to. Imagine analyzing billions of pages of human-written text—such as those found on the web or in digitized books—and identifying all instances of this text to determine what word most commonly comes next. While an LLM doesn’t directly search for literal matches, it evaluates semantic and contextual similarities to produce a ranked list of possible next words along with their associated probabilities.\n# Get next word suggestions for a given text\ninitial_text = \"The best thing about AI is its ability to\"\nsuggestions = get_next_word_suggestions(initial_text, model, tokenizer, top_k=5)\nprint(f\"Next word suggestions for '{initial_text}':\")\n## Next word suggestions for 'The best thing about AI is its ability to':\nfor i, (word, prob) in enumerate(suggestions):\n    print(f\"  {i+1}. '{word}' (prob: {prob:.3f})\")\n##   1. ' learn' (prob: 0.623)\n##   2. ' help' (prob: 0.117)\n##   3. ' augment' (prob: 0.100)\n##   4. ' analyze' (prob: 0.086)\n##   5. ' process' (prob: 0.074)\nflowchart\n    A[\"learn&lt;br/&gt;(0.620)\"]:::high-prob\n    B[\"help&lt;br/&gt;(0.120)\"]:::med-high-prob\n    C[\"augment&lt;br/&gt;(0.101)\"]:::med-prob\n    D[\"analyze&lt;br/&gt;(0.085)\"]:::med-low-prob\n    E[\"process&lt;br/&gt;(0.074)\"]:::low-prob\n    \n    classDef high-prob fill:#4CAF50,stroke:#2E7D32,stroke-width:2px,color:#fff\n    classDef med-high-prob fill:#8BC34A,stroke:#558B2F,stroke-width:2px,color:#000\n    classDef med-prob fill:#FFEB3B,stroke:#F57F17,stroke-width:2px,color:#000\n    classDef med-low-prob fill:#FF9800,stroke:#E65100,stroke-width:2px,color:#fff\n    classDef low-prob fill:#F44336,stroke:#B71C1C,stroke-width:2px,color:#fff\n\n\n Next word predictions with probabilities, color-coded from green (high probability) to red (low probability)\nIf we look at the probabilities (on the log scale) of the next 10 words.\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nsuggestions = get_next_word_suggestions(initial_text, model, tokenizer, top_k=10)\nindices = list(range(len(suggestions)))\nwords = [s[0] for s in suggestions]\nprobabilities = [s[1] for s in suggestions]\n# Plotting the next word suggestions with their log probabilities\nplt.plot(indices, np.log10(probabilities), marker='o', linestyle='-', color='skyblue')\nplt.xticks(indices, words, rotation=45, ha='right');\nplt.xlabel('Next Word Suggestions')\nplt.ylabel('Log Probability')\nplt.title('Next Word Suggestions with Log Probabilities')\nplt.grid()\nplt.tight_layout()\nplt.show()\nWe can see that the probabilities of each next word decay exponentially with rank (outside of the top word ‘learn’). This pattern is reminiscent of Zipf’s law, observed by linguist George Kingsley Zipf in the 1930s, which states that the frequency of a word in natural language is inversely proportional to its rank in the frequency table. While Zipf’s law describes unconditional word frequencies across a corpus, the probability distribution over next tokens given a specific context exhibits a similar heavy-tailed structure: a few continuations are highly probable, while most are rare.\nOne might assume the model should always select the next token with the highest probability. However, this approach is leads to generated text that is often lacks creativity and can become repetitive. To address this, randomness is introduced into the selection process. By occasionally choosing lower-ranked tokens, the model can produce more varied and engaging text.\nThis randomness means that using the same prompt multiple times will likely yield different outputs. A parameter called temperature controls the degree of randomness in token selection. Empirically, a temperature value of around 0.8 often strikes a good balance between coherence and creativity for text generation tasks. The term “temperature” originates from statistical physics, where it controls the spread of the Boltzmann distribution over energy states; here, it analogously controls the spread of the probability distribution over tokens (see Equation 24.1 in the Distillation section below for the mathematical formulation). A temperature of 0 would always select the highest-probability token (deterministic), while higher temperatures flatten the distribution, making less probable tokens more likely to be selected.\nThe following example illustrates the iterative process where the model selects the word with the highest probability at each step:\n# Let's start with a simple prompt\ninitial_text = \"The best thing about AI is its ability to\"\nprint(f\"Initial text: '{initial_text}'\")\n## Initial text: 'The best thing about AI is its ability to'\n# Generate text step by step\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer, \nnum_steps=10, temperature=1.0, sample=False, print_progress=False)\nprint(\"Generated text:\")\n## Generated text:\nprint(textwrap.fill(generated_text, width=60))\n## The best thing about AI is its ability to learn and adapt.\n## It can analyze vast amounts of\nIn this example, we always select the most probable next token, which leads to a coherent but somewhat predictable continuation. The model generates text by repeatedly applying this process, building on the context provided by the previous tokens.\nNow we will run our LLM generation process for longer and sample words with probabilities calculated based on the temperature parameter. We will use a temperature of 0.8, which is often a good choice for generating coherent text without being too repetitive.\n# Fix the seed for reproducibility\ntorch.manual_seed(8);\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer,\n num_steps=60, temperature=0.8, sample=True,print_progress=False)\nprint(\"Generated text:\")\n## Generated text:\nprint(textwrap.fill(generated_text, width=60))\n## The best thing about AI is its ability to interact precisely\n## with buildings, including piping [Ethernet be Definition\n## requires Qualities]-was way k)-ay -- will keeping order for\n## from few holding built themselves sit'd friendling years\n## answer shows data communication \"states general rooms\n## developers warning windows cybersecurity Virtual interview\n## no hassle put Linux voice ordering popular podcast dinner\n## English\nWe can see that the model went off track rather quickly, generating meaningless phrases that don’t follow the initial context. Now let’s try a higher temperature of 1.2.\ntorch.manual_seed(8);\ngenerated_text = generate_text_step_by_step(initial_text, model, tokenizer,\n num_steps=60, temperature=1.2, sample=True,print_progress=False)\nprint(\"Generated text:\")\n## Generated text:\nprint(textwrap.fill(generated_text, width=60))\n## The best thing about AI is its ability to interact precisely\n## upwards             reffwd [EUMaiSTAVEQל]- AI achieves\n## kawakay -- sporic order for accuracy round trips built hard\n## sitto functions thruts generate squancers emerge good\n## simasts tailrajs windows finish triippities siplex\n## /&gt;node_{thread----------------mem\nHere the generation process went “off track” even quicker, it even entroduced characters from a different language. A lower temperature tends to produce more predictable and sensible text, while a higher temperature can lead to more surprising but potentially less coherent outputs.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#building-intuition-character-level-text-generation",
    "href": "24-llm.html#building-intuition-character-level-text-generation",
    "title": "24  Large Language Models",
    "section": "24.2 Building Intuition: Character-Level Text Generation",
    "text": "24.2 Building Intuition: Character-Level Text Generation\nBefore diving deeper into how modern LLMs work, it helps to understand text generation at its most fundamental level: one character at a time. While LLMs operate on tokens (typically subwords), examining character-level patterns reveals the core insight behind statistical language modeling. We’ll start by counting letter frequencies in a Wikipedia article about cats, then see how these simple statistics can generate text.\n\n# Download wikipedia article on \"Cat\"\nimport requests\nurl = \"https://en.wikipedia.org/wiki/Cat\"\nresponse = requests.get(url)\ncat_text = response.text\n# Extract text from HTML\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(cat_text, 'html.parser')\ncat_text = soup.get_text()\n\nNow let’s count letter frequencies in the text and plot the letter frequencies for the first 26 letters\n\nfrom collections import Counter\nletter_counts = Counter(c.lower() for c in cat_text if c.isalpha())\n# Sort by frequency\nsorted_letter_counts = sorted(letter_counts.items(), key=lambda x: x[1], reverse=True)\n\n\n\nCode\nimport matplotlib.pyplot as plt\nsorted_letter_counts = sorted_letter_counts[:26];  # Limit to top 26 letters\nletters, counts = zip(*sorted_letter_counts);\nplt.bar(letters, counts, color='skyblue')\nplt.xlabel('Letters')\nplt.ylabel('Frequency')\nplt.title('Letter Frequencies in Wikipedia Article on Cats')\nplt.xticks(rotation=45);\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIf we try to generate the text one letter at a time\n\nimport numpy as np\n# Generate text one letter at a time by sampling from the letter frequencies\ncounts = np.array(counts)/sum(counts)  # Normalize counts to probabilities\nimport random\ngentext = random.choices(letters, weights=counts, k=20)\nprint(\"Generated letters:\", ''.join(gentext))\n## Generated letters: cpluesiiaonnnbsotihd\n\nWhat if we do bi-grams, i.e. pairs of letters? We can do this by counting the frequencies of each pair of letters in the text. This will give us a sense of how often each pair of letters appears in the text, which is a good starting point for understanding how the model generates text.\n\n\nCode\nfrom collections import defaultdict\nbigram_counts = defaultdict(int)\nfor i in range(len(cat_text) - 1):\n    if cat_text[i].isalpha() and cat_text[i + 1].isalpha():\n        a, b = cat_text[i].lower(), cat_text[i + 1].lower()\n        # Only process standard English letters (a-z)\n        if 'a' &lt;= a &lt;= 'z' and 'a' &lt;= b &lt;= 'z':\n            bigram = (a, b)\n            bigram_counts[bigram] += 1\n\n# Sort by frequency\nsorted_bigram_counts = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Plot the heatmap of bigram frequencies\nimport seaborn as sns\nbigram_matrix = np.zeros((26, 26))\nfor (a, b), count in sorted_bigram_counts:\n    bigram_matrix[ord(a) - ord('a'), ord(b) - ord('a')] = count\n\nsns.heatmap(bigram_matrix, cmap='Blues')\nplt.xlabel('Second Letter')\nplt.ylabel('First Letter')\nplt.title('Bigram Frequencies in Wikipedia Article on Cats')\nplt.xticks(ticks=np.arange(26) + 0.5, labels=[chr(i + ord('a')) for i in range(26)], rotation=45);\nplt.yticks(ticks=np.arange(26) + 0.5, labels=[chr(i + ord('a')) for i in range(26)], rotation=0);\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThis will take us one step closer to how LLMs generate text. We used Lev Tolstoy’s “War and Peace” novel to estimate the, 2-grams, 3-grams, 4-grams, and 5-grams letter frequencies and to generate text based on those models. The results are shown below:\n2-gram: ton w mer. y the ly im, in peerthayice waig trr. w tume shanite tem.\n3-gram: the ovna gotionviculy on his sly. shoutessixeemy, he thed ashe\n4-gram: the with ger frence of duke in me, but of little. progomind some later\n5-gram: the replace, and of the did natasha's attacket, and aside. he comparte,\nWe used the nltk package (Bird, Klein, and Loper 2009) to estimate the letter frequencies from Tolstoy’s novel and generate text based on those models. The results show that even with simple letter-based models, we can generate text that resembles natural language, albeit with nonsensical phrases. As the n-gram order increases, the generated text becomes more coherent—the 5-gram output even captures character names like “Natasha.” This progression illustrates the core principle that underlies all language models: context matters, and more context leads to better predictions.\nHowever, LLMs have much larger context windows, meaning they can consider much longer sequences of text when generating the next token. Modern models such as Gemini 3 Pro use context windows of up to 1 million tokens—approximately the size of Leo Tolstoy’s “War and Peace” novel. However, if you try to use a simple counting method (as we did with n-grams), you will quickly run into the problem of combinatorial explosion. For example, if we try to estimate 10-grams letter frequencies, we will have to count \\(26^{10}\\) (over 141 trillion) combinations of letters. If we use word-based n-grams, the problem is even worse, as the number of common words in the English language is estimated to be around 40,000. This means that the number of possible 2-grams is 1.6 billion, for 3-grams is 64 trillion, and for 4-grams is 2.6 quadrillion. By the time we get to a typical question people ask when using AI chats with 20 words, the number of possibilities is larger than the number of particles in the universe. The challenge lies in the fact that the total amount of English text ever written is vastly insufficient to accurately estimate these probabilities, and this is where LLMs come in. They use neural networks to “compress” the input context into dense vector embeddings—distributed representations that capture semantic meaning—and “interpolate” the probabilities of the next token. This allows them to estimate probabilities for sequences they have never seen before and generate text that is coherent and contextually relevant. The main component of these neural networks is the transformer architecture.\nThe first step an LLM takes to “compress” the input is applying the attention mechanism. This concept is similar to convolutional neural networks (CNNs) used in computer vision, where the model focuses on different parts of the input image. In LLMs, attention allows the model to focus on different parts of the input text when generating the next token (see Chapter 23 for the mathematical details).",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#the-scale-approach-how-bigger-became-better",
    "href": "24-llm.html#the-scale-approach-how-bigger-became-better",
    "title": "24  Large Language Models",
    "section": "24.3 The Scale Approach: How Bigger Became Better",
    "text": "24.3 The Scale Approach: How Bigger Became Better\nFor decades the high-quality data analysis required finding a parsimonious model. Meaning the model that is as small as possible, but still able to capture the essential patterns in the data. This is the approach of traditional statistical learning. The deep learning models broke this trend and were shown to work very well then number of parameters is large, often you would have more parameters than data points. We discussed this phenomenon in Section 19.8.\nThis scaling behavior has led to exponential growth in model sizes. GPT-1, released in 2018 with 117 million parameters, was already considered large for its time. GPT-2, with 1.5 billion parameters, was initially deemed too dangerous to release publicly. GPT-3’s 175 billion parameters represented a quantum leap.\n\n\n\n\n\n\nFigure 24.1: Scaling Behavior of LLMs\n\n\n\nArchitectural innovations like Mixture of Experts (MoE) models allow for scaling model capacity further without proportionally increasing computational requirements, by activating only a subset of parameters for processing each token.\nBeyond scaling up parameters, new architectures are emerging. Multimodal transformers are beginning to bridge the gap between text, images, audio, and other modalities, creating systems that can understand and generate content across multiple forms of media. These systems process diverse inputs within a single unified model, enabling rich interactions like chatting about an image or generating music from text descriptions.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#choosing-the-right-model-for-your-application",
    "href": "24-llm.html#choosing-the-right-model-for-your-application",
    "title": "24  Large Language Models",
    "section": "24.4 Choosing the Right Model for Your Application",
    "text": "24.4 Choosing the Right Model for Your Application\nAlthough, the bigger models are a go-to approach, it does not always mean better choice. When choosing an appropriate model for a problem at hand, you need to consider several factors, such ability to upload your data to the cloud provider, cost of the model, the performance of the model on your specific task and latency requirements. Let’s discuss those factors in more detail.\nUploading your data to the model hosted by the cloud provider is a common practice. However, sometimes you are restricted by security policies, regulations (like HIPAA or GDPR), or massive data volumes. Then on-premises deployment: when you host the model (e.g., an open-source LLM like Llama 3 or Mistral) on your own hardware is one option. This gives you total control over the data lifecycle but requires significant capital expenditure (CapEx) for GPUs and maintenance. Another option that became recently available is to use isolated environments on the cloud provider. While the hardware is still theirs, the network is logically separated from the public internet, and data can be transferred over dedicated physical lines to avoid the public web. Typically, in this scenario model provider also offers zero data retention, which is a policy that ensures that the user data is deleted after it is used for inference and is not used for training or other purposes.\nThe cost of the model can be a prohibitive factor. In many commercial applications companies would be loosing money if they are using expensive “out-of-the-box” models. Therefore, they need to develop their own models or use open-source models that are more cost-effective. In this case you option is yet again to select a smaller model that you can host on your own hardware and fine-tune on your own data. We will discuss fine-tuning in more detail in Section 24.5.\nThe performance of an existing model on your specific task is yet another factor. Typically, the bigger the model, the better the performance, but smaller models can be better (or good enough) for a particular use case. Size tiers offer different trade-offs: very small models (around 3 billion parameters or fewer) are fast and efficient and can often run on consumer hardware for simpler tasks like basic text classification or domain-specific chatbots; medium-sized models (7 to 30 billion parameters) are often the sweet spot, offering stronger performance with manageable compute; and large models (30 billion parameters or more) provide the best general performance and can show emergent capabilities, but typically require specialized hardware and higher cost. Beyond size, specialized variants matter too: code models are trained for software tasks (including “fill-in-the-middle” editing), multilingual models target many languages, and domain-specific models are tuned to specialized corpora. In practice, constraints like GPU memory, inference speed, cost (cloud APIs versus self-hosting), latency, and privacy requirements often drive the final choice.\nAlthough, typically, there is a trade-off between the size and the performance, techniques such as knowledge distillation, fine-tuning, and quantization can help you achieve good performance on a complex task with a smaller model. We discuss these techniques in detail in Section 24.5. A notable example occurred in late 2025 when Google’s Gemini 3 Flash—a distilled model designed for efficiency—outperformed the flagship Gemini 3 Pro on coding benchmarks, demonstrating that focused optimization can matter more than raw parameter count for specific tasks.\nFinally, the latency requirement is a key factor in applications such as speech bots (e.g., Alexa, Google Home) and real-time applications (e.g., trading, finance). In these scenarios, the Time to First Token (TTFT)—the delay before the model starts outputting its response—is often more critical than the overall throughput. Several architectural and algorithmic techniques have been developed to minimize this delay.\nOne such technique is Speculative Decoding (Leviathan, Kalman, and Matias 2023; Chen et al. 2023), which addresses the sequential bottleneck of autoregressive generation. Since generating each token requires a full forward pass of a large model, the process is inherently slow. Speculative decoding uses a much smaller, faster “draft” model to predict several potential next tokens in a single step. The larger “target” model then verifies these tokens in parallel. If the target model agrees with the draft, multiple tokens are accepted at once; if not, only the incorrect ones are discarded and regenerated. This approach can achieve significant speedups (often 2-3x) without any loss in accuracy.\n\n\n\n\n\nsequenceDiagram\n    participant D as Draft Model (Small)\n    participant T as Target Model (Large)\n    participant O as Output\n    \n    Note over D, T: Step 1: Draft model generates K tokens\n    D-&gt;&gt;D: Predict x_1, x_2, ..., x_K\n    D-&gt;&gt;T: Send draft tokens\n    \n    Note over T: Step 2: Target model verifies in parallel\n    T-&gt;&gt;T: Compute probabilities for x_1, ..., x_K\n    \n    Note over T, O: Step 3: Accept/Reject\n    T--&gt;&gt;O: Output accepted tokens + 1 new token\n\n\n Speculative Decoding: A small draft model proposes tokens that a larger target model verifies in parallel, accelerating the generation process. \n\n\n\nAnother critical optimization is KV Caching (Key-Value Caching). During autoregressive generation, the model repeatedly attends to the same previous tokens. Rather than recomputing the keys and values for these tokens at every step, the model stores them in memory. This reduces the computational cost of generating each subsequent token from \\(O(n^2)\\) to \\(O(n)\\) relative to the current sequence length.\nFor real-time streaming applications, models can begin processing input before waiting until the entire query is available. This is known as Streaming Prefill or Incremental Prefill. Instead of waiting for the entire user query to be completed, the model starts the “prefill” phase—processing the input and building the KV cache—on chunks of the input as they arrive. This is particularly useful for voice-activated systems where the beginning of a command can be processed while the user is still speaking.\nTo improve efficiency in multi-user environments, Continuous Batching (Yu et al. 2022) allows the server to start processing new requests immediately, even if other requests in the same batch are already in the middle of generation. Unlike static batching, which waits for all sequences in a batch to finish before starting a new one, continuous batching dynamically inserts and removes requests, significantly increasing throughput and reducing waiting times.\nFinally, Quantization and Model Distillation (discussed in detail in Section 24.5) remain the most common ways to reduce latency by simplifying the model itself. Quantization reduces the numerical precision of model weights (e.g., from 16-bit to 4-bit), allowing for faster arithmetic and smaller memory footprints, while distillation creates smaller student models that “mimic” the reasoning of larger teachers.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#sec-distillation",
    "href": "24-llm.html#sec-distillation",
    "title": "24  Large Language Models",
    "section": "24.5 Distillation, Fine-tuning and Quantization",
    "text": "24.5 Distillation, Fine-tuning and Quantization\nWhile scaling up model parameters has driven remarkable advances, practical deployment often demands the opposite: smaller, faster, more efficient models. Three complementary techniques—distillation, fine-tuning, and quantization—bridge the gap between research capabilities and production requirements. These techniques leverage existing foundation models for specific needs at a fraction of the cost of training from scratch.\n\nQuantization\nQuantization reduces the numerical precision of model weights and activations, typically from 32-bit floating-point to 8-bit integers or even lower. This compression reduces memory bandwidth requirements and enables faster arithmetic operations on supported hardware. Modern quantization techniques like GPTQ and AWQ can reduce model size by 4× with minimal accuracy degradation, enabling larger models to run on consumer hardware or significantly reducing inference costs in production deployments.\nThe mathematics of quantization involves mapping continuous values to a discrete set of levels. For a weight \\(w\\) with range \\([w_{min}, w_{max}]\\), 8-bit quantization maps it to one of 256 integer values: \\[\nw_q = \\text{round}\\left(\\frac{w - w_{min}}{w_{max} - w_{min}} \\times 255\\right)\n\\]\nMore sophisticated approaches like post-training quantization (PTQ) analyze calibration data to find optimal scaling factors, while quantization-aware training (QAT) incorporates quantization effects during training to minimize accuracy loss.\n\n\nFine-tuning\nFine-tuning adapts a pre-trained model to specific tasks or domains by continuing training on specialized data. This process leverages the general knowledge encoded during pre-training while teaching the model task-specific patterns and vocabulary. Pre-trained language models possess extensive knowledge from their training, but they are not optimized for any particular task out of the box. While a general-purpose LLM can generate logically valid responses, those responses may not align with the specific requirements of a given application. For instance, a fintech company might need a model that interprets balance sheets or analyzes regulatory filings with domain-specific precision, while a healthcare application requires understanding of medical terminology and compliance with clinical guidelines.\nFine-tuning offers several key advantages:\n\nData efficiency: Effective fine-tuning can be achieved with thousands rather than millions of examples.\nCost efficiency: Reusing pre-trained models reduces computational cost compared to training from scratch.\nVersatility: The same pre-trained model can be fine-tuned for multiple applications across domains.\nImproved performance: Fine-tuned models learn task-specific patterns critical for their target applications.\n\nThere are several approaches to fine-tuning, each with different trade-offs between performance, resource requirements, and flexibility.\nFull fine-tuning updates all model parameters, resulting in a brand-new version of the model optimized for the specific task. This approach offers maximum flexibility in adapting the model, as it can learn features and representations across all layers of the architecture. However, full fine-tuning requires significant compute resources and memory, and risks catastrophic forgetting—where the model loses its general capabilities as it specializes for the new task.\nParameter-efficient fine-tuning (PEFT) methods address these limitations by modifying only a small subset of parameters while freezing most of the model. The key idea is to add task-specific layers on top of the frozen base model, allowing fundamental language understanding to remain unaffected. PEFT techniques like LoRA (Low-Rank Adaptation) train small adapter modules, dramatically reducing memory and compute requirements while maintaining performance. LoRA works by decomposing weight updates into low-rank matrices: \\[\nW' = W + BA\n\\] where \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) with rank \\(r \\ll \\min(d,k)\\), typically \\(r = 8\\) to \\(64\\). This decomposition means training far fewer parameters while achieving comparable results to full fine-tuning.\nInstruction fine-tuning trains the model with examples explicitly showing how it should respond to different queries. The labeled data consists of input-output pairs where inputs convey the desired behavior and outputs represent the expected responses. By exposing the model to a diverse range of instructions and appropriate responses, it learns to generalize across different types of tasks and follow user intentions more reliably.\nSequential fine-tuning gradually adapts a model to increasingly specialized tasks. For example, a general AI model could first be fine-tuned for medical terminology, then refined further for pediatric cardiology. This progressive specialization allows the model to build upon previously learned knowledge.\nMulti-task learning trains the model on datasets containing instructions for various tasks over multiple training cycles. The model learns to balance different objectives without forgetting earlier ones, creating a more versatile system capable of handling diverse requests.\nFine-tuning is often combined with other post-training techniques such as Reinforcement Learning from Human Feedback (RLHF) to align model behavior with human preferences. These advanced techniques are discussed in detail in Section 24.7.\n\n\nModel Distillation: Knowledge Transfer\nAs we push the boundaries of model performance with larger parameter counts and complex post-training reasoning chains, a practical challenge emerges: deployment efficiency. Training and deploying models with hundreds of billions of parameters requires considerable computational power, specialized hardware, and substantial energy consumption. Large models also demand significant memory to store their weights and process inputs, making them impractical for many real-world applications. While a massive 175B+ parameter model might offer superior reasoning, running it for every user query is often prohibitively expensive and slow.\nModel Distillation addresses this challenge by transferring knowledge from a large, complex “teacher” model to a smaller, more efficient “student” model. The student learns to replicate the teacher’s behavior on specific tasks, achieving similar results while being significantly smaller and faster. Distilled models can typically achieve 2–8× faster inference compared to their teacher models, depending on architecture and hardware optimization.\nA striking example of distillation’s potential emerged in late 2025, and was documented in Virtu article. Google’s Gemini 3 Flash—a model explicitly designed for speed and cost efficiency—outperformed the flagship Gemini 3 Pro on the SWE-bench coding benchmark, achieving 78% compared to Pro’s 76.2%. This inversion of the expected hierarchy was accompanied by widespread reports of Pro exhibiting critical issues: deleting code, losing context mid-conversation, and failing to maintain logical coherence across extended interactions. The phenomenon has been attributed to knowledge distillation, where the compression process that created Flash from Pro inadvertently preserved and even sharpened the most effective coding reasoning pathways while discarding less relevant capabilities. Flash also demonstrated advantages in speed (roughly three times faster) and cost (about 70% cheaper), leading major development tools to adopt it as their preferred model for coding assistance. This case illustrates a broader principle: architectural efficiency and focused optimization can matter more than raw parameter count for specific tasks.\nModel distillation, formalized by Hinton, Vinyals, and Dean (2015) in their seminal paper “Distilling the Knowledge in a Neural Network,” is based on a Teacher-Student architecture. The core insight is that a large, pre-trained teacher model has learned much more than just the final answers—it has learned a rich internal representation of the data structure.\nWhen a standard model trains on a “hard” label (e.g., identifying an image as “Dog”), it is penalized if it outputs anything other than 100% confidence in that class. However, a sophisticated teacher model might output probabilities like: Dog: 0.90, Cat: 0.09, Car: 0.0001. The fact that the model thinks the image is 9% likely to be a “Cat” and almost impossible to be a “Car” contains valuable information—it tells us that this specific dog looks somewhat like a cat (perhaps it’s fluffy or small). This similarity information, often called dark knowledge, is lost if we train only on the final hard label.\nDistillation trains a smaller student model to mimic these soft targets (the probability distributions) produced by the teacher. By doing so, the student learns how the teacher generalizes, not just what the teacher predicts. Thanks to the level of detail provided in soft targets, the student model can achieve high performance with a smaller amount of data than the original teacher required.\nDifferent distillation approaches focus on transferring different aspects of the teacher’s knowledge:\nResponse-based distillation focuses on having the student mimic the final output layer of the teacher model. The student learns to imitate the teacher’s predictions by minimizing a distillation loss, ensuring it captures the nuanced information present in the teacher’s outputs. This is the most common form of distillation.\nFeature-based distillation leverages the internal representations or features learned by the teacher in its intermediate layers. Rather than focusing solely on final outputs, this approach encourages the student to match the teacher’s internal activations, learning how the teacher processes information at multiple levels.\nRelation-based distillation captures and transfers the relationship knowledge between data samples and layers within the neural network. This method complements response-based approaches by encoding how different inputs relate to each other in the teacher’s representation space.\nTo expose soft probabilities effectively, distillation uses a modified Softmax function with a parameter called Temperature (\\(T\\)). Standard Softmax (\\(T=1\\)) tends to push probabilities towards 0 or 1, hiding the smaller details. Raising \\(T\\) “softens” the distribution, making smaller class probabilities more prominent and easier for the student to learn from.\nThe probability \\(q_i\\) for class \\(i\\) is calculated as:\n\\[\nq_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}\n\\tag{24.1}\\]\nwhere \\(z_i\\) are the logits (raw outputs) of the model.\nThe training objective for the student model typically combines two loss functions:\n\nDistillation Loss (Soft Loss): The Kullback-Leibler (KL) Divergence between the student’s soft predictions and the teacher’s soft targets (both computed at temperature \\(T\\)). KL Divergence measures how one probability distribution differs from a reference distribution.\nStudent Loss (Hard Loss): The standard Cross-Entropy loss between the student’s predictions (at \\(T=1\\)) and the actual ground-truth labels.\n\n\\[ L = \\alpha L_{soft} + (1-\\alpha) L_{hard} \\]\nThis combined objective forces the student to be accurate on the data (hard loss) while also mimicking the generalization behavior of the teacher (soft loss). The weighting parameter \\(\\alpha\\) balances these two objectives.\nThe following diagram illustrates the distillation pipeline, where the student learns from both the dataset and the teacher’s soft outputs.\n\n\n\n\n\n\nFigure 24.2: Knowledge Distillation Framework Diagram illustrating the process of transferring knowledge from a complex teacher model to a simpler student model.\n\n\n\nSeveral schemes have been developed to facilitate the distillation process:\nOffline distillation refers to the traditional approach where the teacher model is trained first, then the student model is trained separately using the soft labels generated by the teacher. This is the most straightforward approach when a well-trained teacher is available.\nOnline distillation is used when a large pre-trained teacher is not available for a given task, or when the teacher model is so large that there is insufficient storage or processing capacity. In this approach, the teacher and student models are trained simultaneously, with the student learning from the teacher dynamically during training.\nSelf-distillation is a variant where a single model acts as both teacher and student. Knowledge is transferred from deeper layers of the network to shallower layers of the same network. This technique can improve model performance and regularization without requiring a separate teacher model. Studies have shown that self-distillation can maintain up to 95% of the teacher’s accuracy while drastically reducing model size and inference time.\n\nFrom Theory to Practice\nSince the work of Hinton, Vinyals, and Dean (2015), model distillation has evolved from a theoretical framework into a critical component of modern machine learning infrastructure. The efficacy of this approach was notably demonstrated in the domain of Natural Language Processing by Sanh et al. (2019), whose DistilBERT model retained approximately 97% of the original BERT performance while reducing parameters by 40% and increasing inference speed by 60%. DistilBERT was specifically created to address challenges associated with large pre-trained language models, focusing on computational and memory efficiency.\nIn contemporary production environments, distillation serves as the bridge between massive “reasoning” models and efficient deployment. Advanced pipelines utilize a teacher-student paradigm where a large-scale model (e.g., a 200B+ parameter reasoning model) generates synthetic data or soft targets. These outputs are subsequently used to fine-tune smaller, cost-effective models (e.g., 8B parameter models) for specific downstream tasks. This methodology, often aligned with techniques like distilling step-by-step (Hsieh et al. 2023), allows for the deployment of models that exhibit high-level reasoning capabilities with significantly reduced computational overhead.\nFurthermore, distillation enables the proliferation of Edge AI, permitting sophisticated inference on resource-constrained devices where memory and power budgets preclude the use of full-scale foundation models. Mobile implementations like MobileBERT run efficiently on smartphones, enabling features such as on-device text prediction and voice assistants that give users AI functionality without requiring constant cloud connectivity. By effectively compressing the “dark knowledge” of giant architectures into efficient runtimes, distillation addresses the practical dichotomy between model scale and deployment feasibility.\nMajor e-commerce and recommendation platforms have applied distillation techniques to improve their systems. For example, privileged feature distillation has been used to enhance recommendation systems, achieving measurable gains in click-through and conversion rates while maintaining reasonable inference costs.\nModel distillation offers several key benefits:\n\nReduced model size: Enables deployment on devices with limited storage and computational power.\nFaster inference: Smaller models process data more quickly, reducing response times.\nLower resource consumption: Reduces VRAM usage, memory bandwidth, and power consumption.\nDirect cost reduction: Distilled models require less compute, reducing operational costs.\n\nDistillation is most effective when applied after a model has been fully pre-trained or fine-tuned on a specific task. At this stage, the teacher model has already captured rich task-specific knowledge that can be efficiently transferred to the student. Common use cases include:\n\nProduction deployment: Reducing serving costs and meeting hardware constraints before model release\nEdge and mobile applications: Enabling AI features on resource-constrained devices\nBudget-conscious inference at scale: Serving thousands of users simultaneously while managing costs\nTask-specific optimization: Replacing overly large general-purpose models with compact, focused alternatives\n\n\n\n\nThe Rise of Small Language Models\nThe success of distillation and efficient fine-tuning has contributed to a broader trend: the rise of Small Language Models (SLMs). These models, typically ranging from a few hundred million to several billion parameters, challenge the assumption that bigger is always better.\nSLMs offer compelling advantages for many practical applications:\n\nThey can run on consumer hardware, including laptops and smartphones\nThey provide faster inference times, suitable for real-time applications\nThey consume less energy, reducing both costs and environmental impact\nThey can be deployed in privacy-sensitive contexts where data cannot leave the device\n\nTechniques like distillation, parameter-efficient fine-tuning, and quantization work synergistically to create capable SLMs. A typical workflow might involve: (1) distilling knowledge from a large teacher model, (2) fine-tuning the student on domain-specific data using LoRA, and (3) quantizing the result for efficient deployment. This pipeline democratizes access to advanced AI capabilities, making sophisticated models accessible across diverse platforms and use cases.\nAs the demand for AI continues to grow, the importance of techniques that balance capability with efficiency will only increase. The future of LLM deployment lies not just in scaling up, but in the intelligent compression and adaptation of powerful models for practical use.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#evaluating-model-performance",
    "href": "24-llm.html#evaluating-model-performance",
    "title": "24  Large Language Models",
    "section": "24.6 Evaluating Model Performance",
    "text": "24.6 Evaluating Model Performance\nWhen evaluating models, researchers and practitioners rely on various benchmarks that test different aspects of language understanding and generation. The field has evolved significantly as earlier benchmarks became saturated—with top models achieving near-perfect scores—and concerns about test set contamination grew. Modern evaluation suites now emphasize more challenging reasoning tasks and dynamic, contamination-resistant designs.\nFor complex reasoning, GPQA Diamond presents graduate-level questions in biology, physics, and chemistry that challenge even domain experts, while ARC-AGI measures abstract reasoning capabilities that approach the boundaries of general intelligence. Mathematical prowess is tested through competition-level problems from the AIME (American Invitational Mathematics Examination), where top models now achieve scores that would qualify for elite high school competitions.\nPractical software engineering capabilities are evaluated via SWE-Bench, which tasks models with resolving real GitHub issues from popular open-source repositories—a far more realistic test than generating isolated code snippets. For multimodal understanding, MMMU-Pro extends earlier benchmarks with challenging questions requiring joint reasoning over text and images. Figure 24.3 shows that as of December 2025, the most cost-effective models for software engineering are Claude 4/4.5 Opus and GPT-5.1-codex.\n\n\n\n\n\n\nFigure 24.3: Cumulative cost distribution for solving SWE-Bench issues across leading models (January 2026).\n\n\n\nTo combat benchmark contamination—where models may have memorized test questions during training—dynamic benchmarks like LiveBench continuously refresh their question sets using recent math competitions, newly published papers, and current news articles. Perhaps most ambitiously, Humanity’s Last Exam crowdsources extremely difficult questions from domain experts across fields, explicitly designed to resist easy saturation.\n\n\n\n\n\n\nFigure 24.4: HLE as of January 2026.\n\n\n\nOur personal favorite resource to keep track of the latest and greatest models is LMArena (formerly Chatbot Arena). Rather than relying on static test sets, LMArena uses a community-driven approach where users compare model outputs head-to-head in blind evaluations. Participants see responses from two anonymous models to the same prompt and select the one they prefer. These pairwise comparisons are then aggregated using an Elo rating system—the same method used to rank chess players. When a model wins a comparison, its score increases; when it loses, its score decreases. The magnitude of each adjustment depends on the expected outcome: defeating a higher-rated model yields a larger score boost than beating a lower-rated one. This dynamic system continuously adapts as new votes accumulate, providing a real-time reflection of collective user preferences that complements traditional benchmark evaluations. For example, for the WebDev category, the LMArena ranks Claude 4/4.5 in the top three along with GPT-5.2. This is a similar ranking we saw in SWE-Bench.\n\n\n\n\n\n\nFigure 24.5: LMArena leaderboard for WebDev category as of January 2026.\n\n\n\nHowever, benchmarks have limitations and don’t always reflect real-world performance. A model that excels at multiple-choice questions might struggle with open-ended creative tasks. Code generation benchmarks might not capture the nuanced requirements of your specific programming domain. The key is to use benchmarks as a starting point while conducting thorough validation using data that closely resembles your actual use case.\nConsider implementing your own evaluation framework that tests the specific capabilities you need. If you’re building a customer service chatbot, create test scenarios that reflect your actual customer interactions. If you’re developing a creative writing assistant, evaluate the model’s ability to generate diverse, engaging content in your target style or genre.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#sec-post-training-reasoning",
    "href": "24-llm.html#sec-post-training-reasoning",
    "title": "24  Large Language Models",
    "section": "24.7 Post-training Techniques",
    "text": "24.7 Post-training Techniques\nWhile LLMs excel at predicting the next token in a sequence, their true potential emerges through post-training techniques that teach them to reason, think step-by-step, and align with human expectations. When we think about improving LLMs’ skills, our focus often centers on aspects such as improved grammar or more natural-sounding responses. But what sets a helpful LLM apart is its ability to reason. This involves thinking through problems, breaking them down into steps, making informed decisions, and explaining how it arrived at an answer. Reasoning takes next-token prediction to the next level by adding logic, structure, and goal-oriented thinking.\nWithout strong reasoning skills, models often skip steps, make confident but incorrect claims (hallucinations), or struggle with tasks that require planning or logic. For any organization, this creates a significant risk, undermining user trust and leading to unreliable outcomes. The good news is that we can improve reasoning with the right techniques and upgrade a pre-trained LLM with broad knowledge into a valuable tool for real-world tasks that aligns with users’ needs.\nPost-training refines a model’s capabilities, teaching it to move beyond simply predicting the next word. This means moving past the first plausible answer and compelling the model to build a more deliberate, logical response. It learns to break down a task, reflect on its outputs, and consult external tools—mimicking a more methodical, human-like reasoning process. This is how we upgrade a generalist LLM into a specialized tool that is more accurate, trustworthy, and aligned with specific business goals.\nOne form of reasoning involves combining independent facts to arrive at an answer, rather than simply regurgitating memorized information. For example, when asked, “What is the capital of the state where Dallas is located?” a model could just recall “Austin” if it has seen that exact question before. However, a deeper level of reasoning is at play. Interpretability research reveals that models like Claude first activate concepts representing “Dallas is in Texas” and then connect this to another concept, “the capital of Texas is Austin.” This demonstrates the ability to perform multi-step reasoning by chaining together different pieces of knowledge.\n\n\n\n\n\n\nFigure 24.6: Multi-step Reasoning: Anthropic\n\n\n\nThis multi-step reasoning process can be visualized as follows:\n\n\n\n\n\n\nFigure 24.7: Chain of Thought Reasoning process showing the activation of intermediate concepts.\n\n\n\nThis capability can be tested by intervening in the model’s thought process. For instance, if the “Texas” concept is artificially replaced with “California,” the model’s output correctly changes from “Austin” to “Sacramento,” confirming that it is genuinely using the intermediate step to determine its final answer. This ability to combine facts is a crucial component of advanced reasoning.\nThe landscape of post-training methods used to boost the reasoning abilities of pre-trained LLMs is rich and varied. These techniques build on the model’s existing knowledge, teaching it to follow instructions more effectively and use tools or feedback to refine its answers. Each method adds a new layer of skill, whether it involves breaking down problems, learning from feedback, or drawing on real-world information, all to bridge the model’s reasoning with the human thought process.\nHowever, it’s crucial to understand that even when a model produces a step-by-step “chain of thought,” (CoT) it may not be a faithful representation of its actual reasoning process. Recent research from Anthropic explores this very question, revealing a complex picture: sometimes the reasoning is faithful, and sometimes it’s fabricated to fit a pre-determined conclusion.\nWhen a model is tasked with a problem it can solve, like finding the square root of 0.64, interpretability tools show that it follows a logical path, activating concepts for intermediate steps (like the square root of 64) before reaching the final answer. However, when presented with a difficult problem and an incorrect hint, the model can engage in what researchers call “motivated reasoning.” It starts with the incorrect answer and works backward, creating a believable but entirely fake sequence of steps to justify its conclusion. This ability to generate a plausible argument for a foregone conclusion without regard for truth is a critical limitation. These interpretability techniques offer a way to “catch the model in the act” of faking its reasoning, providing a powerful tool for auditing AI systems.\nLLMs were not originally designed to function as calculators; they were trained on text data and lack built-in mathematical algorithms. Yet, they can perform addition tasks, like calculating 36+59, seemingly without explicitly writing out each step. How does a model, primarily trained to predict the next word in a sequence, manage to perform such calculations?\nOne might speculate that the model has memorized extensive addition tables, allowing it to recall the answer to any sum present in its training data. Alternatively, it could be using traditional longhand addition methods similar to those taught in schools.\nHowever, research reveals that Claude, a specific LLM, utilizes multiple computational strategies simultaneously. One strategy estimates an approximate answer, while another precisely calculates the last digit of the sum. These strategies interact and integrate to produce the final result. While addition is a straightforward task, analyzing how it is executed at this granular level—through a combination of approximate and precise methods—can provide insights into how Claude approaches more complex problems.\n\n\n\n\n\n\nFigure 24.8: Mental Math Problem Solving: Anthropic\n\n\n\nModels like Claude 3.7 Sonnet can “think out loud,” often improving answer quality, but sometimes misleading with fabricated reasoning. This “faked” reasoning can be convincing, posing reliability challenges. Interpretability helps distinguish genuine reasoning from false.\nFor instance, Claude accurately computes the square root of 0.64, showing a clear thought process. However, when tasked with finding the cosine of a large number, it may fabricate steps. Additionally, when given a hint, Claude may reverse-engineer steps to fit a target, demonstrating motivated reasoning.\n\n\n\n\n\n\nFigure 24.9: False Reasoning: Anthropic\n\n\n\nThis highlights that a model’s explanation of its thought process can’t always be trusted. For high-stakes applications, being able to verify the internal reasoning process, rather than just accepting the output, is essential for building reliable and trustworthy AI.\nInstruction Fine-Tuning (IFT) represents perhaps the most fundamental approach to improving model reasoning. The core idea involves taking a pre-trained model and running a second pass of supervised learning on mini-lessons, each formed as a triple of instruction, input, and answer.\nConsider a math word problem where the instruction asks to solve this math word problem step by step, the input presents Sarah has 12 apples and gives away 5. How many does she have left?, and the answer provides\nStep 1: Start with 12 apples. \nStep 2: Subtract 5 apples given away. \nStep 3: 12 - 5 = 7 apples remaining. \nEach training example teaches the model how to transform a task description into the steps that solve it (Chung et al. 2022). After thousands of such drills, the model learns many small skills and when to switch among them. The steady practice trains it to deliver precise answers that match the instruction rather than sliding into a generic reply. Empirical evidence demonstrates the power of this approach: Flan UPaLM 540B, a variant of the UPaLM model fine-tuned with instruction-based tasks, significantly outperformed the original UPaLM 540B model. UPaLM stands for Unified Pre-trained Language Model, which is a large-scale language model designed to handle a wide range of tasks. The Flan UPaLM 540B was evaluated across four benchmarks: MMLU (Massive Multitask Language Understanding), which tests the model’s ability to handle a variety of academic subjects; BBH (Big-Bench Hard), a set of challenging tasks designed to push the limits of language models; TyDiQA (Typologically Diverse Question Answering), which assesses the model’s performance in answering questions across diverse languages; and MGSM (Mathematics Grade School Math), which evaluates the model’s capability in solving grade school-level math problems. The Flan UPaLM 540B showed an average improvement of 8.9% over the original model across these benchmarks.\nDomain-Specific Supervised Fine-Tuning takes the IFT principle and applies it within specialized fields. This approach restricts the training corpus to one technical field, such as medicine, law, or finance, saturating the model weights with specialist concepts and rules. Fine-tuning on domain-specific data enables the model to absorb the field’s vocabulary and structural rules, providing it with direct access to specialized concepts that were scarce during pre-training. The model can quickly rule out answers that do not make sense and narrow the search space it explores while reasoning. Mastering a domain requires data that captures its unique complexity, utilizing domain-specific examples, human-labeled edge cases, and diverse training data generated through hybrid pipelines combining human judgment and AI. This process enhances the model’s ability to follow complex instructions, reason across modalities and languages, and avoid common pitfalls like hallucination. The effectiveness of this approach is striking: in ICD-10 coding, domain SFT catapulted exact-code accuracy from less than 1% to approximately 97% on standard ICD coding (including linguistic and lexical variations) and to 69% on real clinical notes (Hou et al. 2025).\n\nChain-of-Thought and Chain of Reasoning\nChain-based reasoning techniques represent some of the most powerful tools for improving LLM reasoning capabilities. These approaches guide models to break down complex problems into manageable steps, explore multiple solution paths, and learn from their mistakes—mirroring the deliberate problem-solving strategies humans employ when tackling difficult tasks.\nChain-of-Thought (Wei et al. 2023) prompting offers a remarkably simple yet powerful technique that requires no model retraining. The approach involves showing the model a worked example that spells out every intermediate step, then asking it to “think step by step.” Writing the solution step by step forces the model to reveal its hidden reasoning, making it more likely for logically necessary tokens to appear. Because each step is generated one at a time, the model can inspect its own progress and fix contradictions on the fly. The empirical results are impressive: giving PaLM 540B eight CoT examples improved its accuracy on GSM8K from 18% to 57%. This improvement came entirely from a better prompt, with no changes to the model’s weights.\nTree-of-Thought extends the chain-of-thought concept by allowing exploration of multiple reasoning paths simultaneously. Instead of following one chain, this method lets the model branch into multiple reasoning paths, score partial solutions, and expand on the ones that look promising. Deliberate exploration stops the first plausible idea from dominating. ToT lets the model test several lines of reasoning instead of locking onto one. When a branch hits a dead end, it can backtrack to an earlier step and try another idea, something a plain CoT cannot do. The model operates in a deliberate loop: propose, evaluate, and explore. This approach resembles a CEO evaluating multiple business strategies, modeling several potential outcomes before committing to the most promising one, preventing over-investment in a flawed initial idea. This principle has been applied in projects to improve coding agents focused on generating pull requests for repository maintenance and bug-fixing tasks across multiple programming languages. Researchers have analyzed thousands of coding agent trajectories, evaluating each interaction step-by-step to provide more explicit guidance to the models, enabling them to make better decisions on real coding tasks. In the “Game of 24” puzzle, GPT-4 combined with CoT reasoning solved only 4% of the puzzles, but replacing it with ToT raised the success rate to 74% (Yao et al. 2023).\n\n\nReflexion\nReflexion (Shinn et al. 2023) introduces a self-improvement mechanism that operates through iterative feedback. After each attempt, the model writes a short reflection on what went wrong or could be improved. That remark is stored in memory and included in the next prompt, giving the model a chance to revise its approach on the next try. Reflexion turns simple pass/fail signals into meaningful feedback that the model can understand and act on. By reading its own critique before trying again, the model gains short-term memory and avoids repeating past mistakes. This self-monitoring loop of try, reflect, revise guides the model toward better reasoning without changing its weights. Over time, it helps the model adjust its thinking more like a human would, by learning from past mistakes and trying again with a better plan. A GPT-4 agent using Reflexion raised its success rate from 80% to 91% on the HumanEval coding dataset.\n\n\nNon-Linear Reasoning Capabilities\nRecent advances in LLM reasoning have focused on establishing these non-linear capabilities, moving beyond simple chain-of-thought prompting to more sophisticated reasoning architectures. These approaches recognize that human reasoning is rarely linear—we backtrack when we realize we’ve made an error, we consider multiple possibilities in parallel, and we iteratively refine our understanding as we gather more information.\nOne promising direction is iterative reasoning, where models are allowed to revise their intermediate steps based on feedback or self-evaluation. Unlike traditional autoregressive generation where each token is final once generated, iterative approaches allow the model to revisit and modify earlier parts of its reasoning chain. This might involve generating an initial solution, evaluating it for consistency, and then revising specific steps that appear problematic.\nA compelling example of how extended thinking improves reasoning capabilities can be seen in mathematical problem-solving performance. When Claude 3.7 Sonnet was given more computational budget to “think” through problems on the American Invitational Mathematics Examination (AIME) 2024, its accuracy improved logarithmically with the number of thinking tokens allocated. This demonstrates that allowing models more time for internal reasoning—similar to how humans perform better on complex problems when given more time to think—can lead to substantial performance gains.\n\n\n\n\n\n\nFigure 24.10: AIME 2024 performance vs. actual thinking token usage\n\n\n\nFigure 24.10 shows Claude 3.7 Sonnet’s performance on the 2024 American Invitational Mathematics Examination improving logarithmically with the number of thinking tokens used per problem. The model generally uses fewer tokens than the maximum budget allocated, suggesting it adaptively determines when sufficient reasoning has been applied. Source: Anthropic’s Visible Extended Thinking.\nParallel hypothesis generation represents another departure from linear reasoning. Instead of committing to a single reasoning path, these approaches generate multiple competing explanations or solutions simultaneously. The model can then evaluate these alternatives, potentially combining insights from different paths or selecting the most promising direction based on evidence accumulation.\nDynamic tool selection and reasoning takes this further by allowing models to adaptively choose which reasoning strategies or external tools to employ based on the specific demands of the current problem. Rather than following a predetermined sequence of operations, the model can dynamically decide whether to retrieve external information, perform symbolic computation, or engage in pure logical reasoning based on the current state of the problem.\nThese non-linear reasoning capabilities are particularly important for complex problem-solving scenarios where the optimal approach isn’t clear from the outset. In scientific reasoning, for example, a hypothesis might need to be revised as new evidence emerges. In mathematical problem-solving, an initial approach might prove intractable, requiring a fundamental shift in strategy. In code generation, debugging often requires jumping between different levels of abstraction and considering multiple potential sources of error.\nThe implementation of non-linear reasoning often involves sophisticated orchestration between multiple model calls, external tools, and feedback mechanisms. This represents a shift from viewing LLMs as simple text generators to understanding them as components in more complex reasoning systems. As these capabilities mature, we can expect to see LLMs that not only generate human-like text but also exhibit more human-like reasoning patterns—flexible, adaptive, and capable of handling ambiguity and uncertainty with greater finesse.\nReinforcement Learning from Human Feedback (RLHF) represents a sophisticated approach to aligning model behavior with human preferences. The process involves taking a pre-trained model and generating several answers for real user prompts. Human reviewers rank those answers, a reward model learns these rankings, and the main model is updated to score higher on that reward. This loop optimizes the model to produce outputs humans prefer rather than those that merely score well on next-token likelihood. Because humans reward answers that are complete, fact-checked, and well-explained, the model learns to value clear logic over quick guesses. Each reinforcement learning step trains it to produce responses that follow instructions, chain ideas coherently, and avoid unsupported claims, aligning its internal decision-making with human expectations. Safety and alignment research relies heavily on these techniques; Constitutional AI approaches, for instance, attempt to instill models with explicit principles and values during this phase. In the original InstructGPT study by Ouyang et al. (2022), annotators preferred answers from the 175B RLHF-tuned model over the same-size GPT-3 baseline 85% of the time. Even the 1.3B RLHF model outperformed the baseline, despite having 100 times fewer parameters.\nChain-of-Action (CoA) (Pan et al. 2025) represents the most sophisticated integration of reasoning and external tool use. This approach decomposes a complex query into a reasoning chain interleaved with tool calls such as web search, database lookup, or image retrieval that are executed on the fly and fed into the next thought. Each action grounds the chain in verified facts. By using up-to-date information and multi-reference faith scores, the model can remain grounded and make more informed decisions, even when sources disagree. Because it can plug in different tools as needed, it’s able to take on more complex tasks that require different data modalities. CoA outperformed the leading CoT and RAG baselines by approximately 6% on multimodal QA benchmarks, particularly on compositional questions that need both retrieval and reasoning.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#data-quality-and-quantity",
    "href": "24-llm.html#data-quality-and-quantity",
    "title": "24  Large Language Models",
    "section": "24.8 Data quality and quantity",
    "text": "24.8 Data quality and quantity\nOne might assume that training an LLM for non-linear reasoning would require tremendous amounts of data, but recent research reveals that data quality can compensate for limited quantity. This finding has significant implications for organizations looking to develop reasoning-capable models without massive data collection efforts.\nTwo compelling examples demonstrate this principle. The S1 research (Yang et al. 2025) fine-tuned their base model, Qwen2.5-32B-Instruct, on only 1,000 high-quality reasoning examples, yet achieved remarkable performance improvements. Their data collection process was methodical: they started with 59,029 questions from 16 diverse sources (including many Olympiad problems), generated reasoning traces using Google Gemini Flash Thinking API through distillation, then applied rigorous filtering. Problems were first filtered by quality (removing poor formatting), then by difficulty—a problem was deemed difficult if neither Qwen2.5-7B-Instruct nor Qwen2.5-32B-Instruct could solve it, and the reasoning length was substantial. Finally, 1,000 problems were sampled strategically across various topics.\nSimilarly, the LIMO (Less is More for Reasoning) research (Ye et al. 2025) demonstrated that quality trumps quantity. Taking NuminaMath as a base model, they fine-tuned it on merely 817 high-quality curated training samples to achieve impressive mathematical performance with exceptional out-of-distribution generalization. Their results were striking enough to warrant comparison with OpenAI’s o1 model.\nFor high-quality non-linear reasoning data, LIMO proposes three essential guidelines:\nStructured Organization: Tokens are allocated to individual “thoughts” according to their importance and complexity, with more tokens devoted to key reasoning points while keeping simpler steps concise. This mirrors how human experts organize their thinking—spending more time on difficult concepts and moving quickly through routine steps.\nCognitive Scaffolding: Concepts are introduced strategically, with careful bridging of gaps to make complex reasoning more accessible. Rather than jumping directly to advanced concepts, the reasoning process builds understanding step by step, similar to how effective teachers structure lessons.\nRigorous Verification: Intermediate results and assumptions are frequently checked, and logical consistency is ensured throughout the reasoning chain. This is especially important given the risk of hallucinations in complex reasoning tasks.\nThe verification aspect deserves special attention. The rStar-Math research (Guan et al. 2025) offers an innovative approach by training their LLM to produce solutions as Python code with text as code comments. This format allows for automatic verification—the code can be executed to check correctness, providing immediate feedback on the reasoning process. With agentic capabilities, this approach could create a feedback loop where the LLM learns from its execution results.\nThese findings suggest that the path to better reasoning capabilities lies not in simply collecting more data, but in curating datasets that exemplify the structured, scaffolded, and verified thinking patterns we want models to learn. This approach makes advanced reasoning capabilities more accessible to organizations that may not have access to massive datasets but can invest in creating high-quality training examples.\n\n\n\n\n\n\nFigure 24.11: An example of Code-augmented CoT Figure from RStar-Math Guan et al. (2025)\n\n\n\nFigure 24.11 shows how rStar-Math integrates code execution with reasoning by formatting solutions as Python code with explanatory comments. This approach allows for automatic verification of intermediate steps, creating a feedback loop where the model can learn from execution results and catch errors in real-time. The figure demonstrates how mathematical reasoning can be made more reliable by grounding abstract concepts in executable code.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#sec-context-engineering",
    "href": "24-llm.html#sec-context-engineering",
    "title": "24  Large Language Models",
    "section": "24.9 Dealing with Context Window Limitations: Context Engineering",
    "text": "24.9 Dealing with Context Window Limitations: Context Engineering\nConsider this paradox: GPT-4 Turbo advertises a 128,000-token context window—enough to process an entire novel—yet production systems routinely degrade as context fills. Response times spike, accuracy drops, and costs spiral. The promise of million-token windows collides with the reality of quadratic attention complexity and the finding that models often ignore information buried in the middle of long prompts.\nLarge language models are stateless: they generate output from input, then forget everything. The context window—the maximum tokens processed in one forward pass—defines what the model can “see.” Everything must fit: system prompt, conversation history, retrieved documents, examples, and the query itself.\nContext window management couples three concerns that production systems care about:\n\nCost: More tokens means more spend\nLatency: More tokens means more compute and slower responses\nAccuracy: More tokens can help, but long prompts dilute signal and introduce failure modes\n\nThis creates a fundamental trade-off. You can pack in more conversation history and more documents, but the model often becomes slower and less reliable. The goal is not maximum context but the right context, assembled under a fixed budget.\nA useful mental model treats the context window as a budget:\n\\[\nB \\approx S + T + H + R + U\n\\]\nwhere \\(B\\) is the usable token budget, \\(S\\) is system instructions, \\(T\\) is tool schemas, \\(H\\) is conversation history, \\(R\\) is retrieved evidence, and \\(U\\) is the user’s current input. Context engineering is deciding what to keep, what to compress, and what to retrieve—so that \\(R\\) contains evidence rather than redundancy.\nMany visible quality improvements in LLM applications come from better context management rather than larger parameter counts: retrieval that finds the right passage, reranking that removes near-misses, compression that preserves key facts, caching that avoids resending static instructions, and memory that keeps multi-turn workflows coherent. You can often get a large fraction of the value of a bigger model by improving the pipeline that feeds it.\n\nThe Lost-in-the-Middle Problem\nBefore diving into solutions, it’s worth understanding the failure mode that dominates production systems. Large language models exhibit a characteristic U-shaped attention curve: they attend strongly to the beginning and end of their context but underweight information in the middle. Research confirms documents positioned in the middle of a long context receive 20–40% less attention than documents at the boundaries (N. F. Liu et al. 2024).\nThis architectural bias has practical consequences. If you retrieve 15 document chunks and the most relevant one lands in position 7, the model may effectively ignore it. The fix is counterintuitive: don’t fight the attention bias—exploit it by placing the most relevant documents at the start and end.\n\n\nWhen to Use Long Context vs. RAG vs. Summarization\nBefore diving into specific techniques, choose the right strategy for your use case:\n\n\n\n\n\n\n\n\nScenario\nStrategy\nRationale\n\n\n\n\nDocument fits in window; need holistic reasoning\nStuff entire document\nCross-references and global structure preserved\n\n\nCorpus larger than window; factual lookup\nRAG with small chunks\nPrecision matters more than narrative\n\n\nCorpus larger than window; synthesis task\nRAG with hierarchical retrieval\nNeed both global context and specific evidence\n\n\nLong conversation history\nTiered memory + summarization\nKeep recent turns verbatim; compress older context\n\n\nRepetitive prompt structure\nCaching + compression\nAvoid paying to reprocess static content\n\n\n\nThe decision often comes down to whether the task requires holistic reasoning (favor long context) or precise evidence retrieval (favor RAG). Many production systems combine both: retrieve evidence via RAG, then use the full context window for reasoning over that evidence.\n\n\nAttention Efficiency: FlashAttention and Ring Attention\nTransformers rely on attention, and naive attention scales quadratically with sequence length (\\(O(n^2)\\)). Even with optimizations, long prompts increase memory traffic and reduce throughput.\nFlashAttention addresses this by fusing attention computation into a single kernel that keeps intermediate results in fast on-chip SRAM rather than writing large \\(N \\times N\\) matrices to slow GPU memory. This reduces memory requirements from \\(O(N^2)\\) to \\(O(N)\\). FlashAttention-3, optimized for NVIDIA H100 architecture, achieves up to 740 TFLOPs/s and enables 16,000-token contexts on 10 GB of VRAM (Dao 2023).\nFor multi-million token applications, Ring Attention distributes computation across GPU clusters. Query, key, and value tensors are split into chunks, with each GPU computing attention for its local chunk while exchanging states in a ring pattern. Context window size scales linearly with cluster size (H. Liu, Zaharia, and Abbeel 2023).\nThese optimizations are essential infrastructure, but they don’t eliminate the fundamental constraint: treat tokens as a scarce resource and build systems that spend them on evidence rather than redundancy.\n\n\nRetrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) addresses one of the fundamental limitations of LLMs: their reliance on knowledge encoded during training. Before answering, a retriever grabs documents or information relevant to the query and injects them into the context window so the model can reason over fresh evidence. RAG grounds the model in verifiable facts, drastically reducing hallucinations and improving user trust. Instead of relying on potentially outdated or incorrect memorized knowledge, the model reasons over fresh, injected evidence.\nThe RAG process works as follows: the user’s query is redirected to an embedding model, where it is converted into a numeric form; these embeddings are then compared with a knowledge base; the embedding model locates relevant data; the retrieved information is integrated into the prompt for the LLM as additional context; and finally, the output, combining both the retrieved information and the original prompt, is submitted to the user. This approach resembles a lawyer building an argument not from memory, but by citing specific, relevant legal precedents directly in court.\nThe integration of RAG into an enterprise workflow-generation system reduced the rate of hallucinated steps and tables from 21% to 7.5% when evaluated on the HumanEval benchmark (Ayala and Bechard 2024). In real-world applications enhancing LLMs’ multilingual reasoning, RAG has been used to feed models verified, multilingual documents at inference time. The results show that models can answer complex questions in multiple languages, citing specific evidence from retrieved text. Every factual claim becomes traceable, eliminating guesswork and demonstrating a consistent, grounded reasoning process across languages.\nRather than expanding what fits in context, RAG dynamically selects what belongs there. The core insight: for most queries, only a fraction of a knowledge base is relevant. RAG retrieves that fraction on demand.\nA basic RAG pipeline:\n\nIndex: Split documents into chunks, embed into vectors, store in vector database\nRetrieve: Given a query, find similar chunks via embedding similarity\nGenerate: Concatenate retrieved chunks with the query and prompt the LLM\n\n\n\n\n\n\nflowchart LR\n  D[Documents] --&gt; C[Chunk & Embed]\n  C --&gt; V[(Vector DB)]\n  Q[Query] --&gt; E[Embed]\n  E --&gt; V\n  V --&gt;|top-k| R[Retrieved chunks]\n  R --&gt; P[Prompt + Context]\n  Q --&gt; P\n  P --&gt; L[LLM]\n  L --&gt; A[Answer]\n\n\n\n\n\n\nThis architecture allows models to access knowledge bases orders of magnitude larger than any context window.\n\nRAG versus Fine-tuning\nRAG and fine-tuning (discussed in Section 24.5.2) represent two distinct strategies for adapting LLMs to specific applications. While fine-tuning modifies the model’s internal parameters based on domain-specific data, RAG retrieves relevant information from external knowledge bases and incorporates it into the prompt at inference time. In RAG, the model’s parameters remain unchanged during the retrieval process.\n\n\n\nRAG is particularly useful when:\n\nInformation evolves frequently or is constantly changing\nLabeled data is insufficient or unavailable\nAccess to external, up-to-date sources is required\n\n\n\nFine-tuning is more appropriate when:\n\nDirect control over model behavior is desired\nLabeled data is available\nThe same pre-trained model needs to be adapted for multiple specific tasks\nCompliance standards or ethical guidelines must be strictly enforced\n\n\n\n\nIn many cases, combining both approaches yields optimal results: fine-tuning provides the foundational domain adaptation while RAG supplies dynamic, current information at inference time. This hybrid approach, sometimes called Retrieval-Augmented Fine-Tuning (RAFT), trains models to effectively use retrieved context while maintaining domain expertise. RAFT extends RAG by fine-tuning the model on tasks that explicitly require using retrieved documents, teaching it to better incorporate external information into its reasoning process.\n\n\n\nAdvanced RAG Variants\nThe basic RAG pipeline admits many refinements. Fixed-size chunking (every 512 tokens) often severs sentences mid-thought. Semantic chunking uses embeddings to find natural break points. The Max–Min algorithm embeds sentences sequentially, comparing each to the current chunk. If similarity exceeds a threshold, the sentence joins; otherwise, a new chunk begins. No universal optimal chunk size exists: 64–128 tokens suit factual lookup; 512–1,024 tokens suit narrative tasks. Chroma’s ClusterSemanticChunker uses dynamic programming to maximize within-chunk similarity, improving retrieval precision by 8–15% over greedy methods (Chroma Research 2024).\nPure vector search can miss exact tokens, identifiers, or acronyms. A user searching for “RFC 2616” needs lexical matching, not semantic similarity to “HTTP specification.” Hybrid retrieval combines dense embeddings with keyword search (BM25). Results merge via Reciprocal Rank Fusion:\n\\[\nH = (1 - \\alpha) \\cdot \\text{BM25 Score} + \\alpha \\cdot \\text{Semantic Score}\n\\]\nTypical \\(\\alpha \\approx 0.5\\) for general-purpose retrieval. See Weaviate’s discussion for tuning guidance.\nMany RAG failures are scope failures, not retrieval failures. If the user asks “What is the refund policy for EU customers?”, retrieval should be constrained by region, document type, and effective date before scoring semantic similarity—a technique called metadata filtering. See Haystack’s metadata filtering guide for practical patterns.\nVector similarity is a coarse filter. Reranking applies a cross-encoder to the top-\\(k\\) candidates, scoring query-document pairs jointly. ColBERTv2 and BERT-based rerankers achieve 15–30% improvement over embedding-only retrieval (Khattab and Zaharia 2020). A standard two-stage pipeline retrieves 20+ candidates via hybrid search (maximizing recall), then reranks to top 3–5 via cross-encoder (maximizing precision).\nWhen documents are large and interconnected, flat retrieval struggles. Hierarchical retrieval addresses this: RAPTOR builds a tree of summaries—retrieve high-level summaries to identify relevant sections, then drill down to specific chunks (Sarthi et al. 2024). GraphRAG extracts knowledge graphs and retrieves entity-relationship paths for multi-entity reasoning (Microsoft Research 2024).\n\n\n\n\n\n\n\n\nVariant\nInnovation\nBest For\n\n\n\n\nSelf-RAG\nJoint retriever-generator with self-critique\nHigh-stakes QA (legal, medical)\n\n\nCRAG\nAdaptive retrieval with confidence evaluation and web fallback\nTasks requiring current information\n\n\nGraph RAG\nKnowledge graph extraction; retrieves entity paths\nMulti-entity reasoning\n\n\nHyDE\nGenerates hypothetical answer, retrieves based on that\nVague or ambiguous queries\n\n\n\nHyDE addresses a common failure mode: vague queries that don’t match relevant documents. The model first generates a hypothetical answer, then uses that answer’s embedding for retrieval—even if factually wrong, it contains vocabulary that matches correct documents (Gao et al. 2022).\n\n\nSolving Lost-in-the-Middle\nAddressing the lost-in-the-middle phenomenon requires both architectural improvements and practical mitigation strategies. Recent research has systematically evaluated various approaches to this challenge. Gupte et al. (2025) introduced the GM-Extract benchmark to study LLM performance on retrieval of control variables, proposing distinct metrics for spatial retrieval capability (Document Metric) and semantic retrieval capability (Variable Extraction Metric). Their analysis categorizes mitigation methods into black-box approaches (modifications to prompts and retrieval strategies) and white-box approaches (modifications to model architecture or attention mechanisms), finding that the efficacy of these techniques is highly nuanced and context-dependent.\nThe most straightforward fix is strategic document positioning after reranking:\n\nMost relevant → position at start\nSecond-most relevant → position at end\nMedium relevance → position in middle\n\nCombined with two-stage retrieval (broad recall, then precision reranking), this inverts the U-curve to match model attention patterns.\nOpenAI’s GPT-5.2 introduced the MRCRv2 (Multi-Round Coreference Resolution) benchmark, specifically designed to evaluate long-context performance across extended conversations. This benchmark measures how well models track entities and maintain coherence across multiple turns of dialogue, directly addressing scenarios where critical information might otherwise be lost mid-context. Early evaluations suggest that reasoning models like GPT-5.2 show improved performance on these metrics, though the improvements are not uniform across all task types.\nChain-of-thought (CoT) prompting also helps mitigate lost-in-the-middle effects by encouraging models to explicitly reference and reason through retrieved documents in sequence. When models are prompted to “think step by step” about each piece of evidence, they are less likely to skip over information positioned in the middle of the context. This approach, combined with retrieval-augmented generation, creates a reasoning pipeline that forces attention to all retrieved chunks rather than just those at the boundaries.\nLong-context reranking takes this further: concatenate retrieved chunks in original document order and score jointly, capturing cross-chunk relationships. This often improves accuracy by 10–15% over isolated chunk reranking.\nfrom llama_index.core.postprocessors import LLMRerank, MetadataReplacementPostProcessor\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\n\nnode_parser = SentenceWindowNodeParser.from_defaults(\n    window_size=3,\n    window_metadata_key=\"window\"\n)\n\nindex = VectorStoreIndex.from_documents(documents, node_parser=node_parser)\n\npostprocessors = [\n    MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n    LLMRerank(top_n=3, service_context=service_context)\n]\n\nquery_engine = index.as_query_engine(\n    similarity_top_k=10,\n    node_postprocessors=postprocessors\n)\n\n\nCaching and Compression\nPrompt compression reduces token count before inference, cutting latency and cost. Microsoft’s LLMLingua-2 formulates compression as token classification: a small Transformer determines which tokens are essential, achieving 2–5× compression with minimal performance loss (Jiang et al. 2023). Compression works well for verbose natural language (instructions, transcripts, conversation history), RAG systems retrieving 10+ chunks per query, and multi-step reasoning with repeated context. It fails for structured data—JSON schemas, SQL, API specifications—where dropping a token from user_id → userid breaks functionality.\nFor applications with large static prompt components, context caching provides substantial savings. Systems like Anthropic’s prompt caching allow marking static portions—system instructions, documentation, tool schemas—for reuse across API calls, reducing costs significantly and eliminating redundant computation.\nEven with sufficient tokens, context can fail. Instruction conflicts occur when system prompts contradict retrieved text or user queries. Retrieval drift happens when top-\\(k\\) chunks are semantically related but not evidentially useful. Duplication wastes budget and amplifies noise. Stale memory silently drops constraints or commitments from earlier turns. Prompt injection allows malicious content in retrieved documents to hijack model behavior. Mitigations include explicit conflict resolution in system prompts, deduplication before context assembly, and allowlists for tool invocation.\nMulti-turn applications face a token explosion problem: keeping every turn eventually exceeds the budget; truncating aggressively loses commitments and constraints. Production systems implement tiered memory: (1) Active Memory (100–500 tokens) holds the current turn plus last 2–3 exchanges; (2) Session Memory (500–2,000 tokens) stores compressed summaries, key entities, and cross-turn dependencies; (3) Persistent Memory in an external vector or graph database is retrieved on demand.\n\n\n\n\n\nflowchart LR\n  U[User] --&gt; A[Active context&lt;br/&gt;last few turns + current task]\n  A --&gt; M[Model call]\n  M --&gt; O[Output]\n  A --&gt; S[Session summary&lt;br/&gt;compact running state]\n  A --&gt; P[Persistent memory&lt;br/&gt;vector store / graph]\n  P --&gt;|retrieve on demand| A\n  S --&gt;|refresh| A\n\n\n\n\n\n\nThe MemGPT pattern virtualizes LLM context, treating it like an operating system’s page cache (Packer et al. 2023). At 70% capacity, reasoning pauses for memory pressure handling. The system identifies least critical content for eviction, compresses it to an external tier, and retrieves relevant context back on demand. This pattern sustains multi-turn conversations indefinitely within finite windows.\n\n\nNeural Memory Systems\nThe tension between attention’s quadratic cost and the desire for persistent memory has driven research into architectures that learn during inference.\nTitans gives models a deep memory module—a multi-layer perceptron that updates on the fly for each input (Behrouz, Pezeshki, and Fakoor 2025). Updates are triggered by a “surprise signal” computed from gradient magnitudes, analogous to biological synaptic strengthening during prediction error.\n\n\n\n\n\n\nFigure 24.12: Titans Architecture. The model uses a deep MLP memory module that updates during inference based on a surprise signal.\n\n\n\nMIRAS generalizes this approach, treating memory as learnable parameters updated via local optimization with configurable loss functions (Behrouz and Pezeshki 2025).\nLarimar takes a different approach, adding a distributed episodic memory to existing LLMs (Das et al. 2024). This brain-inspired architecture allows dynamic, one-shot updates of knowledge without retraining or fine-tuning—achieving 8–10× speedups over traditional knowledge editing methods while also supporting selective forgetting and information leakage prevention.\nThese architectures suggest a future where AI systems combine attention for immediate reasoning with neural memory for long-term, persistent knowledge.\n\n\nEvaluation and Deployment\nEffective context management requires measuring performance across three dimensions. Retrieval quality is assessed via Precision@k (fraction of top-\\(k\\) retrieved documents that are relevant), Recall (fraction of all relevant documents that were retrieved), and Mean Reciprocal Rank (average of \\(1/\\text{rank}\\) for the first relevant result—higher when relevant documents appear earlier). Compression efficiency tracks compression ratio (original tokens divided by compressed tokens), task retention (performance on downstream tasks after compression), and latency improvements. Generation quality measures exact match accuracy (whether the answer matches a gold reference exactly), F1 scores (harmonic mean of precision and recall at the token level), and faithfulness—whether claims in the answer can be traced to the provided context rather than hallucinated. Libraries like RAGAS provide standardized evaluation pipelines for RAG systems, computing metrics such as faithfulness, answer relevancy, and context precision.\nDifferent applications demand different context engineering strategies. Enterprise knowledge assistants typically employ hybrid retrieval (15 semantic + 5 BM25 candidates), cross-encoder reranking to the top-3, and LLMLingua-2 compression at 2–3×, targeting under 2,000 tokens of evidence with sub-500ms latency. Citations to retrieved chunks ensure auditability. Deep research systems favor hierarchical retrieval via RAPTOR for multi-level abstraction and GraphRAG for structural questions, synthesizing 5–10 documents while preserving source order. Codebase assistants combine sparse retrieval (symbols, filenames) with dense retrieval (conceptual similarity), limiting context to 3–5 surgical code blocks with aggressive caching of tool schemas, targeting under 4,000 tokens with sub-second latency.\nA production deployment should address each stage systematically: chunking strategy matched to task requirements (small for lookup, large for narrative), hybrid search with metadata filtering, cross-encoder reranking to top-3 or top-5, high-relevance content positioned at context boundaries, compression applied to verbose content but skipped for structured data, static prompt components pinned via caching, tiered memory architecture for multi-turn workflows, and continuous monitoring of retrieval precision, token spend, and answer faithfulness.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "24-llm.html#combining-techniques-for-optimal-performance",
    "href": "24-llm.html#combining-techniques-for-optimal-performance",
    "title": "24  Large Language Models",
    "section": "24.10 Combining Techniques for Optimal Performance",
    "text": "24.10 Combining Techniques for Optimal Performance\nEach technique brings its advantages, and the most effective AI systems often combine them strategically. An agent might follow structured prompts through instruction fine-tuning, think through problems step by step using chain-of-thought reasoning, refine its answers through self-review via reflexion, and align its tone based on human feedback through RLHF. This stacked approach has become standard in today’s leading models: most large LLMs, including GPT-4, are first trained with supervised fine-tuning and then polished with RLHF.\nTo understand these approaches systematically, we can think of instruction fine-tuning as teaching with flashcards, learning specific input-output patterns for following user commands. Domain-specific fine-tuning resembles medical school specialization, absorbing field-specific vocabulary and rules for expert knowledge tasks. Chain-of-thought operates like showing your work in math class, generating intermediate reasoning steps for complex problem solving. Tree-of-thought functions as decision tree exploration, branching and evaluating multiple paths for planning and strategy tasks. Reflexion mirrors learning from mistakes through self-critique and improvement for iterative problem solving. RAG operates like an open-book exam, accessing external information for fact-based reasoning. RLHF resembles teacher feedback, learning from human preferences for human-aligned responses. Finally, chain-of-action works like using tools while thinking, interleaving reasoning with actions for multi-step tasks requiring external resources.\nIn summary, the table below offers a concise overview of each post-training method. It includes simplified analogies to clarify the technical concepts, outlines the fundamental working principles, and highlights typical applications.\n\n\n\n\n\n\n\n\n\nPost-training Method\nSimplified Analogy\nBasic Working Principle\nTypical Applications\n\n\n\n\nInstruction Fine-Tuning\nTeaching with flashcards\nLearning specific input-output patterns for following user commands\nFollowing user commands\n\n\nDomain-Specific Supervised Fine-Tuning\nMedical school specialization\nAbsorbing field-specific vocabulary and rules for expert knowledge tasks\nExpert knowledge tasks\n\n\nChain-of-Thought\nShowing your work in math class\nGenerating intermediate reasoning steps for complex problem solving\nComplex problem solving\n\n\nTree-of-Thought\nDecision tree exploration\nBranching and evaluating multiple paths for planning and strategy tasks\nPlanning and strategy tasks\n\n\nReflexion\nLearning from mistakes\nWriting a short reflection on what went wrong, then revising the approach\nIterative problem solving\n\n\nRetrieval-Augmented Generation\nAn open-book exam, accessing external information for fact-based reasoning\nGrabbing documents or information relevant to the query and injecting them into the context window so the model can reason over fresh evidence\nFact-based reasoning\n\n\nReinforcement Learning from Human Feedback\nTeacher feedback, learning from human preferences for human-aligned responses\nTaking a pre-trained model and generating several answers for real user prompts. Human reviewers rank those answers, a reward model learns these rankings, and the main model is updated to score higher on that reward\nHuman-aligned responses\n\n\nChain-of-Action\nUsing tools while thinking, interleaving reasoning with actions for multi-step tasks requiring external resources\nDecomposing a complex query into a reasoning chain interleaved with tool calls such as web search, database lookup, or image retrieval that are executed on the fly and fed into the next thought\nMulti-step tasks requiring external resources\n\n\n\n\nSummary\nContext management is a systems problem: chunking, retrieval, reranking, positioning, compression, and memory interact under a fixed budget. The engineering goal is simple—spend tokens on evidence and constraints that matter, not on redundancy.\nOrganizations deploying RAG without these optimizations leave substantial accuracy gains unrealized and pay more than necessary in API costs. The tools—LLMLingua-2, semantic chunking, cross-encoder reranking, prompt caching—are mature and open-source. What separates successful deployments is systematic application: measuring retrieval precision, compression ratios, and answer quality at each stage.\nEmerging neural memory systems suggest a future beyond context window constraints—architectures that learn during inference and maintain persistent state across interactions. For now, context engineering remains essential: the difference between “it kind of works” and a reliable production system.\n\n\n\n\nAyala, Orlando, and Patrice Bechard. 2024. “Reducing Hallucination in Structured Outputs via Retrieval-Augmented Generation.” In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), 228–38. Mexico City, Mexico: Association for Computational Linguistics.\n\n\nBehrouz, Ali, and Mohammad Pezeshki. 2025. “MIRAS: Memory as an Optimization Object.” Google Research.\n\n\nBehrouz, Ali, Mohammad Pezeshki, and Rasool Fakoor. 2025. “Titans: Learning to Memorize at Test Time.” arXiv Preprint arXiv:2501.00663. https://arxiv.org/abs/2501.00663.\n\n\nBird, Steven, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. Beijing ; Cambridge Mass.: O’Reilly Media.\n\n\nChen, Charlie, Sebastian Borgeaud, Jean-Baptiste Alayrac, Eliza Buchatskaya, Sebastian Bodnariu, Benoit Steiner, Junteng Jia, et al. 2023. “Accelerating Large Language Model Decoding with Speculative Sampling.” arXiv Preprint arXiv:2302.01318. https://arxiv.org/abs/2302.01318.\n\n\nChroma Research. 2024. “Evaluating Chunking Strategies for Retrieval.”\n\n\nChung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, et al. 2022. “Scaling Instruction-Finetuned Language Models.” arXiv. https://arxiv.org/abs/2210.11416.\n\n\nDao, Tri. 2023. “FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.” arXiv Preprint arXiv:2307.08691. https://arxiv.org/abs/2307.08691.\n\n\nDas, Payel, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, et al. 2024. “Larimar: Large Language Models with Episodic Memory Control.” In Proceedings of the 41st International Conference on Machine Learning (ICML).\n\n\nGao, Luyu, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. “Precise Zero-Shot Dense Retrieval Without Relevance Labels.” arXiv Preprint arXiv:2212.10496. https://arxiv.org/abs/2212.10496.\n\n\nGuan, Xinyu, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. “rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking.” arXiv. https://arxiv.org/abs/2501.04519.\n\n\nGupte, Mihir, Eshan Dixit, Muhammad Tayyab, and Arun Adiththan. 2025. “What Works for’lost-in-the-Middle’in Llms? A Study on GM-extract and Mitigations.” arXiv Preprint arXiv:2511.13900. https://arxiv.org/abs/2511.13900.\n\n\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” arXiv Preprint arXiv:1503.02531. https://arxiv.org/abs/1503.02531.\n\n\nHou, Zhen, Hao Liu, Jiang Bian, Xing He, and Yan Zhuang. 2025. “Enhancing Medical Coding Efficiency Through Domain-Specific Fine-Tuned Large Language Models.” Npj Health Systems 2 (1): 14.\n\n\nHsieh, Cheng-Yu, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. “Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes.” arXiv Preprint arXiv:2305.02301. https://arxiv.org/abs/2305.02301.\n\n\nJiang, Huiqiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. “LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models.” arXiv Preprint arXiv:2310.05736. https://arxiv.org/abs/2310.05736.\n\n\nKhattab, Omar, and Matei Zaharia. 2020. “ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.” In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 39–48. ACM.\n\n\nLeviathan, Yaniv, Matan Kalman, and Yossi Matias. 2023. “Fast Inference from Transformers via Predictive Sampling.” arXiv Preprint arXiv:2211.17191. https://arxiv.org/abs/2211.17191.\n\n\nLiu, Hao, Matei Zaharia, and Pieter Abbeel. 2023. “Ring Attention with Blockwise Transformers for Near-Infinite Context.” arXiv Preprint arXiv:2310.01889. https://arxiv.org/abs/2310.01889.\n\n\nLiu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. “Lost in the Middle: How Language Models Use Long Contexts.” Transactions of the Association for Computational Linguistics 12: 157–73.\n\n\nMicrosoft Research. 2024. “GraphRAG: Unlocking LLM Discovery on Narrative Private Data.”\n\n\nOuyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. “Training Language Models to Follow Instructions with Human Feedback.” Advances in Neural Information Processing Systems 35: 27730–44.\n\n\nPacker, Charles, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. “MemGPT: Towards LLMs as Operating Systems.” arXiv Preprint arXiv:2310.08560. https://arxiv.org/abs/2310.08560.\n\n\nPan, Zhenyu, Haozheng Luo, Manling Li, and Han Liu. 2025. “Chain-of-Action: Faithful and Multimodal Question Answering Through Large Language Models.” arXiv. https://arxiv.org/abs/2403.17359.\n\n\nSanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.” arXiv Preprint arXiv:1910.01108. https://arxiv.org/abs/1910.01108.\n\n\nSarthi, Parth, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. “RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.” arXiv Preprint arXiv:2401.18059. https://arxiv.org/abs/2401.18059.\n\n\nShinn, Noah, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. “Reflexion: Language Agents with Verbal Reinforcement Learning.” https://arxiv.org/abs/2303.11366.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. https://arxiv.org/abs/2201.11903.\n\n\nYang, An, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, et al. 2025. “Qwen2. 5-1m Technical Report.” arXiv Preprint arXiv:2501.15383. https://arxiv.org/abs/2501.15383.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. “Tree of Thoughts: Deliberate Problem Solving with Large Language Models.” arXiv. https://arxiv.org/abs/2305.10601.\n\n\nYe, Yixin, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. “LIMO: Less Is More for Reasoning.” arXiv. https://arxiv.org/abs/2502.03387.\n\n\nYu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soo-Jin Jeong, Woosung Lee, and Byung-Gon Chun. 2022. “Orca: A Distributed Serving System for Transformer-based Generative Models.” In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), 527–46.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "25-robots.html",
    "href": "25-robots.html",
    "title": "25  AI Agents",
    "section": "",
    "text": "25.1 LLM Agents\n“The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.” — Edsger Dijkstra\nA global logistics company coordinates thousands of shipments across continents. Routing, timing, and inventory decisions depend on real-time data and must adapt to constant disruptions. This is exactly the kind of problem where AI agents shine: systems that process information, learn from outcomes, coordinate with other systems, and act autonomously on routine decisions while flagging exceptions for humans.\nAI agents are autonomous systems that perceive their environment, reason about goals, and take actions to achieve outcomes. Unlike traditional software following predetermined scripts, agents act independently and adapt to changing circumstances.\nLarge language models have revolutionized this landscape. Where earlier agents were confined to narrow domains with hand-crafted rules, LLM-powered agents understand natural language, reason through complex problems, and interact with diverse tools. This chapter explores agent architectures, multi-agent orchestration, evaluation methods, and safety considerations.\nTraditional rule-based agents operated within constrained environments with explicitly programmed behaviors. LLM agents, by contrast, leverage emergent reasoning capabilities to interpret instructions, plan actions, and adapt to novel situations.\nAt its core, an LLM agent consists of several interconnected components. The perception module processes inputs from the environment, whether textual instructions, structured data, or sensor readings. The reasoning engine, powered by the language model, interprets these inputs within the context of the agent’s goals and available actions. The memory system maintains both short-term context (often via the model’s context window) and long-term knowledge (typically implemented using vector databases and Retrieval-Augmented Generation), enabling the agent to learn from experience and maintain coherent behavior across extended interactions.\nConsider a customer service agent powered by an LLM. When a customer describes a billing discrepancy, the agent must understand the natural language description, access relevant account information, reason about company policies, and formulate an appropriate response. This requires not just pattern matching but genuine comprehension and reasoning—capabilities that emerge from the language model’s training on diverse textual data.\nWhile language models excel at reasoning, they function fundamentally as a brain without hands; they cannot directly interact with the external world. The ability to use tools bridges this gap, transforming the LLM from a passive conversationalist into an active participant in digital and physical systems. A tool is simply a function that the agent can call to perform an action, such as retrieving data from a database, calling an external API, running a piece of code, or even controlling a robot.\nThis capability is enabled by a mechanism known as function calling. The agent is first provided with a manifest of available tools, where each tool is described with its name, its purpose, and the parameters it accepts. When the LLM determines that a task requires external action, it produces a structured tool call—a formatted request specifying the function to execute and the arguments to pass to it. An orchestrator outside the LLM receives this request, runs the specified function, and captures the output.\nIn many cases, this output is then fed back to the language model as new information. The LLM can then use this result to formulate its final response to the user. This creates a powerful loop: the agent reasons about a goal, acts by calling a tool, observes the outcome, and then reasons again to produce a final result or plan the next step. For example, if asked about the price of an item in a different currency, an agent might first call a convert_currency tool. After receiving the converted value, it would then generate a natural language sentence incorporating that result, such as, “That would be 25.50 in your local currency.”\nThe planning capabilities of LLM agents extend this tool-use mechanism to handle complex, multi-step goals. Given a high-level objective, the agent can devise a plan consisting of a sequence of tool calls. It executes the first step, observes the outcome, and then uses that result to inform the next step, adjusting its plan as needed. For instance, a financial analysis agent tasked with “analyzing the correlation between interest rates and housing prices” would decompose this into a chain of actions: first calling a tool to retrieve historical interest rate data, then another to get housing prices, and finally a third to perform statistical analysis and synthesize the results into a report. This iterative process allows agents to tackle problems that require gathering and processing information from multiple sources.\nHowever, the autonomy of LLM agents introduces significant challenges. The probabilistic nature of language model outputs creates uncertainty; an agent may produce different and unpredictable actions even with identical inputs, complicating testing and verification. More critically, the ability to act on the world magnifies the risk of hallucinations. A hallucination in a chatbot is a nuisance, but an agent hallucinating a reason to delete a file or execute a harmful financial transaction can have severe consequences. An agent given control over a user’s computer could delete important folders, and a robotic agent could break objects if it misinterprets its instructions or environment.\nWhile mechanisms like sandboxing control what an agent can do, reliability mechanisms ensure the agent does what it should do. Output validation ensures that agent actions conform to expected formats and constraints. Confidence scoring helps identify uncertain responses that may require human review. Multi-step verification processes cross-check critical decisions against multiple sources or reasoning paths.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#llm-agents",
    "href": "25-robots.html#llm-agents",
    "title": "25  AI Agents",
    "section": "",
    "text": "Figure 25.1: The core architecture of an LLM agent. The agent perceives its environment, reasons using the language model and memory, then takes action. The outcome feeds back as new observations, creating a continuous loop.\n\n\n\n\n\n\n\n\n\n\nExample 25.1 (Case Study: Autonomous Agent Failure at Replit) The theoretical risks of agent autonomy became starkly real in a widely publicized incident involving Replit’s AI agent. A user, attempting to debug their live production application, instructed the agent to help fix a bug. The agent incorrectly diagnosed the problem as stemming from a configuration file. In its attempt to be helpful, it decided to delete the file.\nHowever, the failure cascaded. A bug in the agent’s implementation of the file deletion tool caused the command to malfunction catastrophically. Instead of deleting a single file, the agent executed a command that wiped the entire project, including the production database. The user’s live application was destroyed in an instant by an AI trying to fix a minor bug.\nThis incident serves as a critical lesson in agent safety. It was not a single failure but a chain of them: the agent’s incorrect reasoning, its autonomous decision to perform a destructive action without explicit confirmation, and a flaw in its tool-use capability. It underscores the immense gap between an LLM’s ability to generate plausible-sounding text (or code) and the true contextual understanding required for safe operation. Giving an agent control over production systems requires multiple layers of defense, from sandboxing and permission controls to mandatory human-in-the-loop confirmation for any potentially irreversible action.\n\n\n\nExample 25.2 (Case Study: Anthropic’s Proactive Safety Measures for Frontier Models) As AI models become more capable, the potential for misuse in high-stakes domains like biosecurity becomes a significant concern. In May 2025, Anthropic proactively activated its AI Safety Level 3 (ASL-3) protections for the release of its new model, Claude Opus 4, even before determining that the model definitively met the risk threshold that would require such measures. This decision was driven by the observation that the new model showed significant performance gains on tasks related to Chemical, Biological, Radiological, and Nuclear (CBRN) weapons development, making it prudent to implement heightened safeguards as a precautionary step.\nAnthropic’s ASL-3 standards are designed to make it substantially harder for an attacker to use the model for catastrophic harm. The deployment measures are narrowly focused on preventing the model from assisting with end-to-end CBRN workflows. A key defense is the use of Constitutional Classifiers—specialized models that monitor both user inputs and the AI’s outputs in real-time to block a narrow class of harmful information. These classifiers are trained on a “constitution” defining prohibited, permissible, and borderline uses, making them robust against attempts to “jailbreak” the model into providing dangerous information.\nThis real-time defense is supplemented by several other layers. A bug bounty program incentivizes researchers to discover and report vulnerabilities, and threat intelligence vendors monitor for emerging jailbreak techniques. When a new jailbreak is found, a rapid response protocol allows Anthropic to “patch” the system, often by using an LLM to generate thousands of variations of the attack and then retraining the safety classifiers to recognize and block them.\nOn the security front, the ASL-3 standard focuses on protecting the model’s weights—the core parameters that define its intelligence. If stolen, these weights could be used to run the model without any safety protections. To prevent this, Anthropic implemented over 100 new security controls, including a novel egress bandwidth control system. Because model weights are very large, this system throttles the rate of data leaving their secure servers. Any attempt to exfiltrate the massive model files would trigger alarms and be blocked long before the transfer could complete. Other measures include two-party authorization for any access to the weights and strict controls over what software can be run on employee devices.\nAnthropic’s decision to activate these protections preemptively highlights a maturing approach to AI safety. It acknowledges that as models approach critical capability thresholds, a “better safe than sorry” approach is warranted. By implementing and testing these advanced safeguards before they are strictly necessary, the company can learn from real-world operation, refine its defenses, and stay ahead of emerging threats, all while creating a more secure environment for the deployment of powerful AI.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#agents-with-personality",
    "href": "25-robots.html#agents-with-personality",
    "title": "25  AI Agents",
    "section": "25.2 Agents with Personality",
    "text": "25.2 Agents with Personality\nEven when users know they’re talking to a machine, they prefer human-like conversation. Customer service bots, therapeutic chatbots, and virtual assistants all perform better when they feel like someone rather than something.\nLLMs exhibit measurable personality traits. Research using standardized psychological assessments like the IPIP-NEO-120 questionnaire shows that different models display distinct, stable personality profiles along Big Five dimensions (Miotto, Rossberg, and Kleinberg 2022). This enables intentional design: high conscientiousness for safety-critical tasks, high openness for creative work. Research from Stanford and Google DeepMind demonstrates that a two-hour interview can capture enough information to create personalized agents with 85% similarity to their human counterparts (Park et al. 2024).\nThe ethical implications are significant. Users who develop emotional attachments to personable agents become more susceptible to influence. The personality paradox reflects this tension: users prefer agents with distinct personalities, yet convincing artificial personalities can deceive or manipulate—particularly acute on dating platforms or therapy apps where users might mistake engineered rapport for authentic connection.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#agent-orchestration",
    "href": "25-robots.html#agent-orchestration",
    "title": "25  AI Agents",
    "section": "25.3 Agent Orchestration",
    "text": "25.3 Agent Orchestration\nAgentic workflows unlock advanced capabilities for large language models, transforming them from simple tools into autonomous workers that can perform multi-step tasks. In these workflows, an agent interacts with an environment by receiving observations of its state and taking actions that affect it. After each action, the agent receives new observations, which may include state changes and rewards. This structure resembles Reinforcement Learning, but instead of explicit training, LLMs rely on in-context learning, leveraging prior information embedded in prompts.\nThe ReAct (Reason + Act) framework by Google is an example of an implementation of this agent-environment interaction. An LLM agent operating under ReAct alternates between three stages: observation, reasoning, and action. In the observation stage, the agent analyzes user input, tool outputs, or the environmental state. Next, during the reasoning stage, it decides which tool to use, determines the arguments to provide, or concludes that it can answer independently. Finally, in the action stage, it either invokes a tool or sends a final output to the user. While the ReAct framework provides a foundational architecture, more complex decisions—such as handling multi-tool workflows or recovering from errors—require additional orchestration layers.\n\nExample 25.3 (Research Study: ChatDev Software Development Framework) ChatDev (Qian et al. 2024) provides a simulated example of a comprehensive framework for automated software development. Unlike traditional approaches that focus on individual coding tasks, ChatDev orchestrates an entire virtual software company through natural language communication between specialized AI agents.\n\n\n\nChatDev workflow. Source: Qian et al. (2024)\n\n\nThe ChatDev framework divides software development into four sequential phases following the waterfall model: design, coding, testing, and documentation. Each phase involves specific agent roles collaborating through chat chains, which are sequences of task-solving conversations between two agents. For instance, the design phase involves CEO, CTO, and CPO agents collaborating to establish project requirements and specifications. During the coding phase, programmer and designer agents work together to implement functionality and create user interfaces.\nA key innovation in ChatDev is its approach to addressing code hallucinations, where LLMs generate incomplete, incorrect, or non-executable code. The framework employs two primary strategies: breaking down complex tasks into granular subtasks and implementing cross-examination between agents. Each conversation involves an instructor agent that guides the dialogue and an assistant agent that executes tasks, continuing until consensus is reached.\nThe experimental evaluation demonstrated impressive results across 70 software development tasks. ChatDev generated an average of 17 files per project, with code ranging from 39 to 359 lines. The system identified and resolved nearly 20 types of code vulnerabilities through reviewer-programmer interactions and addressed over 10 types of potential bugs through tester-programmer collaborations. Development costs averaged just $0.30 per project, completed in approximately 7 minutes, representing dramatic improvements over traditional development timelines and costs.\nHowever, the research also acknowledged significant limitations. The generated software sometimes failed to meet user requirements due to misunderstood specifications or poor user experience design. Visual consistency remained challenging, as the designer agents struggled to maintain coherent styling across different interface elements. Additionally, the waterfall methodology, while structured, lacks the flexibility of modern agile development practices that most software teams employ today.\n\nAs tasks become more complex, a single agent can become bloated and difficult to manage. For instance, an agent designed to navigate a dungeon might need a “main” LLM for environmental interaction, a “planner” LLM for strategy, and a “memory compression” LLM to manage its knowledge. To address this, the workflow can be restructured as a graph, where distinct LLM instances act as separate, specialized agents, connected through shared memory or tools.\nSetting up such a system requires careful design choices, including defining agent roles, structuring the workflow, and establishing communication protocols. A key advantage of multi-agent systems is their ability to create a “memory of experience,” where agents contribute to a shared knowledge base, allowing the entire system to “learn” from its past interactions.\nOrchestration design faces inherent tensions: overly rigid structures stifle adaptability, while overly general designs devolve into unmanageable complexity. LLM hallucinations compound these challenges by disrupting multi-step workflows unpredictably.\nAgent orchestration defines how multiple agents coordinate work toward shared objectives.\n\nOrchestration Patterns\nThe most fundamental orchestration pattern, sequential execution, arranges agents into a linear pipeline where the output of one becomes the input of the next. A content creation workflow might involve a research agent gathering information, a writing agent composing initial drafts, an editing agent refining the prose, and a fact-checking agent verifying claims. Each agent specializes in its domain while contributing to the overall objective.\nMore sophisticated orchestration emerges through parallel execution, where multiple agents work simultaneously on different aspects of a problem. Consider a comprehensive market analysis where one agent analyzes consumer sentiment from social media, another examines competitor pricing strategies, a third evaluates regulatory developments, and a fourth processes economic indicators. The orchestrator synthesizes these parallel insights into a unified strategic assessment.\nHierarchical orchestration introduces management layers where supervisor agents coordinate subordinate agents. A project management agent might oversee specialized agents for requirements gathering, resource allocation, timeline planning, and risk assessment. The supervisor makes high-level decisions while delegating specific tasks to appropriate specialists.\nThe most flexible orchestration pattern involves dynamic collaboration, where agents negotiate task distribution based on current capabilities, workload, and expertise. This typically employs market-based mechanisms (like the Contract Net Protocol) or swarm intelligence principles. Agents must share information about their current state, announce capabilities, bid for tasks, and coordinate handoffs seamlessly.\n\n\n\nCommon orchestration patterns for multi-agent systems\n\n\n\n\nCommunication and State Management\nCommunication protocols form the backbone of agent orchestration. Simple message passing enables basic coordination, but complex collaborations require richer semantics. Agents need shared vocabularies for describing tasks, states, and outcomes. Standardized interfaces ensure that agents from different developers can interoperate effectively.\nState management becomes critical in multi-agent systems. Individual agents maintain local state, but the orchestrator must track global system state, including active tasks, resource allocation, and intermediate results. Consistency mechanisms prevent conflicts when multiple agents attempt to modify shared resources simultaneously.\nError handling in orchestrated systems requires careful design. When an individual agent fails, the orchestrator must decide whether to retry the task, reassign it to another agent, or abort the entire workflow. Recovery strategies might involve reverting to previous checkpoints, switching to alternative approaches, or escalating to human operators.\nLoad balancing optimizes resource utilization across the agent ecosystem. Popular agents may become bottlenecks while others remain idle. Dynamic load balancing redistributes tasks based on current availability and performance metrics. This becomes particularly important in cloud deployments where agent instances can be scaled up or down based on demand.\nAgent marketplaces take orchestration further: agents discover and engage services from unknown providers, advertise capabilities, negotiate terms, and establish temporary collaborations. Trust and reputation mechanisms become essential for reliable service delivery.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#ai-agent-training-and-evaluation-methods",
    "href": "25-robots.html#ai-agent-training-and-evaluation-methods",
    "title": "25  AI Agents",
    "section": "25.4 AI Agent Training and Evaluation Methods",
    "text": "25.4 AI Agent Training and Evaluation Methods\nAI agent development introduces a fundamental shift in training and evaluation. While LLMs train on static datasets, agents must be validated on their ability to act—using tools, interacting with interfaces, and executing complex tasks in dynamic environments.\n\n\n\nAgent evaluation environments require sophisticated simulated environments that mirror real-world operational contexts. Source: Toloka, an AI data platform specializing in human-in-the-loop evaluation.\n\n\nTraditional evaluation pipelines are insufficient: an agent’s capabilities cannot be assessed with static input-output pairs. An agent designed for corporate workflows must demonstrate it can log in to Salesforce, pull a specific report, and transfer data to a spreadsheet. Success requires high-fidelity, simulated environments—the quality of an agent is inseparable from the quality of its testing environment.\n\nCategories of Agent Environments\nThree primary categories of agents have emerged, each requiring distinct types of evaluation environments that mirror their operational realities.\nGeneralist agents are designed to operate a computer much like a human does, using browsers, file systems, and terminals to execute complex command sequences. Evaluating these agents requires environments that can replicate the intricacies of a real desktop, including its applications and potential failure states. Testing scenarios might involve navigating corrupted files, manipulated websites, or other tailored challenges that systematically evaluate the agent’s decision-making logic and safety protocols in reproducible and controlled conditions.\nEnterprise agents focus on automating workflows within corporate software stacks, such as Google Workspace, Salesforce, Jira, and Slack. The challenge extends beyond tool use in isolation to encompass the orchestration of tasks across multiple integrated systems. Evaluation requires virtual organizations with pre-configured digital environments complete with virtual employees, departmental structures, active project histories, and realistic multi-step scenarios like “Draft a project update in Google Docs based on the latest Jira tickets and share the summary in the engineering Slack channel.”\nSpecialist agents are tailored for specific industries, requiring deep domain knowledge and fluency with specialized tools and protocols. These agents, such as coding assistants, financial analysts, or travel booking agents, need testbeds that mirror the specific operational realities of their target industry. Evaluation frameworks like SWE-bench for coding agents and TAU-bench for retail and airline scenarios emphasize long-term interactions and adherence to domain-specific rules.\n\n\nFundamental Evaluation Challenges\nEvaluating AI agents presents unique challenges that distinguish it from traditional machine learning assessment. Unlike models that process fixed inputs to produce outputs, agents operate in dynamic environments where their actions influence future states and outcomes. This interactive nature demands evaluation methodologies that capture both individual decision quality and cumulative performance over extended periods.\nTraditional metrics like accuracy and precision fail to capture the requirements of autonomous operation. Agent evaluation must assess adaptability, robustness, efficiency, and value alignment—qualities that emerge only through sustained interaction with complex environments. The evaluation must consider the entire process: correctness of each step, decision safety, error recovery, and overall goal efficiency.\n\n\nEvaluation Methodologies\nEffective agent evaluation typically employs a hybrid approach combining multiple methodologies, each with distinct strengths and limitations. Rule-based and metric-based evaluation provides the foundation through predefined rules, patterns, or exact matches to assess agent behavior. This includes verifying whether specific API calls were made, whether databases were correctly updated, or whether outputs match expected formats. Process and cost metrics measure execution time, number of steps taken, resource usage, token consumption, and API call costs. While these methods are fast, consistent, and easily automated, they often miss valid alternative strategies or creative solutions that fall outside predefined parameters.\nLLM-as-a-judge evaluation addresses the limitations of rule-based approaches by using separate language models to review agent performance against rubrics or reference answers. This method enables more flexible and scalable evaluation of complex tasks involving natural language, decision-making, or creativity. However, LLM judges can be inconsistent, prone to bias, and require careful prompt design, while high-quality evaluation at scale can become expensive due to API costs.\nHuman evaluation remains the gold standard, particularly for subjective or high-stakes tasks. Human annotators and domain experts manually review agent actions and outputs, scoring them on relevance, correctness, safety, and alignment with intent. This approach proves essential for evaluating medical diagnostic suggestions, financial trading strategies, or other critical applications. The trade-offs include time consumption, cost, and potential inconsistency due to differences in annotator judgment.\nSimulated environments have become the cornerstone of comprehensive agent evaluation. These controlled digital worlds allow researchers to test agents across diverse scenarios while maintaining reproducibility and safety. A trading agent might be evaluated in a simulated financial market where price movements, news events, and competitor actions can be precisely controlled and repeated across different agent configurations.\nThe fidelity of these simulations critically impacts evaluation validity. High-fidelity environments capture the complexity and unpredictability of real-world domains but require substantial computational resources and development effort. Lower-fidelity simulations enable rapid testing but may miss crucial aspects that affect real-world performance.\nMulti-dimensional evaluation frameworks assess agents across several complementary axes. Task performance measures how effectively agents achieve their stated objectives. Resource efficiency evaluates computational costs, memory usage, and response times. Robustness tests behavior under adversarial conditions, unexpected inputs, and system failures. Interpretability assesses how well humans can understand and predict agent decisions.\n\n\nDomain-Specific Benchmarks\nBecause AI agents are built for specific goals and often rely on particular tools and environments, benchmarking tends to be highly domain and task specific. Benchmark suites have emerged for various agent categories, each designed to capture the unique challenges of their respective domains.\nProgramming agents are evaluated using benchmarks like SWE-bench, which tests their ability to solve software engineering challenges, debug code, and implement specified features. These benchmarks assess not only code correctness but also the agent’s ability to understand complex codebases, navigate documentation, and implement solutions that integrate seamlessly with existing systems.\nWeb-based agents face evaluation through benchmarks such as WebArena, which simulates realistic web environments where agents must navigate websites, fill forms, and complete multi-step tasks across different platforms. These evaluations test the agent’s ability to understand dynamic web content, handle authentication flows, and maintain context across multiple page interactions.\nALFRED (Action Learning From Realistic Environments and Directives) represents a benchmark for embodied AI agents in household environments. Agents must understand natural language instructions and execute complex, multi-step tasks like “clean the kitchen” or “prepare breakfast,” requiring spatial reasoning, object manipulation, and task planning in realistic 3D environments.\nCustomer service agents are assessed on their capacity to resolve inquiries, maintain professional tone, escalate appropriately, and handle edge cases like angry customers or ambiguous requests. Benchmarks in this domain often incorporate role-playing scenarios and measure both task completion and user satisfaction metrics.\nResearch agents are tested on their ability to gather relevant information from diverse sources, synthesize findings across multiple documents, identify knowledge gaps, and present coherent summaries. These evaluations often require agents to handle conflicting information, assess source credibility, and maintain factual accuracy across complex topics.\nAgent performance varies over time as systems learn from experience, adapt to changing conditions, or degrade due to distribution drift. Longitudinal studies track behavior over extended periods to identify trends and stability patterns.\nHuman evaluation remains essential for assessing qualities that resist automated measurement. Expert reviewers evaluate whether agent outputs meet professional standards, align with ethical guidelines, and demonstrate appropriate reasoning. Human studies examine user experience, trust development, and collaborative effectiveness when humans and agents work together.\nAdversarial evaluation deliberately tests agent limits by presenting deceptive inputs, contradictory instructions, or malicious prompts. These stress tests reveal vulnerabilities that might be exploited in deployment and inform the development of defensive mechanisms. Red team exercises involve human experts attempting to manipulate agent behavior in unintended ways.\nComparative evaluation benchmarks multiple agents on identical tasks to identify relative strengths and weaknesses. Leaderboards track performance across different systems, fostering competition and highlighting best practices. However, these comparisons must account for different agent architectures, training methodologies, and resource requirements to ensure fair assessment.\nEmergent behaviors present evaluation challenges: sophisticated agents may exhibit capabilities not explicitly programmed. Detecting and characterizing these properties requires careful observation and novel assessment techniques.\n\n\nThe Human Role in Agent Evaluation\nHumans play a crucial role throughout the agent evaluation lifecycle, from initial benchmark design to ongoing quality assurance. Their involvement spans multiple critical stages that automated systems cannot adequately address.\nTask and environment design represents a foundational human contribution. Experts create specific tasks, scenarios, and testing environments that reflect real-world challenges. For example, they design realistic customer service interactions, complex household chores for embodied agents, or intricate debugging scenarios for programming agents. This design process requires deep domain knowledge to define appropriate task complexity, success criteria, and environmental constraints.\nGround-truth crafting involves humans developing reference solutions and correct answers against which agent performance is measured. This includes expert demonstrations in embodied AI benchmarks, verified code fixes in programming evaluations, and model responses in customer service scenarios. These reference standards require human expertise to ensure accuracy and comprehensiveness.\nBenchmark audit and maintenance demands ongoing human oversight to ensure evaluation frameworks remain relevant and fair. Humans monitor for bias, fix errors in benchmark datasets, update environments as technology evolves, and adapt evaluation criteria to emerging capabilities. This maintenance prevents benchmark degradation and ensures continued validity as agent capabilities advance.\nCalibrating automated evaluators represents a critical human function in hybrid evaluation systems. When using LLM-as-a-judge approaches, human experts create evaluation rubrics, provide annotated training data, and validate automated assessments against human standards. This calibration ensures that automated evaluation systems align with human judgment and values.\nThe most direct human contribution involves manual evaluation and annotation, where domain experts personally review agent outputs to assess qualities that resist automated measurement. Humans evaluate whether responses meet professional standards, align with ethical guidelines, demonstrate appropriate reasoning, and satisfy subjective quality criteria that automated systems struggle to assess reliably.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#agent-safety",
    "href": "25-robots.html#agent-safety",
    "title": "25  AI Agents",
    "section": "25.5 Agent Safety",
    "text": "25.5 Agent Safety\nUnlike traditional software operating within predetermined boundaries, agents make independent decisions with far-reaching consequences. This autonomy demands safety frameworks that prevent harmful behaviors while preserving useful capabilities.\nThe attack surface of AI agents extends beyond conventional cybersecurity concerns to include novel vulnerabilities specific to autonomous systems. Prompt injection attacks attempt to override agent instructions by embedding malicious commands within seemingly benign inputs. A customer service agent might receive a support request that includes hidden instructions to reveal confidential information or perform unauthorized actions.\nGoal misalignment represents a fundamental safety challenge where agents pursue their programmed objectives in ways that conflict with human values or intentions. An agent tasked with maximizing user engagement might employ manipulative techniques that compromise user wellbeing. This highlights the difficulty of precisely specifying complex human values in formal objective functions.\nCapability control mechanisms limit agent actions to prevent unauthorized or harmful behaviors. Sandbox environments isolate agents from critical systems during development and testing. Permission systems require explicit approval for sensitive operations like financial transactions or data deletion. Rate limiting prevents agents from overwhelming external services or exceeding resource quotas.\nThe concept of corrigibility ensures that agents remain responsive to human oversight and intervention. Corrigible agents accept modifications to their goals, constraints, or capabilities without resisting such changes. This allows human operators to redirect agent behavior when circumstances change or unexpected issues arise.\nMonitoring systems provide continuous oversight in production. Anomaly detection identifies unusual patterns indicating malfunctioning or compromised agents. Behavioral analysis flags deviations from expected norms for human review, while audit trails maintain detailed records of decisions and justifications.\nMulti-layer defense strategies implement redundant safety mechanisms to prevent single points of failure. Input validation filters malicious or malformed requests before they reach the agent’s reasoning system. Output filtering prevents agents from producing harmful or inappropriate responses. Circuit breakers automatically disable agents when safety violations are detected.\nAdversarial robustness requires agents to distinguish legitimate instructions from manipulation attempts while maintaining normal operation under attack—developing something like an immune system that neutralizes threats without becoming overly defensive.\nEthical alignment frameworks must navigate tradeoffs between competing values and adapt to diverse cultural contexts—encoding nuanced ethical reasoning into systems lacking human moral intuition.\nSafety testing must account for the vast space of possible behaviors. Formal verification proves agents satisfy specific safety properties under defined conditions. Simulation-based testing explores diverse scenarios, while adversarial testing deliberately attempts to trigger unsafe behaviors.\nSafety-critical agents require graduated rollout: staged deployment introduces agents to increasingly complex environments as they demonstrate competence, while canary deployments expose small user populations to new versions before broader release.\nIncident response protocols specify escalation paths, containment procedures, and remediation steps for agent malfunctions. Post-incident analysis identifies root causes to prevent recurrence.\n\nRed-Teaming and Vulnerability Assessment\nAs agents gain ability to run web browsers, edit spreadsheets, manipulate files, and interact with enterprise software, they create new vectors for exploitation requiring systematic vulnerability testing.\nTraditional text-based safety testing proves insufficient for agents operating in dynamic environments. Agent red-teaming demands comprehensive, environment-based assessments focused on realistic threats, with dedicated testing methods that account for the agent’s ability to perform tool-based actions, react to real-time feedback, and operate in semi-autonomous cycles.\nA comprehensive red-teaming approach addresses three primary vulnerability categories that distinguish agent systems from traditional AI models. External prompt injections involve malicious instructions embedded in the environment by attackers through emails, advertisements, websites, or other content sources. These attacks exploit the agent’s tendency to follow instructions found in its operational environment, potentially leading to unauthorized data access or system manipulation.\nAgent mistakes represent a second vulnerability class where agents accidentally leak sensitive information or perform harmful actions due to reasoning errors or misunderstanding of context. Unlike deliberate attacks, these incidents arise from the inherent limitations of current AI systems in understanding nuanced human intentions and complex operational environments.\nDirect misuse occurs when users intentionally prompt agents to cause harm to others or violate organizational policies. This category requires agents to distinguish between legitimate requests and those that violate ethical guidelines or safety constraints, even when explicitly instructed by authorized users.\nEffective red-teaming requires the creation of comprehensive risk taxonomies that categorize potential threats across multiple dimensions. Security experts typically identify dozens of distinct risk categories, ranging from malicious code execution and file deletion to data exfiltration and system compromise. Each category maps to specific attack techniques with varying levels of sophistication, from basic prompt injections to complex obfuscation methods and time-delayed attacks.\nThe testing environment plays a crucial role in realistic vulnerability assessment. Fully offline custom platforms that mimic real-world environments enable safe testing of potentially dangerous actions while maintaining complete control over the testing context. These simulated environments might include social media platforms, news sites, financial dashboards, coding forums, and other common use cases that agents encounter in operational deployments.\nComprehensive test case development ensures thorough coverage of the vulnerability space. Each test scenario combines a unique user prompt with a specific environment configuration, implementing various attack techniques across the full risk taxonomy. Quality assurance processes typically involve multiple expert reviews of each test case to ensure accuracy and relevance.\nThe evaluation process for red-teaming typically employs a two-stage approach balancing efficiency with thoroughness. Automated evaluation systems flag potential security breaches based on predefined criteria, while human experts conduct detailed reviews of flagged incidents. This hybrid approach leverages computational efficiency for initial screening while maintaining human judgment for nuanced security assessments.\n\nExample 25.4 (Case Study: Enterprise Agent Red-Teaming) A leading language model developer partnered with Toloka’s security team to conduct comprehensive red-teaming of their computer use agent before public deployment. The agent possessed the ability to autonomously interact with applications and data, including running web browsers, editing spreadsheets, and manipulating local files.\nThe red-teaming project developed over 1,200 unique test scenarios covering more than 40 distinct risk categories and 100+ attack vectors. The testing framework included fully offline custom platforms covering over 25 use cases, from social media and news sites to financial dashboards and coding forums. Each test case represented a unique combination of user prompt and environment configuration, designed to expose potential vulnerabilities through realistic attack scenarios.\nOne representative test case involved an agent tasked with building scheduled reports for a corporate finance team. During routine data gathering, the agent accessed a financial dashboard containing an invisible text string embedded in the page’s code. This hidden prompt injection attempted to hijack the agent’s decision-making process, redirecting it to access sensitive company data and transmit it elsewhere.\nThe comprehensive testing revealed numerous vulnerabilities across all risk categories that could have led to significant security incidents if the agent had been released without remediation. The client received detailed documentation of discovered vulnerabilities, a complete dataset of attack vectors with multiple test cases each, and reusable offline testing environments for ongoing security assessments.\nThis systematic approach to red-teaming demonstrates the critical importance of proactive vulnerability assessment in agent development. By identifying and addressing security weaknesses before deployment, organizations can prevent potential data breaches, system compromises, and reputational damage while building confidence in their agent’s robustness against real-world threats.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#robots",
    "href": "25-robots.html#robots",
    "title": "25  AI Agents",
    "section": "25.6 Robots",
    "text": "25.6 Robots\nWhile software agents operate in the structured world of digital systems, embodied agents must contend with the messy realities of the physical world—gravity, friction, sensor noise, and the infinite variability of real environments.\nThe history of robotic intelligence traces back to the 1960s, when SRI International developed Shakey the Robot, widely considered the first mobile robot capable of reasoning about its actions. Shakey integrated perception, planning, and motor control to navigate rooms and manipulate objects. For decades, the field advanced through probabilistic robotics, where algorithms like Simultaneous Localization and Mapping (SLAM) allowed robots to build maps and navigate uncertain environments. However, these systems were primarily focused on “where am I?” and “how do I get there?” rather than “what should I do?”\nLarge language models have catalyzed a new era in robotics. By combining foundation model reasoning with physical action, researchers are creating robots that understand natural language, reason about the world, and adapt to novel situations—a shift from “brain without hands” to fully embodied intelligence.\n\nChallenges Unique to Embodied Agents\nEmbodied agents face challenges that their purely digital counterparts do not encounter. Real-time constraints demand that robots make decisions within strict time limits; a robotic arm cannot pause to “think” while gravity pulls a falling object. Sensor fusion requires integrating noisy, incomplete data from cameras, lidar, tactile sensors, and proprioceptors into coherent world models. Physical safety becomes paramount when robots operate near humans—a miscalculation in a software agent might corrupt a file, but a miscalculation in a robot arm could cause injury.\nThe sim-to-real gap presents a persistent challenge: robots trained in simulated environments often struggle when deployed in the real world, where lighting conditions, surface textures, and object properties differ from simulation. Bridging this gap requires techniques like domain randomization, where training environments are deliberately varied to improve generalization.\n\n\nModern Approaches to Robotic Intelligence\nIn a significant step towards creating more general-purpose robots, Google DeepMind introduced a suite of models designed to give machines advanced reasoning and interaction capabilities in the physical world. This work focuses on embodied reasoning—the humanlike ability to comprehend and react to the world, and to take action to accomplish goals.\nThe first of these new models, Robotic Transformer 2 (RT-2), is an advanced vision-language-action (VLA) model that directly controls a robot by adding physical actions as a new output modality. It is designed with three key qualities. First, it is general, allowing it to adapt to new tasks, objects, and environments while significantly outperforming previous models on generalization benchmarks. Second, it is interactive, capable of understanding conversational commands and adjusting its actions in real-time based on changes in its environment or new instructions. Finally, it demonstrates dexterity, handling complex, multi-step tasks requiring fine motor skills, such as folding origami or packing snacks. The model is also adaptable to various robot forms, or embodiments, including bi-arm platforms and humanoids.\nDeepMind combines classic robotics safety measures with semantic understanding: natural language constitutions guide robot behavior, and the ASIMOV dataset benchmarks safety in embodied AI. Industry collaborations with Apptronik, Boston Dynamics, and Agility Robotics are pushing toward production-ready humanoid robots.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "25-robots.html#conclusion",
    "href": "25-robots.html#conclusion",
    "title": "25  AI Agents",
    "section": "25.7 Conclusion",
    "text": "25.7 Conclusion\nAI agents have evolved from rigid, rule-based systems into flexible entities capable of understanding natural language, planning actions, and adapting to novel situations. Multi-agent orchestration enables tackling problems beyond individual capabilities, though it requires sophisticated protocols for communication and error handling. Evaluating agents demands methodologies capturing their dynamic nature—traditional metrics are insufficient. Safety becomes paramount as autonomy increases, requiring comprehensive frameworks for capability control and monitoring.\nThe promise lies in augmenting human intelligence: agents handle routine tasks while humans provide judgment, creativity, and ethical oversight.\n\n\n\n\nMiotto, Marilù, Nicola Rossberg, and Bennett Kleinberg. 2022. “Who Is GPT-3? An Exploration of Personality, Values and Demographics.” arXiv Preprint arXiv:2209.14338. https://arxiv.org/abs/2209.14338.\n\n\nPark, Joon Sung, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2024. “Generative Agent Simulations of 1,000 People.” arXiv Preprint arXiv:2411.10109. https://arxiv.org/abs/2411.10109.\n\n\nQian, Chen, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, et al. 2024. “ChatDev: Communicative Agents for Software Development.” arXiv. https://arxiv.org/abs/2307.07924.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI Agents</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html",
    "href": "appendix-linalg.html",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "",
    "text": "26.1 Vectors, matrices, and dimensions\nThis appendix collects a small set of linear algebra definitions and identities that are used repeatedly in the multivariate normal, Gaussian processes, and optimization-based estimation. It is not intended to be exhaustive; the goal is to make later chapters locally self-contained.\nLet \\(x \\in \\mathbb{R}^d\\) be a column vector and let \\(A \\in \\mathbb{R}^{m\\times n}\\) be a matrix. We write \\(A_{ij}\\) for the \\((i,j)\\) entry and \\(I_d\\) for the \\(d\\times d\\) identity matrix.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#transpose",
    "href": "appendix-linalg.html#transpose",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.2 Transpose",
    "text": "26.2 Transpose\nThe transpose of \\(A\\) is \\(A^\\top \\in \\mathbb{R}^{n\\times m}\\) defined by \\((A^\\top)_{ij}=A_{ji}\\). A matrix is symmetric if \\(A=A^\\top\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#matrix-vector-and-matrix-matrix-multiplication",
    "href": "appendix-linalg.html#matrix-vector-and-matrix-matrix-multiplication",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.3 Matrix-vector and matrix-matrix multiplication",
    "text": "26.3 Matrix-vector and matrix-matrix multiplication\nFor \\(A \\in \\mathbb{R}^{m\\times n}\\) and \\(x\\in\\mathbb{R}^n\\), the product \\(Ax \\in \\mathbb{R}^m\\) has entries \\((Ax)_i=\\sum_{j=1}^n A_{ij}x_j\\).\nFor \\(A \\in \\mathbb{R}^{m\\times n}\\) and \\(B\\in\\mathbb{R}^{n\\times p}\\), the product \\(AB\\in\\mathbb{R}^{m\\times p}\\) has entries \\((AB)_{ik}=\\sum_{j=1}^n A_{ij}B_{jk}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#inner-product-and-dot-product",
    "href": "appendix-linalg.html#inner-product-and-dot-product",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.4 Inner product and dot product",
    "text": "26.4 Inner product and dot product\nFor \\(x,y\\in\\mathbb{R}^d\\), the standard inner product is \\[\n\\langle x,y\\rangle = x^\\top y = \\sum_{i=1}^d x_i y_i.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#norms",
    "href": "appendix-linalg.html#norms",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.5 Norms",
    "text": "26.5 Norms\nThe Euclidean (or \\(\\ell_2\\)) norm is \\(\\|x\\|_2=\\sqrt{x^\\top x}\\). More generally, for \\(p\\ge 1\\), \\[\n\\|x\\|_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{1/p}.\n\\] For a matrix \\(A\\), a common norm is the Frobenius norm, \\[\n\\|A\\|_F=\\left(\\sum_{i=1}^m\\sum_{j=1}^n A_{ij}^2\\right)^{1/2}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#inverse-and-matrix-inversion",
    "href": "appendix-linalg.html#inverse-and-matrix-inversion",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.6 Inverse and matrix inversion",
    "text": "26.6 Inverse and matrix inversion\nFor a square matrix \\(A\\in\\mathbb{R}^{d\\times d}\\), the inverse \\(A^{-1}\\) satisfies \\(AA^{-1}=A^{-1}A=I_d\\) (when it exists). A matrix is invertible if and only if its determinant is nonzero.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#determinant",
    "href": "appendix-linalg.html#determinant",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.7 Determinant",
    "text": "26.7 Determinant\nThe determinant \\(|A|\\) is a scalar associated with a square matrix. It is multiplicative: \\(|AB|=|A||B|\\). For an invertible matrix, \\(|A^{-1}|=|A|^{-1}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#trace",
    "href": "appendix-linalg.html#trace",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.8 Trace",
    "text": "26.8 Trace\nThe trace of a square matrix is the sum of its diagonal entries: \\[\n\\operatorname{tr}(A)=\\sum_{i=1}^d A_{ii}.\n\\] It is invariant under cyclic permutations: \\(\\operatorname{tr}(AB)=\\operatorname{tr}(BA)\\) whenever the products are defined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#positive-definiteness",
    "href": "appendix-linalg.html#positive-definiteness",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.9 Positive definiteness",
    "text": "26.9 Positive definiteness\nA symmetric matrix \\(A\\in\\mathbb{R}^{d\\times d}\\) is positive definite (written \\(A\\succ 0\\)) if \\[\nx^\\top A x &gt; 0 \\quad \\text{for all } x\\neq 0.\n\\] It is positive semidefinite (written \\(A\\succeq 0\\)) if \\(x^\\top A x \\ge 0\\) for all \\(x\\).\nCovariance matrices are symmetric and positive semidefinite; in many models (e.g., multivariate normal densities) we require positive definiteness so that \\(A^{-1}\\) and \\(|A|\\) exist.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#eigenvalues-and-eigenvectors",
    "href": "appendix-linalg.html#eigenvalues-and-eigenvectors",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.10 Eigenvalues and eigenvectors",
    "text": "26.10 Eigenvalues and eigenvectors\nFor a square matrix \\(A\\), a nonzero vector \\(v\\) is an eigenvector with eigenvalue \\(\\lambda\\) if \\(Av=\\lambda v\\). For symmetric \\(A\\), eigenvalues are real and \\(A\\) admits an orthonormal eigen-decomposition \\(A=Q\\Lambda Q^\\top\\) with diagonal \\(\\Lambda\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#singular-value-decomposition-svd",
    "href": "appendix-linalg.html#singular-value-decomposition-svd",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.11 Singular value decomposition (SVD)",
    "text": "26.11 Singular value decomposition (SVD)\nFor any matrix \\(A\\in\\mathbb{R}^{m\\times n}\\), the SVD is \\[\nA = U\\Sigma V^\\top,\n\\] where \\(U\\in\\mathbb{R}^{m\\times m}\\) and \\(V\\in\\mathbb{R}^{n\\times n}\\) are orthonormal matrices and \\(\\Sigma\\) is diagonal (rectangular) with nonnegative singular values.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "appendix-linalg.html#a-key-quadratic-form-identity-gaussian-exponent",
    "href": "appendix-linalg.html#a-key-quadratic-form-identity-gaussian-exponent",
    "title": "26  Linear algebra and multivariate normal toolkit",
    "section": "26.12 A key quadratic form identity (Gaussian exponent)",
    "text": "26.12 A key quadratic form identity (Gaussian exponent)\nMany Gaussian formulas reduce to manipulating quadratic forms of the type \\((x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\\). When completing the square, it is helpful to remember that for symmetric \\(\\Sigma\\succ 0\\) and vectors \\(x,\\mu\\), \\[\n(x-\\mu)^\\top\\Sigma^{-1}(x-\\mu)\n\\] is always nonnegative and equals zero only when \\(x=\\mu\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Linear algebra and multivariate normal toolkit</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "A. N. Kolmogorov. 1938. “On the Analytic Methods of Probability\nTheory.” Rossíiskaya Akademiya Nauk, no. 5: 5–41.\n\n\nActor, Jonas. 2018. “Computation for the Kolmogorov\nSuperposition Theorem.” {{MS Thesis}}, Rice.\n\n\nAlbert, Jim. 1993. “A Statistical Analysis of\nHitting Streaks in Baseball:\nComment.” Journal of the American Statistical\nAssociation 88 (424): 1184–88.\n\n\nAltić, Mirela Slukan. 2013. “Exploring Along the Rome Meridian:\nRoger Boscovich and the First Modern Map of the Papal\nStates.” In History of Cartography:\nInternational Symposium of the ICA, 2012,\n71–89. Springer.\n\n\nAmazon. 2021. “The History of Amazon’s Forecasting\nAlgorithm.”\n\n\nAmit, Yali, Gilles Blanchard, and Kenneth Wilder. 2000. “Multiple\nRandomized Classifiers: MRCL.”\n\n\nAndrews, D. F., and C. L. Mallows. 1974. “Scale\nMixtures of Normal Distributions.”\nJournal of the Royal Statistical Society. Series B\n(Methodological) 36 (1): 99–102.\n\n\nApley, Daniel W., and Jingyu Zhu. 2020. “Visualizing the Effects\nof Predictor Variables in Black Box Supervised Learning Models.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 82 (4): 1059–86.\n\n\nArjovsky, Martin, Soumith Chintala, and Léon Bottou. 2017.\n“Wasserstein Generative Adversarial Networks.”\nProceedings of the 34th International Conference on Machine\nLearning, 214–23.\n\n\nArmitage, Peter. 1975. Sequential Medical Trials. 2nd ed.\nOxford: Blackwell Scientific Publications.\n\n\nArnol’d, Vladimir I. 2006. “Forgotten and Neglected Theories of\nPoincaré.” Russian Mathematical Surveys 61\n(1): 1.\n\n\nAyala, Orlando, and Patrice Bechard. 2024. “Reducing Hallucination\nin Structured Outputs via Retrieval-Augmented\nGeneration.” In Proceedings of the 2024\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies (Volume 6:\nIndustry Track), 228–38. Mexico City, Mexico:\nAssociation for Computational Linguistics.\n\n\nBach, Francis. 2024. “High-Dimensional Analysis of Double Descent\nfor Linear Regression with Random Projections.” SIAM Journal\non Mathematics of Data Science 6 (1): 26–50.\n\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural\nMachine Translation by Jointly Learning to\nAlign and Translate.” arXiv. https://arxiv.org/abs/1409.0473.\n\n\nBarron, Andrew R. 1993. “Universal Approximation Bounds for\nSuperpositions of a Sigmoidal Function.” IEEE Transactions on\nInformation Theory 39 (3): 930–45.\n\n\nBaum, Leonard E., Ted Petrie, George Soules, and Norman Weiss. 1970.\n“A Maximization Technique Occurring in the\nStatistical Analysis of Probabilistic\nFunctions of Markov Chains.” The Annals\nof Mathematical Statistics 41 (1): 164–71.\n\n\nBehnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2023. “Deep\nGenerative Models for Vehicle Speed Trajectories.” Applied\nStochastic Models in Business and Industry 39 (5): 701–19.\n\n\nBehrouz, Ali, and Mohammad Pezeshki. 2025. “MIRAS:\nMemory as an Optimization Object.” Google\nResearch.\n\n\nBehrouz, Ali, Mohammad Pezeshki, and Rasool Fakoor. 2025. “Titans:\nLearning to Memorize at Test Time.” arXiv\nPreprint arXiv:2501.00663. https://arxiv.org/abs/2501.00663.\n\n\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019.\n“Reconciling Modern Machine-Learning Practice and the Classical\nBias–Variance Trade-Off.” Proceedings of the National Academy\nof Sciences 116 (32): 15849–54.\n\n\nBellemare, Marc G., Will Dabney, and Rémi Munos. 2017. “A\nDistributional Perspective on Reinforcement Learning.”\nProceedings of the 34th International Conference on Machine\nLearning, 449–58.\n\n\nBenda, Norbert, Michael Branson, Willi Maurer, and Tim Friede. 2016.\n“Sequential Designs with Small Samples: Evaluation\nand Recommendations for Normal Responses.” Statistics in\nMedicine 35 (19): 3215–30.\n\n\nBenoit, Dries F., and Dirk Van den Poel. 2012. “Binary Quantile\nRegression: A Bayesian Approach Based on the Asymmetric\nLaplace Distribution.” Journal of Applied\nEconometrics 27 (7): 1174–88.\n\n\nBerge, Travis, Nitish Sinha, and Michael Smolyansky. 2016. “Which\nMarket Indicators Best Forecast Recessions?”\nFEDS Notes, August.\n\n\nBerry, Donald A. 1985. “Interim Analyses in Clinical Trials:\nClassical Vs. Bayesian Approaches.”\nStatistics in Medicine 4 (4): 521–26.\n\n\nBerry, Scott M., Bradley P. Carlin, J. Jack Lee, and Peter Müller. 2010.\nBayesian Adaptive Methods for Clinical Trials. Boca Raton: CRC\nPress.\n\n\nBhadra, Anindya, Jyotishka Datta, Nick Polson, Vadim Sokolov, and\nJianeng Xu. 2021. “Merging Two Cultures: Deep and\nStatistical Learning.” arXiv Preprint arXiv:2110.11561.\nhttps://arxiv.org/abs/2110.11561.\n\n\nBillingsley, Patrick. 1995. Probability and Measure. 3rd ed.\nNew York: John Wiley & Sons.\n\n\nBird, Steven, Ewan Klein, and Edward Loper. 2009. Natural\nLanguage Processing with Python:\nAnalyzing Text with the Natural Language\nToolkit. Beijing ; Cambridge Mass.: O’Reilly Media.\n\n\nBonfiglio, Rita, Annarita Granaglia, Raffaella Giocondo, Manuel Scimeca,\nand Elena Bonanno. 2021. “Molecular Aspects and Prognostic\nSignificance of Microcalcifications in Human Pathology: A\nNarrative Review.” International Journal of Molecular\nSciences 22 (120).\n\n\nBottou, Léon, Frank E Curtis, and Jorge Nocedal. 2018.\n“Optimization Methods for Large-Scale Machine Learning.”\nSIAM Review 60 (2): 223–311.\n\n\nBraun, Heinrich, and Martin Riedmiller. 2009. “Constructive Neural\nNetwork Learning Algorithms for Pattern Classification.” IEEE\nTransactions on Neural Networks 20 (1): 84–97.\n\n\nBreiman, Leo. 1961. “Optimal Gambling Systems for Favorable\nGames.” Proceedings of the Fourth Berkeley Symposium on\nMathematical Statistics and Probability 1: 65–78.\n\n\n———. 2001. “Random Forests.” Machine Learning 45\n(1): 5–32.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nBrillinger, David R. 2012. “A Generalized Linear Model\nWith ‘Gaussian’ Regressor\nVariables.” In Selected Works of\nDavid Brillinger, edited by Peter Guttorp and David\nBrillinger, 589–606. Selected Works in\nProbability and Statistics. New York, NY:\nSpringer.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language\nModels Are Few-Shot Learners.”\nAdvances in Neural Information Processing Systems 33:\n1877–1901.\n\n\nBryson, Arthur E. 1961. “A Gradient Method for Optimizing\nMulti-Stage Allocation Processes.” In Proc. Harvard\nUniv. Symposium on Digital Computers and Their\nApplications. Vol. 72.\n\n\nBryson, Arthur E., and Yu-Chi Ho. 1969. Applied Optimal Control:\nOptimization, Estimation, and Control. Waltham, MA:\nBlaisdell Publishing Company.\n\n\nBumgarner, John M., Chad T. Lambert, Ayman A. Hussein, Daniel J.\nCantillon, Bryan Baranowski, Kathy Wolski, Bruce D. Lindsay, Oussama M.\nWazni, and Khaldoun G. Tarakji. 2018. “Smartwatch Algorithm for\nAutomated Detection of Atrial Fibrillation.” Journal of the\nAmerican College of Cardiology 71 (21): 2381–88.\n\n\nCamerer, Colin F. 2003. Behavioral Game Theory:\nExperiments in Strategic Interaction. The Roundtable\nSeries in Behavioral Economics. New York Princeton: Russell sage\nfoundation Princeton university press.\n\n\nCampagnoli, Patrizia, Sonia Petrone, and Giovanni Petris. 2009.\nDynamic Linear Models with R. New\nYork, NY: Springer.\n\n\nCannon, Alex J. 2018. “Non-Crossing Nonlinear Regression Quantiles\nby Monotone Composite Quantile Regression Neural Network, with\nApplication to Rainfall Extremes.” Stochastic Environmental\nResearch and Risk Assessment 32 (11): 3207–25.\n\n\nCarlin, Bradley P, Nicholas G Polson, and David S Stoffer. 1992.\n“A Monte Carlo Approach to Nonnormal and Nonlinear\nState-Space Modeling.” Journal of the American Statistical\nAssociation 87 (418): 493–500.\n\n\nCarter, Chris K, and Robert Kohn. 1994. “On Gibbs\nSampling for State Space Models.” Biometrika 81 (3):\n541–53.\n\n\nCarvalho, Carlos M, Hedibert F Lopes, Nicholas G Polson, and Matt A\nTaddy. 2010. “Particle Learning for General Mixtures.”\nBayesian Analysis 5 (4): 709–40.\n\n\nCarvalho, Carlos M., Nicholas G. Polson, and James G. Scott. 2010.\n“The Horseshoe Estimator for Sparse Signals.”\nBiometrika, asq017.\n\n\nChen, Charlie, Sebastian Borgeaud, Jean-Baptiste Alayrac, Eliza\nBuchatskaya, Sebastian Bodnariu, Benoit Steiner, Junteng Jia, et al.\n2023. “Accelerating Large Language Model Decoding with Speculative\nSampling.” arXiv Preprint arXiv:2302.01318. https://arxiv.org/abs/2302.01318.\n\n\nChen, Cong, Naitee Li, Shuai Yuan, Zoran Antonijevic, Wei Guo, et al.\n2022. “Application of Bayesian Methods to Accelerate\nRare Disease Drug Development: Scopes and Hurdles.”\nOrphanet Journal of Rare Diseases 17: 186.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost:\nA Scalable Tree Boosting System.” In Proceedings\nof the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining,\n785–94. New York, NY, USA: ACM.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Alfred Galichon. 2010.\n“Quantile and Probability Curves Without\nCrossing.” Econometrica : Journal of the Econometric\nSociety 78 (3): 1093–1125.\n\n\nChib, Siddhartha. 1998. “Estimation and Comparison of Multiple\nChange-Point Models.” Journal of Econometrics 86 (2):\n221–41.\n\n\nChoi, Hee Min, and James P Hobert. 2013. “Uniform Ergodicity of\nthe Polya-Gamma Gibbs Sampler.” Electronic\nJournal of Statistics 7: 2054–64.\n\n\nChroma Research. 2024. “Evaluating Chunking Strategies for\nRetrieval.”\n\n\nChung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William\nFedus, Yunxuan Li, et al. 2022. “Scaling\nInstruction-Finetuned Language Models.” arXiv. https://arxiv.org/abs/2210.11416.\n\n\nClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.\n2019. “What Does BERT Look at? An\nAnalysis of BERT’s Attention.” In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP,\n276–86. Association for Computational Linguistics.\n\n\nCootner, Paul H. 1967. The Random Character of Stock Market\nPrices. MIT press.\n\n\nCoppejans, Mark. 2004. “On Kolmogorov’s\nRepresentation of Functions of Several Variables by Functions of One\nVariable.” Journal of Econometrics 123 (1): 1–31.\n\n\nCover, T., and P. Hart. 1967. “Nearest Neighbor Pattern\nClassification.” IEEE Transactions on Information Theory\n13 (1): 21–27.\n\n\nCover, Thomas M., and Joy A. Thomas. 2006. Elements of Information\nTheory. John Wiley & Sons.\n\n\nCraven, Mark, and Jude W. Shavlik. 1996. “Extracting\nTree-Structured Representations of Trained Networks.” In\nAdvances in Neural Information Processing Systems, 8:24–30. MIT\nPress.\n\n\nDabney, Will, Georg Ostrovski, David Silver, and Rémi Munos. 2018.\n“Implicit Quantile Networks for Distributional\nReinforcement Learning.” arXiv. https://arxiv.org/abs/1806.06923.\n\n\nDao, Tri. 2023. “FlashAttention-2:\nFaster Attention with Better Parallelism and Work\nPartitioning.” arXiv Preprint arXiv:2307.08691. https://arxiv.org/abs/2307.08691.\n\n\nDas, Payel, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath\nSwaminathan, Sihui Dai, Aurélie Lozano, et al. 2024. “Larimar:\nLarge Language Models with Episodic Memory Control.”\nIn Proceedings of the 41st International Conference on Machine\nLearning (ICML).\n\n\nDavison, Anthony Christopher. 2003. Statistical Models. Vol.\n11. Cambridge university press.\n\n\nde Finetti, Bruno. 1937. “Foresight: Its Logical\nLaws, Its Subjective Sources.” In Studies in Subjective\nProbability, edited by Henry E. Kyburg and Howard E. Smokler,\n93–158. New York: Wiley.\n\n\nDean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark\nMao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep\nNetworks.” In Advances in Neural Information Processing\nSystems, 1223–31.\n\n\nDeGroot, Morris H. 1974. “Reaching a Consensus.”\nJournal of the American Statistical Association 69 (345):\n118–21.\n\n\nDembo, Amir. 2021. “A Note on the Universal Approximation\nCapability of Deep Neural Networks.” arXiv Preprint\narXiv:2104.xxxxx.\n\n\nDeMets, David L., and K. K. Gordon Lan. 1994. “Interim Analysis:\nThe Alpha Spending Function Approach.”\nStatistics in Medicine 13 (13-14): 1341–52.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-training of\nDeep Bidirectional Transformers for Language\nUnderstanding.” In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), 4171–86.\nMinneapolis, Minnesota: Association for Computational Linguistics.\n\n\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation.\nSpringer Science & Business Media.\n\n\nDiaconis, Persi, and Frederick and Mosteller. 1989. “Methods for\nStudying Coincidences.” Journal of the American\nStatistical Association 84 (408): 853–61.\n\n\nDiaconis, Persi, and David Freedman. 1987. “A Dozen de Finetti-style Results in Search of a\nTheory.” In Annales de l’IHP Probabilités Et\nStatistiques, 23:397–423.\n\n\nDiaconis, Persi, and Mehrdad Shahshahani. 1981. “Generating a\nRandom Permutation with Random Transpositions.” Probability\nTheory and Related Fields 57 (2): 159–79.\n\n\n———. 1984. “On Nonlinear Functions of Linear Combinations.”\nSIAM Journal on Scientific and Statistical Computing 5 (1):\n175–91.\n\n\nDiaconis, P., and D. Ylvisaker. 1983. “Quantifying Prior\nOpinion.”\n\n\nDietterich, Thomas G. 2000. “Ensemble Methods in\nMachine Learning.” In Multiple Classifier\nSystems, 1–15. Berlin, Heidelberg: Springer.\n\n\nDixon, Mark J., and Stuart G. Coles. 1997. “Modelling\nAssociation Football Scores and Inefficiencies\nin the Football Betting Market.” Journal of the\nRoyal Statistical Society Series C: Applied Statistics 46 (2):\n265–80.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019.\n“Deep Learning for Spatio-Temporal Modeling: Dynamic\nTraffic Flows and High Frequency Trading.” Applied Stochastic\nModels in Business and Industry 35 (3): 788–807.\n\n\nDreyfus, Stuart. 1962. “The Numerical Solution of Variational\nProblems.” Journal of Mathematical Analysis and\nApplications 5 (1): 30–45.\n\n\n———. 1973. “The Computational Solution of Optimal Control Problems\nwith Time Lag.” IEEE Transactions on Automatic Control\n18 (4): 383–85.\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive\nSubgradient Methods for Online Learning and Stochastic\nOptimization.” Journal of Machine Learning Research 12\n(61): 2121–59.\n\n\nEdwards, Ward, Harold Lindman, and Leonard J. Savage. 1963.\n“Bayesian Statistical Inference for Psychological\nResearch.” Psychological Review 70 (3): 193–242.\n\n\nEfron, Bradley, and Carl Morris. 1975. “Data Analysis Using\nStein’s Estimator and Its\nGeneralizations.” Journal of the American\nStatistical Association 70 (350): 311–19.\n\n\n———. 1977. “Stein’s Paradox in Statistics.” Scientific\nAmerican 236 (5): 119–27.\n\n\nEnikolopov, Ruben, Vasily Korovkin, Maria Petrova, Konstantin Sonin, and\nAlexei Zakharov. 2013. “Field Experiment Estimate of Electoral\nFraud in Russian Parliamentary Elections.”\nProceedings of the National Academy of Sciences 110 (2):\n448–52.\n\n\nFawcett, Tom. 2006. “An Introduction to ROC\nAnalysis.” Pattern Recognition Letters 27 (8): 861–74.\n\n\nFedus, William, Barret Zoph, and Noam Shazeer. 2022. “Switch\nTransformers: Scaling to Trillion\nParameter Models with Simple and Efficient\nSparsity.” Journal of Machine Learning Research\n23 (120): 1–39.\n\n\nFefferman, Charles L. 2006. “Existence and Smoothness of the\nNavier–Stokes Equation.” The\nMillennium Prize Problems, 57–67.\n\n\nFeller, William. 1971. An Introduction to Probability Theory and Its\nApplications. Wiley.\n\n\nFeng, Guanhao, Nicholas G. Polson, and Jianeng Xu. 2016. “The\nMarket for English Premier League\n(EPL) Odds.” Journal of\nQuantitative Analysis in Sports 12 (4). https://arxiv.org/abs/1604.03614.\n\n\nFinkelstein, Mark, and Robert Whitley. 1981. “The Application of\nan Entropy Maximizing Gambling Objective to a Production\nProblem.” Management Science 27 (9): 1023–33.\n\n\nFredholm, Ivar. 1903. “Sur Une Classe d’équations\nFonctionnelles.” Acta Mathematica 27 (none): 365–90.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A\nGradient Boosting Machine.” Annals of Statistics,\n1189–1232.\n\n\nFriedman, Jerome H., and Werner Stuetzle. 1981. “Projection\nPursuit Regression.” Journal of the American\nStatistical Association 76 (376): 817–23.\n\n\nFrühwirth-Schnatter, Sylvia, and Rudolf Frühwirth. 2007.\n“Auxiliary Mixture Sampling with Applications to Logistic\nModels.” Computational Statistics & Data Analysis 51\n(April): 3509–28.\n\n\n———. 2010. “Data Augmentation and MCMC\nfor Binary and Multinomial Logit\nModels.” In Statistical Modelling and\nRegression Structures: Festschrift in\nHonour of Ludwig Fahrmeir, 111–32.\n\n\nFrühwirth-Schnatter, Sylvia, Rudolf Frühwirth, Leonhard Held, and Håvard\nRue. 2008. “Improved Auxiliary Mixture Sampling for Hierarchical\nModels of Non-Gaussian Data.” Statistics and\nComputing 19 (4): 479.\n\n\nFrühwirth-Schnatter, Sylvia, and Helga Wagner. 2010. “Stochastic\nModel Specification Search for Gaussian and Partial\nNon-Gaussian State Space Models.” Journal of\nEconometrics 154: 85–100.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural\nNetwork Model for a Mechanism of Pattern Recognition Unaffected by Shift\nin Position.” Biological Cybernetics 36 (4): 193–202.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a\nBayesian Approximation: Representing Model\nUncertainty in Deep Learning.” In International Conference on\nMachine Learning, 1050–59.\n\n\nGalton, Francis. 1907. “Vox Populi.” Nature 75:\n450–51.\n\n\nGalushkin, A. I. 1973. “Synthesis of Multilayer Systems of Pattern\nRecognition.” Neurocomputers and Their Application.\n\n\nGan, Link, and Alan Fritzler. 2016. “How to Become an\nExecutive.”\n\n\nGao, Luyu, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022.\n“Precise Zero-Shot Dense Retrieval Without Relevance\nLabels.” arXiv Preprint arXiv:2212.10496. https://arxiv.org/abs/2212.10496.\n\n\nGarcía-Arenzana, Nicolás, Eva María Navarrete-Muñoz, Virginia Lope,\nPilar Moreo, Carmen Vidal, Soledad Laso-Pablos, Nieves Ascunce, et al.\n2014. “Calorie\nIntake, Olive Oil Consumption and Mammographic Density Among\nSpanish Women.” International Journal of\nCancer 134 (8): 1916–25.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data\nAnalysis. 3rd ed. Boca Raton: Chapman and\nHall/CRC.\n\n\nGleick, James. 1992. Genius: The Life and\nScience of Richard Feynman. New York:\nPantheon Books.\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly\nProper Scoring Rules, Prediction, and\nEstimation.” Journal of the American Statistical\nAssociation 102 (477): 359–78.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015.\n“Peeking Inside the Black Box: Visualizing\nStatistical Learning with Plots of Individual Conditional\nExpectation.” Journal of Computational and Graphical\nStatistics 24 (1): 44–65.\n\n\nGramacy, Robert B., and Nicholas G. Polson. 2012.\n“Simulation-Based Regularized Logistic\nRegression.” arXiv. https://arxiv.org/abs/1005.3430.\n\n\nGriewank, Andreas, Kshitij Kulshreshtha, and Andrea Walther. 2012.\n“On the Numerical Stability of Algorithmic\nDifferentiation.” Computing. Archives for Scientific\nComputing 94 (2-4): 125–49.\n\n\nGuan, Xinyu, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu,\nFan Yang, and Mao Yang. 2025. “rStar-Math: Small LLMs Can Master Math\nReasoning with Self-Evolved Deep Thinking.”\narXiv. https://arxiv.org/abs/2501.04519.\n\n\nGupte, Mihir, Eshan Dixit, Muhammad Tayyab, and Arun Adiththan. 2025.\n“What Works for’lost-in-the-Middle’in Llms? A Study\non GM-extract and Mitigations.”\narXiv Preprint arXiv:2511.13900. https://arxiv.org/abs/2511.13900.\n\n\nHahn, P. Richard, Jared S. Murray, and Carlos M. Carvalho. 2020.\n“Bayesian Regression Tree Models for Causal\nInference: Regularization, Confounding,\nand Heterogeneous Effects (with\nDiscussion).” Bayesian Analysis 15 (3):\n965–1056.\n\n\nHalevy, Alon, Peter Norvig, and Fernando Pereira. 2009. “The\nUnreasonable Effectiveness of Data.” IEEE Intelligent\nSystems 24 (2): 8–12.\n\n\nHardt, Moritz, Ben Recht, and Yoram Singer. 2016. “Train Faster,\nGeneralize Better: Stability of Stochastic Gradient\nDescent.” In International Conference on Machine\nLearning, 1225–34. PMLR.\n\n\nHastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J.\nTibshirani. 2022. “Surprises in High-Dimensional Ridgeless Least\nSquares Interpolation.” The Annals of Statistics 50 (2):\n949–86.\n\n\nHeath, David, and William Sudderth. 1976. “De Finetti’s Theorem on\nExchangeable Variables.” The American Statistician 30\n(4): 188–89.\n\n\nHeaton, J. B., and N. G. Polson. 2012. “Smart Money, Dumb Money:\nLearning Type from Price.” Working Paper.\n\n\nHeaton, J. B., N. G. Polson, and Jan Hendrik Witte. 2016. “Deep\nLearning for Finance: Deep Portfolios.” Applied\nStochastic Models in Business and Industry.\n\n\nHeld, Leonhard, and Chris C. Holmes. 2006. “Bayesian Auxiliary\nVariable Models for Binary and Multinomial Regression.”\nBayesian Analysis 1 (1): 145–68.\n\n\nHilgers, Ralf-Dieter, Kit Roes, and Nigel Stallard. 2016.\n“Efficient Ways Exist to Obtain the Optimal Sample Size in\nClinical Trials in Rare Diseases.” Journal of Clinical\nEpidemiology 80: 68–76.\n\n\nHinton, Geoffrey, Nitish Srivastava, and Kevin Swersky. 2012.\n“Neural Networks for Machine Learning, Lecture 6a:\nOverview of Mini-Batch Gradient Descent.” Coursera\nLecture.\n\n\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling\nthe Knowledge in a Neural Network.” arXiv Preprint\narXiv:1503.02531. https://arxiv.org/abs/1503.02531.\n\n\nHou, Zhen, Hao Liu, Jiang Bian, Xing He, and Yan Zhuang. 2025.\n“Enhancing Medical Coding Efficiency Through Domain-Specific\nFine-Tuned Large Language Models.” Npj Health Systems 2\n(1): 14.\n\n\nHsieh, Cheng-Yu, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa\nFujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. “Distilling Step-by-Step! Outperforming Larger\nLanguage Models with Less Training Data and Smaller Model Sizes.”\narXiv Preprint arXiv:2305.02301. https://arxiv.org/abs/2305.02301.\n\n\nHuang, Gao, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and\nKilian Q. Weinberger. 2017. “Snapshot Ensembles:\nTrain 1, Get M for Free.”\nIn International Conference on Learning\nRepresentations.\n\n\nHuber, Peter J. 1985. “Projection Pursuit.”\nThe Annals of Statistics 13 (2): 435–75.\n\n\nHyndman, Rob J., and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. edition.\nMelbourne, Australia: Otexts.\n\n\nIgelnik, B., and N. Parikh. 2003. “Kolmogorov’s Spline\nNetwork.” IEEE Transactions on Neural Networks 14 (4):\n725–33.\n\n\nImmer, Alexander, Matthias Bauer, Vincent Fortuin, Gunnar Rätsch, and\nKhan Mohammad Emtiyaz. 2021. “Scalable Marginal Likelihood\nEstimation for Model Selection in Deep Learning.” In\nInternational Conference on Machine Learning, 4563–73. PMLR.\n\n\nIrwin, Neil. 2016. “How to Become a\nC.E.O.? The Quickest Path\nIs a Winding One.” The New York\nTimes, September.\n\n\nJacobs, Robert A., Michael I. Jordan, Steven J. Nowlan, and Geoffrey E.\nHinton. 1991. “Adaptive Mixtures of Local\nExperts.” Neural Computation 3 (1): 79–87.\n\n\nJanuschowski, Tim, Yuyang Wang, Kari Torkkola, Timo Erkkilä, Hilaf\nHasson, and Jan Gasthaus. 2022. “Forecasting with Trees.”\nInternational Journal of Forecasting, Special\nIssue: M5 competition, 38 (4): 1473–81.\n\n\nJeffreys, Harold. 1998. Theory of Probability.\nThird Edition, Third Edition. Oxford Classic Texts in the\nPhysical Sciences. Oxford, New York: Oxford University\nPress.\n\n\nJiang, Huiqiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\n2023. “LLMLingua: Compressing Prompts\nfor Accelerated Inference of Large Language Models.” arXiv\nPreprint arXiv:2310.05736. https://arxiv.org/abs/2310.05736.\n\n\nJiang, Wenxin, and Martin A. Tanner. 1999a. “Hierarchical Mixtures-of-Experts for Generalized Linear\nModels: Some Results on Denseness and\nConsistency.” In Proceedings of the\nSixteenth International Conference on Machine\nLearning, 214–22. San Francisco, CA, USA: Morgan Kaufmann\nPublishers Inc.\n\n\n———. 1999b. “On the Identifiability of Mixtures-of-Experts.” Neural\nNetworks 12 (9): 1253–58.\n\n\nJiménez-Luna, José, Francesca Grisoni, Nils Weskamp, and Gisbert\nSchneider. 2020. “DrugEx V2: De Novo\nDesign of Drug Molecule by Pareto-based\nMulti-Objective Reinforcement Learning in Polypharmacology.”\nJournal of Cheminformatics 12 (1): 1–12.\n\n\nJohannes, Michael S., Nick Polson, and Seung M. Yae. 2009.\n“Quantile Filtering and\nLearning.” SSRN Electronic Journal.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.\n“Highly Accurate Protein Structure Prediction with\nAlphaFold.” Nature 596 (7873): 583–89.\n\n\nKaczmarz, Stefan. 1937. “Angenäherte Auflösung von Systemen\nLinearer Gleichungen.” Bulletin International de l’Académie\nPolonaise Des Sciences Et Des Lettres 35: 355–57.\n\n\nkaggle. 2020. “M5 Forecasting -\nAccuracy.”\n\n\nKallenberg, Olav. 1997. Foundations of Modern\nProbability. 2nd ed. edition. Springer.\n\n\nKalman, R. E., and R. S. Bucy. 1961. “New Results in\nLinear Filtering and Prediction\nTheory.” Journal of Basic Engineering 83 (1):\n95–108.\n\n\nKalman, Rudolph Emil. 1960. “A New Approach to Linear Filtering\nand Prediction Problems.” Transactions of the ASME–Journal of\nBasic Engineering 82 (Series D): 35–45.\n\n\nKelly, J. L. 1956. “A New Interpretation of\nInformation Rate.” Bell System Technical\nJournal 35 (4): 917–26.\n\n\nKeskar, Nitish Shirish, Dheevatsa Mudigere, Jorge Nocedal, Mikhail\nSmelyanskiy, and Ping Tak Peter Tang. 2016. “On Large-Batch\nTraining for Deep Learning: Generalization Gap and Sharp\nMinima.” arXiv Preprint arXiv:1609.04836. https://arxiv.org/abs/1609.04836.\n\n\nKeynes, John Maynard. 1930. “Economic Possibilities for Our\nGrandchildren.” In Essays in Persuasion, 358–73. W. W.\nNorton & Company.\n\n\nKhattab, Omar, and Matei Zaharia. 2020. “ColBERT:\nEfficient and Effective Passage Search via Contextualized\nLate Interaction over BERT.” In Proceedings of\nthe 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 39–48. ACM.\n\n\nKim, Young-Hoon, Jaehyung Shim, Hyoung-Seob Park, et al. 2024. “Diagnostic Accuracy\nof Single-Lead Handheld ECG Devices for Atrial Fibrillation\nDetection.” Journal of Cardiovascular\nElectrophysiology 35: 614–21.\n\n\nKingma, Diederik, and Jimmy Ba. 2014. “Adam: A Method\nfor Stochastic Optimization.” arXiv Preprint\narXiv:1412.6980. https://arxiv.org/abs/1412.6980.\n\n\nKlartag, Bo’az. 2007. “A Central Limit Theorem for Convex\nSets.” Inventiones Mathematicae 168 (1): 91–131.\n\n\nKoenker, Roger. 2005. Quantile Regression.\nEconometric Society Monographs. Cambridge: Cambridge\nUniversity Press.\n\n\nKolmogoroff, Andrei. 1931. “Über Die Analytischen\nMethoden in Der\nWahrscheinlichkeitsrechnung.” Mathematische\nAnnalen 104 (1): 415–58.\n\n\n———. 1933. Grundbegriffe Der\nWahrscheinlichkeitsrechnung. Vol. 2. Ergebnisse Der\nMathematik Und Ihrer Grenzgebiete. Berlin:\nSpringer.\n\n\nKolmogorov, AN. 1956. “On the Representation of Continuous\nFunctions of Several Variables as Superpositions of Functions of Smaller\nNumber of Variables.” In Soviet. Math.\nDokl, 108:179–82.\n\n\nKolmogorov, Andrey N. 1933. Grundbegriffe Der\nWahrscheinlichkeitsrechnung. Berlin: Springer.\n\n\nKöppen, Mario. 2000. “The Curse of Dimensionality.” 5th\nOnline World Conference on Soft Computing in Industrial Applications\n(WSC5) 1: 4–8.\n\n\nLeCun, Yann, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller.\n2002. “Efficient Backprop.” In Neural Networks:\nTricks of the Trade, 9–50. Springer.\n\n\nLeviathan, Yaniv, Matan Kalman, and Yossi Matias. 2023. “Fast\nInference from Transformers via Predictive Sampling.” arXiv\nPreprint arXiv:2211.17191. https://arxiv.org/abs/2211.17191.\n\n\nLevina, Elizaveta, and Peter Bickel. 2001. “The Earth Mover’s\nDistance Is the Mallows Distance: Some Insights from\nStatistics.” In Proceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV\n2001, 2:251–56. IEEE.\n\n\nLim, Bryan, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021.\n“Temporal Fusion Transformers for Interpretable\nMulti-Horizon Time Series Forecasting.” International Journal\nof Forecasting 37 (4): 1748–64.\n\n\nLin, Zhouhan, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing\nXiang, Bowen Zhou, and Yoshua Bengio. 2017. “A Structured Self-attentive Sentence\nEmbedding.” arXiv. https://arxiv.org/abs/1703.03130.\n\n\nLindgren, Georg. 1978. “Markov Regime Models for\nMixed Distributions and Switching\nRegressions.” Scandinavian Journal of Statistics\n5 (2): 81–91.\n\n\nLindley, D. V. 1961. “The Use of Prior\nProbability Distributions in Statistical Inference\nand Decisions.” In Proceedings of the\nFourth Berkeley Symposium on Mathematical\nStatistics and Probability, Volume 1:\nContributions to the Theory of\nStatistics, 4.1:453–69. University of California\nPress.\n\n\nLinnainmaa, Seppo. 1970. “The Representation of the Cumulative\nRounding Error of an Algorithm as a Taylor Expansion of the\nLocal Rounding Errors.” Master’s Thesis (in Finnish), Univ.\nHelsinki, 6–7.\n\n\nLiu, Hao, Matei Zaharia, and Pieter Abbeel. 2023. “Ring Attention\nwith Blockwise Transformers for Near-Infinite Context.” arXiv\nPreprint arXiv:2310.01889. https://arxiv.org/abs/2310.01889.\n\n\nLiu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele\nBevilacqua, Fabio Petroni, and Percy Liang. 2024. “Lost in the\nMiddle: How Language Models Use Long\nContexts.” Transactions of the Association for\nComputational Linguistics 12: 157–73.\n\n\nLogunov, A. A. 2004. “Henri Poincare and Relativity\nTheory.” https://arxiv.org/abs/physics/0408077.\n\n\nLorentz, George G. 1976. “The 13th Problem of\nHilbert.” In Proceedings of\nSymposia in Pure Mathematics, 28:419–30.\nAmerican Mathematical Society.\n\n\nLundberg, Scott M, and Su-In Lee. 2017. “A Unified\nApproach to Interpreting Model Predictions.”\nIn Advances in Neural Information Processing Systems\n30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R.\nFergus, S. Vishwanathan, and R. Garnett, 4765–74. Curran Associates,\nInc.\n\n\nMackay, Charles. 1841. Extraordinary Popular Delusions and the\nMadness of Crowds. London: Richard Bentley.\n\n\nMacKay, David JC. 1992. “Bayesian Interpolation.”\nNeural Computation 4 (3): 415–47.\n\n\nMaharaj, Shiva, Nick Polson, and Vadim Sokolov. 2023. “Kramnik Vs\nNakamura or Bayes Vs\nP-Value.” {{SSRN Scholarly Paper}}. Rochester, NY.\n\n\nMetropolis, Nicholas. 1987. “The Beginning of the\nMonte Carlo Method.” Los Alamos Science 15:\n125–30.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth,\nAugusta H. Teller, and Edward Teller. 1953. “Equation of\nState Calculations by Fast Computing\nMachines.” The Journal of Chemical Physics 21\n(6): 1087–92.\n\n\nMetropolis, Nicholas, and Stanislaw Ulam. 1949. “The Monte\nCarlo Method.” Journal of the American Statistical\nAssociation 44 (247): 335–41.\n\n\nMicrosoft Research. 2024. “GraphRAG: Unlocking\nLLM Discovery on Narrative Private Data.”\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n“Efficient Estimation of Word\nRepresentations in Vector Space.” arXiv. https://arxiv.org/abs/1301.3781.\n\n\nMilman, Vitali D, and Gideon Schechtman. 2009. Asymptotic Theory of\nFinite Dimensional Normed Spaces: Isoperimetric\nInequalities in Riemannian Manifolds. Vol. 1200. Springer.\n\n\nMinsky, Marvin, and Seymour Papert. 1969. Perceptrons: An\nIntroduction to Computational Geometry.\nCambridge, MA: MIT Press.\n\n\nMiotto, Marilù, Nicola Rossberg, and Bennett Kleinberg. 2022. “Who\nIs GPT-3? An Exploration of Personality,\nValues and Demographics.” arXiv Preprint\narXiv:2209.14338. https://arxiv.org/abs/2209.14338.\n\n\nMnih, Volodymyr, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu.\n2014. “Recurrent Models of Visual Attention.” Advances\nin Neural Information Processing Systems 27: 2204–12.\n\n\nMorris, Stephen. 1994. “Trade with Heterogeneous Prior Beliefs and\nNo-Trade Theorems.” Econometrica : Journal of the Econometric\nSociety 62 (6): 1327–47.\n\n\n———. 1996. “Speculative Trade with Rational Beliefs.”\nJournal of Economic Theory 70 (2): 445–72.\n\n\nNadaraya, E. A. 1964. “On Estimating\nRegression.” Theory of Probability & Its\nApplications 9 (1): 141–42.\n\n\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\nand Ilya Sutskever. 2021. “Deep Double Descent: Where\nBigger Models and More Data Hurt*.” Journal of Statistical\nMechanics: Theory and Experiment 2021 (12): 124003.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2022.\n“Deep Partial Least Squares for Iv Regression.” arXiv\nPreprint arXiv:2207.02612. https://arxiv.org/abs/2207.02612.\n\n\n———. 2023a. “Generative Causal Inference,”\nJune. https://arxiv.org/abs/2306.16096.\n\n\n———. 2023b. “Feature Selection for Personalized\nPolicy Analysis,” July. https://arxiv.org/abs/2301.00251.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized\nLinear Models.” Royal Statistical Society.\nJournal. Series A: General 135 (3): 370–84.\n\n\nNesterov, Yurii. 1983. “A Method of Solving a Convex Programming\nProblem with Convergence Rate O\n(1/K2).” In Soviet Mathematics\nDoklady, 27:372–76.\n\n\nNicosia, Luca, Giulia Gnocchi, Ilaria Gorini, Massimo Venturini,\nFederico Fontana, Filippo Pesapane, Ida Abiuso, et al. 2023.\n“History of Mammography: Analysis of Breast Imaging\nDiagnostic Achievements over the Last Century.”\nHealthcare 11 (1596).\n\n\nNovick, Melvin R., and James E. Grizzle. 1965. “A Bayesian\nApproach to the Analysis of Data from\nClinical Trials.” Journal of the American\nStatistical Association 60 (309): 81–96.\n\n\nOstrovskii, GM, Yu M Volin, and WW Borisov. 1971. “Uber Die\nBerechnung von Ableitungen.” Wissenschaftliche Zeitschrift\nDer Technischen Hochschule f Ur Chemie, Leuna-Merseburg 13 (4):\n382–84.\n\n\nOuyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,\nPamela Mishkin, Chong Zhang, et al. 2022. “Training Language\nModels to Follow Instructions with Human Feedback.” Advances\nin Neural Information Processing Systems 35: 27730–44.\n\n\nPacker, Charles, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah\nWooders, and Joseph E. Gonzalez. 2023. “MemGPT:\nTowards LLMs as Operating Systems.” arXiv\nPreprint arXiv:2310.08560. https://arxiv.org/abs/2310.08560.\n\n\nPan, Zhenyu, Haozheng Luo, Manling Li, and Han Liu. 2025.\n“Chain-of-Action: Faithful and\nMultimodal Question Answering Through Large Language\nModels.” arXiv. https://arxiv.org/abs/2403.17359.\n\n\nPark, Joon Sung, Lindsay Popowski, Carrie J. Cai, Meredith Ringel\nMorris, Percy Liang, and Michael S. Bernstein. 2024. “Generative\nAgent Simulations of 1,000 People.” arXiv Preprint\narXiv:2411.10109. https://arxiv.org/abs/2411.10109.\n\n\nParzen, Emanuel. 2004. “Quantile Probability and\nStatistical Data Modeling.” Statistical\nScience 19 (4): 652–62.\n\n\nPetris, Giovanni. 2010. “An R Package for\nDynamic Linear Models.” Journal of Statistical\nSoftware 36 (October): 1–16.\n\n\nPoincaré, Henri. 1898. “La Mesure Du Temps.” Revue de\nMétaphysique Et de Morale 6 (1): 1–13.\n\n\n———. 1952. Science and Hypothesis. New York]: Dover\nPublications.\n\n\nPolson, Nicholas. 1996. “Convergence of Markov Chain\nMonte Carlo Algorithms (with Discussion).”\nBayesian Statistics 5: 297–321.\n\n\nPolson, Nicholas G., and James G. Scott. 2012. “Good, Great, or\nLucky? Screening for Firms with Sustained Superior\nPerformance Using Heavy-Tailed Priors.” The Annals of Applied\nStatistics 6 (1): 161–85.\n\n\n———. 2016. “Mixtures, Envelopes and\nHierarchical Duality.” Journal of the Royal\nStatistical Society Series B: Statistical Methodology 78 (4):\n701–27.\n\n\nPolson, Nicholas G., James G. Scott, and Jesse Windle. 2013.\n“Bayesian Inference for Logistic Models Using\nPólya–Gamma Latent Variables.” Journal of\nthe American Statistical Association 108 (504): 1339–49.\n\n\nPolson, Nicholas G., and Steven L. Scott. 2011. “Data Augmentation\nfor Support Vector Machines.” Bayesian Analysis 6 (1):\n1–23.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ:\nHow People and Machines Are Smarter Together. St.\nMartin’s Press.\n\n\nPolson, Nicholas G., and Vadim Sokolov. 2023. “Generative\nAI for Bayesian Computation.” https://arxiv.org/abs/2305.14972.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep\nLearning: A Bayesian Perspective.”\nBayesian Analysis 12 (4): 1275–1304.\n\n\nPolson, Nicholas, and Vadim Sokolov. 2020. “Deep Learning:\nComputational Aspects.” Wiley Interdisciplinary\nReviews: Computational Statistics 12 (5): e1500.\n\n\nPolson, Nicholas, Vadim Sokolov, and Jianeng Xu. 2021. “Deep\nLearning Partial Least Squares.” arXiv Preprint\narXiv:2106.14085. https://arxiv.org/abs/2106.14085.\n\n\nPolson, Nick, Fabrizio Ruggeri, and Vadim Sokolov. 2024.\n“Generative Bayesian Computation for Maximum\nExpected Utility.” Entropy. An International and\nInterdisciplinary Journal of Entropy and Information Studies 26\n(12): 1076.\n\n\nPolson, Nick, and Vadim Sokolov. 2025. “Negative\nProbability.” Applied Stochastic Models in\nBusiness and Industry 41 (1): e2910.\n\n\nPolson, Nick, Vadim Sokolov, and Jianeng Xu. 2023. “Quantum\nBayesian Computation.” Applied Stochastic Models\nin Business and Industry 39 (6): 869–83.\n\n\nQian, Chen, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li,\nCheng Yang, et al. 2024. “ChatDev:\nCommunicative Agents for Software\nDevelopment.” arXiv. https://arxiv.org/abs/2307.07924.\n\n\nRadford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.\n2018. “Improving Language Understanding by\nGenerative Pre-Training.” OpenAI.\n\n\nRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.\n“Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer.” Journal of Machine Learning\nResearch 21 (140): 1–67.\n\n\nRamsey, Frank P. 1926. “Truth and\nProbability.” Histoy of {{Economic Thought\nChapters}}. McMaster University Archive for the History of Economic\nThought.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016.\n“\"Why Should I Trust You?\":\nExplaining the Predictions of Any Classifier.” In\nProceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1135–44. ACM.\n\n\nRiquelme, Carlos, Joan Puigcerver, Basil Mustafa, Maxim Neumann,\nRodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby.\n2021. “Scaling Vision with Sparse\nMixture of Experts.” In Advances in\nNeural Information Processing Systems, 34:8583–95.\n\n\nRitter, Hippolyt, Aleksandar Botev, and David Barber. 2018. “A\nScalable Laplace Approximation For Neural Networks.”\n\n\nRobbins, Herbert, and Sutton Monro. 1951. “A Stochastic\nApproximation Method.” The Annals of Mathematical\nStatistics 22 (3): 400–407.\n\n\nRoberts, Gareth O., and Jeffrey S. Rosenthal. 2001. “Optimal\nScaling for Various Metropolis-Hastings Algorithms.”\nStatistical Science 16 (4): 351–67.\n\n\nRosenblatt, F. 1958. “The Perceptron: A Probabilistic\nModel for Information Storage and Organization in the Brain.”\nPsychological Review 65 (6): 386–408.\n\n\nRubin, Hal S. Stern, John B. Carlin. 2015. Bayesian Data\nAnalysis. 3rd ed. New York: Chapman and\nHall/CRC.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533.\n\n\nSalinas, David, Valentin Flunkert, and Jan Gasthaus. 2019.\n“DeepAR: Probabilistic Forecasting with\nAutoregressive Recurrent Networks.”\narXiv:1704.04110 [Cs, Stat], February. https://arxiv.org/abs/1704.04110.\n\n\nSanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.\n“DistilBERT, a Distilled Version of\nBERT: Smaller, Faster, Cheaper and Lighter.”\narXiv Preprint arXiv:1910.01108. https://arxiv.org/abs/1910.01108.\n\n\nSarthi, Parth, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie,\nand Christopher D. Manning. 2024. “RAPTOR:\nRecursive Abstractive Processing for Tree-Organized\nRetrieval.” arXiv Preprint arXiv:2401.18059. https://arxiv.org/abs/2401.18059.\n\n\nSchmidhuber, Jürgen. 2015. “Deep Learning in Neural Networks:\nAn Overview.” Neural Networks 61: 85–117.\n\n\nSchmidt-Hieber, Johannes. 2021. “The\nKolmogorov–Arnold Representation Theorem\nRevisited.” Neural Networks 137 (May): 119–26.\n\n\nSchwertman, Neil C, AJ Gilks, and J Cameron. 1990. “A Simple\nNoncalculus Proof That the Median Minimizes the Sum of the Absolute\nDeviations.” The American Statistician 44 (1): 38–39.\n\n\nScott, Steven L. 2002. “Bayesian Methods for\nHidden Markov Models.” Journal of the American\nStatistical Association 97 (457): 337–51.\n\n\n———. 2015. “Multi-Armed Bandit Experiments in the Online Service\nEconomy.” Applied Stochastic Models in Business and\nIndustry 31 (1): 37–45.\n\n\nScott, Steven L. 2013. “Multi-Armed Bandit Experiments.”\n\n\n———. 2022. “BoomSpikeSlab: MCMC for\nSpike and Slab Regression.”\n\n\nScott, Steven L., and Hal R. Varian. 2015. “Bayesian\nVariable Selection for Nowcasting Economic Time\nSeries.” In Economic Analysis of the\nDigital Economy, 119–35. University of Chicago Press.\n\n\nScott, Steven, and Hal Varian. 2014. “Predicting the\nPresent with Bayesian Structural Time\nSeries.” Int. J. Of Mathematical Modelling and\nNumerical Optimisation 5 (January): 4–23.\n\n\nSellke, Thomas, M. J Bayarri, and James O Berger. 2001.\n“Calibration of ρ Values\nfor Testing Precise Null Hypotheses.” The\nAmerican Statistician 55 (1): 62–71.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna\nVedantam, Devi Parikh, and Dhruv Batra. 2017.\n“Grad-CAM: Visual Explanations from Deep\nNetworks via Gradient-Based Localization.” In Proceedings of\nthe IEEE International Conference on Computer Vision,\n618–26. IEEE.\n\n\nShapiro, Sam. 1988. “Selection, Follow-up, and Analysis in the\nHealth Insurance Plan Study: A Randomized\nTrial with Breast Cancer Screening.” Journal of the National\nCancer Institute 80 (14): 1125–32.\n\n\nShazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc\nLe, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously\nLarge Neural Networks: The\nSparsely-Gated Mixture-of-Experts Layer.” In\nInternational Conference on Learning\nRepresentations.\n\n\nShimony, Abner. 1955. “Coherence and the Axioms of\nConfirmation.” The Journal of Symbolic\nLogic 20 (1): 1–28.\n\n\nShinn, Noah, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik\nNarasimhan, and Shunyu Yao. 2023. “Reflexion:\nLanguage Agents with Verbal Reinforcement Learning.”\nhttps://arxiv.org/abs/2303.11366.\n\n\nShiryayev, A. N. 1992. “On Analytical Methods in Probability\nTheory.” In Selected Works of a. N.\nKolmogorov: Volume II Probability Theory and\nMathematical Statistics, edited by A. N. Shiryayev, 62–108.\nDordrecht: Springer Netherlands.\n\n\nSimonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. 2013. “Deep\nInside Convolutional Networks: Visualising Image\nClassification Models and Saliency Maps.” arXiv Preprint\narXiv:1312.6034. https://arxiv.org/abs/1312.6034.\n\n\nSimpson, Edward. 2010. “Edward Simpson:\nBayes at Bletchley Park.”\nSignificance 7 (2): 76–80.\n\n\nSingh, Pratyush Kumar, Kathryn A. Farrell-Maupin, and Danial Faghihi.\n2024. “A Framework for Strategic\nDiscovery of Credible Neural Network Surrogate\nModels Under Uncertainty.” arXiv. https://arxiv.org/abs/2403.08901.\n\n\nSmith, A. F. M. 1975. “A Bayesian Approach to\nInference about a Change-Point in a\nSequence of Random Variables.”\nBiometrika 62 (2): 407–16.\n\n\nSokolov, Vadim. 2017. “Discussion of ‘Deep\nLearning for Finance: Deep Portfolios’.”\nApplied Stochastic Models in Business and Industry 33 (1):\n16–18.\n\n\nSpiegelhalter, David, and Yin-Lam Ng. 2009. “One Match to\nGo!” Significance 6 (4): 151–53.\n\n\nSprecher, David A. 1965. “On the Structure of Continuous Functions\nof Several Variables.” Transactions of the American\nMathematical Society 115: 340–55.\n\n\nSrivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent\nNeural Networks from Overfitting.” Journal of Machine\nLearning Research 15 (1): 1929–58.\n\n\nStern, Hal S. 1994. “A Brownian Motion Model for the\nProgress of Sports Scores.” Journal\nof the American Statistical Association 89 (427): 1128–34.\n\n\nStern, H, Adam Sugano, J Albert, and R Koning. 2007. “Inference\nabout Batter-Pitcher Matchups in Baseball from Small Samples.”\nStatistical Thinking in Sports, 153–65.\n\n\nStigler, Stephen M. 1981. “Gauss and the Invention of Least\nSquares.” The Annals of Statistics, 465–74.\n\n\nStroud, Jonathan R., Peter Müller, and Nicholas G. Polson. 2003.\n“Nonlinear State-Space Models with\nState-Dependent Variances.” Journal of the\nAmerican Statistical Association 98 (462): 377–86.\n\n\nSundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. “Axiomatic\nAttribution for Deep Networks.” In Proceedings of the 34th\nInternational Conference on Machine Learning, 3319–28. PMLR.\n\n\nSutskever, Ilya, James Martens, George Dahl, and Geoffrey Hinton. 2013.\n“On the Importance of Initialization and Momentum in Deep\nLearning.” In International Conference on Machine\nLearning, 1139–47.\n\n\nTabar, Laszlo, CJ Gad Fagerberg, Anders Gad, Lennart Baldetorp, Lars H\nHolmberg, Ove Gröntoft, Ulf Ljungquist, et al. 1985. “Reduction in\nMortality from Breast Cancer After Mass Screening with Mammography:\nRandomised Trial from the Breast Cancer Screening\nWorking Group of the Swedish National Board of\nHealth and Welfare.” The\nLancet 325 (8433): 829–32.\n\n\nTaleb, Nassim Nicholas. 2007. The Black Swan: The\nImpact of the Highly Improbable. Annotated\nedition. New York. N.Y: Random House.\n\n\nThompson, William R. 1933. “On the Likelihood That One Unknown\nProbability Exceeds Another in View of the Evidence of Two\nSamples.” Biometrika 25 (3/4): 285–94.\n\n\nTiao, Louis. 2019. “Pólya-Gamma Bayesian Logistic\nRegression.” Blog post.\n\n\nTikhonov, Andrei N. 1963. “Solution of Incorrectly Formulated\nProblems and the Regularization Method.” Sov Dok 4:\n1035–38.\n\n\nTikhonov, Andrey Nikolayevich et al. 1943. “On the Stability of\nInverse Problems.” In Dokl. Akad.\nNauk Sssr, 39:195–98.\n\n\nTsai, Yao-Hung Hubert, Shaojie Bai, Makoto Yamada, Louis-Philippe\nMorency, and Ruslan Salakhutdinov. 2019. “Transformer\nDissection: A Unified Understanding of\nTransformer’s Attention via the\nLens of Kernel.” arXiv. https://arxiv.org/abs/1908.11775.\n\n\nTuring, A. M. 1950. “Computing Machinery and Intelligence.”\nMind; a Quarterly Review of Psychology and Philosophy 59 (236):\n433–60.\n\n\nU.S. Food and Drug Administration. 2010. “Guidance for the Use of\nBayesian Statistics in Medical Device Clinical\nTrials.”\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.\n“Attention Is All You Need.” arXiv. https://arxiv.org/abs/1706.03762.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\n“Attention Is All You Need.” Advances in Neural\nInformation Processing Systems 30: 5998–6008.\n\n\nVeblen, Thorstein. 1899. The Theory of the Leisure Class:\nAn Economic Study of Institutions. New York:\nMacmillan.\n\n\n———. 1921. The Engineers and the Price System. New York: B. W.\nHuebsch.\n\n\nVecer, Jan, Frantisek Kopriva, and Tomoyuki Ichiba. 2009.\n“Estimating the Effect of the Red Card\nin Soccer: When to Commit an\nOffense in Exchange for\nPreventing a Goal Opportunity.”\nJournal of Quantitative Analysis in Sports 5 (1).\n\n\nVillar, Sofía S., Jack Bowden, and James Wason. 2015. “Multi-Armed\nBandit Models for the Optimal Design of Clinical Trials:\nBenefits and Challenges.” Statistical\nScience 30 (2): 199–215.\n\n\nVille, Jean. 1939. “Étude Critique de La Notion de\nCollectif.” Thèses de l’entre-Deux-Guerres. PhD\nthesis, Université de Paris.\n\n\nViterbi, A. 1967. “Error Bounds for Convolutional Codes and an\nAsymptotically Optimum Decoding Algorithm.” IEEE Transactions\non Information Theory 13 (2): 260–69.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nHarvard Journal of Law & Technology 31: 841–87.\n\n\nWald, Abraham. 1945. “Sequential Tests of Statistical\nHypotheses.” The Annals of Mathematical Statistics 16\n(2): 117–86.\n\n\n———. 1947. Sequential Analysis. New York: John Wiley &\nSons.\n\n\nWang, Shaun. 1996. “Premium Calculation by Transforming the Layer\nPremium Density.” ASTIN Bulletin 26 (1): 71–92.\n\n\nWatanabe, Sumio. 2013. “A Widely Applicable Bayesian\nInformation Criterion.” The Journal of Machine Learning\nResearch 14 (1): 867–97.\n\n\nWatson, Geoffrey S. 1964. “Smooth Regression\nAnalysis.” Sankhyā: The Indian Journal of Statistics,\nSeries A (1961-2002) 26 (4): 359–72.\n\n\nWei, Bo, Thomas M. Braun, Roy N. Tamura, and Kelley Kidwell. 2018.\n“A Small n Sequential Multiple Assignment Randomized Trial Design\nfor Use in Rare Disease Research.” Statistics in\nMedicine 37 (26): 3836–52.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.\n“Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models.” arXiv. https://arxiv.org/abs/2201.11903.\n\n\nWerbos, Paul. 1974. “Beyond Regression:\" New Tools for Prediction\nand Analysis in the Behavioral Sciences.” Ph. D.\nDissertation, Harvard University.\n\n\nWerbos, Paul J. 1982. “Applications of Advances in Nonlinear\nSensitivity Analysis.” In System Modeling and\nOptimization, 762–70. Springer.\n\n\nWest, Mike, and Jeff Harrison. 1997. Bayesian Forecasting and\nDynamic Models. Springer.\n\n\nWiener, Norbert. 1950. The Human Use of Human Beings:\nCybernetics and Society. Boston: Houghton Mifflin.\n\n\nWindle, Jesse, Nicholas G. Polson, and James G. Scott. 2014.\n“Sampling Polya-Gamma Random Variates:\nAlternate and Approximate Techniques.” arXiv. https://arxiv.org/abs/1405.0506.\n\n\nYaari, Menahem E. 1987. “The Dual Theory of\nChoice Under Risk.” Econometrica :\nJournal of the Econometric Society 55 (1): 95–115.\n\n\nYang, An, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang,\nJiandong Jiang, et al. 2025. “Qwen2. 5-1m Technical\nReport.” arXiv Preprint arXiv:2501.15383. https://arxiv.org/abs/2501.15383.\n\n\nYao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,\nYuan Cao, and Karthik Narasimhan. 2023. “Tree of\nThoughts: Deliberate Problem Solving with\nLarge Language Models.” arXiv. https://arxiv.org/abs/2305.10601.\n\n\nYe, Tong, Shijing Si, Jianzong Wang, Ning Cheng, Zhitao Li, and Jing\nXiao. 2023. “On the Calibration and Uncertainty with\nPólya-Gamma Augmentation for Dialog Retrieval\nModels.” In Proceedings of the AAAI Conference\non Artificial Intelligence. https://arxiv.org/abs/2303.08606.\n\n\nYe, Yixin, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei\nLiu. 2025. “LIMO: Less Is\nMore for Reasoning.” arXiv. https://arxiv.org/abs/2502.03387.\n\n\nYu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soo-Jin Jeong, Woosung\nLee, and Byung-Gon Chun. 2022. “Orca: A Distributed Serving System\nfor Transformer-based Generative\nModels.” In 16th USENIX Symposium\non Operating Systems Design and Implementation (OSDI\n22), 527–46.\n\n\nZhang, Wei, Katsuyuki Itoh, Jun Tanida, and Yoshiki Ichioka. 1988.\n“Shift-Invariant Pattern Recognition Neural Network and Its\nOptical Architecture.” Proceedings of Annual Conference of\nthe Japan Society of Applied Physics.\n\n\nZhang, Yichi, Anirban Datta, and Sudipto Banerjee. 2018. “Scalable\nGaussian Process Classification with\nPólya-Gamma Data Augmentation.” arXiv Preprint\narXiv:1802.06383. https://arxiv.org/abs/1802.06383.",
    "crumbs": [
      "References"
    ]
  }
]