[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes, AI and Deep Learning",
    "section": "",
    "text": "Preface\nWelcome to the fascinating world of Bayesian learning, artificial intelligence, and deep learning! This book is your guide to understanding these powerful tools and their applications in various fields. This book is a culmination of our experiences teaching these exciting fields to two distinct audiences: business school students at the University of Chicago and engineers at George Mason University.\nThis unique blend of perspectives allows us to present these complex concepts in a way that is accessible to data scientists, business professionals and technical experts. Whether you’re a manager seeking to leverage AI in your organization or an engineer building the next generation of intelligent systems, this book has something for you.\nTechniques discussed in this book emerged as a transformative force in modern society, and its impact on automation is undeniable. From self-driving cars to virtual assistants, these technologies are already a part of our daily lives. In the coming years, they will become even more ubiquitous, impacting every industry and aspect of our lives. Understanding these technologies is essential for anyone who wants to stay ahead of the curve.\nThrough its ability to learn, adapt, and make decisions, AI is accelerating the pace of automation across various industries and sectors. This impact is multifaceted, encompassing both positive and negative aspects that warrant careful consideration. AI algorithms can analyze vast amounts of data to identify patterns and trends, providing valuable insights for informed decision-making. This leads to better resource allocation, optimized processes, and improved outcomes across various domains. Chatbots and virtual assistants powered by AI can handle customer inquiries and provide support 24/7, offering a personalized and efficient experience. It even works on Jewish holidays! This improves customer satisfaction and loyalty, ultimately benefiting businesses.\nAs a result, AI enables the creation of entirely new business models and industries that were previously not possible. This disrupts traditional markets and creates opportunities for innovation and growth. AI is driving significant progress in fields like self-driving cars, personalized medicine, and space exploration. This has the potential to revolutionize these industries and improve lives in numerous ways.\nThe term AI has morphed over time. It was first coined in 1956 by John McCarthy, who defined it as “the science and engineering of making intelligent machines.” Since then, the field has evolved significantly, and the definition of AI has changed accordingly. Today, AI is a broad field that encompasses various subfields, including machine learning, deep learning, and natural language processing. These subfields are often used interchangeably, but they are not the same thing. Machine learning is a subfield of AI that focuses on algorithms that can learn from data. Deep learning is a subfield of machine learning that uses artificial neural networks to learn complex patterns and relationships in data. Natural language processing is a subfield of AI that focuses on algorithms that can understand and generate human language.\nSince 1956, the field of artificial intelligence (AI) has undergone significant transformations traditional AI was mostly focused on rule-based systems and boolean logic programming, with limited learning capabilities. It lead to them being brittle in changing environments. On the other hand, emerging AI is focused on modeling uncertainties, pattern matching, and deep learning. All of those are data-driven approaches. These approaches are more adaptable and can handle complex and unstructured data. They are also more data-dependent and lack interpretability.\n\n\n\n\n\n\n\n\n\nOld AI\n\n\n\nIf rain outside, then take umbrella\nThis rule cannot be learned from data. It does not allow inference. Cannot say anything about rain outside if I see an umbrella.\n\n\n\n\n\n \n\n\n\n\n\n\n\n\nNew AI\n\n\n\nProbability of taking umbrella, given there is rain\nConditional probability rule can be learned from data. Allows for inference. We can calculate the probability of rain outside if we see an umbrella.\n\n\n\n\n\nThis book is based on the lecture notes from our courses, which have been refined and expanded over years of teaching. We have incorporated valuable feedback from students, both at the University of Chicago and George Mason University, to create a comprehensive and engaging learning experience. This book is organized into three parts:\n\nPart 1: Bayesian Learning: This part covers the basics of probability and Bayesian inference.\nPart 2: Artificial Intelligence: This part explores the core concepts of AI such and mostly focuses on pattern matching techniques such as decision trees and generalized linear models.\nPart 3: Deep Learning: This part delves into the world of deep learning, focusing on the architecture and training of deep neural networks. It covers convolutional neural networks, recurrent neural networks, and generative adversarial networks.\n\nThis work is inspired by the contributions of many great thinkers in the field of AI and machine learning. We acknowledge the foundational work of pioneers such as: Shannon (Reinforcement Learning), von Neumann (Theory of Choice), Bellman (Optimality and Learning).\nThe evolution of AI can be summarised in three stages:\n\nSearch. Started with one question and webpages ranked by relevance. Larry Page and Sergey Brin developed PageRank algorithm that uses power iterations to rank webpages by relevance. The statistical tools such as Kendall’s tau and Spearman’s rank correlation were used to measure the similarity between the ranking and the actual relevance.\nSuggestions. The first popular suggestion algorithm was developed by Netflix. It used collaborative filtering to recommend movies to users based on their viewing history and the viewing history of other users. The idea was to minimize the burden on the user to search for movies to watch.\nSummaries. Current AI systems like ChatGPT and Perplexity have revolutionized how we interact with information by excelling at summarization and generalization. These large language models can take vast amounts of complex information and distill it into clear, coherent summaries that capture the essential points while maintaining accuracy. They can generalize across different domains, connecting concepts from various fields and providing insights that might not be immediately obvious. For example, ChatGPT can summarize lengthy research papers into key findings, while Perplexity can synthesize information from multiple sources to provide comprehensive answers to complex questions. This ability to summarize and generalize represents a significant leap forward from simple search and recommendation systems, as these AI agents can now act as intelligent intermediaries that understand context, identify patterns, and present information in ways that are most useful to users.\n\nFirst it was one question and one answer, next it was suggestions. You do not know what to search for, collaborative filtering, recommender systems, and search engines. Now, it is summarise, you are the manager of AI agents agents interpret your request, formulate a plan and necessary questions, and then execute the plan. This is the future of AI, where agents can work together to solve complex problems and provide valuable insights.\nBayesian learning is a powerful statistical framework based on the work of Thomas Bayes. It provides a probabilistic approach to reasoning and learning, allowing us to update our beliefs about the world as we gather new data. This makes it a natural fit for artificial intelligence, where we often need to deal with uncertainty and incomplete information. Artificial intelligence (AI) is a vast field that seeks to create intelligent agents capable of performing tasks that typically require human intelligence. These tasks can include perception, reasoning, learning, problem-solving, decision-making, and language processing. AI has made significant progress in recent years, driven by advances in computing power, data availability, and algorithms. Deep learning is a subfield of AI that uses artificial neural networks to learn from data. These networks are inspired by the structure and function of the human brain and have the ability to learn complex patterns and relationships in data. Deep learning has achieved remarkable results in various tasks such as image recognition, natural language processing, and machine translation.\nThe world of business and engineering are increasingly intertwined, as AI becomes an essential tool in both domains. This book bridges the gap between these disciplines by demonstrating how Bayesian learning, AI, and deep learning can be applied to address real-world challenges in:\n\nBusiness: Market analysis, customer segmentation, risk management, and strategic decision-making.\nEngineering: Robotics, image recognition, natural language processing, and data-driven automation.\n\nKey Features of This Book:\n\nAccessible explanations: We break down complex concepts into manageable chunks, using real-world examples and analogies to illustrate key principles.\nCase studies: We showcase practical applications of Bayesian learning, AI, and deep learning across diverse industries.\nHands-on exercises: We provide practical exercises and code examples to help you apply the concepts covered in the book to your own projects.\n\nJoining the AI Revolution:\nThe field of AI is rapidly evolving, and this book equips you with the knowledge and skills necessary to stay ahead of the curve. Whether you’re looking to enhance your business acumen or advance your engineering career, understanding the power of Bayesian learning, AI, and deep learning is crucial.\nWe invite you to join us on this exciting journey and discover the transformative potential of these powerful tools!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Generative AI\n“If you tell me precisely what it is a machine cannot do, then I can always make a machine which will do just that. John von Neumann, 1956”\nWhen you open an Amazon page there are many personal suggestions of goods to purchase. By analyzing previous product pages visited and purchases made by you and other people who have bought similar products Amazon uses AI and machine learning to predict what would of interest to you next time you shop.\nWhen you apply for a loan online, you typically get an immediate answer after filling an application. The information you provide, combined with your credit history pulled from a credit history bureau is used by a predictive model which can tell with high level of confidence whether you are to default on the loan or not.\nYou might ask, what is common among one of the most successful Internet retail company, finance industry and a phenomenal baseball team? All of these decisions use AI and methods of predictive analytics to improve the operations. They used historical observations combined with rigorous statistical analysis and efficient computer algorithms to predict future outcomes and change the decisions. The ability to collect and analyze complex data sets has been a prerogative of a small number of people for many year. It vital to have experience in data engineering, statistics, machine learning and probability. A data scientists has all of those skills. Current tools developed by industry and academic institutions makes data science profession accessible to a wider audience without requiring a training in a specific technical filed.\nOver the past decade, there has been an explosion of work, mostly applied, on deep learning. Applications of deep learning are everywhere. The main reason for this is that large Internet companies such as Google, Facebook, Amazon and Netflix increasingly displace traditional statistical and machine learning methods with deep learning techniques. Though, such companies are at the frontier of applying deep learning, virtually any industry can be impacted by applying deep learning (DL).\nData Science is a relatively new field that refers to sets of mathematical and statistical models, algorithms, and software that allow extracting patterns from data sets. The algorithms are the adoptions of applied mathematics techniques to specific computer architectures and the software implements those algorithms.\nPredictive analytics applies AI models to design predictive rules which then can be used by engineers and business for forecasting or what-if analysis. For example, a company that is interested in predicting sales as a result of advertisement campaign would use predictive model to identify the best way to allocate its marketing budget or a logistics company would use a predictive model to forecast demand for shipments to estimate the number of drivers it would need in the next few months.\nArtificial Intelligence has been around for decades. In fact the term AI was coined by a famous computer scientist John McCarthy in 1955. While being tightly connected to the field of robotics for many years, the AI concepts are widely applicable in other fields, including predictive analytics. Currently, the AI is understood as a set of mathematical tools that are used to develop algorithms that can perform tasks, typically done by humans, for example, drive a car or schedule a doctor’s appointment. This set of mathematical tools include probabilistic models, machine learning algorithms and deep learning. The previous successful applications included the victory of IBM’s DeepBlue over then world champion Garry Kasparov in 1997.\nTree search algorithms were developed by DeepBlue engineers to implement the chess robot. A modification was the addition of heuristics to cut branches of the tree that would not lead to a win. Those heuristics were designed by chess grand masters based on their intuition and previous experience. Vehicles in grand challenge also relied on traditional techniques such as Kalman filters and PID (proportional-integral-derivative) controllers that have been in use for many years.\nTwo distinguishing features of AI algorithms:\nA major difference between modern and historical AI algorithms is that most of the recent AI approaches rely on learning patterns from data. For example, DeepBlue algorithm was “hardcoded” and the human inputs were implemented as if-then statements by the IBM engineers. On the other hand, modern AlphaGo zero algorithm did not use any human inputs whatsoever and learned optimal strategies from a large data sets generated from self-plays. Although handcrafted systems were shown to perform well in some tasks, such as chess playing, the are hard to design for many complex applications, such as self-driving cars. On the other hand large data sets allow us to replace set of rules designed by engineers with a set of rules learned automatically from data. Thus, the learning algorithms, such as deep learning are at the core of the most of modern AI systems.\nThe main driving factor behind the growth of modern AI applications is the availability of massive and often unstructured data sets. Om the other hand, we now have appropriate computing power to develop computationally intensive AI algorithms. The three main modern AI enablers are:\nFitting complicated models to describe complicated patterns without overfitting requires millions or billions of data points. Two key ideas behind pattern-recognition systems are\nIn AI, prediction rules are often referred to as “models”. The process of using data to find a gooo prediction rule is often called “training the model”. With millions (or billions) of datapoints and fast pattern-matching skills, machines can find needles in a haystack proving insights for human health, transportation, ... etc.\nMachine learning (ML) arises from this question: could a computer go beyond “what we know how to order it to perform” and learn on its own how to perform a specified task? Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data? This question opens the door to a new programming paradigm. In classical programming, the paradigm of symbolic AI, humans input rules (a program) and data to be processed according to these rules, and out come answers. With machine learning, humans input data as well as the answers expected from the data, and out come the rules. These rules can then be applied to new data to produce original answers.\nA machine-learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task. For instance, if you wished to automate the task of tagging your vacation pictures, you could present a machine-learning system with many examples of pictures already tagged by humans, and the system would learn statistical rules for associating specific pictures to specific tags.\nAlthough machine learning only started to flourish in the 1990s, it has quickly become the most popular and most successful subfield of AI, a trend driven by the availability of faster hardware and larger datasets. Machine learning is tightly related to mathematical statistics, but it differs from statistics in several important ways. Unlike statistics, machine learning tends to deal with large, complex datasets (such as a dataset of millions of images, each consisting of tens of thousands of pixels) for which classical statistical analysis such as Bayesian analysis would be impractical. As a result, machine learning, and especially deep learning, exhibits comparatively little mathematical theory—maybe too little—and is engineering oriented. It’s a hands-on discipline in which ideas are proven empirically more often than theoretically.\nDeep learning DL is a type of machine learning which performs a sequence of transformations (filters) on a data. Output of each of those filters is called a factor in traditional statistical language and hidden feature in machine learning. Word deep means that there is a large number of filters that process the data. The power of this approach comes from the hierarchical nature of the model.\nThe three main factors driving AI are:\nThe widespread of mobile phones leads to generation of vast amounts of data. Besides images, users generate space and time trajectories, which are currently used to estimate and predict traffic, text messages, website clicking patterns, etc.\nDeep learning with many successful applications, has been frequently discussed in popular media. The popularity of the topic has led to hype people tend to think that deep learning techniques are capable to replace many of the human tasks, such as medical diagnostics, accountings. On the pessimistic side, people think that after a short hype, the DL techniques will disappoint and companies will stop funding R&D work on its development. However, the research on pushing this filed further is slow and it will take time before deep learning penetrates a wide range of industries. At any rate, the demand for data scientists in general and AI specialists has been increasing over the last few years with biggest markets being on silicon valley, NYC and Washington, DC(indeed 2018).\nThe field of predictive analytics was popularized by many famous competitions in which people compete to build the model with lowest prediction error. One of the first of this types of competitions was the Netflix prize. In 2009 Netflix payed $1 million to a team that developed the most accurate model for predicting movies a user would like to watch. At that time Netflix’s recommendation system generated 30 billion predictions per day. The initial goal of improving recommendation algorithm by 10 percent was overachieved by the winning team. The wining team used what is called an ensemble technique, which takes a weighted average from different prediction algorithms. Thus, the first lesson from this competition is that we typically need to build several predictive models to achieve a good results. On the other had, the model developed by the winning team was never used by Netflix due to complexity of those models and the fact that by the end of competition Netflix mostly shifted to streaming movies versus sending DVDs over mail. The second lesson is that simplicity and interpretability of models matters when they are deployed on a large scale. The third lesson, is that models need to adapt accordingly to meet the fast changing business requirements.\nDeep Learning’s (DL) growing popularity is summarized by the grown of products that Google is developing using DL. Figure 2 shows this immense growth. One key differentiating effect is that DL algorithms are scalable and can be implemented across the interned in apps such as YouTube and Gmail.\nApplications of Machine Learning/Deep Learning are endless, you just have to look at the right opportunity! There is a similar dynamics in popularity of deep learning search queries on Google. The growth is again exponential, although it is not yet close to popularity of traditional statistical techniques, such as linear regression analysis.\nMeanwhile, some ethical concurs are being raised as a result of growing popularity of AI. The most discussed thus far is the impact on the job market and many jobs being replaced by deep learning models. Although, some economic analysis (Acemoglu and Restrepo 2018) shows that while jobs displacement leads to reduced demand for labor and wages, it counteracted by a productivity effect and increases in demand for labor in non-automated tasks.\nThe algorithmic aspects of deep learning has existed for decades. In 1956, Kolmogorov has shown that any function can be represented as a superposition of univariate functions (this is exactly what deep learning does). In 1951 Robbins and Monro proposed stochastic approximations algorithms. This is the main technique for finding weights of a deep learning model today.\nBackpropagation algorithm for finding derivatives was first published and implemented by Werbos in 1974. In mid 1980s Schmidhuber studied many practical aspects of applying neural networks to real-life problems. Since the key ingredients of DL has been around for several decades, one could wonder why we observe a recent peak in popularity of those methods.\nOne of the strong driving forces is adoption of DL by internet companies that need to analyze large scale high dimensional datasets, such as human-written text, speech and images. Smartphone photography led to people uploading vast amounts of images to services like Instagram and Facebook. In 2012 more mobile devices were sold than PCs. The number of images shared on the Internet has skyrocketed as well. This can be see in products that Google is developing using DL.\nThe proliferation of smartphones globally has been one of the most dramatic technological adoptions in human history. From just 173 million smartphone users worldwide in 2010, the number exploded to over 6.8 billion users by 2023, representing nearly 86% of the global population. This exponential growth has been particularly pronounced in developing markets, where smartphones often serve as the primary gateway to the internet. Countries like India and China have seen smartphone penetration rates exceed 80%, while regions in Africa and Southeast Asia continue to show rapid adoption curves. The ubiquity of smartphones has fundamentally transformed how data is generated and consumed - these devices produce continuous streams of location data, user interactions, images, messages, and behavioral patterns that form the foundation for modern AI applications. The convergence of increasingly powerful mobile processors, high-resolution cameras, and always-on internet connectivity has created an unprecedented data generation ecosystem that feeds directly into the machine learning models powering everything from recommendation systems to autonomous vehicles.\nTherefore, data generated by Internet users creates a demand for techniques to analyze large scale data sets. Mathematical methodologies were in place for many years. One missing ingredient in the explosive nature of DL popularity is the availability of computing power. DL models are computationally hungry, trial and error process is required to build a useful model. Sometimes hundreds or thousands of different models are required to be evaluated before choosing one to be used in an application. Training models can be computationally expensive, we are usually talking about large amounts of training data that need to be analyzed to build a model.\nThe adoption rate of AI technologies, particularly generative AI like ChatGPT, has shattered all previous records for technology adoption. While it took the internet 7 years to reach 100 million users, the telephone 75 years, and television 13 years, ChatGPT achieved this milestone in just 2 months after its launch in November 2022. This unprecedented speed of adoption reflects not just the accessibility of AI tools, but also their immediate utility across diverse user needs. Unlike previous innovations that required significant infrastructure changes or learning curves, AI chatbots could be accessed through simple web interfaces and provided immediate value for tasks ranging from writing assistance to problem-solving. The viral nature of AI adoption has been further accelerated by social media demonstrations and word-of-mouth sharing of impressive AI capabilities, creating a network effect that compounds the growth rate. This rapid adoption suggests that AI represents a fundamentally different type of technological shift - one that augments human capabilities rather than replacing existing systems entirely. The chart below illustrates the explosive growth potential of AI technologies.\nThe first generation of AI models was fundamentally enabled by the availability of powerful GPU chips, which provided the parallel processing capabilities necessary to train deep neural networks on large datasets. The breakthrough in deep learning around 2012, including innovations like AlexNet for image recognition, would not have been possible without GPUs that could perform thousands of matrix operations simultaneously. Current AI models, including ChatGPT, Claude, and other large language models, continue to rely primarily on GPUs for both training and inference. Modern AI training clusters consist of thousands of interconnected GPUs working together for weeks or months to process the enormous datasets required for today’s sophisticated models. While some companies have developed specialized AI chips like Google’s TPUs, GPUs remain the dominant platform for AI development due to their versatility, widespread availability, and established software ecosystems.\nThe gaming industry was one of the earliest drivers of GPU development, as game developers demanded increasingly sophisticated graphics rendering capabilities to create immersive virtual worlds with realistic lighting, textures, and physics simulations. Companies like NVIDIA and AMD invested heavily in parallel processing architectures optimized for the matrix operations required to render complex 3D scenes in real-time. The rise of cryptocurrency mining, particularly Bitcoin and Ethereum, created an unexpected second wave of GPU demand as miners discovered that graphics cards were far more efficient than traditional CPUs for the repetitive hash calculations required by proof-of-work algorithms. This mining boom drove massive investments in GPU manufacturing capacity and spurred innovations in memory bandwidth and energy efficiency. More recently, the explosion of AI-generated video content has created a third major demand driver, as video generation models require enormous computational power to process and synthesize high-resolution video frames. The convergence of these three use cases - gaming graphics, cryptocurrency mining, and AI video generation - has accelerated GPU development far beyond what any single application could have achieved alone, creating the powerful hardware infrastructure that now enables training of large language models and other AI applications.\nTable 1 illustrates the dramatic evolution of GPU performance over two decades, from early graphics cards to specialized AI accelerators. The data shows exponential growth in computational power: from the modest 0.23 TeraFLOPS of the 2006 GeForce 7900 GTX to the projected 100 PetaFLOPS (FP4) of the 2027 Rubin Ultra - representing a performance increase of over 400,000x. Here FP4 is a lower precision (4-bit) floating-point arithmetic that is used for AI workloads. It is an alternative to FP32 (32-bit) floating-point arithmetic that is used for general purpose computing.\nMemory capacity has similarly exploded from 0.5GB to a projected 1TB. Modern GPUs have evolved from simple graphics processors to sophisticated AI-optimized architectures featuring specialized tensor cores, mixed-precision arithmetic (FP8/FP4), and massive high-bandwidth memory systems. The transition from traditional FP32 floating-point operations to lower-precision AI workloads (FP8/FP4) has enabled unprecedented computational throughput measured in PetaFLOPS and ExaFLOPS scales, making current and future GPUs the primary engines driving the deep learning revolution and large language model training.\nNow AI models are the main consumers of those processors. The more popular of those are ChatGPT-4, Anthropic’s Claude and Perplexity. ChatGPT-4 is based on the transformer architecture. It is able to handle long conversations and maintain better context over multiple turns. It is stronger in creative writing, technical writing, reasoning tasks, and code generation. It has better performance on logic-heavy tasks and answering technical queries. It is mainly used for chatbots, automated content creation, code writing, customer support, and more advanced AI tasks.\nOpenAI, the company behind ChatGPT, has experienced remarkable growth in both valuation and revenue. As of late 2024, OpenAI reached a valuation of $157 billion following its latest funding round, making it one of the most valuable private companies in the world. The company’s annual recurring revenue (ARR) has grown exponentially, reaching approximately $3.7 billion in 2024, driven primarily by ChatGPT subscriptions and API usage. OpenAI has raised over $13 billion in total funding, with major investors including Microsoft, which has invested $13 billion and maintains a strategic partnership that includes exclusive cloud computing arrangements. This rapid financial growth reflects the massive demand for generative AI capabilities across industries and the transformative potential of large language models.\nClaudeis the main competitor of OpenAI. It is supported by Amazon and excels at complex reasoning tasks, problem-solving, and in-depth analysis across a wide range of domains. Claude can write, debug, and explain code in many programming languages. It can analyze images and documents in addition to text and can engage in various conversation styles, from formal analysis to creative writing to casual discussion.\nAmazon has made a significant strategic investment in Anthropic, Claude’s creator, committing up to $4 billion to advance AI safety research and development. This partnership positions Amazon Web Services (AWS) as Anthropic’s primary cloud provider while giving Amazon a minority ownership stake in the company. Unlike ChatGPT, which excels in creative writing and general-purpose conversations, Claude is specifically designed with a focus on safety, harmlessness, and nuanced reasoning. Claude demonstrates superior performance in tasks requiring careful analysis, ethical reasoning, and handling sensitive topics. It employs Constitutional AI training methods that make it more reliable in avoiding harmful outputs and better at acknowledging uncertainty when it doesn’t know something. Recent advances in Claude 3.7 and Claude 4.0 have introduced groundbreaking multimodal capabilities, allowing these models to process and analyze images, documents, and code with unprecedented accuracy. Claude 4.0 represents a significant leap forward in mathematical reasoning, coding assistance, and complex problem-solving tasks, with performance improvements of 40-60% over previous versions in benchmark evaluations. These newer models feature enhanced “thinking” processes that are more transparent, often explaining their reasoning step-by-step with greater depth and clarity, which makes them particularly valuable for educational applications, research assistance, and professional analysis where understanding the AI’s decision-making process is crucial. Claude 4.0 also introduces improved long-context understanding, capable of processing documents up to 200,000 tokens, and demonstrates remarkable advances in scientific reasoning and technical writing. This approach has made Claude increasingly popular among researchers, academics, and professionals who require more thoughtful and contextually aware AI assistance.\nPerplexity synthesizes information from multiple sources and presents it with proper citations. Each response includes references for easy verification. It functions as a conversational search engine. Perplexity has emerged as a formidable competitor to Google Search by offering a fundamentally different approach to information discovery. Unlike traditional search engines that provide links to websites, Perplexity acts as an AI-powered research assistant that directly answers questions while citing sources. The company has attracted significant investment, including backing from Amazon founder Jeff Bezos, who participated in Perplexity’s $74 million Series B funding round in 2024. This strategic investment reflects growing confidence in AI-first search alternatives that could disrupt Google’s longstanding dominance in the search market.\nThe company has also developed innovative partnerships with major brands like Marriott and Nike, demonstrating how AI search can be integrated into enterprise applications. Marriott has explored using Perplexity’s technology to enhance customer service by providing instant, cited answers about hotel amenities, local attractions, and booking policies. Similarly, Nike has experimented with Perplexity’s capabilities to help customers find specific product information, sizing guides, and availability across different locations. These enterprise partnerships showcase Perplexity’s potential to move beyond general web search into specialized, domain-specific applications.\nPerplexity’s advertising model differs significantly from Google’s traditional approach. Rather than displaying ads alongside search results, Perplexity is exploring sponsored answers and branded content integration that maintains the conversational flow while clearly identifying commercial partnerships. This approach could prove less intrusive than traditional search advertising while providing new revenue streams. The company’s growth trajectory and enterprise adoption suggest it could pose a meaningful challenge to Google’s search monopoly, particularly among users who prefer direct answers over browsing multiple websites.\nThe explosive growth of Large Language Models (LLMs) like ChatGPT, Claude, and Perplexity has been fundamentally enabled by the vast repositories of digital text that have accumulated over the past three decades. The “fuel” powering these sophisticated AI systems comes from an unprecedented collection of human knowledge digitized and made accessible through the internet. Wikipedia alone contains over 60 million articles across hundreds of languages, representing one of humanity’s largest collaborative knowledge projects. Web crawling technologies have systematically captured billions of web pages, blog posts, news articles, and forum discussions, creating massive text corpora that encode diverse writing styles, domains of expertise, and forms of human expression. The digitization of literature through projects like Google Books and Internet Archive has made millions of books searchable and processable, from classical literature to technical manuals. Social media platforms have contributed streams of conversational text, while academic databases provide formal scientific and scholarly writing. This digital text explosion created training datasets containing trillions of words - orders of magnitude larger than what any human could read in multiple lifetimes. By processing these enormous text collections through transformer architectures, LLMs learned statistical patterns of language use, absorbing grammar, syntax, semantics, and even reasoning patterns embedded in human writing. The models discovered how words relate to each other, how concepts connect across different contexts, and how to generate coherent, contextually appropriate responses by predicting the most likely next word given preceding text. This approach allowed AI systems to develop surprisingly sophisticated language understanding and generation capabilities without explicit programming of linguistic rules, instead learning the deep structure of human communication from the collective digital footprint of our species.\nThe mathematical operations used for manipulating and rendering images are the same as those used in deep learning models. Researchers started to used graphical processing units (GPUs) (a.k.a graphics cards) to train deep learning models in 2010s. The wide availability of GPUs made deep learning modeling accessible for a large number of researchers and engineers and eventually led to popularity of DL. Recently, several competitive hardware architectures were developed by large companies like Google, which uses its own TPU (Tensor Processing Units) as well as smaller start-ups.\nThis course will focus on practical and theoretical aspects of predicting using deep learning models. Currently, deep learning techniques are almost exclusively used for image analysis and natural language processing and are practiced by a handful number of scientists and engineers with most of them being trained in computer science. However, modern methodologies, software and availability of cloud computing make deep learning accessible to a wide range of data scientists who would typically use more traditional predictive models such as generalized linear regression or tree-based methods.\nA unified approach to analyze and apply deep learning models to a wide range or problems that arise in business and engineering is required. To make this happen, we will bring together ideas from probability and statistics, optimization, scalable linear algebra and high performance computing. Although, deep learning models are very interesting to study from methodological point of view, the most important aspect of those is the predictive power unseen before with more traditional models. Ability to learn very complex patterns in data and generate accurate predictions make the deep learning a useful and exciting methodology to use, we hope to convey that excitement. This set of notes is self-contained and has a set of references for a reader interested in learning further.\nAlthough basics of probability, statistics and linear algebra will be revisited, it is targeted towards students who have completed a course in introductory statistics and high school calculus. We will make extensive use of computational tools, such as R language, as well as PyTorch and TensorFlow libraries for predictive modeling, both for illustration and in homework problems.\nThere are many aspects of data analysis that do not deal with building predictive models, for example data processing and labeling can require significant human resources(Hermann and Balso 2017; Baylor et al. 2017).\nThe landscape of Artificial Intelligence is rapidly being reshaped by the rise of Generative AI (Gen AI). As of 2025, Gen AI has moved beyond hype and into practical application across a multitude of personal and professional domains. A recent article in the Harvard Business Review, “How People Are Really Using Gen AI in 2025” by Marc Zao-Sanders, highlights this shift, noting that user interest has significantly increased and investment in AI is skyrocketing.\nThe article reveals a fascinating trend: a move from purely technical applications towards more emotive and personal uses. The top use cases in 2025 reflect this, with “Therapy/companionship” leading the list. Other prominent uses include “Organizing my life,” “Finding purpose,” “Enhanced learning,” and “Generating code (for pros).” This indicates that individuals are leveraging Gen AI not just for productivity, but also for personal development and well-being.\nSome concrete examples of how people are using Gen AI, as cited in the article, include:\nThe article also points to the increasing sophistication of Gen AI users, who are developing a deeper understanding of the technology’s capabilities and limitations, including concerns around data privacy and the potential for over-reliance.\nBelow is an image from the HBR article summarizing the top 10 use cases:\nSource: Marc Zao-Sanders, “How People Are Really Using Gen AI in 2025,” Harvard Business Review, April 9, 2025, https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025.\nThe continued evolution of Gen AI promises even more sophisticated applications in the future, moving from providing information to taking action (agentic behavior).\nThe computer therapist is not something new. In 1966, Joseph Weizenbaum created ELIZA, a computer program that could simulate a conversation with a psychotherapist. ELIZA used simple pattern matching to respond to user inputs, creating the illusion of understanding. While it was a groundbreaking achievement at the time, it lacked true comprehension and relied on scripted responses.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#generative-ai",
    "href": "00-intro.html#generative-ai",
    "title": "Principles of Data Science",
    "section": "",
    "text": "Therapy/Companionship: Providing accessible mental health support and a sense of connection, especially in regions with limited access to human therapists. Users find AI to be available 24/7 and non-judgmental.\nOrganizing My Life: Creating timelines for tasks, planning daily habits, and managing personal projects.\nEnhanced Learning: Using AI as a study guide to explain complex topics and reinforce learning.\nHealthier Living: Generating meal plans based on specific dietary needs and macro calculations.\nCreating Travel Itineraries: Planning detailed vacations, including finding rustic accommodations and hidden gems while optimizing travel time.\nDisputing Fines: Drafting appeal letters for things like parking tickets.\n\n\n\n\n\n\nTop 10 Gen AI Use Cases in 2025. Source: Harvard Business Review, “How People Are Really Using Gen AI in 2025”, April 9, 2025.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "00-intro.html#agi-and-aiq",
    "href": "00-intro.html#agi-and-aiq",
    "title": "Principles of Data Science",
    "section": "AGI and AIQ",
    "text": "AGI and AIQ\n\n“I visualize a time when we will be to robots what dogs are to humans. And I am rooting for the machines.” - Claude Shannon\n\n\n“Let us suppose we have set up a machine with certain initial instruction tables, so constructed that these tables might on occasion, if good reason arose, modify those tables. One can imagine that after the machine had been operating for some time, the instructions would have altered out of all recognition, but nevertheless still be such that one would have to admit that the machine was still doing very worthwhile calculations. Possibly it might still be getting results of the type desired when the machine was first set up, but in a much more efficient manner. In such a case one would have to admit that the progress of the machine had not been foreseen when its original instructions were put in. It would be like a pupil who had learnt much from his master, but had added much more by his own work. When this happens I feel that one is obliged to regard the machine as showing intelligence.” – Alan Turing\n\nPeople, organizations and markets. AI does the organization and hence connects people to markets faster and simplesly. Hence it creates economic values. Most of th recessions in the 19th centurey was a result of not being able to get goods to markets quick enough which led to banking crises. AI accelerates speed to market. It reates growth. The age of abundance is here.\nSkynet and terminator\nTransfer learning\nOlga comments (Toloka)\n\nChat does not know what it does not know\nStill need humans and their skills\nLike co-pilot, we need collaboration between humans and AI, humans became managers\nBefore people would build many classifiers for a specific task. The economics of the model there is one big winner. They combine all the models together.\nNeed humans for ground truth, for labeling data, for training models\nAI is very good at decomposing and planning, and humans are not as good at executing the plan, because it is against their intuition.\n\nAndrej Karpathy’s talk, “Software Is Changing (Again),” explores how large language models (LLMs) are fundamentally transforming the way software is developed and used. He describes this new era as “Software 3.0,” where natural language becomes the primary programming interface and LLMs act as a new kind of computer and compares it to the previous generations of software development approaches sumamrised in the table below.\n\n\n\n\n\n\n\n\n\nParadigm\n“Program” is…\nDeveloper’s main job\nCanonical depot\n\n\n\n\nSoftware 1.0\nHand-written code\nWrite logic\nGitHub\n\n\nSoftware 2.0\nNeural-net weights\nCurate data & train\nHugging Face / Model Atlas\n\n\nSoftware 3.0\nNatural-language prompts\nCompose/police English instructions\nPrompt libraries\n\n\n\nCurrenlty LLMs are collaborative partners that can augment human abilities, democratizing software creation and allowing people without traditional programming backgrounds to build complex applications simply by describing what they want in plain English.\nPolson and Scott (2018) have predicted that human-machine interaction will be the next frontier of AI.\nOlga sais that humans are callable function.\nThe same will happen to university professors. They will become lablers for content. And simply will be responsible for clicking yes, when content is appropriate and no, when it is not.\nHal Varian’s 2010 paper “Computer Mediated Transactions” Varian (2010) provides a foundational framework for understanding how computers can automate routine tasks and decision-making processes, reducing transaction costs and increasing efficiency. This includes automated pricing, inventory management, and customer service systems. He talks about system that can coordinate between multiple parties by providing real-time information sharing and communication platforms. This enables more complex multi-party transactions and supply chain management.\nThis framework remains highly relevant for understanding modern AI and machine learning applications in business, as these technologies represent the next evolution of computer-mediated transactions, enabling even more sophisticated automation, coordination, and communication capabilities.\nIn his talk on “Why are LLMs not Better at Finding Proofs?”, Timothy Gowers discusses that while large language models (LLMs) can display some sensible reasoning—such as narrowing down the search space in a problem—they tend to falter when they get stuck, relying too heavily on intelligent guesswork rather than systematic problem-solving. Unlike humans, who typically respond to a failed attempt with a targeted adjustment based on what went wrong, LLMs often just make another guess that isn’t clearly informed by previous failures. He also highlights a key difference in approach: humans usually build up to a solution incrementally, constructing examples that satisfy parts of the problem and then refining their approach based on the requirements. For example, when trying to prove an existential statement, a human might first find examples satisfying one condition, then look for ways to satisfy additional conditions, adjusting parameters as needed. LLMs, by contrast, are more likely to skip these intermediate steps and try to jump directly to the final answer, missing the structured, iterative reasoning that characterizes human problem-solving.\nWhile there are indeed limitations to what current large language models can solve, particularly in areas requiring systematic mathematical reasoning, they continue to demonstrate remarkable capabilities in solving complex problems through alternative approaches. A notable example is the application of deep learning to the classical three-body problem in physics, a problem that has challenged mathematicians and physicists for centuries. Traditional analytical methods have struggled to find closed-form solutions for the three-body problem, but deep neural networks have shown surprising success in approximating solutions through pattern recognition and optimization techniques. These neural networks can learn the underlying dynamics from training data and generate accurate predictions for orbital trajectories, even when analytical solutions remain elusive. This success demonstrates that the trial-and-error approach, when combined with sophisticated pattern recognition capabilities, can lead to practical solutions for problems that have resisted traditional mathematical approaches. The key insight is that while these methods may not provide the elegant closed-form solutions that mathematicians prefer, they offer valuable computational tools that can advance scientific understanding and enable practical applications in fields ranging from astrophysics to spacecraft navigation.\n\n\n\n\nAcemoglu, Daron, and Pascual Restrepo. 2018. “Artificial Intelligence, Automation and Work.” National Bureau of Economic Research.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, et al. 2017. “Tfx: A Tensorflow-Based Production-Scale Machine Learning Platform.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016. “End to End Learning for Self-Driving Cars.” arXiv Preprint arXiv:1604.07316.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo: Uber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in Artificial Intelligence.”\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg Mori. 2017. “Learning Person Trajectory Representations for Team Activity Analysis.” arXiv Preprint arXiv:1706.00893.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How People and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018. “Prediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learning.” Nature Biomedical Engineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” arXiv.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.” Communications of the ACM 38 (3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated Transactions.” American Economic Review 100 (2): 1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of Structured Information from Street View Imagery.” arXiv Preprint arXiv:1704.03549.",
    "crumbs": [
      "Principles of Data Science"
    ]
  },
  {
    "objectID": "19-select.html",
    "href": "19-select.html",
    "title": "1  Model Selection",
    "section": "",
    "text": "1.1 Prediction vs Interpretation\nIn the world of statistical modeling and machine learning, we face a fundamental challenge that echoes Sherlock Holmes’ famous words about eliminating the impossible to find the truth. When confronted with data, we must navigate through countless possible models, each representing a different hypothesis about the underlying relationships in our data. The art and science of model selection lies in systematically choosing the model that best captures the true signal while avoiding the trap of fitting noise. This chapter explores the critical decisions on how to choose the right model.\nIn previous chapter ?sec-regularization we focussed on regularization techniques, which also can be viewd as a model selection mechanis. Specifically, we will discuss Ridge regression, LASSO, and their Bayesian interpretations. These methods offer elegant solutions to the overfitting problem by introducing penalties for model complexity, automatically balancing fit and parsimony. We’ll see how these frequentist approaches connect to Bayesian thinking through the lens of prior distributions and posterior inference.\nWe begin by examining the fundamental considerations that guide model selection: the bias-variance tradeoff, the challenges of overfitting and underfitting, and the practical constraints of computational resources and data quality. We’ll explore how the purpose of our analysis—whether prediction or interpretation—should influence our modeling choices, drawing on Leo Breiman’s influential distinction between the “two cultures” of statistical modeling.\nThe chapter then delves into practical methodologies for model evaluation and selection. We’ll cover exploratory data analysis techniques that help us understand our data before committing to a particular model form, followed by rigorous approaches to measuring out-of-sample performance through cross-validation and information criteria. These tools provide the foundation for making principled decisions about model complexity.\nBy the end of this chapter, you’ll have a comprehensive toolkit for approaching model selection problems, understanding when different techniques are appropriate, and implementing these methods in practice. Most importantly, you’ll develop the judgment to balance the competing demands of accuracy, interpretability, and computational efficiency that characterize real-world modeling challenges.\nLet us start with several important considerations when building predictive models.\n1. Model Selection: Choosing the right model for the relationship between \\(x\\) and \\(y\\) is crucial and involves navigating a fundamental trade-off between model complexity and generalization ability. If the chosen model is too simple (e.g., linear regression when the true relationship is polynomial), it might underfit the data and fail to capture important relationships, leading to high bias and poor performance on both training and test data. Conversely, a model that is too complex (e.g., high-degree polynomials or deep neural networks with insufficient data) might overfit the data by memorizing training examples rather than learning the underlying pattern, resulting in excellent training performance but poor generalization to unseen examples. This problem becomes even more complex when dealing with non-linear relationships, high-dimensional data, or noisy data, where the optimal model complexity is not immediately obvious and may require systematic experimentation with different model architectures, regularization techniques, and hyperparameter tuning to find the right balance between capturing the true signal while avoiding noise.\n2. Overfitting and Underfitting: Overfitting occurs when the model fits the training data too closely, capturing not only the true underlying relationship but also random noise and idiosyncrasies specific to the training dataset. This phenomenon typically manifests when a model has too many parameters relative to the amount of training data available, allowing it to essentially “memorize” the training examples rather than learning the generalizable patterns. The model may achieve excellent performance metrics on the training data (low training error) but will perform poorly on new, unseen data (high generalization error). This is because the model has learned to recognize specific noise patterns in the training data that don’t exist in the broader population. Common signs of overfitting include a large gap between training and validation/test performance, or performance that improves on training data while degrading on validation data during training iterations.\nUnderfitting occurs when the model is too simple and fails to capture the true relationship between \\(x\\) and \\(y\\), often due to insufficient model complexity or inadequate training. This can happen when using a model that is inherently too simple for the problem at hand (e.g., linear regression for a highly non-linear relationship), when the model hasn’t been trained for enough iterations, or when regularization is applied too aggressively. Underfitting results in poor performance on both training and test data, as the model lacks the capacity to learn the underlying patterns in the data. The model essentially misses important features or relationships that are necessary for accurate predictions. Unlike overfitting, underfitting typically shows similar poor performance across training, validation, and test sets, indicating that the model is not capturing the signal in the data regardless of the dataset.\n3. Data Quality and Quantity: The accuracy of predictions heavily relies on the quality and quantity of the available data. If the data is noisy, inaccurate, or incomplete, it can lead to misleading predictions. A sufficient amount of data is also crucial to ensure the model can learn the underlying relationship effectively. Insufficient data can result in underfitting and poor generalization.\nData quality issues can manifest in various forms, including missing values, inconsistent formatting, labeling errors, and biased sampling. These problems are particularly acute in machine learning applications where large volumes of labeled data are required for training. To address these challenges, companies have emerged that specialize in data quality improvement and annotation services.\nCompanies like Scale AI and Toloka provide platforms that help organizations improve data quality through human-in-the-loop annotation and validation processes. These platforms employ large networks of human annotators who can perform tasks such as image labeling, text classification, data validation, and quality control. Scale AI, for example, offers services for creating high-quality training datasets through human annotation, with built-in quality control mechanisms that include multiple annotators per task and consensus-based validation. Their platform can handle various data types including images, text, and video, making it suitable for computer vision, natural language processing, and other AI applications.\nToloka, similarly, provides a crowdsourcing platform that connects businesses with a global network of contributors who can perform data labeling, content moderation, and quality assessment tasks. Their platform includes quality control features such as skill-based routing, where tasks are assigned to annotators based on their demonstrated expertise, and dynamic overlap, where multiple workers verify the same data to ensure accuracy.\nThese platforms help address several key data quality challenges: they can identify and correct labeling errors through consensus mechanisms, handle missing data through targeted collection efforts, and ensure consistency in data formatting and annotation standards. By leveraging human expertise at scale, these services enable organizations to create more reliable training datasets, which in turn leads to better-performing machine learning models and more accurate predictions.\n4. Model Explainability: In many applications, it is crucial to understand how the model arrives at its predictions. This is particularly important in areas like healthcare or finance, where transparency and interpretability are essential. Some models, particularly complex ones like deep neural networks, can be difficult to interpret, making it challenging to understand the rationale behind their predictions. However, modern machine learning has developed several techniques to address this challenge and make complex models more interpretable.\nThe importance of explainability extends beyond mere curiosity about model behavior. In healthcare applications, doctors need to understand why a model recommended a particular diagnosis or treatment plan to make informed decisions and maintain trust in the system. A model that predicts a patient has a 90% chance of having cancer but cannot explain which symptoms or test results led to this conclusion would be of limited clinical value. Similarly, in financial services, regulators require explanations for credit decisions to ensure compliance with fair lending laws and to prevent discriminatory practices. When a loan application is denied, both the applicant and regulatory bodies need to understand the specific factors that influenced this decision.\nIn legal and compliance contexts, explainability becomes a legal requirement. The European Union’s General Data Protection Regulation (GDPR) includes a “right to explanation” that allows individuals to request information about automated decisions that affect them. This has created a legal imperative for organizations to develop explainable AI systems. In criminal justice applications, where AI systems might be used for risk assessment or sentencing recommendations, the stakes are particularly high. Judges, lawyers, and defendants all need to understand how these systems arrive at their conclusions to ensure fair and just outcomes.\nOne prominent approach is the use of interpretable surrogate models, such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations). These methods work by approximating the complex model’s behavior in the vicinity of a specific prediction using simpler, more interpretable models like linear regression or decision trees. LIME, for instance, creates local explanations by sampling points around the prediction of interest and fitting a linear model to explain the model’s behavior in that neighborhood. This allows us to understand which features contributed most to a particular prediction, even for complex models like deep neural networks.\nAnother powerful technique is attention mechanisms, which have become increasingly popular in natural language processing and computer vision. Attention mechanisms allow models to “focus” on specific parts of the input when making predictions, providing a form of built-in interpretability. For example, in image classification tasks, attention maps can highlight which regions of an image the model is focusing on when making its prediction, making it easier to understand the model’s decision-making process.\nGradient-based methods offer another approach to model interpretability. Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) use gradients to identify which parts of the input are most important for the model’s prediction. By computing the gradient of the model’s output with respect to the input features, these methods can create heatmaps that show which features or regions contributed most to the final prediction.\nFor tree-based models like random forests and gradient boosting machines, built-in feature importance measures provide natural interpretability. These methods can rank features based on their contribution to the model’s predictive performance, offering insights into which variables are most important for making predictions.\nModel distillation techniques represent another approach, where a complex model (the teacher) is used to train a simpler, more interpretable model (the student) that mimics the teacher’s behavior. The student model, being simpler, is easier to interpret while maintaining much of the teacher’s predictive performance.\nFinally, counterfactual explanations provide a different type of interpretability by showing what changes to the input would be needed to change the model’s prediction. For example, if a loan application is rejected, a counterfactual explanation might show that the application would have been approved if the applicant’s income were $10,000 higher or if their credit score were 50 points better.\nThese modern interpretability techniques have made it possible to understand and explain the behavior of even the most complex models, addressing the “black box” problem that has historically limited the adoption of advanced machine learning methods in critical applications where transparency is essential.\n5. Computational Cost: Training and using prediction models can be computationally expensive, especially for complex models with large datasets. This can limit their applicability in resource-constrained environments. Finding a balance between model complexity, accuracy, and computational cost is critical for practical applications.\nThe computational demands of machine learning models have been significantly addressed through the development of specialized hardware, particularly Graphics Processing Units (GPUs). Originally designed for rendering graphics in video games, GPUs have become essential for deep learning due to their parallel processing architecture. Unlike traditional Central Processing Units (CPUs) that process tasks sequentially, GPUs can perform thousands of mathematical operations simultaneously, making them ideal for the matrix multiplications and tensor operations that are fundamental to neural network training. This parallel processing capability has reduced training times from weeks to hours or even minutes for many deep learning models, democratizing access to advanced machine learning techniques.\nHowever, the computational cost challenge extends beyond just training to the deployment phase, where models need to run efficiently in production environments. This has led to the emergence of edge computing as a crucial solution. Edge computing involves processing data and running models closer to where the data is generated, rather than sending everything to centralized cloud servers. This approach offers several advantages for machine learning applications: reduced latency for real-time predictions, lower bandwidth costs by processing data locally, and improved privacy by keeping sensitive data on local devices.\nEdge computing is particularly important for applications requiring real-time decision making, such as autonomous vehicles, industrial IoT systems, and mobile applications. For example, a self-driving car cannot afford the latency of sending sensor data to a cloud server and waiting for predictions to return; it needs to process information and make decisions locally within milliseconds. Similarly, smart manufacturing systems use edge computing to monitor equipment and predict maintenance needs in real-time without the delays associated with cloud processing.\nQuantization and lower precision calculations have emerged as powerful techniques for reducing computational costs while maintaining model performance. Traditional neural networks use 32-bit floating-point numbers (FP32) for all calculations, which provides high precision but requires significant computational resources and memory. Quantization reduces the precision of these numbers, typically to 16-bit (FP16), 8-bit integers (INT8), or even 4-bit integers (INT4), dramatically reducing both memory usage and computational requirements. For example, converting from FP32 to INT8 can reduce memory usage by 75% and computational cost by 2-4x, while often maintaining acceptable accuracy levels. This is particularly valuable for deployment on edge devices with limited resources, such as smartphones, IoT devices, and embedded systems. Modern hardware, including specialized AI accelerators like Google’s Tensor Processing Units (TPUs) and NVIDIA’s Tensor Cores, are specifically designed to handle these lower precision calculations efficiently, further reducing the computational cost barrier.\nThe trade-offs between computational cost and model performance are becoming increasingly sophisticated. Techniques like model pruning, which removes unnecessary connections from neural networks, can create smaller, faster models. Knowledge distillation allows large, complex models to transfer their knowledge to smaller, more efficient models that can run on resource-constrained devices.\nThese developments have created a spectrum of deployment options, from powerful cloud-based systems that can run the most complex models to lightweight edge devices that can perform basic predictions locally. The choice depends on the specific requirements of the application, including latency requirements, accuracy needs, privacy concerns, and cost constraints. As hardware continues to improve and optimization techniques become more sophisticated, the computational cost barrier to deploying machine learning models continues to decrease, opening up new possibilities for AI applications in previously inaccessible domains.\n6. Ethical Considerations: Predictions can have significant real-world consequences, raising ethical concerns about bias, fairness, and potential misuse. It is crucial to consider the potential harms and unintended consequences of predictions and implement safeguards to mitigate them.\nThe ethical implications of predictive models have become increasingly prominent as these systems are deployed in critical domains such as healthcare, criminal justice, employment, and financial services. One of the most significant concerns is algorithmic bias, which can perpetuate or amplify existing societal inequalities. For example, facial recognition systems have been shown to have higher error rates for people of color, potentially leading to wrongful arrests or surveillance. Similarly, hiring algorithms trained on historical data may perpetuate gender or racial biases present in past hiring decisions, creating a feedback loop that reinforces discrimination.\nFairness in machine learning has emerged as a critical research area, with multiple definitions and approaches to ensure equitable treatment across different demographic groups. Statistical parity, equalized odds, and individual fairness are among the various fairness metrics that can be applied depending on the specific context and requirements of the application. However, achieving fairness often involves trade-offs with model accuracy, and different fairness definitions may conflict with each other, requiring careful consideration of which definition is most appropriate for a given use case.\nThe potential for misuse of predictive models is another significant concern. Models designed for legitimate purposes can be repurposed for harmful applications, such as using facial recognition for mass surveillance or employing predictive policing algorithms that disproportionately target certain communities. Additionally, the increasing sophistication of deepfake technology, which uses predictive models to generate realistic but fake images, videos, or audio, raises concerns about misinformation and manipulation.\nPrivacy concerns arise when predictive models require access to sensitive personal data. The collection, storage, and processing of personal information for training and deploying these models can violate individual privacy rights and create risks of data breaches. Differential privacy techniques, which add carefully calibrated noise to data or model outputs, have emerged as a promising approach to protect individual privacy while maintaining model utility.\nTransparency and accountability are essential for addressing ethical concerns. Organizations deploying predictive models must be able to explain their decisions and be held accountable for any harms that result. This includes maintaining audit trails, implementing human oversight mechanisms, and establishing clear procedures for addressing complaints or errors. The concept of “algorithmic impact assessments” has been proposed as a framework for evaluating the potential social impacts of automated decision-making systems before deployment.\nRegulatory frameworks are evolving to address these ethical challenges. The European Union’s General Data Protection Regulation (GDPR) includes provisions for automated decision-making and profiling, while various jurisdictions are developing specific regulations for AI systems. These regulations often require transparency, human oversight, and the ability to contest automated decisions.\nTechnical approaches to addressing ethical concerns include adversarial training to reduce bias, interpretability techniques to increase transparency, and robust testing procedures to identify potential harms before deployment. Regular monitoring and updating of deployed models is also crucial, as societal norms and legal requirements evolve over time.\nAddressing these challenges requires careful consideration of the specific problem, selection of appropriate techniques, and continuous evaluation and improvement of the prediction model. It also requires collaboration between technical experts, domain specialists, ethicists, and stakeholders to ensure that predictive models serve the public good while minimizing potential harms.\nAs we have discussed at the beginning of this chapter the predictive rule can be used for two purposes: prediction and interpretation. The goal of interpretation is to understand the relationship between the input and output variables. The two goals are not mutually exclusive, but they are often in conflict. For example, a model that is good at predicting the target variable might not be good at interpreting the relationship between the input and output variables. A nice feature of a linear model is that it can be used for both purposes, unlike more complex predictive rules with many parameters that can be difficult to interpret.\nTypically the problem of interpretation requires a simpler model. We prioritize models that are easy to interpret and explain, even if they have slightly lower predictive accuracy. Also, evaluation metrics are different, we typically use coefficient of determination (R-squared) or p-values, which provide insights into the model’s fit and the significance of the estimated relationships.\nThe choice between using a model for prediction or interpretation depends on the specific task and desired outcome. If the primary goal is accurate predictions, a complex model with high predictive accuracy might be preferred, even if it is less interpretable. However, if understanding the underlying relationships and causal mechanisms is crucial, a simpler and more interpretable model might be chosen, even if it has slightly lower predictive accuracy. Typically interpretive models are used in scientific research, social sciences, and other fields where understanding the underlying causes and relationships is crucial.\nIn practice, it’s often beneficial to consider both prediction and interpretation when building and evaluating models. However, it is not unusual to build two different models, one for prediction and one for interpretation. This allows for a more nuanced analysis of the data and can lead to better insights than using a single model for both purposes.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#prediction-vs-interpretation",
    "href": "19-select.html#prediction-vs-interpretation",
    "title": "1  Model Selection",
    "section": "",
    "text": "1.1.1 Breiman’s Two Cultures\nLet \\(x\\) be a high dimensional input containing a large set of potentially relevant data. Let \\(y\\) represent an output (or response) to a task which we aim to solve based on the information in \\(x\\). Brieman [2000] summaries the difference between statistical and machine learning philosophy as follows.\n\n“There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.”\n\n\n“The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.”\n\n\n“Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”\n\n\nStatistical prediction problems are of great practical and theoretical interest. The deep learning predictor has a number of advantages over traditional predictors, including that\n\ninput data can include all data of possible relevance to the prediction problem at hand\nnonlinearities and complex interactions among input data are accounted for seamlessly\noverfitting is more easily avoided than traditional high dimensional procedures\nthere exists fast, scale computational frameworks (TensorFlow)",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#what-makes-a-good-predictive-model",
    "href": "19-select.html#what-makes-a-good-predictive-model",
    "title": "1  Model Selection",
    "section": "1.2 What makes a good predictive model?",
    "text": "1.2 What makes a good predictive model?\nWhat makes a good model? If the goal is prediction, then the model is as good as its prediction. The easiest way to visualize the quality of the prediction is to plot \\(y\\) vs \\(\\hat y\\). Most of the times we are to use empirical assesment of model’s quality. However, sometimes theoretical bounds can be derived for a model that describe it acuracy. For example, in the case of the linear regression model, the prediction interval is defined by \\[\n\\hat y \\pm s\\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar x)^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\n\\] where \\(s\\) is the standard deviation of the residuals. The prediction interval is the confidence interval for the prediction. The prediction interval is wider than the confidence interval because it includes the uncertainty in the prediction.\nAssume we have predictive model \\[\ny = f(x) + \\epsilon\n\\] and we have some modeling assumption regarding the distribution of \\(\\epsilon\\). For example, when we use linrar regrssion or BART, we assume that \\(\\epsilon\\) follows normal distribution. One simple approach to test if observed samples \\(\\epsilon_1,\\ldots,\\epsilon_n\\) follow a specific distribution is to use an Exploratory Data Analysis (EDA).\nThe two most common tools for exploratory data analysis are Q-Q plot, scatter plots and bar plots/histograms.\nA Q-Q plot simply compares the quantiles of your data with the quantiles of a theoretical distribution (like normal, exponential, etc.). Quantile is the fraction (or percent) of points below the given value. That is, the \\(i\\)-th quantile is the point \\(x\\) for which \\(i\\)% of the data lies below \\(x\\). On a Q-Q plot, if the two data sets come from a population with the same distribution, we should see the points forming a line that’s roughly straight. More precisely, if the two data sets \\(x\\) and \\(y\\) come from the same distribution, then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on the line \\(y = x\\). If \\(y\\) comes from a distribution that’s linear in \\(x\\), then the points \\((x_{(i)}, y_{(i)})\\) should lie roughly on a line, but not necessarily on the line \\(y = x\\).\n\nExample 1.1 (Normal Q-Q plot) Figure 1.1 shows the normal Q-Q plot for the Data on birth weights of babies born in a Brisbane hospital on December 18, 1997. The data set contains 44 records. A more detailed description of the data set can be found in UsingR manual.\n\nbabyboom = read.csv(\"../data/babyboom.csv\")\nqqnorm(babyboom$wt)\nqqline(babyboom$wt)\n\n\n\n\n\n\n\nFigure 1.1: Normal Q-Q plot of baby weights\n\n\n\n\n\nVisually, the answer to answer the question “Are Birth Weights Normally Distributed?” is no. We can see that on the left side of the plot the points are below the line. This indicates that the data is skewed to the left. The data is not normally distributed.\nThe Q-Q plots look different if we split the data based on the gender\ng = babyboom %&gt;% filter(gender==\"girl\") %&gt;% pull(wt) \nb = babyboom %&gt;% filter(gender==\"boy\")  %&gt;% pull(wt) \nqqnorm(g); qqline(g)\nqqnorm(b); qqline(b)\n\n\n\n\n\n\nGirls\n\n\n\n\n\n\n\nBoys\n\n\n\n\n\n\nHistogram of baby weights by gender\n\n\n\nHow about the times in hours between births of babies?\n\nhr = ceiling(babyboom$running.time/60)\nBirthsByHour = tabulate(hr)\n# Number of hours with 0, 1, 2, 3, 4 births\nObservedCounts = table(BirthsByHour) \n# Average number of births per hour\nBirthRate=sum(BirthsByHour)/24    \n# Expected counts for Poisson distribution\nExpectedCounts=dpois(0:4,BirthRate)*24    \n# bind into matrix for plotting\nObsExp &lt;- rbind(ObservedCounts,ExpectedCounts) \nbarplot(ObsExp,names=0:4, beside=TRUE,legend=c(\"Observed\",\"Expected\"))\n\n\n\n\n\n\n\n\nWhat about the Q-Q plot?\n\n# birth intervals\nbirthinterval=diff(babyboom$running.time) \n # quantiles of standard exponential distribution (rate=1)   \nexponential.quantiles = qexp(ppoints(43)) \nqqplot(exponential.quantiles, birthinterval)\nlmb=mean(birthinterval)\nlines(exponential.quantiles,exponential.quantiles*lmb) # Overlay a line\n\n\n\n\n\n\n\n\nHere\n\nppoints function computes the sequence of probability points\nqexp function computes the quantiles of the exponential distribution\ndiff function computes the difference between consecutive elements of a vector",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#out-of-sample-performance",
    "href": "19-select.html#out-of-sample-performance",
    "title": "1  Model Selection",
    "section": "1.3 Out of Sample Performance",
    "text": "1.3 Out of Sample Performance\nA parametric model that we choose to fit to data is chosen from a family of functions. Then, we use optimization to find the best model from that family. To find the best model from a given family of functions we either minimize empirical loss or maximize the likelihood. Finding an appropriate family of functions is a major problem and is called model selection problem. For example, the choice of input variables to be included in the model is part of the model selection process. Model selection involves determining which predictors, interactions, or transformations should be included in the model to achieve the best balance between complexity and predictive accuracy. In practice, we often encounter several models for the same dataset that perform nearly identically, making the selection process challenging.\nIt is important to note that a good model is not necessarily the one that fits the data perfectly. Overfitting can occur when a model is overly complex, capturing noise rather than the underlying pattern. A good model strikes a balance between fitting the data well and maintaining simplicity to ensure generalizability to new, unseen data. For instance, including too many parameters can lead to a perfect fit when the number of observations equals the number of parameters, but such a model is unlikely to perform well on out-of-sample data.\nThe goal of model selection is not only to achieve a good fit but also to reduce complexity by excluding unnecessary parameters. This process typically involves selecting a model from a relevant class of functions while keeping in mind the trade-offs between bias, variance, and model complexity. Techniques such as cross-validation, information criteria (e.g., AIC, BIC), and regularization methods are commonly used to guide the model selection process.\nThe model selection task is sometimes one of the most consuming parts of the data analysis. Unfortunately, there is no single rule to find the best model. One way to think about the model choice problem as yet another optimization problem, with the goal to find best family of functions that describe the data. With a small number of predictors we can do brute force (check all possible models). For example, with \\(p\\) predictors there are \\(2^p\\) possible models with no interactions. Thus, the number of potential family functions is huge even for modest values of \\(p\\). One cannot consider all transformations and interactions.\nOur goal is to build a model that predicts well for out-of-sample data, e.g. the data that was not used for training. Eventually, we are interested in using our models for prediction and thus, the out of sample performance is the most important metric and should be used to choose the final model. In-sample performance is of little interest when predictive model need to be chosen, as one of the winners of Netflix prize put it, “It’s like predicting how much someone will like a movie, having them watch it and tell you how much they really liked it”. The out-of-sample performance is the final judge of the quality of our model. The goal is to use data to find a pattern that we can exploit. The pattern will be “statistical” in its nature. To uncover the pattern we start with a training dataset, denoted by \\[\nD = (y_i,x_i)_{i=1}^n\n\\] and to test the validity of our mode we use out-of-sample testing dataset \\[\nD^* = (y_j^*, x_j^*)_{j=1}^m,\n\\] where \\(x_i\\) is a set of \\(p\\) predictors ans \\(y_i\\) is response variable.\nA good predictor will “generalize” well and provide low MSE out-of-sample. These are a number of methods/objective functions that we will use to find, \\(\\hat f\\). In a parameter-based style we will find a black box. There are a number of ways to build our black box model. Our goal is to find the map \\(f\\) that approximates the process that generated the data. For example data could be representing some physical observations and our goal is recover the “laws of nature\" that led to those observations. One of the pitfalls is to find a map \\(f\\) that does not generalize. Generalization means that our model actually did learn the”laws of nature\" and not just identified patterns presented in training. The lack of generalization of the model is called over-fitting. It can be demonstrated in one dimension by remembering the fact from calculus that any set of \\(n\\) points can be approximated by a polynomial of degree \\(n\\), e.g we can alway draw a line that connects two points. Thus, in one dimension we can always find a function with zero empirical risk. However, such a function is unlikely to generalize to the observations that were not in our training data. In other words, the empirical risk measure for \\(D^*\\) is likely to be very high. Let us illustrate that in-sample fit can be deceiving.\n\nExample 1.2 (Hard Function) Say we want to approximate the following function \\[\nf(x) = \\dfrac{1}{1+25x^2}.\n\\] This function is simply a ratio of two polynomial functions and we will try to build a liner model to reconstruct this function\n\nx = seq(-2,2,by=0.01)\ny = 1/(1+25*x^2)\n# Approximate with polynomial of degree 1 and 2\nm1 = lm(y~x)\nm2 = lm(y~poly(x,2))\n# Approximate with polynomial of degree 20 and 5\nm20 = lm(y~poly(x,20))\nm5 = lm(y~poly(x,5))\nx = seq(-3,3,by=0.01)\ny = 1/(1+25*x^2)\nplot(x,y,type='l',col='black',lwd=2)\nlines(x,predict(m1,list(x=x)),lwd=2, col=1)\nlines(x,predict(m2,poly(x,2)),lwd=2, col=2)\nlines(x,predict(m5,poly(x,5)),lwd=2, col=3)\nlines(x,predict(m20,poly(x,20)),lwd=2, col=4)\nlegend(\"topright\", legend=c(\"f(x)\",\"m1\",\"m2\",\"m5\",\"m20\"), col=c(\"black\",1:4), lty=1, cex=0.8, bty='n')\n\n\n\n\n\n\n\nFigure 1.2: Runge-Kutta function\n\n\n\n\n\nFigure 1.2 shows the function itself (black line) on the interval \\([-3,3]\\). We used observations of \\(x\\) from the interval \\([-2,2]\\) to train the data (solid line) and from \\([-3,-2) \\cup (2,3]\\) (dotted line) to test the model and measure the out-of-sample performance. We tried four different linear functions to capture the relations. We see that linear model \\(\\hat y = \\beta_0 + \\beta_1 x\\) is not a good model. However, as we increas the degree of the polynomial to 20, the resulting model \\(\\hat y = \\beta_0 + \\beta_1x + \\beta_2 x^2 +\\ldots+\\beta_{20}x^{20}\\) does fit the training data set quite well, but does very poor job on the test data set. Thus, while in-sample performance is good, the out-of sample performance is unsatisfactory. We should not use the degree 20 polynomial function as a predictive model. In practice in-sample out-of-simple loss or classification rates provide us with a metric for providing horse race between different predictors. It is worth mentioning here there should be a penalty for overly complex rules which fits extremely well in sample but perform poorly on out-of-sample data. As Einstein famous said “model should be simple, but not simpler.”\n\nTo a Bayesian, the solution to these decision problems are rather obvious: compute posterior distributions, and then make decisions by maximizing expected utility, where the posterior distribution is used to calculate the expectations. Classical solutions to these problems are different, and use repeated sampling ideas, whereby the performance of a decision rule is judged on its performance if the same decision problem were repeated infinitely. Thus, the decisions are made based on their population properties. One of the main uses of statistical decision theory is to compare different estimators or hypothesis testing procedures. This theory generates many important findings, most notably that many of the common classical estimators are “bad”,in some sense, and that Bayesian estimators are always “good”.\nThese results have major implications for empirical work and practical applications, as they provide a guide for forecasting.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#bias-variance-trade-off",
    "href": "19-select.html#bias-variance-trade-off",
    "title": "1  Model Selection",
    "section": "1.4 Bias-Variance Trade-off",
    "text": "1.4 Bias-Variance Trade-off\nFor any predictive model we seek to achieve best possible results, i.e. smallest MSE or misclassification rate. However, a model performance can be different as data used in one training/validation split may produce results dissimilar to another random split. In addition, a model that performed well on the test set may not produce good results given additional data. Sometimes we observe a situation, when a small change in the data leads to large change in the final estimated model, e.g. parameters of the model. These results exemplify the bias/variance tradeoff, where increasing model bias produces large variance in the final results. Similarly, low bias results in low variance, but can also produce an oversimplification of the final model. While Bias/variance concept is depicted below.\n\n\n\n\n\n\nFigure 1.3: Bias-variance trade-off\n\n\n\n\nExample 1.3 (Bias-variance) We demonstrate bias-variance concept using Boston housing example. We fit a model \\(\\mathrm{medv} = f(\\mathrm{lstat})\\). We use polynomial functions to approximate this relation. We fitted twelve polynomial functions with degree \\(1,\\ldots,12\\) ten time. Each time we randomly selected 20% of sample for testing and the rest for training. We estimated in-of-sample performance (bias) and out-of-sample performance by calculating MSE on training and testing sets correspondingly. For each polynomial \\(f\\) we averaged MSE from each of the ten models.\nFigure 1.4 (a) shows bias and variance for our twelve different models. As expected, bias increases while variance increases as model complexity grows. On the other hand out-of-sample MSE is a U-shaped curve. The optimal model is the one that has smallest out-of-sample MSE. In our case it is polynomial of degree 5!\n\n\n\n\n\n\n\n\n\n\n\n(a) Metrics for twelve polynomial functions fitted into Boston housing data set. As model complexity (degree of the polynomial function) increases, model variance increase and bias decreases. Out-of-sample MSE is smallest for 5th degree polynomial function, which is the optimal model in terms of bias-variance trade-off.\n\n\n\n\n\n\n\n\n\n\n\n(b) Optimal complexity model, which is 5th degree polynomial used to predict observations from testing data set. Model predictions (red line) are compared to actual observed values of medv variable (dots)\n\n\n\n\n\n\n\nFigure 1.4: Metrics for 12 models\n\n\n\nLet’s take another, a more formal, look at bias-variance trade-off for a linear regression problem. We are interested in the decomposition of the error \\(\\E{(y-\\hat y)^2}\\) as a function of bias \\(\\E{y-\\hat y}\\) and variance \\(\\Var{\\hat y}\\).\nHere \\(\\hat y = \\hat f_{\\beta}(x)\\) prediction from the model, and \\(y = f(x) + \\epsilon\\) is the true value, which is measured with noise \\(\\Var{\\epsilon} = \\sigma^2\\), \\(f(x)\\) is the true unknown function. The expectation above measures squared error of our model on a random sample \\(x\\). \\[\n\\begin{aligned}\n\\E{(y - \\hat{y})^2}\n& = \\E{y^2 + \\hat{y}^2 - 2 y\\hat{y}} \\\\\n& = \\E{y^2} + \\E{\\hat{y}^2} - \\E{2y\\hat{y}} \\\\\n& = \\Var{y} + \\E{y}^2 + \\Var{\\hat{y}} + \\E{\\hat{y}}^2 - 2f\\E{\\hat{y}} \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f^2 - 2f\\E{\\hat{y}} + \\E{\\hat{y}}^2) \\\\\n& = \\Var{y} + \\Var{\\hat{y}} + (f - \\E{\\hat{y}})^2 \\\\\n& = \\sigma^2 + \\Var{\\hat{y}} + \\mathrm{Bias}(\\hat{y})^2\\end{aligned}\n\\] Here we used the following identity: \\(\\Var{X} = \\E{X^2} - \\E{X}^2\\) and the fact that \\(f\\) is deterministic and \\(\\E{\\epsilon} = 0\\), thus \\(\\E{y} = \\E{f(x)+\\epsilon} = f + \\E{\\epsilon} = f\\).",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#cross-validation",
    "href": "19-select.html#cross-validation",
    "title": "1  Model Selection",
    "section": "1.5 Cross-Validation",
    "text": "1.5 Cross-Validation\nIf the data set at-hand is small and we cannot dedicate large enough sample size for testing, simply measuring error on test data set can lead to wrong conclusions. When size of the testing set \\(D^*\\) is small, the estimated out-of-sample performance is of high variance, depending on precisely which observations are included in the test set. On the other hand, when training set \\(D^*\\) is a large fraction of the entire sample available, estimated out-of-sample performance will be underestimated. Why?\nA trivial solution is to perform the training/testing split randomly several times and then use average out-of-sample errors. This procedure has two parameters, the fraction of samples to be selected for testing \\(p\\) and number of estimates to be performed \\(K\\). The resulting algorithm is as follows\nfsz = as.integer(p*n)\nerror = rep(0,K)\nfor (k in 1:K)\n{\n    test_ind = sample(1:n,size = fsz)\n    training = d[-test_ind,]\n    testing  = d[test_ind,]\n    m = lm(y~x, data=training)\n    yhat = predict(m,newdata = testing)\n    error[k] = mean((yhat-testing$y)^2)\n}\nres = mean(error)\nFigure 1.5 shows the process of splitting data set randomly five times.\nCross validation modifies the random splitting approach uses more “disciplined” way to split data set for training and testing. Instead of randomly selecting training data points, CV chooses consecutive observations and thus, each data point is used once for testing. As the random approach, CV helps addressing the high variance issue of out-of-sample performance estimation when data set available is small. Figure 1.6 shows the process of splitting data set five times using cross-validation approach.\n\n\n\n\n\n\n\n\n\nFigure 1.5: Bootstrap\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.6: Cross-validation\n\n\n\n\n\n\n\nTraining set (red) and testing set (green)\n\n\n\n\nExample 1.4 (Simulated) We use simulated data set to demonstrate difference between estimated out-of-sample performance using random 20/80 split, 5-fold cross-validation and random split. We used \\(x=-2,-1.99,-1.98,\\ldots,2\\) and \\(y = 2+3x + \\epsilon, ~ \\epsilon \\sim N(0,\\sqrt{3})\\). We simulated 35 datasets of size 100. For each of the simulated data sets, we fitted a linear model and estimated out-of-sample performance using three different approaches. Figure 1.7 compares empirical distribution of errors estimated from 35 samples.\n\n\n\n\n\n\nFigure 1.7: Empirical comparison of simple split, cross-validation, and bootstrap approaches to estimate out-of sample performance.\n\n\n\nAs we can see the estimated out-of-sample performance by a training set approach is of high variance. While, both cross-validation and bootstrap approaches lead to better estimates, they require model to be fitted 5 times, which can be computationally costly for a complex model. On the other hand, estimate from cross-validation is of lower variance and less bias compared to the bootstrap estimate. Thus, we should prefer cross-validation.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#bayesian-model-selection",
    "href": "19-select.html#bayesian-model-selection",
    "title": "1  Model Selection",
    "section": "1.6 Bayesian Model Selection",
    "text": "1.6 Bayesian Model Selection\nThe probabilistic models of interest are the joint probability distribution \\(p(D,\\theta)\\) (called a generative model) and \\(P(Y,\\theta \\mid X)\\) (discriminative model). Discriminative models are easier to build and are more frequently used in practice. Generative model requires modeling a distribution over the set of observed variables, which makes our model more complicated. Text analysis provides an illustrative example. The task of identifying a topic of an article can be solved using discriminative distribution. The problem of generating a new article requires generative model.\nLet \\(D\\) denote data. Let \\(\\theta_M \\in \\Theta_M\\) denote a set of parameters under model \\(M \\in \\mathcal{M}\\). Let \\(\\theta_M = (\\theta_1, \\ldots, \\theta_p)\\) be the \\(p\\)-vector of parameters. The Bayesian approach is straightforward: implement the Bayesian paradigm by executing Bayes rule. This requires the laws of probability and not optimization techniques. The notion of model complexity is no different. Let \\(\\mathcal{M}\\) denote the space of models and \\(\\theta\\) be the parameter vector. The Bayesian paradigm simply places probabilities over parameters and models given the data, namely \\(p(\\theta_M, M | y)\\), where \\(y = (y_1, \\ldots, y_n)\\). This has a number of decompositions. Bayes theorem calculates the joint posterior over parameters and models given data \\(D\\), namely \\[\n    P(\\theta_M,M\\mid D) = P(\\theta_M \\mid M,D)P(M\\mid D).\n\\] Notice how this factors the posterior into two terms: the conditional posterior over parameters given the model and the posterior over models given data.\nThe key quantity is the weight of evidence (a.k.a. marginal distribution of the data \\(D\\) given the model \\(M\\)), defined by \\[\np( D | M ) = \\int_{ \\Theta_M } p( D \\mid \\theta_M , M )  p ( \\theta_M | M ) d \\theta_M.\n\\] Here \\(p( D \\mid \\theta_M , M )\\) is the traditional likelihood function. The key conditional distribution, however, is the specification of the prior over parameters \\(p(\\theta_M | M)\\). As this is used in the marginalization, it can affect the Bayes risk dramatically. Occam’s razor comes from the fact that this marginalization provides a weight of evidence that favors simpler models over more complex ones.\nThis leads to a posterior over models, which is calculated as: \\[\\begin{align*}\n    P(M\\mid D)  & = \\dfrac{P(D\\mid M)P(M)}{P(D)}, \\\\\n    P(D\\mid M ) & = \\int_{ \\Theta_M} P(D\\mid \\theta_M , M ) p( \\theta_M | M ) d \\theta_M.\n\\end{align*}\\] Notice that this requires a joint prior specification \\(p(\\theta_M, M) = p(\\theta_M | M)p(M)\\) over parameters and models. The quantity \\(p(M| D)\\) is the marginal posterior for model complexity given the data. There is an equivalent posterior \\(p(\\theta_M | D)\\) for the parameters. \\(p(D \\mid M)\\) is the evidence of the data \\(D\\) given the complexity (a.k.a. conditional likelihood). The full evidence is \\[\np( D ) = \\int p( D| M ) p(M) d M.\n\\] This has been used to select the amount of hyperparameter regularization; see, for example,MacKay (1992).\nWe will see that the prior \\(p(\\theta_M | M)\\) will lead to an Occam’s razor effect, namely that the marginal distribution will favor simpler models. Importantly, this Occam’s razor effect is not in conflict with the Bayesian double descent phenomenon, which emerges from the marginal posterior of models given data and the conditional prior specification \\(p(\\theta_M | M)\\).\n\nExample 1.5 (Dice Example) Let’s consider a simple example of throwing a dice. Say, there are three dices, one that has numbers 1 through 6 (regular dice), one with three sides with ones and three sides with 2s (dice 1-2), one with three sides with ones and three sides with 2s and three sides with 3s (dice 1-2-3).\n\n\n\n\n\n\nDice 1-2\n\n\n\n\n\n\n\nDice 1-2-3\n\n\n\n\n\n\n\nRegular Dice\n\n\n\n\n\nYou observe outcome of one dice throw and it is 3. Which dice is it?\nTwo out of three explanations are plausible (dice 1-2-3 and regular dice). Intuitively, the 1-2-3 dice is more likely to produce 3 than the regular dice. Thus, if we need to choose, we would choose the 1-2-3 dice. For the sake of completeness, we can use the Bayes rule to calculate the evidence for each model.\nUsing Bayes’ rule: \\[P(M_i | D) = \\frac{P(D | M_i) P(M_i)}{P(D)}\\]\nwhere \\(M_i\\) represents each dice model and \\(D\\) is the observed data (outcome = 3).\nWe equal prior probabilities for each dice: \\[P(M_1) = P(M_2) = P(M_3) = \\frac{1}{3}.\\] Now, we calculate the likelihood for each model.\n\n\n\n\n\n\n\nDice Type\nProbability of Rolling a 3\n\n\n\n\nRegular dice (\\(M_1\\))\n\\(P(3 | M_1) = \\frac{1}{6}\\)\n\n\nDice 1-2 (\\(M_2\\))\n\\(P(3 | M_2) = 0\\) (impossible since this dice only has 1s and 2s)\n\n\nDice 1-2-3 (\\(M_3\\))\n\\(P(3 | M_3) = \\frac{1}{3}\\) (2 out of 6 sides show 3)\n\n\n\nThen, the marginal likelihood is: \\[P(D) = \\sum_{i=1}^{3} P(D | M_i) P(M_i) = \\frac{1}{6} \\cdot \\frac{1}{3} + 0 \\cdot \\frac{1}{3} + \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{1}{6}.\\]\nFinally, posterior probabilities are\n\n\n\n\n\n\n\n\nDice Type\nPosterior Probability Calculation\nPosterior Probability\n\n\n\n\nRegular dice\n\\(P(M_1 | D) = \\frac{\\frac{1}{6} \\cdot \\frac{1}{3}}{\\frac{1}{6}}\\)\n\\(\\frac{1}{3}\\) (33.3%)\n\n\nDice 1-2\n\\(P(M_2 | D) = \\frac{0 \\cdot \\frac{1}{3}}{\\frac{1}{6}}\\)\n\\(0\\) (0%)\n\n\nDice 1-2-3\n\\(P(M_3 | D) = \\frac{\\frac{1}{3} \\cdot \\frac{1}{3}}{\\frac{1}{6}}\\)\n\\(\\frac{2}{3}\\) (66.7%)\n\n\n\nGiven the observation of outcome 3, the dice 1-2-3 is twice as likely as the regular dice. The dice 1-2 is completely ruled out since it cannot produce a 3. This demonstrates how Bayesian model selection naturally eliminates impossible explanations and provides relative evidence for competing hypotheses.\n\nThis example demonstrates how the Bayesian paradigm provides a coherent framework to simultaneously infer parameters and model complexity. The fact that Bayesian model selects simplest possible explanation for the observed data, is called the automatic Occam’s razor. The Occam’s razor is a principle that states that the simplest explanation is the best explanation.\nWhile performing data analysis using learning algorithms, we perform two tasks, namely training and inference which are summarized in the table below\n\n\n\nStep\nGiven\nHidden\nWhat to find\n\n\n\n\nTraining\n\\(D = (X,Y) = \\{x_i,y_i\\}_{i=1}^n\\)\n\\(\\theta\\)\n\\(p(\\theta \\mid D)\\)\n\n\nPrediction\n\\(x_{\\text{new}}\\)\n\\(y_{\\text{new}}\\)\n\\(p(y_{\\text{new}}  \\mid  x_{\\text{new}}, D)\\)\n\n\n\nThe training can be performed via the Bayes rule \\[\np(\\theta \\mid D) = \\dfrac{p(Y \\mid \\theta,X)p(\\theta)}{\\int p(Y \\mid \\theta,X)p(\\theta)d\\theta}.\n\\] Now to perform the second step (prediction), we calculate \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta\n\\] Thus, full Bayesian inference requires calculating two integrals, which might be difficult. We mentioned earlier that MAP allows us to avoid those calculations by approximating the posterior with \\[\np(\\theta \\mid D) \\approx \\delta(\\theta_{\\text{MAP}}),~~\\theta_{\\text{MAP}} \\in \\argmax_{\\theta}p(\\theta \\mid D)\n\\] To calculate \\(\\theta_{\\text{MAP}}\\), we do not need to know the normalizing constant for calculating posterior, since the solution of optimization problem does not depend on this constant. Further, the second integral for inference becomes degenerate and get approximated by \\[\np(y_{\\text{new}}  \\mid  x_{\\text{new}}, D) = \\int p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta)p(\\theta \\mid D)d\\theta \\approx p(y_{\\text{new}}  \\mid  x_{\\text{new}},\\theta_{\\text{MAP}}).\n\\]\nThe Figure 1.8 below illustrates the Bayesian model selection process. The figure shows the joint distribution over parameters and data for three models. You can think of each ellipse as the region where most of the probability mass is concentrated.\n\n\n\n\n\n\nFigure 1.8: Bayesian model Selection\n\n\n\nIf we project the ellipses onto the parameter space, we get the prior distributions for each model. We can see that the \\(M_2\\) is the most concentrated. If we project the ellipses onto the data space, we get the prior distributions over data for each model.\nAfter observing data \\(D\\) (horizontal line), each prior gets updated. The intersection of the observed data line with each ellipse shows how well each model can explain the data. Models with good overlap between prior and observed data will have higher posterior probability. \\(M_3\\) appears to have the best intersection with the observed data, it is the model with the highest marginal likelihood.\nThis illustrates how Bayesian model selection naturally favors models that achieve the best balance between explaining the observed data and maintaining appropriate complexity, automatically implementing Occam’s razor through the evidence calculation.\n\nExample 1.6 (Racial discrimination) Say we want to analyze racial discrimination by the US courts. We have three variables:\n\nMurderer: \\(m \\in {0,1}\\) (black/white)\nVictim: \\(v \\in \\{0,1\\}\\) (black/white)\nVerdict: \\(d \\in \\{0,1\\}\\) (prison/death penalty)\n\nSay we have the data\n\n\n\nm\nv\nd\nn\n\n\n\n\n0\n0\n0\n132\n\n\n0\n0\n1\n19\n\n\n0\n1\n0\n9\n\n\n0\n1\n1\n0\n\n\n1\n0\n0\n52\n\n\n1\n0\n1\n11\n\n\n1\n1\n0\n97\n\n\n1\n1\n1\n6\n\n\n\nWe would like to establish a causal relations between the race and verdict variables. For this, we consider several models\n\n\\(p(d \\mid m,v) = p(d) = \\theta\\)\n\\(p(d \\mid m,v) = p(d \\mid v)\\); \\(p(d \\mid v=0) = \\alpha, ~p(d \\mid v=1)=\\beta\\)\n\\(p(d \\mid v,m) = p(d \\mid m)\\); \\(p(d \\mid m=1) = \\gamma,~p(d \\mid m=1) = \\delta\\)\n\\(p(d|v,m)\\) cannot be reduced, and\n\n\n\n\n\\(p(d=1 \\mid m,v)\\)\n\\(m=0\\)\n\\(m=1\\)\n\n\n\n\n\\(v=0\\)\n\\(\\tau\\)\n\\(\\chi\\)\n\n\n\\(v=1\\)\n\\(\\nu\\)\n\\(\\zeta\\)\n\n\n\n\nWe calculate which model describes data the best, we calculate the evidences. We need to describe the discriminative model \\[\np(Y ,\\theta \\mid X) = p(Y \\mid X,\\theta)p(\\theta \\mid X)\n\\] Here \\(X\\) is the number of cases, and \\(Y\\) is the number of death penalties. We use uninformative prior \\(\\theta \\sim U[0,1]\\). To specify the likelihood, we use Binomial distribution \\[\nY \\mid X,\\theta \\sim B(X,\\theta),~~B(Y \\mid X,\\theta) = C_Y^Xp^Y(1-\\theta)^{X-Y}\n\\] We assume \\(p(\\theta)\\sim Uniform\\). Now lets calculate the evidence \\[\np(Y, \\theta \\mid X) = \\int p(Y  \\mid  X,\\theta)p(\\theta)d\\theta\n\\] for each of the four models\n\n\\(p(Y \\mid X) = \\int B(19 \\mid 151,\\theta)B(0 \\mid 9,\\theta)B(11 \\mid 63,\\theta)B(6 \\mid 103,\\theta)d\\theta\\) \\(\\propto \\int_0^{1} \\theta^{36}(1-\\theta)^{290}d\\theta = B(37,291) = 2.8\\times 10^{-51}\\)\n\\(p(Y \\mid X) = \\int\\int B(19 \\mid 151,\\alpha)B(0 \\mid 9,\\beta)B(11 \\mid 63,\\alpha)B(6 \\mid 103,\\beta)d\\alpha d\\beta \\propto 4.7\\times 10^{-51}\\)\n\\(p(d \\mid v,m) = p(d \\mid m)=\\int\\int B(19 \\mid 151,\\gamma)B(0 \\mid 9,\\gamma)B(11 \\mid 63,\\delta)B(6 \\mid 103,\\delta)d\\gamma d\\delta \\propto 0.27\\times10^{-51}\\)\n\\(p(d \\mid v,m) = \\int\\int\\int\\int B(19 \\mid 151,\\tau)B(0 \\mid 9,\\nu)B(11 \\mid 63,\\chi)B(6 \\mid 103,\\zeta)d\\tau d\\nu d\\chi d\\zeta \\propto 0.18\\times10^{-51}\\)\n\nThe last model is too complex, it can explain any relations in the data and this, has the lowest evidence score! However, if we are to use ML estimates, the fourth model will have the highest likelihood. Bayesian approach allows to avoid over-fitting! You can also see that this data set contains the Simpson’s paradox. Check it! A related problem is Bertrand’s gold box problem.\n\n\n1.6.1 The Bayesian Information Criterion\nThe Bayesian Information Criterion (BIC) is a model selection criterion that penalizes the complexity of the model. It is derived from a Bayesian approach. The BIC is defined as:\n\\[\n\\mathrm{BIC} =  \\log P(D\\mid \\hat{\\theta}_k, M_k) - \\frac{k}{2} \\log n.\n\\]\nHere \\(\\hat{\\theta}_k\\) is the MAP estimate of the \\(k\\) parameters in model \\(M_k\\), and \\(n\\) is the sample size. As such, there is a penalty \\(-\\frac{k}{2} \\log n\\) for increasing the dimensionality \\(k\\) of the model under consideration.\nBIC uses the marginal likelihood of the data under model \\(M_k\\) (denoted \\(M\\) for simplicity here) is approximated using Laplace’s method.\nThe idea of Laplace’s method is to approximate integrals of the form \\(\\int f(\\theta) e^{-g(\\theta)} d\\theta\\) where \\(g(\\theta)\\) has a sharp minimum at some point \\(\\hat{\\theta}\\). The method works by approximating \\(g(\\theta)\\) with its second-order Taylor expansion around the minimum \\(\\hat{\\theta}\\):\n\\[\ng(\\theta) \\approx g(\\hat{\\theta}) + \\frac{1}{2}g''(\\hat{\\theta})(\\theta-\\hat{\\theta})^2\n\\]\nsince \\(g'(\\hat{\\theta}) = 0\\) at the minimum. This transforms the integral into a Gaussian form:\n\\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} \\int e^{-\\frac{1}{2}g''(\\hat{\\theta})(\\theta-\\hat{\\theta})^2} d\\theta\n\\]\nThe remaining integral is a standard Gaussian integral that evaluates to \\(\\sqrt{\\frac{2\\pi}{g''(\\hat{\\theta})}}\\), giving us:\n\\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} \\sqrt{\\frac{2\\pi}{g''(\\hat{\\theta})}}\n\\].\nIn the multivariate case, we have \\(\\theta \\in \\mathbb{R}^k\\) is a \\(k\\)-dimensional parameter vector. The second-order Taylor expansion around the minimum \\(\\hat{\\theta}\\) becomes:\n\\[\ng(\\theta) \\approx g(\\hat{\\theta}) + \\frac{1}{2}(\\theta-\\hat{\\theta})^T \\mathbf{H}(\\hat{\\theta}) (\\theta-\\hat{\\theta})\n\\]\nwhere \\(\\mathbf{H}(\\hat{\\theta})\\) is the \\(k \\times k\\) Hessian matrix of second derivatives at \\(\\hat{\\theta}\\). The multivariate Gaussian integral then evaluates to:\n\\[\n\\int f(\\theta) e^{-g(\\theta)} d\\theta \\approx f(\\hat{\\theta}) e^{-g(\\hat{\\theta})} (2\\pi)^{k/2} |\\det(\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}\n\\]\nIn the context of Bayesian model selection, we apply this to approximate the marginal likelihood (evidence). We have:\n\\[P(D\\mid M) = \\int P(D\\mid \\theta,M)P(\\theta\\mid M)d\\theta\\]\nTaking the logarithm and identifying \\(g(\\theta) = -\\log P(D\\mid \\theta,M)P(\\theta\\mid M)\\), the maximum a posteriori (MAP) estimate \\(\\hat{\\theta}\\) corresponds to the minimum of \\(g(\\theta)\\). The second derivative (Hessian) \\(\\mathbf{H}(\\hat{\\theta})\\) at this point determines the curvature of the log-posterior.\n\\[\nP(D\\mid M) = \\int P(D\\mid \\theta,M)P(\\theta\\mid M)d\\theta \\approx P(D\\mid \\hat{\\theta},M)P(\\hat{\\theta}\\mid M) (2 \\pi)^{k/2} |\\det(\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}.\n\\]\nHere \\(\\hat{\\theta}\\) is the posterior mode (MAP estimate), and \\(\\mathbf{H}(\\hat{\\theta})\\) is the negative Hessian of the log-posterior at the mode. Taking the logarithm, and assuming \\(P(\\hat{\\theta}|M)\\) and Hessian terms are \\(O_p(1)\\) or scale appropriately with \\(n\\) (this assumption is justified because as \\(n\\) increases, the likelihood dominates the prior, making the prior term negligible relative to the \\(O(\\log n)\\) likelihood term, while the Hessian determinant typically grows polynomially in \\(n\\), contributing at most \\(O(\\log n)\\) terms that are absorbed into the approximation), we get:\n\\[\n\\log P(D\\mid M) \\approx \\log P(D\\mid \\hat{\\theta},M) - \\dfrac{k}{2}\\log n,\n\\]\nwhich is proportional to the BIC. (Note: The exact definition and derivation of BIC can vary slightly, but this captures the essence). The BIC approximation shows how the Bayesian approach naturally penalizes model complexity through the dimensionality term \\(-\\frac{k}{2}\\log n\\).\nThe Bayesian approach averages over the posterior distribution of models given data. Suppose that we have a finite list of models \\(M \\in \\{M_1, \\ldots, M_J\\}\\). Then we can calculate the posterior over models as:\n\\[\np(M_j | y) = \\frac{p(y | M_j) p(M_j)}{\\sum_{i=1}^J p(y | M_i) p(M_i)}, \\quad {\\rm where} \\; p(y | M_j) = \\int L_j(\\theta_j|y) p(\\theta_j | M_j) d\\theta_j.\n\\]\nLaplace’s approximation provides a simple (Lindley 1961) illustration of how dimensionality is weighted in the Bayesian paradigm. Hence, BIC is related to a log-posterior approximation. Hence, if prior model probabilities \\(P(M_j)\\) are uniform, then \\(P(M_j\\mid D) \\propto P(D \\mid M_j) \\approx \\exp(\\mathrm{BIC}_j)\\).\nIn a more general case, the evidence (a.k.a. marginal likelihood) for hypotheses (a.k.a. models) \\(M_i\\) is calculated as follows:\n\\[\nP(D\\mid M_i) = \\int P(D\\mid \\theta, M_i)P(\\theta\\mid M_i)d\\theta.\n\\]\nLaplace approximation, in the one-dimensional case (\\(k=1\\)), yields:\n\\[\nP(D\\mid M_i) \\approx P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i)\\sqrt{2\\pi}\\sigma_{\\text{post}}.\n\\]\nHere \\(\\hat{\\theta}\\) is the maximum (MAP) estimate of the parameter and \\(\\sigma_{\\text{post}} = (-H(\\hat{\\theta}))^{-1/2}\\) where \\(H(\\hat{\\theta})\\) is the second derivative of the log-posterior at \\(\\hat{\\theta}\\).\nGenerally, in the \\(k\\)-dimensional case, we have:\n\\[\nP(D\\mid M_i) \\approx P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i) (2\\pi)^{k/2} |\\det(-\\mathbf{H}(\\hat{\\theta}))|^{-\\frac{1}{2}}.\n\\]\nHere \\(\\mathbf{H}(\\hat{\\theta}) = \\nabla^2\\log (P(D\\mid \\hat{\\theta}, M_i)P(\\hat{\\theta}\\mid M_i))\\) is the Hessian of the log-posterior function evaluated at the mode \\(\\hat{\\theta}\\). As the amount of data collected increases, this Gaussian approximation is expected to become increasingly accurate.\nMackay (MacKay 1992) proposes the NIC criterion for selection of neural networks.\n\n\n1.6.2 NIC and evidence framework\nDavid MacKay’s Neural Information Criterion (NIC) and the associated evidence framework represent pioneering efforts to apply Bayesian model selection principles to neural networks. This framework addresses the fundamental challenge of selecting appropriate network architectures and hyperparameters by computing approximations to the marginal likelihood, or model evidence, for different neural network configurations.\nThe evidence framework builds upon the Laplace approximation to estimate the marginal likelihood of neural network models. Given a neural network with parameters \\(\\theta\\) and hyperparameters \\(\\alpha\\) (such as weight decay parameters), the evidence for a particular model configuration is:\n\\[\nP(D|M,\\alpha) = \\int P(D|\\theta,M) P(\\theta|M,\\alpha) d\\theta\n\\]\nwhere \\(D\\) represents the training data, \\(M\\) denotes the model architecture, and \\(P(\\theta|M,\\alpha)\\) is the prior distribution over network weights. The Laplace approximation evaluates this integral by expanding the log-posterior around its mode \\(\\theta^*\\), yielding:\n\\[\n\\log P(D|M,\\alpha) \\approx \\log P(D|\\theta^*,M) + \\log P(\\theta^*|M,\\alpha) - \\frac{1}{2}\\log|H|\n\\]\nwhere \\(H\\) is the Hessian of the negative log-posterior at the mode \\(\\theta^*\\). This approximation transforms the intractable integral into a computation involving the maximum a posteriori (MAP) estimate and the curvature of the posterior at that point.\nThe evidence framework provides a principled approach to several critical decisions in neural network design. For hyperparameter selection, the framework automatically determines optimal regularization strengths by maximizing the evidence with respect to hyperparameters such as weight decay coefficients. Rather than relying on cross-validation, which can be computationally expensive and may not capture the full uncertainty in hyperparameter selection, the evidence provides a direct measure of how well different hyperparameter values support the observed data.\nArchitecture comparison becomes feasible through direct evidence computation for different network structures. The framework can compare networks with different numbers of hidden units, layers, or connectivity patterns by evaluating their respective marginal likelihoods. This comparison naturally incorporates Occam’s razor, as more complex architectures are penalized through the integration over their larger parameter spaces, unless the additional complexity is justified by substantially improved fit to the data.\nThe Hessian computation required for the Laplace approximation presents significant computational challenges for modern deep networks with millions or billions of parameters. The full Hessian matrix would be prohibitively large to compute and store explicitly. MacKay’s original framework addressed this through various approximation strategies, including the use of automatic relevance determination (ARD) priors that allow the network to effectively prune irrelevant connections by driving their associated precision parameters to infinity.\nModern adaptations of the evidence framework have developed sophisticated methods to handle the computational challenges of contemporary deep learning. Linearized variants approximate the neural network through its first-order Taylor expansion around the MAP estimate, reducing the complexity of Hessian computations while maintaining reasonable approximation quality. These linearized approaches are particularly effective for networks that are sufficiently wide or when the posterior is approximately Gaussian.\nKronecker-structured approximations represent another significant advancement, exploiting the structure of neural network computations to factorize the Hessian matrix into more manageable components. By recognizing that gradients in neural networks can be expressed as Kronecker products of activations and error signals, these methods achieve substantial computational savings while preserving much of the information contained in the full Hessian matrix.\nThe evidence framework also naturally handles the multiple scales of uncertainty present in neural networks. Parameter uncertainty captures the uncertainty in individual weight values given the training data, while hyperparameter uncertainty reflects uncertainty about the appropriate level of regularization or architectural choices. Model uncertainty encompasses uncertainty about the fundamental model class or architecture family. The hierarchical Bayesian treatment allows simultaneous reasoning about all these sources of uncertainty within a unified framework.\nContemporary applications of the evidence framework extend beyond traditional supervised learning to include unsupervised learning, reinforcement learning, and generative modeling. In variational autoencoders, for example, the evidence framework provides principled methods for selecting latent dimensionalities and regularization strengths. In Bayesian neural networks, the evidence guides the selection of prior distributions and network architectures while providing uncertainty quantification for predictions.\nDespite its theoretical elegance, the evidence framework faces practical limitations in very large-scale applications. The computational requirements of Hessian approximation, even with modern efficient methods, can be substantial for networks with hundreds of millions of parameters. The Laplace approximation itself may be inadequate when the posterior is highly non-Gaussian, which can occur in networks with many local minima or complex loss landscapes.\nRecent developments in the field have sought to address these limitations through alternative approximation schemes and computational innovations. Variational approaches replace the Laplace approximation with more flexible posterior approximations, while sampling-based methods use advanced MCMC techniques to explore the posterior distribution more thoroughly. These developments maintain the conceptual advantages of the evidence framework while extending its applicability to the scale and complexity of modern deep learning applications.\nThe enduring value of MacKay’s evidence framework lies in its principled approach to the fundamental trade-offs in machine learning model design. By providing a theoretically grounded method for balancing model complexity against data fit, the framework offers insights that remain relevant even as the scale and sophistication of machine learning models continue to evolve. The automatic hyperparameter selection and architecture comparison capabilities of the evidence framework continue to influence contemporary approaches to neural architecture search and automated machine learning.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#widely-applicable-bayesian-information-criterion",
    "href": "19-select.html#widely-applicable-bayesian-information-criterion",
    "title": "1  Model Selection",
    "section": "1.7 Widely applicable Bayesian Information Criterion",
    "text": "1.7 Widely applicable Bayesian Information Criterion\nThe Widely applicable Bayesian Information Criterion (WBIC) is a generalization of the traditional BIC that addresses some of its fundamental limitations, particularly when dealing with singular statistical models and complex machine learning architectures. WBIC was developed by Sumio Watanabe as part of his singular learning theory, which provides a mathematical framework for understanding the behavior of statistical models that don’t satisfy the regularity conditions assumed by classical statistical theory.\n\n1.7.1 Motivation for WBIC\nThe standard BIC, while useful, relies on several restrictive assumptions that often fail in modern machine learning contexts. First, BIC assumes that the true parameter lies in the interior of the parameter space and that the Fisher information matrix is positive definite. These regularity conditions fail for many important models such as neural networks with hidden units, mixture models where the number of components is unknown, tree-based models with unknown structure, and models with parameter constraints or boundaries.\nSecond, BIC requires knowing the effective number of parameters \\(k\\), which can be ambiguous for complex models. This becomes problematic when dealing with shared parameters across different parts of the model, regularization that effectively reduces the parameter dimension, or hierarchical structures where the effective dimensionality depends on the data. The challenge of defining the “true” number of parameters in modern machine learning models makes BIC’s penalty term difficult to specify correctly.\nThird, BIC’s theoretical guarantees assume large sample sizes, but its performance can be poor for finite samples, especially when the model is complex relative to the data size. This limitation is particularly relevant in contemporary applications where high-dimensional models are applied to moderate-sized datasets, a common scenario in domains such as genomics, natural language processing, and computer vision.\nWBIC addresses these limitations through a temperature-based approach that connects Bayesian model selection to statistical mechanics. The WBIC is defined as:\n\\[\n\\text{WBIC} = 2n \\mathbb{E}_{\\beta^{-1}}[\\log p(D|\\theta)] - 2 \\log \\int p(D|\\theta)^{\\beta} p(\\theta) d\\theta\n\\]\nwhere \\(\\beta = 1/\\log n\\) is an inverse temperature parameter that decreases as the sample size \\(n\\) increases, and \\(\\mathbb{E}_{\\beta^{-1}}[\\cdot]\\) denotes expectation with respect to the posterior distribution at inverse temperature \\(\\beta^{-1}\\).\nMore practically, WBIC can be computed as:\n\\[\n\\text{WBIC} = \\frac{1}{T} \\sum_{t=1}^{T} \\left[ -2 \\log p(D|\\theta^{(t)}) \\right]\n\\]\nwhere \\(\\{\\theta^{(t)}\\}_{t=1}^{T}\\) are samples from the tempered posterior distribution:\n\\[\np_{\\beta}(\\theta|D) \\propto p(D|\\theta)^{\\beta} p(\\theta)\n\\]\nwith \\(\\beta = 1/\\log n\\).\nThe temperature parameter \\(\\beta = 1/\\log n\\) serves several important purposes in the WBIC framework. As the sample size \\(n\\) increases, \\(\\beta\\) decreases, making the tempered posterior more diffuse than the standard posterior. This provides automatic regularization that prevents overfitting by spreading the posterior probability over a wider range of parameter values. The temperature mechanism allows WBIC to work with singular models where the standard likelihood-based methods fail, addressing one of the key limitations of traditional information criteria.\nUnlike BIC’s asymptotic approximation, WBIC provides finite-sample corrections that improve performance for moderate sample sizes. This is particularly valuable in practical applications where the asymptotic regime may not be reached or where the effective sample size is reduced due to high-dimensional parameter spaces.\nWhen the regularity conditions hold and the sample size is large, WBIC converges to BIC:\n\\[\n\\lim_{n \\to \\infty} \\text{WBIC} = \\text{BIC} = -2\\log p(D|\\hat{\\theta}) + k \\log n\n\\]\nHowever, for singular models or finite samples, WBIC can provide substantially different and often more accurate model selection results. The convergence property ensures that WBIC maintains the desirable asymptotic properties of BIC while extending its applicability to a much broader class of models.\nThe WBIC naturally implements Occam’s razor through its temperature mechanism. Simpler models with fewer effective parameters receive higher weight because they have smaller variance in their log-likelihood values, the temperature averaging tends to favor models with more concentrated likelihood surfaces, and complex models are automatically penalized through the integral term in the WBIC expression. This automatic penalty adjustment is particularly valuable when the effective number of parameters is difficult to determine a priori.\n\n\n1.7.2 Practical Implementation\nComputing WBIC requires sampling from a tempered posterior, which can be accomplished through several computational approaches. Tempered MCMC modifies standard MCMC algorithms to sample from \\(p_{\\beta}(\\theta|D)\\) instead of the standard posterior, requiring adjustments to the acceptance probabilities and proposal distributions. Parallel tempering runs multiple MCMC chains at different temperatures and uses the chain at \\(\\beta = 1/\\log n\\), which can be computationally intensive but provides robust sampling. When MCMC is computationally prohibitive, variational approximation methods can be used to approximate the tempered posterior, trading some accuracy for computational efficiency.\nHere’s a simple example of how WBIC might be computed for a Bayesian linear regression model:\n# Compute WBIC for Bayesian linear regression\ncompute_wbic &lt;- function(X, y, prior_precision = 1) {\n  n &lt;- length(y)\n  beta_temp &lt;- 1 / log(n)  # Temperature parameter\n  \n  # Set up tempered posterior sampling\n  # (This is a simplified example - real implementation would use proper MCMC)\n  \n  # Sample from tempered posterior\n  n_samples &lt;- 1000\n  log_likes &lt;- numeric(n_samples)\n  \n  for (i in 1:n_samples) {\n    # Sample theta from tempered posterior p(theta|D)^beta\n    # ... MCMC sampling code ...\n    \n    # Compute log-likelihood at sampled theta\n    log_likes[i] &lt;- compute_log_likelihood(theta_sample, X, y)\n  }\n  \n  # WBIC estimate\n  wbic &lt;- -2 * mean(log_likes)\n  return(wbic)\n}\nWBIC offers several significant advantages over traditional information criteria. It works with singular models where BIC fails, provides finite-sample corrections that improve performance in practical applications, automatically handles effective parameter counting without requiring explicit specification, is theoretically grounded in singular learning theory, and often provides more accurate model selection for complex models. These properties make WBIC particularly valuable for modern machine learning applications where model singularities and complex parameter structures are common.\nHowever, WBIC also has important limitations that practitioners should consider. It is computationally more expensive than BIC because it requires MCMC sampling rather than simple point estimates. It is less familiar to practitioners than standard information criteria, potentially creating barriers to adoption. Implementation can be complex for some model types, particularly when designing appropriate tempering schemes. Finally, while theoretically grounded, the practical properties of WBIC are less well-understood for some applications compared to the extensive literature on traditional information criteria.\nThe choice between WBIC and BIC depends on several factors related to the model, data, and computational constraints. WBIC is preferable when working with neural networks, mixture models, or other potentially singular models where BIC’s regularity assumptions fail. It is also advantageous when the effective number of parameters is unclear or when sample sizes are moderate and BIC’s asymptotic approximations may not be reliable. Additionally, WBIC should be considered when computational resources allow for the additional complexity of tempering and sampling procedures.\nConversely, BIC remains the better choice when working with simple, regular models such as linear regression or basic generalized linear models where the regularity conditions are satisfied. BIC is also preferable for large sample sizes where asymptotic approximations are accurate, when computational efficiency is paramount and the additional complexity of WBIC cannot be justified, or when the number of parameters is well-defined and small enough that BIC’s penalty term is appropriate.\nWBIC represents an important advancement in Bayesian model selection, extending the applicability of information-theoretic model selection to the complex, high-dimensional models that are increasingly common in modern data science and machine learning applications. As the field continues to develop models that violate traditional statistical assumptions, tools like WBIC become essential for principled model comparison and selection.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "19-select.html#double-descent",
    "href": "19-select.html#double-descent",
    "title": "1  Model Selection",
    "section": "1.8 Double Descent",
    "text": "1.8 Double Descent\nDouble descent is a phenomenon of over-parameterized statistical models. In this section, we present a view of double descent from a Bayesian perspective. Over-parameterized models such as deep neural networks have an interesting re-descending property in their risk characteristics. This is a recent phenomenon in machine learning and has been the subject of many studies. As the complexity of the model increases, there is a U-shaped region corresponding to the traditional bias-variance trade-off, but then as the number of parameters equals the number of observations and the model becomes one of interpolation, the risk can become infinite and then, in the over-parameterized region, it re-descends—the double descent effect. We show that this has a natural Bayesian interpretation. Moreover, we show that it is not in conflict with the traditional Occam’s razor that Bayesian models possess, in that they tend to prefer simpler models when possible.\nEmpirically, the double descent effect was initially observed for high-dimensional neural network regression models and the good performance of these models on such tasks as large language models, image processing, and generative AI methods(Nareklishvili, Polson, and Sokolov 2023). The double descent effect extends the classical bias-variance trade-off curve that shrinkage estimators possess. This phenomenon was first observed in the context of linear regression(Belkin et al. 2019). The authors showed that the test error of the estimator can decrease as the number of parameters increases. Bach (2024) extends these results to stochastic regression models.\nInterpolators—estimators that achieve zero training error—were then shown to have attractive properties due to the double descent effect(Hastie et al. 2022). Our goal is to show that Bayesian estimators can also possess a double descent phenomenon. Interpolators such as ReLU neural networks(Polson, Sokolov, et al. 2017) have increased in popularity with many applications such as traffic flow modeling(Polson, Sokolov, et al. 2017) and high-frequency trading(Dixon, Polson, and Sokolov 2019), among many others.\nOccam’s razor—the favoring of simpler models over complex ones—is a natural feature of Bayesian methods that are based on the weight of evidence (a.k.a. the marginal likelihood of the data). To do this, they penalize models with higher complexity via a correction term as in the Bayesian Information Criterion (BIC). This seems inconsistent with the double descent phenomenon. We show that this is not the case, as even though Bayesian methods shift the posterior towards lower-complexity models, highly parameterized Bayesian models can also have good risk properties due to the conditional prior of parameters given the model. We illustrate this with an application to neural network models.\nDouble descent has been studied from a frequentist point of view in Belkin et al. (2019), Bach (2024). The phenomenon of double descent is illustrated in ?fig-double-descent. The first part of the curve represents the classical U-shaped bias-variance trade-off. The second part demonstrates the double descent phenomenon, where the test error of the estimator can decrease as the model becomes over-parameterized beyond the interpolation threshold. This phenomenon was later observed in the context of deep learning(Nakkiran et al. 2021). The authors showed that the test error of the estimator can decrease as the number of parameters increases.\n\nExample 1.7 (Double Descent Demonstration using Polynomial Regression) To illustrate the double descent phenomenon in a concrete setting, we present a detailed example using polynomial regression with Legendre basis functions. This example demonstrates how the test error can exhibit the characteristic U-shaped curve followed by a re-descent as model complexity increases far beyond the interpolation threshold.\nOur demonstration uses a one-dimensional regression problem where we attempt to learn a sinusoidal function \\(f(x) = \\sin(5x)\\) from a small dataset of only \\(n = 20\\) observations sampled from the interval \\([-1, 1]\\). We add Gaussian noise with standard deviation \\(\\sigma = 0.3\\) to simulate realistic measurement error. The choice of a small sample size is crucial for observing double descent, as it creates a regime where the number of model parameters can substantially exceed the number of observations.\nWe fit polynomial models of varying degrees \\(d = 1, 2, \\ldots, 50\\) using Legendre polynomial basis functions. Legendre polynomials provide a numerically stable orthogonal basis that helps avoid the numerical instabilities associated with standard monomial bases in high-degree polynomial fitting. For each degree \\(d\\), we estimate the coefficients using the Moore-Penrose pseudoinverse, which provides the minimum-norm solution when the system is overdetermined (i.e., when \\(d &gt; n\\)).\nFigure 1.9 illustrates how model behavior changes dramatically across different polynomial degrees. The four panels show representative cases that capture the key phases of the double descent phenomenon:\n\nDegree 1 (Underparameterized): The linear model is too simple to capture the oscillatory nature of the underlying sine function, resulting in high bias and poor fit to both training and test data.\nDegree 5 (Classical Optimum): This represents the sweet spot of the classical bias-variance tradeoff, where the model has sufficient complexity to capture the main features of the sine function without overfitting severely.\nDegree 20 (Interpolation Threshold): At this degree, the model has exactly as many parameters as training observations, enabling perfect interpolation of the training data. However, the resulting fit exhibits wild oscillations between data points, leading to poor generalization performance.\nDegree 50 (Over-parameterized): Surprisingly, despite having far more parameters than observations, this highly over-parameterized model achieves better test performance than the interpolating model, demonstrating the double descent effect.\n\n\n\nCode\nn = 20  # number of samples\nsigma = 0.3  # stdev of noise\na = -1\nb = 1  # range of x values\n# Use Legendre polynomial basis by default\nG = np.polynomial.legendre.legvander\n\ndef poly(pts, beta, d):\n    return G(pts, d).dot(beta)\n\n# Initialize data\nground_truth = sin\nx = np.linspace(a, b, n)\ny = ground_truth(x) + sigma * np.random.normal(size=n)\nxdense = np.linspace(a, b, 100)\nygt = ground_truth(xdense)\n\n# Core functions\ndef solve_reg(A, y, lamb):\n    return la.solve(A.T.dot(A) + lamb * np.identity(A.shape[1]), A.T.dot(y))\n\ndef fit(d):\n    betaHat = la.pinv(G(x, d)).dot(y)\n    mseos = np.sqrt(np.mean((G(xdense, d).dot(betaHat) - ygt)**2))\n    mseis = np.sqrt(np.mean((G(x, d).dot(betaHat) - y)**2))\n    return betaHat, mseos, mseis\n\ndef run(d, ax):\n    \"\"\"Compute the regressor for degree d, and plot the solution.\"\"\"\n    betaHat, mseos, mseois = fit(d)\n    ax.plot(xdense, ygt, label='ground-truth')\n    ax.scatter(x, y, c='r', label='samples')\n    ax.plot(xdense, poly(xdense, betaHat, d), label='model')\n    ax.set_ylim(-2, 2)\n    ax.axis('off')\n    ax.legend()\n    ax.set_title('d=%d, MSE: Test=%.2f, Train=%.2f' % (d, mseos, mseois))\n\n\n\n\nCode\n# Create 2x2 subplot grid (all functions and variables are now persistent!)\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.suptitle('Double Descent Phenomenon: Polynomial Regression', fontsize=16)\n\n# Plot each degree in its respective subplot\nrun(1, axes[0, 0])   # Top left\nrun(5, axes[0, 1])   # Top right  \nrun(20, axes[1, 0])  # Bottom left\nrun(50, axes[1, 1])  # Bottom right\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1.9: Double Descent Phenomenon: Polynomial Regression with Different Degrees\n\n\n\n\n\nNow, let’s plot the MSE curve. We will plot the test error (blue line) and the training error (red line) for different polynomial degrees from 1 to 50.\n\n\nCode\n# Generate MSE data for different polynomial degrees (using persistent functions!)\nnd = 50\nmse1 = np.zeros(nd)  # Test MSE\nmse2 = np.zeros(nd)  # Train MSE\n\nfor d in range(1, nd):\n    betaHat, mseos, mseois = fit(d)\n    mse1[d] = mseos\n    mse2[d] = mseois\nfig, ax1 = plt.subplots()\nax1.set_xlabel('Number of Features')\nax1.set_ylabel('MSE', color='black')\nax1.plot(np.log10(mse1), color='blue', label='out-of-sample')\nax2 = ax1.twinx()\nax2.plot(np.log10(mse2), color='red', label='in-sample')\n# plt.plot(mse1, label='out-of-sample')\n# plt.plot(mse2, label='in-sample')\nfig.legend()\n\n\n\n\n\n\n\n\nFigure 1.10: Bias-Variance Trade-off: Training and Test MSE vs Model Complexity\n\n\n\n\n\nThe key insight from Figure 1.10 is the characteristic double descent shape in the test error (blue line). The curve exhibits three distinct phases:\n\nClassical Regime: For low degrees (\\(d &lt; 5\\)), increasing model complexity reduces both bias and test error, following the traditional understanding of the bias-variance tradeoff.\nInterpolation Crisis: Around the interpolation threshold (\\(d \\approx n = 20\\)), test error peaks dramatically as the model begins to perfectly fit the training data while generalizing poorly.\nOver-parameterized Regime: For very high degrees (\\(d &gt; 30\\)), test error decreases again, demonstrating that extreme over-parameterization can lead to improved generalization despite the model’s ability to memorize the training data.\n\nThis behavior challenges the conventional wisdom that more parameters necessarily lead to worse generalization. The double descent phenomenon arises from the implicit regularization effects of minimum-norm solutions in over-parameterized settings. When \\(d &gt; n\\), the pseudoinverse solution corresponds to the minimum \\(\\ell_2\\)-norm coefficients among all possible interpolating solutions. This implicit bias toward simpler functions can lead to surprisingly good generalization properties.\nWhile this example uses polynomial regression for clarity, the double descent phenomenon has been observed across a wide range of modern machine learning models, including deep neural networks, random forests, and kernel methods. The implications for practice are significant. Given that model selection is time consuming and computationally expensive, this example shows, that instead of spending time to do model selection to find the “sweet spot” model with 5-degree polynomial, we just over-parametrise and get a good model for free!\nThis example serves as a concrete illustration of how classical statistical intuitions about model complexity may not apply in contemporary machine learning settings, particularly when dealing with over-parameterized models that have become increasingly common in practice.\n\n\n\n\n\nBach, Francis. 2024. “High-Dimensional Analysis of Double Descent for Linear Regression with Random Projections.” SIAM Journal on Mathematics of Data Science 6 (1): 26–50.\n\n\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. “Reconciling Modern Machine-Learning Practice and the Classical Bias–Variance Trade-Off.” Proceedings of the National Academy of Sciences 116 (32): 15849–54.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019. “Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows and High Frequency Trading.” Applied Stochastic Models in Business and Industry 35 (3): 788–807.\n\n\nHastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. 2022. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation.” The Annals of Statistics 50 (2): 949–86.\n\n\nLindley, D. V. 1961. “The Use of Prior Probability Distributions in Statistical Inference and Decisions.” In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, 4.1:453–69. University of California Press.\n\n\nMacKay, David JC. 1992. “Bayesian Interpolation.” Neural Computation 4 (3): 415–47.\n\n\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2021. “Deep Double Descent: Where Bigger Models and More Data Hurt*.” Journal of Statistical Mechanics: Theory and Experiment 2021 (12): 124003.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2023. “Generative Causal Inference,” June.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep Learning: A Bayesian Perspective.” Bayesian Analysis 12 (4): 1275–1304.",
    "crumbs": [
      "Deep Learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Acemoglu, Daron, and Pascual Restrepo. 2018. “Artificial\nIntelligence, Automation and Work.” National Bureau of Economic\nResearch.\n\n\nBach, Francis. 2024. “High-Dimensional Analysis of Double Descent\nfor Linear Regression with Random Projections.” SIAM Journal\non Mathematics of Data Science 6 (1): 26–50.\n\n\nBaylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,\nZakaria Haque, Salem Haykal, et al. 2017. “Tfx: A\nTensorflow-Based Production-Scale Machine Learning Platform.” In\nProceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 1387–95. ACM.\n\n\nBelkin, Mikhail, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019.\n“Reconciling Modern Machine-Learning Practice and the Classical\nBias–Variance Trade-Off.” Proceedings of the National Academy\nof Sciences 116 (32): 15849–54.\n\n\nBojarski, Mariusz, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, et al. 2016.\n“End to End Learning for Self-Driving Cars.” arXiv\nPreprint arXiv:1604.07316.\n\n\nDixon, Matthew F, Nicholas G Polson, and Vadim O Sokolov. 2019.\n“Deep Learning for Spatio-Temporal Modeling: Dynamic Traffic Flows\nand High Frequency Trading.” Applied Stochastic Models in\nBusiness and Industry 35 (3): 788–807.\n\n\nHastie, Trevor, Andrea Montanari, Saharon Rosset, and Ryan J.\nTibshirani. 2022. “Surprises in High-Dimensional Ridgeless Least\nSquares Interpolation.” The Annals of Statistics 50 (2):\n949–86.\n\n\nHermann, Jeremy, and Mike Del Balso. 2017. “Meet Michelangelo:\nUber’s Machine Learning Platform.”\n\n\nindeed. 2018. “Jobs of the Future: Emerging Trends in\nArtificial Intelligence.”\n\n\nLindley, D. V. 1961. “The Use of Prior\nProbability Distributions in Statistical Inference\nand Decisions.” In Proceedings of the\nFourth Berkeley Symposium on Mathematical\nStatistics and Probability, Volume 1:\nContributions to the Theory of\nStatistics, 4.1:453–69. University of California\nPress.\n\n\nMacKay, David JC. 1992. “Bayesian Interpolation.”\nNeural Computation 4 (3): 415–47.\n\n\nMehrasa, Nazanin, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\nMori. 2017. “Learning Person Trajectory Representations for Team\nActivity Analysis.” arXiv Preprint arXiv:1706.00893.\n\n\nNakkiran, Preetum, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak,\nand Ilya Sutskever. 2021. “Deep Double Descent: Where Bigger\nModels and More Data Hurt*.” Journal of Statistical\nMechanics: Theory and Experiment 2021 (12): 124003.\n\n\nNareklishvili, Maria, Nicholas Polson, and Vadim Sokolov. 2023.\n“Generative Causal Inference,” June.\n\n\nPolson, Nicholas G, and James Scott. 2018. AIQ: How\nPeople and Machines Are Smarter Together. St. Martin’s Press.\n\n\nPolson, Nicholas G, Vadim Sokolov, et al. 2017. “Deep\nLearning: A Bayesian Perspective.”\nBayesian Analysis 12 (4): 1275–1304.\n\n\nPoplin, Ryan, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V\nMcConnell, Greg S Corrado, Lily Peng, and Dale R Webster. 2018.\n“Prediction of Cardiovascular Risk Factors from Retinal Fundus\nPhotographs via Deep Learning.” Nature Biomedical\nEngineering 2 (3): 158.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering\nChess and Shogi by Self-Play with\na General Reinforcement Learning Algorithm.” arXiv.\n\n\nTesauro, Gerald. 1995. “Temporal Difference Learning and\nTD-Gammon.” Communications of the ACM 38\n(3): 58–68.\n\n\nVarian, Hal R. 2010. “Computer Mediated\nTransactions.” American Economic Review 100 (2):\n1–10.\n\n\nWojna, Zbigniew, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu,\nYeqing Li, and Julian Ibarz. 2017. “Attention-Based Extraction of\nStructured Information from Street View Imagery.” arXiv\nPreprint arXiv:1704.03549.",
    "crumbs": [
      "References"
    ]
  }
]